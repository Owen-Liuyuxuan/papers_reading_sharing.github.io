<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-02 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-01/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-05/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-02">Arxiv Computer Vision Papers - 2026-01-02</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#spacetimepilot-generative-rendering-of-dynamic-scenes-across-space-and-time" class="nav-link">SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</a>
                </li>
                <li class="nav-item">
                    <a href="#gamo-geometry-aware-multi-view-diffusion-outpainting-for-sparse-view-3d-reconstruction" class="nav-link">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#edit3r-instant-3d-scene-editing-from-sparse-unposed-images" class="nav-link">Edit3r: Instant 3D Scene Editing from Sparse Unposed Images</a>
                </li>
                <li class="nav-item">
                    <a href="#from-inpainting-to-editing-a-self-bootstrapping-framework-for-context-rich-visual-dubbing" class="nav-link">From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing</a>
                </li>
                <li class="nav-item">
                    <a href="#foundationslam-unleashing-the-power-of-depth-foundation-models-for-end-to-end-dense-visual-slam" class="nav-link">FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</a>
                </li>
                <li class="nav-item">
                    <a href="#phystalk-language-driven-real-time-physics-in-3d-gaussian-scenes" class="nav-link">PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes</a>
                </li>
                <li class="nav-item">
                    <a href="#darkeqa-benchmarking-vision-language-models-for-embodied-question-answering-in-low-light-indoor-environments" class="nav-link">DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</a>
                </li>
                <li class="nav-item">
                    <a href="#viper-process-aware-evaluation-for-generative-video-reasoning" class="nav-link">VIPER: Process-aware Evaluation for Generative Video Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#semi-supervised-diversity-aware-domain-adaptation-for-3d-object-detection" class="nav-link">Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</a>
                </li>
                <li class="nav-item">
                    <a href="#finmmdocr-benchmarking-financial-multimodal-reasoning-with-scenario-awareness-document-understanding-and-multi-step-computation" class="nav-link">FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-02">Arxiv Computer Vision Papers - 2026-01-02</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域近期论文的执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2025-12-31)</strong></p>
<p><strong>1. 主要主题与趋势观察：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的快速进展：</p>
<ul>
<li><strong>3D 内容生成与重建的泛化与高效化：</strong> 多篇论文致力于从稀疏或非结构化数据中生成高质量的 3D 内容，并强调了跨空间和时间动态场景的渲染能力。</li>
<li><strong>基础模型（Foundation Models）在视觉任务中的应用深化：</strong> 特别是在 SLAM（同步定位与地图构建）领域，基础模型展现出端到端解决复杂问题的潜力。</li>
<li><strong>多模态理解与交互的进步：</strong> 结合语言、视觉和物理模拟，实现更具交互性和智能性的场景理解与内容生成。</li>
<li><strong>生成式模型的编辑与控制能力增强：</strong> 不仅限于生成，更侧重于对生成内容的精细化编辑和特定场景下的应用。</li>
<li><strong>特定领域（如金融、低光照环境）的视觉智能挑战：</strong> 针对实际应用场景的痛点，提出新的数据集和评估方法。</li>
</ul>
<p><strong>2. 亮点与创新论文：</strong></p>
<ul>
<li><strong>"SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time"</strong> 极具潜力，它展示了在时间和空间维度上生成动态场景的能力，这对于虚拟现实、电影制作等领域具有重要意义。</li>
<li><strong>"FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM"</strong> 标志着基础模型在 SLAM 领域的突破性进展，有望实现更鲁棒、更高效的 3D 地图构建。</li>
<li><strong>"PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes"</strong> 结合了语言驱动的物理模拟，为创建更具交互性和真实感的 3D 环境提供了新途径。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>跨时空动态场景生成：</strong> 从静态场景生成向动态、随时间变化的场景生成迈进。</li>
<li><strong>基于基础模型的端到端视觉 SLAM：</strong> 利用预训练的大规模模型解决传统 SLAM 的挑战。</li>
<li><strong>语言驱动的 3D 场景交互与物理模拟：</strong> 将自然语言指令转化为 3D 环境中的物理行为。</li>
<li><strong>稀疏数据下的 3D 重建与编辑：</strong> 降低对输入数据的要求，提高 3D 内容创作的灵活性。</li>
<li><strong>面向特定场景（低光照、金融）的视觉语言模型：</strong> 解决实际应用中的特定挑战，推动模型在真实世界中的落地。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>基于其潜在影响力和创新性，以下论文建议优先阅读全文：</p>
<ul>
<li><strong>"SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time"</strong>: 探索动态场景生成的前沿。</li>
<li><strong>"FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM"</strong>: 了解基础模型在 SLAM 领域的最新进展。</li>
<li><strong>"PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes"</strong>: 学习如何将语言指令与 3D 物理模拟结合。</li>
<li><strong>"GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction"</strong>: 关注从稀疏视图进行高质量 3D 重建的技术。</li>
</ul>
<hr />
<p>这份摘要旨在帮助您快速了解本期 Arxiv 论文的核心内容和重要趋势。希望它能为您节省宝贵的研究时间。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.25075v1">SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</a></li>
<li><a href="#2512.25073v1">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</a></li>
<li><a href="#2512.25071v1">Edit3r: Instant 3D Scene Editing from Sparse Unposed Images</a></li>
<li><a href="#2512.25066v1">From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing</a></li>
<li><a href="#2512.25008v1">FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</a></li>
<li><a href="#2512.24986v1">PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes</a></li>
<li><a href="#2512.24985v1">DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</a></li>
<li><a href="#2512.24952v1">VIPER: Process-aware Evaluation for Generative Video Reasoning</a></li>
<li><a href="#2512.24922v1">Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</a></li>
<li><a href="#2512.24903v1">FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.25075v1'></a></p>
<h2 id="spacetimepilot-generative-rendering-of-dynamic-scenes-across-space-and-time"><a href="https://arxiv.org/abs/2512.25075v1">SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</a></h2>
<p><strong>Authors:</strong> Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</p>
<p><strong>作者：</strong> Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决从单个单目视频中实现对动态场景进行可控的、任意的空间和时间探索性渲染这一核心挑战。现有的方法在独立控制相机视角和场景时间动态方面存在局限性，难以实现非单调的时间变化（如慢动作、倒放、子弹时间）与自由的相机运动相结合。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
SpaceTimePilot 提出了一种新颖的视频扩散模型，其核心创新在于：</p>
<ul>
<li><strong>空间与时间解耦：</strong> 模型能够独立地改变相机视角和运动序列，从而实现对场景的连续和任意的空间时间探索。</li>
<li><strong>动画时间嵌入机制（Animation Time-Embedding）：</strong> 引入了一种有效的机制，允许在扩散过程中明确控制输出视频的运动序列相对于源视频的运动。</li>
<li><strong>时间扭曲（Temporal Warping）训练策略：</strong> 为了解决缺乏包含连续时间变化的动态场景配对数据集的问题，作者提出了一种简单而有效的训练策略。该策略通过重用现有的多视图数据集，并对其进行时间扭曲处理，来模拟不同的时间变化，从而使模型能够学习到时间控制能力。</li>
<li><strong>改进的相机条件化机制：</strong> 引入了一种更精确的相机条件化方法，允许从第一帧开始就改变相机视角，并考虑源视频的相机轨迹，以实现更一致的生成。</li>
<li><strong>Cam×Time 数据集：</strong> 构建了一个首个合成的、覆盖完整空间时间网格的渲染数据集。该数据集提供了场景内完全自由的空间时间视频轨迹，为模型提供了丰富的监督信号，以学习解耦的空间和时间控制。</li>
</ul>
<p><strong>3. 主要结果及意义：</strong>
通过上述创新，SpaceTimePilot 在以下方面取得了显著成果：</p>
<ul>
<li><strong>实现完全解耦的空间和时间控制：</strong> 模型能够生成具有任意相机轨迹和时间动态（如慢动作、倒放、子弹时间、重复、加速、之字形运动等）的连贯视频。</li>
<li><strong>优于现有方法：</strong> 在定量和定性评估中，SpaceTimePilot 在时间控制和相机控制方面均显著优于现有最先进的方法，尤其是在处理复杂的时空变化时。</li>
<li><strong>支持更长视频生成：</strong> 通过多轮自回归推理，模型能够生成任意长度的、连贯的时空探索视频。</li>
<li><strong>为可控视频生成设定新基准：</strong> Cam×Time 数据集的构建为未来在精细化时空建模方面的研究提供了新的基准。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到了一些潜在的局限性：</p>
<ul>
<li><strong>计算成本：</strong> 扩散模型本身通常计算成本较高，生成高质量视频需要大量的计算资源。</li>
<li><strong>数据集的合成性质：</strong> 虽然 Cam×Time 数据集提供了丰富的监督，但其合成性质可能无法完全捕捉真实世界场景的复杂性。</li>
<li><strong>单目视频的固有挑战：</strong> 从单目视频中恢复完整的 3D 几何信息仍然是一个挑战，尽管模型在一定程度上克服了这个问题。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于该工作，未来的研究方向可能包括：</p>
<ul>
<li><strong>提高生成效率：</strong> 进一步优化模型架构和训练策略，以降低生成视频的计算成本，实现更快的实时生成。</li>
<li><strong>更真实的场景渲染：</strong> 探索更先进的渲染技术，以生成更逼真、细节更丰富的视频，并更好地处理复杂的光照和材质。</li>
<li><strong>更广泛的应用：</strong> 将该技术应用于更广泛的领域，如虚拟现实、电影制作、游戏开发等，实现更具沉浸感和交互性的内容创作。</li>
<li><strong>更精细的控制：</strong> 探索更细粒度的控制方式，例如允许用户直接编辑场景中的特定对象或运动。</li>
<li><strong>结合其他模态：</strong> 探索将文本、音频等其他模态信息融入到控制中，实现更丰富的多模态视频生成。</li>
</ul>
<p>总而言之，SpaceTimePilot 是一个重要的进展，它通过引入创新的时间嵌入机制、时间扭曲训练策略和专门的数据集，成功地实现了视频中空间和时间控制的解耦，为生成具有任意时空轨迹的动态场景视频开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering.</li>
<li>To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video.</li>
<li>As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences.</li>
<li>To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.25075v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.25075v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.25073v1'></a></p>
<h2 id="gamo-geometry-aware-multi-view-diffusion-outpainting-for-sparse-view-3d-reconstruction"><a href="https://arxiv.org/abs/2512.25073v1">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</a></h2>
<p><strong>Authors:</strong> Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a <script type="math/tex">25\times</script> speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction”论文的全面中文摘要：</p>
<p><strong>论文题目：</strong> GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</p>
<p><strong>作者：</strong> Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文主要解决了在<strong>稀疏多视图输入下进行高质量三维（3D）场景重建</strong>的挑战。尽管现有的三维重建技术在密集视图下表现出色，但在输入视图有限的情况下，它们往往会遇到以下问题：
*   <strong>覆盖范围不足：</strong> 难以重建输入视图之外的区域，导致场景中存在空洞（holes）和未覆盖区域。
*   <strong>几何不一致性：</strong> 生成的新视图之间可能存在几何上的不匹配，导致鬼影（ghosting）和不连贯的几何结构。
*   <strong>计算成本高昂：</strong> 现有的一些先进方法，特别是基于扩散模型生成新视图的方法，计算流程复杂且耗时。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
为了解决上述问题，作者提出了<strong>GaMO（Geometry-aware Multi-view Outpainter）</strong>框架，其核心创新在于将<strong>多视图“外绘画”（outpainting）</strong>作为一种新的范式来处理稀疏视图重建。与生成全新视角不同，GaMO旨在<strong>扩展现有视图的视野（Field of View, FOV）</strong>，从而在不引入新相机位姿的情况下，填充场景的未知区域。</p>
<p>GaMO框架包含三个主要阶段：
*   <strong>粗糙三维初始化（Coarse 3D Initialization）：</strong> 利用DUSt3R生成初始点云，并训练一个粗糙的3DGS模型，以获取场景的几何先验（如不透明度掩码和粗糙渲染图）。这些先验用于指导后续的外绘画过程。
*   <strong>几何感知多视图外绘画（GaMO: Geometry-aware Multi-view Outpainter）：</strong> 这是GaMO的核心。它利用一个预训练的多视图扩散模型，通过以下关键技术实现几何感知的外绘画：
    *   <strong>多视图条件化（Multi-view Conditioning）：</strong> 结合输入视图的Plücker射线嵌入、相机参数、以及经过变换和增强的规范坐标图（CCM）和RGB图像，为扩散模型提供丰富的几何和外观信息。
    *   <strong>掩码隐层融合（Mask Latent Blending）：</strong> 利用粗糙几何先验（不透明度掩码和粗糙渲染图）来指导扩散模型的去噪过程。通过迭代地调整掩码大小和进行噪声重采样，确保外绘画内容与已知区域的几何一致性，并逐步融合。
    *   <strong>零样本（Zero-shot）推理：</strong> GaMO在预训练模型上进行推理，无需针对特定场景进行微调，大大提高了效率。
*   <strong>三维高斯溅射（3DGS）精炼重建（Refined Reconstruction）：</strong> 将外绘画生成的高质量、宽FOV视图与原始输入视图结合，用于精炼3DGS模型，最终获得更完整、更一致的三维场景重建。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在Replica和ScanNet++等数据集上，GaMO在3、6和9个输入视图的设置下，均取得了<strong>最先进（state-of-the-art）的重建质量</strong>，在PSNR和LPIPS等指标上显著优于现有方法。
*   <strong>效率提升：</strong> GaMO的<strong>处理速度比最先进的扩散模型方法快25倍</strong>，并且在10分钟内即可完成重建，大大降低了计算成本。
*   <strong>解决关键问题：</strong> GaMO有效地解决了稀疏视图重建中的空洞、鬼影和几何不一致性等问题，生成了更完整、更具几何一致性和视觉清晰度的新视图。
*   <strong>泛化能力：</strong> 在Mip-NeRF 360数据集上的评估表明，GaMO在处理大规模、复杂场景（包括室内外环境）时也表现出良好的泛化能力。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>严重遮挡区域：</strong> GaMO（以及其他生成新视图的方法）在处理输入视图中<strong>完全未被观察到的、严重遮挡的区域</strong>时仍然存在局限性。
*   <strong>输入视图分布依赖性：</strong> 重建效果会受到输入视图分布的影响，过于聚集或对齐不佳的视图可能导致较差的结果。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>自适应外绘画尺度选择：</strong> 探索更智能的方法来选择外绘画的尺度，以更好地适应不同场景和遮挡情况。
*   <strong>多视角几何感知：</strong> 利用更先进的几何感知机制，例如从鸟瞰图或俯视角的视角生成外绘画，以覆盖更多被遮挡的区域。
*   <strong>混合方法：</strong> 结合外绘画和新视图生成等多种技术，以应对更具挑战性的场景。</p>
<p><strong>总结：</strong>
GaMO通过引入“多视图外绘画”这一新颖范式，巧妙地解决了稀疏视图三维重建中的核心挑战。它利用几何先验和多视图条件化，有效地扩展了现有视图的视野，生成了具有高度几何一致性和视觉质量的外绘画视图，进而极大地提升了三维重建的完整性和准确性。GaMO不仅在性能上取得了显著突破，还在效率上实现了大幅提升，为稀疏视图三维重建领域提供了一种更高效、更有效的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques.</li>
<li>Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines.</li>
<li>We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting.</li>
<li>Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage.</li>
<li>Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training.</li>
<li>Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a <script type="math/tex">25\times</script> speedup over SOTA diffusion-based methods with processing time under 10 minutes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.25073v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.25073v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.25071v1'></a></p>
<h2 id="edit3r-instant-3d-scene-editing-from-sparse-unposed-images"><a href="https://arxiv.org/abs/2512.25071v1">Edit3r: Instant 3D Scene Editing from Sparse Unposed Images</a></h2>
<p><strong>Authors:</strong> Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Edit3r: Instant 3D Scene Editing from Sparse Unposed Images</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话总结)</strong></p>
<p>Edit3r 提出了一种新颖的、前馈式的 3D 场景编辑框架，能够从稀疏、无姿态、甚至视图不一致的编辑图像中，在单次推理中直接预测并实现 3D 场景的编辑和渲染。该方法克服了传统方法需要逐场景优化的痛点，通过创新的训练策略解决了缺乏多视图一致性编辑图像的监督问题，实现了快速、逼真的 3D 编辑。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>Edit3r 的核心创新在于其<strong>直接预测指令对齐的 3D 编辑</strong>的能力，避免了耗时的逐场景优化和姿态估计。其关键方法论体现在以下两个方面：</p>
<ul>
<li><strong>SAM2-based recoloring strategy (基于 SAM2 的重着色策略):</strong> 这是解决训练数据稀缺问题的核心。由于缺乏多视图一致性的编辑图像作为监督信号，作者利用 SAM2 (Segment Anything Model 2) 的强大分割能力，在参考视图上进行精确的语义分割和重着色，生成跨视图一致的监督信号。这使得模型能够学习到如何将 2D 编辑指令映射到 3D 空间中的一致性变化。</li>
<li><strong>Asymmetric input strategy (非对称输入策略):</strong> 该策略将一个经过重着色的参考视图与原始的辅助视图配对输入网络。这种设计鼓励网络去融合和对齐来自不同视图（包括经过编辑和未编辑的）的异质信息，从而提升 3D 一致性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>Edit3r 的出现可能对 3D 内容创作、虚拟现实/增强现实以及游戏开发等领域产生深远影响：</p>
<ul>
<li><strong>降低 3D 编辑门槛:</strong> 极大地简化了 3D 场景的编辑流程，使得非专业人士也能通过简单的文本指令或 2D 编辑工具来修改 3D 环境，从而 democratize 3D 内容创作。</li>
<li><strong>加速 3D 内容生成:</strong> 实时或近乎实时的编辑能力将显著提高 3D 内容的迭代速度和生产效率。</li>
<li><strong>推动 3D 场景理解和生成的研究:</strong> 这种直接从指令到 3D 编辑的端到端方法，将激励更多研究关注如何让模型更深入地理解 3D 场景的语义和结构，并直接进行修改。</li>
<li><strong>促进 2D 和 3D 技术的融合:</strong> 该工作展示了如何有效地将强大的 2D 编辑工具（如 InstructPix2Pix）的能力迁移到 3D 领域，为未来的跨模态内容生成提供了新的思路。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 用户可以实时编辑虚拟环境，例如在 AR 应用中改变现实场景的物体颜色、材质或添加/移除元素。</li>
<li><strong>游戏开发:</strong> 游戏设计师可以快速迭代关卡设计、场景布局和物体属性，提高开发效率。</li>
<li><strong>3D 内容创作平台:</strong> 如 Sketchfab, Unity, Unreal Engine 等平台可以集成此类技术，提供更直观、更强大的编辑工具。</li>
<li><strong>建筑可视化和室内设计:</strong> 用户可以轻松地修改建筑模型或室内设计方案，例如改变墙壁颜色、家具摆放等。</li>
<li><strong>数字孪生和仿真:</strong> 在数字孪生场景中，可以根据需求快速修改场景元素以进行仿真测试。</li>
<li><strong>电影和视觉特效 (VFX):</strong> 为后期制作提供更快捷的场景修改手段。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要中强调了 Edit3r 的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对输入图像的依赖性:</strong> 尽管处理的是稀疏图像，但编辑效果仍然会受到输入图像的数量、质量和覆盖范围的影响。如果输入图像过于稀疏或视角覆盖不足，可能难以生成完整和准确的 3D 编辑。</li>
<li><strong>对 2D 编辑工具的依赖:</strong> Edit3r 的训练和推理依赖于 2D 编辑工具（如 InstructPix2Pix）生成的编辑图像。如果 2D 编辑工具本身存在局限性，例如生成不自然的编辑效果，那么 Edit3r 的输出也可能受到影响。</li>
<li><strong>“指令对齐”的精度:</strong> 摘要中提到“指令对齐的 3D 编辑”，但“对齐”的程度和精确度可能是一个挑战。模型能否完全理解并精确执行复杂的、细粒度的编辑指令，仍需进一步验证。</li>
<li><strong>对新颖场景的泛化能力:</strong> 虽然引入了 DL3DV-Edit-Bench 进行大规模评估，但模型在训练数据之外的、完全新颖的场景类型上的泛化能力仍需观察。</li>
<li><strong>“视图不一致”的处理能力:</strong> 摘要提到处理“视图不一致”的图像，但这种不一致的程度和类型可能有限。极端的不一致性（例如，物体在不同视图中存在根本性的差异）可能仍然难以处理。</li>
<li><strong>计算资源需求:</strong> 虽然强调了“高速推理”，但训练如此复杂的模型通常需要大量的计算资源。</li>
</ul>
<p>总而言之，Edit3r 是一项令人兴奋的研究，它通过创新的训练策略和前馈式架构，显著提升了 3D 场景编辑的效率和便捷性，有望成为未来 3D 内容创作的重要工具。其核心贡献在于将 2D 指令驱动的编辑能力无缝迁移到 3D 空间，并解决了训练数据稀缺的难题。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images.</li>
<li>For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total.</li>
<li>Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.25071v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.25071v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.25066v1'></a></p>
<h2 id="from-inpainting-to-editing-a-self-bootstrapping-framework-for-context-rich-visual-dubbing"><a href="https://arxiv.org/abs/2512.25066v1">From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing</a></h2>
<p><strong>Authors:</strong> Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing
<strong>Authors:</strong> Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu
<strong>Categories:</strong> cs.CV
<strong>Published Date:</strong> 2025-12-31</p>
<p><strong>Abstract:</strong>
Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.</p>
<hr />
<p><strong>分析结果：</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了一种新颖的“自举”框架，将音频驱动的视频配音任务从一个不适定的图像修复问题重新定义为一个条件良好的视频编辑问题。通过利用 Diffusion Transformer 生成理想的配对训练数据，并训练一个基于 DiT 的编辑器，该框架能够利用丰富的视觉上下文信息，实现更精确的唇部同步、更忠实的身份保持以及更强的鲁棒性。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>从 Inpainting 到 Editing 的范式转变：</strong> 这是最核心的创新。传统方法将视频配音视为一个“修复”任务，即在遮盖唇部区域后进行内容填充，这导致模型需要同时处理内容生成和唇部同步，容易出错。该论文将其重塑为“编辑”任务，即在完整的、对齐的视频帧上进行唇部修改，从而将问题简化并提升了条件。</li>
<li><strong>自举（Self-Bootstrapping）框架：</strong> 论文的核心在于如何获得“理想的训练数据”。它利用 Diffusion Transformer (DiT) 作为数据生成器，为每个真实视频样本生成一个“唇部修改但其他视觉信息一致”的伴侣视频。这样就创造了完美的配对数据，解决了现实世界中难以获得的训练数据问题。</li>
<li><strong>利用丰富的视觉上下文（Context-Rich Visual Dubbing）：</strong> 通过使用完整的、对齐的输入视频帧作为条件，编辑器能够获取到完整的身份线索、场景交互信息和时空动态。这种丰富的上下文信息是实现高精度唇部同步、身份保持和鲁棒性的关键。</li>
<li><strong>DiT 作为核心模型：</strong> 论文同时利用了 DiT 的生成能力（数据生成器）和编辑能力（音频驱动编辑器），充分发挥了其在图像和视频生成/编辑领域的强大潜力。</li>
<li><strong>Timestep-Adaptive Multi-Phase Learning Strategy：</strong> 针对扩散模型训练中可能出现的跨时间步的编辑目标冲突问题，提出了一种自适应多阶段学习策略，以实现更稳定的训练和更好的性能。</li>
<li><strong>ContextDubBench 数据集：</strong> 提出一个新的基准数据集，用于在更具挑战性的实际应用场景下进行鲁棒性评估，这对于推动该领域的研究和发展至关重要。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升视频配音的质量和真实感：</strong> 该方法有望显著提高视频配音的准确性、自然度和视觉一致性，减少现有方法的视觉伪影和身份漂移问题。</li>
<li><strong>解决训练数据稀缺的难题：</strong> 自举框架通过生成合成数据来解决真实数据稀缺的问题，为其他类似任务提供了新的思路。</li>
<li><strong>推动视频编辑和内容生成技术的发展：</strong> 该研究展示了如何利用先进的生成模型（如 DiT）进行精细化的视频编辑，并为其他需要精确内容控制的视频生成任务提供了借鉴。</li>
<li><strong>为多模态内容创作提供更强大的工具：</strong> 能够生成高质量的、与音频同步的视频内容，将极大地促进电影后期制作、虚拟现实、游戏开发等领域的创新。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>电影和电视制作：</strong> 用于为现有视频添加新的配音，实现多语言版本制作，或修复配音不准确的片段。</li>
<li><strong>虚拟角色和数字人：</strong> 使虚拟角色能够更自然地与用户进行语音交互，提升沉浸感。</li>
<li><strong>教育和培训：</strong> 为教学视频添加不同语言的配音，或更新课程内容。</li>
<li><strong>社交媒体和短视频：</strong> 允许用户轻松地为自己的视频添加口型同步的配音，创造更具吸引力的内容。</li>
<li><strong>无障碍技术：</strong> 为听障人士提供更直观的视频内容理解方式，例如通过精确的唇语同步。</li>
<li><strong>游戏开发：</strong> 提高游戏中角色对话的真实感和表现力。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>计算资源需求：</strong> Diffusion Transformer 本身是计算密集型的模型，生成高质量的视频数据和进行端到端的训练可能需要大量的计算资源（GPU/TPU）。</li>
<li><strong>对生成数据质量的依赖：</strong> 虽然自举框架旨在生成理想数据，但生成数据的质量直接影响最终编辑器的性能。如果生成器产生有偏差或不准确的数据，可能会影响编辑器的训练效果。</li>
<li><strong>泛化能力（In-the-wild scenarios 的挑战）：</strong> 尽管摘要声称具有“卓越的鲁棒性”，但“in-the-wild scenarios”通常意味着光照变化、遮挡、低分辨率、多人物等复杂情况。完全克服所有这些挑战可能仍然是一个持续的研究方向，摘要中的“鲁棒性”可能是在特定范围内或相对而言的。</li>
<li><strong>身份保持的绝对性：</strong> 摘要提到“忠实的身份保持”，但完全保证在所有情况下不发生任何细微的身份漂移（例如，微妙的面部表情变化）可能仍然是一个挑战，尤其是在极端编辑的情况下。</li>
<li><strong>多阶段学习策略的复杂性：</strong> Timestep-adaptive multi-phase learning strategy 听起来是一种复杂的技术，其实现和调优可能需要深入的专业知识。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文的亮点在于其<strong>范式上的根本性转变</strong>和<strong>创新的自举数据生成机制</strong>。通过将视频配音从一个“修复”问题转化为一个“编辑”问题，并利用 DiT 生成高质量的配对数据，该方法有望显著提升视频配音的质量和鲁棒性。其对“上下文丰富性”的强调，以及提出的自适应学习策略和新的基准数据集，都表明了该研究在技术深度和实用性上的追求。这篇论文对计算机视觉领域，特别是视频生成、编辑和多模态理解方向的研究者来说，具有重要的参考价值和启发意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical.</li>
<li>In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem.</li>
<li>Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs.</li>
<li>Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios.</li>
<li>Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.25066v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.25066v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.25008v1'></a></p>
<h2 id="foundationslam-unleashing-the-power-of-depth-foundation-models-for-end-to-end-dense-visual-slam"><a href="https://arxiv.org/abs/2512.25008v1">FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</a></h2>
<p><strong>Authors:</strong> Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM”的论文的全面摘要，重点关注其研究问题、创新方法、主要结果、局限性和未来方向。</p>
<p><strong>论文题目：</strong> FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</p>
<p><strong>作者：</strong> Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决了现有基于光流的单目稠密视觉SLAM系统在<strong>几何一致性</strong>方面的不足。以往的方法主要依赖于像素级的2D光流对应关系进行跟踪和建图，这导致重建的深度图容易出现结构性伪影、层叠模糊或几何不完整等问题，最终影响位姿估计的准确性和重建质量。其根本原因在于：
*   <strong>局部匹配的局限性：</strong> 密集对应估计仅在图像空间进行，缺乏对底层场景几何的感知，尤其在纹理稀疏或模糊区域容易产生不一致的匹配。
*   <strong>缺乏全局几何约束：</strong> 当前系统在优化过程中缺乏显式的多视图几何约束强制执行，也未能有效利用这些约束来优化光流预测，导致累积误差。</p>
<p><strong>2. 主要创新/方法贡献：</strong></p>
<p>FoundationSLAM 提出了一种<strong>端到端的、全可微的、紧耦合的框架</strong>，旨在将几何先验知识与多视图一致性优化相结合，以解决上述问题。其核心创新点包括：</p>
<ul>
<li><strong>混合光流网络 (Hybrid Flow Network)：</strong> 该网络利用<strong>基础深度模型（Foundation Depth Models）提供的几何先验</strong>来指导光流估计。这使得网络能够生成<strong>几何感知</strong>的对应关系，从而在不同关键帧之间实现更一致的深度和位姿推断。</li>
<li><strong>双一致性束调整层 (Bi-Consistent Bundle Adjustment Layer)：</strong> 这是一个新颖的优化层，它<strong>联合优化关键帧的位姿和深度</strong>，并强制执行<strong>多视图约束</strong>。通过引入<strong>流一致性残差</strong>（确保投影点与预测的对应点一致）和<strong>几何一致性残差</strong>（确保反向投影点与原始点一致），该层能够实现更全局、更一致的场景结构重建。</li>
<li><strong>可靠性感知细化机制 (Reliability-Aware Refinement Mechanism)：</strong> 该机制通过<strong>区分可靠和不确定的区域</strong>来动态调整光流更新过程。它利用束调整层产生的<strong>几何残差</strong>来构建像素级的可靠性掩码，并根据掩码的指示，对可靠区域采用局部相关性搜索，对不确定区域则依赖于几何先验进行修正。这形成了一个<strong>闭环反馈</strong>，使系统能够逐步改进光流预测的准确性和鲁棒性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>卓越的性能：</strong> FoundationSLAM 在 TUM-RGBD、EuRoC MAV 和 ETH3D-SLAM 等多个具有挑战性的标准 SLAM 基准测试中取得了<strong>最先进的轨迹精度和稠密重建质量</strong>。</li>
<li><strong>实时性：</strong> 该方法在单目 RGB 输入下，能够以 <strong>18 FPS 的速度运行</strong>，实现了性能与效率的良好平衡。</li>
<li><strong>泛化能力：</strong> 实验表明，FoundationSLAM 在各种场景下都表现出<strong>强大的泛化能力</strong>，尤其在反射性或纹理稀疏的环境中表现出色。</li>
<li><strong>鲁棒性：</strong> 在快速运动、灰度图像和宽基线等具有挑战性的场景下，FoundationSLAM 展现出<strong>高鲁棒性</strong>，优于许多现有方法。</li>
<li><strong>意义：</strong> 该研究成功地将强大的基础深度模型引入到稠密视觉 SLAM 中，通过<strong>端到端的、几何感知的优化框架</strong>，显著提升了传统基于光流方法的几何一致性和重建质量，为未来基于深度学习的 SLAM 系统开辟了新的方向。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中明确提及的局限性主要体现在：</p>
<ul>
<li><strong>对基础深度模型的依赖：</strong> 方法的性能在一定程度上依赖于基础深度模型的质量和准确性。</li>
<li><strong>计算成本：</strong> 虽然实现了实时性，但与一些更简单的 SLAM 方法相比，其计算成本仍然较高，尤其是在训练阶段。</li>
<li><strong>单目限制：</strong> 该方法是单目的，因此在缺乏纹理或存在大面积光滑表面时，其性能可能会受到影响（尽管其设计旨在缓解此问题）。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>基于该论文的研究，可以推测以下潜在的未来研究方向：</p>
<ul>
<li><strong>多模态融合：</strong> 将 FoundationSLAM 与其他传感器（如 IMU、立体相机）进行融合，以进一步提升在极端条件下的鲁棒性和精度。</li>
<li><strong>更强大的基础模型：</strong> 探索使用更先进、更通用的基础模型（如更强大的 3D 视觉模型）来进一步增强几何先验的质量和多样性。</li>
<li><strong>大规模场景下的长期 SLAM：</strong> 进一步研究如何处理大规模、动态变化的环境，以实现更长期的、无漂移的 SLAM。</li>
<li><strong>更高效的优化和推理：</strong> 探索更高效的优化算法和模型压缩技术，以在资源受限的设备上实现更高的帧率和更低的内存占用。</li>
<li><strong>主动感知与建图：</strong> 将 FoundationSLAM 与主动感知策略相结合，例如通过机器人控制来选择最优的观察视角，以最大化信息增益和建图质量。</li>
</ul>
<p>总而言之，FoundationSLAM 是一项重要的工作，它通过巧妙地融合基础深度模型和创新的几何一致性优化技术，显著提升了单目稠密视觉 SLAM 的性能，为该领域的研究和应用带来了新的突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping.</li>
<li>To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints.</li>
<li>Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization.</li>
<li>Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.25008v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.25008v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.24986v1'></a></p>
<h2 id="phystalk-language-driven-real-time-physics-in-3d-gaussian-scenes"><a href="https://arxiv.org/abs/2512.24986v1">PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes</a></h2>
<p><strong>Authors:</strong> Luca Collorone, Mert Kiray, Indro Spinelli, Fabio Galasso, Benjamin Busam</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a "render and wait" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>PhysTalk 的主要贡献在于提出了一种新颖的框架，能够将文本指令实时转化为基于物理的 3D 高斯场景（3DGS）的 4D 动画。它通过 LLM 生成可执行代码，直接驱动 3DGS 参数，实现了无需网格提取即可耦合 3DGS 与物理模拟，从而实现开放词汇、交互式的物理动画。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>直接耦合 3DGS 与物理模拟：</strong> 这是 PhysTalk 最核心的创新。以往的物理模拟通常需要先从场景中提取网格（mesh），然后进行物理计算，最后再渲染。PhysTalk 绕过了耗时的网格提取步骤，直接通过“轻量级代理”（lightweight proxies）和粒子动力学来修改 3DGS 的参数，实现了高效的物理交互。</li>
<li><strong>LLM 驱动的代码生成：</strong> 利用大型语言模型（LLM）将自然语言指令转化为可执行的代码，这是实现“语言驱动”的关键。这种方法使得用户可以通过简单的文本描述来控制复杂的物理动画。</li>
<li><strong>开放词汇和实时交互：</strong> PhysTalk 支持开放词汇的指令，意味着用户可以使用各种描述性的语言来驱动动画。同时，其实时性使得用户可以进行交互式的“对话式”动画创作，而非传统的“渲染等待”模式。</li>
<li><strong>轻量级代理和粒子动力学：</strong> 通过引入轻量级代理来代表 3DGS 中的高斯点，并结合粒子动力学来模拟物理行为，这是实现高效计算和实时性的技术基础。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>降低 3D 内容创作门槛：</strong> PhysTalk 有可能极大地降低 3D 内容创作的门槛，使得非专业人士也能通过自然语言轻松创建复杂的物理动画。</li>
<li><strong>推动实时物理模拟的发展：</strong> 将物理模拟与实时渲染技术（如 3DGS）高效结合，将为实时游戏、虚拟现实（VR）、增强现实（AR）等领域带来更逼真的交互体验。</li>
<li><strong>改变动画制作流程：</strong> 从传统的离线渲染和手动动画制作，转向更具交互性和迭代性的“对话式”创作模式，提高创作效率和灵活性。</li>
<li><strong>促进多模态 AI 的融合：</strong> PhysTalk 是语言理解与计算机图形学、物理模拟深度融合的典范，将推动多模态 AI 在内容生成领域的进一步发展。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>游戏开发：</strong> 实时、物理驱动的场景交互和特效生成。</li>
<li><strong>虚拟现实/增强现实：</strong> 创建更具沉浸感和交互性的虚拟环境。</li>
<li><strong>电影和视觉特效：</strong> 快速原型设计和生成复杂的物理效果。</li>
<li><strong>机器人学和仿真：</strong> 在虚拟环境中进行物理交互的训练和测试。</li>
<li><strong>教育和培训：</strong> 创建交互式的物理实验模拟。</li>
<li><strong>数字孪生：</strong> 实时模拟物理世界的动态变化。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>物理模拟的精度和复杂性：</strong> 虽然摘要强调了“物理真实性”，但直接通过代理和粒子动力学修改 3DGS 参数，可能在模拟极其复杂或精密的物理现象时存在精度上的限制，例如流体动力学、软体变形等。</li>
<li><strong>LLM 理解的鲁棒性：</strong> LLM 的理解能力虽然强大，但对于模糊、歧义或非常规的指令，可能仍会产生误解，导致动画不符合预期。</li>
<li><strong>3DGS 本身的局限性：</strong> 3DGS 在处理动态场景、透明物体、细微纹理等方面仍存在一些挑战，这些挑战可能会影响 PhysTalk 的整体表现。</li>
<li><strong>“轻量级代理”的通用性：</strong> 摘要提到“轻量级代理”，但其具体实现方式和对不同材质、形状物体的泛化能力需要进一步验证。</li>
<li><strong>训练数据和模型规模：</strong> 尽管 PhysTalk 是“train-free”的，但其底层的 LLM 和可能用于代理映射的模型可能需要大量的训练数据和计算资源。摘要中未详细说明这一点。</li>
<li><strong>交互的粒度：</strong> 摘要提到“交互式”，但具体的交互粒度（例如，用户可以控制到什么程度的细节）并未明确。</li>
</ul>
<p><strong>总结来说，PhysTalk 在技术上的趣味性和重要性在于它成功地打破了传统 3D 内容创作中“建模-物理模拟-渲染”的串联流程，通过 LLM 和创新的 3DGS 参数驱动方式，实现了语言到实时物理动画的直接转化。这不仅极大地提升了创作的效率和交互性，也为未来更智能、更易用的 3D 内容生成工具指明了方向。</strong></p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects.</li>
<li>Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a "render and wait" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.24986v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.24986v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.24985v1'></a></p>
<h2 id="darkeqa-benchmarking-vision-language-models-for-embodied-question-answering-in-low-light-indoor-environments"><a href="https://arxiv.org/abs/2512.24985v1">DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</a></h2>
<p><strong>Authors:</strong> Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments”的全面中文摘要：</p>
<p><strong>论文题目：</strong> DarkEQA: 针对低光照室内环境具身问答的视觉语言模型基准测试</p>
<p><strong>作者：</strong> Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>本文旨在解决当前视觉语言模型（VLM）在具身问答（EQA）任务中，尤其是在低光照室内环境下的鲁棒性评估不足的问题。现有的 EQA 基准测试大多假设理想的、光照充足的条件，而忽略了机器人（如家用机器人）在夜间、黑暗房间或停电等真实世界场景中经常遇到的低光照挑战。这种忽视导致 VLM 在实际应用中的表现可能远不如预期，限制了其24/7全天候运行的能力。因此，研究的核心问题是：<strong>如何系统地评估 VLM 在不同程度的低光照条件下的具身问答能力，并揭示其在这些挑战性视觉条件下的局限性？</strong></p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<ul>
<li><strong>DarkEQA 基准测试：</strong> 作者提出了一个名为 DarkEQA 的开源基准测试，专门用于评估 VLM 在多层次低光照条件下的 EQA 相关感知能力。</li>
<li><strong>物理保真度的低光照合成：</strong> DarkEQA 的核心创新在于其低光照图像合成方法。它在 <strong>线性 RAW 空间</strong> 中模拟了物理上的光照衰减（EV drop）和传感器噪声（如散粒噪声、读出噪声、行模式噪声和量化噪声），然后通过一个模拟图像信号处理（ISP）的渲染管线生成低光照图像。这种方法比简单的亮度调整更具物理真实性，能够更准确地模拟真实世界的低光照退化。</li>
<li><strong>解耦退化因素：</strong> 合成流程生成了两种类型的低光照图像：一种仅包含 EV drop（无噪声），另一种则同时包含 EV drop 和传感器噪声。这种设计允许研究者<strong>解耦光照衰减和传感器噪声对 VLM 性能的影响</strong>，从而进行更精细的鲁棒性分析。</li>
<li><strong>确定性的 QA 对生成：</strong> 为了确保基准测试的完整性并避免数据污染，DarkEQA 使用一个<strong>基于规则的确定性流程</strong>来生成问题-答案（QA）对，而不是依赖于商品化的 VLM 服务。这保证了每个 QA 对都有一个可验证的答案，并且整个过程是可复现的。</li>
<li><strong>多维度评估：</strong> DarkEQA 不仅评估 VLM 在不同低光照等级下的整体性能，还通过分析不同问题类型（如房间类型识别、房间功能检查、物体识别、物体属性、最近物体识别）下的准确率，深入揭示了 VLM 在特定感知任务上的脆弱性。此外，论文还评估了低光照图像增强（LLIE）模型作为预处理步骤的效果。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>VLM 在低光照下性能显著下降：</strong> 实验结果表明，随着低光照程度的加剧（EV drop 和噪声的增加），所有被评估的 VLM 的准确率都显著下降。</li>
<li><strong>传感器噪声是关键挑战：</strong> 引入传感器噪声比单纯的光照衰减对 VLM 性能的影响更为严重，凸显了传感器噪声在低光照感知中的关键作用。</li>
<li><strong>LLIE 效果不稳定：</strong> LLIE 模型在一定程度上可以提高 VLM 在极端低光照条件下的性能，但在中等退化级别下效果不稳定，有时甚至会降低性能。这表明<strong>单纯的图像增强不足以解决 VLM 在低光照下的根本性问题</strong>，并且 LLIE 模型可能存在对特定退化级别的偏见。</li>
<li><strong>特定任务的脆弱性：</strong> 在“房间类型识别”和“物体属性-颜色”等任务上，VLM 的准确率在严重低光照条件下甚至低于仅依赖文本输入的 GPT-4（盲 LLM）基线。这表明 VLM 在这些情况下难以提取或保留关键的视觉语义信息，特别是颜色信息，这与人类在黑暗中主要依赖亮度感知而颜色感知能力下降的现象类似。</li>
<li><strong>意义：</strong> DarkEQA 的研究揭示了当前 VLM 在低光照室内环境下的脆弱性，强调了在实际应用中进行<strong>鲁棒性评估和方法开发</strong>的重要性。它为研究者提供了一个标准化的平台来衡量和改进 VLM 在真实世界复杂光照条件下的性能。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>数据集的合成性质：</strong> 虽然作者努力使合成过程物理保真，但它仍然是基于渲染的合成数据，可能存在与真实世界低光照图像之间存在的“真实到模拟”（real-to-sim）差距。</li>
<li><strong>LLIE 的局限性：</strong> LLIE 模型的效果不稳定，表明现有 LLIE 方法可能并非针对 VLM 的感知需求进行优化，或者存在对特定退化级别的偏见。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更深入的因果分析：</strong> 对 VLM 在低光照条件下失败原因进行更详细的因果分析。</li>
<li><strong>任务导向的 LLIE：</strong> 开发专门针对 VLM 感知任务的 LLIE 模型，以提高其在低光照条件下的有效性。</li>
<li><strong>弥合真实到模拟差距：</strong> 探索如何减少合成数据与真实世界低光照数据之间的差距，以提高模型在真实场景中的泛化能力。</li>
<li><strong>适应性 VLM 设计：</strong> 设计能够自适应低光照条件的 VLM 架构或训练策略。</li>
<li><strong>扩展到其他退化：</strong> 将此基准测试的理念扩展到其他视觉退化（如模糊、遮挡、天气影响等）的组合场景。</li>
</ul>
<p>总而言之，DarkEQA 论文通过引入一个具有物理保真度的低光照合成方法和一套严格的评估流程，有效地填补了当前 VLM 评估在低光照室内环境下的空白。其研究结果揭示了现有 VLM 的局限性，并为未来开发更鲁棒、更实用的具身智能体提供了重要的研究方向和基准。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions.</li>
<li>We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.24985v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.24985v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.24952v1'></a></p>
<h2 id="viper-process-aware-evaluation-for-generative-video-reasoning"><a href="https://arxiv.org/abs/2512.24952v1">VIPER: Process-aware Evaluation for Generative Video Reasoning</a></h2>
<p><strong>Authors:</strong> Yifan Li, Yukai Gu, Yingqian Min, Zikang Liu, Yifan Du, Kun Zhou, Min Yang, Wayne Xin Zhao, Minghui Qiu</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VIPER: Process-aware Evaluation for Generative Video Reasoning”的全面中文摘要，重点关注其研究问题、创新贡献、主要结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> VIPER: Process-aware Evaluation for Generative Video Reasoning
<strong>作者：</strong> Yifan Li, Yukai Gu, Yingqian Min, Zikang Liu, Yifan Du, Kun Zhou, Min Yang, Wayne Xin Zhao, Minghui Qiu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文的核心问题在于，当前视频生成模型在执行生成式视频推理（Generative Video Reasoning, GVR）任务时，现有的评估框架存在严重缺陷。这些框架通常依赖于对视频<strong>单个帧</strong>（通常是最后一帧或最佳匹配帧）的评估，这导致了“<strong>结果导向型黑客攻击</strong>”（Outcome-hacking）现象。在这种现象下，模型可能通过一个<strong>错误或不合逻辑的过程</strong>生成了正确的最终结果，但现有评估方法却将其误判为成功。这严重低估了模型在真实视觉推理能力上的不足，并阻碍了对模型真实推理过程的理解和改进。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
*   <strong>VIPER基准测试：</strong> 论文提出了一个名为VIPER（VIdeo Process Evaluation for Reasoning tasks）的<strong>全面的、过程感知的视频推理评估基准</strong>。VIPER包含16个任务，涵盖了时间（Temporal）、结构（Structural）、符号（Symbolic）、空间（Spatial）、物理（Physics）和规划（Planning）六个推理领域，共计309个样本。其设计旨在捕捉视频固有的时间性和过程性属性。
*   <strong>过程-结果一致性（POC@r）指标：</strong> 论文引入了一个新的评估指标——<strong>过程-结果一致性（Process-outcome Consistency, POC@r）</strong>。该指标通过VLM（Vision-Language Model）作为裁判，并结合分层评分标准（Hierarchical Rubric），<strong>同时评估生成视频的中间步骤（过程一致性，PC）和最终结果（结果一致性，OC）</strong>。只有当视频的<strong>所有</strong>采样帧都符合过程约束，并且<strong>至少有一帧</strong>满足任务目标时，视频才被认为是正确的。这有效地解决了Outcome-hacking问题。
*   <strong>分层评分标准：</strong> 为了提高VLM裁判的评估准确性和可扩展性，论文设计了一个三层级的评分标准，包括系统提示（System Prompt）、领域介绍（Domain Introduction）和任务约束（Task Constraints），以提供清晰的评估指南。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>模型表现不佳：</strong> 实验结果表明，即使是当前最先进的视频生成模型（如Sora 2和Veo 3.1），在VIPER基准上的POC@1.0得分也普遍低于30%，远低于预期。这表明现有模型在<strong>通用视觉推理能力方面存在巨大差距</strong>。
*   <strong>普遍存在的Outcome-hacking：</strong> 论文的实证研究揭示了Outcome-hacking现象的普遍性，其中Veo 3.1的黑客率高达35.8%，Sora 2也达到46%。这进一步证实了现有评估方法的不足。
*   <strong>测试时扩展和采样率的影响：</strong> 研究还探讨了测试时扩展（Test-time Scaling）和采样率（Sampling Rate）对评估结果的影响。发现增加采样次数（Pass@k）可以提高性能，但不足以根本性地提升推理能力；而增加采样率（r）则会提高评估的严谨性，但也会增加计算开销。
*   <strong>意义：</strong> 这些结果强调了开发更<strong>过程导向</strong>的评估方法和模型的重要性。VIPER基准和POC@r指标为未来GVR模型的研究和评估提供了更可靠的工具，有助于推动模型从简单的内容生成向真正的视觉推理迈进。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>模型能力限制：</strong> 论文明确指出，当前视频模型在处理符号推理（如文本生成和可读性）和物理推理方面存在显著困难。
*   <strong>评估的计算成本：</strong> 虽然POC@r指标更准确，但增加采样率（r）会显著增加评估的计算成本。
*   <strong>模型对约束的理解：</strong> 实验表明，即使显式提供过程约束（如在Table 5中所示），模型在某些领域（如物理推理）的性能提升也有限，表明模型对隐式约束的理解仍需加强。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提升模型的过程推理能力：</strong> 论文强调，未来的研究应侧重于提高模型在生成视频时<strong>忠实地表示推理过程</strong>的能力，而不仅仅是达到正确的结果。
*   <strong>改进对隐式约束的处理：</strong> 需要开发能够更好地理解和遵循复杂、隐式约束的模型。
*   <strong>更鲁棒的评估方法：</strong> 尽管POC@r有所改进，但仍可能存在进一步优化的空间，例如在处理更复杂的视频内容时。
*   <strong>探索新的GVR任务和领域：</strong> VIPER基准的成功可以启发更多新颖的GVR任务设计，以覆盖更广泛的推理能力。
*   <strong>模型架构和训练策略的改进：</strong> 需要探索新的模型架构和训练方法，以直接提升模型的推理能力，而非仅仅依赖于评估方法的改进。</p>
<p>总而言之，这篇论文通过引入VIPER基准和POC@r指标，有力地指出了当前视频生成模型在真实视觉推理能力上的不足，并为未来研究指明了方向，即从关注“结果”转向更重视“过程”，以推动视频模型向更智能、更通用的视觉推理系统发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose a process-aware evaluation paradigm.</li>
<li>We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning.</li>
<li>Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result.</li>
<li>Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.24952v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.24952v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.24922v1'></a></p>
<h2 id="semi-supervised-diversity-aware-domain-adaptation-for-3d-object-detection"><a href="https://arxiv.org/abs/2512.24922v1">Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</a></h2>
<p><strong>Authors:</strong> Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection”的全面中文摘要，重点关注其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文题目：</strong> Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection
<strong>作者：</strong> Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak</p>
<p><strong>摘要：</strong></p>
<p>这篇论文聚焦于解决自动驾驶领域中3D目标检测器在跨领域泛化能力方面的挑战，并提出了一种新颖的、基于神经元激活模式的半监督多样性感知域适应方法。</p>
<p><strong>1. 研究问题/核心问题：</strong></p>
<p>自动驾驶车辆中的3D目标检测器在标准基准测试上表现出色，但在面对不同地理区域（如美国、欧洲、亚洲）或不同驾驶环境（即不同的操作设计域 ODD）时，其性能会显著下降。这是因为不同领域的数据在车辆类型、交通基础设施、天气条件等方面存在差异，导致模型泛化能力不足。传统的解决方案通常需要大量标注数据来重新训练模型，成本高昂且耗时。因此，研究如何在仅有少量目标域样本的情况下，有效地提升3D目标检测器在不同域上的性能，是本文要解决的核心问题。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>新颖的域适应方法：</strong> 论文提出了一种基于<strong>神经元激活模式（neuron activation patterns）</strong>的LiDAR域适应方法。该方法的核心在于利用目标域中<strong>少量、有代表性且多样化</strong>的样本进行模型调整，以实现跨域泛化。</li>
<li><strong>多样性感知样本选择：</strong> 通过分析模型在目标域的<strong>神经元激活模式</strong>，论文开发了一种<strong>多样性帧选择算法</strong>，能够智能地选择最能代表目标域特征且与现有已选样本差异最大的帧。这确保了即使样本数量极少，也能覆盖目标域的关键变化。</li>
<li><strong>半监督学习与持续学习结合：</strong> 该方法采用<strong>半监督</strong>的方式，仅需少量目标域的标注样本。同时，结合<strong>持续学习（continual learning）</strong>的技术，如L2-SP正则化，来<strong>防止模型在适应新域时发生灾难性遗忘（catastrophic forgetting）</strong>，即在提升新域性能的同时，尽量保持在原域的性能。</li>
<li><strong>高效的后训练策略：</strong> 论文探索并评估了多种<strong>后训练（post-training）</strong>策略，包括L2-SP正则化、学习率衰减和恒定学习率等，以找到最适合3D目标检测域适应的设置。</li>
</ul>
<p><strong>3. 主要研究结果及其意义：</strong></p>
<ul>
<li><strong>显著的性能提升：</strong> 实验结果表明，所提出的多样性感知域适应方法，即使仅使用<strong>10到100帧</strong>的目标域样本进行后训练，也能显著提升模型在目标域的检测性能，<strong>超越了线性探测（linear probing）和现有的最先进（state-of-the-art）域适应技术</strong>。</li>
<li><strong>低标注成本：</strong> 该方法极大地降低了域适应的标注成本，证明了在<strong>极小的标注预算下</strong>实现高性能域适应的可行性。</li>
<li><strong>模型泛化能力增强：</strong> 通过有效适应目标域的特征分布，模型能够更好地检测新环境下的物体，为自动驾驶系统在不同区域的部署提供了更经济高效的解决方案。</li>
<li><strong>对不同域的适应性：</strong> 实验证明，该方法在KITTI、NuScenes和Waymo等多个数据集之间进行域适应时均表现出色。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>对样本选择的敏感性：</strong> 尽管方法旨在选择多样性样本，但研究也指出，在样本数量极少（如10帧）时，样本选择策略对最终性能仍有一定敏感性。</li>
<li><strong>跨多个域的适应性：</strong> 论文提到，同时适应多个目标域的效果不如单独适应单一目标域，并且在跨域适应后，模型在原始源域的性能可能会有所下降。</li>
<li><strong>类依赖性：</strong> 研究发现，后训练策略对不同类别的目标（如行人与车辆）可能产生不同的影响，行人类别在某些情况下甚至能获得更高的性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更鲁棒的样本选择：</strong> 进一步研究更鲁棒的样本选择策略，以减少对少量样本选择的敏感性，并提高在更复杂场景下的适应能力。</li>
<li><strong>多域适应的优化：</strong> 探索更有效的多域适应方法，以实现模型在多个目标域上的稳定且高性能的泛化，并解决源域性能下降的问题。</li>
<li><strong>持续学习技术的深入应用：</strong> 结合更先进的持续学习技术，以更好地平衡新旧知识，防止灾难性遗忘，并可能实现模型在多个域上的“通用”适应。</li>
<li><strong>类别的独立适应：</strong> 研究针对不同类别（如行人、车辆、自行车等）进行独立或协同适应的方法，以最大化整体性能。</li>
<li><strong>更广泛的ODD场景：</strong> 将该方法扩展到更广泛的操作设计域（ODD）场景，包括极端天气、复杂交通状况等，以验证其在更具挑战性环境下的有效性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文在3D目标检测的域适应领域做出了重要贡献，通过提出一种创新的、基于神经元激活模式的多样性感知样本选择方法，显著降低了域适应的标注成本，并实现了优于现有方法的性能。该研究为自动驾驶系统在不同环境下的部署提供了切实可行的解决方案，并为未来的域适应研究开辟了新的方向。其核心价值在于证明了<strong>少量、精心挑选的样本</strong>能够实现<strong>高效且有效的跨域泛化</strong>，这对于资源有限的自动驾驶公司具有重要的实际意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected.</li>
<li>Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.24922v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.24922v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.24903v1'></a></p>
<h2 id="finmmdocr-benchmarking-financial-multimodal-reasoning-with-scenario-awareness-document-understanding-and-multi-step-computation"><a href="https://arxiv.org/abs/2512.24903v1">FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</a></h2>
<p><strong>Authors:</strong> Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao</p>
<p><strong>Published:</strong> 2025-12-31</p>
<p><strong>Categories:</strong> cs.CV, cs.CE</p>
<p><strong>Abstract:</strong></p>
<p>We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了 FinMMDocR，一个新颖的双语多模态基准，用于评估多模态大型语言模型（MLLMs）在真实世界金融数值推理方面的能力。与现有基准相比，FinMMDocR 在场景感知、文档理解和多步计算方面均有显著提升，旨在推动 MLLMs 在复杂多模态推理任务上的进步。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>FinMMDocR 的关键创新在于其三个核心方面：</p>
<ul>
<li><strong>场景感知 (Scenario Awareness):</strong> 引入了 12 种隐式的金融场景，要求模型进行专家级别的推理，这超越了简单的文本或数值匹配，需要模型理解更深层次的金融语境和隐含假设。</li>
<li><strong>文档理解 (Document Understanding):</strong> 提供了大规模、多样化且深度丰富的中文/英文金融文档集，其平均页数和视觉元素数量远超现有基准，对模型的文档解析、信息提取和跨文档整合能力提出了严峻挑战。</li>
<li><strong>多步计算 (Multi-Step Computation):</strong> 设计了需要平均 11 步推理（包括提取和计算）的问题，并且大部分问题需要跨越多页证据，这极大地考验了模型的逻辑推理链条构建、信息定位和计算能力。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>FinMMDocR 的发布有望对以下方面产生深远影响：</p>
<ul>
<li><strong>推动 MLLMs 的发展:</strong> 该基准的严苛性将迫使研究人员开发更强大的 MLLMs，使其能够更好地理解和处理包含复杂场景、丰富视觉信息和多步逻辑的真实世界数据。</li>
<li><strong>促进推理增强方法的研究:</strong> 论文中提到的 RAG 方法在 FinMMDocR 上表现出显著差异，预示着对更先进的检索增强生成技术和推理策略的需求，以应对复杂金融推理任务。</li>
<li><strong>提升金融领域 AI 应用的可靠性:</strong> 通过更贴近实际应用的基准，FinMMDocR 有助于提高金融领域 AI 应用的准确性和鲁棒性，尤其是在需要数值推理和文档分析的场景下。</li>
<li><strong>为多模态理解和推理设定新标准:</strong> FinMMDocR 的设计理念和数据集规模，为未来多模态理解和推理基准的构建提供了新的思路和更高的标杆。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>金融科技 (FinTech):</strong> 智能投研、风险评估、合规审查、自动化报告生成等。</li>
<li><strong>商业智能 (Business Intelligence):</strong> 市场分析、竞争对手研究、财务报表分析等。</li>
<li><strong>法律科技 (LegalTech):</strong> 合同审查、法律文件分析等，尤其是在涉及大量数值和复杂条款的场景。</li>
<li><strong>学术研究:</strong> 推动多模态学习、知识图谱构建、复杂推理模型等领域的研究。</li>
<li><strong>教育领域:</strong> 用于训练和评估学生在金融分析和数据解读方面的能力。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要强调了 FinMMDocR 的优势，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>数据集的构建成本和维护:</strong> 专家标注、文档收集和整理需要大量的人力和财力，未来数据集的更新和维护可能面临挑战。</li>
<li><strong>模型性能的瓶颈:</strong> 即使是最好的 MLLM 也只达到了 58.0% 的准确率，这表明当前 MLLMs 在处理此类复杂任务时仍存在显著的性能差距，需要进一步的研究和突破。</li>
<li><strong>RAG 方法的敏感性:</strong> 不同 RAG 方法表现出的性能差异，可能意味着当前 RAG 技术在处理 FinMMDocR 这种复杂多模态数据时，其鲁棒性和泛化能力仍有待提高。</li>
<li><strong>特定领域的局限性:</strong> FinMMDocR 主要聚焦于金融领域，虽然其方法论具有普适性，但其数据集和场景的特定性可能限制了其直接应用于其他领域的程度，需要进行适配和扩展。</li>
<li><strong>对“场景感知”的评估方式:</strong> 摘要中提到“挑战模型进行专家级推理基于假设”，但具体如何量化和评估模型对“隐式场景”的理解和推理能力，在摘要中未详细说明，这可能是未来研究需要关注的方面。</li>
</ul>
<p><strong>对计算机视觉领域的潜在趣味性或重要性：</strong></p>
<p>对于计算机视觉领域而言，FinMMDocR 的重要性体现在：</p>
<ul>
<li><strong>视觉信息在复杂推理中的作用:</strong> 金融文档通常包含图表、表格、公司 Logo 等丰富的视觉元素。FinMMDocR 的大规模文档集要求模型不仅能理解文本，还能有效地从这些视觉信息中提取关键数据和模式，并将其与文本信息结合进行推理。这直接推动了<strong>视觉问答 (VQA)</strong>、<strong>表格理解 (Table Understanding)</strong>、<strong>图表识别与分析 (Chart Recognition and Analysis)</strong> 等计算机视觉子领域的发展，并要求模型具备更强的<strong>多模态融合 (Multimodal Fusion)</strong> 能力。</li>
<li><strong>真实世界场景下的视觉理解:</strong> 论文强调“真实世界金融数值推理”，这意味着模型需要处理现实世界中常见的、可能存在噪声、不完整或格式多样的视觉信息。这对于提升计算机视觉模型在实际应用中的鲁棒性和泛化能力至关重要。</li>
<li><strong>跨模态信息整合与推理:</strong> FinMMDocR 的核心在于将文本、数值和视觉信息整合起来进行多步推理。这要求计算机视觉模型不仅仅是识别和提取信息，更要能够理解这些信息之间的逻辑关系，并将其作为推理过程中的重要组成部分。这对于发展更具认知能力的视觉模型具有重要意义。</li>
<li><strong>评估新一代 MLLMs 的视觉能力:</strong> 随着 MLLMs 的兴起，评估它们在处理复杂视觉信息和进行视觉推理方面的能力变得越来越重要。FinMMDocR 提供了一个极具挑战性的平台，可以用来衡量当前 MLLMs 在视觉理解和多模态推理方面的真实水平，并指导未来模型的设计方向。</li>
</ul>
<p>总而言之，FinMMDocR 作为一个严谨且具有挑战性的金融多模态基准，不仅推动了 MLLMs 在金融领域的应用，也为计算机视觉领域在处理真实世界复杂视觉信息、实现跨模态信息整合与推理方面提供了新的机遇和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.24903v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.24903v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-02 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
