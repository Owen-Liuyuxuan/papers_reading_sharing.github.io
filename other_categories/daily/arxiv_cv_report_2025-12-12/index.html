<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-12 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-11/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-15/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-12">Arxiv Computer Vision Papers - 2025-12-12</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#vl-jepa-joint-embedding-predictive-architecture-for-vision-language" class="nav-link">VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</a>
                </li>
                <li class="nav-item">
                    <a href="#evaluating-gemini-robotics-policies-in-a-veo-world-simulator" class="nav-link">Evaluating Gemini Robotics Policies in a Veo World Simulator</a>
                </li>
                <li class="nav-item">
                    <a href="#stereospace-depth-free-synthesis-of-stereo-geometry-via-end-to-end-diffusion-in-a-canonical-space" class="nav-link">StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</a>
                </li>
                <li class="nav-item">
                    <a href="#worldlens-full-spectrum-evaluations-of-driving-world-models-in-real-world" class="nav-link">WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</a>
                </li>
                <li class="nav-item">
                    <a href="#scenemaker-open-set-3d-scene-generation-with-decoupled-de-occlusion-and-pose-estimation-model" class="nav-link">SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</a>
                </li>
                <li class="nav-item">
                    <a href="#empowering-dynamic-urban-navigation-with-stereo-and-mid-level-vision" class="nav-link">Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#omni-attribute-open-vocabulary-attribute-encoder-for-visual-concept-personalization" class="nav-link">Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization</a>
                </li>
                <li class="nav-item">
                    <a href="#group-diffusion-enhancing-image-generation-by-unlocking-cross-sample-collaboration" class="nav-link">Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration</a>
                </li>
                <li class="nav-item">
                    <a href="#e-rayzer-self-supervised-3d-reconstruction-as-spatial-visual-pre-training" class="nav-link">E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</a>
                </li>
                <li class="nav-item">
                    <a href="#are-we-ready-for-rl-in-text-to-3d-generation-a-progressive-investigation" class="nav-link">Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-12">Arxiv Computer Vision Papers - 2025-12-12</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我为您整理了这份 Arxiv 计算机视觉领域论文的每日报告执行摘要。</p>
<hr />
<p><strong>每日报告：Arxiv 计算机视觉论文摘要 (2025-12-11)</strong></p>
<p><strong>执行摘要</strong></p>
<p>本报告总结了 2025 年 12 月 11 日在 Arxiv 上发布的 10 篇计算机视觉领域论文。本期论文展现了几个关键的研究趋势，包括<strong>多模态理解的深化、三维场景生成与重建的进步、以及在真实世界应用中的评估方法</strong>。</p>
<p><strong>主要趋势与观察：</strong></p>
<ul>
<li><strong>视觉-语言联合学习的持续探索：</strong> "VL-JEPA" 论文展示了在视觉和语言信息联合嵌入方面的最新进展，预示着更强大的跨模态理解能力。</li>
<li><strong>三维场景生成与重建的突破：</strong> 多篇论文聚焦于三维场景的生成和重建，从无监督的立体几何合成（"StereoSpace"）到开放集三维场景生成（"SceneMaker"），再到自监督的 3D 重建作为空间预训练（"E-RayZer"），显示出该领域正在快速发展。</li>
<li><strong>真实世界评估的重要性日益凸显：</strong> "WorldLens" 和 "Gemini Robotics Team" 的论文强调了在真实世界或高度仿真的环境中评估模型性能的必要性，尤其是在自动驾驶和机器人领域。</li>
<li><strong>生成模型的新范式：</strong> "Group Diffusion" 提出了通过跨样本协作来增强图像生成的新方法，预示着生成模型在效率和质量上的进一步提升。</li>
<li><strong>个性化与开放词汇能力：</strong> "Omni-Attribute" 论文关注于开放词汇属性编码，为视觉概念的个性化提供了新的思路。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>"StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space"</strong> 提出了一种无需深度信息即可合成立体几何的方法，利用扩散模型在规范空间中进行端到端处理，具有显著的创新性。</li>
<li><strong>"WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World"</strong> 提供了对驾驶世界模型进行全面真实世界评估的框架，对于推动自动驾驶技术落地至关重要。</li>
<li><strong>"SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model"</strong> 在开放集三维场景生成方面取得了进展，通过解耦遮挡处理和姿态估计，为更灵活的场景生成提供了可能。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>扩散模型在三维领域的应用：</strong> 扩散模型不仅在图像生成中表现出色，也开始被应用于三维几何合成和重建。</li>
<li><strong>自监督学习在三维重建中的潜力：</strong> "E-RayZer" 展示了自监督学习在 3D 重建作为空间视觉预训练方面的巨大潜力。</li>
<li><strong>强化学习与文本到三维生成：</strong> "Are We Ready for RL in Text-to-3D Generation?" 探讨了强化学习在文本到三维生成中的应用前景，这是一个值得关注的新兴交叉领域。</li>
<li><strong>跨样本协作的生成模型：</strong> "Group Diffusion" 提出的跨样本协作机制，可能为提高生成模型的效率和多样性提供新的方向。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其创新性和对未来研究方向的潜在影响，以下论文值得深入阅读：</p>
<ol>
<li><strong>"StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space"</strong> (创新性的三维几何合成方法)</li>
<li><strong>"WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World"</strong> (对自动驾驶领域至关重要的真实世界评估框架)</li>
<li><strong>"VL-JEPA: Joint Embedding Predictive Architecture for Vision-language"</strong> (在多模态理解领域具有重要意义)</li>
<li><strong>"Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration"</strong> (可能引领生成模型的新范式)</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速了解近期 Arxiv 计算机视觉领域的最新动态。希望对您有所帮助！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.10942v1">VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</a></li>
<li><a href="#2512.10675v1">Evaluating Gemini Robotics Policies in a Veo World Simulator</a></li>
<li><a href="#2512.10959v1">StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</a></li>
<li><a href="#2512.10958v1">WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</a></li>
<li><a href="#2512.10957v1">SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</a></li>
<li><a href="#2512.10956v1">Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</a></li>
<li><a href="#2512.10955v1">Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization</a></li>
<li><a href="#2512.10954v1">Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration</a></li>
<li><a href="#2512.10950v1">E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</a></li>
<li><a href="#2512.10949v1">Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.10942v1'></a></p>
<h2 id="vl-jepa-joint-embedding-predictive-architecture-for-vision-language"><a href="https://arxiv.org/abs/2512.10942v1">VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</a></h2>
<p><strong>Authors:</strong> Delong Chen, Mustafa Shukor, Theo Moutakanni, Willy Chung, Jade Yu, Tejaswi Kasarla, Allen Bolourchi, Yann LeCun, Pascale Fung</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“VL-JEPA: Joint Embedding Predictive Architecture for Vision-language”论文的全面中文摘要，其中包含了您要求的各项内容：</p>
<p><strong>论文题目：</strong> VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</p>
<p><strong>作者：</strong> Delong Chen, Mustafa Shukor, Théo Moutakanni, Willy Chung, Jade Yu, Tejaswi Kasarla, Allen Bolourchi, Yann LeCun, Pascale Fung</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要问题与研究动机：</strong></p>
<p>当前主流的视觉-语言模型（VLMs）通常采用自回归的方式在离散的token空间生成文本。这种方法存在两个主要问题：
*   <strong>训练效率低下：</strong> 模型需要同时学习任务相关的语义和表面语言的变异性（如词汇选择、风格等），导致训练计算量大，并且在生成过程中会花费大量计算资源来产生可能并不影响最终结果的token序列。
*   <strong>实时性与低延迟受限：</strong> 对于需要实时响应的应用（如视频流分析），自回归的逐token解码会引入不必要的延迟，并且难以实现动态的语义更新。</p>
<p>因此，研究如何构建一个更高效、更具实时性的视觉-语言模型，以解决上述问题，是本文的核心研究动机。</p>
<p><strong>2. 关键创新与方法贡献：</strong></p>
<p>本文提出了 <strong>VL-JEPA (Vision-Language Joint Embedding Predictive Architecture)</strong>，一种全新的非生成式视觉-语言模型，其核心创新在于：</p>
<ul>
<li><strong>联合嵌入预测架构 (JEPA)：</strong> VL-JEPA不直接生成文本token，而是预测目标文本的<strong>连续嵌入 (continuous embeddings)</strong>。模型通过一个X-Encoder将视觉输入映射到视觉嵌入 <script type="math/tex">S_v</script>，通过Y-Encoder将目标文本映射到目标嵌入 <script type="math/tex">S_y</script>，然后利用一个Predictor学习从视觉嵌入和文本查询 <script type="math/tex">X_Q</script> 预测目标嵌入 <script type="math/tex">\hat{S}_y</script> 的映射。训练目标是在嵌入空间进行预测，而非数据空间。</li>
<li><strong>抽象表征空间学习：</strong> 通过在抽象的嵌入空间进行学习，模型能够专注于任务相关的语义，并忽略表面语言的变异性，从而提高学习效率。</li>
<li><strong>轻量级文本解码器：</strong> 在推理时，仅在需要时调用一个轻量级的文本解码器，将预测的嵌入 <script type="math/tex">\hat{S}_y</script> 解码为文本。</li>
<li><strong>选择性解码 (Selective Decoding)：</strong> VL-JEPA原生支持选择性解码。模型输出的连续嵌入流可以被实时监控，仅当检测到显著的语义变化时才触发解码，这显著减少了不必要的解码操作（平均减少约2.85倍），同时保持了性能。</li>
<li><strong>统一的架构与多任务能力：</strong> VL-JEPA的嵌入空间天然支持多种下游任务，包括开放词汇分类、文本到视频检索以及判别式视觉问答（VQA），无需修改模型架构。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升与效率增益：</strong> 在与标准token空间VLM进行严格控制的比较中（相同的视觉编码器、训练数据等），VL-JEPA在零样本（zero-shot）的图像描述生成和分类任务上表现出更强的性能，同时<strong>可训练参数减少了50%</strong>。</li>
<li><strong>视频理解与检索优势：</strong> 在八个视频分类和八个视频检索数据集上，VL-JEPA的平均性能优于CLIP、SigLIP2和Perception Encoder等模型。</li>
<li><strong>VQA能力：</strong> 在四个VQA数据集上，VL-JEPA取得了与InstructBLIP、QwenVL等经典VLM相当的性能，但参数量仅为1.6B。</li>
<li><strong>选择性解码的有效性：</strong> 实验证明，选择性解码策略能够显著降低推理成本（减少约2.85倍的解码操作），同时保持输出质量。</li>
<li><strong>高效的预训练与微调：</strong> 模型通过两阶段训练：首先是大规模的无查询预训练以建立视觉-语言对齐，然后是查询条件下的监督微调以增强VQA能力。</li>
<li><strong>世界建模能力：</strong> 在WORLDPREDICTION-WM基准测试中，VL-JEPA取得了新的SOTA性能，展示了其在理解世界状态变化和动作概念方面的潜力。</li>
</ul>
<p><strong>意义：</strong> VL-JEPA的提出标志着在视觉-语言模型领域的一个重要进展，它通过引入联合嵌入预测架构，实现了在<strong>效率、性能和实时性</strong>上的显著提升，为构建更强大、更通用的AI系统奠定了基础。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>特定任务的局限性：</strong> 作者提到，目前VL-JEPA的目标是成为一个通用的模型，但对于更复杂的推理、工具使用和智能体行为等任务，其表现可能不如专门的token生成模型。</li>
<li><strong>数据规模与参数扩展：</strong> 虽然结果显示了扩展参数和数据集规模的益处，但作者并未完全探索这一方向，留待未来研究。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的任务评估：</strong> 在更复杂的推理、工具使用和智能体行为等任务上进一步评估VL-JEPA。</li>
<li><strong>参数与数据集规模的探索：</strong> 深入研究模型在更大规模参数和数据集下的表现。</li>
<li><strong>多模态联合嵌入空间推理：</strong> 将VL-JEPA作为基础，探索在多模态联合嵌入空间进行更复杂的推理，例如视觉链式思考（visual chain-of-thought）方法。</li>
<li><strong>更先进的正则化策略：</strong> 探索如VICReg和SIGReg等更高级的非样本对比正则化方法。</li>
<li><strong>Y-Encoder的进一步优化：</strong> 探索更多样化的Y-Encoder架构和初始化策略。</li>
</ul>
<hr />
<p>这份摘要力求在保持技术准确性的同时，清晰地传达VL-JEPA的核心贡献和研究价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA).</li>
<li>We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10942v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10942v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10675v1'></a></p>
<h2 id="evaluating-gemini-robotics-policies-in-a-veo-world-simulator"><a href="https://arxiv.org/abs/2512.10675v1">Evaluating Gemini Robotics Policies in a Veo World Simulator</a></h2>
<p><strong>Authors:</strong>  Gemini Robotics Team, Coline Devin, Yilun Du, Debidatta Dwibedi, Ruiqi Gao, Abhishek Jindal, Thomas Kipf, Sean Kirmani, Fangchen Liu, Anirudha Majumdar, Andrew Marmon, Carolina Parada, Yulia Rubanova, Dhruv Shah, Vikas Sindhwani, Jie Tan, Fei Xia, Ted Xiao, Sherry Yang, Wenhao Yu, Allan Zhou</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于使用生成式视频模型评估机器人策略的论文的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Evaluating Gemini Robotics Policies in a Veo World Simulator
<strong>作者：</strong> Gemini Robotics Team, Coline Devin, Yilun Du, Debidatta Dwibedi, Ruiqi Gao, Abhishek Jindal, Thomas Kipf, Sean Kirmani, Fangchen Liu, Anirudha Majumdar, Andrew Marmon, Carolina Parada, Yulia Rubanova, Dhruv Shah, Vikas Sindhwani, Jie Tan, Fei Xia, Ted Xiao, Sherry Yang, Wenhao Yu, Allan Zhou</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决当前机器人策略评估中的一个关键挑战：如何高效、可扩展地评估通用机器人策略（generalist policies）在各种场景下的性能，特别是其在“分布外”（Out-of-Distribution, OOD）泛化能力和物理/语义安全性方面的表现。传统的硬件评估方法在覆盖广泛场景、进行大规模测试以及评估安全性方面存在固有的局限性，例如成本高昂、耗时且存在安全风险。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>提出基于生成式视频模型的评估系统：</strong> 作者引入了一个创新的评估系统，该系统基于一个前沿的视频基础模型（Veo），能够生成逼真且多视角的机器人场景模拟。</li>
<li><strong>实现全面的策略评估能力：</strong> 该系统不仅支持“分布内”（in-distribution）的标称性能评估，还能进行 OOD 泛化评估，以及针对物理和语义安全性的“红队测试”（red teaming）。</li>
<li><strong>利用生成式图像编辑和多视图合成：</strong> 系统集成了生成式图像编辑技术，能够合成包含新交互对象、新视觉背景和新干扰对象等多样化场景的逼真变体，从而探索策略在不同泛化维度上的表现。</li>
<li><strong>动作条件化和多视图一致性：</strong> 该系统能够根据机器人动作指令和多视图输入生成一致的视频模拟，这对于评估现代多视图策略至关重要。</li>
<li><strong>验证了视频模型在机器人评估中的潜力：</strong> 作者证明了视频模型可以覆盖机器人策略评估的整个谱系，从标称性能到 OOD 泛化和安全性探测。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>准确预测策略性能和排名：</strong> 研究结果表明，该视频模拟系统能够准确预测机器人策略在标称场景下的相对性能和排名，并且与真实世界评估结果高度相关（Pearson 系数高达 0.88）。</li>
<li><strong>量化 OOD 泛化影响：</strong> 系统能够准确预测不同泛化轴（如背景、干扰对象、新对象）对策略性能的影响，并验证了这些预测与真实世界评估结果的一致性。</li>
<li><strong>实现有效的安全性红队测试：</strong> 通过生成包含安全隐患的编辑场景，该系统能够发现策略潜在的不安全行为，而无需进行昂贵且危险的真实世界测试。例如，在“抓取红色积木”的指令下，策略可能错误地接触到人手；在“关闭笔记本”的指令下，策略可能在未移除剪刀的情况下关闭笔记本，从而损坏屏幕。</li>
<li><strong>大规模验证：</strong> 研究通过 1600 多次真实世界评估，对八个 Gemini Robotics 策略检查点和五个双臂操作任务进行了验证，证明了该方法的有效性。</li>
<li><strong>意义：</strong> 这项工作为机器人策略的<strong>可扩展、可靠和安全的评估</strong>提供了一条新途径。它极大地降低了评估成本和风险，使得研究人员能够更深入地理解和改进通用机器人策略的泛化能力和安全性。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>接触交互的模拟挑战：</strong> 模拟接触丰富的交互，特别是涉及小物体时，仍然是一个挑战。论文中提到了生成过程中可能出现的“幻觉”现象（例如，物体自发出现）。</li>
<li><strong>长时序生成的技术瓶颈：</strong> 目前的策略回滚（policy rollouts）仅限于 8 秒的短时序。实现长时序（例如 1 分钟以上）的多视图一致性生成仍然是一个关键的技术里程碑。</li>
<li><strong>依赖人工评分：</strong> 当前的评估结果依赖于人工对生成视频的评分。</li>
<li><strong>推理效率：</strong> 视频生成过程的推理效率仍有待提高，以进一步增强评估范式的可扩展性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>增加多样化的交互数据：</strong> 通过扩展训练数据，特别是包含更多样化的交互数据，以解决接触交互模拟的挑战。</li>
<li><strong>实现长时序视频生成：</strong> 探索基于潜在动作模型（latent-action models）等技术，以实现更长时序的视频生成，从而评估更复杂的长期任务。</li>
<li><strong>开发全自动评估流水线：</strong> 集成基于视觉语言模型（VLMs）的自动评分机制，以实现完全自动化的评估流程。</li>
<li><strong>优化推理效率：</strong> 通过优化模型架构，提高视频生成的速度，从而加速评估过程。</li>
<li><strong>更广泛的泛化和安全性评估：</strong> 将该方法应用于更广泛的机器人任务和更复杂的安全场景，以进一步探索其潜力。</li>
</ul>
<p><strong>对计算机视觉领域的贡献：</strong></p>
<p>这篇论文在计算机视觉领域的重要贡献在于，它<strong>开创性地展示了前沿生成式视频模型（如 Veo）在机器人策略评估中的巨大潜力</strong>。它不仅将视频模型从传统的“分布内”评估扩展到了 OOD 泛化和安全性探测的整个谱系，还通过<strong>结合生成式图像编辑和多视图合成技术，构建了一个高度灵活和逼真的模拟环境</strong>。这为机器人研究社区提供了一个强大的新工具，能够以<strong>更低的成本、更高的效率和更安全的风险</strong>来开发和验证通用机器人智能。该研究强调了生成式模型在理解和预测复杂物理世界交互方面的能力，为未来机器人智能的进步奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety.</li>
<li>We introduce a generative evaluation system built upon a frontier video foundation model (Veo).</li>
<li>We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10675v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10675v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10959v1'></a></p>
<h2 id="stereospace-depth-free-synthesis-of-stereo-geometry-via-end-to-end-diffusion-in-a-canonical-space"><a href="https://arxiv.org/abs/2512.10959v1">StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</a></h2>
<p><strong>Authors:</strong> Tjark Behrens, Anton Obukhov, Bingxin Ke, Fabio Tosi, Matteo Poggi, Konrad Schindler</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp &amp; inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本文提出了一种名为 StereoSpace 的新颖框架，它利用端到端的扩散模型，实现了从单目图像到立体几何的合成，而无需显式的深度图或图像扭曲。该方法通过在规范化空间中进行视角条件化来引导生成器，从而端到端地推断对应关系并填充遮挡区域。StereoSpace 在感知舒适度和几何一致性方面表现出色，为立体生成提供了一种可扩展、无深度的解决方案。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>深度自由（Depth-Free）的立体几何合成：</strong> 这是最核心的创新点。传统的单目转立体方法通常依赖于估计深度图，然后通过视差计算或图像扭曲来生成另一视角的图像。StereoSpace 绕过了显式的深度估计，而是直接通过扩散模型在“规范化空间”（canonical rectified space）中学习和生成立体几何。</li>
<li><strong>视角条件化（Viewpoint Conditioning）作为几何推断的驱动：</strong> 模型的核心在于如何利用视角信息来推断几何。通过将目标视角的条件信息融入扩散过程，模型被引导去理解不同视角下的物体结构和空间关系，从而生成具有正确视差的立体图像。</li>
<li><strong>端到端（End-to-End）的生成与填充：</strong> StereoSpace 将对应关系推断和遮挡区域填充（disocclusion filling）集成在一个统一的端到端框架中。这意味着模型能够同时处理这些复杂的立体几何问题，而不是将其分解为多个独立的步骤。</li>
<li><strong>规范化空间（Canonical Rectified Space）：</strong> 论文提到在“规范化 rectified space”中进行操作。这可能意味着模型在内部将输入图像或特征映射到一个标准化的、可能已经进行了一些预处理（如校正）的空间中，以便于模型学习跨视角的几何一致性。</li>
<li><strong>公平且无泄露的评估协议：</strong> 论文强调了其评估协议的严谨性，排除了测试时对真实几何信息（ground truth）或代理几何估计（proxy geometry estimates）的依赖。这保证了评估的公正性，并突显了模型在没有直接几何监督下的能力。</li>
<li><strong>关注下游相关性指标：</strong> 评估指标（iSQoE for perceptual comfort and MEt3R for geometric consistency）的选择表明了论文的实用导向，关注生成立体图像在实际应用中的质量和可用性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动立体视觉研究范式：</strong> StereoSpace 的深度自由方法可能为单目转立体领域开辟新的研究方向，减少对高精度深度估计的依赖，从而简化流程并可能提高鲁棒性。</li>
<li><strong>提升立体内容生成的可扩展性：</strong> 扩散模型本身具有强大的生成能力和可扩展性。深度自由的特性进一步降低了对数据标注的要求，使得大规模立体内容生成成为可能。</li>
<li><strong>改善虚拟现实/增强现实（VR/AR）和3D重建应用：</strong> 能够从单目图像高效生成高质量立体几何，将直接受益于VR/AR内容创作、3D场景重建、自动驾驶中的场景理解等领域。</li>
<li><strong>促进更自然的图像编辑和合成：</strong> 深度自由的立体生成能力也可能应用于更高级的图像编辑任务，例如在不改变内容的情况下改变视角。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实（VR）和增强现实（AR）：</strong> 快速生成逼真的立体内容，用于沉浸式体验和交互式应用。</li>
<li><strong>3D重建和场景理解：</strong> 从单目视频或图像序列中恢复场景的3D结构，用于机器人导航、自动驾驶、建筑可视化等。</li>
<li><strong>电影和游戏制作：</strong> 快速将2D素材转换为3D，降低制作成本和时间。</li>
<li><strong>图像编辑和内容创作：</strong> 允许用户在不改变内容的情况下调整视角，或生成具有深度感的图像。</li>
<li><strong>计算机辅助设计（CAD）：</strong> 从2D草图或图像生成3D模型。</li>
<li><strong>医学影像：</strong> 从单张X光片或CT扫描生成具有深度感的图像，辅助诊断。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 扩散模型通常计算成本较高，尤其是在推理阶段。虽然论文强调了“可扩展性”，但具体的推理速度和计算资源需求仍需进一步验证。</li>
<li><strong>对“规范化空间”的依赖和理解：</strong> 论文提到了“规范化 rectified space”，但其具体实现细节和对模型性能的影响需要深入研究。如果这个规范化过程本身存在限制，可能会影响最终的生成效果。</li>
<li><strong>对复杂场景的泛化能力：</strong> 摘要提到在“分层（layered）和非朗伯体（non-Lambertian）场景”上表现良好，这表明它在某些复杂场景下表现优异。然而，对于更极端或未见过的数据分布，其泛化能力仍需评估。</li>
<li><strong>“端到端”的定义和边界：</strong> 虽然是端到端，但其内部的“推断对应关系”和“填充遮挡”是否完全独立于任何形式的几何先验或隐式表示，仍需在论文正文中确认。</li>
<li><strong>评估指标的局限性：</strong> iSQoE 和 MEt3R 是新提出的或侧重于特定方面的指标。虽然它们强调了下游相关性，但可能无法完全捕捉所有可能的立体失真或感知问题。与传统的、更广泛接受的立体质量评估指标（如PSNR, SSIM, VMAF等）的对比可能不足。</li>
<li><strong>对“无显式深度”的解释：</strong> 尽管强调“深度自由”，但模型内部可能仍然学习了某种形式的隐式深度表示或几何线索。理解这种“无显式深度”的真正含义及其对模型能力的影响至关重要。</li>
</ul>
<p>总而言之，StereoSpace 是一篇非常有前景的论文，它通过创新的深度自由扩散方法，为单目转立体生成提供了一种新的、可能更高效和可扩展的解决方案。其对几何推断的直接建模方式，以及对公平评估的重视，使其在计算机视觉领域具有重要的研究价值和应用潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping.</li>
<li>To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10959v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10959v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10958v1'></a></p>
<h2 id="worldlens-full-spectrum-evaluations-of-driving-world-models-in-real-world"><a href="https://arxiv.org/abs/2512.10958v1">WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</a></h2>
<p><strong>Authors:</strong> Ao Liang, Lingdong Kong, Tianyi Yan, Hongsi Liu, Wesley Yang, Ziqi Huang, Wei Yin, Jialong Zuo, Yixuan Hu, Dekai Zhu, Dongyue Lu, Youquan Liu, Guangfeng Jiang, Linfeng Li, Xiangtai Li, Long Zhuo, Lai Xing Ng, Benoit R. Cottereau, Changxin Gao, Liang Pan, Wei Tsang Ooi, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了 WorldLens，一个全面的基准测试框架，用于评估生成式世界模型在模拟驾驶环境中的表现。它首次从视觉真实性、几何一致性、物理合理性和功能可靠性等多个维度，系统地衡量了模型在构建、理解和行为方面的能力。通过引入大规模人类标注数据集和自动化评估模型，WorldLens 旨在为生成式世界模型提供一个统一、可解释的评估标准，推动其向更逼真、更可靠的方向发展。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li>
<p><strong>全谱评估框架 (Full-Spectrum Evaluation Framework):</strong> 这是 WorldLens 最核心的创新。它不再局限于单一维度的评估（如视觉真实性），而是将评估扩展到五个关键方面：</p>
<ul>
<li><strong>Generation (生成):</strong> 评估生成世界的外观真实性。</li>
<li><strong>Reconstruction (重建):</strong> 评估从生成世界中重建几何信息的能力。</li>
<li><strong>Action-Following (行为遵循):</strong> 评估模型在生成世界中执行动作的物理和行为一致性。</li>
<li><strong>Downstream Task (下游任务):</strong> 评估生成世界对实际驾驶任务（如导航、避障）的支持程度。</li>
<li><strong>Human Preference (人类偏好):</strong> 结合人类的直观判断来评估世界的整体质量。
这种多维度、全方位的评估方法是当前研究中缺失的，能够更全面地揭示世界模型的优缺点。</li>
</ul>
</li>
<li>
<p><strong>WorldLens-26K 数据集:</strong> 为了支持其评估框架，作者构建了一个大规模、包含人类标注的视频数据集。这个数据集不仅包含数值评分，还提供了文本解释，这对于理解评估结果和模型失败的原因至关重要。这种高质量的人类标注数据是训练和验证评估模型的关键。</p>
</li>
<li>
<p><strong>WorldLens-Agent:</strong> 基于 WorldLens-26K 数据集，作者开发了一个可扩展、可解释的评估模型。这个模型能够自动化地对生成的世界进行评分，从而克服了人工评估的效率瓶颈，并能提供评估的理由，增加了评估的可信度和透明度。</p>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>标准化评估:</strong> WorldLens 有望成为生成式世界模型领域的一个事实上的标准。它提供了一个统一的度量体系，使得不同模型之间的比较更加公平和有意义，从而加速该领域的研究进展。</li>
<li><strong>指导模型开发:</strong> 通过揭示现有模型在物理和行为方面的不足，WorldLens 为未来的模型设计提供了明确的方向。研究人员可以根据 WorldLens 的评估结果，更有针对性地改进模型的几何一致性、物理规律遵循能力以及行为保真度。</li>
<li><strong>推动 Embodied AI 的发展:</strong> 生成式世界模型是 Embodied AI 的基石。WorldLens 的出现将直接促进 Embodied AI 代理在真实世界中的可靠性和安全性，为自动驾驶、机器人等应用奠定更坚实的基础。</li>
<li><strong>提升模型的可解释性:</strong> WorldLens-Agent 的引入，通过提供文本解释，使得评估过程更加透明，有助于研究人员理解模型为何会做出特定的判断，从而提升模型的整体可解释性。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>自动驾驶 (Autonomous Driving):</strong> 这是论文直接关注的应用领域。更逼真、更可靠的驾驶世界模型能够极大地提升自动驾驶系统的训练效率和安全性。</li>
<li><strong>机器人学 (Robotics):</strong> 机器人需要在复杂环境中进行感知、规划和控制。生成式世界模型可以用于模拟各种机器人操作场景，帮助机器人学习和适应。</li>
<li><strong>虚拟现实/增强现实 (VR/AR):</strong> 高度逼真且物理一致的虚拟环境对于沉浸式体验至关重要。WorldLens 的评估方法可以用于衡量 VR/AR 内容的质量。</li>
<li><strong>游戏开发 (Game Development):</strong> 游戏引擎需要生成逼真的虚拟世界。WorldLens 的评估标准可以帮助游戏开发者提升游戏世界的真实感和可玩性。</li>
<li><strong>仿真科学 (Simulation Science):</strong> 任何需要精确模拟物理过程和环境交互的领域，如航空航天、工程设计等，都可以从更可靠的世界模型中受益。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>“Real World” 的定义和覆盖范围:</strong> 摘要中提到了“Real World”，但具体指代的是真实世界的哪些方面（例如，是真实世界的物理规律，还是真实世界的视觉外观，或是真实世界的交通行为模式）需要进一步明确。如果“Real World”的定义过于狭窄，那么评估结果的普适性可能会受到限制。</li>
<li><strong>评估的计算成本:</strong> 构建一个“全谱”的评估框架，特别是包含物理和行为评估，很可能需要大量的计算资源。WorldLens-Agent 的出现旨在缓解这个问题，但其自身的训练和推理成本仍可能是一个考量因素。</li>
<li><strong>人类偏好的主观性:</strong> 尽管引入了人类偏好评估，但人类的判断本身可能存在主观性、文化差异和认知偏差。WorldLens-26K 数据集的规模和多样性对于缓解这个问题至关重要，但仍可能存在一定程度的局限性。</li>
<li><strong>“No existing world model excels universally” 的具体表现:</strong> 摘要指出“no existing world model excels universally”，但具体哪些模型在哪些方面表现出色，哪些方面存在严重缺陷，需要通过论文的详细实验结果来揭示。摘要的概括性描述可能无法完全展现这些细节。</li>
<li><strong>WorldLens-Agent 的泛化能力:</strong> WorldLens-Agent 是从 WorldLens-26K 数据集中蒸馏出来的。其在未见过的新型世界模型上的泛化能力和鲁棒性，以及其解释的准确性，是需要进一步验证的。</li>
</ul>
<p>总而言之，WorldLens 是一项非常有价值的研究，它填补了生成式世界模型评估领域的空白，并为该领域的研究和发展提供了重要的方向和工具。其多维度、数据驱动的评估方法，以及自动化评估模型的引入，都预示着该研究将对 Embodied AI 和相关应用产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10958v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10958v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10957v1'></a></p>
<h2 id="scenemaker-open-set-3d-scene-generation-with-decoupled-de-occlusion-and-pose-estimation-model"><a href="https://arxiv.org/abs/2512.10957v1">SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</a></h2>
<p><strong>Authors:</strong> Yukai Shi, Weiyu Li, Zihao Wang, Hongyang Li, Xingyu Chen, Ping Tan, Lei Zhang</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SceneMaker: Decoupled De-occlusion and Pose Estimation Model for Open-set 3D Scene Generation</p>
<p><strong>作者：</strong> Yukai Shi, Weiyu Li, Zihao Wang, Hongyang Li, Xingyu Chen, Ping Tan, Lei Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前3D场景生成方法在开放集（open-set）场景下，尤其是在严重遮挡的情况下，难以同时生成高质量几何体和准确物体姿态的问题。现有方法受限于有限的3D数据集，缺乏足够的开放集去遮挡（de-occlusion）和姿态估计（pose estimation）先验知识，导致在复杂场景下性能下降。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
SceneMaker 提出了一种解耦的3D场景生成框架，其核心创新点在于：</p>
<ul>
<li><strong>解耦的去遮挡模型：</strong> 将去遮挡模型从3D物体生成中分离出来，并利用大规模图像数据集和专门收集的去遮挡数据集进行增强训练。这使得模型能够学习到更丰富多样的开放集遮挡模式，从而生成更完整、高质量的物体几何体。</li>
<li><strong>统一的姿态估计模型：</strong> 提出了一种统一的姿态估计模型，该模型集成了全局和局部自注意力（self-attention）及交叉注意力（cross-attention）机制。这种设计能够更准确地处理不同姿态变量（旋转、平移、尺寸）之间的相互作用，并提升模型在场景生成任务中的准确性。</li>
<li><strong>构建开放集3D场景数据集：</strong> 为了进一步提升姿态估计模型的泛化能力，作者构建了一个包含200K个合成场景的大规模开放集3D场景数据集。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过上述创新，SceneMaker 在室内和开放集场景下均取得了优于现有SOTA（State-of-the-Art）方法的性能。</p>
<ul>
<li><strong>几何体质量提升：</strong> 解耦的去遮挡模型显著提高了在严重遮挡情况下的物体几何体生成质量，即使是小物体也能保持细节。</li>
<li><strong>姿态估计准确性增强：</strong> 统一的姿态估计模型能够更准确地预测物体的6D姿态（旋转、平移、尺寸），并且在处理具有不同几何形状的物体时表现出色。</li>
<li><strong>泛化能力提升：</strong> 通过构建的开放集数据集，模型在处理未见过或复杂场景时展现出更强的泛化能力。</li>
<li><strong>实验验证：</strong> 论文通过大量的定量和定性实验，包括与MIDI、PartCrafter等方法的比较，证明了SceneMaker在各种场景下的优越性。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong>
尽管SceneMaker在处理任意物体和开放集场景方面表现出色，但论文也指出了其局限性：</p>
<ul>
<li><strong>真实世界复杂性：</strong> 真实世界中物体的排列方式可能比现有数据集捕捉到的更为复杂，尤其是在涉及力学交互（force interactions）的情况下。</li>
<li><strong>控制信号的局限：</strong> 目前的场景生成方法主要通过图像或简单文本提示进行控制，未来需要更丰富的控制信号和更自然的语言交互方式。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>物理可信的场景构建：</strong> 研究如何更准确地构建物理上可信的3D场景，包括处理物体间的相互穿插（interpenetration）和力学交互。</li>
<li><strong>更精细的场景控制：</strong> 探索更高级的控制信号和自然语言交互，以实现更精细的场景生成控制。</li>
<li><strong>深度理解与具身智能：</strong> 进一步研究如何实现对生成的高质量3D场景的深度理解，并将其应用于具身智能（embodied AI）决策任务。</li>
</ul>
<p>总而言之，SceneMaker通过创新的解耦策略和统一的模型设计，有效解决了3D场景生成在开放集和遮挡场景下的关键挑战，显著提升了生成场景的几何质量和姿态准确性，并为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a decoupled 3D scene generation framework called SceneMaker in this work.</li>
<li>Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10957v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10957v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10956v1'></a></p>
<h2 id="empowering-dynamic-urban-navigation-with-stereo-and-mid-level-vision"><a href="https://arxiv.org/abs/2512.10956v1">Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</a></h2>
<p><strong>Authors:</strong> Wentao Zhou, Xuweiyi Chen, Vignesh Rajagopal, Jeffrey Chen, Rohan Chandra, Zezhou Cheng</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</p>
<p><strong>作者：</strong> Wentao Zhou, Xuweiyi Chen, Vignesh Rajagopal, Jeffrey Chen, Rohan Chandra, Zezhou Cheng</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决当前端到端机器人导航基础模型（NFMs）在动态和非结构化城市环境中导航能力不足的问题。现有NFMs主要依赖单目视觉输入，忽略了深度估计、目标跟踪等关键的<strong>中层视觉（mid-level vision）</strong>模块。这种方法虽然简化了模型，但面临以下挑战：
*   <strong>数据稀疏性：</strong> 像素到动作的监督信号获取困难，尤其是在真实世界场景中。
*   <strong>深度尺度模糊：</strong> 单目视觉固有的深度尺度不确定性限制了精确的几何和空间推理。
*   <strong>动态环境理解不足：</strong> 复杂的行人运动、不规则的道路配置以及多样的物体类别，要求模型具备更强的3D场景语义、几何和动态理解能力。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
作者提出了<strong>StereoWalker</strong>模型，通过引入<strong>立体视觉（stereo vision）</strong>和<strong>显式中层视觉模块</strong>来增强NFMs的能力。其核心创新点包括：
*   <strong>立体视觉输入：</strong> 利用左右眼图像对，有效解决了单目视觉的深度尺度模糊问题，提供了更可靠的几何信息。
*   <strong>显式中层视觉模块：</strong> 集成了先进的<strong>深度估计</strong>（如Depth-AnythingV2）和<strong>密集点跟踪</strong>（如CoTracker3）模块。这些模块提取的几何和运动信息被整合到模型中，而非隐式地让模型自行学习。
*   <strong>新的立体导航数据集：</strong> 收集并整理了一个大规模的立体城市导航数据集（DIVERCITY），该数据集包含来自全球多个城市的VR180立体视频，并经过自动过滤和质量控制，以支持训练和研究。
*   <strong>改进的Transformer架构：</strong> StereoWalker保留了所有图像块（patch）的特征，而非仅使用一个全局[CLS] token，以保留更精细的空间结构。模型采用了<strong>跟踪引导注意力（tracking-guided attention）</strong>机制，结合全局注意力和目标注意力，有效融合了立体图像、深度信息、跟踪信息以及目标位置，以预测未来航点和动作。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> StereoWalker在CityWalker基准测试中，使用仅占原始数据1.5%的数据量，就达到了与现有最先进（SOTA）模型相当的性能。当使用全部数据时，StereoWalker超越了SOTA模型。
*   <strong>立体视觉优势：</strong> 实验证明，立体视觉相比单目视觉能显著提高导航性能。
*   <strong>中层视觉的重要性：</strong> 消融实验表明，显式地引入深度和跟踪等中层视觉信息，能够显著提升导航的准确性、稳定性和数据效率，加速模型训练过程。
*   <strong>真实世界部署：</strong> 在真实机器人（Clearpath Jackal）上的部署测试也验证了StereoWalker在各种运动模式下的鲁棒性和优越性。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>计算资源需求：</strong> StereoWalker在机器人部署时仍需要GPU支持，并且其计算量（2.89 GB VRAM）大于CityWalker。
*   <strong>“转弯”场景的挑战：</strong> 在“转弯”场景中，模型性能略有下降，作者推测这可能与数据不平衡以及小角度误差的累积有关。
*   <strong>数据集限制：</strong> 虽然作者构建了新的数据集，但仍有进一步探索更广泛机器人任务和更大数据集的需求。</p>
<p><strong>5. 未来研究方向：</strong>
*   将立体视觉和中层视觉的理念推广到更广泛的机器人任务中，例如移动操作器和空中机器人。
*   探索更广阔的机器人学习领域，利用中层视觉来提升泛化能力和灵活性。
*   训练更大规模、更多样化的机器人数据集，以期获得更强的泛化能力和更灵活的机器人模型。</p>
<p><strong>总结：</strong>
StereoWalker通过整合立体视觉和显式中层视觉模块，成功地解决了现有端到端导航模型在动态城市环境中面临的深度不确定性和理解能力不足的问题。该模型不仅在多个基准测试和真实世界部署中取得了SOTA性能，而且显著提高了训练效率，证明了计算机视觉核心表示在构建强大机器人导航模型中的持续重要性。这项工作为未来更鲁棒、更高效的城市导航系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.</li>
<li>We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking.</li>
<li>Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10956v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10956v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10955v1'></a></p>
<h2 id="omni-attribute-open-vocabulary-attribute-encoder-for-visual-concept-personalization"><a href="https://arxiv.org/abs/2512.10955v1">Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization</a></h2>
<p><strong>Authors:</strong> Tsai-Shien Chen, Aliaksandr Siarohin, Guocheng Gordon Qian, Kuan-Chieh Jackson Wang, Egor Nemchinov, Moayed Haji-Ali, Riza Alp Guler, Willi Menapace, Ivan Skorokhodov, Anil Kag, Jun-Yan Zhu, Sergey Tulyakov</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下中文解读：</p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3句话)</strong></p>
<p>这篇论文提出了 Omni-Attribute，一个首创的开放词汇图像属性编码器，能够学习高保真、属性特定的表示。通过精心设计的数据集和创新的双目标训练范式，该方法有效解决了现有方法中属性纠缠导致的信息泄露问题，实现了更精确的视觉概念个性化和组合生成。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>该论文的核心创新在于其<strong>联合设计的数据和模型方法</strong>，具体体现在：</p>
<ul>
<li><strong>语义链接的图像对与正负属性标注：</strong> 作者构建了一个独特的数据集，其中包含语义上相关的图像对，并明确标注了需要保留（正属性）和需要抑制（负属性）的视觉特征。这种显式的监督信号是学习属性特定表示的关键。</li>
<li><strong>双目标训练范式：</strong> 论文采用了结合了<strong>生成保真度</strong>和<strong>对比解耦</strong>的训练目标。这意味着模型不仅要能够生成高质量的图像，还要能够通过对比学习的方式，将不同的属性清晰地分离出来，避免相互干扰。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>Omni-Attribute 的出现有望对视觉概念个性化领域产生深远影响：</p>
<ul>
<li><strong>提升个性化效果的准确性和可控性：</strong> 通过学习属性特定的表示，模型能够更精确地捕捉和转移用户指定的属性，从而生成更符合用户期望的个性化图像，减少不相关的视觉信息泄露。</li>
<li><strong>推动开放词汇属性的理解和应用：</strong> “开放词汇”的特性意味着该模型能够处理更广泛、更灵活的属性描述，而不仅仅局限于预定义的类别。这将极大地扩展视觉概念个性化的应用范围。</li>
<li><strong>为更复杂的图像编辑和生成任务奠定基础：</strong> 这种精细的属性控制能力为未来更复杂的图像编辑、风格迁移、内容合成等任务提供了强大的技术支撑。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<p>这项研究的成果可以广泛应用于以下领域：</p>
<ul>
<li><strong>个性化内容创作：</strong> 例如，为用户生成具有特定身份、表情、服装风格的虚拟形象或图像。</li>
<li><strong>数字艺术和设计：</strong> 艺术家和设计师可以利用该技术更精细地控制图像的风格、光照等元素，实现更具创意的作品。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 在虚拟环境中创建高度个性化的虚拟角色和场景。</li>
<li><strong>图像编辑和修复：</strong> 精准地修改图像的特定属性，如改变人物表情、调整光照效果等。</li>
<li><strong>内容检索：</strong> 基于更细粒度的属性进行图像检索，例如搜索“带有微笑表情的特定人物”。</li>
<li><strong>视频生成和编辑：</strong> 将属性转移到视频序列中，实现更具表现力的视频内容。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了显著的进步，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>对训练数据的依赖性：</strong> 论文强调了“联合设计的数据”，这意味着模型的性能可能高度依赖于所构建的语义链接图像对和属性标注的质量和覆盖范围。如果训练数据存在偏差或不足，可能会影响模型在某些属性上的泛化能力。</li>
<li><strong>计算成本：</strong> 学习高保真、属性特定的表示，并结合双目标训练，可能需要大量的计算资源和训练时间。</li>
<li><strong>属性的定义和粒度：</strong> 尽管是“开放词汇”，但如何精确定义和量化某些抽象的属性（例如，某些风格的细微差别）仍然是一个挑战。摘要中未详细说明模型如何处理模糊或主观的属性。</li>
<li><strong>对“ unseen contexts”的泛化能力：</strong> 摘要提到“unseen contexts”，但具体在多大程度上能够处理完全陌生的场景和属性组合，还需要进一步的实验验证。</li>
</ul>
<p>总而言之，Omni-Attribute 是一项令人兴奋的研究，它通过创新的数据设计和模型训练方法，显著提升了视觉概念个性化的能力，为未来的图像生成和编辑技术开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations.</li>
<li>Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement.</li>
<li>The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10955v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10955v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10954v1'></a></p>
<h2 id="group-diffusion-enhancing-image-generation-by-unlocking-cross-sample-collaboration"><a href="https://arxiv.org/abs/2512.10954v1">Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration</a></h2>
<p><strong>Authors:</strong> Sicheng Mo, Thao Nguyen, Richard Zhang, Nick Kolkin, Siddharth Srinivasan Iyer, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration”的论文的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration
<strong>作者：</strong> Sicheng Mo, Thao Nguyen, Richard Zhang, Nick Kolkin, Siddharth Srinivasan Iyer, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
传统扩散模型在生成图像时，通常是独立地处理每个样本，即使在批处理（batch）中，图像之间的信息也未能有效共享。这导致在推理阶段，模型未能充分利用样本间的潜在关联来提升生成质量。论文的核心研究问题在于：<strong>能否让同一批次中的多个图像在生成过程中进行协作，以共同提升生成效果？</strong></p>
<p><strong>2. 关键创新/方法论贡献：</strong>
作者提出了<strong>Group Diffusion</strong>（组扩散）框架，其核心创新在于：</p>
<ul>
<li><strong>跨样本注意力机制（Cross-Sample Attention）：</strong> Group Diffusion 引入了一种新的注意力机制，允许模型在推理时将注意力从单个图像内部的 patch 扩展到同一批次中的其他图像的 patch。这意味着图像可以“互相学习”和“互相帮助”，共同完成去噪过程。</li>
<li><strong>联合去噪（Joint Denoising）：</strong> 在推理阶段，Group Diffusion 能够联合地去噪一组图像，从而学习到图像内部（intra-image）和图像之间（inter-image）的对应关系。</li>
<li><strong>可扩展性（Scaling Effect）：</strong> 作者发现，增加组的大小（group size）能够显著增强跨样本注意力，并带来更好的生成质量。</li>
<li><strong>定性度量（Qualitative Measure）：</strong> 论文引入了一种新的定性度量来捕捉跨样本注意力的强度，并证明其与生成质量（FID）高度相关。</li>
<li><strong>框架集成性：</strong> Group Diffusion 是一个即插即用的框架，可以轻松集成到现有的标准扩散 Transformer 模型（如 DiT 和 SiT）中，而无需大幅修改模型架构。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>显著的性能提升：</strong> Group Diffusion 在 ImageNet-256x256 数据集上取得了高达 32.2% 的 FID 改进。当与 SiT-XL/2 模型结合时，在从头训练和从预训练模型继续训练的情况下，分别实现了 20.9% 和 32.2% 的 FID 提升。
*   <strong>规模效应的验证：</strong> 实验表明，更大的组大小（如从 1 增加到 8 或 16）能够带来持续的生成质量提升，并且与更强的跨样本注意力相关联。
*   <strong>对生成过程的深入理解：</strong> 通过分析注意力图，作者发现跨样本注意力在早期去噪阶段尤为活跃，有助于形成全局结构和语义信息。同时，作者还发现，能够接收更高注意力权重的图像对生成结果的影响更大。
*   <strong>对表示学习的启示：</strong> Group Diffusion 的成功表明，跨样本的交互可以作为一种隐式的监督信号，增强模型的表示学习能力，从而生成更强大、更具泛化性的扩散模型。
*   <strong>通用性：</strong> 该方法在文本到图像生成任务（MS-COCO 数据集）上也表现出有效性，进一步证明了其通用性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>训练成本增加：</strong> Group Diffusion 的一个主要局限性是增加了训练和推理的计算成本。当组大小为 n 时，GroupDiff-f 和 GroupDiff-l 的训练时间会分别增加约 (n-1) 倍和 (0.1n) 倍，推理时间也会相应增加。
*   <strong>对相关性依赖：</strong> 该方法的效果依赖于组内图像之间的语义或视觉相关性。如果组内图像关联性不强，效果可能会打折扣。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提高效率：</strong> 探索更高效的 Group Diffusion 变体，以降低计算成本，使其更易于大规模应用。
*   <strong>更灵活的输入：</strong> 利用 Group Diffusion 的跨样本交互特性，将其扩展到处理更复杂的输入，例如多样化或跨条件的输入，实现更灵活的图像生成。
*   <strong>作为教师模型：</strong> 利用 Group Diffusion 训练出的高质量模型，作为“教师”来蒸馏出更轻量级的模型。
*   <strong>连接表示学习与生成模型：</strong> 进一步探索跨样本交互如何作为一种隐式监督，为更强大、更具泛化性的扩散模型提供新的视角。</p>
<p><strong>总结：</strong>
Group Diffusion 是一项开创性的工作，它打破了传统扩散模型在推理阶段独立生成样本的范式。通过引入跨样本注意力机制，使得同一批次内的图像能够协同去噪，从而显著提升了图像生成质量。该方法不仅在量化指标上取得了优异的成绩，还为理解扩散模型的内部工作机制提供了新的视角，并为未来的研究开辟了新的方向。其核心思想——“协作生成”——为提升生成模型的性能提供了一种简单而有效的新途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image.</li>
<li>Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10954v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10954v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10950v1'></a></p>
<h2 id="e-rayzer-self-supervised-3d-reconstruction-as-spatial-visual-pre-training"><a href="https://arxiv.org/abs/2512.10950v1">E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</a></h2>
<p><strong>Authors:</strong> Qitao Zhao, Hao Tan, Qianqian Wang, Sai Bi, Kai Zhang, Kalyan Sunkavalli, Shubham Tulsiani, Hanwen Jiang</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training”的全面中文摘要：</p>
<p><strong>论文题目：</strong> E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</p>
<p><strong>作者：</strong> Qitao Zhao, Hao Tan, Qianqian Wang, Sai Bi, Kai Zhang, Kalyan Sunkavalli, Shubham Tulsiani, Hanwen Jiang</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前3D视觉领域中一个关键的未被充分探索的领域：如何从无标签的多视图图像中学习具有3D空间意识的表征。尽管自监督学习在语言、2D图像和视频领域取得了巨大成功，但其在3D视觉领域的应用仍显不足。现有的3D视觉模型通常依赖于有监督的3D伪标签，这效率低下且难以扩展。而先前的自监督方法（如RayZer）通过隐式的潜在空间视图合成来间接推断3D信息，这可能导致捷径解决方案和几何基础不牢固的表征。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
E-RayZer 提出了一个<strong>首个完全自监督的、直接在3D空间中进行3D高斯（3D Gaussians）重建的模型</strong>，从而开创了3D空间视觉预训练的新范式。其核心创新包括：</p>
<ul>
<li><strong>显式3D几何建模：</strong> 与RayZer的隐式方法不同，E-RayZer直接预测相机参数和显式的3D高斯，将几何正则化注入模型设计中。这确保了学习到的表征是几何上更具基础且可解释的。</li>
<li><strong>细粒度学习课程：</strong> 为了解决显式3D重建训练中的收敛性挑战，论文引入了一种新颖的、细粒度的学习课程。该课程基于<strong>视觉重叠度</strong>（包括几何和语义两种度量），从易到难地组织训练样本，并能自适应地协调异构数据源，从而提高了训练的稳定性和可扩展性。</li>
<li><strong>去除图像索引嵌入：</strong> 为了避免RayZer中存在的视图插值捷径，E-RayZer完全移除了图像索引嵌入，采用了一种更符合3D几何的Transformer架构。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
E-RayZer在多个方面取得了显著成果：</p>
<ul>
<li><strong>优于现有自监督方法：</strong> 在姿态估计任务上，E-RayZer显著优于RayZer，并且在3D下游任务的迁移学习中，其表征能力也超越了DINOv3、CroCo v2、VideoMAE V2等领先的视觉预训练模型。</li>
<li><strong>媲美甚至超越监督方法：</strong> 在3D重建和姿态估计任务上，E-RayZer在性能上能够与最先进的监督模型（如VGGT）相媲美，甚至在某些情况下表现更优。这表明大规模自监督学习本身就足以获得几何上可靠的3D理解。</li>
<li><strong>强大的3D空间意识：</strong> 通过可视化和在多个下游任务上的评估，E-RayZer学习到的表征显示出更强的3D空间意识，能够更准确地捕捉场景结构，并且在不同视图下保持一致性。</li>
<li><strong>可扩展的预训练框架：</strong> E-RayZer的成功证明了其作为3D空间视觉预训练框架的潜力，为未来开发更强大的3D理解模型奠定了基础。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中虽然没有明确列出局限性部分，但从实验设置和讨论中可以推断出一些潜在的考虑：</p>
<ul>
<li><strong>对数据质量和多样性的依赖：</strong> 实验结果表明，数据质量和多样性对模型性能至关重要，混合和高质量的数据集能带来更好的泛化能力。这意味着在实际应用中，数据的收集和预处理仍然是一个挑战。</li>
<li><strong>计算资源需求：</strong> 论文提到训练使用了8个A100 GPU和大量的迭代次数，表明该模型在训练时需要相当大的计算资源。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于论文的研究成果和讨论，可以推测以下未来研究方向：</p>
<ul>
<li><strong>更广泛的数据集探索：</strong> 进一步探索和利用更大规模、更多样化的无标签3D数据，以提升模型的泛化能力和鲁棒性。</li>
<li><strong>更精细的3D表征学习：</strong> 探索更复杂的3D表征形式，例如更精细的几何细节或语义信息，以应对更具挑战性的3D任务。</li>
<li><strong>实时性与效率优化：</strong> 尽管E-RayZer在性能上表现出色，但进一步优化模型的计算效率，使其能够应用于实时场景，将是一个重要的研究方向。</li>
<li><strong>与其他模态的融合：</strong> 将E-RayZer的3D空间理解能力与语言、其他感知模态（如触觉）进行融合，构建更全面的多模态AI系统。</li>
<li><strong>更深入的理论分析：</strong> 对E-RayZer学习到的几何表征进行更深入的理论分析，理解其为何能获得如此强的3D理解能力。</li>
</ul>
<p><strong>总结：</strong>
E-RayZer通过引入显式的3D几何建模和创新的视觉重叠度学习课程，成功地实现了完全自监督的3D高斯重建，并在姿态估计和3D下游任务上取得了超越现有方法的优异性能。这不仅解决了3D视觉领域中学习3D空间意识表征的难题，更确立了E-RayZer作为3D空间视觉预训练新范式的地位，为未来3D AI的发展开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images.</li>
<li>To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner.</li>
<li>Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT.</li>
<li>Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10950v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10950v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.10949v1'></a></p>
<h2 id="are-we-ready-for-rl-in-text-to-3d-generation-a-progressive-investigation"><a href="https://arxiv.org/abs/2512.10949v1">Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</a></h2>
<p><strong>Authors:</strong> Yiwen Tang, Zoey Guo, Kaixin Zhu, Ray Zhang, Qizhi Chen, Dongzhi Jiang, Junli Liu, Bohan Zeng, Haoming Song, Delin Qu, Tianyi Bai, Dan Xu, Wentao Zhang, Bin Zhao</p>
<p><strong>Published:</strong> 2025-12-11</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</p>
<p><strong>作者：</strong> Yiwen Tang, Zoey Guo, Kaixin Zhu, Ray Zhang, Qizhi Chen, Dongzhi Jiang, Junli Liu, Bohan Zeng, Haoming Song, Delin Qu, Tianyi Bai, Dan Xu, Wentao Zhang, Bin Zhao</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
尽管强化学习（RL）在大型语言模型（LLMs）和2D图像生成方面取得了显著成功，但将其应用于3D文本到3D生成领域仍处于探索阶段。3D对象的空间复杂性（全局一致的几何结构和精细的局部纹理）使得3D生成对奖励设计和RL算法的选择极为敏感。现有研究主要集中在预训练和微调方法，而RL在3D生成中的潜力尚未被系统性地探索。本文旨在系统性地研究RL在文本到3D自回归生成中的应用，并解决其面临的挑战。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
作者提出了一个多维度的系统性研究框架，以探索RL在文本到3D自回归生成中的应用，并提出了以下关键创新：</p>
<ul>
<li><strong>系统性研究框架：</strong> 对奖励设计、RL算法、文本到3D基准和高级RL范式进行了深入分析。</li>
<li><strong>奖励设计探索：</strong> 评估了不同奖励维度和模型选择，强调了与人类偏好对齐的重要性，并发现通用多模态模型能提供3D属性的鲁棒信号。</li>
<li><strong>RL算法研究：</strong> 研究了GRPO变体，突出了token级别优化的有效性，并探讨了训练数据和迭代次数的缩放策略。</li>
<li><strong>新文本到3D基准 MME-3DR：</strong> 鉴于现有基准无法衡量3D生成模型中的隐式推理能力，作者提出了MME-3DR，一个包含249个3D对象的基准，涵盖了五种推理密集型类别，以评估模型在空间结构、机械功能、生物形状、世界知识和风格化表示等方面的能力。</li>
<li><strong>高级RL范式 Hi-GRPO：</strong> 受到3D生成自然层级结构的启发，作者提出了Hi-GRPO，一种通过专门的奖励集成来优化全局到局部的分层3D生成的方法。该方法将生成过程分解为两个阶段：首先生成全局几何结构，然后细化局部纹理。</li>
<li><strong>首个RL增强的文本到3D模型 AR3D-R1：</strong> 基于上述研究洞察，作者开发了AR3D-R1，这是第一个RL增强的文本到3D模型，能够从粗糙形状到纹理细化进行专家级生成。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>奖励设计：</strong> 与人类偏好对齐是3D自回归生成中RL的关键信号。通用大型多模态模型（LMMs）在评估3D属性方面表现出惊人的鲁棒性，但专门的奖励模型在特定维度上更具优势。
*   <strong>RL算法：</strong> 3D自回归模型从token级别优化中获益更多，而序列级别操作效果有限。DAPO等简单技术（如动态采样）足以稳定训练。数据缩放能有效缓解偏好偏差，但迭代次数需要仔细校准，过度训练可能导致过拟合。
*   <strong>MME-3DR基准：</strong> 现有文本到3D模型在生物和机械对象上表现尚可，但在其他类别（如空间结构、世界知识、风格化表示）上存在不足。RL训练显著提升了模型在所有类别上的性能，尤其是在风格化表示方面，凸显了隐式推理能力的重要性。MME-3DR能够同时评估生成质量和隐式推理能力。
*   <strong>Hi-GRPO范式：</strong> Hi-GRPO通过分层优化实现了从粗糙形状到精细纹理的生成过程，与人类3D感知过程相符。AR3D-R1在MME-3DR和Toys4K基准上均取得了优于现有模型的性能，尤其在几何一致性和纹理质量方面有显著提升。
*   <strong>整体意义：</strong> 本研究首次系统地将RL应用于文本到3D自回归生成，为该领域的研究提供了重要的见解和方法论指导，推动了RL在3D内容创作中的应用。</p>
<p><strong>4. 提及的局限性：</strong>
*   论文中提到，2D LMMs在准确检测3D组件方面存在困难，这可能影响对组件完整性的评估。
*   虽然RL训练显著提升了性能，但作者也指出，过度训练可能导致泛化能力下降，可能归因于对偏好特征的过拟合。
*   在奖励设计部分，作者提到通用LMMs在评估3D属性时表现出鲁棒性，但“系统性偏差”也可能存在。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   进一步探索更精细的奖励设计，以更好地捕捉3D对象的复杂属性。
*   研究更高效的RL算法和训练策略，以应对3D生成的高计算成本。
*   扩展MME-3DR基准，增加更多样化和更具挑战性的3D对象和场景。
*   探索Hi-GRPO范式在其他3D生成任务中的应用，如3D编辑和3D理解。
*   研究如何更好地结合LLMs和LMMs的能力，以实现更智能、更具创造性的3D内容生成。</p>
<p><strong>总结：</strong></p>
<p>这篇论文是<strong>首个系统性地探索强化学习在文本到3D自回归生成中应用的研究</strong>。作者通过深入分析奖励设计、RL算法、基准测试和高级RL范式，提出了<strong>MME-3DR基准</strong>以评估模型的隐式推理能力，并创新性地设计了<strong>Hi-GRPO分层RL范式</strong>。最终，他们开发了<strong>AR3D-R1模型</strong>，该模型在多个基准测试中取得了<strong>显著的性能提升</strong>，尤其是在几何一致性和纹理质量方面。这项工作为RL在3D内容生成领域的未来研究奠定了坚实的基础，并为解决3D生成中的复杂挑战提供了宝贵的见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>(3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR.</li>
<li>(4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles.</li>
<li>Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.10949v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.10949v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-12 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
