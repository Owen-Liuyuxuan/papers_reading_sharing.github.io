<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-30 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-31/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-30">Arxiv Computer Vision Papers - 2025-12-30</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#embodied-robot-manipulation-in-the-era-of-foundation-models-planning-and-learning-perspectives" class="nav-link">Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-knows-transparency-repurposing-video-diffusion-for-transparent-object-depth-and-normal-estimation" class="nav-link">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#idt-a-physically-grounded-transformer-for-feed-forward-multi-view-intrinsic-decomposition" class="nav-link">IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</a>
                </li>
                <li class="nav-item">
                    <a href="#robomirror-understand-before-you-imitate-for-video-to-humanoid-locomotion" class="nav-link">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</a>
                </li>
                <li class="nav-item">
                    <a href="#omniagent-audio-guided-active-perception-agent-for-omnimodal-audio-video-understanding" class="nav-link">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#rethinking-the-spatio-temporal-alignment-of-end-to-end-3d-perception" class="nav-link">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#memorization-in-3d-shape-generation-an-empirical-study" class="nav-link">Memorization in 3D Shape Generation: An Empirical Study</a>
                </li>
                <li class="nav-item">
                    <a href="#same-or-not-enhancing-visual-perception-in-vision-language-models" class="nav-link">Same or Not? Enhancing Visual Perception in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#livetalk-real-time-multimodal-interactive-video-diffusion-via-improved-on-policy-distillation" class="nav-link">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</a>
                </li>
                <li class="nav-item">
                    <a href="#thinkgen-generalized-thinking-for-visual-generation" class="nav-link">ThinkGen: Generalized Thinking for Visual Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-30">Arxiv Computer Vision Papers - 2025-12-30</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我为您整理了这份 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2025-12-28)</strong></p>
<p><strong>报告日期:</strong> 2025-12-28
<strong>涵盖论文数量:</strong> 10</p>
<p><strong>1. 主要主题与趋势概览</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的快速进展：</p>
<ul>
<li><strong>基础模型 (Foundation Models) 的广泛应用与融合:</strong> 尤其是在机器人操作、多模态理解和视频生成等领域，基础模型正成为驱动创新的核心力量。</li>
<li><strong>多模态理解与生成:</strong> 音频-视频理解、视觉-语言交互以及实时交互式视频生成是研究热点，强调跨模态信息的有效整合。</li>
<li><strong>物理世界的理解与交互:</strong> 论文关注如何让模型更深入地理解物理规律，例如透明物体深度估计、具有物理约束的 3D 形状生成以及机器人模仿学习。</li>
<li><strong>端到端 3D 感知与生成:</strong> 在 3D 形状生成和时空对齐方面，研究人员正在探索更高效、更具泛化性的方法。</li>
</ul>
<p><strong>2. 重点关注的创新性论文</strong></p>
<ul>
<li><strong>"Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"</strong>: 该论文巧妙地将视频扩散模型应用于透明物体的深度和法线估计，这是一种非常有前景的跨领域应用，有望解决传统方法在处理透明物体时的挑战。</li>
<li><strong>"RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion"</strong>: 提出了一种“先理解后模仿”的视频到人形机器人运动迁移方法，强调了对视频内容的深入理解而非简单像素级模仿，对于提升机器人运动的鲁棒性和泛化性具有重要意义。</li>
<li><strong>"OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"</strong>: 构建了一个能够进行音频引导的主动感知智能体，实现了更全面的音视频理解，这标志着多模态智能体在复杂场景理解方面迈出了重要一步。</li>
</ul>
<p><strong>3. 新兴研究方向与技术</strong></p>
<ul>
<li><strong>扩散模型 (Diffusion Models) 的多功能化:</strong> 从视频生成扩展到物理属性（如深度、法线）的估计，显示了扩散模型在不同视觉任务中的强大潜力。</li>
<li><strong>物理约束的引入:</strong> 在 3D 形状生成和内在分解任务中，显式地引入物理规律，以提高模型的准确性和可信度。</li>
<li><strong>主动感知与交互式生成:</strong> 智能体能够根据音频线索主动感知环境，以及实时、交互式的视频生成，预示着更智能、更具动态性的视觉系统。</li>
<li><strong>视觉-语言模型的增强:</strong> 关注如何提升视觉-语言模型在细粒度视觉感知上的能力，例如区分相似物体。</li>
</ul>
<p><strong>4. 建议阅读全文的论文</strong></p>
<p>考虑到其创新性、潜在影响力和对前沿趋势的代表性，以下论文建议您优先阅读全文：</p>
<ul>
<li><strong>"Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"</strong>: 鉴于其新颖的应用方向和对扩散模型的创造性运用。</li>
<li><strong>"RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion"</strong>: 对于机器人学和行为模仿领域的研究者来说，其“理解优先”的范式具有重要启发意义。</li>
<li><strong>"OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"</strong>: 代表了多模态智能体在复杂场景理解方面的重要进展。</li>
<li><strong>"Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives"</strong>: 作为一篇综述性论文，它能帮助您快速了解基础模型在机器人操作领域的最新发展和未来方向。</li>
</ul>
<hr />
<p>这份摘要旨在为您提供一个快速了解近期 Arxiv 计算机视觉领域研究动态的窗口。希望它能帮助您高效地把握该领域的最新进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.22983v1">Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives</a></li>
<li><a href="#2512.23705v1">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</a></li>
<li><a href="#2512.23667v1">IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</a></li>
<li><a href="#2512.23649v1">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</a></li>
<li><a href="#2512.23646v1">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</a></li>
<li><a href="#2512.23635v1">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</a></li>
<li><a href="#2512.23628v1">Memorization in 3D Shape Generation: An Empirical Study</a></li>
<li><a href="#2512.23592v1">Same or Not? Enhancing Visual Perception in Vision-Language Models</a></li>
<li><a href="#2512.23576v1">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</a></li>
<li><a href="#2512.23568v1">ThinkGen: Generalized Thinking for Visual Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.22983v1'></a></p>
<h2 id="embodied-robot-manipulation-in-the-era-of-foundation-models-planning-and-learning-perspectives"><a href="https://arxiv.org/abs/2512.22983v1">Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives</a></h2>
<p><strong>Authors:</strong> Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, Cheng Chi, Chang Xu, Xiaolong Zheng, Donglin Wang, Haoang Li, Shanghang Zhang, Badong Chen</p>
<p><strong>Published:</strong> 2025-12-28</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives”的全面中文摘要，重点关注其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文题目：</strong> Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives</p>
<p><strong>作者：</strong> Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, Cheng Chi, Chang Xu, Xiaolong Zheng, Donglin Wang, Haoang Li, Shanghang Zhang, Badong Chen</p>
<p><strong>摘要：</strong></p>
<p>这篇论文是一篇全面的综述，旨在为机器人基础模型时代的具身机器人操作提供一个统一的理解框架，重点关注规划和学习的视角。随着计算机视觉、语言和多模态学习的飞速发展，机器人基础模型取得了显著进展，但机器人操作仍然是一个核心且充满挑战的问题。</p>
<p><strong>1. 主要研究问题/研究目标：</strong></p>
<p>论文的核心目标是<strong>系统地梳理和组织当前基于学习的机器人操作方法，并提出一个统一的框架来理解它们在“高层规划”和“低层控制”两个层面的算法原理。</strong> 此外，论文还旨在<strong>识别当前机器人操作领域面临的开放性挑战，并提出未来的研究方向，以推动机器人基础模型在真实世界中的鲁棒性和可扩展性。</strong></p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<ul>
<li><strong>统一的抽象框架：</strong> 论文最核心的贡献在于提出了一个<strong>高层规划与低层学习控制的统一抽象框架</strong>。<ul>
<li><strong>高层规划的扩展：</strong> 将经典的“任务规划”概念扩展到包含<strong>语言、代码、运动、功能（affordances）和三维表示（3D representations）的推理</strong>，强调它们在结构化和长时序决策中的作用。</li>
<li><strong>低层控制的分类法：</strong> 提出了一个<strong>面向训练范式的分类法</strong>，将学习控制方法按照<strong>输入建模（Input Modeling）、潜在表征学习（Latent Representation Learning）和策略学习（Policy Learning）</strong>进行组织。这种分类法有助于更清晰地理解不同方法的联系和差异。</li>
</ul>
</li>
<li><strong>算法视角而非模型类别：</strong> 与许多侧重于特定模型类别（如视觉-语言-动作模型）的综述不同，本文从<strong>规划和学习的抽象视角</strong>出发，将各种方法置于一个统一的框架下进行分析。</li>
<li><strong>识别关键能力：</strong> 论文将高层规划的能力分解为<strong>任务规划与技能选择、代码生成、运动规划、功能学习和三维表征</strong>，并详细阐述了它们如何支持机器人操作。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>清晰的结构化分析：</strong> 通过提出的框架，论文为理解和比较当前机器人操作领域的各种方法提供了一个清晰的路线图。这有助于研究人员快速掌握该领域的最新进展，并识别研究空白。</li>
<li><strong>揭示基础模型的作用：</strong> 论文强调了大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在高层规划中的关键作用，以及它们如何简化系统设计并提升泛化能力。</li>
<li><strong>连接感知与行动：</strong> 论文详细阐述了低层控制如何将感知输入转化为可执行的动作，从而实现高层规划的物理执行。这对于构建能够理解和执行复杂指令的具身智能体至关重要。</li>
<li><strong>指导未来研究：</strong> 论文识别出的开放性挑战和未来研究方向，为该领域的研究人员提供了宝贵的参考，指明了未来发展的重点。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<p>论文虽然全面，但其本身作为一篇综述，其局限性主要体现在：</p>
<ul>
<li><strong>对现有工作的组织和总结：</strong> 论文的重点在于对现有工作的梳理和分类，而非提出全新的实验结果或模型。</li>
<li><strong>对未来研究的展望：</strong> 论文指出的挑战和方向是基于当前的研究现状和趋势进行的预测，实际的进展可能受到多种因素的影响。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>论文明确指出了四个核心的未来研究方向：</p>
<ul>
<li><strong>构建真正的机器人大脑（Core Challenge 1: Building a True Robot Brain）：</strong><ul>
<li>开发支持灵活模态和具身接口的通用架构，并能随数据和计算能力扩展。</li>
<li>持续学习和终身学习，以缓解遗忘并实现正向迁移。</li>
<li>鲁棒的长时序执行，通过高层规划与低层闭环控制的紧密结合，维持“成功漏斗”内的行为。</li>
<li>稳定平滑的运动生成，结合动力学一致的轨迹和符合物理特性的行为，以支持安全的物理交互。</li>
</ul>
</li>
<li><strong>数据瓶颈与仿真到真实（Sim-to-Real Gap）（Core Challenge 2: Data Bottleneck and Sim-to-Real Gap）：</strong><ul>
<li>建立可扩展的“数据飞轮”，使机器人能够自主收集经验，并选择性地提炼高价值信号。</li>
<li>实现更高保真度的仿真，尤其是在接触丰富和可变形物体交互方面。</li>
<li>开发可优化的仿真管线，以实现比纯粹试错更高效的策略优化。</li>
</ul>
</li>
<li><strong>多模态物理交互（Core Challenge 3: Multimodal Physical Interaction）：</strong><ul>
<li>融合更丰富的感知流（触觉、听觉、本体感觉等），以获得统一、时序连贯的表征。</li>
<li>处理可变形和复杂材料（如布料、线缆、流体）的操纵，这些场景的状态空间维度极高且接触动力学占主导。</li>
<li>开发新的物体表征（如图或场模型），以及更强的物理信息推理和对不确定接触的稳定学习算法。</li>
</ul>
</li>
<li><strong>安全与协作（Core Challenge 4: Safety and Collaboration）：</strong><ul>
<li>开发内在安全控制，实时尊重运动学/动力学限制并调节平滑度和力/能量。</li>
<li>实现跨机器人安全，通过预测性协调和共享协议实现多智能体操作。</li>
<li>支持人类意图推理和共享自主性，以实现有效的人机协作。</li>
<li>鲁棒部署需要自主故障检测和恢复，以及在安全关键场景下结合学习的适应性和经典方法的稳定性。</li>
</ul>
</li>
</ul>
<p><strong>在计算机视觉领域的意义：</strong></p>
<p>这篇综述对于计算机视觉领域具有重要意义，因为它：</p>
<ul>
<li><strong>强调了多模态融合的重要性：</strong> 论文深入探讨了如何将视觉信息与语言、触觉、力觉等其他模态信息融合，以实现更鲁棒和通用的机器人操作。这推动了多模态视觉模型的研究，特别是如何有效地整合不同模态的信息以获得更丰富的场景理解和行为决策。</li>
<li><strong>推动了三维视觉在机器人操作中的应用：</strong> 论文详细阐述了三维表示（如点云、高斯溅射、神经描述符场）在机器人操作中的作用，以及如何利用它们来增强空间推理和物理交互的准确性。这促进了三维重建、场景理解和三维几何推理等计算机视觉技术在机器人领域的应用。</li>
<li><strong>为视觉基础模型在机器人领域的应用提供了框架：</strong> 论文将大型视觉基础模型（如LLMs、MLLMs）视为机器人操作中的关键组成部分，并分析了它们如何用于高层规划和低层控制。这为研究人员如何利用和适配这些强大的视觉模型来解决机器人操作的挑战提供了指导。</li>
<li><strong>指明了未来视觉研究的方向：</strong> 论文提出的挑战，如数据瓶颈、Sim-to-Real Gap、多模态交互和安全性，都直接或间接地指向了未来计算机视觉研究需要关注的方向，例如更高效的数据利用、更鲁棒的仿真到真实迁移、更精细的物理交互理解以及更安全的视觉感知系统。</li>
</ul>
<p>总而言之，这篇综述为理解和推进具身机器人操作领域的研究提供了宝贵的框架和方向，尤其是在利用基础模型和多模态信息方面，对计算机视觉领域的研究者具有重要的启发意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22983v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22983v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23705v1'></a></p>
<h2 id="diffusion-knows-transparency-repurposing-video-diffusion-for-transparent-object-depth-and-normal-estimation"><a href="https://arxiv.org/abs/2512.23705v1">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</a></h2>
<p><strong>Authors:</strong> Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation”的全面中文摘要：</p>
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决计算机视觉领域中一个长期存在的难题：<strong>透明和反光物体的精确深度和法线估计</strong>。传统方法（如双目立体视觉、ToF传感器和单目深度估计模型）在处理这些物体时存在固有的局限性。透明物体会引起光线的折射、反射和透射，打破了这些方法的关键假设，导致深度图出现空洞、估计不稳定，尤其是在视频序列中， temporal consistency（时间一致性）差，严重影响了下游任务（如机器人抓取）的性能。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>核心洞察：</strong> 作者观察到，现代视频扩散模型（VDM）能够生成逼真的透明物体现象，这表明它们已经内化了相关的光学规律。</li>
<li><strong>TransPhy3D 数据集：</strong> 构建了<strong>首个大规模的透明/反光物体合成视频数据集</strong>，包含 11,000 个序列（132 万帧）。该数据集通过 Blender/Cycles 渲染，使用了丰富的静态和程序化 3D 模型库，并结合了物理渲染和 OptiX 去噪，生成了高质量的 RGB、深度和法线视频。</li>
<li><strong>DKT 模型（Diffusion Knows Transparency）：</strong><ul>
<li><strong>视频到视频翻译范式：</strong> 将视频深度估计重塑为<strong>视频到视频的翻译问题</strong>，而非传统的判别式估计。</li>
<li><strong>基于 LoRA 的 VDM 微调：</strong> 利用<strong>轻量级的 LoRA (Low-Rank Adaptation) 技术</strong>，在预训练的大型视频扩散模型基础上进行微调，以学习透明物体深度（和法线）的视频到视频翻译。</li>
<li><strong>联合训练策略：</strong> 采用<strong>共训练策略</strong>，将新构建的 TransPhy3D 数据集与现有的逐帧合成数据集结合训练，以充分利用现有数据并提高泛化能力。</li>
<li><strong>模型架构：</strong> 在 DiT (Diffusion Transformer) 主干网络中，将 RGB 和（带噪声的）深度潜变量进行拼接，实现对透明物体深度和法线的预测。</li>
</ul>
</li>
<li><strong>DKT-Normal：</strong> 提出了一个法线估计的变体模型，同样基于 VDM 微调。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>零样本 SOTA 性能：</strong> DKT 模型在<strong>零样本（zero-shot）设置下</strong>，在多个真实和合成视频数据集（包括 ClearPose, DREDS (CatKnown/CatNovel), 和 TransPhy3D-Test）上取得了<strong>当前最优（SOTA）的深度估计性能</strong>。</li>
<li><strong>显著优于基线：</strong> DKT 在准确性和时间一致性方面，显著优于强大的图像/视频基线方法（如 Depth-Anything-v2, Depth Crafter）。</li>
<li><strong>视频法线估计突破：</strong> DKT-Normal 模型在 ClearPose 数据集上取得了<strong>最佳的视频法线估计结果</strong>。</li>
<li><strong>高效性：</strong> 一个紧凑的 1.3B 参数版本可以在 0.17 秒/帧（832x480 分辨率）的速度下运行，满足实时性要求。</li>
<li><strong>实际应用价值：</strong> 将 DKT 集成到机器人抓取系统中，显著<strong>提高了在半透明、反光和漫反射表面上的抓取成功率</strong>，优于之前的估计器。</li>
<li><strong>理论意义：</strong> 研究结果有力地支持了“<strong>扩散模型理解透明性</strong>”的观点，表明生成式视频先验可以被高效、无标签地重新利用，用于实现鲁棒、时间一致的感知，尤其是在具有挑战性的真实世界操作任务中。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>数据集限制：</strong> 虽然 TransPhy3D 数据集规模庞大且多样，但作者也提到，在渲染过程中，相机轨迹被设计为围绕物体进行圆形运动，这可能对模型的时序一致性提出了更高的要求。</li>
<li><strong>零样本性能：</strong> 虽然零样本性能优异，但作者也通过消融实验表明，<strong>增加推理步数对性能提升有限</strong>，而过少的步数会导致预测不准确。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的场景和物体类别：</strong> 进一步扩展数据集以覆盖更广泛的场景和更复杂的透明/反光物体类型。</li>
<li><strong>实时性优化：</strong> 探索更高效的模型架构或推理策略，以进一步提升实时性能，满足更严苛的机器人应用需求。</li>
<li><strong>跨领域迁移：</strong> 研究如何将这种基于生成模型的方法更有效地迁移到其他具有挑战性的感知任务中。</li>
<li><strong>结合其他传感器信息：</strong> 探索将 DKT 与其他传感器（如 LiDAR、事件相机）的信息融合，以获得更鲁棒的感知结果。</li>
<li><strong>更精细的物理理解：</strong> 进一步挖掘扩散模型在理解和模拟复杂光学现象（如次表面散射、多重反射）方面的潜力。</li>
</ul>
<p>总而言之，这篇论文提出了一种新颖的方法，通过<strong>重新利用预训练的视频扩散模型</strong>，并结合<strong>大规模合成数据集</strong>，成功解决了透明物体深度和法线估计的长期难题。其提出的 DKT 模型在多个基准测试中取得了 SOTA 性能，并证明了其在机器人操作等实际应用中的有效性，为利用生成模型进行鲁棒感知开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23705v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23705v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23667v1'></a></p>
<h2 id="idt-a-physically-grounded-transformer-for-feed-forward-multi-view-intrinsic-decomposition"><a href="https://arxiv.org/abs/2512.23667v1">IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</a></h2>
<p><strong>Authors:</strong> Kang Du, Yirui Guan, Zeyu Wang</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition”的全面中文摘要：</p>
<p><strong>论文题目：</strong> IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition (IDT：一种物理约束的、用于前馈多视图内在分解的Transformer)</p>
<p><strong>作者：</strong> Kang Du, Yirui Guan, Zeyu Wang</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决多视图内在图像分解中的核心挑战：<strong>视图一致性</strong>。传统的单视图内在分解方法（如基于扩散模型的方法）在处理单个图像时表现出色，但将其扩展到多视图场景时，往往难以保证不同视角下分解出的内在因素（如反射率、阴影）保持一致，尤其是在存在视角相关效应（如镜面反射）的情况下。这种视图不一致性严重限制了多视图内在分解在三维重建、场景理解等下游任务中的应用。此外，现有的迭代生成式方法难以有效地强制跨视图的一致性。</p>
<p><strong>2. 关键创新与方法贡献：</strong></p>
<p>作者提出了<strong>内在分解Transformer (Intrinsic Decomposition Transformer, IDT)</strong>，一个前馈（feed-forward）的多视图内在图像分解框架，其核心创新点包括：</p>
<ul>
<li><strong>多视图Transformer聚合：</strong> 借鉴了多视图几何推理的最新进展，IDT利用Transformer的注意力机制，能够在一个前馈过程中联合推理多个输入图像，有效地聚合跨视图信息。</li>
<li><strong>物理约束的图像形成模型：</strong> IDT采用一个物理上合理的图像形成模型，将观察到的图像显式地分解为<strong>视图不变的漫反射率（albedo）</strong>、<strong>视图相关的漫射阴影</strong>（受光照影响的朗伯体效应）和<strong>视图相关的镜面阴影</strong>（非朗伯体效应）。这种分解方式明确分离了朗伯体和非朗伯体光传输，使得分解出的材质和光照因素更具可解释性和可控性。</li>
<li><strong>外观适配器（Appearance Adapters）：</strong> 为了让不同内在因素（如漫反射率、漫射阴影、镜面阴影）能够选择性地利用Transformer聚合后的信息，IDT引入了外观适配器。这些适配器通过轻量级的交叉注意力机制，将共享的场景级Token路由到各自的预测头，实现了因素特异性的推理，避免了材质和光照信息之间的混淆。</li>
<li><strong>前馈推理：</strong> IDT在一个前馈过程中完成所有视图的内在分解，避免了迭代生成式采样，从而提高了效率和可扩展性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>提升视图一致性：</strong> IDT在合成和真实世界数据集上均取得了显著优于现有方法的视图一致性，尤其是在漫反射率和漫射阴影方面。</li>
<li><strong>提高分解精度：</strong> IDT能够生成更清晰的漫反射率、更连贯的漫射阴影，并将镜面高光更有效地分离到镜面阴影分量中，减少了对漫反射率的污染。</li>
<li><strong>可解释性与可控性：</strong> 物理约束的分解模型使得输出的内在因素更具可解释性，便于后续的材质编辑和光照控制。</li>
<li><strong>鲁棒性：</strong> IDT在处理具有复杂几何、材质和光照的室内场景时表现出良好的泛化能力。</li>
</ul>
<p>这些结果表明，联合多视图推理和物理约束的分解模型对于实现高质量、视图一致的内在图像分解至关重要。IDT为多视图内在理解提供了一个简单而有效的框架。</p>
<p><strong>4. 局限性：</strong></p>
<p>论文中虽然没有明确列出局限性，但可以推断出：</p>
<ul>
<li><strong>对室内场景的侧重：</strong> 论文主要在室内数据集上进行了评估，其在室外或更复杂光照环境下的泛化能力可能需要进一步验证。</li>
<li><strong>对物理模型的依赖：</strong> 尽管物理模型是其优势，但模型的准确性仍依赖于该模型对真实世界光照和材质的近似程度。</li>
<li><strong>计算成本：</strong> 尽管是前馈模型，但Transformer的计算量仍然可能较高，尤其是在处理大量视图时。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<p>基于本文的研究，未来的工作可以探索：</p>
<ul>
<li><strong>更广泛的场景和光照条件：</strong> 将IDT扩展到室外场景，以及更具挑战性的光照环境。</li>
<li><strong>更精细的材质分解：</strong> 进一步细化材质属性的分解，例如区分不同类型的镜面反射或引入更复杂的BRDF模型。</li>
<li><strong>与下游任务的结合：</strong> 将IDT作为基础模块，应用于更复杂的视觉任务，如多视图三维重建、场景编辑、光照估计和新视图合成。</li>
<li><strong>效率优化：</strong> 探索更高效的Transformer架构或注意力机制，以进一步降低计算成本，支持更大规模或实时应用。</li>
<li><strong>端到端训练的改进：</strong> 探索更优化的训练策略，以进一步提升模型性能和收敛速度。</li>
</ul>
<p>总而言之，这篇论文通过引入IDT，成功地将Transformer的强大跨视图推理能力与物理约束的图像形成模型相结合，解决了多视图内在图像分解中的视图一致性难题，为该领域的研究和应用开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23667v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23667v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23649v1'></a></p>
<h2 id="robomirror-understand-before-you-imitate-for-video-to-humanoid-locomotion"><a href="https://arxiv.org/abs/2512.23649v1">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</a></h2>
<p><strong>Authors:</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</p>
<p><strong>作者：</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>当前人形机器人运动控制系统在理解和模仿人类运动方面存在显著差距。现有方法要么依赖于预先采集的动作捕捉数据，要么依赖于稀疏的文本指令，这两种方式都无法充分利用视频中丰富的视觉信息。基于文本的方法存在语义稀疏和流程错误的问题，而基于视频的方法则往往局限于机械的姿态模仿，缺乏真正的视觉理解。因此，研究如何让机器人能够真正理解视频内容并据此生成逼真、协调的人形运动是关键挑战。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>本文提出了 <strong>RoboMirror</strong>，一个首创的、无需姿态估计和重定向（retargeting-free）的视频到人形运动控制框架，其核心理念是“先理解，再模仿”。</p>
<ul>
<li><strong>“理解-重构-控制”范式：</strong> RoboMirror 采用了一个新颖的两阶段框架。<ul>
<li><strong>第一阶段（理解与重构）：</strong> 利用强大的视觉语言模型（VLM），如 Qwen3-VL，从原始的单目或第三人称视频中提取高层次的视觉运动意图（visual motion intents）。这些意图被用来指导一个基于扩散模型的运动潜变量（motion latent）重构器，将视频语义信息转化为具有运动学意义的潜变量。这种“重构优于对齐”的设计，确保了生成的运动潜变量在语义上与视频一致，同时具备物理上的合理性。</li>
<li><strong>第二阶段（控制）：</strong> 将重构的运动潜变量作为条件，输入到一个基于扩散模型的部署策略（diffusion-based deployment policy）中。该策略直接生成可执行的机器人动作，无需中间的姿态估计或重定向步骤。</li>
</ul>
</li>
<li><strong>VLM 的创新应用：</strong> RoboMirror 首次将 VLM 深度整合到人形机器人控制循环中，用于从原始视频中提取丰富的运动语义信息，克服了传统方法对稀疏指令或精确姿态的依赖。</li>
<li><strong>无姿态估计与重定向：</strong> 框架的核心在于绕过了传统方法中耗时且易出错的姿态估计和重定向环节，直接从视频理解到机器人动作生成，大大提高了效率和鲁棒性。</li>
<li><strong>MoE 教师策略与扩散学生策略：</strong> 引入了基于混合专家（MoE）的教师策略，利用模拟器中的特权信息进行训练，为学生策略提供高质量的监督信号。学生策略则是一个扩散模型，能够从视频潜变量中学习生成动作。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<p>RoboMirror 在多项实验中展现了卓越的性能：</p>
<ul>
<li><strong>显著降低延迟：</strong> 与基于姿态估计的方法相比，RoboMirror 将第三人称视频到机器人动作的控制延迟降低了 80%（从 9.22 秒降至 1.84 秒）。</li>
<li><strong>提高任务成功率：</strong> 在任务成功率方面，RoboMirror 比基线方法提高了 3.7%。</li>
<li><strong>实现“身临其境”的遥操作：</strong> 利用第一人称（egocentric）视频，RoboMirror 能够实现逼真的“身临其境”式遥操作体验。</li>
<li><strong>无需人类姿态监督：</strong> 在 egocentric 视频场景下，RoboMirror 能够合成鲁棒的、语义上一致的运动，而无需显式的人类姿态监督，这是传统姿态估计管道无法实现的。</li>
<li><strong>跨领域泛化能力：</strong> 实验证明了该框架在不同模拟器（IsaacGym 和 MuJoCo）之间以及跨硬件平台（Unitree G1 机器人）的泛化能力。</li>
<li><strong>推动理解驱动的控制：</strong> RoboMirror 通过将人形机器人控制的核心从机械模仿转向视觉理解，为“理解驱动的控制”奠定了基础，弥合了视觉理解与动作执行之间的鸿沟。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中虽然没有明确列出局限性，但可以推断出一些潜在的方面：</p>
<ul>
<li><strong>对 VLM 的依赖：</strong> 框架的性能在很大程度上依赖于 VLM 的理解能力。如果 VLM 对视频内容的理解不准确或不全面，可能会影响后续的运动生成。</li>
<li><strong>训练数据的需求：</strong> 尽管框架旨在减少对特定数据格式的依赖，但训练一个鲁棒的 VLM 和扩散模型仍然需要大量的视频-运动数据。</li>
<li><strong>复杂动态场景的挑战：</strong> 对于极其复杂、快速变化的动态场景，VLM 的实时理解能力和扩散模型的生成能力可能面临挑战。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更强大的 VLM 集成：</strong> 探索更先进、更具泛化能力的 VLM，以提升对各种复杂视频内容的理解能力。</li>
<li><strong>实时性优化：</strong> 进一步优化模型架构和推理过程，以实现更低延迟的实时控制，尤其是在资源受限的机器人平台上。</li>
<li><strong>多模态融合：</strong> 结合其他感官信息（如触觉、听觉）或更丰富的指令（如更精细的语言指令、意图描述），以实现更高级的交互和控制。</li>
<li><strong>长序列运动生成：</strong> 探索更有效的机制来处理和生成更长、更连贯的运动序列，以应对更复杂的任务。</li>
<li><strong>人机协作与交互：</strong> 将 RoboMirror 的能力扩展到人机协作场景，使机器人能够更好地理解和响应人类的意图和行为。</li>
</ul>
<p>总而言之，RoboMirror 是一项重要的研究成果，它通过创新的“理解-重构-控制”范式，利用 VLM 实现了从视频到人形机器人运动的端到端、无重定向的控制，显著提升了机器人的运动模仿能力、效率和鲁棒性，为未来人形机器人更智能、更自然的运动控制开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control.</li>
<li>We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate".</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23649v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23649v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23646v1'></a></p>
<h2 id="omniagent-audio-guided-active-perception-agent-for-omnimodal-audio-video-understanding"><a href="https://arxiv.org/abs/2512.23646v1">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</a></h2>
<p><strong>Authors:</strong> Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding”的全面中文摘要：</p>
<p><strong>论文题目：</strong> OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding
<strong>作者：</strong> Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</p>
<p><strong>摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
尽管全模态大语言模型（OmniLLMs）在统一音频和视觉模态方面取得了显著进展，但它们在细粒度的跨模态理解和模态对齐方面仍然存在不足。现有的方法通常依赖于僵化的、静态的工作流程，或者密集的帧-字幕生成，这限制了它们在复杂音频-视频理解任务中的表现。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
本文提出了一种名为 <strong>OmniAgent</strong> 的新型 <strong>音频引导的主动感知智能体</strong>。其核心创新在于：</p>
<ul>
<li><strong>主动感知范式：</strong> OmniAgent 将音频-视频理解从被动响应生成转变为主动的多模态探究。它通过一个迭代的“思考-行动-观察-反思”（Think-Act-Observe-Reflect）循环，动态地编排和调用专门的工具（视频、音频和事件工具），以实现更细粒度的音频-视频推理。</li>
<li><strong>粗粒度到细粒度的音频引导感知：</strong> 该方法的核心是一种新颖的“粗粒度到细粒度”的音频引导感知范式。它利用音频线索来精确定位时间事件，并以此指导后续的推理过程，从而有效解决跨模态对齐的难题。</li>
<li><strong>动态工具编排：</strong> OmniAgent 能够自主地规划和调用工具，根据需求动态地分配感知注意力到任务相关的线索上，而不是依赖预设的固定流程。它能够智能地决定何时依赖低成本的音频线索，何时进行高成本的视觉检查。</li>
<li><strong>模态感知专家工具集：</strong> 构建了一个全面的模态感知工具集，包括视频感知工具（如全局视频QA、片段QA）、音频感知工具（如ASR、全局音频字幕）和事件感知工具（如事件列表、事件位置）。这些工具在不同粒度和信息密度上提供服务。</li>
<li><strong>音频引导的事件定位：</strong> 提出了一种新颖的音频引导事件定位算法，能够高效地提取可检测的声音事件，并返回精确的时间戳，为细粒度的跨模态推理提供关键的时间锚点。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
在三个广泛使用的音频-视频理解基准测试（Daily-Omni, OmniVideoBench, WorldSense）上的广泛实验表明，OmniAgent 取得了 <strong>最先进（State-of-the-Art, SoTA）的性能</strong>。</p>
<ul>
<li>OmniAgent 的整体准确率显著优于现有的领先的开源和闭源模型，例如 Qwen3-Omni 和 Gemini2.5-Flash，在准确率上提高了 <strong>10%-20%</strong>。</li>
<li>实验结果证明了 OmniAgent 的主动感知范式和音频引导机制在解决跨模态对齐挑战和实现细粒度理解方面的有效性。</li>
<li>通过分析其推理行为，研究表明 OmniAgent 能够有效地利用不同粒度的工具，从全局上下文到局部细节进行推理，并能识别和处理模态间的潜在不一致性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   论文中提到，虽然当前方法依赖外部模型和扩展上下文提高了性能，但这也 <strong>限制了推理效率</strong>。
*   研究人员指出，在某些情况下，模型可能会 <strong>过早收敛到粗粒度证据</strong>，或者表现出 <strong>模态偏见</strong>（例如，过度依赖视觉信息而忽略音频），这会影响准确性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>训练一个全模态智能体模型：</strong> 研究人员设想未来可以训练一个能够自主调用工具的全模态智能体模型，该模型能够直接接收多种模态输入，并自主决定如何关注特定的音频或视频内容。
*   <strong>提高推理效率：</strong> 通过在 KV 缓存中显式保留记忆，可以解决推理延迟的瓶颈问题，从而提高效率。
*   <strong>集成更多模态特定工具：</strong> OmniAgent 的框架具有良好的可扩展性，可以集成更多模态的工具，例如图像 OCR 或传感器接口，以实现更全面的多模态感知。
*   <strong>更高效的改进：</strong> 尽管 OmniAgent 在理解方面表现出色，但研究人员表示未来将致力于开发更高效的改进方法。</p>
<p><strong>总结：</strong>
OmniAgent 论文提出了一种新颖的音频引导主动感知智能体，通过迭代的“思考-行动-观察-反思”循环和动态工具编排，有效解决了现有全模态大语言模型在细粒度跨模态理解和模态对齐方面的挑战。其核心的粗粒度到细粒度音频引导感知范式，以及精心设计的工具集，使其在多项音频-视频理解基准测试中取得了显著的性能提升，标志着音频-视频理解领域的一个重要进展。该研究为未来开发更强大、更高效的全模态智能体奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning.</li>
<li>Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning.</li>
<li>Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23646v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23646v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23635v1'></a></p>
<h2 id="rethinking-the-spatio-temporal-alignment-of-end-to-end-3d-perception"><a href="https://arxiv.org/abs/2512.23635v1">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</a></h2>
<p><strong>Authors:</strong> Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于自动驾驶领域中时空对齐（Spatio-Temporal Alignment, STA）的论文的全面摘要，重点关注其创新性、结果和潜在影响。</p>
<p><strong>论文题目：</strong> Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</p>
<p><strong>作者：</strong> Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>本文旨在解决当前端到端（E2E）3D感知系统中时空对齐（STA）的局限性。现有方法主要依赖于注意力机制和单一的显式物理运动模型（如恒定速度）来对齐跨帧的对象，这在面对复杂多变的运动状态和对象特征时表现不佳，导致对齐效果次优。研究的核心问题是：<strong>E2E感知系统如何才能在不继承现有STA模块脆弱性的前提下，整合其优势，实现更鲁棒和自适应的时空对齐？</strong></p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>作者提出了<strong>HAT（Multiple Hypotheses Spatio-Temporal Alignment）</strong>模块，这是一个即插即用的STA模块，其核心创新在于：</p>
<ul>
<li><strong>多假设生成与自适应解码：</strong> HAT不再依赖单一的运动假设，而是利用一个<strong>运动模型库（Motion Model Library, MML）</strong>，包含多种显式运动模型（如CV, STATIC, CA, CTRV, CTRA），为历史实例生成<strong>多个空间锚点（spatial anchors）和运动感知的特征提案（motion-aware feature proposals）</strong>。</li>
<li><strong>隐式-显式混合对齐：</strong> HAT通过一个<strong>自适应解码器</strong>，结合历史对象的查询（queries）中嵌入的语义和运动线索，从多个假设中<strong>自适应地解码出最优的对齐提案</strong>，而无需直接监督。</li>
<li><strong>特征-锚点混合（Feature-Anchor Mixing）：</strong> 最终的对齐提案通过特征解码、锚点解码和特征-锚点混合三个关键组件生成，将历史帧的运动信息与当前帧的语义信息有效融合。</li>
<li><strong>低集成开销与高泛化性：</strong> HAT被设计为即插即用，可以无缝集成到现有的基于查询的3D时空检测器、跟踪器以及E2E AD系统中，且对系统整体性能提升显著，而计算开销增加有限。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>性能提升显著：</strong> 在大规模nuScenes数据集上，HAT在多种3D时空检测器和跟踪器上均取得了持续的性能提升。<ul>
<li>与DETR3D检测器结合时，在测试集上实现了<strong>46.0%的AMOTA</strong>，达到当时最先进（state-of-the-art）的跟踪结果。</li>
<li>在面向对象的E2E AD方法（如SparseDrive）中，HAT显著提升了感知精度（<strong>+1.3% mAP, +3.1% AMOTA</strong>）并<strong>降低了32%的碰撞率</strong>。</li>
<li>在nuScenes-C（语义受损）基准测试中，HAT的运动建模增强能力使得感知和规划在语义退化的情况下更加鲁棒。</li>
</ul>
</li>
<li><strong>鲁棒性验证：</strong> HAT在语义受损场景下的表现尤为突出，证明了显式运动建模在E2E 3D感知中的关键作用，即使在语义信息不足时也能提供有效的先验。</li>
<li><strong>泛化能力：</strong> HAT在不同基线方法上的持续改进，以及在检测、跟踪和端到端AD任务中的广泛适用性，证明了其强大的跨任务泛化能力。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对仅依赖边界框表示的方法效果减弱：</strong> 论文提到，HAT的多假设解码机制依赖于查询中包含的<strong>时间上渐进的运动线索</strong>。对于那些仅将解码后的边界框作为实例表示的方法，HAT的有效性可能会受到限制（如表7所示）。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的运动模型融合：</strong> 虽然HAT已经集成了多种运动模型，但未来可以探索更复杂的模型融合策略，或者动态学习模型权重，以适应更极端或罕见的运动模式。</li>
<li><strong>端到端学习运动模型：</strong> 当前HAT利用的是预定义的显式运动模型。未来研究可以探索如何将运动模型的学习端到端地集成到整个感知框架中，使其能够从数据中自适应地学习更优的运动预测能力。</li>
<li><strong>多模态融合中的STA：</strong> 将HAT的STA思想扩展到融合激光雷达（LiDAR）、雷达等多种传感器数据的场景，以实现更全面的时空对齐。</li>
<li><strong>对特定类别或场景的自适应：</strong> 进一步研究如何使HAT能够根据不同的对象类别（如行人、车辆）或不同的驾驶场景（如城市道路、高速公路）自适应地调整其运动模型选择和解码策略。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文成功地提出了HAT模块，通过引入多假设生成和自适应解码机制，显著提升了E2E 3D感知系统中的时空对齐能力。HAT克服了传统方法对单一运动假设的依赖，并在各种下游任务中展现出强大的性能提升和鲁棒性，尤其是在语义信息受损的情况下。该工作强调了显式运动建模在现代E2E感知系统中的重要性，并为未来更鲁棒、更自适应的时空对齐研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision.</li>
<li>It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23635v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23635v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23628v1'></a></p>
<h2 id="memorization-in-3d-shape-generation-an-empirical-study"><a href="https://arxiv.org/abs/2512.23628v1">Memorization in 3D Shape Generation: An Empirical Study</a></h2>
<p><strong>Authors:</strong> Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>本研究首次系统性地量化了3D形状生成模型中的“记忆”现象，并深入探讨了数据和模型设计如何影响这种记忆。研究者提出了一种创新的评估框架，并基于此框架，揭示了数据多样性、条件粒度以及模型参数（如引导尺度、潜在向量集长度）对记忆程度的关键作用。最终，论文提出了一些简单有效的策略来减少记忆，同时不损害生成质量，为开发更具泛化性和隐私保护性的3D生成模型提供了实证依据。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>量化记忆的评估框架：</strong> 这是本研究的核心创新。论文设计了一个专门的框架来“量化”3D生成模型对训练数据的记忆程度。虽然摘要未详细说明具体方法，但可以推测该框架能够提供一个可量化的指标来衡量生成样本与训练样本之间的相似性或重叠度，从而区分真正的生成和简单的复制。</li>
<li><strong>系统性的实证研究：</strong> 研究者通过控制变量的实验，系统地研究了数据和模型设计对记忆的影响。这包括：<ul>
<li><strong>数据层面：</strong> 分析了数据模态、数据多样性以及条件粒度（finer-grained conditioning）的影响。</li>
<li><strong>模型层面：</strong> 重点研究了潜在向量集（Vecset）扩散模型，并考察了引导尺度（guidance scale）、Vecset长度以及简单的旋转增强（rotation augmentation）的作用。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升3D生成模型的可靠性与安全性：</strong> 明确量化和理解记忆现象，有助于研究者开发出更不容易“泄露”训练数据隐私的模型。这对于处理敏感3D数据（如医疗扫描、设计模型）尤为重要。</li>
<li><strong>促进3D生成模型的多样性：</strong> 记忆过度的模型往往会生成与训练数据高度相似的样本，限制了生成的多样性。通过理解和控制记忆，可以鼓励模型生成更具创造性和新颖性的3D形状。</li>
<li><strong>指导模型设计与数据准备：</strong> 研究结果为3D生成模型的开发者提供了宝贵的指导，例如如何选择合适的数据集、如何调整模型参数以平衡生成质量和记忆程度。</li>
<li><strong>为3D生成模型评估提供新视角：</strong> 除了传统的FID、IS等指标外，记忆度可以成为衡量3D生成模型性能的一个重要补充维度。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>3D内容创作与设计：</strong> 游戏开发、虚拟现实/增强现实（VR/AR）、产品设计、建筑可视化等领域，需要大量高质量且多样化的3D模型。本研究有助于生成更具原创性的内容。</li>
<li><strong>3D数据隐私与安全：</strong> 在医疗影像、工业设计等领域，训练数据的隐私至关重要。本研究的成果可以直接应用于开发更安全的3D数据生成和处理工具。</li>
<li><strong>机器人学与自动驾驶：</strong> 需要生成逼真的3D场景和物体模型用于训练和测试。减少记忆有助于生成更广泛的场景变化，提高模型的鲁棒性。</li>
<li><strong>3D形状检索与分析：</strong> 理解生成模型的记忆机制，也可能间接帮助理解和改进3D形状的表示和检索方法。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>评估框架的普适性：</strong> 摘要提到“设计了一个评估框架”，但并未详细说明其具体实现和是否适用于所有类型的3D生成模型（如NeRF、GANs等）。其有效性和鲁棒性有待进一步验证。</li>
<li><strong>模型范围的局限性：</strong> 研究主要集中在“潜在向量集（Vecset）扩散模型”上。虽然扩散模型是当前3D生成的热点，但研究结果是否能直接推广到其他类型的3D生成模型（如GANs、VAEs、NeRFs等）尚不明确。</li>
<li><strong>“简单旋转增强”的有效性：</strong> 摘要提到“简单的旋转增强”可以缓解记忆，但其具体实现方式（例如，是数据增强还是模型层面的处理）以及其在不同数据集和模型上的效果仍需深入研究。</li>
<li><strong>“不降代生成质量”的定义：</strong> 摘要声称“不降代生成质量”，但“生成质量”的定义和衡量标准在摘要中未明确。在实际应用中，减少记忆是否会以牺牲某些生成质量指标（如细节、真实感）为代价，需要更详细的实验数据支持。</li>
<li><strong>“数据多样性”和“细粒度条件”的影响机制：</strong> 虽然指出了这些因素的影响，但其背后的具体机制（例如，为什么多样性会增加记忆，细粒度条件如何诱导记忆）可能需要更深入的理论分析。</li>
</ul>
<p>总而言之，这篇论文通过引入量化记忆的评估框架，并进行系统性的实证研究，为理解和控制3D生成模型的记忆现象提供了重要的实证基础。其研究成果有望在提升3D生成模型的隐私性、多样性和可靠性方面发挥关键作用，对3D内容创作、数据安全等多个领域具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23628v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23628v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23592v1'></a></p>
<h2 id="same-or-not-enhancing-visual-perception-in-vision-language-models"><a href="https://arxiv.org/abs/2512.23592v1">Same or Not? Enhancing Visual Perception in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Same or Not? Enhancing Visual Perception in Vision-Language Models”的全面摘要：</p>
<p><strong>论文题目：</strong> Same or Not? Enhancing Visual Perception in Vision-Language Models</p>
<p><strong>作者：</strong> Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkixoari</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
当前主流的视觉语言模型（VLMs）在理解图像方面表现出色，但其感知能力仍然是粗粒度的，容易出现视觉偏差，并且会忽略图像中的细微视觉细节。现有的大规模训练语料库往往侧重于通用的图像识别任务（例如，“这是猫还是狗？”），而未能充分训练模型进行精细的、实例级别的视觉辨别。这导致VLMs在区分外观相似但实际上不同的物体实例时存在困难。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，该研究提出了两项主要贡献：</p>
<ul>
<li><strong>TWIN 数据集：</strong> 作者引入了一个名为 TWIN（TWo-image INstance comparisons）的大规模数据集，包含 561,000 个图像对查询。该数据集专门设计用于训练模型进行精细的视觉理解，要求模型判断两个视觉上相似的图像是否描绘的是同一个物体实例。TWIN 数据集涵盖了各种日常用品，并包含了多样的视角、背景和光照条件，以鼓励模型关注细微的视觉线索，如形状、纹理和局部几何特征。数据集的构建强调了“硬负样本”（hard negatives），即那些外观相似但实际上是不同物体的图像对，这对于提升模型的辨别能力至关重要。</li>
<li><strong>FGVQA 基准测试套件：</strong> 作者还提出了 FGVQA（Fine-grained Visual Question Answering）基准测试套件，包含 12,000 个查询。FGVQA 通过整合来自多个领域的精细识别和检索数据集（包括艺术品、动物、植物、地标和零售产品）来评估 VLM 的精细视觉问答能力。该套件旨在衡量模型在不同领域泛化精细视觉理解的能力。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
研究的主要结果表明：</p>
<ul>
<li><strong>TWIN 训练的显著提升：</strong> 将现有的 VLM（如 Qwen2.5-VL 和 InternVL3.5）在 TWIN 数据集上进行微调（post-training），可以显著提高它们在 FGVQA 基准测试上的性能，最高可提升 19.3%。这种提升在各种数据集上都得到了体现，甚至在 TWIN 数据集中未出现的领域（如艺术、动物、植物和地标）也表现出良好的泛化能力。</li>
<li><strong>对精细感知能力的增强：</strong> 通过对模型进行 TWIN 训练，其对细微视觉线索的关注度显著提高，从而增强了精细识别能力。这不仅体现在 FGVQA 上的提升，还体现在模型生成的解释性文本中，它们能更准确地捕捉到图像中的细微差异。</li>
<li><strong>不影响通用 VQA 性能：</strong> 重要的是，在 TWIN 上进行微调并没有损害模型在通用 VQA 基准测试上的性能，反而可能带来小幅提升，这表明 TWIN 训练能够增强模型整体的视觉理解能力，而非导致任务过拟合。</li>
<li><strong>规模的重要性：</strong> 研究表明，TWIN 数据集的规模对于提升模型性能至关重要。随着训练样本数量的增加，模型在 FGVQA 上的准确率也随之提高。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>模型在极端情况下的挑战：</strong> 尽管 TWIN 训练带来了显著提升，但模型在处理极端视角变化、不完整的物体视图以及细微的颜色差异时仍然会遇到挑战。
*   <strong>对“硬负样本”的依赖：</strong> 研究强调了“硬负样本”在 TWIN 数据集中的重要性，这表明收集和构建高质量的“硬负样本”可能是一个成本高昂且耗时的过程。
*   <strong>现有 VLM 的不足：</strong> FGVQA 基准测试的结果表明，即使是强大的开源 VLM 在精细视觉理解方面仍然存在显著差距，这表明该领域仍有很大的改进空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更强的奖励信号：</strong> 探索更复杂的奖励信号，例如使用多模态验证器来指导训练，可能有助于进一步提升模型的感知能力。
*   <strong>自动化“硬负样本”挖掘：</strong> 开发自动化技术来挖掘更具挑战性的“硬负样本”，例如利用“模型在循环”（model-in-the-loop）的数据引擎，可以提高数据集构建的效率。
*   <strong>整合 3D 表示：</strong> TWIN 数据集包含大量的视角变化，要求模型理解精细的局部几何特征。整合 3D 表示可能有助于模型更好地编码空间结构，从而提升在 TWIN 任务上的表现。
*   <strong>作为通用 VLM 训练语料库的补充：</strong> 作者设想 TWIN 可以作为开源 VLM 训练语料库的“即插即用”的补充，以推动未来模型感知精度的提升。</p>
<p>总而言之，这篇论文通过引入创新的 TWIN 数据集和 FGVQA 基准测试，有效地解决了当前 VLM 在精细视觉理解方面的不足。研究结果表明，专门针对实例级别辨别能力的训练能够显著提升模型的感知精度，并且这种提升具有良好的泛化性，为构建更强大的视觉语言模型提供了重要的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues.</li>
<li>To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23592v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23592v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23576v1'></a></p>
<h2 id="livetalk-real-time-multimodal-interactive-video-diffusion-via-improved-on-policy-distillation"><a href="https://arxiv.org/abs/2512.23576v1">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</a></h2>
<p><strong>Authors:</strong> Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</p>
<p><strong>作者：</strong> Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前视频生成模型在实现实时、多模态交互式应用方面存在的瓶颈。现有的扩散模型虽然能生成高质量视频，但其迭代式双向注意力机制导致高昂的计算成本和延迟，无法满足实时交互的需求。现有的蒸馏方法主要集中在文本到视频生成，在处理包含文本、图像和音频等多模态条件时，容易出现视觉伪影（如闪烁、黑帧、质量下降）等问题，导致人机交互不自然且效率低下。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了克服这些挑战，论文提出了一个改进的在线策略蒸馏（on-policy distillation）方法，专门针对多模态视频扩散模型。其核心贡献包括：</p>
<ul>
<li><strong>改进的蒸馏配方：</strong> 针对“Self Forcing”等现有在线策略蒸馏方法在多模态条件下遇到的稳定性问题，论文提出了一个改进的蒸馏策略，重点关注：<ul>
<li><strong>精炼多模态条件：</strong> 强调高质量的条件输入，例如使用高质量的参考图像和侧重于运动的文本提示。</li>
<li><strong>收敛的ODE初始化：</strong> 在应用在线DMD（Distribution Matching Distillation）训练之前，确保ODE初始化阶段充分收敛，为后续训练打下坚实基础。</li>
<li><strong>最大化有限学习窗口内的学习：</strong> 采用更积极的学习率策略和调整的分类器引导（CFG）尺度，以在模型性能衰退前最大化学习效果。</li>
</ul>
</li>
<li><strong>LiveTalk系统构建：</strong> 基于改进的蒸馏模型，论文构建了一个名为LiveTalk的实时多模态交互式虚拟形象系统。该系统集成了：<ul>
<li><strong>实时视频生成模型：</strong> 经过蒸馏的4步AR（Autoregressive）视频扩散模型，能够生成同步的视频响应。</li>
<li><strong>多模态条件输入：</strong> 接收用户输入的文本、参考图像以及由Qwen3-Omni生成的流式音频。</li>
<li><strong>Anchor-Heavy Identity Sinks (AHIS)：</strong> 一种训练无关的技术，用于在长视频流中保持说话人身份的一致性，防止视觉漂移。</li>
<li><strong>并行流水线：</strong> 采用流水线并行技术，实现视频去噪和解码的同步，以减少延迟并实现无缝实时渲染。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>显著的效率提升：</strong> 论文提出的蒸馏模型实现了20倍的推理成本和延迟降低，吞吐量达到24.82 FPS，首帧延迟降至亚秒级。
*   <strong>高质量的视频生成：</strong> 蒸馏后的模型在HDTF、AVSpeech和CelebV-HQ等基准测试中，达到了与同等或更大尺寸的双向、多步基线模型相当的视觉质量，甚至在某些方面有所超越。
*   <strong>优越的多轮交互性能：</strong> 在论文自建的多轮交互基准测试中，LiveTalk系统在多视频连贯性和内容质量方面显著优于Sora2和Veo3等最先进模型，将响应延迟从几分钟缩短到实时，实现了真正意义上的无缝人机多模态交互。
*   <strong>推动了实时交互式AI应用的发展：</strong> 该研究为构建更自然、更高效的多模态交互式AI系统（如虚拟形象、虚拟助手等）奠定了基础。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>多模态条件下的稳定性挑战：</strong> 尽管论文提出了改进方法，但多模态条件下的蒸馏过程仍然比文本到视频更具挑战性，需要精细的调优。
*   <strong>数据质量的重要性：</strong> 论文强调了高质量多模态条件的重要性，低质量的输入条件会导致训练不稳定和视觉质量下降。
*   <strong>有限的学习窗口：</strong> 多模态条件下的DMD训练存在一个相对较短的有效学习窗口，模型在达到峰值性能后容易出现性能衰退。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更广泛的多模态条件：</strong> 探索更多样化、更复杂的多模态条件（如动作捕捉、表情捕捉等）对蒸馏过程的影响。
*   <strong>更鲁棒的蒸馏方法：</strong> 进一步研究如何提高蒸馏过程在各种复杂多模态场景下的鲁棒性，减少对数据质量的依赖。
*   <strong>长时记忆和上下文理解：</strong> 进一步提升系统在长时对话中的记忆能力和上下文理解能力，实现更深层次的交互。
*   <strong>个性化和风格化生成：</strong> 探索如何通过更精细的控制，实现虚拟形象的个性化和风格化视频生成。</p>
<p>总而言之，这篇论文在实时多模态视频生成领域取得了重要进展，通过改进在线策略蒸馏方法，成功构建了一个高性能的LiveTalk系统，为实现更自然、更沉浸式的人机交互提供了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23576v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23576v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23568v1'></a></p>
<h2 id="thinkgen-generalized-thinking-for-visual-generation"><a href="https://arxiv.org/abs/2512.23568v1">ThinkGen: Generalized Thinking for Visual Generation</a></h2>
<p><strong>Authors:</strong> Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ThinkGen: Generalized Thinking for Visual Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> ThinkGen: Generalized Thinking for Visual Generation</p>
<p><strong>作者：</strong> Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei</p>
<p><strong>1. 研究问题与背景</strong></p>
<ul>
<li><strong>核心问题：</strong> 当前多模态大语言模型（MLLMs）在理解任务中展现出的链式思考（Chain-of-Thought, CoT）推理能力，在生成任务中的应用仍处于初步阶段，且现有机制往往针对特定场景，泛化性和适应性受限。这导致在各种视觉生成场景下，模型难以有效利用 CoT 推理来提升生成质量和指令遵循能力。</li>
<li><strong>背景：</strong> CoT 推理在数学、编程和视觉语言理解等领域取得了显著成功。研究人员正积极探索将其应用于生成任务，以期实现更系统化、更智能的生成过程。</li>
</ul>
<p><strong>2. 主要创新与方法贡献</strong></p>
<ul>
<li><strong>ThinkGen 框架：</strong> 论文提出了 ThinkGen，这是首个“思考驱动”的视觉生成框架，它显式地利用 MLLM 的 CoT 推理能力来处理多种生成场景。</li>
<li><strong>解耦架构：</strong> ThinkGen 采用解耦的架构，由一个预训练的 MLLM 和一个扩散 Transformer（DiT）组成。MLLM 负责根据用户意图生成定制化的指令，DiT 则根据这些指令生成高质量图像。</li>
<li><strong>VGI-refine 模块：</strong> 为了解决 MLLM CoT 输出中的冗余信息，引入了 Visual Generation Instruction Refinement (VGI-refine) 模块。该模块提取 MLLM 推理链中的关键指令信息，并与可学习的 Prepadding States 拼接，以自适应地调整 MLLM 的表示分布，使其更好地与 DiT 对齐。</li>
<li><strong>SepGRPO 训练范式：</strong> 论文提出了一种可分离的基于 GRPO（Proximal Policy Optimization）的训练范式（SepGRPO）。该范式交替地在 MLLM 和 DiT 模块之间进行强化学习，先冻结 DiT 优化 MLLM，再冻结 MLLM 优化 DiT。这种灵活的设计支持跨多种数据集进行联合训练，从而促进在广泛生成场景下的有效 CoT 推理。</li>
<li><strong>数据增强与伪标签：</strong> 针对现有生成数据集缺乏显式 <code>&lt;think&gt;</code> 标签的问题，论文开发了一种数据模板，从图像-文本对生成伪 CoT 注释，用于监督训练。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>性能提升：</strong> 大量实验表明，ThinkGen 在多个生成基准上取得了稳健的、最先进的性能。</li>
<li><strong>推理能力增强：</strong> 启用 CoT 推理后，ThinkGen 在推理基准上取得了显著的性能提升（例如，WISE: 0.55 → 0.76, RISEBench: 3.6 → 13.0）。</li>
<li><strong>泛化性：</strong> 通过多场景联合训练，ThinkGen 展现了在各种生成任务（包括文本到图像生成、图像编辑、文本渲染和推理生成）中的强大泛化能力。</li>
<li><strong>状态-艺术（State-of-the-art）：</strong> 在多个评估指标上，ThinkGen 达到了或超越了现有最先进水平。</li>
<li><strong>意义：</strong> ThinkGen 是一个重要的里程碑，它展示了如何将 MLLM 的 CoT 推理能力有效地集成到视觉生成流程中，为构建更智能、更通用的生成模型铺平了道路。</li>
</ul>
<p><strong>4. 局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 尽管采用了解耦设计，但训练过程仍然需要大量的计算资源。</li>
<li><strong>特定场景的优化：</strong> 虽然框架具有泛化性，但在某些非常细粒度的或高度专业的场景下，可能仍需要进一步的微调或定制。</li>
<li><strong>对 CoT 质量的依赖：</strong> 模型的性能在一定程度上依赖于 MLLM 生成的 CoT 推理的质量和准确性。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<ul>
<li><strong>更高效的训练策略：</strong> 探索更高效的训练方法，以降低计算成本，并加速模型的收敛。</li>
<li><strong>更精细的 CoT 控制：</strong> 研究如何更精细地控制 MLLM 生成的 CoT 推理过程，以应对更复杂的生成挑战。</li>
<li><strong>跨模态的深度融合：</strong> 进一步探索 MLLM 和视觉生成器之间的深度融合机制，以实现更紧密的协同工作。</li>
<li><strong>更广泛的应用场景：</strong> 将 ThinkGen 的思想扩展到其他模态的生成任务，如视频生成、3D 内容生成等。</li>
<li><strong>可解释性增强：</strong> 进一步研究 ThinkGen 的 CoT 推理过程的可解释性，以理解模型是如何做出决策的。</li>
</ul>
<p>总而言之，ThinkGen 论文提出了一种创新的“思考驱动”视觉生成框架，通过显式集成 MLLM 的 CoT 推理能力，显著提升了视觉生成任务的性能和泛化性，尤其是在需要复杂推理和指令遵循的场景下。该工作为未来更智能、更通用的生成模型的发展奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios.</li>
<li>Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23568v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23568v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-30 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
