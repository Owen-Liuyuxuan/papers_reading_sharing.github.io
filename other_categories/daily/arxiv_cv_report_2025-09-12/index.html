<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-12 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-11/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-15/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-12">Arxiv Computer Vision Papers - 2025-09-12</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-understanding-by-design-how-datasets-shape-architectures-and-insights" class="nav-link">Video Understanding by Design: How Datasets Shape Architectures and Insights</a>
                </li>
                <li class="nav-item">
                    <a href="#computational-imaging-for-enhanced-computer-vision" class="nav-link">Computational Imaging for Enhanced Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#a-structured-review-of-underwater-object-detection-challenges-and-solutions-from-traditional-to-large-vision-language-models" class="nav-link">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#flux-reason-6m-prism-bench-a-million-scale-text-to-image-reasoning-dataset-and-comprehensive-benchmark" class="nav-link">FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#spatialvid-a-large-scale-video-dataset-with-spatial-annotations" class="nav-link">SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-grounding-from-event-cameras" class="nav-link">Visual Grounding from Event Cameras</a>
                </li>
                <li class="nav-item">
                    <a href="#interact-advancing-large-scale-versatile-3d-human-object-interaction-generation" class="nav-link">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#semantic-concentration-for-self-supervised-dense-representations-learning" class="nav-link">Semantic Concentration for Self-Supervised Dense Representations Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-image-fusion-and-super-resolution" class="nav-link">FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#omnieva-embodied-versatile-planner-via-task-adaptive-3d-grounded-and-embodiment-aware-reasoning" class="nav-link">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-12">Arxiv Computer Vision Papers - 2025-09-12</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月11日Arxiv计算机视觉论文的执行摘要，旨在帮助忙碌的研究人员快速了解关键发展。</p>
<hr />
<p><strong>Arxiv 计算机视觉论文每日执行摘要 (2025-09-11)</strong></p>
<p><strong>概述：</strong>
今日的Arxiv计算机视觉论文展现了该领域在<strong>数据驱动的进步、多模态融合、具身智能以及对复杂场景理解</strong>方面的持续关注。主要趋势包括：构建更大、更精细标注的数据集以推动模型发展；利用新兴传感器（如事件相机）或计算成像技术来增强视觉感知；以及将视觉能力与推理、规划和交互相结合，以实现更高级的AI系统。</p>
<p><strong>主要主题和趋势：</strong></p>
<ol>
<li><strong>大规模数据集与基准的构建 (Data-Centric AI):</strong> 多篇论文致力于创建新的、大规模且具有复杂标注的数据集，以解决现有数据集的局限性，涵盖视频理解、文本到图像推理、空间视频标注和3D人机交互等领域。这强调了高质量数据在推动模型性能和泛化能力方面的重要性。</li>
<li><strong>多模态与跨领域融合 (Multimodal &amp; Cross-Domain Fusion):</strong> 论文探索了文本、图像、视频、3D信息以及事件数据等多种模态的融合。例如，文本到图像推理、多模态图像融合以及将3D信息融入具身智能规划。</li>
<li><strong>具身智能与交互 (Embodied AI &amp; Interaction):</strong> 具身智能和人机交互是显著的趋势，体现在3D人机交互生成和具身通用规划器等工作中，旨在让AI系统能更好地理解和操作物理世界。</li>
<li><strong>新兴传感器与计算成像 (Novel Sensors &amp; Computational Imaging):</strong> 事件相机和计算成像技术被提出作为增强传统视觉系统感知能力的新途径，尤其在低光、高速运动等挑战性场景下。</li>
<li><strong>复杂场景理解与推理 (Complex Scene Understanding &amp; Reasoning):</strong> 从水下目标检测的挑战到文本到图像的复杂推理，再到视频中的空间理解，研究人员正努力使模型能够处理更复杂、更具挑战性的视觉场景。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark" (Rongyao Fang et al.):</strong> 这篇论文通过构建一个百万级规模的文本到图像推理数据集和基准，直接解决了当前多模态模型在复杂推理能力上的瓶颈。其规模和复杂性有望显著推动多模态大模型的推理能力发展。</li>
<li><strong>"OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning" (Yuecheng Liu et al.):</strong> OmniEVA代表了具身智能领域的一个重要进展，它结合了3D感知、任务适应性和具身意识推理，旨在实现更通用和高效的具身规划。这对于构建能够与物理世界有效交互的机器人和AI系统至关重要。</li>
<li><strong>"Visual Grounding from Event Cameras" (Lingdong Kong et al.):</strong> 利用事件相机进行视觉定位是一个新颖且具有潜力的方向。事件相机在高速、高动态范围场景下的独特优势，结合视觉定位任务，有望在传统相机受限的场景中开辟新的应用。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>事件相机在高级视觉任务中的应用：</strong> 不仅仅是低级感知，事件相机正被探索用于更复杂的任务，如视觉定位。</li>
<li><strong>3D-Grounded Reasoning for Embodied AI：</strong> 将3D空间信息深度融入具身智能的规划和推理，以实现更精确和鲁棒的物理世界交互。</li>
<li><strong>大规模多模态推理数据集的系统性构建：</strong> 不仅仅是数据量，更强调数据中蕴含的复杂推理能力，以推动大模型的智能水平。</li>
<li><strong>语义集中（Semantic Concentration）的自监督学习：</strong> 探索新的自监督范式，以学习更具语义区分度的密集表示。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对多模态大模型和推理感兴趣：</strong><ul>
<li><strong>"FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark"</strong> (Rongyao Fang et al.) - 了解如何构建和评估复杂多模态推理能力。</li>
</ul>
</li>
<li><strong>对具身智能、机器人和3D交互感兴趣：</strong><ul>
<li><strong>"OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning"</strong> (Yuecheng Liu et al.) - 深入了解具身规划的最新进展。</li>
<li><strong>"InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation"</strong> (Sirui Xu et al.) - 关注3D人机交互的生成和理解。</li>
</ul>
</li>
<li><strong>对视频理解和数据集构建感兴趣：</strong><ul>
<li><strong>"Video Understanding by Design: How Datasets Shape Architectures and Insights"</strong> (Lei Wang et al.) - 提供关于数据集如何影响视频理解模型设计的深刻见解。</li>
<li><strong>"SpatialVID: A Large-Scale Video Dataset with Spatial Annotations"</strong> (Jiahao Wang et al.) - 了解新的视频数据集及其空间标注的价值。</li>
</ul>
</li>
<li><strong>对新兴传感器或计算成像感兴趣：</strong><ul>
<li><strong>"Visual Grounding from Event Cameras"</strong> (Lingdong Kong et al.) - 探索事件相机在高级视觉任务中的潜力。</li>
<li><strong>"Computational Imaging for Enhanced Computer Vision"</strong> (Humera Shaikh, Kaur Jashanpreet) - 了解计算成像如何赋能计算机视觉。</li>
</ul>
</li>
<li><strong>对自监督学习和表示学习感兴趣：</strong><ul>
<li><strong>"Semantic Concentration for Self-Supervised Dense Representations Learning"</strong> (Peisong Wen et al.) - 探索新的自监督学习范式。</li>
</ul>
</li>
</ul>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.09151v1">Video Understanding by Design: How Datasets Shape Architectures and Insights</a></li>
<li><a href="#2509.08712v1">Computational Imaging for Enhanced Computer Vision</a></li>
<li><a href="#2509.08490v1">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a></li>
<li><a href="#2509.09680v1">FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</a></li>
<li><a href="#2509.09676v1">SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</a></li>
<li><a href="#2509.09584v1">Visual Grounding from Event Cameras</a></li>
<li><a href="#2509.09555v1">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a></li>
<li><a href="#2509.09429v1">Semantic Concentration for Self-Supervised Dense Representations Learning</a></li>
<li><a href="#2509.09427v1">FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</a></li>
<li><a href="#2509.09332v1">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.09151v1'></a></p>
<h2 id="video-understanding-by-design-how-datasets-shape-architectures-and-insights"><a href="https://arxiv.org/abs/2509.09151v1">Video Understanding by Design: How Datasets Shape Architectures and Insights</a></h2>
<p><strong>Authors:</strong> Lei Wang, Piotr Koniusz, Yongsheng Gao</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Lei Wang, Piotr Koniusz, Yongsheng Gao撰写的论文“Video Understanding by Design: How Datasets Shape Architectures and Insights”的全面摘要。</p>
<hr />
<h3 id="video-understanding-by-design-how-datasets-shape-architectures-and-insights_1">论文摘要：Video Understanding by Design: How Datasets Shape Architectures and Insights</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的视频理解综述大多根据任务或模型家族对模型进行分类，忽略了数据集内在的“结构性压力”如何引导架构演进。这导致领域缺乏一个概念性地图，无法将过去的进展置于语境中，也无法预测未来的趋势。本文旨在解决这一问题，通过采用数据集驱动的视角，揭示数据集特性如何塑造模型架构和归纳偏置，从而推动通用视频理解的发展。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>数据集驱动的视角：</strong> 本文首次提出并系统性地采用数据集驱动的视角来审视视频理解领域，强调数据集的内在结构属性（如运动复杂度、时间跨度、层次化构成和多模态丰富性）如何施加归纳偏置，并指导模型架构设计。
*   <strong>里程碑式架构的重新解读：</strong> 论文将视频理解领域的里程碑式架构（从双流和3D CNN到序列模型、Transformer和多模态基础模型）重新解读为对这些数据集驱动压力的具体响应。
*   <strong>统一框架：</strong> 本文将数据集、归纳偏置和架构统一到一个连贯的框架中，提供了一个全面的回顾和前瞻性的路线图。
*   <strong>实用指导：</strong> 基于这种综合分析，论文为模型设计提供了实用指导，以在数据集不变性与可扩展性和任务需求之间取得平衡。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据集特性与架构演进的关联：</strong> 论文详细展示了不同数据集特性如何驱动特定架构的出现和发展。
    *   <strong>运动复杂度：</strong> 早期关注高幅度、全局运动的数据集（如UCF101）促成了双流CNN和3D CNN的发展。而现代数据集（如Diving48、FineGym）对细粒度、微小运动的识别需求，推动了多尺度3D卷积、时间Transformer和基于姿态的表示。
    *   <strong>时间跨度与层次化构成：</strong> 长时序、包含多步骤和并发动作的数据集（如Breakfast、Charades、EPIC-Kitchens）促使模型需要捕获长距离依赖、时间层次结构和关系推理，从而推动了序列模型、时间推理网络、Transformer和图模型的应用。
    *   <strong>多模态丰富性：</strong> 包含语言、音频等多种模态的数据集（如HowTo100M、Koala-36M）推动了跨模态对齐模块和大型视频-语言基础模型（VLMs）的兴起，以实现语义接地和跨模态推理。
*   <strong>性能趋势的解释：</strong> 论文通过分析不同模型在各项基准测试（如动作识别、时序定位、视频检索和问答）上的表现，验证了数据集驱动的归纳偏置与模型性能之间的强关联。例如，短片段数据集上3D CNN表现优异，而长时序、多模态数据集上Transformer和VLMs则更具优势。
*   <strong>通用视频理解的路线图：</strong> 论文提供了一个将数据集属性、架构原则和任务特定考量相结合的框架，以指导下一代视频理解模型的设计。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据集偏差：</strong> 现有数据集往往反映狭隘的文化或环境背景（如体育、厨房、脚本电影），导致模型可能利用这些先验知识而非获得鲁棒的时空推理能力，在真实世界部署中表现不佳。
*   <strong>标注成本和粒度：</strong> 细粒度时序标签、层次化任务分解和多模态对齐的获取成本高昂，导致许多数据集监督稀疏、时间密度有限或边界模糊。
*   <strong>生态有效性不足：</strong> 许多数据集缺乏生态有效性，剪辑的短片段和修剪过的动作边界捕获的是孤立的瞬间，而非日常活动中典型的连续、重叠和模糊的工作流。
*   <strong>评估碎片化：</strong> 跨数据集的评估指标和协议不一致，难以评估模型的泛化能力、组合推理、因果推断或对分布变化的鲁棒性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>数据集的演进：</strong> 未来的基准测试应超越静态、领域特定的语料库，拥抱多样性、生态有效性和可扩展性。应捕获连续、未修剪、多模态和多智能体活动，支持更高层次的推理任务（如预测、因果分析和交互式决策）。
*   <strong>架构的整合：</strong> 未来的架构应整合CNN的时间精度、程序所需的层次化构成、Transformer的长时序可扩展性以及多模态预训练和指令调优提供的语义接地能力。
*   <strong>时序-语义统一：</strong> 解决时序定位精确但缺乏开放词汇语义理解，以及语义能力强但边界定位差的模型之间的差距。
*   <strong>长时序组合推理：</strong> 结合检索增强的视频理解、将事件和步骤索引到持久记忆中，并通过语言进行查询。
*   <strong>评估实践的改进：</strong> 采用标准化的跨数据集协议，引入新的评估指标来衡量组合泛化、推理忠实度、噪声鲁棒性和计算效率。</p>
<hr />
<p>这篇论文通过其独特的数据集驱动视角，为视频理解领域提供了一个深刻的洞察，不仅系统地回顾了过去，更指明了未来研究和发展的方向，强调了数据集设计在推动通用、鲁棒和可扩展视频理解系统中的核心作用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands.</li>
<li>By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09151v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09151v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08712v1'></a></p>
<h2 id="computational-imaging-for-enhanced-computer-vision"><a href="https://arxiv.org/abs/2509.08712v1">Computational Imaging for Enhanced Computer Vision</a></h2>
<p><strong>Authors:</strong> Humera Shaikh, Kaur Jashanpreet</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents a comprehensive survey of computational imaging (CI)
techniques and their transformative impact on computer vision (CV)
applications. Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems. Computational imaging techniques, including light field imaging, high
dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare
mitigation, address these limitations by enhancing image acquisition and
reconstruction processes. This survey systematically explores the synergies
between CI techniques and core CV tasks, including object detection, depth
estimation, optical flow, face recognition, and keypoint detection. By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions. We emphasize the potential for
task-specific, adaptive imaging pipelines that improve robustness, accuracy,
and efficiency in real-world scenarios, such as autonomous navigation,
surveillance, augmented reality, and robotics.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Humera Shaikh和Kaur Jashanpreet撰写的论文“Computational Imaging for Enhanced Computer Vision”的全面摘要。</p>
<hr />
<p><strong>论文摘要：计算成像增强计算机视觉</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
传统成像方法在低光照、运动模糊或高动态范围等挑战性条件下，往往无法提供高保真视觉数据，从而限制了最先进计算机视觉（CV）系统的性能。本研究旨在全面探讨计算成像（CI）技术如何克服这些限制，并通过增强图像采集和重建过程，显著提升CV应用的鲁棒性、准确性和效率。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的核心创新在于系统性地调查和整合了多种计算成像技术与核心计算机视觉任务之间的协同作用。具体方法论贡献包括：
*   <strong>光场成像（Light Field Imaging）：</strong> 通过捕获光线的角度方向，提供多视角数据，显著增强深度估计、关键点检测和遮挡处理。
*   <strong>高动态范围（HDR）成像：</strong> 通过融合不同曝光水平的图像，捕捉场景中亮部和暗部的细节，克服传统传感器的动态范围限制，改善高对比度环境下的目标检测和人脸识别。
*   <strong>图像去模糊（Image Deblurring）：</strong> 通过建模和逆转模糊过程（如运动模糊和散焦模糊），恢复图像清晰度，提升运动估计和目标检测的准确性。
*   <strong>高速成像（High-Speed Imaging）：</strong> 提高时间分辨率，捕捉快速运动事件的精细细节，支持高速目标跟踪和运动分析。
*   <strong>眩光缓解（Glare Mitigation）：</strong> 利用偏振成像或多曝光方法抑制反射和强光源引起的眩光，确保在反射性或复杂光照环境下CV系统仍能保持高精度。</p>
<p>论文通过分析CI方法与目标检测、深度估计、光流、人脸识别和关键点检测等核心CV任务之间的关系，揭示了CI技术如何为这些任务提供更可靠的输入，从而显著提高其准确性和鲁棒性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>提升CV任务性能：</strong> CI技术为CV系统提供了更丰富、更准确的视觉数据。例如，光场成像通过提供3D信息增强了深度估计和遮挡处理；HDR成像在高对比度场景中改善了目标检测和人脸识别；去模糊技术提高了动态环境中目标检测和光流的准确性；高速成像实现了对快速移动物体的精确跟踪；眩光缓解确保了在不利光照条件下CV系统的可靠性。
*   <strong>跨任务协同效应：</strong> 论文强调了CI技术在不同CV任务之间产生的协同效应，例如，光场成像不仅增强了深度估计，还间接改善了目标检测和关键点匹配。
*   <strong>实际应用潜力：</strong> 这些增强功能对于自动驾驶、监控、增强现实和机器人等现实世界应用至关重要，因为它们需要高鲁棒性、准确性和效率。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>计算复杂性：</strong> 许多CI方法，特别是实时应用，涉及大量数据和高计算需求，需要强大的处理能力。
*   <strong>硬件与软件的权衡：</strong> CI技术依赖于专业光学设计和传感器架构，需要在物理复杂性、成本和可扩展性之间取得平衡。
*   <strong>数据质量和泛化性：</strong> 尽管CI技术提高了数据保真度，但在稀疏角度采样（光场成像）或快速运动场景（HDR成像）中仍可能引入伪影，需要更鲁棒的算法来处理不完善或不完整的数据。
*   <strong>互操作性：</strong> 确保新兴CI硬件与现有CV框架之间的互操作性，需要标准化管道和数据集。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>任务特定、自适应成像管道：</strong> 开发能够根据特定CV任务需求动态调整图像采集过程的CI系统，例如，光场相机根据场景深度复杂性调整角度采样，或高速相机优先处理快速运动区域。
*   <strong>深度学习与数据驱动方法：</strong> 利用深度学习进一步增强CI管道，通过学习原始传感器数据与任务特定输出之间的端到端映射，实现更快、更准确、更灵活的成像解决方案。
*   <strong>多模态成像系统集成：</strong> 结合空间、角度、光谱和时间成像方法，以创建更丰富的场景表示，使CV系统在复杂环境中表现更鲁棒，例如，将光谱成像与光场技术结合以改善材料分类。
*   <strong>边缘计算部署：</strong> 开发高效算法和硬件加速器（如GPU和专用神经网络处理器），以在资源受限的边缘设备和移动平台上实现CI增强型CV系统的实时部署。</p>
<hr />
<p>这篇论文全面概述了计算成像如何通过解决传统成像的固有局限性来彻底改变计算机视觉领域。它不仅详细介绍了各种CI技术及其对CV任务的贡献，还深入探讨了这些技术之间的协同作用、面临的挑战以及未来的研究方向，为该领域的进一步发展提供了宝贵的路线图。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems.</li>
<li>By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08490v1'></a></p>
<h2 id="a-structured-review-of-underwater-object-detection-challenges-and-solutions-from-traditional-to-large-vision-language-models"><a href="https://arxiv.org/abs/2509.08490v1">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a></h2>
<p><strong>Authors:</strong> Edwine Nabahirwa, Wei Song, Minghua Zhang, Yi Fang, Zhou Ni</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Underwater object detection (UOD) is vital to diverse marine applications,
including oceanographic research, underwater robotics, and marine conservation.
However, UOD faces numerous challenges that compromise its performance. Over
the years, various methods have been proposed to address these issues, but they
often fail to fully capture the complexities of underwater environments. This
review systematically categorizes UOD challenges into five key areas: Image
quality degradation, target-related issues, data-related challenges,
computational and processing constraints, and limitations in detection
methodologies. To address these challenges, we analyze the progression from
traditional image processing and object detection techniques to modern
approaches. Additionally, we explore the potential of large vision-language
models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated
in other domains. We also present case studies, including synthetic dataset
generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review
identifies three key insights: (i) Current UOD methods are insufficient to
fully address challenges like image degradation and small object detection in
dynamic underwater environments. (ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability. (iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Edwine Nabahirwa等人的论文“A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models”的全面摘要。</p>
<p><strong>论文摘要：水下目标检测的挑战与大型视觉语言模型的潜力</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在系统性地回顾和分析水下目标检测（UOD）所面临的诸多挑战，并探讨从传统方法到现代深度学习技术，特别是大型视觉语言模型（LVLMs）在解决这些问题上的进展和潜力。UOD对于海洋研究、水下机器人和海洋保护等应用至关重要，但水下环境的复杂性严重影响了其性能。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>系统性挑战分类：</strong> 论文将UOD挑战系统地归纳为五个关键领域：图像质量退化、目标相关问题、数据相关挑战、计算和处理限制以及检测方法论的局限性。
*   <strong>技术发展路线分析：</strong> 论文分析了UOD技术从传统图像处理和目标检测方法到现代深度学习方法的演进过程。
*   <strong>LVLMs在UOD中的潜力探索：</strong> 首次深入探讨了LVLMs在UOD领域的应用潜力，利用其在其他领域展示的多模态能力来解决UOD的固有复杂性。
*   <strong>案例研究：</strong> 提供了两个具体的案例研究：
    *   使用DALL-E 3进行合成数据集生成，以解决数据稀缺和多样性问题。
    *   使用LoRA技术对Florence-2 LVLM进行微调，以适应UOD任务。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>合成数据增强的有效性：</strong> 案例研究表明，通过DALL-E 3生成的合成数据（经过OpenCV增强以增加真实感）与真实数据集结合，可以略微提升YOLO11模型在UOD任务上的性能（mAP@50从0.793提升到0.796，mAP@50-95从0.501提升到0.505），并提高了召回率（从0.714提升到0.736）。这表明LVLMs在数据增强方面具有潜力，尤其是在解决数据稀缺和类不平衡问题上。
*   <strong>LVLMs在UOD中的定位能力：</strong> 对Florence-2 LVLM进行微调的案例研究显示，该模型在定位小型水下目标方面表现出强大的能力，尤其是在具有挑战性的类别（如海星和海胆）上。这突显了LVLMs在处理复杂水下环境中的小目标和遮挡目标方面的潜力。
*   <strong>LVLMs的局限性：</strong> 微调后的Florence-2模型存在“类名幻觉”问题（生成拼写错误或不正确的类名），这严重影响了评估指标的计算。此外，模型还表现出“灾难性遗忘”现象，即在适应特定领域任务时难以保留其更广泛的预训练知识。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>现有UOD方法的不足：</strong> 论文指出，当前的UOD方法仍不足以完全解决动态水下环境中图像退化和小目标检测等挑战。
*   <strong>合成数据的真实性和适用性：</strong> 尽管LVLMs生成的合成数据具有潜力，但仍需进一步完善，以确保其真实性和在复杂水下场景中的适用性（例如，缺乏自然缺陷、光照变化和不可预测的遮挡）。
*   <strong>LVLMs的实时应用：</strong> LVLMs在UOD中的实时应用仍未得到充分探索，需要进一步研究优化技术以满足计算资源受限的水下平台的需求。
*   <strong>LVLMs的幻觉和泛化问题：</strong> LVLMs在微调后可能出现类名幻觉和灾难性遗忘，这限制了其在实际应用中的准确性和可靠性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>高效微调技术：</strong> 进一步研究适配器微调和提示微调等参数高效微调技术，以更好地适应LVLMs在低光照、散射或颜色失真等复杂水下条件下的应用，同时最小化计算开销。
*   <strong>更真实的合成数据生成：</strong> 探索将扩散模型与其他生成方法（如VAE和GAN）结合的混合方法，以生成更逼真、更具多样性的水下图像，并解决水下特定合成的挑战。
*   <strong>自动化数据集标注：</strong> 利用Label-driven Automated Prompt Tuning (LAPT)等框架，通过自动化提示工程和图像合成/检索方法，减少水下图像的手动标注工作。
*   <strong>轻量级实时处理架构：</strong> 针对AUVs和实时监测系统，开发和优化轻量级LVLMs架构，例如采用模型剪枝和Transformer压缩技术，以实现实时检测。
*   <strong>解决LVLMs幻觉和泛化问题：</strong> 通过精心设计的提示工程、使用更大规模和更多样化的训练数据、指令微调以及改进的数据增强策略来减轻LVLMs的幻觉问题，并提高其泛化能力。</p>
<p>总而言之，这篇论文为水下目标检测领域提供了一个全面的视角，不仅系统地梳理了现有挑战，还前瞻性地探讨了大型视觉语言模型在解决这些挑战中的巨大潜力，并指出了未来研究的关键方向，为该领域的发展提供了宝贵的指导。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>(ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability.</li>
<li>(iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08490v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08490v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09680v1'></a></p>
<h2 id="flux-reason-6m-prism-bench-a-million-scale-text-to-image-reasoning-dataset-and-comprehensive-benchmark"><a href="https://arxiv.org/abs/2509.09680v1">FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</a></h2>
<p><strong>Authors:</strong> Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Rongyao Fang等人撰写的论文“FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark”的全面摘要：</p>
<p><strong>论文摘要：FLUX-Reason-6M &amp; PRISM-Bench</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
当前开源文本到图像（T2I）模型的发展受到限制，主要原因在于缺乏大规模、以推理为重点的数据集和全面的评估基准。这导致开源模型与领先的闭源系统之间存在显著的性能差距，尤其是在处理复杂推理任务时。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，论文提出了两项核心贡献：
*   <strong>FLUX-Reason-6M 数据集：</strong> 这是一个大规模的合成数据集，包含600万张高质量的FLUX生成图像和2000万条双语（英语和中文）描述。该数据集专门设计用于教授复杂的推理能力，并根据六个关键特征（想象力、实体、文本渲染、风格、情感和构图）进行组织。此外，它引入了明确的“生成思维链”（Generation Chain-of-Thought, GCoT），详细分解了图像生成步骤，为模型提供了强大的中间监督信号。整个数据整理耗时15,000 A100 GPU天，是目前最昂贵的开源数据集之一。
*   <strong>PRISM-Bench 基准：</strong> 这是一个新颖的评估标准，具有七个独立的评估赛道，包括FLUX-Reason-6M的六个特征类别以及一个极具挑战性的“长文本”赛道（利用GCoT提示）。该基准通过精心设计的提示，利用先进的视觉-语言模型（如GPT-4.1和Qwen2.5-VL-72B）对提示-图像对齐和图像美学进行细致的、与人类判断一致的评估。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文对19个领先的T2I模型在PRISM-Bench上进行了广泛评估，结果揭示了关键的性能差距，并突出了需要改进的具体领域：
*   <strong>闭源模型表现优异：</strong> 领先的闭源系统（如GPT-Image-1和Gemini2.5-Flash-Image）在整体性能上表现出色，尤其是在想象力、实体、风格和情感等类别中。GPT-Image-1在风格和构图方面表现最佳，Gemini2.5-Flash-Image在想象力和情感方面领先。
*   <strong>文本渲染和长文本挑战：</strong> 文本渲染仍然是所有T2I模型面临的重大挑战，该类别在所有赛道中得分最低。长文本赛道也显示出所有模型普遍较低的得分，这表明模型在遵循复杂、多层次指令方面存在显著改进空间。
*   <strong>开源模型进步显著：</strong> 尽管与顶尖闭源模型仍有差距，但Qwen-Image、SEEDream 3.0以及FLUX系列等开源模型也展现出强大的竞争力，尤其是在中文文本渲染方面，SEEDream 3.0和GPT-Image-1取得了最高分，表明在处理中文排版方面取得了显著进展。
*   <strong>推理能力差距：</strong> 评估结果强调了FLUX-Reason-6M旨在解决的推理差距问题，即模型在理解和合成复杂场景逻辑方面的不足。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   尽管闭源模型表现出色，但在某些维度上仍有改进空间。
*   文本渲染和长文本处理仍然是所有模型的共同弱点，表明这些任务的复杂性远超现有模型的处理能力。
*   现有数据集缺乏结构化信号来教授模型复杂的推理能力，且评估基准缺乏细粒度，无法有效区分最先进模型的实际性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   利用FLUX-Reason-6M数据集训练下一代以推理为导向的T2I模型，以弥合当前模型在复杂推理任务上的性能差距。
*   进一步研究和改进文本渲染和长文本处理能力，开发更有效的架构和训练方法。
*   利用PRISM-Bench作为可靠的评估工具，持续测量和比较T2I模型的真实能力，推动领域发展。
*   通过公开数据集、基准和评估代码，降低研究门槛，促进全球研究人员共同开发更智能、更强大的生成模型。</p>
<p>总而言之，这篇论文通过引入FLUX-Reason-6M数据集和PRISM-Bench基准，为解决T2I模型在推理能力和评估方面的核心问题提供了全面的解决方案，并为未来推理导向的T2I生成研究奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).</li>
<li>PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09680v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09680v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09676v1'></a></p>
<h2 id="spatialvid-a-large-scale-video-dataset-with-spatial-annotations"><a href="https://arxiv.org/abs/2509.09676v1">SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</a></h2>
<p><strong>Authors:</strong> Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, Xiaoxiao Long, Hao Zhu, Zhaoxiang Zhang, Xun Cao, Yao Yao</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Jiahao Wang等人撰写的论文“SpatialVID: A Large-Scale Video Dataset with Spatial Annotations”的全面摘要。</p>
<hr />
<p><strong>论文摘要：SpatialVID: 一个带有空间标注的大规模视频数据集</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
当前空间智能领域（包括空间重建和世界探索）的模型在可扩展性和真实世界保真度方面受到高质量、大规模训练数据稀缺的严重限制。现有数据集虽然提供相机姿态信息，但在规模、多样性和标注丰富性上不足，尤其缺乏真实世界动态场景的地面真实相机运动数据。这阻碍了模型泛化能力和性能的提升。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
为了解决上述问题，作者提出了 <strong>SpatialVID</strong>，一个大规模的野外视频数据集，其主要创新和方法学贡献包括：
*   <strong>大规模数据收集与处理：</strong> 收集了超过21,000小时的原始视频，并通过分层过滤管道处理成270万个视频片段，总计7,089小时的动态内容。
*   <strong>全面的几何标注：</strong> 为每个视频片段提供了逐帧的相机姿态、深度图和动态掩码等密集的3D标注。这些标注通过调整后的MegaSaM管道生成，确保了高精度和鲁棒性。
*   <strong>空间感知字幕和运动指令：</strong> 数据集通过一个创新的标注管道，将视觉语言模型（VLLMs）与大型语言模型（LLMs）结合，生成结构化字幕，整合了场景描述、相机运动细节以及天气、光照、时间等分层语义属性。此外，还从相机轨迹中提取了序列化的运动指令，为导航相关模型训练提供精确监督。
*   <strong>高质量子集（SpatialVID-HQ）：</strong> 通过进一步的过滤和采样，创建了一个1,146小时的平衡子集SpatialVID-HQ，优化了模型训练和评估的鲁棒性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据丰富性与多样性：</strong> 对SpatialVID数据统计的分析表明，该数据集在场景类型、相机运动模式、语义属性等方面具有显著的丰富性和多样性。例如，相机运动方向分布更均衡，轨迹转弯类型更多样。
*   <strong>超越现有数据集：</strong> 与Panda-70M等现有数据集的对比显示，SpatialVID在视频质量（美学、亮度、运动）和相机运动统计（旋转角度、移动距离、轨迹转弯）方面表现出更高的质量和更均衡的分布，尤其在动态内容和几何标注方面显著优越。
*   <strong>促进模型泛化：</strong> SpatialVID的丰富性和多样性直接促进了模型泛化能力和性能的提升，使其成为视频和3D视觉研究社区的关键资产。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>MegaSaM的局限性：</strong> 尽管MegaSaM具有先进的能力，但在极端情况下（如移动物体主导视野或相机与物体运动共线）仍存在局限性。它也未被设计用于处理可变焦距或显著径向畸变，并且其性能受外部单目深度模型的影响。
*   <strong>VLLMs在空间信息捕获上的局限性：</strong> 原始的视觉语言模型在捕获空间信息方面存在局限性，经常遗漏或错误表示几何细节。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>物理接地相机控制视频生成：</strong> SpatialVID通过引入更强的3D归纳偏差，为物理接地相机控制视频生成、动态场景合成和具身智能体等未来研究奠定了基础。
*   <strong>复杂交互模拟：</strong> 通过将显式3D运动控制与大规模文本语义相结合，SpatialVID为未来能够模拟真实世界场景中复杂交互的工作提供了基础。
*   <strong>集成更先进的相机姿态估计器：</strong> 论文提到，未来的相机姿态估计器（如ViPE）的集成将进一步提升标注管道的性能。</p>
<hr />
<p>总而言之，SpatialVID数据集通过提供大规模、高质量的野外视频，并辅以密集的逐帧几何（相机姿态、深度图、动态掩码）和语义（结构化字幕、运动指令）标注，有效弥补了现有数据集在规模、多样性和空间信息丰富性上的不足。这对于推动空间智能、3D重建、视频生成和世界模型等领域的发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions.</li>
<li>Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09676v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09676v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09584v1'></a></p>
<h2 id="visual-grounding-from-event-cameras"><a href="https://arxiv.org/abs/2509.09584v1">Visual Grounding from Event Cameras</a></h2>
<p><strong>Authors:</strong> Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Lingdong Kong等人撰写的论文“Visual Grounding from Event Cameras”的全面摘要：</p>
<p><strong>论文题目：</strong> Visual Grounding from Event Cameras
<strong>作者：</strong> Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau</p>
<p><strong>摘要：</strong></p>
<ol>
<li>
<p><strong>主要问题或研究问题：</strong>
    尽管事件相机在处理动态场景、运动模糊和挑战性光照方面具有显著优势，但其与自然语言理解的结合却鲜有研究，导致多模态感知领域存在空白。本文旨在解决这一问题，即如何利用自然语言描述，在事件相机捕获的动态场景中准确地定位（“视觉接地”）物体。</p>
</li>
<li>
<p><strong>关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>引入Talk2Event数据集：</strong> 论文首次提出了一个大规模的、基于事件数据的语言驱动物体视觉接地基准数据集——Talk2Event。该数据集基于真实的驾驶场景，包含5,567个场景、13,458个标注物体和超过30,000个经过仔细验证的指代表达式。</li>
<li><strong>属性中心的设计：</strong> Talk2Event数据集的每个指代表达式都通过四个结构化属性进行丰富：<ul>
<li><strong>外观 (Appearance)：</strong> 捕捉静态场景和物体特性（如类别、形状、大小、颜色）。</li>
<li><strong>状态 (Status)：</strong> 捕捉动态方面（如物体是否移动、停止、转弯或穿过）。</li>
<li><strong>与观察者的关系 (Relation-to-Viewer)：</strong> 捕捉物体相对于观察者的自我中心位置（如前方、左侧、远处或面向自车）。</li>
<li><strong>与周围物体的关系 (Relation-to-Others)：</strong> 捕捉物体与周围物体之间的上下文关系（如空间布局或群体行为）。</li>
</ul>
</li>
<li><strong>多模态数据整合：</strong> Talk2Event不仅包含事件流数据，还同步提供了RGB帧，支持三种互补的评估设置：仅使用事件体素、仅使用伴随帧以及结合两种数据源进行多模态接地。</li>
<li><strong>数据标注流程：</strong> 论文设计了一个上下文感知的提示策略，结合LLM辅助解析和人工验证，生成并精炼了语言丰富的描述，确保了标注的准确性和多样性。</li>
</ul>
</li>
<li>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>实现可解释和组合式接地：</strong> 这种属性中心的设计使得视觉接地任务不仅限于简单的物体识别，而是能够进行更深层次的上下文推理，从而在动态环境中实现可解释和组合式的物体定位。</li>
<li><strong>填补多模态感知空白：</strong> Talk2Event数据集的引入，首次将事件相机数据与自然语言理解相结合，为研究多模态、时间感知型感知提供了一个独特的平台。</li>
<li><strong>推动事件视觉研究：</strong> 该数据集为未来在机器人、人机交互等应用中，利用事件相机数据进行更精细、更鲁棒的物体接地研究奠定了基础，尤其是在运动模糊和低光照等传统相机难以应对的场景中。</li>
</ul>
</li>
<li>
<p><strong>局限性：</strong></p>
<ul>
<li>论文中并未明确提及Talk2Event数据集本身的局限性。然而，作为首个此类数据集，可能存在的潜在局限包括：<ul>
<li><strong>数据规模：</strong> 尽管被描述为“大规模”，但与一些成熟的RGB或视频数据集相比，其规模可能仍有提升空间。</li>
<li><strong>场景多样性：</strong> 数据集主要基于“真实世界驾驶场景”，这可能限制了其在其他非驾驶场景（如室内、工业等）的泛化能力。</li>
<li><strong>标注复杂性：</strong> 属性中心的设计虽然提供了丰富的信息，但也增加了标注的复杂性，可能对未来扩展数据集带来挑战。</li>
<li><strong>事件数据表示：</strong> 将异步事件流离散化为体素化4D张量，虽然兼容现有骨干网络，但可能损失部分原始事件数据的精细时间分辨率。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>多模态融合策略：</strong> 探索更有效的事件数据和RGB帧融合方法，以充分利用两种模态的优势。</li>
<li><strong>时间感知型模型：</strong> 开发专门针对事件数据时间特性设计的模型，以更好地捕捉动态物体的运动和状态变化。</li>
<li><strong>上下文推理：</strong> 利用数据集丰富的属性标注，研究更高级的上下文推理机制，实现超越简单物体识别的复杂场景理解。</li>
<li><strong>鲁棒性分析：</strong> 在运动模糊、低光照等挑战性条件下，对基于事件的视觉接地模型的鲁棒性进行深入研究。</li>
<li><strong>开放词汇接地：</strong> 结合大型语言模型，探索事件相机数据的开放词汇视觉接地能力。</li>
<li><strong>应用拓展：</strong> 将Talk2Event的成果应用于机器人导航、人机交互、自动驾驶等实际场景。</li>
</ul>
</li>
</ol>
<p>总而言之，这篇论文通过引入Talk2Event数据集，成功地弥合了事件相机感知与自然语言理解之间的鸿沟，为计算机视觉领域在动态、多模态和时间感知型物体接地方面开辟了新的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09584v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09584v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09555v1'></a></p>
<h2 id="interact-advancing-large-scale-versatile-3d-human-object-interaction-generation"><a href="https://arxiv.org/abs/2509.09555v1">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a></h2>
<p><strong>Authors:</strong> Sirui Xu, Dongting Li, Yucheng Zhang, Xiyan Xu, Qi Long, Ziyin Wang, Yunzhi Lu, Shuchang Dong, Hezi Jiang, Akshat Gupta, Yu-Xiong Wang, Liang-Yan Gui</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Sirui Xu等人撰写的论文“InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation”的全面摘要。</p>
<hr />
<h3 id="interact-3d-">论文摘要：InterAct: 推进大规模多功能3D人-物交互生成</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前3D人-物交互（HOI）生成领域面临的核心挑战。尽管大规模人体动作捕捉数据集已显著推动了人体动作生成的发展，但对动态3D HOI的建模和生成仍然充满挑战。现有数据集普遍存在局限性，包括缺乏广泛、高质量的动作和标注，以及存在接触穿透、浮空和不正确手部动作等伪影，这些问题严重阻碍了真实感HOI的生成。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
InterAct通过以下三个主要创新点来解决上述问题：</p>
<ul>
<li>
<p><strong>大规模、高质量数据集构建：</strong></p>
<ul>
<li><strong>数据整合与标准化：</strong> 论文首先整合并标准化了来自不同来源的21.81小时HOI数据，并辅以详细的文本标注，显著提升了数据集的规模和丰富度。</li>
<li><strong>统一优化框架提升数据质量：</strong> 提出了一种统一的优化框架，通过减少伪影和校正手部动作来提高数据质量。该框架分三个顺序步骤：全身校正、手部校正和交互增强，旨在解决穿透、浮空和不自然手部姿态等问题。</li>
<li><strong>接触不变性原理的数据增强：</strong> 利用接触不变性原理（受动作镜像技术启发），通过在保持物体接触一致性的同时引入人体动作变化，生成了逼真的合成数据。这使得数据集扩展到30.70小时，并显著提升了生成模型的性能。</li>
</ul>
</li>
<li>
<p><strong>统一的HOI生成建模视角与基准任务：</strong></p>
<ul>
<li>定义了六个关键的HOI生成基准任务：文本到交互、动作到交互、物体到人、人到物体、交互预测和交互模仿。</li>
<li>提出了一种统一的建模和表示方法，用于运动生成任务，并利用多任务学习联合建模运动和接触，实现了最先进的性能。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据质量显著提升：</strong> 广泛的实验验证了InterAct数据集作为推进3D HOI生成的基础资源的实用性。优化框架显著改善了原始MoCap数据的质量，减少了穿透伪影，并增强了人-物接触。
*   <strong>合成数据的高质量：</strong> 增强后的合成数据质量与校正后的数据相当，且优于原始数据集，表明了数据增强方法的有效性。
*   <strong>最先进的性能：</strong> 在六个HOI生成基准任务上，InterAct实现了最先进的性能，尤其是在文本条件交互生成任务中，通过引入接触建模和BPS编码，显著提高了生成HOI的质量和FID分数。
*   <strong>标记点表示的有效性：</strong> 实验表明，基于标记点的表示方法在减少伪影方面优于其他人体表示方法。
*   <strong>多任务学习的优势：</strong> 统一的多任务学习方法显著提升了模型性能，尤其是在人到物体生成和交互预测任务中。
*   <strong>大规模数据的益处：</strong> 实验结果证实，更大规模的数据集（InterAct-X）支持训练模型实现更好的性能，克服了有限数据导致的过拟合问题。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>规模限制：</strong> 尽管InterAct数据集将物体数量扩展到217个，远超现有HOI数据集，但仍存在规模限制，未能完全覆盖现实世界中遇到的各种物体类别。
*   <strong>泛化能力：</strong> 尽管模型对数据集范围内的分布外物体表现出泛化能力，但实现对更广泛的未见物体的鲁棒泛化仍需进一步扩展。
*   <strong>去噪和校正的挑战：</strong> 在处理原始数据中存在的显著噪声时，去噪和校正全身HOI数据仍面临固有挑战。例如，初始数据中的浮空物体问题可能难以完全校正，尤其是在统一超参数配置下。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>数据集扩展：</strong> 未来工作需要进一步扩展数据集，以覆盖更广泛的物体类别和更丰富的现实世界交互，从而提升模型的泛化能力。
*   <strong>优化框架改进：</strong> 改进优化框架，使其能够更有效地处理原始数据中的极端噪声和伪影，例如通过自适应超参数配置来解决特定问题。
*   <strong>隐私和伦理考量：</strong> 尽管论文已采取措施（如将数据转换为SMPL或标记点表示）来减少识别细节，但未来研究仍需持续关注数据收集和生成中的隐私和伦理问题，确保数据使用符合道德规范。</p>
<hr />
<p>总而言之，InterAct论文通过构建一个大规模、高质量、多功能的3D HOI基准数据集，并提出创新的数据优化和增强方法，以及统一的生成建模视角，为3D人-物交互生成领域做出了重要贡献。该工作不仅解决了现有数据集的局限性，还为未来的研究奠定了坚实的基础，有望推动机器人、动画和计算机视觉等领域的进一步发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements.</li>
<li>Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions.</li>
<li>Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09555v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09555v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09429v1'></a></p>
<h2 id="semantic-concentration-for-self-supervised-dense-representations-learning"><a href="https://arxiv.org/abs/2509.09429v1">Semantic Concentration for Self-Supervised Dense Representations Learning</a></h2>
<p><strong>Authors:</strong> Peisong Wen, Qianqian Xu, Siran Dai, Runmin Cong, Qingming Huang</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Peisong Wen等人撰写的论文“Semantic Concentration for Self-Supervised Dense Representations Learning”的全面摘要。</p>
<hr />
<h3 id="semantic-concentration-for-self-supervised-dense-representations-learning_1">论文摘要：Semantic Concentration for Self-Supervised Dense Representations Learning</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决自监督学习（SSL）在学习<strong>密集表示（dense representations）</strong>时面临的挑战，特别是针对图像中<strong>补丁（patches）</strong>的表示学习。尽管图像级SSL取得了显著进展，但在密集任务（如语义分割和目标检测）中，现有方法存在一个<strong>“过分散（over-dispersion）”现象</strong>：来自同一实例/类别的补丁在嵌入空间中分散，从而损害了下游密集任务的性能。论文的核心问题是：如何为密集SSL引入<strong>语义集中（semantic concentration）</strong>机制，以克服过分散问题并提高表示的细粒度对齐能力？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了一个基于自蒸馏（self-distillation）的密集SSL框架，其主要创新点包括：</p>
<ul>
<li><strong>揭示图像级SSL的隐式语义集中机制：</strong> 作者分析了图像级SSL如何通过“非严格空间对齐”（确保实例内一致性）和“共享模式”（确保图像间一致性）来避免过分散，从而实现隐式语义集中。</li>
<li><strong>引入补丁对应蒸馏（Patch Correspondence Distillation）与噪声容忍排序损失（Noise-Tolerant Ranking Loss）：</strong> 为了打破密集SSL中严格的空间对齐限制，论文提出蒸馏图像对之间的补丁对应关系。针对伪标签的噪声和不平衡问题，作者将传统的Average Precision (AP) 损失扩展到<strong>连续目标平均精度（Continuous-Target Average Precision, CoTAP）损失</strong>。CoTAP损失具有决策无关和自适应聚焦特性，能有效处理噪声和不平衡的伪标签，防止模型被误导。</li>
<li><strong>提出对象感知过滤器（Object-Aware Filter, OAF）：</strong> 为了在复杂场景中区分共享模式，论文引入了OAF模块。该模块通过<strong>交叉注意力（cross-attention）</strong>机制，利用可学习的<strong>对象原型（object prototypes）</strong>将补丁特征映射到对象基空间。这有助于消除场景干扰，更有效地捕获细粒度的共享模式。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
论文通过在多种任务上的实证研究，验证了所提方法的有效性：</p>
<ul>
<li><strong>图像级和密集任务的性能提升：</strong> 在ImageNet-1k上的图像分类任务中，所提方法将Top-1准确率提高了1.1%~4.2%。在COCOStuff-27上的语义分割任务中，mIoU提高了2.5%~10.8%。</li>
<li><strong>泛化能力强：</strong> 在语义分割、目标检测、实例分割和视频目标分割等多种下游密集任务上，所提方法均取得了持续的性能提升，证明了其生成的密集表示具有良好的泛化能力。</li>
<li><strong>细粒度表示的改进：</strong> 尤其在目标检测的小物体性能上，CoTAP方法显示出显著优势，表明其能生成更细粒度的表示。</li>
<li><strong>语义集中机制的有效性：</strong> 消融实验表明，语义集中损失（CoTAP损失）和对象感知过滤器（OAF）模块对性能提升至关重要，尤其在处理噪声和复杂场景方面。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到的一项局限性是：
*   <strong>对对象中心数据的依赖：</strong> 为了提取干净的对象原型，所提方法需要对象中心数据。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了未来的研究方向：
*   <strong>自适应提取对象原型：</strong> 未来工作将探索如何从非策展（uncurated）图像中自适应地提取对象原型，以适应大规模数据。</p>
<hr />
<p>这篇论文通过引入显式语义集中机制，成功解决了自监督学习在密集表示学习中面临的过分散问题。CoTAP损失和OAF模块的结合，使得模型能够更好地处理噪声伪标签、区分复杂场景中的共享模式，并最终生成更具细粒度和泛化能力的表示，为密集视觉任务的自监督学习开辟了新途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>First, to break the strict spatial alignment, we propose to distill
the patch correspondences.</li>
<li>Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.</li>
<li>Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09429v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09429v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09427v1'></a></p>
<h2 id="fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-image-fusion-and-super-resolution"><a href="https://arxiv.org/abs/2509.09427v1">FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</a></h2>
<p><strong>Authors:</strong> Yuchan Jie, Yushen Xu, Xiaosong Li, Fuqiang Zhou, Jianming Lv, Huafeng Li</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yuchan Jie等人撰写的论文“FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution”的全面摘要。</p>
<hr />
<h3 id="fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-image-fusion-and-super-resolution_1">论文摘要：FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前多模态图像融合（MMIF）和超分辨率技术在实际应用中面临挑战，尤其是在军事侦察和远距离探测等场景中。这些场景下的多模态图像（如红外和可见光图像）往往分辨率低、语义信息弱，导致目标和背景结构容易受损，现有方法难以自适应地处理低分辨率图像，且无法有效融合语义信息，从而产生次优的融合结果。论文旨在解决如何同时实现图像融合和超分辨率，以生成具有丰富细节和语义信息的高分辨率融合图像。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
FS-Diff提出了一个语义引导和清晰度感知的联合图像融合与超分辨率方法，其主要创新点包括：</p>
<ul>
<li><strong>统一的条件生成问题框架：</strong> FS-Diff将图像融合和超分辨率统一为一个条件生成问题，通过迭代去噪过程从纯高斯噪声初始化目标融合结果。</li>
<li><strong>语义引导和清晰度感知机制（CLSE）：</strong> 引入CLSE机制，结合提出的CA-CLIP模型，实现对低分辨率图像的自适应感知和跨模态特征提取。CA-CLIP通过预训练的CLIP架构，能够从异构输入模态中精确提取语义信息，即使图像清晰度不一致也能有效工作。它还能预测图像的分辨率类型（清晰或模糊），从而在单幅图像模糊时从清晰图像中提取高质量语义信息。</li>
<li><strong>双向特征Mamba（BFM）模块：</strong> 引入BFM模块来提取多模态图像的全局特征，构建多模态数据的联合表示，增强模型提取全局信息和跨模态特征的能力。</li>
<li><strong>基于U-Net的随机迭代去噪过程：</strong> 利用源图像和语义信息作为条件，通过修改后的U-Net网络实现随机迭代去噪过程，该网络在多个噪声水平下进行去噪训练，以生成具有丰富跨模态特征和语义信息的高分辨率融合结果。</li>
<li><strong>新型航空视角多场景（AVMS）基准数据集：</strong> 构建了一个包含859对对齐的可见光和红外图像、3821个标注目标（涵盖白天、夜晚、黄昏和复杂天气条件，以及七种以上不同场景）的AVMS数据集，适用于图像融合、超分辨率、远距离检测和语义分割任务。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的融合与超分辨率性能：</strong> 在六个公共数据集和自建的AVMS数据集上进行的广泛实验表明，FS-Diff在不同放大倍数下均优于现有最先进方法，能够恢复融合图像中更丰富的细节和语义信息。
*   <strong>在高级视觉任务中的应用价值：</strong> 在目标检测和语义分割任务上的实验也验证了FS-Diff的灵活性和优越性能，能够生成更准确的分割结果和高置信度的检测结果，尤其是在处理小目标时。
*   <strong>CLSE和BFM机制的有效性：</strong> 消融实验证明了CLSE机制和BFM模块的协同作用对提升融合性能至关重要，它们分别负责自适应感知图像清晰度和提取全局跨模态特征。
*   <strong>CLSE机制的零样本泛化能力：</strong> 在未见过的有雾和低分辨率AVMS数据集上，CLSE机制展现出优异的零样本泛化能力，无需特定训练即可提升跨条件性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>模型复杂度和运行时间：</strong> FS-Diff框架虽然在联合图像超分辨率和融合方面表现出色，但也存在模型复杂度高和运行时间长等局限性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>优化U-Net结构：</strong> 计划改进U-Net结构以降低模型复杂度，并进一步增强FS-Diff快速适应复杂多变真实场景的能力。
*   <strong>集成到实际应用：</strong> 将FS-Diff进一步整合到自动驾驶系统、无人机精准农业灌溉技术和无人机军事作战策略中，以拓展其在智能交通、农业现代化和国防科技等领域的应用和实践。</p>
<hr />
<p>总而言之，FS-Diff通过创新的语义引导和清晰度感知机制，结合双向特征Mamba和扩散模型，成功地将图像融合和超分辨率任务统一为一个条件生成问题，显著提升了多模态图像处理的性能，并在实际应用中展现出巨大潜力。新构建的AVMS数据集也为相关研究提供了宝贵的基准。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method.</li>
<li>Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09427v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09427v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09332v1'></a></p>
<h2 id="omnieva-embodied-versatile-planner-via-task-adaptive-3d-grounded-and-embodiment-aware-reasoning"><a href="https://arxiv.org/abs/2509.09332v1">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a></h2>
<p><strong>Authors:</strong> Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Yuecheng Liu等人撰写的论文“OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning”的全面摘要。</p>
<hr />
<h3 id="omnieva-3d-">论文摘要：OmniEVA: 通过任务自适应3D-接地和具身感知推理实现的具身多功能规划器</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前基于多模态大语言模型（MLLMs）的具身系统在具身智能方面面临两个关键限制：
*   <strong>几何适应性差距（Geometric Adaptability Gap）：</strong> 仅在2D输入上训练或硬编码3D几何注入的模型，在处理需要强空间推理（如物体堆叠、遮挡处理、3D场景导航）的任务时表现不佳，因为它们缺乏足够的空间信息或2D泛化能力受限。
*   <strong>具身约束差距（Embodiment Constraint Gap）：</strong> 现有工作常忽略真实机器人的物理约束和能力，导致生成的任务规划在理论上可行但在实践中不可执行。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为解决上述问题，作者提出了<strong>OmniEVA</strong>，一个具身多功能规划器，通过以下两项核心创新实现先进的具身推理和任务规划：
*   <strong>任务自适应3D接地机制（Task-Adaptive 3D Grounding）：</strong> 引入了一个门控路由器（gated router），根据上下文需求对3D融合进行显式选择性调节。这使得模型能够针对不同的具身任务进行上下文感知的3D接地，避免了静态3D融合的缺点，并在2D和3D推理任务中实现鲁棒性能。
*   <strong>具身感知推理框架（Embodiment-Aware Reasoning）：</strong> 将任务目标和具身约束共同整合到推理循环中。通过提出的<strong>任务和具身感知强化学习（TE-GRPO）算法</strong>进行后训练，模型学习生成既目标导向又可执行的规划决策，尊重物体可供性、工作空间边界和运动学限制。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> OmniEVA在8个具身推理基准测试中的7个上实现了最先进的性能，这些基准测试涵盖了图像、视频和3D基础的问答任务，以及从基本空间理解到多维输入下高级几何推理的广泛任务。
*   <strong>强大的适应性和通用性：</strong> 实验结果表明，OmniEVA不仅在通用具身推理方面表现出色，而且在各种下游场景中也展现出强大的能力。
*   <strong>鲁棒和多功能规划能力：</strong> 对一系列提出的具身基准测试（包括原始任务和复合任务）的评估证实了其鲁棒和多功能规划能力。
*   <strong>动态3D接地机制的有效性：</strong> 与硬编码3D集成和无3D集成基线相比，任务自适应3D接地机制在多模态基准测试中表现出优越的性能，平均性能提升1.22%，突显了模型在上下文适当时利用3D信息的卓越适应性。
*   <strong>具身感知推理的提升：</strong> TE-GRPO训练方法显著提高了原始技能基准（Where2Approach, Where2Fit, Where2Grasp）和涉及物理执行的下游任务（Mobile Placement, Mobile Pickup）的性能，例如在Mobile Placement任务中成功率提高了43%（Easy）和50%（Hard）。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>低级控制策略的性能瓶颈：</strong> 尽管TE-GRPO提高了具身感知推理的性能，但在某些任务（如Mobile Pickup）中，如果低级控制策略本身存在性能瓶颈，具身感知推理的有效性会减弱。这意味着模型的最终执行性能仍受抓取策略泛化能力的限制。
*   <strong>下游任务的评估：</strong> 论文提到，下游任务只需要一个可行的局部运动和操作解决方案，而基准测试要求的是最优解。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进低级控制策略：</strong> 解决抓取策略的泛化能力限制，以充分发挥具身感知推理的潜力。
*   <strong>更复杂的具身约束集成：</strong> 探索更细致的物理约束和机器人能力，以进一步提高规划的实际可行性。
*   <strong>扩展到更广泛的具身任务：</strong> 将OmniEVA的框架应用于更多样化、更复杂的具身任务，包括长期、多步骤的移动操作和人机协作场景。
*   <strong>实时部署和泛化：</strong> 进一步研究模型在真实世界物理环境中的实时部署和泛化能力，以应对动态和不可预测的挑战。</p>
<hr />
<p>这篇论文通过引入任务自适应3D接地和具身感知推理，为具身智能领域做出了重要贡献，有效地弥合了MLLMs在理论规划与实际机器人执行之间的差距。OmniEVA的创新性在于其动态地整合多模态信息并考虑物理约束，使其成为一个强大的、通用的具身规划器。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.</li>
<li>Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks.</li>
<li>Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09332v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09332v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-12 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
