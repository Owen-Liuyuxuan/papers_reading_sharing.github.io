<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-12 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-11/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-15/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-12">Arxiv Computer Vision Papers - 2025-09-12</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-understanding-by-design-how-datasets-shape-architectures-and-insights" class="nav-link">Video Understanding by Design: How Datasets Shape Architectures and Insights</a>
                </li>
                <li class="nav-item">
                    <a href="#computational-imaging-for-enhanced-computer-vision" class="nav-link">Computational Imaging for Enhanced Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#a-structured-review-of-underwater-object-detection-challenges-and-solutions-from-traditional-to-large-vision-language-models" class="nav-link">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#flux-reason-6m-prism-bench-a-million-scale-text-to-image-reasoning-dataset-and-comprehensive-benchmark" class="nav-link">FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#spatialvid-a-large-scale-video-dataset-with-spatial-annotations" class="nav-link">SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-grounding-from-event-cameras" class="nav-link">Visual Grounding from Event Cameras</a>
                </li>
                <li class="nav-item">
                    <a href="#interact-advancing-large-scale-versatile-3d-human-object-interaction-generation" class="nav-link">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#semantic-concentration-for-self-supervised-dense-representations-learning" class="nav-link">Semantic Concentration for Self-Supervised Dense Representations Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-image-fusion-and-super-resolution" class="nav-link">FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#omnieva-embodied-versatile-planner-via-task-adaptive-3d-grounded-and-embodiment-aware-reasoning" class="nav-link">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-12">Arxiv Computer Vision Papers - 2025-09-12</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ11æ¥Arxivè®¡ç®æºè§è§è®ºæçæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£å³é®åå±ã</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§è®ºææ¯æ¥æ§è¡æè¦ (2025-09-11)</strong></p>
<p><strong>æ¦è¿°ï¼</strong>
ä»æ¥çArxivè®¡ç®æºè§è§è®ºæå±ç°äºè¯¥é¢åå¨<strong>æ°æ®é©±å¨çè¿æ­¥ãå¤æ¨¡æèåãå·èº«æºè½ä»¥åå¯¹å¤æåºæ¯çè§£</strong>æ¹é¢çæç»­å³æ³¨ãä¸»è¦è¶å¿åæ¬ï¼æå»ºæ´å¤§ãæ´ç²¾ç»æ æ³¨çæ°æ®éä»¥æ¨å¨æ¨¡ååå±ï¼å©ç¨æ°å´ä¼ æå¨ï¼å¦äºä»¶ç¸æºï¼æè®¡ç®æåææ¯æ¥å¢å¼ºè§è§æç¥ï¼ä»¥åå°è§è§è½åä¸æ¨çãè§ååäº¤äºç¸ç»åï¼ä»¥å®ç°æ´é«çº§çAIç³»ç»ã</p>
<p><strong>ä¸»è¦ä¸»é¢åè¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤§è§æ¨¡æ°æ®éä¸åºåçæå»º (Data-Centric AI):</strong> å¤ç¯è®ºæè´åäºåå»ºæ°çãå¤§è§æ¨¡ä¸å·æå¤ææ æ³¨çæ°æ®éï¼ä»¥è§£å³ç°ææ°æ®éçå±éæ§ï¼æ¶µçè§é¢çè§£ãææ¬å°å¾åæ¨çãç©ºé´è§é¢æ æ³¨å3Däººæºäº¤äºç­é¢åãè¿å¼ºè°äºé«è´¨éæ°æ®å¨æ¨å¨æ¨¡åæ§è½åæ³åè½åæ¹é¢çéè¦æ§ã</li>
<li><strong>å¤æ¨¡æä¸è·¨é¢åèå (Multimodal &amp; Cross-Domain Fusion):</strong> è®ºææ¢ç´¢äºææ¬ãå¾åãè§é¢ã3Dä¿¡æ¯ä»¥åäºä»¶æ°æ®ç­å¤ç§æ¨¡æçèåãä¾å¦ï¼ææ¬å°å¾åæ¨çãå¤æ¨¡æå¾åèåä»¥åå°3Dä¿¡æ¯èå¥å·èº«æºè½è§åã</li>
<li><strong>å·èº«æºè½ä¸äº¤äº (Embodied AI &amp; Interaction):</strong> å·èº«æºè½åäººæºäº¤äºæ¯æ¾èçè¶å¿ï¼ä½ç°å¨3Däººæºäº¤äºçæåå·èº«éç¨è§åå¨ç­å·¥ä½ä¸­ï¼æ¨å¨è®©AIç³»ç»è½æ´å¥½å°çè§£åæä½ç©çä¸çã</li>
<li><strong>æ°å´ä¼ æå¨ä¸è®¡ç®æå (Novel Sensors &amp; Computational Imaging):</strong> äºä»¶ç¸æºåè®¡ç®æåææ¯è¢«æåºä½ä¸ºå¢å¼ºä¼ ç»è§è§ç³»ç»æç¥è½åçæ°éå¾ï¼å°¤å¶å¨ä½åãé«éè¿å¨ç­æææ§åºæ¯ä¸ã</li>
<li><strong>å¤æåºæ¯çè§£ä¸æ¨ç (Complex Scene Understanding &amp; Reasoning):</strong> ä»æ°´ä¸ç®æ æ£æµçææå°ææ¬å°å¾åçå¤ææ¨çï¼åå°è§é¢ä¸­çç©ºé´çè§£ï¼ç ç©¶äººåæ­£åªåä½¿æ¨¡åè½å¤å¤çæ´å¤æãæ´å·æææ§çè§è§åºæ¯ã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark" (Rongyao Fang et al.):</strong> è¿ç¯è®ºæéè¿æå»ºä¸ä¸ªç¾ä¸çº§è§æ¨¡çææ¬å°å¾åæ¨çæ°æ®éååºåï¼ç´æ¥è§£å³äºå½åå¤æ¨¡ææ¨¡åå¨å¤ææ¨çè½åä¸çç¶é¢ãå¶è§æ¨¡åå¤ææ§æææ¾èæ¨å¨å¤æ¨¡æå¤§æ¨¡åçæ¨çè½ååå±ã</li>
<li><strong>"OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning" (Yuecheng Liu et al.):</strong> OmniEVAä»£è¡¨äºå·èº«æºè½é¢åçä¸ä¸ªéè¦è¿å±ï¼å®ç»åäº3Dæç¥ãä»»å¡éåºæ§åå·èº«æè¯æ¨çï¼æ¨å¨å®ç°æ´éç¨åé«æçå·èº«è§åãè¿å¯¹äºæå»ºè½å¤ä¸ç©çä¸çææäº¤äºçæºå¨äººåAIç³»ç»è³å³éè¦ã</li>
<li><strong>"Visual Grounding from Event Cameras" (Lingdong Kong et al.):</strong> å©ç¨äºä»¶ç¸æºè¿è¡è§è§å®ä½æ¯ä¸ä¸ªæ°é¢ä¸å·ææ½åçæ¹åãäºä»¶ç¸æºå¨é«éãé«å¨æèå´åºæ¯ä¸çç¬ç¹ä¼å¿ï¼ç»åè§è§å®ä½ä»»å¡ï¼ææå¨ä¼ ç»ç¸æºåéçåºæ¯ä¸­å¼è¾æ°çåºç¨ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>äºä»¶ç¸æºå¨é«çº§è§è§ä»»å¡ä¸­çåºç¨ï¼</strong> ä¸ä»ä»æ¯ä½çº§æç¥ï¼äºä»¶ç¸æºæ­£è¢«æ¢ç´¢ç¨äºæ´å¤æçä»»å¡ï¼å¦è§è§å®ä½ã</li>
<li><strong>3D-Grounded Reasoning for Embodied AIï¼</strong> å°3Dç©ºé´ä¿¡æ¯æ·±åº¦èå¥å·èº«æºè½çè§ååæ¨çï¼ä»¥å®ç°æ´ç²¾ç¡®åé²æ£çç©çä¸çäº¤äºã</li>
<li><strong>å¤§è§æ¨¡å¤æ¨¡ææ¨çæ°æ®éçç³»ç»æ§æå»ºï¼</strong> ä¸ä»ä»æ¯æ°æ®éï¼æ´å¼ºè°æ°æ®ä¸­è´å«çå¤ææ¨çè½åï¼ä»¥æ¨å¨å¤§æ¨¡åçæºè½æ°´å¹³ã</li>
<li><strong>è¯­ä¹éä¸­ï¼Semantic Concentrationï¼çèªçç£å­¦ä¹ ï¼</strong> æ¢ç´¢æ°çèªçç£èå¼ï¼ä»¥å­¦ä¹ æ´å·è¯­ä¹åºååº¦çå¯éè¡¨ç¤ºã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹å¤æ¨¡æå¤§æ¨¡ååæ¨çæå´è¶£ï¼</strong><ul>
<li><strong>"FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark"</strong> (Rongyao Fang et al.) - äºè§£å¦ä½æå»ºåè¯ä¼°å¤æå¤æ¨¡ææ¨çè½åã</li>
</ul>
</li>
<li><strong>å¯¹å·èº«æºè½ãæºå¨äººå3Däº¤äºæå´è¶£ï¼</strong><ul>
<li><strong>"OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning"</strong> (Yuecheng Liu et al.) - æ·±å¥äºè§£å·èº«è§åçææ°è¿å±ã</li>
<li><strong>"InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation"</strong> (Sirui Xu et al.) - å³æ³¨3Däººæºäº¤äºççæåçè§£ã</li>
</ul>
</li>
<li><strong>å¯¹è§é¢çè§£åæ°æ®éæå»ºæå´è¶£ï¼</strong><ul>
<li><strong>"Video Understanding by Design: How Datasets Shape Architectures and Insights"</strong> (Lei Wang et al.) - æä¾å³äºæ°æ®éå¦ä½å½±åè§é¢çè§£æ¨¡åè®¾è®¡çæ·±å»è§è§£ã</li>
<li><strong>"SpatialVID: A Large-Scale Video Dataset with Spatial Annotations"</strong> (Jiahao Wang et al.) - äºè§£æ°çè§é¢æ°æ®éåå¶ç©ºé´æ æ³¨çä»·å¼ã</li>
</ul>
</li>
<li><strong>å¯¹æ°å´ä¼ æå¨æè®¡ç®æåæå´è¶£ï¼</strong><ul>
<li><strong>"Visual Grounding from Event Cameras"</strong> (Lingdong Kong et al.) - æ¢ç´¢äºä»¶ç¸æºå¨é«çº§è§è§ä»»å¡ä¸­çæ½åã</li>
<li><strong>"Computational Imaging for Enhanced Computer Vision"</strong> (Humera Shaikh, Kaur Jashanpreet) - äºè§£è®¡ç®æåå¦ä½èµè½è®¡ç®æºè§è§ã</li>
</ul>
</li>
<li><strong>å¯¹èªçç£å­¦ä¹ åè¡¨ç¤ºå­¦ä¹ æå´è¶£ï¼</strong><ul>
<li><strong>"Semantic Concentration for Self-Supervised Dense Representations Learning"</strong> (Peisong Wen et al.) - æ¢ç´¢æ°çèªçç£å­¦ä¹ èå¼ã</li>
</ul>
</li>
</ul>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.09151v1">Video Understanding by Design: How Datasets Shape Architectures and Insights</a></li>
<li><a href="#2509.08712v1">Computational Imaging for Enhanced Computer Vision</a></li>
<li><a href="#2509.08490v1">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a></li>
<li><a href="#2509.09680v1">FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</a></li>
<li><a href="#2509.09676v1">SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</a></li>
<li><a href="#2509.09584v1">Visual Grounding from Event Cameras</a></li>
<li><a href="#2509.09555v1">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a></li>
<li><a href="#2509.09429v1">Semantic Concentration for Self-Supervised Dense Representations Learning</a></li>
<li><a href="#2509.09427v1">FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</a></li>
<li><a href="#2509.09332v1">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.09151v1'></a></p>
<h2 id="video-understanding-by-design-how-datasets-shape-architectures-and-insights"><a href="https://arxiv.org/abs/2509.09151v1">Video Understanding by Design: How Datasets Shape Architectures and Insights</a></h2>
<p><strong>Authors:</strong> Lei Wang, Piotr Koniusz, Yongsheng Gao</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Lei Wang, Piotr Koniusz, Yongsheng Gaoæ°åçè®ºæâVideo Understanding by Design: How Datasets Shape Architectures and Insightsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="video-understanding-by-design-how-datasets-shape-architectures-and-insights_1">è®ºææè¦ï¼Video Understanding by Design: How Datasets Shape Architectures and Insights</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçè§é¢çè§£ç»¼è¿°å¤§å¤æ ¹æ®ä»»å¡ææ¨¡åå®¶æå¯¹æ¨¡åè¿è¡åç±»ï¼å¿½ç¥äºæ°æ®éåå¨çâç»ææ§ååâå¦ä½å¼å¯¼æ¶ææ¼è¿ãè¿å¯¼è´é¢åç¼ºä¹ä¸ä¸ªæ¦å¿µæ§å°å¾ï¼æ æ³å°è¿å»çè¿å±ç½®äºè¯­å¢ä¸­ï¼ä¹æ æ³é¢æµæªæ¥çè¶å¿ãæ¬ææ¨å¨è§£å³è¿ä¸é®é¢ï¼éè¿éç¨æ°æ®éé©±å¨çè§è§ï¼æ­ç¤ºæ°æ®éç¹æ§å¦ä½å¡é æ¨¡åæ¶æåå½çº³åç½®ï¼ä»èæ¨å¨éç¨è§é¢çè§£çåå±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>æ°æ®éé©±å¨çè§è§ï¼</strong> æ¬æé¦æ¬¡æåºå¹¶ç³»ç»æ§å°éç¨æ°æ®éé©±å¨çè§è§æ¥å®¡è§è§é¢çè§£é¢åï¼å¼ºè°æ°æ®éçåå¨ç»æå±æ§ï¼å¦è¿å¨å¤æåº¦ãæ¶é´è·¨åº¦ãå±æ¬¡åææåå¤æ¨¡æä¸°å¯æ§ï¼å¦ä½æ½å å½çº³åç½®ï¼å¹¶æå¯¼æ¨¡åæ¶æè®¾è®¡ã
*   <strong>éç¨ç¢å¼æ¶æçéæ°è§£è¯»ï¼</strong> è®ºæå°è§é¢çè§£é¢åçéç¨ç¢å¼æ¶æï¼ä»åæµå3D CNNå°åºåæ¨¡åãTransformeråå¤æ¨¡æåºç¡æ¨¡åï¼éæ°è§£è¯»ä¸ºå¯¹è¿äºæ°æ®éé©±å¨ååçå·ä½ååºã
*   <strong>ç»ä¸æ¡æ¶ï¼</strong> æ¬æå°æ°æ®éãå½çº³åç½®åæ¶æç»ä¸å°ä¸ä¸ªè¿è´¯çæ¡æ¶ä¸­ï¼æä¾äºä¸ä¸ªå¨é¢çåé¡¾ååç»æ§çè·¯çº¿å¾ã
*   <strong>å®ç¨æå¯¼ï¼</strong> åºäºè¿ç§ç»¼ååæï¼è®ºæä¸ºæ¨¡åè®¾è®¡æä¾äºå®ç¨æå¯¼ï¼ä»¥å¨æ°æ®éä¸åæ§ä¸å¯æ©å±æ§åä»»å¡éæ±ä¹é´åå¾å¹³è¡¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ°æ®éç¹æ§ä¸æ¶ææ¼è¿çå³èï¼</strong> è®ºæè¯¦ç»å±ç¤ºäºä¸åæ°æ®éç¹æ§å¦ä½é©±å¨ç¹å®æ¶æçåºç°ååå±ã
    *   <strong>è¿å¨å¤æåº¦ï¼</strong> æ©æå³æ³¨é«å¹åº¦ãå¨å±è¿å¨çæ°æ®éï¼å¦UCF101ï¼ä¿æäºåæµCNNå3D CNNçåå±ãèç°ä»£æ°æ®éï¼å¦Diving48ãFineGymï¼å¯¹ç»ç²åº¦ãå¾®å°è¿å¨çè¯å«éæ±ï¼æ¨å¨äºå¤å°ºåº¦3Då·ç§¯ãæ¶é´Transformerååºäºå§¿æçè¡¨ç¤ºã
    *   <strong>æ¶é´è·¨åº¦ä¸å±æ¬¡åææï¼</strong> é¿æ¶åºãåå«å¤æ­¥éª¤åå¹¶åå¨ä½çæ°æ®éï¼å¦BreakfastãCharadesãEPIC-Kitchensï¼ä¿ä½¿æ¨¡åéè¦æè·é¿è·ç¦»ä¾èµãæ¶é´å±æ¬¡ç»æåå³ç³»æ¨çï¼ä»èæ¨å¨äºåºåæ¨¡åãæ¶é´æ¨çç½ç»ãTransformeråå¾æ¨¡åçåºç¨ã
    *   <strong>å¤æ¨¡æä¸°å¯æ§ï¼</strong> åå«è¯­è¨ãé³é¢ç­å¤ç§æ¨¡æçæ°æ®éï¼å¦HowTo100MãKoala-36Mï¼æ¨å¨äºè·¨æ¨¡æå¯¹é½æ¨¡ååå¤§åè§é¢-è¯­è¨åºç¡æ¨¡åï¼VLMsï¼çå´èµ·ï¼ä»¥å®ç°è¯­ä¹æ¥å°åè·¨æ¨¡ææ¨çã
*   <strong>æ§è½è¶å¿çè§£éï¼</strong> è®ºæéè¿åæä¸åæ¨¡åå¨åé¡¹åºåæµè¯ï¼å¦å¨ä½è¯å«ãæ¶åºå®ä½ãè§é¢æ£ç´¢åé®ç­ï¼ä¸çè¡¨ç°ï¼éªè¯äºæ°æ®éé©±å¨çå½çº³åç½®ä¸æ¨¡åæ§è½ä¹é´çå¼ºå³èãä¾å¦ï¼ç­çæ®µæ°æ®éä¸3D CNNè¡¨ç°ä¼å¼ï¼èé¿æ¶åºãå¤æ¨¡ææ°æ®éä¸TransformeråVLMsåæ´å·ä¼å¿ã
*   <strong>éç¨è§é¢çè§£çè·¯çº¿å¾ï¼</strong> è®ºææä¾äºä¸ä¸ªå°æ°æ®éå±æ§ãæ¶æåååä»»å¡ç¹å®èéç¸ç»åçæ¡æ¶ï¼ä»¥æå¯¼ä¸ä¸ä»£è§é¢çè§£æ¨¡åçè®¾è®¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®éåå·®ï¼</strong> ç°ææ°æ®éå¾å¾åæ ç­éçæåæç¯å¢èæ¯ï¼å¦ä½è²ãå¨æ¿ãèæ¬çµå½±ï¼ï¼å¯¼è´æ¨¡åå¯è½å©ç¨è¿äºåéªç¥è¯èéè·å¾é²æ£çæ¶ç©ºæ¨çè½åï¼å¨çå®ä¸çé¨ç½²ä¸­è¡¨ç°ä¸ä½³ã
*   <strong>æ æ³¨ææ¬åç²åº¦ï¼</strong> ç»ç²åº¦æ¶åºæ ç­¾ãå±æ¬¡åä»»å¡åè§£åå¤æ¨¡æå¯¹é½çè·åææ¬é«æï¼å¯¼è´è®¸å¤æ°æ®éçç£ç¨çãæ¶é´å¯åº¦æéæè¾¹çæ¨¡ç³ã
*   <strong>çææææ§ä¸è¶³ï¼</strong> è®¸å¤æ°æ®éç¼ºä¹çææææ§ï¼åªè¾çç­çæ®µåä¿®åªè¿çå¨ä½è¾¹çæè·çæ¯å­¤ç«çç¬é´ï¼èéæ¥å¸¸æ´»å¨ä¸­å¸åçè¿ç»­ãéå åæ¨¡ç³çå·¥ä½æµã
*   <strong>è¯ä¼°ç¢çåï¼</strong> è·¨æ°æ®éçè¯ä¼°ææ ååè®®ä¸ä¸è´ï¼é¾ä»¥è¯ä¼°æ¨¡åçæ³åè½åãç»åæ¨çãå ææ¨æ­æå¯¹åå¸ååçé²æ£æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ°æ®éçæ¼è¿ï¼</strong> æªæ¥çåºåæµè¯åºè¶è¶éæãé¢åç¹å®çè¯­æåºï¼æ¥æ±å¤æ ·æ§ãçææææ§åå¯æ©å±æ§ãåºæè·è¿ç»­ãæªä¿®åªãå¤æ¨¡æåå¤æºè½ä½æ´»å¨ï¼æ¯ææ´é«å±æ¬¡çæ¨çä»»å¡ï¼å¦é¢æµãå æåæåäº¤äºå¼å³ç­ï¼ã
*   <strong>æ¶æçæ´åï¼</strong> æªæ¥çæ¶æåºæ´åCNNçæ¶é´ç²¾åº¦ãç¨åºæéçå±æ¬¡åææãTransformerçé¿æ¶åºå¯æ©å±æ§ä»¥åå¤æ¨¡æé¢è®­ç»åæä»¤è°ä¼æä¾çè¯­ä¹æ¥å°è½åã
*   <strong>æ¶åº-è¯­ä¹ç»ä¸ï¼</strong> è§£å³æ¶åºå®ä½ç²¾ç¡®ä½ç¼ºä¹å¼æ¾è¯æ±è¯­ä¹çè§£ï¼ä»¥åè¯­ä¹è½åå¼ºä½è¾¹çå®ä½å·®çæ¨¡åä¹é´çå·®è·ã
*   <strong>é¿æ¶åºç»åæ¨çï¼</strong> ç»åæ£ç´¢å¢å¼ºçè§é¢çè§£ãå°äºä»¶åæ­¥éª¤ç´¢å¼å°æä¹è®°å¿ä¸­ï¼å¹¶éè¿è¯­è¨è¿è¡æ¥è¯¢ã
*   <strong>è¯ä¼°å®è·µçæ¹è¿ï¼</strong> éç¨æ ååçè·¨æ°æ®éåè®®ï¼å¼å¥æ°çè¯ä¼°ææ æ¥è¡¡éç»åæ³åãæ¨çå¿ å®åº¦ãåªå£°é²æ£æ§åè®¡ç®æçã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¶ç¬ç¹çæ°æ®éé©±å¨è§è§ï¼ä¸ºè§é¢çè§£é¢åæä¾äºä¸ä¸ªæ·±å»çæ´å¯ï¼ä¸ä»ç³»ç»å°åé¡¾äºè¿å»ï¼æ´ææäºæªæ¥ç ç©¶ååå±çæ¹åï¼å¼ºè°äºæ°æ®éè®¾è®¡å¨æ¨å¨éç¨ãé²æ£åå¯æ©å±è§é¢çè§£ç³»ç»ä¸­çæ ¸å¿ä½ç¨ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands.</li>
<li>By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09151v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09151v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08712v1'></a></p>
<h2 id="computational-imaging-for-enhanced-computer-vision"><a href="https://arxiv.org/abs/2509.08712v1">Computational Imaging for Enhanced Computer Vision</a></h2>
<p><strong>Authors:</strong> Humera Shaikh, Kaur Jashanpreet</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents a comprehensive survey of computational imaging (CI)
techniques and their transformative impact on computer vision (CV)
applications. Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems. Computational imaging techniques, including light field imaging, high
dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare
mitigation, address these limitations by enhancing image acquisition and
reconstruction processes. This survey systematically explores the synergies
between CI techniques and core CV tasks, including object detection, depth
estimation, optical flow, face recognition, and keypoint detection. By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions. We emphasize the potential for
task-specific, adaptive imaging pipelines that improve robustness, accuracy,
and efficiency in real-world scenarios, such as autonomous navigation,
surveillance, augmented reality, and robotics.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Humera ShaikhåKaur Jashanpreetæ°åçè®ºæâComputational Imaging for Enhanced Computer Visionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼è®¡ç®æåå¢å¼ºè®¡ç®æºè§è§</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ä¼ ç»æåæ¹æ³å¨ä½åç§ãè¿å¨æ¨¡ç³æé«å¨æèå´ç­æææ§æ¡ä»¶ä¸ï¼å¾å¾æ æ³æä¾é«ä¿çè§è§æ°æ®ï¼ä»èéå¶äºæåè¿è®¡ç®æºè§è§ï¼CVï¼ç³»ç»çæ§è½ãæ¬ç ç©¶æ¨å¨å¨é¢æ¢è®¨è®¡ç®æåï¼CIï¼ææ¯å¦ä½åæè¿äºéå¶ï¼å¹¶éè¿å¢å¼ºå¾åééåéå»ºè¿ç¨ï¼æ¾èæåCVåºç¨çé²æ£æ§ãåç¡®æ§åæçã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿åæ°å¨äºç³»ç»æ§å°è°æ¥åæ´åäºå¤ç§è®¡ç®æåææ¯ä¸æ ¸å¿è®¡ç®æºè§è§ä»»å¡ä¹é´çååä½ç¨ãå·ä½æ¹æ³è®ºè´¡ç®åæ¬ï¼
*   <strong>ååºæåï¼Light Field Imagingï¼ï¼</strong> éè¿æè·åçº¿çè§åº¦æ¹åï¼æä¾å¤è§è§æ°æ®ï¼æ¾èå¢å¼ºæ·±åº¦ä¼°è®¡ãå³é®ç¹æ£æµåé®æ¡å¤çã
*   <strong>é«å¨æèå´ï¼HDRï¼æåï¼</strong> éè¿èåä¸åæåæ°´å¹³çå¾åï¼ææåºæ¯ä¸­äº®é¨åæé¨çç»èï¼åæä¼ ç»ä¼ æå¨çå¨æèå´éå¶ï¼æ¹åé«å¯¹æ¯åº¦ç¯å¢ä¸çç®æ æ£æµåäººè¸è¯å«ã
*   <strong>å¾åå»æ¨¡ç³ï¼Image Deblurringï¼ï¼</strong> éè¿å»ºæ¨¡åéè½¬æ¨¡ç³è¿ç¨ï¼å¦è¿å¨æ¨¡ç³åæ£ç¦æ¨¡ç³ï¼ï¼æ¢å¤å¾åæ¸æ°åº¦ï¼æåè¿å¨ä¼°è®¡åç®æ æ£æµçåç¡®æ§ã
*   <strong>é«éæåï¼High-Speed Imagingï¼ï¼</strong> æé«æ¶é´åè¾¨çï¼ææå¿«éè¿å¨äºä»¶çç²¾ç»ç»èï¼æ¯æé«éç®æ è·è¸ªåè¿å¨åæã
*   <strong>ç©åç¼è§£ï¼Glare Mitigationï¼ï¼</strong> å©ç¨åæ¯æåæå¤æåæ¹æ³æå¶åå°åå¼ºåæºå¼èµ·çç©åï¼ç¡®ä¿å¨åå°æ§æå¤æåç§ç¯å¢ä¸CVç³»ç»ä»è½ä¿æé«ç²¾åº¦ã</p>
<p>è®ºæéè¿åæCIæ¹æ³ä¸ç®æ æ£æµãæ·±åº¦ä¼°è®¡ãåæµãäººè¸è¯å«åå³é®ç¹æ£æµç­æ ¸å¿CVä»»å¡ä¹é´çå³ç³»ï¼æ­ç¤ºäºCIææ¯å¦ä½ä¸ºè¿äºä»»å¡æä¾æ´å¯é çè¾å¥ï¼ä»èæ¾èæé«å¶åç¡®æ§åé²æ£æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåCVä»»å¡æ§è½ï¼</strong> CIææ¯ä¸ºCVç³»ç»æä¾äºæ´ä¸°å¯ãæ´åç¡®çè§è§æ°æ®ãä¾å¦ï¼ååºæåéè¿æä¾3Dä¿¡æ¯å¢å¼ºäºæ·±åº¦ä¼°è®¡åé®æ¡å¤çï¼HDRæåå¨é«å¯¹æ¯åº¦åºæ¯ä¸­æ¹åäºç®æ æ£æµåäººè¸è¯å«ï¼å»æ¨¡ç³ææ¯æé«äºå¨æç¯å¢ä¸­ç®æ æ£æµååæµçåç¡®æ§ï¼é«éæåå®ç°äºå¯¹å¿«éç§»å¨ç©ä½çç²¾ç¡®è·è¸ªï¼ç©åç¼è§£ç¡®ä¿äºå¨ä¸å©åç§æ¡ä»¶ä¸CVç³»ç»çå¯é æ§ã
*   <strong>è·¨ä»»å¡ååæåºï¼</strong> è®ºæå¼ºè°äºCIææ¯å¨ä¸åCVä»»å¡ä¹é´äº§ççååæåºï¼ä¾å¦ï¼ååºæåä¸ä»å¢å¼ºäºæ·±åº¦ä¼°è®¡ï¼è¿é´æ¥æ¹åäºç®æ æ£æµåå³é®ç¹å¹éã
*   <strong>å®éåºç¨æ½åï¼</strong> è¿äºå¢å¼ºåè½å¯¹äºèªå¨é©¾é©¶ãçæ§ãå¢å¼ºç°å®åæºå¨äººç­ç°å®ä¸çåºç¨è³å³éè¦ï¼å ä¸ºå®ä»¬éè¦é«é²æ£æ§ãåç¡®æ§åæçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è®¡ç®å¤ææ§ï¼</strong> è®¸å¤CIæ¹æ³ï¼ç¹å«æ¯å®æ¶åºç¨ï¼æ¶åå¤§éæ°æ®åé«è®¡ç®éæ±ï¼éè¦å¼ºå¤§çå¤çè½åã
*   <strong>ç¡¬ä»¶ä¸è½¯ä»¶çæè¡¡ï¼</strong> CIææ¯ä¾èµäºä¸ä¸åå­¦è®¾è®¡åä¼ æå¨æ¶æï¼éè¦å¨ç©çå¤ææ§ãææ¬åå¯æ©å±æ§ä¹é´åå¾å¹³è¡¡ã
*   <strong>æ°æ®è´¨éåæ³åæ§ï¼</strong> å°½ç®¡CIææ¯æé«äºæ°æ®ä¿çåº¦ï¼ä½å¨ç¨çè§åº¦éæ ·ï¼ååºæåï¼æå¿«éè¿å¨åºæ¯ï¼HDRæåï¼ä¸­ä»å¯è½å¼å¥ä¼ªå½±ï¼éè¦æ´é²æ£çç®æ³æ¥å¤çä¸å®åæä¸å®æ´çæ°æ®ã
*   <strong>äºæä½æ§ï¼</strong> ç¡®ä¿æ°å´CIç¡¬ä»¶ä¸ç°æCVæ¡æ¶ä¹é´çäºæä½æ§ï¼éè¦æ ååç®¡éåæ°æ®éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ä»»å¡ç¹å®ãèªéåºæåç®¡éï¼</strong> å¼åè½å¤æ ¹æ®ç¹å®CVä»»å¡éæ±å¨æè°æ´å¾åééè¿ç¨çCIç³»ç»ï¼ä¾å¦ï¼ååºç¸æºæ ¹æ®åºæ¯æ·±åº¦å¤ææ§è°æ´è§åº¦éæ ·ï¼æé«éç¸æºä¼åå¤çå¿«éè¿å¨åºåã
*   <strong>æ·±åº¦å­¦ä¹ ä¸æ°æ®é©±å¨æ¹æ³ï¼</strong> å©ç¨æ·±åº¦å­¦ä¹ è¿ä¸æ­¥å¢å¼ºCIç®¡éï¼éè¿å­¦ä¹ åå§ä¼ æå¨æ°æ®ä¸ä»»å¡ç¹å®è¾åºä¹é´çç«¯å°ç«¯æ å°ï¼å®ç°æ´å¿«ãæ´åç¡®ãæ´çµæ´»çæåè§£å³æ¹æ¡ã
*   <strong>å¤æ¨¡ææåç³»ç»éæï¼</strong> ç»åç©ºé´ãè§åº¦ãåè°±åæ¶é´æåæ¹æ³ï¼ä»¥åå»ºæ´ä¸°å¯çåºæ¯è¡¨ç¤ºï¼ä½¿CVç³»ç»å¨å¤æç¯å¢ä¸­è¡¨ç°æ´é²æ£ï¼ä¾å¦ï¼å°åè°±æåä¸ååºææ¯ç»åä»¥æ¹åææåç±»ã
*   <strong>è¾¹ç¼è®¡ç®é¨ç½²ï¼</strong> å¼åé«æç®æ³åç¡¬ä»¶å éå¨ï¼å¦GPUåä¸ç¨ç¥ç»ç½ç»å¤çå¨ï¼ï¼ä»¥å¨èµæºåéçè¾¹ç¼è®¾å¤åç§»å¨å¹³å°ä¸å®ç°CIå¢å¼ºåCVç³»ç»çå®æ¶é¨ç½²ã</p>
<hr />
<p>è¿ç¯è®ºæå¨é¢æ¦è¿°äºè®¡ç®æåå¦ä½éè¿è§£å³ä¼ ç»æåçåºæå±éæ§æ¥å½»åºæ¹åè®¡ç®æºè§è§é¢åãå®ä¸ä»è¯¦ç»ä»ç»äºåç§CIææ¯åå¶å¯¹CVä»»å¡çè´¡ç®ï¼è¿æ·±å¥æ¢è®¨äºè¿äºææ¯ä¹é´çååä½ç¨ãé¢ä¸´çææä»¥åæªæ¥çç ç©¶æ¹åï¼ä¸ºè¯¥é¢åçè¿ä¸æ­¥åå±æä¾äºå®è´µçè·¯çº¿å¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems.</li>
<li>By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08490v1'></a></p>
<h2 id="a-structured-review-of-underwater-object-detection-challenges-and-solutions-from-traditional-to-large-vision-language-models"><a href="https://arxiv.org/abs/2509.08490v1">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a></h2>
<p><strong>Authors:</strong> Edwine Nabahirwa, Wei Song, Minghua Zhang, Yi Fang, Zhou Ni</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Underwater object detection (UOD) is vital to diverse marine applications,
including oceanographic research, underwater robotics, and marine conservation.
However, UOD faces numerous challenges that compromise its performance. Over
the years, various methods have been proposed to address these issues, but they
often fail to fully capture the complexities of underwater environments. This
review systematically categorizes UOD challenges into five key areas: Image
quality degradation, target-related issues, data-related challenges,
computational and processing constraints, and limitations in detection
methodologies. To address these challenges, we analyze the progression from
traditional image processing and object detection techniques to modern
approaches. Additionally, we explore the potential of large vision-language
models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated
in other domains. We also present case studies, including synthetic dataset
generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review
identifies three key insights: (i) Current UOD methods are insufficient to
fully address challenges like image degradation and small object detection in
dynamic underwater environments. (ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability. (iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Edwine Nabahirwaç­äººçè®ºæâA Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Modelsâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼æ°´ä¸ç®æ æ£æµçææä¸å¤§åè§è§è¯­è¨æ¨¡åçæ½å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨ç³»ç»æ§å°åé¡¾ååææ°´ä¸ç®æ æ£æµï¼UODï¼æé¢ä¸´çè¯¸å¤ææï¼å¹¶æ¢è®¨ä»ä¼ ç»æ¹æ³å°ç°ä»£æ·±åº¦å­¦ä¹ ææ¯ï¼ç¹å«æ¯å¤§åè§è§è¯­è¨æ¨¡åï¼LVLMsï¼å¨è§£å³è¿äºé®é¢ä¸çè¿å±åæ½åãUODå¯¹äºæµ·æ´ç ç©¶ãæ°´ä¸æºå¨äººåæµ·æ´ä¿æ¤ç­åºç¨è³å³éè¦ï¼ä½æ°´ä¸ç¯å¢çå¤ææ§ä¸¥éå½±åäºå¶æ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç³»ç»æ§ææåç±»ï¼</strong> è®ºæå°UODææç³»ç»å°å½çº³ä¸ºäºä¸ªå³é®é¢åï¼å¾åè´¨ééåãç®æ ç¸å³é®é¢ãæ°æ®ç¸å³ææãè®¡ç®åå¤çéå¶ä»¥åæ£æµæ¹æ³è®ºçå±éæ§ã
*   <strong>ææ¯åå±è·¯çº¿åæï¼</strong> è®ºæåæäºUODææ¯ä»ä¼ ç»å¾åå¤çåç®æ æ£æµæ¹æ³å°ç°ä»£æ·±åº¦å­¦ä¹ æ¹æ³çæ¼è¿è¿ç¨ã
*   <strong>LVLMså¨UODä¸­çæ½åæ¢ç´¢ï¼</strong> é¦æ¬¡æ·±å¥æ¢è®¨äºLVLMså¨UODé¢åçåºç¨æ½åï¼å©ç¨å¶å¨å¶ä»é¢åå±ç¤ºçå¤æ¨¡æè½åæ¥è§£å³UODçåºæå¤ææ§ã
*   <strong>æ¡ä¾ç ç©¶ï¼</strong> æä¾äºä¸¤ä¸ªå·ä½çæ¡ä¾ç ç©¶ï¼
    *   ä½¿ç¨DALL-E 3è¿è¡åææ°æ®éçæï¼ä»¥è§£å³æ°æ®ç¨ç¼ºåå¤æ ·æ§é®é¢ã
    *   ä½¿ç¨LoRAææ¯å¯¹Florence-2 LVLMè¿è¡å¾®è°ï¼ä»¥éåºUODä»»å¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åææ°æ®å¢å¼ºçæææ§ï¼</strong> æ¡ä¾ç ç©¶è¡¨æï¼éè¿DALL-E 3çæçåææ°æ®ï¼ç»è¿OpenCVå¢å¼ºä»¥å¢å çå®æï¼ä¸çå®æ°æ®éç»åï¼å¯ä»¥ç¥å¾®æåYOLO11æ¨¡åå¨UODä»»å¡ä¸çæ§è½ï¼mAP@50ä»0.793æåå°0.796ï¼mAP@50-95ä»0.501æåå°0.505ï¼ï¼å¹¶æé«äºå¬åçï¼ä»0.714æåå°0.736ï¼ãè¿è¡¨æLVLMså¨æ°æ®å¢å¼ºæ¹é¢å·ææ½åï¼å°¤å¶æ¯å¨è§£å³æ°æ®ç¨ç¼ºåç±»ä¸å¹³è¡¡é®é¢ä¸ã
*   <strong>LVLMså¨UODä¸­çå®ä½è½åï¼</strong> å¯¹Florence-2 LVLMè¿è¡å¾®è°çæ¡ä¾ç ç©¶æ¾ç¤ºï¼è¯¥æ¨¡åå¨å®ä½å°åæ°´ä¸ç®æ æ¹é¢è¡¨ç°åºå¼ºå¤§çè½åï¼å°¤å¶æ¯å¨å·ææææ§çç±»å«ï¼å¦æµ·æåæµ·èï¼ä¸ãè¿çªæ¾äºLVLMså¨å¤çå¤ææ°´ä¸ç¯å¢ä¸­çå°ç®æ åé®æ¡ç®æ æ¹é¢çæ½åã
*   <strong>LVLMsçå±éæ§ï¼</strong> å¾®è°åçFlorence-2æ¨¡åå­å¨âç±»åå¹»è§âé®é¢ï¼çææ¼åéè¯¯æä¸æ­£ç¡®çç±»åï¼ï¼è¿ä¸¥éå½±åäºè¯ä¼°ææ çè®¡ç®ãæ­¤å¤ï¼æ¨¡åè¿è¡¨ç°åºâç¾é¾æ§éå¿âç°è±¡ï¼å³å¨éåºç¹å®é¢åä»»å¡æ¶é¾ä»¥ä¿çå¶æ´å¹¿æ³çé¢è®­ç»ç¥è¯ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç°æUODæ¹æ³çä¸è¶³ï¼</strong> è®ºææåºï¼å½åçUODæ¹æ³ä»ä¸è¶³ä»¥å®å¨è§£å³å¨ææ°´ä¸ç¯å¢ä¸­å¾åéååå°ç®æ æ£æµç­ææã
*   <strong>åææ°æ®ççå®æ§åéç¨æ§ï¼</strong> å°½ç®¡LVLMsçæçåææ°æ®å·ææ½åï¼ä½ä»éè¿ä¸æ­¥å®åï¼ä»¥ç¡®ä¿å¶çå®æ§åå¨å¤ææ°´ä¸åºæ¯ä¸­çéç¨æ§ï¼ä¾å¦ï¼ç¼ºä¹èªç¶ç¼ºé·ãåç§åååä¸å¯é¢æµçé®æ¡ï¼ã
*   <strong>LVLMsçå®æ¶åºç¨ï¼</strong> LVLMså¨UODä¸­çå®æ¶åºç¨ä»æªå¾å°ååæ¢ç´¢ï¼éè¦è¿ä¸æ­¥ç ç©¶ä¼åææ¯ä»¥æ»¡è¶³è®¡ç®èµæºåéçæ°´ä¸å¹³å°çéæ±ã
*   <strong>LVLMsçå¹»è§åæ³åé®é¢ï¼</strong> LVLMså¨å¾®è°åå¯è½åºç°ç±»åå¹»è§åç¾é¾æ§éå¿ï¼è¿éå¶äºå¶å¨å®éåºç¨ä¸­çåç¡®æ§åå¯é æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>é«æå¾®è°ææ¯ï¼</strong> è¿ä¸æ­¥ç ç©¶ééå¨å¾®è°åæç¤ºå¾®è°ç­åæ°é«æå¾®è°ææ¯ï¼ä»¥æ´å¥½å°éåºLVLMså¨ä½åç§ãæ£å°æé¢è²å¤±çç­å¤ææ°´ä¸æ¡ä»¶ä¸çåºç¨ï¼åæ¶æå°åè®¡ç®å¼éã
*   <strong>æ´çå®çåææ°æ®çæï¼</strong> æ¢ç´¢å°æ©æ£æ¨¡åä¸å¶ä»çææ¹æ³ï¼å¦VAEåGANï¼ç»åçæ··åæ¹æ³ï¼ä»¥çææ´é¼çãæ´å·å¤æ ·æ§çæ°´ä¸å¾åï¼å¹¶è§£å³æ°´ä¸ç¹å®åæçææã
*   <strong>èªå¨åæ°æ®éæ æ³¨ï¼</strong> å©ç¨Label-driven Automated Prompt Tuning (LAPT)ç­æ¡æ¶ï¼éè¿èªå¨åæç¤ºå·¥ç¨åå¾ååæ/æ£ç´¢æ¹æ³ï¼åå°æ°´ä¸å¾åçæå¨æ æ³¨å·¥ä½ã
*   <strong>è½»éçº§å®æ¶å¤çæ¶æï¼</strong> éå¯¹AUVsåå®æ¶çæµç³»ç»ï¼å¼ååä¼åè½»éçº§LVLMsæ¶æï¼ä¾å¦éç¨æ¨¡ååªæåTransformeråç¼©ææ¯ï¼ä»¥å®ç°å®æ¶æ£æµã
*   <strong>è§£å³LVLMså¹»è§åæ³åé®é¢ï¼</strong> éè¿ç²¾å¿è®¾è®¡çæç¤ºå·¥ç¨ãä½¿ç¨æ´å¤§è§æ¨¡åæ´å¤æ ·åçè®­ç»æ°æ®ãæä»¤å¾®è°ä»¥åæ¹è¿çæ°æ®å¢å¼ºç­ç¥æ¥åè½»LVLMsçå¹»è§é®é¢ï¼å¹¶æé«å¶æ³åè½åã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºæ°´ä¸ç®æ æ£æµé¢åæä¾äºä¸ä¸ªå¨é¢çè§è§ï¼ä¸ä»ç³»ç»å°æ¢³çäºç°æææï¼è¿åç»æ§å°æ¢è®¨äºå¤§åè§è§è¯­è¨æ¨¡åå¨è§£å³è¿äºææä¸­çå·¨å¤§æ½åï¼å¹¶æåºäºæªæ¥ç ç©¶çå³é®æ¹åï¼ä¸ºè¯¥é¢åçåå±æä¾äºå®è´µçæå¯¼ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>(ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability.</li>
<li>(iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08490v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08490v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09680v1'></a></p>
<h2 id="flux-reason-6m-prism-bench-a-million-scale-text-to-image-reasoning-dataset-and-comprehensive-benchmark"><a href="https://arxiv.org/abs/2509.09680v1">FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark</a></h2>
<p><strong>Authors:</strong> Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºRongyao Fangç­äººæ°åçè®ºæâFLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmarkâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼FLUX-Reason-6M &amp; PRISM-Bench</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åå¼æºææ¬å°å¾åï¼T2Iï¼æ¨¡åçåå±åå°éå¶ï¼ä¸»è¦åå å¨äºç¼ºä¹å¤§è§æ¨¡ãä»¥æ¨çä¸ºéç¹çæ°æ®éåå¨é¢çè¯ä¼°åºåãè¿å¯¼è´å¼æºæ¨¡åä¸é¢åçé­æºç³»ç»ä¹é´å­å¨æ¾èçæ§è½å·®è·ï¼å°¤å¶æ¯å¨å¤çå¤ææ¨çä»»å¡æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼è®ºææåºäºä¸¤é¡¹æ ¸å¿è´¡ç®ï¼
*   <strong>FLUX-Reason-6M æ°æ®éï¼</strong> è¿æ¯ä¸ä¸ªå¤§è§æ¨¡çåææ°æ®éï¼åå«600ä¸å¼ é«è´¨éçFLUXçæå¾åå2000ä¸æ¡åè¯­ï¼è±è¯­åä¸­æï¼æè¿°ãè¯¥æ°æ®éä¸é¨è®¾è®¡ç¨äºææå¤æçæ¨çè½åï¼å¹¶æ ¹æ®å­ä¸ªå³é®ç¹å¾ï¼æ³è±¡åãå®ä½ãææ¬æ¸²æãé£æ ¼ãææåæå¾ï¼è¿è¡ç»ç»ãæ­¤å¤ï¼å®å¼å¥äºæç¡®çâçææç»´é¾âï¼Generation Chain-of-Thought, GCoTï¼ï¼è¯¦ç»åè§£äºå¾åçææ­¥éª¤ï¼ä¸ºæ¨¡åæä¾äºå¼ºå¤§çä¸­é´çç£ä¿¡å·ãæ´ä¸ªæ°æ®æ´çèæ¶15,000 A100 GPUå¤©ï¼æ¯ç®åææè´µçå¼æºæ°æ®éä¹ä¸ã
*   <strong>PRISM-Bench åºåï¼</strong> è¿æ¯ä¸ä¸ªæ°é¢çè¯ä¼°æ åï¼å·æä¸ä¸ªç¬ç«çè¯ä¼°èµéï¼åæ¬FLUX-Reason-6Mçå­ä¸ªç¹å¾ç±»å«ä»¥åä¸ä¸ªæå·æææ§çâé¿ææ¬âèµéï¼å©ç¨GCoTæç¤ºï¼ãè¯¥åºåéè¿ç²¾å¿è®¾è®¡çæç¤ºï¼å©ç¨åè¿çè§è§-è¯­è¨æ¨¡åï¼å¦GPT-4.1åQwen2.5-VL-72Bï¼å¯¹æç¤º-å¾åå¯¹é½åå¾åç¾å­¦è¿è¡ç»è´çãä¸äººç±»å¤æ­ä¸è´çè¯ä¼°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæå¯¹19ä¸ªé¢åçT2Iæ¨¡åå¨PRISM-Benchä¸è¿è¡äºå¹¿æ³è¯ä¼°ï¼ç»ææ­ç¤ºäºå³é®çæ§è½å·®è·ï¼å¹¶çªåºäºéè¦æ¹è¿çå·ä½é¢åï¼
*   <strong>é­æºæ¨¡åè¡¨ç°ä¼å¼ï¼</strong> é¢åçé­æºç³»ç»ï¼å¦GPT-Image-1åGemini2.5-Flash-Imageï¼å¨æ´ä½æ§è½ä¸è¡¨ç°åºè²ï¼å°¤å¶æ¯å¨æ³è±¡åãå®ä½ãé£æ ¼åææç­ç±»å«ä¸­ãGPT-Image-1å¨é£æ ¼åæå¾æ¹é¢è¡¨ç°æä½³ï¼Gemini2.5-Flash-Imageå¨æ³è±¡ååæææ¹é¢é¢åã
*   <strong>ææ¬æ¸²æåé¿ææ¬ææï¼</strong> ææ¬æ¸²æä»ç¶æ¯ææT2Iæ¨¡åé¢ä¸´çéå¤§ææï¼è¯¥ç±»å«å¨ææèµéä¸­å¾åæä½ãé¿ææ¬èµéä¹æ¾ç¤ºåºæææ¨¡åæ®éè¾ä½çå¾åï¼è¿è¡¨ææ¨¡åå¨éµå¾ªå¤æãå¤å±æ¬¡æä»¤æ¹é¢å­å¨æ¾èæ¹è¿ç©ºé´ã
*   <strong>å¼æºæ¨¡åè¿æ­¥æ¾èï¼</strong> å°½ç®¡ä¸é¡¶å°é­æºæ¨¡åä»æå·®è·ï¼ä½Qwen-ImageãSEEDream 3.0ä»¥åFLUXç³»åç­å¼æºæ¨¡åä¹å±ç°åºå¼ºå¤§çç«äºåï¼å°¤å¶æ¯å¨ä¸­æææ¬æ¸²ææ¹é¢ï¼SEEDream 3.0åGPT-Image-1åå¾äºæé«åï¼è¡¨æå¨å¤çä¸­ææçæ¹é¢åå¾äºæ¾èè¿å±ã
*   <strong>æ¨çè½åå·®è·ï¼</strong> è¯ä¼°ç»æå¼ºè°äºFLUX-Reason-6Mæ¨å¨è§£å³çæ¨çå·®è·é®é¢ï¼å³æ¨¡åå¨çè§£ååæå¤æåºæ¯é»è¾æ¹é¢çä¸è¶³ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   å°½ç®¡é­æºæ¨¡åè¡¨ç°åºè²ï¼ä½å¨æäºç»´åº¦ä¸ä»ææ¹è¿ç©ºé´ã
*   ææ¬æ¸²æåé¿ææ¬å¤çä»ç¶æ¯æææ¨¡åçå±åå¼±ç¹ï¼è¡¨æè¿äºä»»å¡çå¤ææ§è¿è¶ç°ææ¨¡åçå¤çè½åã
*   ç°ææ°æ®éç¼ºä¹ç»æåä¿¡å·æ¥æææ¨¡åå¤æçæ¨çè½åï¼ä¸è¯ä¼°åºåç¼ºä¹ç»ç²åº¦ï¼æ æ³ææåºåæåè¿æ¨¡åçå®éæ§è½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å©ç¨FLUX-Reason-6Mæ°æ®éè®­ç»ä¸ä¸ä»£ä»¥æ¨çä¸ºå¯¼åçT2Iæ¨¡åï¼ä»¥å¼¥åå½åæ¨¡åå¨å¤ææ¨çä»»å¡ä¸çæ§è½å·®è·ã
*   è¿ä¸æ­¥ç ç©¶åæ¹è¿ææ¬æ¸²æåé¿ææ¬å¤çè½åï¼å¼åæ´ææçæ¶æåè®­ç»æ¹æ³ã
*   å©ç¨PRISM-Benchä½ä¸ºå¯é çè¯ä¼°å·¥å·ï¼æç»­æµéåæ¯è¾T2Iæ¨¡åççå®è½åï¼æ¨å¨é¢ååå±ã
*   éè¿å¬å¼æ°æ®éãåºååè¯ä¼°ä»£ç ï¼éä½ç ç©¶é¨æ§ï¼ä¿è¿å¨çç ç©¶äººåå±åå¼åæ´æºè½ãæ´å¼ºå¤§ççææ¨¡åã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥FLUX-Reason-6Mæ°æ®éåPRISM-Benchåºåï¼ä¸ºè§£å³T2Iæ¨¡åå¨æ¨çè½ååè¯ä¼°æ¹é¢çæ ¸å¿é®é¢æä¾äºå¨é¢çè§£å³æ¹æ¡ï¼å¹¶ä¸ºæªæ¥æ¨çå¯¼åçT2Içæç ç©¶å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).</li>
<li>PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09680v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09680v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09676v1'></a></p>
<h2 id="spatialvid-a-large-scale-video-dataset-with-spatial-annotations"><a href="https://arxiv.org/abs/2509.09676v1">SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</a></h2>
<p><strong>Authors:</strong> Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, Xiaoxiao Long, Hao Zhu, Zhaoxiang Zhang, Xun Cao, Yao Yao</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Jiahao Wangç­äººæ°åçè®ºæâSpatialVID: A Large-Scale Video Dataset with Spatial Annotationsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼SpatialVID: ä¸ä¸ªå¸¦æç©ºé´æ æ³¨çå¤§è§æ¨¡è§é¢æ°æ®é</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åç©ºé´æºè½é¢åï¼åæ¬ç©ºé´éå»ºåä¸çæ¢ç´¢ï¼çæ¨¡åå¨å¯æ©å±æ§åçå®ä¸çä¿çåº¦æ¹é¢åå°é«è´¨éãå¤§è§æ¨¡è®­ç»æ°æ®ç¨ç¼ºçä¸¥ééå¶ãç°ææ°æ®éè½ç¶æä¾ç¸æºå§¿æä¿¡æ¯ï¼ä½å¨è§æ¨¡ãå¤æ ·æ§åæ æ³¨ä¸°å¯æ§ä¸ä¸è¶³ï¼å°¤å¶ç¼ºä¹çå®ä¸çå¨æåºæ¯çå°é¢çå®ç¸æºè¿å¨æ°æ®ãè¿é»ç¢äºæ¨¡åæ³åè½ååæ§è½çæåã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäº <strong>SpatialVID</strong>ï¼ä¸ä¸ªå¤§è§æ¨¡çéå¤è§é¢æ°æ®éï¼å¶ä¸»è¦åæ°åæ¹æ³å­¦è´¡ç®åæ¬ï¼
*   <strong>å¤§è§æ¨¡æ°æ®æ¶éä¸å¤çï¼</strong> æ¶éäºè¶è¿21,000å°æ¶çåå§è§é¢ï¼å¹¶éè¿åå±è¿æ»¤ç®¡éå¤çæ270ä¸ä¸ªè§é¢çæ®µï¼æ»è®¡7,089å°æ¶çå¨æåå®¹ã
*   <strong>å¨é¢çå ä½æ æ³¨ï¼</strong> ä¸ºæ¯ä¸ªè§é¢çæ®µæä¾äºéå¸§çç¸æºå§¿æãæ·±åº¦å¾åå¨ææ©ç ç­å¯éç3Dæ æ³¨ãè¿äºæ æ³¨éè¿è°æ´åçMegaSaMç®¡éçæï¼ç¡®ä¿äºé«ç²¾åº¦åé²æ£æ§ã
*   <strong>ç©ºé´æç¥å­å¹åè¿å¨æä»¤ï¼</strong> æ°æ®ééè¿ä¸ä¸ªåæ°çæ æ³¨ç®¡éï¼å°è§è§è¯­è¨æ¨¡åï¼VLLMsï¼ä¸å¤§åè¯­è¨æ¨¡åï¼LLMsï¼ç»åï¼çæç»æåå­å¹ï¼æ´åäºåºæ¯æè¿°ãç¸æºè¿å¨ç»èä»¥åå¤©æ°ãåç§ãæ¶é´ç­åå±è¯­ä¹å±æ§ãæ­¤å¤ï¼è¿ä»ç¸æºè½¨è¿¹ä¸­æåäºåºååçè¿å¨æä»¤ï¼ä¸ºå¯¼èªç¸å³æ¨¡åè®­ç»æä¾ç²¾ç¡®çç£ã
*   <strong>é«è´¨éå­éï¼SpatialVID-HQï¼ï¼</strong> éè¿è¿ä¸æ­¥çè¿æ»¤åéæ ·ï¼åå»ºäºä¸ä¸ª1,146å°æ¶çå¹³è¡¡å­éSpatialVID-HQï¼ä¼åäºæ¨¡åè®­ç»åè¯ä¼°çé²æ£æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ°æ®ä¸°å¯æ§ä¸å¤æ ·æ§ï¼</strong> å¯¹SpatialVIDæ°æ®ç»è®¡çåæè¡¨æï¼è¯¥æ°æ®éå¨åºæ¯ç±»åãç¸æºè¿å¨æ¨¡å¼ãè¯­ä¹å±æ§ç­æ¹é¢å·ææ¾èçä¸°å¯æ§åå¤æ ·æ§ãä¾å¦ï¼ç¸æºè¿å¨æ¹ååå¸æ´åè¡¡ï¼è½¨è¿¹è½¬å¼¯ç±»åæ´å¤æ ·ã
*   <strong>è¶è¶ç°ææ°æ®éï¼</strong> ä¸Panda-70Mç­ç°ææ°æ®éçå¯¹æ¯æ¾ç¤ºï¼SpatialVIDå¨è§é¢è´¨éï¼ç¾å­¦ãäº®åº¦ãè¿å¨ï¼åç¸æºè¿å¨ç»è®¡ï¼æè½¬è§åº¦ãç§»å¨è·ç¦»ãè½¨è¿¹è½¬å¼¯ï¼æ¹é¢è¡¨ç°åºæ´é«çè´¨éåæ´åè¡¡çåå¸ï¼å°¤å¶å¨å¨æåå®¹åå ä½æ æ³¨æ¹é¢æ¾èä¼è¶ã
*   <strong>ä¿è¿æ¨¡åæ³åï¼</strong> SpatialVIDçä¸°å¯æ§åå¤æ ·æ§ç´æ¥ä¿è¿äºæ¨¡åæ³åè½ååæ§è½çæåï¼ä½¿å¶æä¸ºè§é¢å3Dè§è§ç ç©¶ç¤¾åºçå³é®èµäº§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>MegaSaMçå±éæ§ï¼</strong> å°½ç®¡MegaSaMå·æåè¿çè½åï¼ä½å¨æç«¯æåµä¸ï¼å¦ç§»å¨ç©ä½ä¸»å¯¼è§éæç¸æºä¸ç©ä½è¿å¨å±çº¿ï¼ä»å­å¨å±éæ§ãå®ä¹æªè¢«è®¾è®¡ç¨äºå¤çå¯åç¦è·ææ¾èå¾åç¸åï¼å¹¶ä¸å¶æ§è½åå¤é¨åç®æ·±åº¦æ¨¡åçå½±åã
*   <strong>VLLMså¨ç©ºé´ä¿¡æ¯æè·ä¸çå±éæ§ï¼</strong> åå§çè§è§è¯­è¨æ¨¡åå¨æè·ç©ºé´ä¿¡æ¯æ¹é¢å­å¨å±éæ§ï¼ç»å¸¸éæ¼æéè¯¯è¡¨ç¤ºå ä½ç»èã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ç©çæ¥å°ç¸æºæ§å¶è§é¢çæï¼</strong> SpatialVIDéè¿å¼å¥æ´å¼ºç3Då½çº³åå·®ï¼ä¸ºç©çæ¥å°ç¸æºæ§å¶è§é¢çæãå¨æåºæ¯åæåå·èº«æºè½ä½ç­æªæ¥ç ç©¶å¥ å®äºåºç¡ã
*   <strong>å¤æäº¤äºæ¨¡æï¼</strong> éè¿å°æ¾å¼3Dè¿å¨æ§å¶ä¸å¤§è§æ¨¡ææ¬è¯­ä¹ç¸ç»åï¼SpatialVIDä¸ºæªæ¥è½å¤æ¨¡æçå®ä¸çåºæ¯ä¸­å¤æäº¤äºçå·¥ä½æä¾äºåºç¡ã
*   <strong>éææ´åè¿çç¸æºå§¿æä¼°è®¡å¨ï¼</strong> è®ºææå°ï¼æªæ¥çç¸æºå§¿æä¼°è®¡å¨ï¼å¦ViPEï¼çéæå°è¿ä¸æ­¥æåæ æ³¨ç®¡éçæ§è½ã</p>
<hr />
<p>æ»èè¨ä¹ï¼SpatialVIDæ°æ®ééè¿æä¾å¤§è§æ¨¡ãé«è´¨éçéå¤è§é¢ï¼å¹¶è¾ä»¥å¯éçéå¸§å ä½ï¼ç¸æºå§¿æãæ·±åº¦å¾ãå¨ææ©ç ï¼åè¯­ä¹ï¼ç»æåå­å¹ãè¿å¨æä»¤ï¼æ æ³¨ï¼ææå¼¥è¡¥äºç°ææ°æ®éå¨è§æ¨¡ãå¤æ ·æ§åç©ºé´ä¿¡æ¯ä¸°å¯æ§ä¸çä¸è¶³ãè¿å¯¹äºæ¨å¨ç©ºé´æºè½ã3Déå»ºãè§é¢çæåä¸çæ¨¡åç­é¢åçåå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions.</li>
<li>Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09676v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09676v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09584v1'></a></p>
<h2 id="visual-grounding-from-event-cameras"><a href="https://arxiv.org/abs/2509.09584v1">Visual Grounding from Event Cameras</a></h2>
<p><strong>Authors:</strong> Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºLingdong Kongç­äººæ°åçè®ºæâVisual Grounding from Event Camerasâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºæé¢ç®ï¼</strong> Visual Grounding from Event Cameras
<strong>ä½èï¼</strong> Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau</p>
<p><strong>æè¦ï¼</strong></p>
<ol>
<li>
<p><strong>ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
    å°½ç®¡äºä»¶ç¸æºå¨å¤çå¨æåºæ¯ãè¿å¨æ¨¡ç³åæææ§åç§æ¹é¢å·ææ¾èä¼å¿ï¼ä½å¶ä¸èªç¶è¯­è¨çè§£çç»åå´é²æç ç©¶ï¼å¯¼è´å¤æ¨¡ææç¥é¢åå­å¨ç©ºç½ãæ¬ææ¨å¨è§£å³è¿ä¸é®é¢ï¼å³å¦ä½å©ç¨èªç¶è¯­è¨æè¿°ï¼å¨äºä»¶ç¸æºæè·çå¨æåºæ¯ä¸­åç¡®å°å®ä½ï¼âè§è§æ¥å°âï¼ç©ä½ã</p>
</li>
<li>
<p><strong>å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong></p>
<ul>
<li><strong>å¼å¥Talk2Eventæ°æ®éï¼</strong> è®ºæé¦æ¬¡æåºäºä¸ä¸ªå¤§è§æ¨¡çãåºäºäºä»¶æ°æ®çè¯­è¨é©±å¨ç©ä½è§è§æ¥å°åºåæ°æ®éââTalk2Eventãè¯¥æ°æ®éåºäºçå®çé©¾é©¶åºæ¯ï¼åå«5,567ä¸ªåºæ¯ã13,458ä¸ªæ æ³¨ç©ä½åè¶è¿30,000ä¸ªç»è¿ä»ç»éªè¯çæä»£è¡¨è¾¾å¼ã</li>
<li><strong>å±æ§ä¸­å¿çè®¾è®¡ï¼</strong> Talk2Eventæ°æ®éçæ¯ä¸ªæä»£è¡¨è¾¾å¼é½éè¿åä¸ªç»æåå±æ§è¿è¡ä¸°å¯ï¼<ul>
<li><strong>å¤è§ (Appearance)ï¼</strong> ææéæåºæ¯åç©ä½ç¹æ§ï¼å¦ç±»å«ãå½¢ç¶ãå¤§å°ãé¢è²ï¼ã</li>
<li><strong>ç¶æ (Status)ï¼</strong> ææå¨ææ¹é¢ï¼å¦ç©ä½æ¯å¦ç§»å¨ãåæ­¢ãè½¬å¼¯æç©¿è¿ï¼ã</li>
<li><strong>ä¸è§å¯èçå³ç³» (Relation-to-Viewer)ï¼</strong> ææç©ä½ç¸å¯¹äºè§å¯èçèªæä¸­å¿ä½ç½®ï¼å¦åæ¹ãå·¦ä¾§ãè¿å¤æé¢åèªè½¦ï¼ã</li>
<li><strong>ä¸å¨å´ç©ä½çå³ç³» (Relation-to-Others)ï¼</strong> ææç©ä½ä¸å¨å´ç©ä½ä¹é´çä¸ä¸æå³ç³»ï¼å¦ç©ºé´å¸å±æç¾¤ä½è¡ä¸ºï¼ã</li>
</ul>
</li>
<li><strong>å¤æ¨¡ææ°æ®æ´åï¼</strong> Talk2Eventä¸ä»åå«äºä»¶æµæ°æ®ï¼è¿åæ­¥æä¾äºRGBå¸§ï¼æ¯æä¸ç§äºè¡¥çè¯ä¼°è®¾ç½®ï¼ä»ä½¿ç¨äºä»¶ä½ç´ ãä»ä½¿ç¨ä¼´éå¸§ä»¥åç»åä¸¤ç§æ°æ®æºè¿è¡å¤æ¨¡ææ¥å°ã</li>
<li><strong>æ°æ®æ æ³¨æµç¨ï¼</strong> è®ºæè®¾è®¡äºä¸ä¸ªä¸ä¸ææç¥çæç¤ºç­ç¥ï¼ç»åLLMè¾å©è§£æåäººå·¥éªè¯ï¼çæå¹¶ç²¾ç¼äºè¯­è¨ä¸°å¯çæè¿°ï¼ç¡®ä¿äºæ æ³¨çåç¡®æ§åå¤æ ·æ§ã</li>
</ul>
</li>
<li>
<p><strong>ä¸»è¦ç»æåå¶æä¹ï¼</strong></p>
<ul>
<li><strong>å®ç°å¯è§£éåç»åå¼æ¥å°ï¼</strong> è¿ç§å±æ§ä¸­å¿çè®¾è®¡ä½¿å¾è§è§æ¥å°ä»»å¡ä¸ä»éäºç®åçç©ä½è¯å«ï¼èæ¯è½å¤è¿è¡æ´æ·±å±æ¬¡çä¸ä¸ææ¨çï¼ä»èå¨å¨æç¯å¢ä¸­å®ç°å¯è§£éåç»åå¼çç©ä½å®ä½ã</li>
<li><strong>å¡«è¡¥å¤æ¨¡ææç¥ç©ºç½ï¼</strong> Talk2Eventæ°æ®éçå¼å¥ï¼é¦æ¬¡å°äºä»¶ç¸æºæ°æ®ä¸èªç¶è¯­è¨çè§£ç¸ç»åï¼ä¸ºç ç©¶å¤æ¨¡æãæ¶é´æç¥åæç¥æä¾äºä¸ä¸ªç¬ç¹çå¹³å°ã</li>
<li><strong>æ¨å¨äºä»¶è§è§ç ç©¶ï¼</strong> è¯¥æ°æ®éä¸ºæªæ¥å¨æºå¨äººãäººæºäº¤äºç­åºç¨ä¸­ï¼å©ç¨äºä»¶ç¸æºæ°æ®è¿è¡æ´ç²¾ç»ãæ´é²æ£çç©ä½æ¥å°ç ç©¶å¥ å®äºåºç¡ï¼å°¤å¶æ¯å¨è¿å¨æ¨¡ç³åä½åç§ç­ä¼ ç»ç¸æºé¾ä»¥åºå¯¹çåºæ¯ä¸­ã</li>
</ul>
</li>
<li>
<p><strong>å±éæ§ï¼</strong></p>
<ul>
<li>è®ºæä¸­å¹¶æªæç¡®æåTalk2Eventæ°æ®éæ¬èº«çå±éæ§ãç¶èï¼ä½ä¸ºé¦ä¸ªæ­¤ç±»æ°æ®éï¼å¯è½å­å¨çæ½å¨å±éåæ¬ï¼<ul>
<li><strong>æ°æ®è§æ¨¡ï¼</strong> å°½ç®¡è¢«æè¿°ä¸ºâå¤§è§æ¨¡âï¼ä½ä¸ä¸äºæççRGBæè§é¢æ°æ®éç¸æ¯ï¼å¶è§æ¨¡å¯è½ä»ææåç©ºé´ã</li>
<li><strong>åºæ¯å¤æ ·æ§ï¼</strong> æ°æ®éä¸»è¦åºäºâçå®ä¸çé©¾é©¶åºæ¯âï¼è¿å¯è½éå¶äºå¶å¨å¶ä»éé©¾é©¶åºæ¯ï¼å¦å®¤åãå·¥ä¸ç­ï¼çæ³åè½åã</li>
<li><strong>æ æ³¨å¤ææ§ï¼</strong> å±æ§ä¸­å¿çè®¾è®¡è½ç¶æä¾äºä¸°å¯çä¿¡æ¯ï¼ä½ä¹å¢å äºæ æ³¨çå¤ææ§ï¼å¯è½å¯¹æªæ¥æ©å±æ°æ®éå¸¦æ¥ææã</li>
<li><strong>äºä»¶æ°æ®è¡¨ç¤ºï¼</strong> å°å¼æ­¥äºä»¶æµç¦»æ£åä¸ºä½ç´ å4Då¼ éï¼è½ç¶å¼å®¹ç°æéª¨å¹²ç½ç»ï¼ä½å¯è½æå¤±é¨ååå§äºä»¶æ°æ®çç²¾ç»æ¶é´åè¾¨çã</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong></p>
<ul>
<li><strong>å¤æ¨¡æèåç­ç¥ï¼</strong> æ¢ç´¢æ´ææçäºä»¶æ°æ®åRGBå¸§èåæ¹æ³ï¼ä»¥ååå©ç¨ä¸¤ç§æ¨¡æçä¼å¿ã</li>
<li><strong>æ¶é´æç¥åæ¨¡åï¼</strong> å¼åä¸é¨éå¯¹äºä»¶æ°æ®æ¶é´ç¹æ§è®¾è®¡çæ¨¡åï¼ä»¥æ´å¥½å°ææå¨æç©ä½çè¿å¨åç¶æååã</li>
<li><strong>ä¸ä¸ææ¨çï¼</strong> å©ç¨æ°æ®éä¸°å¯çå±æ§æ æ³¨ï¼ç ç©¶æ´é«çº§çä¸ä¸ææ¨çæºå¶ï¼å®ç°è¶è¶ç®åç©ä½è¯å«çå¤æåºæ¯çè§£ã</li>
<li><strong>é²æ£æ§åæï¼</strong> å¨è¿å¨æ¨¡ç³ãä½åç§ç­æææ§æ¡ä»¶ä¸ï¼å¯¹åºäºäºä»¶çè§è§æ¥å°æ¨¡åçé²æ£æ§è¿è¡æ·±å¥ç ç©¶ã</li>
<li><strong>å¼æ¾è¯æ±æ¥å°ï¼</strong> ç»åå¤§åè¯­è¨æ¨¡åï¼æ¢ç´¢äºä»¶ç¸æºæ°æ®çå¼æ¾è¯æ±è§è§æ¥å°è½åã</li>
<li><strong>åºç¨æå±ï¼</strong> å°Talk2Eventçææåºç¨äºæºå¨äººå¯¼èªãäººæºäº¤äºãèªå¨é©¾é©¶ç­å®éåºæ¯ã</li>
</ul>
</li>
</ol>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥Talk2Eventæ°æ®éï¼æåå°å¼¥åäºäºä»¶ç¸æºæç¥ä¸èªç¶è¯­è¨çè§£ä¹é´çé¸¿æ²ï¼ä¸ºè®¡ç®æºè§è§é¢åå¨å¨æãå¤æ¨¡æåæ¶é´æç¥åç©ä½æ¥å°æ¹é¢å¼è¾äºæ°çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09584v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09584v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09555v1'></a></p>
<h2 id="interact-advancing-large-scale-versatile-3d-human-object-interaction-generation"><a href="https://arxiv.org/abs/2509.09555v1">InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</a></h2>
<p><strong>Authors:</strong> Sirui Xu, Dongting Li, Yucheng Zhang, Xiyan Xu, Qi Long, Ziyin Wang, Yunzhi Lu, Shuchang Dong, Hezi Jiang, Akshat Gupta, Yu-Xiong Wang, Liang-Yan Gui</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Sirui Xuç­äººæ°åçè®ºæâInterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="interact-3d-">è®ºææè¦ï¼InterAct: æ¨è¿å¤§è§æ¨¡å¤åè½3Däºº-ç©äº¤äºçæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½å3Däºº-ç©äº¤äºï¼HOIï¼çæé¢åé¢ä¸´çæ ¸å¿ææãå°½ç®¡å¤§è§æ¨¡äººä½å¨ä½æææ°æ®éå·²æ¾èæ¨å¨äºäººä½å¨ä½çæçåå±ï¼ä½å¯¹å¨æ3D HOIçå»ºæ¨¡åçæä»ç¶åæ»¡ææãç°ææ°æ®éæ®éå­å¨å±éæ§ï¼åæ¬ç¼ºä¹å¹¿æ³ãé«è´¨éçå¨ä½åæ æ³¨ï¼ä»¥åå­å¨æ¥è§¦ç©¿éãæµ®ç©ºåä¸æ­£ç¡®æé¨å¨ä½ç­ä¼ªå½±ï¼è¿äºé®é¢ä¸¥éé»ç¢äºçå®æHOIççæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
InterActéè¿ä»¥ä¸ä¸ä¸ªä¸»è¦åæ°ç¹æ¥è§£å³ä¸è¿°é®é¢ï¼</p>
<ul>
<li>
<p><strong>å¤§è§æ¨¡ãé«è´¨éæ°æ®éæå»ºï¼</strong></p>
<ul>
<li><strong>æ°æ®æ´åä¸æ ååï¼</strong> è®ºæé¦åæ´åå¹¶æ ååäºæ¥èªä¸åæ¥æºç21.81å°æ¶HOIæ°æ®ï¼å¹¶è¾ä»¥è¯¦ç»çææ¬æ æ³¨ï¼æ¾èæåäºæ°æ®éçè§æ¨¡åä¸°å¯åº¦ã</li>
<li><strong>ç»ä¸ä¼åæ¡æ¶æåæ°æ®è´¨éï¼</strong> æåºäºä¸ç§ç»ä¸çä¼åæ¡æ¶ï¼éè¿åå°ä¼ªå½±åæ ¡æ­£æé¨å¨ä½æ¥æé«æ°æ®è´¨éãè¯¥æ¡æ¶åä¸ä¸ªé¡ºåºæ­¥éª¤ï¼å¨èº«æ ¡æ­£ãæé¨æ ¡æ­£åäº¤äºå¢å¼ºï¼æ¨å¨è§£å³ç©¿éãæµ®ç©ºåä¸èªç¶æé¨å§¿æç­é®é¢ã</li>
<li><strong>æ¥è§¦ä¸åæ§åççæ°æ®å¢å¼ºï¼</strong> å©ç¨æ¥è§¦ä¸åæ§åçï¼åå¨ä½éåææ¯å¯åï¼ï¼éè¿å¨ä¿æç©ä½æ¥è§¦ä¸è´æ§çåæ¶å¼å¥äººä½å¨ä½ååï¼çæäºé¼ççåææ°æ®ãè¿ä½¿å¾æ°æ®éæ©å±å°30.70å°æ¶ï¼å¹¶æ¾èæåäºçææ¨¡åçæ§è½ã</li>
</ul>
</li>
<li>
<p><strong>ç»ä¸çHOIçæå»ºæ¨¡è§è§ä¸åºåä»»å¡ï¼</strong></p>
<ul>
<li>å®ä¹äºå­ä¸ªå³é®çHOIçæåºåä»»å¡ï¼ææ¬å°äº¤äºãå¨ä½å°äº¤äºãç©ä½å°äººãäººå°ç©ä½ãäº¤äºé¢æµåäº¤äºæ¨¡ä»¿ã</li>
<li>æåºäºä¸ç§ç»ä¸çå»ºæ¨¡åè¡¨ç¤ºæ¹æ³ï¼ç¨äºè¿å¨çæä»»å¡ï¼å¹¶å©ç¨å¤ä»»å¡å­¦ä¹ èåå»ºæ¨¡è¿å¨åæ¥è§¦ï¼å®ç°äºæåè¿çæ§è½ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ°æ®è´¨éæ¾èæåï¼</strong> å¹¿æ³çå®éªéªè¯äºInterActæ°æ®éä½ä¸ºæ¨è¿3D HOIçæçåºç¡èµæºçå®ç¨æ§ãä¼åæ¡æ¶æ¾èæ¹åäºåå§MoCapæ°æ®çè´¨éï¼åå°äºç©¿éä¼ªå½±ï¼å¹¶å¢å¼ºäºäºº-ç©æ¥è§¦ã
*   <strong>åææ°æ®çé«è´¨éï¼</strong> å¢å¼ºåçåææ°æ®è´¨éä¸æ ¡æ­£åçæ°æ®ç¸å½ï¼ä¸ä¼äºåå§æ°æ®éï¼è¡¨æäºæ°æ®å¢å¼ºæ¹æ³çæææ§ã
*   <strong>æåè¿çæ§è½ï¼</strong> å¨å­ä¸ªHOIçæåºåä»»å¡ä¸ï¼InterActå®ç°äºæåè¿çæ§è½ï¼å°¤å¶æ¯å¨ææ¬æ¡ä»¶äº¤äºçæä»»å¡ä¸­ï¼éè¿å¼å¥æ¥è§¦å»ºæ¨¡åBPSç¼ç ï¼æ¾èæé«äºçæHOIçè´¨éåFIDåæ°ã
*   <strong>æ è®°ç¹è¡¨ç¤ºçæææ§ï¼</strong> å®éªè¡¨æï¼åºäºæ è®°ç¹çè¡¨ç¤ºæ¹æ³å¨åå°ä¼ªå½±æ¹é¢ä¼äºå¶ä»äººä½è¡¨ç¤ºæ¹æ³ã
*   <strong>å¤ä»»å¡å­¦ä¹ çä¼å¿ï¼</strong> ç»ä¸çå¤ä»»å¡å­¦ä¹ æ¹æ³æ¾èæåäºæ¨¡åæ§è½ï¼å°¤å¶æ¯å¨äººå°ç©ä½çæåäº¤äºé¢æµä»»å¡ä¸­ã
*   <strong>å¤§è§æ¨¡æ°æ®ççå¤ï¼</strong> å®éªç»æè¯å®ï¼æ´å¤§è§æ¨¡çæ°æ®éï¼InterAct-Xï¼æ¯æè®­ç»æ¨¡åå®ç°æ´å¥½çæ§è½ï¼åæäºæéæ°æ®å¯¼è´çè¿æåé®é¢ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è§æ¨¡éå¶ï¼</strong> å°½ç®¡InterActæ°æ®éå°ç©ä½æ°éæ©å±å°217ä¸ªï¼è¿è¶ç°æHOIæ°æ®éï¼ä½ä»å­å¨è§æ¨¡éå¶ï¼æªè½å®å¨è¦çç°å®ä¸çä¸­éå°çåç§ç©ä½ç±»å«ã
*   <strong>æ³åè½åï¼</strong> å°½ç®¡æ¨¡åå¯¹æ°æ®éèå´åçåå¸å¤ç©ä½è¡¨ç°åºæ³åè½åï¼ä½å®ç°å¯¹æ´å¹¿æ³çæªè§ç©ä½çé²æ£æ³åä»éè¿ä¸æ­¥æ©å±ã
*   <strong>å»åªåæ ¡æ­£çææï¼</strong> å¨å¤çåå§æ°æ®ä¸­å­å¨çæ¾èåªå£°æ¶ï¼å»åªåæ ¡æ­£å¨èº«HOIæ°æ®ä»é¢ä¸´åºæææãä¾å¦ï¼åå§æ°æ®ä¸­çæµ®ç©ºç©ä½é®é¢å¯è½é¾ä»¥å®å¨æ ¡æ­£ï¼å°¤å¶æ¯å¨ç»ä¸è¶åæ°éç½®ä¸ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ°æ®éæ©å±ï¼</strong> æªæ¥å·¥ä½éè¦è¿ä¸æ­¥æ©å±æ°æ®éï¼ä»¥è¦çæ´å¹¿æ³çç©ä½ç±»å«åæ´ä¸°å¯çç°å®ä¸çäº¤äºï¼ä»èæåæ¨¡åçæ³åè½åã
*   <strong>ä¼åæ¡æ¶æ¹è¿ï¼</strong> æ¹è¿ä¼åæ¡æ¶ï¼ä½¿å¶è½å¤æ´ææå°å¤çåå§æ°æ®ä¸­çæç«¯åªå£°åä¼ªå½±ï¼ä¾å¦éè¿èªéåºè¶åæ°éç½®æ¥è§£å³ç¹å®é®é¢ã
*   <strong>éç§åä¼¦çèéï¼</strong> å°½ç®¡è®ºæå·²éåæªæ½ï¼å¦å°æ°æ®è½¬æ¢ä¸ºSMPLææ è®°ç¹è¡¨ç¤ºï¼æ¥åå°è¯å«ç»èï¼ä½æªæ¥ç ç©¶ä»éæç»­å³æ³¨æ°æ®æ¶éåçæä¸­çéç§åä¼¦çé®é¢ï¼ç¡®ä¿æ°æ®ä½¿ç¨ç¬¦åéå¾·è§èã</p>
<hr />
<p>æ»èè¨ä¹ï¼InterActè®ºæéè¿æå»ºä¸ä¸ªå¤§è§æ¨¡ãé«è´¨éãå¤åè½ç3D HOIåºåæ°æ®éï¼å¹¶æåºåæ°çæ°æ®ä¼ååå¢å¼ºæ¹æ³ï¼ä»¥åç»ä¸ççæå»ºæ¨¡è§è§ï¼ä¸º3Däºº-ç©äº¤äºçæé¢åååºäºéè¦è´¡ç®ãè¯¥å·¥ä½ä¸ä»è§£å³äºç°ææ°æ®éçå±éæ§ï¼è¿ä¸ºæªæ¥çç ç©¶å¥ å®äºåå®çåºç¡ï¼æææ¨å¨æºå¨äººãå¨ç»åè®¡ç®æºè§è§ç­é¢åçè¿ä¸æ­¥åå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements.</li>
<li>Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions.</li>
<li>Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09555v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09555v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09429v1'></a></p>
<h2 id="semantic-concentration-for-self-supervised-dense-representations-learning"><a href="https://arxiv.org/abs/2509.09429v1">Semantic Concentration for Self-Supervised Dense Representations Learning</a></h2>
<p><strong>Authors:</strong> Peisong Wen, Qianqian Xu, Siran Dai, Runmin Cong, Qingming Huang</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Peisong Wenç­äººæ°åçè®ºæâSemantic Concentration for Self-Supervised Dense Representations Learningâçå¨é¢æè¦ã</p>
<hr />
<h3 id="semantic-concentration-for-self-supervised-dense-representations-learning_1">è®ºææè¦ï¼Semantic Concentration for Self-Supervised Dense Representations Learning</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³èªçç£å­¦ä¹ ï¼SSLï¼å¨å­¦ä¹ <strong>å¯éè¡¨ç¤ºï¼dense representationsï¼</strong>æ¶é¢ä¸´çææï¼ç¹å«æ¯éå¯¹å¾åä¸­<strong>è¡¥ä¸ï¼patchesï¼</strong>çè¡¨ç¤ºå­¦ä¹ ãå°½ç®¡å¾åçº§SSLåå¾äºæ¾èè¿å±ï¼ä½å¨å¯éä»»å¡ï¼å¦è¯­ä¹åå²åç®æ æ£æµï¼ä¸­ï¼ç°ææ¹æ³å­å¨ä¸ä¸ª<strong>âè¿åæ£ï¼over-dispersionï¼âç°è±¡</strong>ï¼æ¥èªåä¸å®ä¾/ç±»å«çè¡¥ä¸å¨åµå¥ç©ºé´ä¸­åæ£ï¼ä»èæå®³äºä¸æ¸¸å¯éä»»å¡çæ§è½ãè®ºæçæ ¸å¿é®é¢æ¯ï¼å¦ä½ä¸ºå¯éSSLå¼å¥<strong>è¯­ä¹éä¸­ï¼semantic concentrationï¼</strong>æºå¶ï¼ä»¥åæè¿åæ£é®é¢å¹¶æé«è¡¨ç¤ºçç»ç²åº¦å¯¹é½è½åï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªåºäºèªè¸é¦ï¼self-distillationï¼çå¯éSSLæ¡æ¶ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>æ­ç¤ºå¾åçº§SSLçéå¼è¯­ä¹éä¸­æºå¶ï¼</strong> ä½èåæäºå¾åçº§SSLå¦ä½éè¿âéä¸¥æ ¼ç©ºé´å¯¹é½âï¼ç¡®ä¿å®ä¾åä¸è´æ§ï¼åâå±äº«æ¨¡å¼âï¼ç¡®ä¿å¾åé´ä¸è´æ§ï¼æ¥é¿åè¿åæ£ï¼ä»èå®ç°éå¼è¯­ä¹éä¸­ã</li>
<li><strong>å¼å¥è¡¥ä¸å¯¹åºè¸é¦ï¼Patch Correspondence Distillationï¼ä¸åªå£°å®¹å¿æåºæå¤±ï¼Noise-Tolerant Ranking Lossï¼ï¼</strong> ä¸ºäºæç ´å¯éSSLä¸­ä¸¥æ ¼çç©ºé´å¯¹é½éå¶ï¼è®ºææåºè¸é¦å¾åå¯¹ä¹é´çè¡¥ä¸å¯¹åºå³ç³»ãéå¯¹ä¼ªæ ç­¾çåªå£°åä¸å¹³è¡¡é®é¢ï¼ä½èå°ä¼ ç»çAverage Precision (AP) æå¤±æ©å±å°<strong>è¿ç»­ç®æ å¹³åç²¾åº¦ï¼Continuous-Target Average Precision, CoTAPï¼æå¤±</strong>ãCoTAPæå¤±å·æå³ç­æ å³åèªéåºèç¦ç¹æ§ï¼è½ææå¤çåªå£°åä¸å¹³è¡¡çä¼ªæ ç­¾ï¼é²æ­¢æ¨¡åè¢«è¯¯å¯¼ã</li>
<li><strong>æåºå¯¹è±¡æç¥è¿æ»¤å¨ï¼Object-Aware Filter, OAFï¼ï¼</strong> ä¸ºäºå¨å¤æåºæ¯ä¸­åºåå±äº«æ¨¡å¼ï¼è®ºæå¼å¥äºOAFæ¨¡åãè¯¥æ¨¡åéè¿<strong>äº¤åæ³¨æåï¼cross-attentionï¼</strong>æºå¶ï¼å©ç¨å¯å­¦ä¹ ç<strong>å¯¹è±¡ååï¼object prototypesï¼</strong>å°è¡¥ä¸ç¹å¾æ å°å°å¯¹è±¡åºç©ºé´ãè¿æå©äºæ¶é¤åºæ¯å¹²æ°ï¼æ´ææå°æè·ç»ç²åº¦çå±äº«æ¨¡å¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¨å¤ç§ä»»å¡ä¸çå®è¯ç ç©¶ï¼éªè¯äºæææ¹æ³çæææ§ï¼</p>
<ul>
<li><strong>å¾åçº§åå¯éä»»å¡çæ§è½æåï¼</strong> å¨ImageNet-1kä¸çå¾ååç±»ä»»å¡ä¸­ï¼æææ¹æ³å°Top-1åç¡®çæé«äº1.1%~4.2%ãå¨COCOStuff-27ä¸çè¯­ä¹åå²ä»»å¡ä¸­ï¼mIoUæé«äº2.5%~10.8%ã</li>
<li><strong>æ³åè½åå¼ºï¼</strong> å¨è¯­ä¹åå²ãç®æ æ£æµãå®ä¾åå²åè§é¢ç®æ åå²ç­å¤ç§ä¸æ¸¸å¯éä»»å¡ä¸ï¼æææ¹æ³ååå¾äºæç»­çæ§è½æåï¼è¯æäºå¶çæçå¯éè¡¨ç¤ºå·æè¯å¥½çæ³åè½åã</li>
<li><strong>ç»ç²åº¦è¡¨ç¤ºçæ¹è¿ï¼</strong> å°¤å¶å¨ç®æ æ£æµçå°ç©ä½æ§è½ä¸ï¼CoTAPæ¹æ³æ¾ç¤ºåºæ¾èä¼å¿ï¼è¡¨æå¶è½çææ´ç»ç²åº¦çè¡¨ç¤ºã</li>
<li><strong>è¯­ä¹éä¸­æºå¶çæææ§ï¼</strong> æ¶èå®éªè¡¨æï¼è¯­ä¹éä¸­æå¤±ï¼CoTAPæå¤±ï¼åå¯¹è±¡æç¥è¿æ»¤å¨ï¼OAFï¼æ¨¡åå¯¹æ§è½æåè³å³éè¦ï¼å°¤å¶å¨å¤çåªå£°åå¤æåºæ¯æ¹é¢ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æå°çä¸é¡¹å±éæ§æ¯ï¼
*   <strong>å¯¹å¯¹è±¡ä¸­å¿æ°æ®çä¾èµï¼</strong> ä¸ºäºæåå¹²åçå¯¹è±¡ååï¼æææ¹æ³éè¦å¯¹è±¡ä¸­å¿æ°æ®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºæªæ¥çç ç©¶æ¹åï¼
*   <strong>èªéåºæåå¯¹è±¡ååï¼</strong> æªæ¥å·¥ä½å°æ¢ç´¢å¦ä½ä»éç­å±ï¼uncuratedï¼å¾åä¸­èªéåºå°æåå¯¹è±¡ååï¼ä»¥éåºå¤§è§æ¨¡æ°æ®ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥æ¾å¼è¯­ä¹éä¸­æºå¶ï¼æåè§£å³äºèªçç£å­¦ä¹ å¨å¯éè¡¨ç¤ºå­¦ä¹ ä¸­é¢ä¸´çè¿åæ£é®é¢ãCoTAPæå¤±åOAFæ¨¡åçç»åï¼ä½¿å¾æ¨¡åè½å¤æ´å¥½å°å¤çåªå£°ä¼ªæ ç­¾ãåºåå¤æåºæ¯ä¸­çå±äº«æ¨¡å¼ï¼å¹¶æç»çææ´å·ç»ç²åº¦åæ³åè½åçè¡¨ç¤ºï¼ä¸ºå¯éè§è§ä»»å¡çèªçç£å­¦ä¹ å¼è¾äºæ°éå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>First, to break the strict spatial alignment, we propose to distill
the patch correspondences.</li>
<li>Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.</li>
<li>Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09429v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09429v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09427v1'></a></p>
<h2 id="fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-image-fusion-and-super-resolution"><a href="https://arxiv.org/abs/2509.09427v1">FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</a></h2>
<p><strong>Authors:</strong> Yuchan Jie, Yushen Xu, Xiaosong Li, Fuqiang Zhou, Jianming Lv, Huafeng Li</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yuchan Jieç­äººæ°åçè®ºæâFS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolutionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="fs-diff-semantic-guidance-and-clarity-aware-simultaneous-multimodal-image-fusion-and-super-resolution_1">è®ºææè¦ï¼FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åå¤æ¨¡æå¾åèåï¼MMIFï¼åè¶åè¾¨çææ¯å¨å®éåºç¨ä¸­é¢ä¸´ææï¼å°¤å¶æ¯å¨åäºä¾¦å¯åè¿è·ç¦»æ¢æµç­åºæ¯ä¸­ãè¿äºåºæ¯ä¸çå¤æ¨¡æå¾åï¼å¦çº¢å¤åå¯è§åå¾åï¼å¾å¾åè¾¨çä½ãè¯­ä¹ä¿¡æ¯å¼±ï¼å¯¼è´ç®æ åèæ¯ç»æå®¹æåæï¼ç°ææ¹æ³é¾ä»¥èªéåºå°å¤çä½åè¾¨çå¾åï¼ä¸æ æ³ææèåè¯­ä¹ä¿¡æ¯ï¼ä»èäº§çæ¬¡ä¼çèåç»æãè®ºææ¨å¨è§£å³å¦ä½åæ¶å®ç°å¾åèååè¶åè¾¨çï¼ä»¥çæå·æä¸°å¯ç»èåè¯­ä¹ä¿¡æ¯çé«åè¾¨çèåå¾åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
FS-Diffæåºäºä¸ä¸ªè¯­ä¹å¼å¯¼åæ¸æ°åº¦æç¥çèåå¾åèåä¸è¶åè¾¨çæ¹æ³ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>ç»ä¸çæ¡ä»¶çæé®é¢æ¡æ¶ï¼</strong> FS-Diffå°å¾åèååè¶åè¾¨çç»ä¸ä¸ºä¸ä¸ªæ¡ä»¶çæé®é¢ï¼éè¿è¿­ä»£å»åªè¿ç¨ä»çº¯é«æ¯åªå£°åå§åç®æ èåç»æã</li>
<li><strong>è¯­ä¹å¼å¯¼åæ¸æ°åº¦æç¥æºå¶ï¼CLSEï¼ï¼</strong> å¼å¥CLSEæºå¶ï¼ç»åæåºçCA-CLIPæ¨¡åï¼å®ç°å¯¹ä½åè¾¨çå¾åçèªéåºæç¥åè·¨æ¨¡æç¹å¾æåãCA-CLIPéè¿é¢è®­ç»çCLIPæ¶æï¼è½å¤ä»å¼æè¾å¥æ¨¡æä¸­ç²¾ç¡®æåè¯­ä¹ä¿¡æ¯ï¼å³ä½¿å¾åæ¸æ°åº¦ä¸ä¸è´ä¹è½ææå·¥ä½ãå®è¿è½é¢æµå¾åçåè¾¨çç±»åï¼æ¸æ°ææ¨¡ç³ï¼ï¼ä»èå¨åå¹å¾åæ¨¡ç³æ¶ä»æ¸æ°å¾åä¸­æåé«è´¨éè¯­ä¹ä¿¡æ¯ã</li>
<li><strong>ååç¹å¾Mambaï¼BFMï¼æ¨¡åï¼</strong> å¼å¥BFMæ¨¡åæ¥æåå¤æ¨¡æå¾åçå¨å±ç¹å¾ï¼æå»ºå¤æ¨¡ææ°æ®çèåè¡¨ç¤ºï¼å¢å¼ºæ¨¡åæåå¨å±ä¿¡æ¯åè·¨æ¨¡æç¹å¾çè½åã</li>
<li><strong>åºäºU-Netçéæºè¿­ä»£å»åªè¿ç¨ï¼</strong> å©ç¨æºå¾ååè¯­ä¹ä¿¡æ¯ä½ä¸ºæ¡ä»¶ï¼éè¿ä¿®æ¹åçU-Netç½ç»å®ç°éæºè¿­ä»£å»åªè¿ç¨ï¼è¯¥ç½ç»å¨å¤ä¸ªåªå£°æ°´å¹³ä¸è¿è¡å»åªè®­ç»ï¼ä»¥çæå·æä¸°å¯è·¨æ¨¡æç¹å¾åè¯­ä¹ä¿¡æ¯çé«åè¾¨çèåç»æã</li>
<li><strong>æ°åèªç©ºè§è§å¤åºæ¯ï¼AVMSï¼åºåæ°æ®éï¼</strong> æå»ºäºä¸ä¸ªåå«859å¯¹å¯¹é½çå¯è§ååçº¢å¤å¾åã3821ä¸ªæ æ³¨ç®æ ï¼æ¶µçç½å¤©ãå¤æãé»æåå¤æå¤©æ°æ¡ä»¶ï¼ä»¥åä¸ç§ä»¥ä¸ä¸ååºæ¯ï¼çAVMSæ°æ®éï¼éç¨äºå¾åèåãè¶åè¾¨çãè¿è·ç¦»æ£æµåè¯­ä¹åå²ä»»å¡ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çèåä¸è¶åè¾¨çæ§è½ï¼</strong> å¨å­ä¸ªå¬å±æ°æ®éåèªå»ºçAVMSæ°æ®éä¸è¿è¡çå¹¿æ³å®éªè¡¨æï¼FS-Diffå¨ä¸åæ¾å¤§åæ°ä¸åä¼äºç°ææåè¿æ¹æ³ï¼è½å¤æ¢å¤èåå¾åä¸­æ´ä¸°å¯çç»èåè¯­ä¹ä¿¡æ¯ã
*   <strong>å¨é«çº§è§è§ä»»å¡ä¸­çåºç¨ä»·å¼ï¼</strong> å¨ç®æ æ£æµåè¯­ä¹åå²ä»»å¡ä¸çå®éªä¹éªè¯äºFS-Diffççµæ´»æ§åä¼è¶æ§è½ï¼è½å¤çææ´åç¡®çåå²ç»æåé«ç½®ä¿¡åº¦çæ£æµç»æï¼å°¤å¶æ¯å¨å¤çå°ç®æ æ¶ã
*   <strong>CLSEåBFMæºå¶çæææ§ï¼</strong> æ¶èå®éªè¯æäºCLSEæºå¶åBFMæ¨¡åçååä½ç¨å¯¹æåèåæ§è½è³å³éè¦ï¼å®ä»¬åå«è´è´£èªéåºæç¥å¾åæ¸æ°åº¦åæåå¨å±è·¨æ¨¡æç¹å¾ã
*   <strong>CLSEæºå¶çé¶æ ·æ¬æ³åè½åï¼</strong> å¨æªè§è¿çæé¾åä½åè¾¨çAVMSæ°æ®éä¸ï¼CLSEæºå¶å±ç°åºä¼å¼çé¶æ ·æ¬æ³åè½åï¼æ éç¹å®è®­ç»å³å¯æåè·¨æ¡ä»¶æ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ¨¡åå¤æåº¦åè¿è¡æ¶é´ï¼</strong> FS-Diffæ¡æ¶è½ç¶å¨èåå¾åè¶åè¾¨çåèåæ¹é¢è¡¨ç°åºè²ï¼ä½ä¹å­å¨æ¨¡åå¤æåº¦é«åè¿è¡æ¶é´é¿ç­å±éæ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ä¼åU-Netç»æï¼</strong> è®¡åæ¹è¿U-Netç»æä»¥éä½æ¨¡åå¤æåº¦ï¼å¹¶è¿ä¸æ­¥å¢å¼ºFS-Diffå¿«ééåºå¤æå¤åçå®åºæ¯çè½åã
*   <strong>éæå°å®éåºç¨ï¼</strong> å°FS-Diffè¿ä¸æ­¥æ´åå°èªå¨é©¾é©¶ç³»ç»ãæ äººæºç²¾ååä¸çæºææ¯åæ äººæºåäºä½æç­ç¥ä¸­ï¼ä»¥æå±å¶å¨æºè½äº¤éãåä¸ç°ä»£ååå½é²ç§æç­é¢åçåºç¨åå®è·µã</p>
<hr />
<p>æ»èè¨ä¹ï¼FS-Difféè¿åæ°çè¯­ä¹å¼å¯¼åæ¸æ°åº¦æç¥æºå¶ï¼ç»åååç¹å¾Mambaåæ©æ£æ¨¡åï¼æåå°å°å¾åèååè¶åè¾¨çä»»å¡ç»ä¸ä¸ºä¸ä¸ªæ¡ä»¶çæé®é¢ï¼æ¾èæåäºå¤æ¨¡æå¾åå¤ççæ§è½ï¼å¹¶å¨å®éåºç¨ä¸­å±ç°åºå·¨å¤§æ½åãæ°æå»ºçAVMSæ°æ®éä¹ä¸ºç¸å³ç ç©¶æä¾äºå®è´µçåºåã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method.</li>
<li>Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09427v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09427v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09332v1'></a></p>
<h2 id="omnieva-embodied-versatile-planner-via-task-adaptive-3d-grounded-and-embodiment-aware-reasoning"><a href="https://arxiv.org/abs/2509.09332v1">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</a></h2>
<p><strong>Authors:</strong> Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan</p>
<p><strong>Published:</strong> 2025-09-11</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Yuecheng Liuç­äººæ°åçè®ºæâOmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoningâçå¨é¢æè¦ã</p>
<hr />
<h3 id="omnieva-3d-">è®ºææè¦ï¼OmniEVA: éè¿ä»»å¡èªéåº3D-æ¥å°åå·èº«æç¥æ¨çå®ç°çå·èº«å¤åè½è§åå¨</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½ååºäºå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼çå·èº«ç³»ç»å¨å·èº«æºè½æ¹é¢é¢ä¸´ä¸¤ä¸ªå³é®éå¶ï¼
*   <strong>å ä½éåºæ§å·®è·ï¼Geometric Adaptability Gapï¼ï¼</strong> ä»å¨2Dè¾å¥ä¸è®­ç»æç¡¬ç¼ç 3Då ä½æ³¨å¥çæ¨¡åï¼å¨å¤çéè¦å¼ºç©ºé´æ¨çï¼å¦ç©ä½å å ãé®æ¡å¤çã3Dåºæ¯å¯¼èªï¼çä»»å¡æ¶è¡¨ç°ä¸ä½³ï¼å ä¸ºå®ä»¬ç¼ºä¹è¶³å¤çç©ºé´ä¿¡æ¯æ2Dæ³åè½ååéã
*   <strong>å·èº«çº¦æå·®è·ï¼Embodiment Constraint Gapï¼ï¼</strong> ç°æå·¥ä½å¸¸å¿½ç¥çå®æºå¨äººçç©ççº¦æåè½åï¼å¯¼è´çæçä»»å¡è§åå¨çè®ºä¸å¯è¡ä½å¨å®è·µä¸­ä¸å¯æ§è¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäº<strong>OmniEVA</strong>ï¼ä¸ä¸ªå·èº«å¤åè½è§åå¨ï¼éè¿ä»¥ä¸ä¸¤é¡¹æ ¸å¿åæ°å®ç°åè¿çå·èº«æ¨çåä»»å¡è§åï¼
*   <strong>ä»»å¡èªéåº3Dæ¥å°æºå¶ï¼Task-Adaptive 3D Groundingï¼ï¼</strong> å¼å¥äºä¸ä¸ªé¨æ§è·¯ç±å¨ï¼gated routerï¼ï¼æ ¹æ®ä¸ä¸æéæ±å¯¹3Dèåè¿è¡æ¾å¼éæ©æ§è°èãè¿ä½¿å¾æ¨¡åè½å¤éå¯¹ä¸åçå·èº«ä»»å¡è¿è¡ä¸ä¸ææç¥ç3Dæ¥å°ï¼é¿åäºéæ3Dèåçç¼ºç¹ï¼å¹¶å¨2Då3Dæ¨çä»»å¡ä¸­å®ç°é²æ£æ§è½ã
*   <strong>å·èº«æç¥æ¨çæ¡æ¶ï¼Embodiment-Aware Reasoningï¼ï¼</strong> å°ä»»å¡ç®æ åå·èº«çº¦æå±åæ´åå°æ¨çå¾ªç¯ä¸­ãéè¿æåºç<strong>ä»»å¡åå·èº«æç¥å¼ºåå­¦ä¹ ï¼TE-GRPOï¼ç®æ³</strong>è¿è¡åè®­ç»ï¼æ¨¡åå­¦ä¹ çææ¢ç®æ å¯¼ååå¯æ§è¡çè§åå³ç­ï¼å°éç©ä½å¯ä¾æ§ãå·¥ä½ç©ºé´è¾¹çåè¿å¨å­¦éå¶ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> OmniEVAå¨8ä¸ªå·èº«æ¨çåºåæµè¯ä¸­ç7ä¸ªä¸å®ç°äºæåè¿çæ§è½ï¼è¿äºåºåæµè¯æ¶µçäºå¾åãè§é¢å3Dåºç¡çé®ç­ä»»å¡ï¼ä»¥åä»åºæ¬ç©ºé´çè§£å°å¤ç»´è¾å¥ä¸é«çº§å ä½æ¨ççå¹¿æ³ä»»å¡ã
*   <strong>å¼ºå¤§çéåºæ§åéç¨æ§ï¼</strong> å®éªç»æè¡¨æï¼OmniEVAä¸ä»å¨éç¨å·èº«æ¨çæ¹é¢è¡¨ç°åºè²ï¼èä¸å¨åç§ä¸æ¸¸åºæ¯ä¸­ä¹å±ç°åºå¼ºå¤§çè½åã
*   <strong>é²æ£åå¤åè½è§åè½åï¼</strong> å¯¹ä¸ç³»åæåºçå·èº«åºåæµè¯ï¼åæ¬åå§ä»»å¡åå¤åä»»å¡ï¼çè¯ä¼°è¯å®äºå¶é²æ£åå¤åè½è§åè½åã
*   <strong>å¨æ3Dæ¥å°æºå¶çæææ§ï¼</strong> ä¸ç¡¬ç¼ç 3Déæåæ 3Déæåºçº¿ç¸æ¯ï¼ä»»å¡èªéåº3Dæ¥å°æºå¶å¨å¤æ¨¡æåºåæµè¯ä¸­è¡¨ç°åºä¼è¶çæ§è½ï¼å¹³åæ§è½æå1.22%ï¼çªæ¾äºæ¨¡åå¨ä¸ä¸æéå½æ¶å©ç¨3Dä¿¡æ¯çåè¶éåºæ§ã
*   <strong>å·èº«æç¥æ¨ççæåï¼</strong> TE-GRPOè®­ç»æ¹æ³æ¾èæé«äºåå§æè½åºåï¼Where2Approach, Where2Fit, Where2Graspï¼åæ¶åç©çæ§è¡çä¸æ¸¸ä»»å¡ï¼Mobile Placement, Mobile Pickupï¼çæ§è½ï¼ä¾å¦å¨Mobile Placementä»»å¡ä¸­æåçæé«äº43%ï¼Easyï¼å50%ï¼Hardï¼ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ä½çº§æ§å¶ç­ç¥çæ§è½ç¶é¢ï¼</strong> å°½ç®¡TE-GRPOæé«äºå·èº«æç¥æ¨ççæ§è½ï¼ä½å¨æäºä»»å¡ï¼å¦Mobile Pickupï¼ä¸­ï¼å¦æä½çº§æ§å¶ç­ç¥æ¬èº«å­å¨æ§è½ç¶é¢ï¼å·èº«æç¥æ¨ççæææ§ä¼åå¼±ãè¿æå³çæ¨¡åçæç»æ§è¡æ§è½ä»åæåç­ç¥æ³åè½åçéå¶ã
*   <strong>ä¸æ¸¸ä»»å¡çè¯ä¼°ï¼</strong> è®ºææå°ï¼ä¸æ¸¸ä»»å¡åªéè¦ä¸ä¸ªå¯è¡çå±é¨è¿å¨åæä½è§£å³æ¹æ¡ï¼èåºåæµè¯è¦æ±çæ¯æä¼è§£ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¹è¿ä½çº§æ§å¶ç­ç¥ï¼</strong> è§£å³æåç­ç¥çæ³åè½åéå¶ï¼ä»¥åååæ¥å·èº«æç¥æ¨ççæ½åã
*   <strong>æ´å¤æçå·èº«çº¦æéæï¼</strong> æ¢ç´¢æ´ç»è´çç©ççº¦æåæºå¨äººè½åï¼ä»¥è¿ä¸æ­¥æé«è§åçå®éå¯è¡æ§ã
*   <strong>æ©å±å°æ´å¹¿æ³çå·èº«ä»»å¡ï¼</strong> å°OmniEVAçæ¡æ¶åºç¨äºæ´å¤æ ·åãæ´å¤æçå·èº«ä»»å¡ï¼åæ¬é¿æãå¤æ­¥éª¤çç§»å¨æä½åäººæºåä½åºæ¯ã
*   <strong>å®æ¶é¨ç½²åæ³åï¼</strong> è¿ä¸æ­¥ç ç©¶æ¨¡åå¨çå®ä¸çç©çç¯å¢ä¸­çå®æ¶é¨ç½²åæ³åè½åï¼ä»¥åºå¯¹å¨æåä¸å¯é¢æµçææã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥ä»»å¡èªéåº3Dæ¥å°åå·èº«æç¥æ¨çï¼ä¸ºå·èº«æºè½é¢åååºäºéè¦è´¡ç®ï¼ææå°å¼¥åäºMLLMså¨çè®ºè§åä¸å®éæºå¨äººæ§è¡ä¹é´çå·®è·ãOmniEVAçåæ°æ§å¨äºå¶å¨æå°æ´åå¤æ¨¡æä¿¡æ¯å¹¶èèç©ççº¦æï¼ä½¿å¶æä¸ºä¸ä¸ªå¼ºå¤§çãéç¨çå·èº«è§åå¨ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.</li>
<li>Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks.</li>
<li>Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09332v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09332v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-12 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
