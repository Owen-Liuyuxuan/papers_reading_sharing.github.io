<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-08 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-07/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-09/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-08">Arxiv Computer Vision Papers - 2025-10-08</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis" class="nav-link">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a>
                </li>
                <li class="nav-item">
                    <a href="#video-lmm-post-training-a-deep-dive-into-video-reasoning-with-large-multimodal-models" class="nav-link">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a>
                </li>
                <li class="nav-item">
                    <a href="#bridging-text-and-video-generation-a-survey" class="nav-link">Bridging Text and Video Generation: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#human3r-everyone-everywhere-all-at-once" class="nav-link">Human3R: Everyone Everywhere All at Once</a>
                </li>
                <li class="nav-item">
                    <a href="#egonight-towards-egocentric-vision-understanding-at-night-with-a-challenging-benchmark" class="nav-link">EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#overlap-aware-segmentation-for-topological-reconstruction-of-obscured-objects" class="nav-link">Overlap-aware segmentation for topological reconstruction of obscured objects</a>
                </li>
                <li class="nav-item">
                    <a href="#videominer-iteratively-grounding-key-frames-of-hour-long-videos-via-tree-based-group-relative-policy-optimization" class="nav-link">VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</a>
                </li>
                <li class="nav-item">
                    <a href="#continual-learning-for-image-captioning-through-improved-image-text-alignment" class="nav-link">Continual Learning for Image Captioning through Improved Image-Text Alignment</a>
                </li>
                <li class="nav-item">
                    <a href="#gaussian-embeddings-how-jepas-secretly-learn-your-data-density" class="nav-link">Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</a>
                </li>
                <li class="nav-item">
                    <a href="#kaputt-a-large-scale-dataset-for-visual-defect-detection" class="nav-link">Kaputt: A Large-Scale Dataset for Visual Defect Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-08">Arxiv Computer Vision Papers - 2025-10-08</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月7日Arxiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解该领域的重要发展。</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文执行摘要 (2025-10-07)</strong></p>
<p><strong>概述与主要趋势：</strong></p>
<p>今天的论文集展示了计算机视觉领域持续的活力和多维度发展。主要趋势包括：</p>
<ol>
<li><strong>多模态与大模型 (LMMs) 的深入探索：</strong> 多篇论文聚焦于视频与文本的结合，以及大型多模态模型在视频理解和生成中的应用，表明LMMs在视频领域的潜力正被积极挖掘。</li>
<li><strong>鲁棒性与挑战性环境：</strong> 针对低光照、夜间、遮挡等复杂场景的图像增强、理解和分割问题是研究热点，强调了模型在真实世界部署中的实用性。</li>
<li><strong>数据与基准的持续构建：</strong> 新的、大规模的、具有挑战性的数据集（如夜间、缺陷检测）的发布，为推动特定领域的研究提供了关键支撑。</li>
<li><strong>生成模型与扩散模型的演进：</strong> 扩散模型在低光照增强中的应用，以及文本到视频生成技术的综述，显示了生成模型在图像和视频合成方面的持续影响力。</li>
</ol>
<p><strong>特别显著或创新性论文：</strong></p>
<ul>
<li><strong>"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models" (Yunlong Tang et al.)</strong>：这篇论文深入探讨了LMMs在视频推理中的应用，可能揭示了如何有效利用和微调这些大型模型以实现更高级的视频理解能力，对LMMs在视频领域的实际落地具有指导意义。</li>
<li><strong>"Human3R: Everyone Everywhere All at Once" (Yue Chen et al.)</strong>：标题暗示了对通用、多场景人体姿态或行为理解的雄心，如果能实现“无处不在，无时无刻”的理解，将是人体分析领域的一大突破。</li>
<li><strong>"Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density" (Randall Balestriero et al.)</strong>：这篇论文可能从理论层面揭示了JEPAs（联合嵌入预测架构）的工作机制，特别是它们如何学习数据密度。理解这种底层机制对于优化和设计更有效的自监督学习方法至关重要，具有较高的理论价值。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>视频LMMs的后训练与微调策略：</strong> 随着LMMs的普及，如何针对视频任务进行高效的后训练和微调，以充分发挥其潜力，是一个重要的研究方向。</li>
<li><strong>夜间/低光照下的具身视觉理解：</strong> "EgoNight" 和 "Diffusion Models for Low-Light Image Enhancement" 强调了在挑战性光照条件下，特别是第一人称视角下的视觉理解需求，这对于自动驾驶、机器人等领域至关重要。</li>
<li><strong>拓扑重建与遮挡处理：</strong> "Overlap-aware segmentation for topological reconstruction of obscured objects" 关注复杂遮挡下的物体结构重建，是传统分割任务的进阶，具有较高的应用价值。</li>
<li><strong>迭代式视频关键帧定位与策略优化：</strong> "VideoMiner" 提出的通过树状组相对策略优化来迭代定位长视频关键帧的方法，为长视频理解提供了一种新颖的解决方案。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对于关注LMMs和视频理解的研究人员：</strong><ul>
<li><strong>"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models"</strong> (Yunlong Tang et al.)：深入了解LMMs在视频推理中的应用和潜力。</li>
<li><strong>"Bridging Text and Video Generation: A Survey"</strong> (Nilay Kumar et al.)：全面了解文本到视频生成领域的现状和挑战。</li>
</ul>
</li>
<li><strong>对于关注鲁棒性和挑战性环境的研究人员：</strong><ul>
<li><strong>"EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark"</strong> (Deheng Zhang et al.)：了解夜间具身视觉理解的最新进展和新基准。</li>
<li><strong>"Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis"</strong> (Eashan Adhikarla et al.)：深入了解扩散模型在低光照图像增强中的应用。</li>
</ul>
</li>
<li><strong>对于关注理论基础和自监督学习的研究人员：</strong><ul>
<li><strong>"Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density"</strong> (Randall Balestriero et al.)：探索JEPAs的底层学习机制。</li>
</ul>
</li>
<li><strong>对于关注数据集和实际应用的研究人员：</strong><ul>
<li><strong>"Kaputt: A Large-Scale Dataset for Visual Defect Detection"</strong> (Sebastian Höfer et al.)：对于工业检测和缺陷识别领域的研究人员非常有价值。</li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究方向最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></li>
<li><a href="#2510.05034v1">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></li>
<li><a href="#2510.04999v1">Bridging Text and Video Generation: A Survey</a></li>
<li><a href="#2510.06219v1">Human3R: Everyone Everywhere All at Once</a></li>
<li><a href="#2510.06218v1">EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</a></li>
<li><a href="#2510.06194v1">Overlap-aware segmentation for topological reconstruction of obscured objects</a></li>
<li><a href="#2510.06040v1">VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</a></li>
<li><a href="#2510.06009v1">Continual Learning for Image Captioning through Improved Image-Text Alignment</a></li>
<li><a href="#2510.05949v1">Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</a></li>
<li><a href="#2510.05903v1">Kaputt: A Large-Scale Dataset for Visual Defect Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.05976v1'></a></p>
<h2 id="diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis"><a href="https://arxiv.org/abs/2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></h2>
<p><strong>Authors:</strong> Eashan Adhikarla, Yixin Liu, Brian D. Davison</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Eashan Adhikarla, Yixin Liu, Brian D. Davison撰写的论文“Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决低光照图像增强（LLIE）领域中的核心挑战，即在安全关键应用（如监控、自动驾驶、医学成像）中，可见度下降会严重影响下游任务的性能。具体而言，它关注扩散模型在LLIE中的应用，并对其进行全面的批判性分析，包括与现有最先进方法的比较、实际部署挑战以及新兴范式（如基础模型）的作用。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
*   <strong>多视角分类法：</strong> 论文提出了一个涵盖六个类别的多视角分类法：内在分解（Intrinsic Decomposition）、光谱与潜在（Spectral &amp; Latent）、加速（Accelerated）、引导（Guided）、多模态（Multimodal）和自主（Autonomous）扩散模型。该分类法基于模型机制和条件信号的混合视图，将增强方法映射到物理先验、条件方案和计算效率上。
*   <strong>性能评估：</strong> 论文对扩散模型与基于生成对抗网络（GAN）和Transformer的最先进方法进行了深入的比较性能评估，涵盖了定性失效模式、基准测试不一致性以及可解释性、泛化性和推理效率之间的权衡。
*   <strong>挑战与未来展望：</strong> 论文深入探讨了实际部署挑战（如内存、能耗）和伦理考量，并提出了扩散模型在LLIE领域的未来研究方向，包括新颖的条件作用、实时适应性以及基础模型的潜力。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>扩散模型的优势：</strong> 扩散模型因其通过迭代去噪建模复杂图像分布的能力，在LLIE中展现出巨大的潜力，能够生成高度逼真的细节和纹理，解决了早期方法的关键缺陷。它们在输出质量和训练稳定性方面表现出色，并能有效缓解模式崩溃。
*   <strong>LLIE生成三难困境：</strong> 论文识别了LLIE生成模型在质量、多样性和延迟之间的三难困境。扩散模型通过蒸馏、校正/一致性流和潜在空间操作，扩展了可行区域，在较低采样成本下实现了更高的感知质量和更好的模式覆盖。
*   <strong>权衡分析：</strong> 论文强调了在嵌入强物理先验（可解释性高但受限于假设）和保持模型灵活性（泛化性强但可能产生幻觉）之间的“先验与可塑性”困境。此外，还讨论了感知质量、保真度和效率之间的权衡。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>计算开销和推理延迟：</strong> 扩散模型的迭代去噪过程导致高计算成本和推理延迟，使其难以实时部署。
*   <strong>数据依赖性和稀缺性：</strong> 监督式扩散模型严重依赖大规模、高质量、多样化的配对训练数据，但此类数据获取困难，限制了模型的泛化能力。
*   <strong>泛化性和鲁棒性：</strong> 模型在面对与训练数据显著不同的分布外（OOD）输入时，性能可能下降，尤其是在极端黑暗、非均匀照明和特定传感器噪声模式下。
*   <strong>可解释性：</strong> 扩散模型作为“黑箱”运行，难以精确理解其增强输出或产生特定伪影的原因。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>利用基础模型：</strong> 有效地将现有强大的图像生成扩散模型（如Stable Diffusion）应用于LLIE任务，通过专门的微调技术、新颖的提示策略或零样本引导机制。
*   <strong>多模态基础模型：</strong> 探索能够处理文本、音频或其他传感器信息与视觉数据相结合的多模态基础模型，以实现更具上下文感知和可控性的LLIE。
*   <strong>实时和设备端LLIE：</strong> 持续研究模型压缩、知识蒸馏和高效网络架构设计，以及开发专门的硬件以加速扩散模型计算。
*   <strong>无监督、自监督和零样本学习：</strong> 开发更复杂的无监督方法，以更好地表示真实世界低光照退化，实现跨不同域的鲁棒泛化，并学习内容、照明、噪声和颜色等解耦表示。
*   <strong>增强可控性和可解释性：</strong> 实现更细粒度的语义控制，超越简单的文本提示或掩码，并开发针对生成式扩散模型的XAI技术，以提高信任度和调试能力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models.</li>
<li>We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency.</li>
<li>This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05976v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05976v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05034v1'></a></p>
<h2 id="video-lmm-post-training-a-deep-dive-into-video-reasoning-with-large-multimodal-models"><a href="https://arxiv.org/abs/2510.05034v1">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></h2>
<p><strong>Authors:</strong> Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yunlong Tang等人撰写的论文“Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models”的全面摘要。</p>
<p><strong>论文摘要：Video-LMM 后训练：深入探索大型多模态模型中的视频推理</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决视频理解领域的核心挑战，即如何将新兴的视频大型多模态模型（Video-LMMs）从基本的感知系统提升为复杂的推理引擎。具体而言，它关注于“后训练”这一关键阶段，该阶段在现有文献中仍显零散，缺乏系统性的整合和分析。论文的核心问题是，如何通过系统化的后训练方法，使Video-LMMs能够有效处理复杂的时空关系、长期依赖和多模态证据，并应对视频特有的挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要创新在于首次对Video-LMMs的后训练方法进行了全面的系统性审查和分类。它将后训练分为三个核心支柱：
*   <strong>带思维链的监督微调（SFT-CoT）：</strong> 强调SFT在建立结构化推理行为、多模态对齐和指令遵循能力方面的作用，并将其视为强化学习（RL）的冷启动阶段。
*   <strong>基于可验证目标的强化学习（RL）：</strong> 详细阐述了RL，特别是GRPO（Group Relative Policy Optimization）及其变体，如何通过可验证的奖励（如答案正确性、时空定位精度）来增强推理和自我修正，避免对人工偏好数据的依赖。
*   <strong>通过增强推理计算进行测试时扩展（TTS）：</strong> 探讨了TTS如何通过推理样本增强、投票机制、自洽性检查、外部验证器和多路径搜索来提高模型可靠性。</p>
<p>论文还提出了一个结构化的分类法，阐明了这些技术在解决视频特定挑战（如时间定位、时空基础、长视频效率和多模态证据整合）中的作用、相互联系和适应性。</p>
<p><strong>3. 主要结果及其意义：</strong>
该论文通过对代表性方法的系统分析，综合了关键的设计原则、见解和评估协议，并强调了以下发现：
*   <strong>SFT作为RL的冷启动：</strong> SFT-CoT为RL提供了结构化的推理格式和稳定的初始化，有效防止了RL驱动策略优化中的不稳定性。
*   <strong>GRPO在视频推理中的有效性：</strong> GRPO及其变体（如T-GRPO、Reg-GRPO、TW-GRPO、DGRPO）在视频理解任务中表现出色，尤其是在利用可验证奖励方面，这提高了数据效率并减少了对人工标注的依赖。
*   <strong>TTS提升可靠性：</strong> TTS方法（如思维链提示、自洽性解码、基于置信度的迭代推理、自改进循环、蒙特卡洛树搜索和工具增强推理）在推理阶段分配计算资源，显著提高了Video-LMMs的可靠性和准确性。
*   <strong>统一的评估框架：</strong> 论文整理了重要的基准、数据集和评估指标，为Video-LMM后训练效果的严格评估提供了统一的框架，有助于研究人员更准确地诊断模型改进的来源。</p>
<p>这些结果的意义在于，它们为Video-LMMs从感知到复杂推理的演进提供了一个清晰的路线图，并强调了后训练在实现这一目标中的核心作用。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文在讨论开放挑战时提及了当前方法的局限性：
*   <strong>奖励设计挑战：</strong> 尽管GRPO等方法取得了进展，但设计可验证、组合式奖励，特别是针对复杂时空语义检查的奖励，仍然是一个挑战。过程奖励模型（PRMs）虽然能提供密集信用，但其构建成本和偏差控制仍需改进。
*   <strong>可扩展性问题：</strong> 在长视频上扩展强化学习仍然面临预算限制，需要更高效的帧选择和缓存机制。
*   <strong>成本-性能优化：</strong> 帧优化和压缩框架仍然成本高昂，未来工作需要使其在数据和计算上更高效。
*   <strong>数据稀缺和偏差：</strong> 高质量的视频思维链监督数据获取成本高昂，且现有数据可能存在模板和单模型偏差。
*   <strong>幻觉问题：</strong> 尽管引入视觉基础信息有助于减少幻觉，但模型仍可能捏造不存在的实体或事件。
*   <strong>评估偏差：</strong> 使用LLM作为评估器时，判断偏差和长度偏差可能扭曲评估结果，需要更严格的报告标准和人工/验证器审计。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文提出了以下未来研究方向：
*   <strong>结构化接口和基础思维链：</strong> 规范推理格式，将步骤与证据（时间戳、帧ID、区域）绑定，以提高忠实度并简化验证器设计。
*   <strong>大规模验证器在环思维链合成：</strong> 自动化草稿-细化-审计流程，从ASR/OCR/镜头元数据开始，通过轻量级检查器进行细化和过滤，以减少幻觉。
*   <strong>三模态监督和字幕控制：</strong> 扩展SFT以对齐语音、事件和视觉证据，并始终报告带字幕和不带字幕的结果，以避免ASR快捷方式。
*   <strong>幻觉感知指令微调：</strong> 结合反事实和缺失案例，训练模型进行校准的弃权和验证行为。
*   <strong>多语言、OCR和叙事结构：</strong> 扩展SFT以处理多语言、退化文本和长篇故事推理。
*   <strong>组合式、可验证奖励：</strong> 开发更精细的奖励机制，以处理复杂的时空语义检查。
*   <strong>样本效率和长视频成本：</strong> 探索离线和基于模型的RL变体、世界模型和微型滚动，以提高探索效率。
*   <strong>超越教师的探索：</strong> 开发多样性驱动的目标和自博弈机制，以发现超越教师策略的新策略。
*   <strong>自信感知、验证器引导的TTS：</strong> 将停止规则与不确定性结合，并与验证器检查耦合，以实现随时准确性。
*   <strong>工具增强推理和蒸馏：</strong> 将工具调用（检索、跟踪、ASR对齐）与推理交织，并通过后验蒸馏将这些优势转移到基础模型中。
*   <strong>带记忆的流式代理：</strong> 开发能够决定何时观看、观看什么，并维护任务感知工作记忆的代理规划器。
*   <strong>标准化报告和泄漏控制：</strong> 报告观看预算、推理长度、路径计数、延迟/吞吐量和字幕使用情况，并进行偏袒诊断。
*   <strong>受限观看下的计算-准确性权衡：</strong> 协同调整帧选择和压缩与推理质量，以在处理少量帧时保持系统强大。</p>
<p>总而言之，这篇论文为Video-LMMs的后训练提供了一个全面的视角，不仅系统地梳理了现有技术，还指出了未来的研究方向，旨在推动视频理解领域向更强大、更通用的人工智能系统发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05034v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05034v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04999v1'></a></p>
<h2 id="bridging-text-and-video-generation-a-survey"><a href="https://arxiv.org/abs/2510.04999v1">Bridging Text and Video Generation: A Survey</a></h2>
<p><strong>Authors:</strong> Nilay Kumar, Priyansh Bhandari, G. Maragatham</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.GR, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Nilay Kumar, Priyansh Bhandari, G. Maragatham撰写的论文“Bridging Text and Video Generation: A Survey”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：文本到视频生成技术综述</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在全面综述文本到视频（Text-to-Video, T2V）生成技术的发展现状、挑战和未来方向。T2V技术具有巨大的潜力，能够根据自然语言提示创建连贯的视觉内容，从而在教育、营销、娱乐和辅助技术等多个领域带来变革。然而，该领域仍面临诸多挑战，包括生成内容的语义对齐、长期时间连贯性以及计算效率等问题。本综述试图系统地梳理T2V模型从早期到最新的演变，并探讨其如何应对这些挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文本身是一篇综述性文章，其主要贡献在于对T2V领域进行了系统性、结构化的梳理和分析，而非提出新的模型或算法。其关键贡献包括：
*   <strong>模型发展路线图：</strong> 详细追溯了T2V生成模型从早期的生成对抗网络（GANs）和变分自编码器（VAEs）到当前混合扩散-Transformer（DiT）架构的演变过程。文章深入探讨了这些模型的内部工作机制、它们如何解决前代模型的局限性，以及为何需要转向新的架构范式以克服质量、连贯性和控制方面的挑战。
*   <strong>数据集和训练配置的系统性介绍：</strong> 提供了对T2V模型训练和评估所用数据集的系统性描述，包括其规模、多样性和内容特征。为了支持研究的可复现性和评估模型训练的可行性，论文还详细列出了训练配置，如硬件规格、GPU数量、批次大小、学习率、优化器、训练周期等关键超参数。
*   <strong>评估指标和基准的全面分析：</strong> 概述了T2V模型常用的评估指标，并展示了它们在标准基准上的性能。同时，论文讨论了这些现有指标的局限性，并强调了向更全面、感知对齐的评估策略转变的必要性。</p>
<p><strong>3. 主要结果及其意义：</strong>
本综述通过对现有T2V模型的详细分析，揭示了该领域在以下方面取得的显著进展：
*   <strong>模型架构的演进：</strong> T2V模型已从最初的GANs和VAEs发展到更先进的扩散模型和Transformer架构，显著提高了生成视频的保真度和时间一致性。特别是扩散模型，通过逐步去噪过程，在生成高质量、语义对齐的视频方面表现出色。
*   <strong>性能提升：</strong> 随着模型和训练策略的改进，T2V模型在各种基准测试（如UCF-101、MSRVTT、Kinetics-600等）上的定量评估指标（如IS、FID、CLIP-SIM、FVD、KVD）显示出持续的性能提升。
*   <strong>对可复现性的关注：</strong> 论文详细列举了模型的训练配置，这对于未来研究者理解、复现和进一步开发T2V模型具有重要意义，有助于降低该领域的进入门槛。
*   <strong>评估方法的演变：</strong> 强调了从单一定量指标向结合人类评估和更全面的、感知对齐的评估策略（如VBench）转变的重要性，以更好地捕捉视频质量的细微差别和主观感知。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文明确指出了T2V领域当前面临的几大局限性：
*   <strong>数据可用性限制：</strong> 缺乏大规模、高质量的文本-视频配对数据集，这限制了模型的泛化能力和生成视频的质量。
*   <strong>计算成本高昂：</strong> 视频生成任务的计算成本远高于图像生成，对硬件资源和训练时间提出了巨大挑战。
*   <strong>时间一致性建模困难：</strong> 确保生成视频的长期时间连贯性、避免视觉跳变和不自然过渡仍然是一个核心挑战。
*   <strong>语义对齐不足：</strong> 尤其是在多对象或动作丰富的场景中，文本描述与视频内容之间的语义对齐仍需改进。
*   <strong>现有评估指标的局限性：</strong> 传统的定量指标（如IS、FID）往往无法全面捕捉人类对视频真实感、语义对齐和时间连贯性的感知。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于对当前挑战的分析，论文提出了几个有前景的未来研究方向：
*   <strong>数据集丰富：</strong> 探索使用游戏引擎（如Unity或Unreal Engine）合成大规模、高分辨率、多样化的数据集，以克服版权限制和数据稀缺问题。开发通用的提示框架，通过结构化提示生成视频，以实现语义正确和视觉真实的视频。
*   <strong>模型架构和优化：</strong> 研发更高效的T2V模型架构和算法，以优化计算效率，更好地处理时间序列数据。改进时间建模机制，以生成更长、更连贯的视频。
*   <strong>增强对齐和控制：</strong> 进一步发展注意力机制、多模态数据融合和损失函数，以更有效地关注连贯性和真实感，并更好地模拟物理交互。
*   <strong>更全面的评估策略：</strong> 推广和标准化像VBench这样多维度的、结合人类偏好注释的评估基准，以提供更细粒度、更符合人类感知的模型性能评估。
*   <strong>实际应用拓展：</strong> 探索T2V技术在教育、辅助技术、内容创作、营销、文化遗产保护、法律取证、合成数据生成和游戏/VR等领域的更广泛应用。</p>
<hr />
<p>总而言之，这篇综述为文本到视频生成领域提供了一个全面的快照，不仅总结了该领域的历史发展和当前成就，还明确指出了其面临的挑战，并为未来的研究指明了方向。对于希望深入了解T2V技术现状和未来趋势的研究人员来说，这是一份宝贵的资源。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04999v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04999v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06219v1'></a></p>
<h2 id="human3r-everyone-everywhere-all-at-once"><a href="https://arxiv.org/abs/2510.06219v1">Human3R: Everyone Everywhere All at Once</a></h2>
<p><strong>Authors:</strong> Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, Gerard Pons-Moll</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos. Unlike previous approaches that rely on multi-stage
pipelines, iterative contact-aware refinement between humans and scenes, and
heavy dependencies, e.g., human detection, depth estimation, and SLAM
pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies
("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a
single forward pass ("all-at-once"). Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates
heavy dependencies and iterative refinement. After being trained on the
relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it
achieves superior performance with remarkable efficiency: it reconstructs
multiple humans in a one-shot manner, along with 3D scenes, in one stage, at
real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model. We hope that Human3R will serve as a simple yet strong
baseline, be easily extended for downstream applications.Code available in
https://fanegg.github.io/Human3R</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yue Chen等人撰写的论文“Human3R: Everyone Everywhere All at Once”的全面摘要。</p>
<hr />
<h3 id="human3r-everyone-everywhere-all-at-once_1">论文摘要：Human3R: Everyone Everywhere All at Once</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决从随意捕获的单目视频中，在世界坐标系下进行在线4D人体-场景重建的挑战。传统方法通常依赖多阶段流程、迭代优化以及对人体检测、深度估计和SLAM预处理等重度依赖，这限制了它们在实时、统一和可扩展应用中的性能。Human3R寻求提供一个统一的、前向传播的框架，能够同时、实时地重建多个人体网格、密集3D场景和相机轨迹。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>统一的、前向传播框架：</strong> Human3R是第一个实现“一次性”（all-at-once）重建的统一模型，它在一个单一的前向传播过程中共同恢复全局多个人体SMPL-X模型（“每个人”）、密集3D场景（“无处不在”）和相机轨迹。这消除了对多阶段管道、迭代细化和重度依赖的需求。
*   <strong>基于CUT3R的4D在线重建：</strong> 该方法建立在4D在线重建基础模型CUT3R之上，该模型编码了丰富的时空先验。Human3R通过参数高效的视觉提示微调（Visual Prompt Tuning, VPT）来扩展CUT3R，以保留其强大的时空先验，同时直接读取多个人体SMPL-X模型。
*   <strong>人类先验的注入：</strong> 为了增强重建人体姿态和形状的细节，Human3R将来自Multi-HMR [3] ViT DINO编码器（在以人为中心的数据集上预训练）的人类特定特征注入到头部token中，作为人类先验。这有助于提高对精细人体姿态和形状的预测。
*   <strong>在线人体分割和跟踪：</strong> Human3R还支持训练后的人体分割和跟踪，通过预测每个图像块是否包含人体部位来生成像素对齐的密集掩码，并通过匹配精炼的人体token特征实现跟踪。
*   <strong>测试时序列长度自适应（TTT3R）：</strong> 为了解决RNN模型在序列长度超出训练上下文时性能下降的问题，Human3R采用了TTT3R [12]，通过动态学习率自适应地将当前观测值编码到记忆状态中，平衡历史上下文的保留和新观测值的整合。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的效率：</strong> Human3R在仅使用一块48GB GPU训练一天后，在实时（15 FPS）下以低内存占用（8 GB）重建多个人体和3D场景，实现了显著的效率。
*   <strong>最先进或有竞争力的性能：</strong> 论文通过广泛实验证明，Human3R在多项任务中实现了最先进或有竞争力的性能，包括全局人体运动估计、局部人体网格恢复、视频深度估计和相机姿态估计，所有这些都通过一个统一的模型完成。
*   <strong>鲁棒性提升：</strong> 相比Multi-HMR，Human3R在处理不同图像宽高比时表现出更强的一致性，并且无需相机内参，这得益于CUT3R提供的3D场景感知能力。
*   <strong>相互受益：</strong> 实验表明，通过对人体重建进行微调，3D场景重建也得到了改善，这突显了对人体和场景进行联合推理的相互益处。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>头部作为关键点：</strong> 该方法依赖头部作为检测人类的判别性关键点，当头部不可见时可能导致失败。
*   <strong>代理SMPL网格：</strong> 目前使用代理SMPL网格表示人类，未能建模衣物或外观细节。
*   <strong>交互和物理限制：</strong> Human3R虽然隐式建模了人类交互，但尚未完全解决它们，并且在重建精度上未能匹配一些强大的离线方法（如JOSH [53]）。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>整合像素对齐的身体点定位器：</strong> 引入像素对齐的身体点定位器（如[40, 76]）可以缓解头部不可见时的检测失败问题。
*   <strong>扩展到3DGS锚定的SMPL：</strong> 将框架扩展到基于3DGS（Gaussian Splatting）锚定的SMPL，可以实现更丰富、更全面的重建，包括衣物和外观。
*   <strong>作为优化方法的初始化：</strong> Human3R可以作为优化方法（如[53]）的有效初始化，以在需要更高精度时提高准确性，尽管会增加计算成本。
*   <strong>扩展到其他动态实体：</strong> 论文提出的底层原理可以扩展到重建动物、车辆或其他具有完整6D姿态的移动物体，从而支持野生动物监测、交通分析、人-物交互和机器人等应用。</p>
<hr />
<p>总而言之，Human3R通过引入一个统一的、前向传播的框架，结合参数高效的视觉提示微调和人类先验注入，显著推动了在线4D人体-场景重建领域的发展。它在效率和性能上都表现出色，为未来的实时应用和更广泛的动态实体重建奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos.</li>
<li>Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies.</li>
<li>Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06218v1'></a></p>
<h2 id="egonight-towards-egocentric-vision-understanding-at-night-with-a-challenging-benchmark"><a href="https://arxiv.org/abs/2510.06218v1">EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</a></h2>
<p><strong>Authors:</strong> Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, Yu-Gang Jiang, Luc Van Gool, Danda Pani Paudel</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Most existing benchmarks for egocentric vision understanding focus primarily
on daytime scenarios, overlooking the low-light conditions that are inevitable
in real-world applications. To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task. A key feature of EgoNight is the
introduction of day-night aligned videos, which enhance night annotation
quality using the daytime data and reveal clear performance gaps between
lighting conditions. To achieve this, we collect both synthetic videos rendered
by Blender and real-world recordings, ensuring that scenes and actions are
visually and temporally aligned. Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification. Each QA pair is double-checked
by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs
across 90 videos, spanning 12 diverse QA types, with more than 300 hours of
human work. Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.
Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night
correspondence retrieval and egocentric depth estimation at night, that further
explore the boundaries of existing models. We believe EgoNight-VQA provides a
strong foundation for advancing application-driven egocentric vision research
and for developing models that generalize across illumination domains. All the
data and code will be made available upon acceptance.</p>
<p><strong>Analysis:</strong></p>
<p>以下是对Deheng Zhang等人撰写的论文“EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
现有的大多数以自我为中心的视觉理解基准主要关注白天场景，忽略了现实世界应用中不可避免的低光照条件。这篇论文旨在解决这一空白，即在夜间低光照条件下，以自我为中心的视觉系统如何进行复杂的场景理解和推理。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
*   <strong>EgoNight基准数据集：</strong> 首次提出了一个全面解决夜间条件的以自我为中心的基准数据集，其核心是视觉问答（VQA）。该数据集包含合成视频（由Blender渲染）和真实世界录像，确保场景和动作在视觉和时间上对齐，并涵盖了白天和夜间两种条件。
*   <strong>日夜对齐视频：</strong> 引入了日夜对齐视频，利用白天数据提高夜间标注质量，并揭示了不同光照条件下的模型性能差距。
*   <strong>EgoNight-VQA：</strong> 构建了一个包含3658个QA对的VQA任务，涵盖12种不同的QA类型，通过新颖的“日间增强夜间自动标注引擎”和广泛的人工验证进行构建。
*   <strong>辅助任务：</strong> 除了VQA，EgoNight还引入了日夜对应检索和夜间以自我为中心的深度估计两个辅助任务，以进一步探索现有模型的边界。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   对最先进的多模态大型语言模型（MLLMs）的评估显示，从白天到夜间转换时，模型性能显著下降，突显了在低光照条件下进行推理的挑战。
*   这表明现有MLLMs在夜间以自我为中心的视觉理解方面存在局限性，需要开发更具鲁棒性、能够泛化到不同光照领域的模型。
*   新提出的QA类型（如光照识别/动态、场景序列推理、导航和非常规推理）比传统类别更具挑战性，揭示了MLLMs面临的新难题。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据集规模：</strong> EgoNight数据集的规模与大型视觉语言语料库相比仍然适中。尽管作者认为现有规模已足以进行基准测试，但未来计划通过合成更多数据和录制更多真实世界视频来进一步扩大规模，以支持预训练和微调。
*   <strong>环境条件：</strong> EgoNight主要关注日夜光照变化，而其他现实世界挑战（如天气变化、极端相机运动）尚未涵盖。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   进一步扩大夜间视频数据集的规模，以支持MLLM的预训练和微调，从而提高其性能。
*   将以自我为中心的视觉理解研究扩展到其他现实世界挑战，如雨、雾等天气条件以及极端相机运动。
*   开发更具光照鲁棒性的模型，以弥合白天和夜间条件下的性能差距。</p>
<p>总而言之，EgoNight基准为推动以自我为中心的夜间视觉理解研究提供了一个强大而及时的基础，旨在促进开发能够泛化到各种光照条件下的可靠AI助手。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task.</li>
<li>Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification.</li>
<li>Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06218v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06218v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06194v1'></a></p>
<h2 id="overlap-aware-segmentation-for-topological-reconstruction-of-obscured-objects"><a href="https://arxiv.org/abs/2510.06194v1">Overlap-aware segmentation for topological reconstruction of obscured objects</a></h2>
<p><strong>Authors:</strong> J. Schueler, H. M. Araújo, S. N. Balashov, J. E. Borg, C. Brew, F. M. Brunbauer, C. Cazzaniga, A. Cottle, D. Edgeman, C. D. Frost, F. Garcia, D. Hunt, M. Kastriotou, P. Knights, H. Kraus, A. Lindote, M. Lisowska, D. Loomba, E. Lopez Asamar, P. A. Majewski, T. Marley, C. McCabe, L. Millins, R. Nandakumar, T. Neep, F. Neves, K. Nikolopoulos, E. Oliveri, A. Roy, T. J. Sumner, E. Tilly, W. Thompson, M. A. Vogiatzi</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> hep-ex, astro-ph.IM, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The separation of overlapping objects presents a significant challenge in
scientific imaging. While deep learning segmentation-regression algorithms can
predict pixel-wise intensities, they typically treat all regions equally rather
than prioritizing overlap regions where attribution is most ambiguous. Recent
advances in instance segmentation show that weighting regions of pixel overlap
in training can improve segmentation boundary predictions in regions of
overlap, but this idea has not yet been extended to segmentation regression. We
address this with Overlap-Aware Segmentation of ImageS (OASIS): a new
segmentation-regression framework with a weighted loss function designed to
prioritize regions of object-overlap during training, enabling extraction of
pixel intensities and topological features from heavily obscured objects. We
demonstrate OASIS in the context of the MIGDAL experiment, which aims to
directly image the Migdal effect--a rare process where electron emission is
induced by nuclear scattering--in a low-pressure optical time projection
chamber. This setting poses an extreme test case, as the target for
reconstruction is a faint electron recoil track which is often heavily-buried
within the orders-of-magnitude brighter nuclear recoil track. Compared to
unweighted training, OASIS improves median intensity reconstruction errors from
-32% to -14% for low-energy electron tracks (4-5 keV) and improves topological
intersection-over-union scores from 0.828 to 0.855. These performance gains
demonstrate OASIS's ability to recover obscured signals in overlap-dominated
regions. The framework provides a generalizable methodology for scientific
imaging where pixels represent physical quantities and overlap obscures
features of interest. All code is openly available to facilitate cross-domain
adoption.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供J. Schueler等人撰写的论文“Overlap-aware segmentation for topological reconstruction of obscured objects”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Overlap-aware segmentation for topological reconstruction of obscured objects</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文主要解决科学成像中重叠物体分离的重大挑战。具体而言，它关注的是深度学习分割-回归算法在处理像素级强度预测时，通常对所有区域一视同仁，而未能优先处理归因最模糊的重叠区域。这导致在重叠严重、强度差异巨大的情况下，难以准确提取被遮挡物体的像素强度和拓扑特征。论文以MIGDAL实验为例，该实验旨在直接成像Migdal效应（一种由核散射引起的电子发射过程），其中重建目标是微弱的电子反冲径迹，这些径迹通常被亮度高出几个数量级的核反冲径迹严重掩盖。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文引入了<strong>Overlap-Aware Segmentation of ImageS (OASIS)</strong> 框架，这是一个新的分割-回归框架，其核心创新在于：
*   <strong>加权损失函数：</strong> OASIS设计了一个定制的加权损失函数，该函数在训练过程中优先处理物体重叠区域。通过为不同通道（例如，微弱的电子反冲径迹）和重叠区域分配特定的权重，模型能够将训练注意力集中在归因最关键的区域。
*   <strong>扩展重叠感知训练范式：</strong> 现有实例分割方法（如MultiStar和BCNet）已证明重叠区域加权可以改善分割边界预测，但OASIS首次将这一思想扩展到分割回归任务，使其能够预测物理量（像素强度）而非仅仅是类别或掩码。
*   <strong>通用性：</strong> 该框架提供了一种可推广的方法，适用于像素代表物理量且重叠遮挡感兴趣特征的科学成像领域。它在计算上高效，不需要对标准分割-回归网络的架构进行修改，并且与特定的骨干网络无关。</p>
<p><strong>3. 主要结果及其意义：</strong>
OASIS在MIGDAL实验背景下的性能评估显示出显著改进：
*   <strong>强度重建误差降低：</strong> 对于低能量电子径迹（4-5 keV），OASIS将中位强度重建误差从-32%提高到-14%。这表明在重叠区域中，模型能够更准确地归因微弱信号的强度。
*   <strong>拓扑交并比（IoU）分数提高：</strong> 拓扑交并比分数从0.828提高到0.855。这表明OASIS在重建被遮挡信号的拓扑结构方面也表现出更好的性能。
*   <strong>低能量区域的改进：</strong> 在Migdal效应截面随能量降低呈指数增长的低能量区域（4-6 keV），OASIS的改进尤为关键，因为这些区域的准确重建对于验证理论预测至关重要。
*   <strong>角度一致性：</strong> 论文还展示了OASIS在低能量区域内，通过主曲线拟合得到的角度一致性在20°以内，这对于测量Migdal效应的角分布具有重要意义。
*   <strong>假阳性率：</strong> 尽管加权训练显著提高了低能量ER重建性能，但代价是假阳性率略有增加（从0.5%到1.5%）。然而，对于MIGDAL实验而言，由于OASIS并非用于搜索Migdal效应候选者，而是用于提取已检测到的ER的准确表示，因此这不是一个主要问题。</p>
<p>这些性能提升证明了OASIS在重叠主导区域恢复被遮挡信号的能力，为科学成像中像素代表物理量且重叠遮挡感兴趣特征的问题提供了一种通用方法。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>假阳性率：</strong> 尽管加权训练显著提高了低能量ER重建性能，但与未加权训练相比，它导致了更高的假阳性率（1.5% vs 0.5%），即更容易重建不存在的信号。论文指出，对于MIGDAL实验，这并非主要关注点，因为OASIS并非用于搜索Migdal效应候选者，而是用于提取已检测到的ER的准确表示。
*   <strong>ER顶点重建：</strong> 论文提到，ER顶点难以精确确定。虽然他们利用NR顶点位置来构建主轴，但ER顶点本身的精确重建仍是一个挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>跨领域应用：</strong> OASIS的通用性使其可以应用于其他科学成像领域，例如天文学中的星系去混合（deblending）和医学成像中的体积数据分析。论文特别提到了Galaxy Zoo DECALS数据库，这是一个公开可用的数据集，非常适合应用OASIS。
*   <strong>多波段输入：</strong> 框架可以扩展以支持多通道输入，例如天文学中的多波段图像。
*   <strong>三维重建：</strong> 支持完整的3D体素（3个空间维度+强度）将使其能够应用于体积医学成像和具有完整3D重建能力的升级粒子探测器。
*   <strong>Migdal效应角分布测量：</strong> 论文指出，OASIS在低能量区域的角敏感性有望用于当前一代MIGDAL探测器测试Migdal效应角产生理论模型的能力，这将在未来的工作中进行报告。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We
address this with Overlap-Aware Segmentation of ImageS (OASIS): a new
segmentation-regression framework with a weighted loss function designed to
prioritize regions of object-overlap during training, enabling extraction of
pixel intensities and topological features from heavily obscured objects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06194v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06194v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06040v1'></a></p>
<h2 id="videominer-iteratively-grounding-key-frames-of-hour-long-videos-via-tree-based-group-relative-policy-optimization"><a href="https://arxiv.org/abs/2510.06040v1">VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</a></h2>
<p><strong>Authors:</strong> Xinye Cao, Hongcan Guo, Jiawen Qian, Guoshun Nan, Chao Wang, Yuqi Pan, Tianhao Hou, Xiaojuan Wang, Yutong Gao</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Understanding hour-long videos with multi-modal large language models
(MM-LLMs) enriches the landscape of human-centered AI applications. However,
for end-to-end video understanding with LLMs, uniformly sampling video frames
results in LLMs being overwhelmed by a vast amount of irrelevant information as
video length increases. Existing hierarchical key frame extraction methods
improve the accuracy of video understanding but still face two critical
challenges. 1) How can the interference of extensive redundant information in
long videos be mitigated? 2) How can a model dynamically adapt to complex
hierarchical structures while accurately identifying key frames? To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure. The proposed
VideoMiner progresses from long videos to events to frames while preserving
temporal coherence, effectively addressing the first challenge. To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner. The proposed T-GRPO is specifically designed for tree
structures, integrating spatiotemporal information at the event level while
being guided by the question, thus solving the second challenge. We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights. Our proposed T-GRPO surprisingly incentivizes the model
to spontaneously generate a reasoning chain. Additionally, the designed tree
growth auxin dynamically adjusts the expansion depth, obtaining accuracy and
efficiency gains. The code is publicly available at
https://github.com/caoxinye/VideoMiner.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xinye Cao等人撰写的论文“VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization”的全面摘要。</p>
<hr />
<p><strong>论文摘要：VideoMiner：通过基于树的组相对策略优化迭代地定位长视频中的关键帧</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决长视频理解中存在的两个关键挑战：
1. <strong>如何减轻长视频中大量冗余信息的干扰？</strong> 随着视频长度的增加，多模态大型语言模型（MM-LLMs）在端到端视频理解中，由于均匀采样帧导致模型被大量不相关信息淹没。
2. <strong>模型如何动态适应复杂的层次结构，同时准确识别关键帧？</strong> 现有的层次化关键帧提取方法虽然提高了视频理解的准确性，但在处理复杂层次结构和精确关键帧定位方面仍面临挑战。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
为了解决上述挑战，论文提出了<strong>VideoMiner</strong>，一个新颖的基于强化学习的视频理解框架，其核心创新包括：
*   <strong>层次化树结构构建：</strong> VideoMiner通过迭代地对长视频进行分割、生成字幕和聚类，构建了一个层次化的树结构。它从长视频层面逐步深入到事件层面，再到帧层面，同时保持时间连贯性，有效缓解了冗余信息干扰的问题。
*   <strong>T-GRPO（Tree-based Group Relative Policy Optimization）：</strong> 为了精确地定位关键帧，论文引入了T-GRPO，这是一种基于树结构的组相对策略优化强化学习方法，用于指导VideoMiner的探索过程。T-GRPO专门为树结构设计，它在事件层面整合了时空信息，并由问题引导，从而解决了动态适应复杂层次结构和准确识别关键帧的问题。
*   <strong>奖励函数设计：</strong> T-GRPO的奖励函数被分解为节点级奖励（评估单个节点决策质量）和树级奖励（反映最终树级结果的正确性），以指导策略模型做出更结构化、详细和准确的关键帧决策。
*   <strong>树生长素（Tree Growth Auxin）机制：</strong> 引入了类似植物生长素的机制<code>Aauxin</code>，动态调整树的扩展深度，以平衡准确性和效率，并调节策略模型的探索倾向。
*   <strong>推理链的自发生成：</strong> T-GRPO出人意料地激励模型自发生成推理链，显著提升了模型的推理能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> VideoMiner在所有长视频理解任务中均取得了卓越的性能，并在短视频基准测试中也表现出色，显著优于多种基线方法。
*   <strong>长视频理解的优势：</strong> 随着视频长度的增加，VideoMiner与基线方法之间的性能差距逐渐扩大，表明其在长视频理解任务中的优越性。这得益于其场景分割和聚类方法最大限度地保留了时间信息，以及强化学习训练的策略模型具备自定向决策能力。
*   <strong>事件聚类的有效性：</strong> 事件聚类相比帧聚类能保留更丰富的时间信息，并促进树结构的有效构建，在大多数基准测试中实现了最短的运行时间和最高的准确性。
*   <strong>T-GRPO的有效性：</strong> T-GRPO引入的树级奖励设计显著增强了策略模型的推理能力，使其能够考虑当前决策对未来结果的影响，从而提高了准确性。
*   <strong>补全长度和生长速率的影响：</strong> 实验表明，更长的补全长度（即扩展的补全过程）会带来更高的准确性，因为强化学习过程会自然地诱导思维链行为。此外，树生长速率<code>Aauxin</code>动态调节扩展深度，平衡了准确性和效率。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>短视频任务的性能差距：</strong> 尽管VideoMiner在长视频任务中表现优异，但在短视频任务中，与端到端方法相比仍存在一定的性能差距。这主要是因为VideoMiner和端到端方法使用了不同的基础模型，且其模型是专门为视频任务训练和增强的。VideoMiner主要为关键帧选择至关重要的长视频理解而设计，而短视频则不一定需要。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文未明确提出未来研究方向，但从其贡献和局限性可以推断：
*   <strong>优化短视频理解：</strong> 进一步研究如何调整VideoMiner的架构或训练策略，以缩小其在短视频任务中与端到端方法的性能差距。
*   <strong>更广泛的应用场景：</strong> 探索VideoMiner在其他需要精确关键帧定位和层次化理解的长视频应用中的潜力，例如视频编辑、内容检索或教育领域。
*   <strong>多模态信息融合的深度探索：</strong> 进一步研究如何更有效地整合和利用视频中的各种模态信息（如音频、文本、视觉），以提升理解能力。
*   <strong>可解释性与透明度：</strong> 鉴于T-GRPO能够自发生成推理链，未来可以深入研究如何利用这些推理链来提高模型的透明度和可解释性，帮助用户更好地理解模型的决策过程。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure.</li>
<li>To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner.</li>
<li>We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06040v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06040v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06009v1'></a></p>
<h2 id="continual-learning-for-image-captioning-through-improved-image-text-alignment"><a href="https://arxiv.org/abs/2510.06009v1">Continual Learning for Image Captioning through Improved Image-Text Alignment</a></h2>
<p><strong>Authors:</strong> Bertram Taetz, Gal Bordelius</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating accurate and coherent image captions in a continual learning
setting remains a major challenge due to catastrophic forgetting and the
difficulty of aligning evolving visual concepts with language over time. In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks. Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation. We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods. The code can
be found via the following link https://github.com/
Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Bertram Taetz和Gal Bordelius撰写的论文“Continual Learning for Image Captioning through Improved Image-Text Alignment”的全面摘要。</p>
<hr />
<p><strong>论文摘要：持续学习图像字幕生成中的改进图像-文本对齐</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决持续学习（Continual Learning）环境下图像字幕生成所面临的核心挑战。具体来说，它关注两个主要问题：
* <strong>灾难性遗忘（Catastrophic Forgetting）：</strong> 在模型学习新任务时，如何防止其遗忘先前学习到的知识。
* <strong>视觉概念与语言的对齐：</strong> 如何有效地将不断演变的视觉概念与语言描述进行对齐，以生成准确且连贯的图像字幕。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
作者提出了一种新颖的<strong>多损失框架（multi-loss framework）</strong>，用于持续学习图像字幕生成，其核心在于通过基于提示的持续学习（prompt-based continual learning）和对比对齐（contrastive alignment）来整合语义指导。该方法基于预训练的ViT-GPT-2骨干网络，并结合了以下三个额外的损失组件：</p>
<ul>
<li><strong>基于提示的余弦相似度损失（Prompt-based Cosine Similarity Loss - <script type="math/tex">L_{nouns}</script>）：</strong> 旨在将图像嵌入（image embeddings）与人工构建的、编码了对象、属性和动作的提示嵌入（synthetically constructed prompts）进行对齐。这有助于模型更好地理解场景中的关键视觉元素。</li>
<li><strong>CLIP风格的损失（CLIP-style Loss - <script type="math/tex">L_{CLIP}</script>）：</strong> 促进图像嵌入与目标字幕嵌入（target caption embedding）之间的对齐，从而增强视觉和文本模态在共享语义空间中的一致性。</li>
<li><strong>语言引导的对比损失（Language-Guided Contrastive Loss - <script type="math/tex">L_{LGCL}</script>）：</strong> 采用三元组损失（triplet loss）来增强任务之间类级别的可区分性。它鼓励图像嵌入与其正确的语言对应物保持接近，同时与不匹配的负样本保持一定距离。</li>
</ul>
<p>值得注意的是，该方法在推理时<strong>不引入额外的开销</strong>，并且在字幕生成过程中<strong>不需要提示</strong>。</p>
<p><strong>3. 主要结果及其意义：</strong>
实验结果表明，该方法在缓解灾难性遗忘方面表现出色，并且与现有最先进的方法相比，实现了更好的语义字幕对齐。</p>
<ul>
<li><strong>缓解灾难性遗忘：</strong> 在ContCap和RATT数据集上的实验显示，CLICITA方法在知识保留方面优于预训练基线模型，平均遗忘率更低。</li>
<li><strong>改进语义对齐：</strong> 与现有方法相比，CLICITA在METEOR等语义指标上表现出显著提升，表明其生成的字幕在语义上更准确、更连贯。</li>
<li><strong>推理效率：</strong> 该方法在推理时无需额外开销和提示，使其适用于资源受限的环境。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确提及当前方法的具体局限性，但作为持续学习领域的研究，通常会面临以下潜在挑战（尽管论文未直接指出）：
*   <strong>模型复杂性：</strong> 引入多个损失函数可能会增加训练的复杂性。
*   <strong>超参数调优：</strong> 多个损失组件的权重平衡可能需要精细的超参数调优。
*   <strong>泛化能力：</strong> 尽管在特定数据集上表现良好，但在更广泛、更多样化的持续学习场景中的泛化能力仍需进一步验证。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
作者提出了以下未来研究方向：
*   <strong>更强大和鲁棒的模型：</strong> 探索将该方法应用于更强大和鲁棒的图像字幕生成模型。
*   <strong>更大的持续学习图像字幕数据集：</strong> 在更大的持续学习图像字幕数据集上进行实验，以进一步验证和提升方法的性能。</p>
<hr />
<p>这份摘要旨在清晰、简洁地传达论文的核心内容，突出其在持续学习图像字幕生成领域的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment.</li>
<li>Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks.</li>
<li>Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation.</li>
<li>We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06009v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06009v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05949v1'></a></p>
<h2 id="gaussian-embeddings-how-jepas-secretly-learn-your-data-density"><a href="https://arxiv.org/abs/2510.05949v1">Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</a></h2>
<p><strong>Authors:</strong> Randall Balestriero, Nicolas Ballas, Mike Rabbat, Yann LeCun</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, stat.ML</p>
<p><strong>Abstract:</strong></p>
<p>Joint Embedding Predictive Architectures (JEPAs) learn representations able
to solve numerous downstream tasks out-of-the-box. JEPAs combine two
objectives: (i) a latent-space prediction term, i.e., the representation of a
slightly perturbed sample must be predictable from the original sample's
representation, and (ii) an anti-collapse term, i.e., not all samples should
have the same representation. While (ii) is often considered as an obvious
remedy to representation collapse, we uncover that JEPAs' anti-collapse term
does much more--it provably estimates the data density. In short, any
successfully trained JEPA can be used to get sample probabilities, e.g., for
data curation, outlier detection, or simply for density estimation. Our
theoretical finding is agnostic of the dataset and architecture used--in any
case one can compute the learned probabilities of sample <script type="math/tex">x</script> efficiently and in
closed-form using the model's Jacobian matrix at <script type="math/tex">x</script>. Our findings are
empirically validated across datasets (synthetic, controlled, and Imagenet) and
across different Self Supervised Learning methods falling under the JEPA family
(I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the
method extracting the JEPA learned density as {\bf JEPA-SCORE}.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要进行如下分析：</p>
<hr />
<p><strong>论文摘要分析：Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于揭示了联合嵌入预测架构（JEPAs）中的“反坍缩项”不仅能防止表示坍缩，还能<strong>可证明地估计数据密度</strong>。这意味着任何成功训练的JEPA模型，其内部机制实际上已经学习到了数据分布的概率信息。作者提出了一种名为JEPA-SCORE的方法，能够高效且以闭合形式，利用模型在样本处的雅可比矩阵，从JEPA中提取这些学习到的样本概率。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>关键创新在于<strong>理论上证明了JEPA的反坍缩项与数据密度估计之间的内在联系</strong>。这颠覆了传统上认为反坍缩项仅仅是防止模型退化的观点。方法论上，论文提出了一种<strong>基于模型雅可比矩阵的闭合形式计算方法</strong>，用于从已训练的JEPA中高效地提取样本概率，并将其命名为JEPA-SCORE。这种方法具有普适性，不依赖于具体数据集或架构。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>统一了自监督学习与密度估计：</strong> 这项工作将自监督学习（特别是JEPA家族）与生成模型和密度估计领域联系起来，为理解自监督学习的深层机制提供了新的视角。</li>
<li><strong>提升JEPA模型的价值：</strong> JEPA模型不再仅仅是用于下游任务的特征提取器，它们现在被证明内在地包含了数据分布信息，极大地扩展了其应用潜力。</li>
<li><strong>新的数据理解工具：</strong> JEPA-SCORE提供了一种从现有自监督模型中“免费”获取数据密度信息的方法，无需额外训练专门的密度估计模型。</li>
<li><strong>推动理论研究：</strong> 这一发现可能会激发更多关于自监督学习中不同组件功能及其与统计学原理之间联系的理论研究。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>数据策展 (Data Curation)：</strong> 可以利用学习到的样本概率来识别有代表性的样本，或过滤掉低质量、冗余的数据，从而优化训练数据集。</li>
<li><strong>异常检测 (Outlier Detection)：</strong> 低概率的样本很可能是异常值，JEPA-SCORE可以直接用于识别这些异常。</li>
<li><strong>密度估计 (Density Estimation)：</strong> 提供了一种新的、可能更高效的密度估计方法，尤其是在自监督学习已经广泛应用的环境中。</li>
<li><strong>生成模型 (Generative Models)：</strong> 对数据密度的理解是生成模型的基础，这项研究可能为条件生成、样本质量评估等提供新的思路。</li>
<li><strong>多模态学习 (Multimodal Learning)：</strong> 摘要中提到在MetaCLIP等多模态模型上的验证，表明该方法在处理复杂多模态数据分布方面也具有潜力。</li>
<li><strong>自监督学习的可解释性：</strong> 深入理解JEPA的工作原理，有助于提高自监督学习模型的可解释性。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 虽然摘要声称“高效且以闭合形式”计算雅可比矩阵，但在非常大的模型和高维数据上，计算雅可比矩阵的成本仍然可能是一个实际的考虑因素。</li>
<li><strong>“成功训练”的定义：</strong> 论文强调“任何成功训练的JEPA”，但“成功”的定义可能需要进一步明确。例如，如果JEPA训练不充分或收敛到次优解，其学习到的密度估计的准确性如何？</li>
<li><strong>密度估计的质量：</strong> 尽管能够估计密度，但其估计的准确性、鲁棒性以及与专门的密度估计方法（如流模型、扩散模型）相比的优劣，仍需在更广泛的场景下进行深入评估。</li>
<li><strong>理论与实践的差距：</strong> 理论证明了联系，但实际应用中，这种密度估计在各种下游任务中的表现（例如，异常检测的F1分数）是否能超越现有SOTA方法，仍需大量实验验证。</li>
<li><strong>仅限于JEPA家族：</strong> 尽管JEPA家族涵盖了I-JEPA和DINOv2等重要方法，但该理论是否能推广到其他非JEPA类的自监督学习方法（如对比学习、掩码图像建模等），摘要中并未提及。</li>
</ul>
<hr />
<p>总而言之，这篇论文为自监督学习领域带来了令人兴奋的新视角，它不仅揭示了JEPA模型的一个“隐藏”能力，还提供了一种实用的工具来利用这一能力。如果其理论和实验结果能够得到广泛验证，它将对我们理解和应用自监督学习模型产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our findings are
empirically validated across datasets (synthetic, controlled, and Imagenet) and
across different Self Supervised Learning methods falling under the JEPA family
(I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP.</li>
<li>We denote the
method extracting the JEPA learned density as {\bf JEPA-SCORE}.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05949v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05949v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05903v1'></a></p>
<h2 id="kaputt-a-large-scale-dataset-for-visual-defect-detection"><a href="https://arxiv.org/abs/2510.05903v1">Kaputt: A Large-Scale Dataset for Visual Defect Detection</a></h2>
<p><strong>Authors:</strong> Sebastian Höfer, Dorian Henning, Artemij Amiranashvili, Douglas Morrison, Mariliza Tzes, Ingmar Posner, Marc Matvienko, Alessandro Rennola, Anton Milan</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<p><strong>论文摘要分析：Kaputt: A Large-Scale Dataset for Visual Defect Detection</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的主要贡献是引入了一个名为“Kaputt”的全新大规模数据集，用于零售物流环境中的视觉缺陷检测。该数据集旨在解决现有工业异常检测基准（如MVTec-AD和VisA）在处理物体姿态和外观多样性方面的局限性，这些现有基准在受控制造场景中已接近饱和。Kaputt数据集的规模是MVTec-AD的40倍，包含超过23万张图像和2.9万个缺陷实例，并明确指出当前最先进的异常检测方法在该数据集上的表现远低于预期（AUROC仅为56.96%），从而为该领域设定了新的挑战。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>这篇论文的关键创新在于<strong>数据集的构建和其所针对的新颖且更具挑战性的问题设定</strong>。它没有提出新的算法，而是通过创建一个大规模、高多样性的数据集来推动领域发展。具体来说：</p>
<ul>
<li><strong>问题域的转移和扩展：</strong> 从高度受控的制造场景转向零售物流，引入了物体姿态和外观的巨大多样性，这在现有基准中是缺失的。</li>
<li><strong>数据集规模和复杂性：</strong> Kaputt数据集的规模（23万张图像，2.9万个缺陷实例，4.8万个独立物体）远超现有基准，提供了更丰富的训练和测试数据。</li>
<li><strong>明确的挑战设定：</strong> 通过展示现有SOTA方法在该数据集上的低性能（56.96% AUROC），明确指出了当前方法的局限性，并为未来的研究设定了明确的改进目标。</li>
<li><strong>强调“正常样本”的利用挑战：</strong> 摘要中提到“现有方法难以在严重的姿态和外观变化下利用正常样本”，这暗示了该数据集的复杂性不仅在于缺陷本身，还在于如何有效学习“正常”的分布。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动异常检测研究的新方向：</strong> 该数据集将促使研究人员开发更鲁棒、更泛化的异常检测算法，以应对高姿态和外观多样性。</li>
<li><strong>加速零售物流自动化：</strong> 解决零售物流中的缺陷检测问题，将直接促进该领域的自动化和效率提升，减少人工检查的成本和错误。</li>
<li><strong>促进自监督/无监督学习的发展：</strong> 异常检测本质上是无监督或半监督问题，该数据集的挑战性将激励在这些领域进行更深入的研究，尤其是在如何从大量“正常”但高度变化的样本中学习有效表示方面。</li>
<li><strong>新的基准和比较平台：</strong> Kaputt将成为未来异常检测算法性能评估的新标准，尤其是在非受控工业场景中。</li>
</ul>
<p><strong>4. 相关领域或应用可能受益于这项研究</strong></p>
<ul>
<li><strong>智能仓储和物流：</strong> 自动识别包裹、商品或设备在运输、存储过程中的损坏或缺陷。</li>
<li><strong>质量控制（非受控环境）：</strong> 例如，在回收分类、农产品分拣等场景中，物体姿态和外观变化大。</li>
<li><strong>机器人抓取和操作：</strong> 机器人需要识别并避免抓取有缺陷的物体，或在复杂环境中进行操作。</li>
<li><strong>计算机视觉中的域泛化（Domain Generalization）和少样本学习（Few-Shot Learning）：</strong> 应对高多样性数据，需要模型具备更好的泛化能力，并可能需要从少量缺陷样本中学习。</li>
<li><strong>数据增强和合成数据生成：</strong> 面对如此复杂的数据，如何有效地进行数据增强或生成合成数据以提高模型性能将成为一个研究方向。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>数据集的缺陷类型未明确说明：</strong> 摘要中只提到“缺陷实例”，但没有详细说明缺陷的种类、大小、严重程度等，这可能会影响研究人员对数据集复杂性的理解和算法设计。</li>
<li><strong>“零售物流”的具体场景未完全展开：</strong> 虽然提到了零售物流，但具体是哪种商品、哪种包装、哪种物流环节（如入库、出库、分拣）等细节缺失，这可能影响研究人员对问题背景的深入理解。</li>
<li><strong>缺乏对“正常样本”多样性的量化：</strong> 摘要强调了“姿态和外观变化”，但没有提供量化的指标来描述这种多样性，例如姿态变化的范围、光照变化的程度等。</li>
<li><strong>现有方法失败的具体原因分析有限：</strong> 摘要中提到“现有方法难以在严重的姿态和外观变化下利用正常样本”，但更深入的定性或定量分析（例如，是特征提取不足？还是异常分数计算机制不适应？）在摘要中并未详细展开。</li>
<li><strong>数据集的标注质量和一致性：</strong> 作为一个大规模数据集，其标注的准确性和一致性至关重要，但摘要中没有提及相关的标注协议或质量控制措施。</li>
<li><strong>仅关注AUROC指标：</strong> 虽然AUROC是异常检测的常用指标，但在某些实际应用中，其他指标如F1分数、精确率-召回率曲线下的面积（AUPRC）等也可能很重要，尤其是在缺陷样本稀少的情况下。摘要中未提及其他指标。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入一个具有挑战性的新数据集，为计算机视觉领域的异常检测研究注入了新的活力。它明确指出了当前SOTA方法在处理真实世界复杂性方面的局限性，并为未来的研究设定了清晰的方向。其对零售物流领域的关注也使其具有重要的实际应用价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a novel large-scale dataset for defect detection in a logistics
setting.</li>
<li>Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores.</li>
<li>In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance.</li>
<li>Leading anomaly detection methods fall short when applied
to this new setting.</li>
<li>To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets.</li>
<li>To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset.</li>
<li>With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05903v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05903v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-08 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
