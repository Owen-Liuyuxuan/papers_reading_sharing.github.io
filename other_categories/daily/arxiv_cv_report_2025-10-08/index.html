<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-08 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-07/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-09/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-08">Arxiv Computer Vision Papers - 2025-10-08</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis" class="nav-link">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a>
                </li>
                <li class="nav-item">
                    <a href="#video-lmm-post-training-a-deep-dive-into-video-reasoning-with-large-multimodal-models" class="nav-link">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a>
                </li>
                <li class="nav-item">
                    <a href="#bridging-text-and-video-generation-a-survey" class="nav-link">Bridging Text and Video Generation: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#human3r-everyone-everywhere-all-at-once" class="nav-link">Human3R: Everyone Everywhere All at Once</a>
                </li>
                <li class="nav-item">
                    <a href="#egonight-towards-egocentric-vision-understanding-at-night-with-a-challenging-benchmark" class="nav-link">EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#overlap-aware-segmentation-for-topological-reconstruction-of-obscured-objects" class="nav-link">Overlap-aware segmentation for topological reconstruction of obscured objects</a>
                </li>
                <li class="nav-item">
                    <a href="#videominer-iteratively-grounding-key-frames-of-hour-long-videos-via-tree-based-group-relative-policy-optimization" class="nav-link">VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</a>
                </li>
                <li class="nav-item">
                    <a href="#continual-learning-for-image-captioning-through-improved-image-text-alignment" class="nav-link">Continual Learning for Image Captioning through Improved Image-Text Alignment</a>
                </li>
                <li class="nav-item">
                    <a href="#gaussian-embeddings-how-jepas-secretly-learn-your-data-density" class="nav-link">Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</a>
                </li>
                <li class="nav-item">
                    <a href="#kaputt-a-large-scale-dataset-for-visual-defect-detection" class="nav-link">Kaputt: A Large-Scale Dataset for Visual Defect Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-08">Arxiv Computer Vision Papers - 2025-10-08</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ7æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£è¯¥é¢åçéè¦åå±ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ§è¡æè¦ (2025-10-07)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong></p>
<p>ä»å¤©çè®ºæéå±ç¤ºäºè®¡ç®æºè§è§é¢åæç»­çæ´»ååå¤ç»´åº¦åå±ãä¸»è¦è¶å¿åæ¬ï¼</p>
<ol>
<li><strong>å¤æ¨¡æä¸å¤§æ¨¡å (LMMs) çæ·±å¥æ¢ç´¢ï¼</strong> å¤ç¯è®ºæèç¦äºè§é¢ä¸ææ¬çç»åï¼ä»¥åå¤§åå¤æ¨¡ææ¨¡åå¨è§é¢çè§£åçæä¸­çåºç¨ï¼è¡¨æLMMså¨è§é¢é¢åçæ½åæ­£è¢«ç§¯æææã</li>
<li><strong>é²æ£æ§ä¸æææ§ç¯å¢ï¼</strong> éå¯¹ä½åç§ãå¤é´ãé®æ¡ç­å¤æåºæ¯çå¾åå¢å¼ºãçè§£ååå²é®é¢æ¯ç ç©¶ç­ç¹ï¼å¼ºè°äºæ¨¡åå¨çå®ä¸çé¨ç½²ä¸­çå®ç¨æ§ã</li>
<li><strong>æ°æ®ä¸åºåçæç»­æå»ºï¼</strong> æ°çãå¤§è§æ¨¡çãå·ææææ§çæ°æ®éï¼å¦å¤é´ãç¼ºé·æ£æµï¼çåå¸ï¼ä¸ºæ¨å¨ç¹å®é¢åçç ç©¶æä¾äºå³é®æ¯æã</li>
<li><strong>çææ¨¡åä¸æ©æ£æ¨¡åçæ¼è¿ï¼</strong> æ©æ£æ¨¡åå¨ä½åç§å¢å¼ºä¸­çåºç¨ï¼ä»¥åææ¬å°è§é¢çæææ¯çç»¼è¿°ï¼æ¾ç¤ºäºçææ¨¡åå¨å¾ååè§é¢åææ¹é¢çæç»­å½±ååã</li>
</ol>
<p><strong>ç¹å«æ¾èæåæ°æ§è®ºæï¼</strong></p>
<ul>
<li><strong>"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models" (Yunlong Tang et al.)</strong>ï¼è¿ç¯è®ºææ·±å¥æ¢è®¨äºLMMså¨è§é¢æ¨çä¸­çåºç¨ï¼å¯è½æ­ç¤ºäºå¦ä½ææå©ç¨åå¾®è°è¿äºå¤§åæ¨¡åä»¥å®ç°æ´é«çº§çè§é¢çè§£è½åï¼å¯¹LMMså¨è§é¢é¢åçå®éè½å°å·ææå¯¼æä¹ã</li>
<li><strong>"Human3R: Everyone Everywhere All at Once" (Yue Chen et al.)</strong>ï¼æ é¢æç¤ºäºå¯¹éç¨ãå¤åºæ¯äººä½å§¿ææè¡ä¸ºçè§£çéå¿ï¼å¦æè½å®ç°âæ å¤ä¸å¨ï¼æ æ¶æ å»âççè§£ï¼å°æ¯äººä½åæé¢åçä¸å¤§çªç ´ã</li>
<li><strong>"Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density" (Randall Balestriero et al.)</strong>ï¼è¿ç¯è®ºæå¯è½ä»çè®ºå±é¢æ­ç¤ºäºJEPAsï¼èååµå¥é¢æµæ¶æï¼çå·¥ä½æºå¶ï¼ç¹å«æ¯å®ä»¬å¦ä½å­¦ä¹ æ°æ®å¯åº¦ãçè§£è¿ç§åºå±æºå¶å¯¹äºä¼ååè®¾è®¡æ´ææçèªçç£å­¦ä¹ æ¹æ³è³å³éè¦ï¼å·æè¾é«ççè®ºä»·å¼ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>è§é¢LMMsçåè®­ç»ä¸å¾®è°ç­ç¥ï¼</strong> éçLMMsçæ®åï¼å¦ä½éå¯¹è§é¢ä»»å¡è¿è¡é«æçåè®­ç»åå¾®è°ï¼ä»¥åååæ¥å¶æ½åï¼æ¯ä¸ä¸ªéè¦çç ç©¶æ¹åã</li>
<li><strong>å¤é´/ä½åç§ä¸çå·èº«è§è§çè§£ï¼</strong> "EgoNight" å "Diffusion Models for Low-Light Image Enhancement" å¼ºè°äºå¨æææ§åç§æ¡ä»¶ä¸ï¼ç¹å«æ¯ç¬¬ä¸äººç§°è§è§ä¸çè§è§çè§£éæ±ï¼è¿å¯¹äºèªå¨é©¾é©¶ãæºå¨äººç­é¢åè³å³éè¦ã</li>
<li><strong>ææéå»ºä¸é®æ¡å¤çï¼</strong> "Overlap-aware segmentation for topological reconstruction of obscured objects" å³æ³¨å¤æé®æ¡ä¸çç©ä½ç»æéå»ºï¼æ¯ä¼ ç»åå²ä»»å¡çè¿é¶ï¼å·æè¾é«çåºç¨ä»·å¼ã</li>
<li><strong>è¿­ä»£å¼è§é¢å³é®å¸§å®ä½ä¸ç­ç¥ä¼åï¼</strong> "VideoMiner" æåºçéè¿æ ç¶ç»ç¸å¯¹ç­ç¥ä¼åæ¥è¿­ä»£å®ä½é¿è§é¢å³é®å¸§çæ¹æ³ï¼ä¸ºé¿è§é¢çè§£æä¾äºä¸ç§æ°é¢çè§£å³æ¹æ¡ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºå³æ³¨LMMsåè§é¢çè§£çç ç©¶äººåï¼</strong><ul>
<li><strong>"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models"</strong> (Yunlong Tang et al.)ï¼æ·±å¥äºè§£LMMså¨è§é¢æ¨çä¸­çåºç¨åæ½åã</li>
<li><strong>"Bridging Text and Video Generation: A Survey"</strong> (Nilay Kumar et al.)ï¼å¨é¢äºè§£ææ¬å°è§é¢çæé¢åçç°ç¶åææã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨é²æ£æ§åæææ§ç¯å¢çç ç©¶äººåï¼</strong><ul>
<li><strong>"EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark"</strong> (Deheng Zhang et al.)ï¼äºè§£å¤é´å·èº«è§è§çè§£çææ°è¿å±åæ°åºåã</li>
<li><strong>"Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis"</strong> (Eashan Adhikarla et al.)ï¼æ·±å¥äºè§£æ©æ£æ¨¡åå¨ä½åç§å¾åå¢å¼ºä¸­çåºç¨ã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨çè®ºåºç¡åèªçç£å­¦ä¹ çç ç©¶äººåï¼</strong><ul>
<li><strong>"Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density"</strong> (Randall Balestriero et al.)ï¼æ¢ç´¢JEPAsçåºå±å­¦ä¹ æºå¶ã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨æ°æ®éåå®éåºç¨çç ç©¶äººåï¼</strong><ul>
<li><strong>"Kaputt: A Large-Scale Dataset for Visual Defect Detection"</strong> (Sebastian HÃ¶fer et al.)ï¼å¯¹äºå·¥ä¸æ£æµåç¼ºé·è¯å«é¢åçç ç©¶äººåéå¸¸æä»·å¼ã</li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></li>
<li><a href="#2510.05034v1">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></li>
<li><a href="#2510.04999v1">Bridging Text and Video Generation: A Survey</a></li>
<li><a href="#2510.06219v1">Human3R: Everyone Everywhere All at Once</a></li>
<li><a href="#2510.06218v1">EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</a></li>
<li><a href="#2510.06194v1">Overlap-aware segmentation for topological reconstruction of obscured objects</a></li>
<li><a href="#2510.06040v1">VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</a></li>
<li><a href="#2510.06009v1">Continual Learning for Image Captioning through Improved Image-Text Alignment</a></li>
<li><a href="#2510.05949v1">Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</a></li>
<li><a href="#2510.05903v1">Kaputt: A Large-Scale Dataset for Visual Defect Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.05976v1'></a></p>
<h2 id="diffusion-models-for-low-light-image-enhancement-a-multi-perspective-taxonomy-and-performance-analysis"><a href="https://arxiv.org/abs/2510.05976v1">Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</a></h2>
<p><strong>Authors:</strong> Eashan Adhikarla, Yixin Liu, Brian D. Davison</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Eashan Adhikarla, Yixin Liu, Brian D. Davisonæ°åçè®ºæâDiffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysisâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä½åç§å¾åå¢å¼ºï¼LLIEï¼é¢åä¸­çæ ¸å¿ææï¼å³å¨å®å¨å³é®åºç¨ï¼å¦çæ§ãèªå¨é©¾é©¶ãå»å­¦æåï¼ä¸­ï¼å¯è§åº¦ä¸éä¼ä¸¥éå½±åä¸æ¸¸ä»»å¡çæ§è½ãå·ä½èè¨ï¼å®å³æ³¨æ©æ£æ¨¡åå¨LLIEä¸­çåºç¨ï¼å¹¶å¯¹å¶è¿è¡å¨é¢çæ¹å¤æ§åæï¼åæ¬ä¸ç°ææåè¿æ¹æ³çæ¯è¾ãå®éé¨ç½²ææä»¥åæ°å´èå¼ï¼å¦åºç¡æ¨¡åï¼çä½ç¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
*   <strong>å¤è§è§åç±»æ³ï¼</strong> è®ºææåºäºä¸ä¸ªæ¶µçå­ä¸ªç±»å«çå¤è§è§åç±»æ³ï¼åå¨åè§£ï¼Intrinsic Decompositionï¼ãåè°±ä¸æ½å¨ï¼Spectral &amp; Latentï¼ãå éï¼Acceleratedï¼ãå¼å¯¼ï¼Guidedï¼ãå¤æ¨¡æï¼Multimodalï¼åèªä¸»ï¼Autonomousï¼æ©æ£æ¨¡åãè¯¥åç±»æ³åºäºæ¨¡åæºå¶åæ¡ä»¶ä¿¡å·çæ··åè§å¾ï¼å°å¢å¼ºæ¹æ³æ å°å°ç©çåéªãæ¡ä»¶æ¹æ¡åè®¡ç®æçä¸ã
*   <strong>æ§è½è¯ä¼°ï¼</strong> è®ºæå¯¹æ©æ£æ¨¡åä¸åºäºçæå¯¹æç½ç»ï¼GANï¼åTransformerçæåè¿æ¹æ³è¿è¡äºæ·±å¥çæ¯è¾æ§è½è¯ä¼°ï¼æ¶µçäºå®æ§å¤±ææ¨¡å¼ãåºåæµè¯ä¸ä¸è´æ§ä»¥åå¯è§£éæ§ãæ³åæ§åæ¨çæçä¹é´çæè¡¡ã
*   <strong>ææä¸æªæ¥å±æï¼</strong> è®ºææ·±å¥æ¢è®¨äºå®éé¨ç½²ææï¼å¦åå­ãè½èï¼åä¼¦çèéï¼å¹¶æåºäºæ©æ£æ¨¡åå¨LLIEé¢åçæªæ¥ç ç©¶æ¹åï¼åæ¬æ°é¢çæ¡ä»¶ä½ç¨ãå®æ¶éåºæ§ä»¥ååºç¡æ¨¡åçæ½åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>æ©æ£æ¨¡åçä¼å¿ï¼</strong> æ©æ£æ¨¡åå å¶éè¿è¿­ä»£å»åªå»ºæ¨¡å¤æå¾ååå¸çè½åï¼å¨LLIEä¸­å±ç°åºå·¨å¤§çæ½åï¼è½å¤çæé«åº¦é¼ççç»èåçº¹çï¼è§£å³äºæ©ææ¹æ³çå³é®ç¼ºé·ãå®ä»¬å¨è¾åºè´¨éåè®­ç»ç¨³å®æ§æ¹é¢è¡¨ç°åºè²ï¼å¹¶è½ææç¼è§£æ¨¡å¼å´©æºã
*   <strong>LLIEçæä¸é¾å°å¢ï¼</strong> è®ºæè¯å«äºLLIEçææ¨¡åå¨è´¨éãå¤æ ·æ§åå»¶è¿ä¹é´çä¸é¾å°å¢ãæ©æ£æ¨¡åéè¿è¸é¦ãæ ¡æ­£/ä¸è´æ§æµåæ½å¨ç©ºé´æä½ï¼æ©å±äºå¯è¡åºåï¼å¨è¾ä½éæ ·ææ¬ä¸å®ç°äºæ´é«çæç¥è´¨éåæ´å¥½çæ¨¡å¼è¦çã
*   <strong>æè¡¡åæï¼</strong> è®ºæå¼ºè°äºå¨åµå¥å¼ºç©çåéªï¼å¯è§£éæ§é«ä½åéäºåè®¾ï¼åä¿ææ¨¡åçµæ´»æ§ï¼æ³åæ§å¼ºä½å¯è½äº§çå¹»è§ï¼ä¹é´çâåéªä¸å¯å¡æ§âå°å¢ãæ­¤å¤ï¼è¿è®¨è®ºäºæç¥è´¨éãä¿çåº¦åæçä¹é´çæè¡¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è®¡ç®å¼éåæ¨çå»¶è¿ï¼</strong> æ©æ£æ¨¡åçè¿­ä»£å»åªè¿ç¨å¯¼è´é«è®¡ç®ææ¬åæ¨çå»¶è¿ï¼ä½¿å¶é¾ä»¥å®æ¶é¨ç½²ã
*   <strong>æ°æ®ä¾èµæ§åç¨ç¼ºæ§ï¼</strong> çç£å¼æ©æ£æ¨¡åä¸¥éä¾èµå¤§è§æ¨¡ãé«è´¨éãå¤æ ·åçéå¯¹è®­ç»æ°æ®ï¼ä½æ­¤ç±»æ°æ®è·åå°é¾ï¼éå¶äºæ¨¡åçæ³åè½åã
*   <strong>æ³åæ§åé²æ£æ§ï¼</strong> æ¨¡åå¨é¢å¯¹ä¸è®­ç»æ°æ®æ¾èä¸åçåå¸å¤ï¼OODï¼è¾å¥æ¶ï¼æ§è½å¯è½ä¸éï¼å°¤å¶æ¯å¨æç«¯é»æãéååç§æåç¹å®ä¼ æå¨åªå£°æ¨¡å¼ä¸ã
*   <strong>å¯è§£éæ§ï¼</strong> æ©æ£æ¨¡åä½ä¸ºâé»ç®±âè¿è¡ï¼é¾ä»¥ç²¾ç¡®çè§£å¶å¢å¼ºè¾åºæäº§çç¹å®ä¼ªå½±çåå ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å©ç¨åºç¡æ¨¡åï¼</strong> ææå°å°ç°æå¼ºå¤§çå¾åçææ©æ£æ¨¡åï¼å¦Stable Diffusionï¼åºç¨äºLLIEä»»å¡ï¼éè¿ä¸é¨çå¾®è°ææ¯ãæ°é¢çæç¤ºç­ç¥æé¶æ ·æ¬å¼å¯¼æºå¶ã
*   <strong>å¤æ¨¡æåºç¡æ¨¡åï¼</strong> æ¢ç´¢è½å¤å¤çææ¬ãé³é¢æå¶ä»ä¼ æå¨ä¿¡æ¯ä¸è§è§æ°æ®ç¸ç»åçå¤æ¨¡æåºç¡æ¨¡åï¼ä»¥å®ç°æ´å·ä¸ä¸ææç¥åå¯æ§æ§çLLIEã
*   <strong>å®æ¶åè®¾å¤ç«¯LLIEï¼</strong> æç»­ç ç©¶æ¨¡ååç¼©ãç¥è¯è¸é¦åé«æç½ç»æ¶æè®¾è®¡ï¼ä»¥åå¼åä¸é¨çç¡¬ä»¶ä»¥å éæ©æ£æ¨¡åè®¡ç®ã
*   <strong>æ çç£ãèªçç£åé¶æ ·æ¬å­¦ä¹ ï¼</strong> å¼åæ´å¤æçæ çç£æ¹æ³ï¼ä»¥æ´å¥½å°è¡¨ç¤ºçå®ä¸çä½åç§éåï¼å®ç°è·¨ä¸ååçé²æ£æ³åï¼å¹¶å­¦ä¹ åå®¹ãç§æãåªå£°åé¢è²ç­è§£è¦è¡¨ç¤ºã
*   <strong>å¢å¼ºå¯æ§æ§åå¯è§£éæ§ï¼</strong> å®ç°æ´ç»ç²åº¦çè¯­ä¹æ§å¶ï¼è¶è¶ç®åçææ¬æç¤ºææ©ç ï¼å¹¶å¼åéå¯¹çæå¼æ©æ£æ¨¡åçXAIææ¯ï¼ä»¥æé«ä¿¡ä»»åº¦åè°è¯è½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models.</li>
<li>We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral &amp; Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency.</li>
<li>This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05976v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05976v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05034v1'></a></p>
<h2 id="video-lmm-post-training-a-deep-dive-into-video-reasoning-with-large-multimodal-models"><a href="https://arxiv.org/abs/2510.05034v1">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></h2>
<p><strong>Authors:</strong> Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yunlong Tangç­äººæ°åçè®ºæâVideo-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Modelsâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼Video-LMM åè®­ç»ï¼æ·±å¥æ¢ç´¢å¤§åå¤æ¨¡ææ¨¡åä¸­çè§é¢æ¨ç</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è§é¢çè§£é¢åçæ ¸å¿ææï¼å³å¦ä½å°æ°å´çè§é¢å¤§åå¤æ¨¡ææ¨¡åï¼Video-LMMsï¼ä»åºæ¬çæç¥ç³»ç»æåä¸ºå¤æçæ¨çå¼æãå·ä½èè¨ï¼å®å³æ³¨äºâåè®­ç»âè¿ä¸å³é®é¶æ®µï¼è¯¥é¶æ®µå¨ç°ææç®ä¸­ä»æ¾é¶æ£ï¼ç¼ºä¹ç³»ç»æ§çæ´åååæãè®ºæçæ ¸å¿é®é¢æ¯ï¼å¦ä½éè¿ç³»ç»åçåè®­ç»æ¹æ³ï¼ä½¿Video-LMMsè½å¤ææå¤çå¤æçæ¶ç©ºå³ç³»ãé¿æä¾èµåå¤æ¨¡æè¯æ®ï¼å¹¶åºå¯¹è§é¢ç¹æçææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçä¸»è¦åæ°å¨äºé¦æ¬¡å¯¹Video-LMMsçåè®­ç»æ¹æ³è¿è¡äºå¨é¢çç³»ç»æ§å®¡æ¥ååç±»ãå®å°åè®­ç»åä¸ºä¸ä¸ªæ ¸å¿æ¯æ±ï¼
*   <strong>å¸¦æç»´é¾ççç£å¾®è°ï¼SFT-CoTï¼ï¼</strong> å¼ºè°SFTå¨å»ºç«ç»æåæ¨çè¡ä¸ºãå¤æ¨¡æå¯¹é½åæä»¤éµå¾ªè½åæ¹é¢çä½ç¨ï¼å¹¶å°å¶è§ä¸ºå¼ºåå­¦ä¹ ï¼RLï¼çå·å¯å¨é¶æ®µã
*   <strong>åºäºå¯éªè¯ç®æ çå¼ºåå­¦ä¹ ï¼RLï¼ï¼</strong> è¯¦ç»éè¿°äºRLï¼ç¹å«æ¯GRPOï¼Group Relative Policy Optimizationï¼åå¶åä½ï¼å¦ä½éè¿å¯éªè¯çå¥å±ï¼å¦ç­æ¡æ­£ç¡®æ§ãæ¶ç©ºå®ä½ç²¾åº¦ï¼æ¥å¢å¼ºæ¨çåèªæä¿®æ­£ï¼é¿åå¯¹äººå·¥åå¥½æ°æ®çä¾èµã
*   <strong>éè¿å¢å¼ºæ¨çè®¡ç®è¿è¡æµè¯æ¶æ©å±ï¼TTSï¼ï¼</strong> æ¢è®¨äºTTSå¦ä½éè¿æ¨çæ ·æ¬å¢å¼ºãæç¥¨æºå¶ãèªæ´½æ§æ£æ¥ãå¤é¨éªè¯å¨åå¤è·¯å¾æç´¢æ¥æé«æ¨¡åå¯é æ§ã</p>
<p>è®ºæè¿æåºäºä¸ä¸ªç»æåçåç±»æ³ï¼éæäºè¿äºææ¯å¨è§£å³è§é¢ç¹å®ææï¼å¦æ¶é´å®ä½ãæ¶ç©ºåºç¡ãé¿è§é¢æçåå¤æ¨¡æè¯æ®æ´åï¼ä¸­çä½ç¨ãç¸äºèç³»åéåºæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥è®ºæéè¿å¯¹ä»£è¡¨æ§æ¹æ³çç³»ç»åæï¼ç»¼åäºå³é®çè®¾è®¡ååãè§è§£åè¯ä¼°åè®®ï¼å¹¶å¼ºè°äºä»¥ä¸åç°ï¼
*   <strong>SFTä½ä¸ºRLçå·å¯å¨ï¼</strong> SFT-CoTä¸ºRLæä¾äºç»æåçæ¨çæ ¼å¼åç¨³å®çåå§åï¼ææé²æ­¢äºRLé©±å¨ç­ç¥ä¼åä¸­çä¸ç¨³å®æ§ã
*   <strong>GRPOå¨è§é¢æ¨çä¸­çæææ§ï¼</strong> GRPOåå¶åä½ï¼å¦T-GRPOãReg-GRPOãTW-GRPOãDGRPOï¼å¨è§é¢çè§£ä»»å¡ä¸­è¡¨ç°åºè²ï¼å°¤å¶æ¯å¨å©ç¨å¯éªè¯å¥å±æ¹é¢ï¼è¿æé«äºæ°æ®æçå¹¶åå°äºå¯¹äººå·¥æ æ³¨çä¾èµã
*   <strong>TTSæåå¯é æ§ï¼</strong> TTSæ¹æ³ï¼å¦æç»´é¾æç¤ºãèªæ´½æ§è§£ç ãåºäºç½®ä¿¡åº¦çè¿­ä»£æ¨çãèªæ¹è¿å¾ªç¯ãèç¹å¡æ´æ æç´¢åå·¥å·å¢å¼ºæ¨çï¼å¨æ¨çé¶æ®µåéè®¡ç®èµæºï¼æ¾èæé«äºVideo-LMMsçå¯é æ§ååç¡®æ§ã
*   <strong>ç»ä¸çè¯ä¼°æ¡æ¶ï¼</strong> è®ºææ´çäºéè¦çåºåãæ°æ®éåè¯ä¼°ææ ï¼ä¸ºVideo-LMMåè®­ç»ææçä¸¥æ ¼è¯ä¼°æä¾äºç»ä¸çæ¡æ¶ï¼æå©äºç ç©¶äººåæ´åç¡®å°è¯æ­æ¨¡åæ¹è¿çæ¥æºã</p>
<p>è¿äºç»æçæä¹å¨äºï¼å®ä»¬ä¸ºVideo-LMMsä»æç¥å°å¤ææ¨ççæ¼è¿æä¾äºä¸ä¸ªæ¸æ°çè·¯çº¿å¾ï¼å¹¶å¼ºè°äºåè®­ç»å¨å®ç°è¿ä¸ç®æ ä¸­çæ ¸å¿ä½ç¨ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæå¨è®¨è®ºå¼æ¾æææ¶æåäºå½åæ¹æ³çå±éæ§ï¼
*   <strong>å¥å±è®¾è®¡ææï¼</strong> å°½ç®¡GRPOç­æ¹æ³åå¾äºè¿å±ï¼ä½è®¾è®¡å¯éªè¯ãç»åå¼å¥å±ï¼ç¹å«æ¯éå¯¹å¤ææ¶ç©ºè¯­ä¹æ£æ¥çå¥å±ï¼ä»ç¶æ¯ä¸ä¸ªææãè¿ç¨å¥å±æ¨¡åï¼PRMsï¼è½ç¶è½æä¾å¯éä¿¡ç¨ï¼ä½å¶æå»ºææ¬ååå·®æ§å¶ä»éæ¹è¿ã
*   <strong>å¯æ©å±æ§é®é¢ï¼</strong> å¨é¿è§é¢ä¸æ©å±å¼ºåå­¦ä¹ ä»ç¶é¢ä¸´é¢ç®éå¶ï¼éè¦æ´é«æçå¸§éæ©åç¼å­æºå¶ã
*   <strong>ææ¬-æ§è½ä¼åï¼</strong> å¸§ä¼åååç¼©æ¡æ¶ä»ç¶ææ¬é«æï¼æªæ¥å·¥ä½éè¦ä½¿å¶å¨æ°æ®åè®¡ç®ä¸æ´é«æã
*   <strong>æ°æ®ç¨ç¼ºååå·®ï¼</strong> é«è´¨éçè§é¢æç»´é¾çç£æ°æ®è·åææ¬é«æï¼ä¸ç°ææ°æ®å¯è½å­å¨æ¨¡æ¿ååæ¨¡ååå·®ã
*   <strong>å¹»è§é®é¢ï¼</strong> å°½ç®¡å¼å¥è§è§åºç¡ä¿¡æ¯æå©äºåå°å¹»è§ï¼ä½æ¨¡åä»å¯è½æé ä¸å­å¨çå®ä½æäºä»¶ã
*   <strong>è¯ä¼°åå·®ï¼</strong> ä½¿ç¨LLMä½ä¸ºè¯ä¼°å¨æ¶ï¼å¤æ­åå·®åé¿åº¦åå·®å¯è½æ­æ²è¯ä¼°ç»æï¼éè¦æ´ä¸¥æ ¼çæ¥åæ ååäººå·¥/éªè¯å¨å®¡è®¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>ç»æåæ¥å£ååºç¡æç»´é¾ï¼</strong> è§èæ¨çæ ¼å¼ï¼å°æ­¥éª¤ä¸è¯æ®ï¼æ¶é´æ³ãå¸§IDãåºåï¼ç»å®ï¼ä»¥æé«å¿ å®åº¦å¹¶ç®åéªè¯å¨è®¾è®¡ã
*   <strong>å¤§è§æ¨¡éªè¯å¨å¨ç¯æç»´é¾åæï¼</strong> èªå¨åèç¨¿-ç»å-å®¡è®¡æµç¨ï¼ä»ASR/OCR/éå¤´åæ°æ®å¼å§ï¼éè¿è½»éçº§æ£æ¥å¨è¿è¡ç»ååè¿æ»¤ï¼ä»¥åå°å¹»è§ã
*   <strong>ä¸æ¨¡æçç£åå­å¹æ§å¶ï¼</strong> æ©å±SFTä»¥å¯¹é½è¯­é³ãäºä»¶åè§è§è¯æ®ï¼å¹¶å§ç»æ¥åå¸¦å­å¹åä¸å¸¦å­å¹çç»æï¼ä»¥é¿åASRå¿«æ·æ¹å¼ã
*   <strong>å¹»è§æç¥æä»¤å¾®è°ï¼</strong> ç»ååäºå®åç¼ºå¤±æ¡ä¾ï¼è®­ç»æ¨¡åè¿è¡æ ¡åçå¼æåéªè¯è¡ä¸ºã
*   <strong>å¤è¯­è¨ãOCRååäºç»æï¼</strong> æ©å±SFTä»¥å¤çå¤è¯­è¨ãéåææ¬åé¿ç¯æäºæ¨çã
*   <strong>ç»åå¼ãå¯éªè¯å¥å±ï¼</strong> å¼åæ´ç²¾ç»çå¥å±æºå¶ï¼ä»¥å¤çå¤æçæ¶ç©ºè¯­ä¹æ£æ¥ã
*   <strong>æ ·æ¬æçåé¿è§é¢ææ¬ï¼</strong> æ¢ç´¢ç¦»çº¿ååºäºæ¨¡åçRLåä½ãä¸çæ¨¡ååå¾®åæ»å¨ï¼ä»¥æé«æ¢ç´¢æçã
*   <strong>è¶è¶æå¸çæ¢ç´¢ï¼</strong> å¼åå¤æ ·æ§é©±å¨çç®æ åèªåå¼æºå¶ï¼ä»¥åç°è¶è¶æå¸ç­ç¥çæ°ç­ç¥ã
*   <strong>èªä¿¡æç¥ãéªè¯å¨å¼å¯¼çTTSï¼</strong> å°åæ­¢è§åä¸ä¸ç¡®å®æ§ç»åï¼å¹¶ä¸éªè¯å¨æ£æ¥è¦åï¼ä»¥å®ç°éæ¶åç¡®æ§ã
*   <strong>å·¥å·å¢å¼ºæ¨çåè¸é¦ï¼</strong> å°å·¥å·è°ç¨ï¼æ£ç´¢ãè·è¸ªãASRå¯¹é½ï¼ä¸æ¨çäº¤ç»ï¼å¹¶éè¿åéªè¸é¦å°è¿äºä¼å¿è½¬ç§»å°åºç¡æ¨¡åä¸­ã
*   <strong>å¸¦è®°å¿çæµå¼ä»£çï¼</strong> å¼åè½å¤å³å®ä½æ¶è§çãè§çä»ä¹ï¼å¹¶ç»´æ¤ä»»å¡æç¥å·¥ä½è®°å¿çä»£çè§åå¨ã
*   <strong>æ ååæ¥ååæ³æ¼æ§å¶ï¼</strong> æ¥åè§çé¢ç®ãæ¨çé¿åº¦ãè·¯å¾è®¡æ°ãå»¶è¿/ååéåå­å¹ä½¿ç¨æåµï¼å¹¶è¿è¡åè¢è¯æ­ã
*   <strong>åéè§çä¸çè®¡ç®-åç¡®æ§æè¡¡ï¼</strong> ååè°æ´å¸§éæ©ååç¼©ä¸æ¨çè´¨éï¼ä»¥å¨å¤çå°éå¸§æ¶ä¿æç³»ç»å¼ºå¤§ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºVideo-LMMsçåè®­ç»æä¾äºä¸ä¸ªå¨é¢çè§è§ï¼ä¸ä»ç³»ç»å°æ¢³çäºç°æææ¯ï¼è¿æåºäºæªæ¥çç ç©¶æ¹åï¼æ¨å¨æ¨å¨è§é¢çè§£é¢ååæ´å¼ºå¤§ãæ´éç¨çäººå·¥æºè½ç³»ç»åå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05034v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05034v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04999v1'></a></p>
<h2 id="bridging-text-and-video-generation-a-survey"><a href="https://arxiv.org/abs/2510.04999v1">Bridging Text and Video Generation: A Survey</a></h2>
<p><strong>Authors:</strong> Nilay Kumar, Priyansh Bhandari, G. Maragatham</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.GR, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Nilay Kumar, Priyansh Bhandari, G. Maragathamæ°åçè®ºæâBridging Text and Video Generation: A Surveyâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼ææ¬å°è§é¢çæææ¯ç»¼è¿°</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨å¨é¢ç»¼è¿°ææ¬å°è§é¢ï¼Text-to-Video, T2Vï¼çæææ¯çåå±ç°ç¶ãææåæªæ¥æ¹åãT2Vææ¯å·æå·¨å¤§çæ½åï¼è½å¤æ ¹æ®èªç¶è¯­è¨æç¤ºåå»ºè¿è´¯çè§è§åå®¹ï¼ä»èå¨æè²ãè¥éãå¨±ä¹åè¾å©ææ¯ç­å¤ä¸ªé¢åå¸¦æ¥åé©ãç¶èï¼è¯¥é¢åä»é¢ä¸´è¯¸å¤ææï¼åæ¬çæåå®¹çè¯­ä¹å¯¹é½ãé¿ææ¶é´è¿è´¯æ§ä»¥åè®¡ç®æçç­é®é¢ãæ¬ç»¼è¿°è¯å¾ç³»ç»å°æ¢³çT2Væ¨¡åä»æ©æå°ææ°çæ¼åï¼å¹¶æ¢è®¨å¶å¦ä½åºå¯¹è¿äºææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææ¬èº«æ¯ä¸ç¯ç»¼è¿°æ§æç« ï¼å¶ä¸»è¦è´¡ç®å¨äºå¯¹T2Vé¢åè¿è¡äºç³»ç»æ§ãç»æåçæ¢³çååæï¼èéæåºæ°çæ¨¡åæç®æ³ãå¶å³é®è´¡ç®åæ¬ï¼
*   <strong>æ¨¡ååå±è·¯çº¿å¾ï¼</strong> è¯¦ç»è¿½æº¯äºT2Vçææ¨¡åä»æ©æççæå¯¹æç½ç»ï¼GANsï¼åååèªç¼ç å¨ï¼VAEsï¼å°å½åæ··åæ©æ£-Transformerï¼DiTï¼æ¶æçæ¼åè¿ç¨ãæç« æ·±å¥æ¢è®¨äºè¿äºæ¨¡åçåé¨å·¥ä½æºå¶ãå®ä»¬å¦ä½è§£å³åä»£æ¨¡åçå±éæ§ï¼ä»¥åä¸ºä½éè¦è½¬åæ°çæ¶æèå¼ä»¥åæè´¨éãè¿è´¯æ§åæ§å¶æ¹é¢çææã
*   <strong>æ°æ®éåè®­ç»éç½®çç³»ç»æ§ä»ç»ï¼</strong> æä¾äºå¯¹T2Væ¨¡åè®­ç»åè¯ä¼°æç¨æ°æ®éçç³»ç»æ§æè¿°ï¼åæ¬å¶è§æ¨¡ãå¤æ ·æ§ååå®¹ç¹å¾ãä¸ºäºæ¯æç ç©¶çå¯å¤ç°æ§åè¯ä¼°æ¨¡åè®­ç»çå¯è¡æ§ï¼è®ºæè¿è¯¦ç»ååºäºè®­ç»éç½®ï¼å¦ç¡¬ä»¶è§æ ¼ãGPUæ°éãæ¹æ¬¡å¤§å°ãå­¦ä¹ çãä¼åå¨ãè®­ç»å¨æç­å³é®è¶åæ°ã
*   <strong>è¯ä¼°ææ ååºåçå¨é¢åæï¼</strong> æ¦è¿°äºT2Væ¨¡åå¸¸ç¨çè¯ä¼°ææ ï¼å¹¶å±ç¤ºäºå®ä»¬å¨æ ååºåä¸çæ§è½ãåæ¶ï¼è®ºæè®¨è®ºäºè¿äºç°æææ çå±éæ§ï¼å¹¶å¼ºè°äºåæ´å¨é¢ãæç¥å¯¹é½çè¯ä¼°ç­ç¥è½¬åçå¿è¦æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
æ¬ç»¼è¿°éè¿å¯¹ç°æT2Væ¨¡åçè¯¦ç»åæï¼æ­ç¤ºäºè¯¥é¢åå¨ä»¥ä¸æ¹é¢åå¾çæ¾èè¿å±ï¼
*   <strong>æ¨¡åæ¶æçæ¼è¿ï¼</strong> T2Væ¨¡åå·²ä»æåçGANsåVAEsåå±å°æ´åè¿çæ©æ£æ¨¡ååTransformeræ¶æï¼æ¾èæé«äºçæè§é¢çä¿çåº¦åæ¶é´ä¸è´æ§ãç¹å«æ¯æ©æ£æ¨¡åï¼éè¿éæ­¥å»åªè¿ç¨ï¼å¨çæé«è´¨éãè¯­ä¹å¯¹é½çè§é¢æ¹é¢è¡¨ç°åºè²ã
*   <strong>æ§è½æåï¼</strong> éçæ¨¡ååè®­ç»ç­ç¥çæ¹è¿ï¼T2Væ¨¡åå¨åç§åºåæµè¯ï¼å¦UCF-101ãMSRVTTãKinetics-600ç­ï¼ä¸çå®éè¯ä¼°ææ ï¼å¦ISãFIDãCLIP-SIMãFVDãKVDï¼æ¾ç¤ºåºæç»­çæ§è½æåã
*   <strong>å¯¹å¯å¤ç°æ§çå³æ³¨ï¼</strong> è®ºæè¯¦ç»åä¸¾äºæ¨¡åçè®­ç»éç½®ï¼è¿å¯¹äºæªæ¥ç ç©¶èçè§£ãå¤ç°åè¿ä¸æ­¥å¼åT2Væ¨¡åå·æéè¦æä¹ï¼æå©äºéä½è¯¥é¢åçè¿å¥é¨æ§ã
*   <strong>è¯ä¼°æ¹æ³çæ¼åï¼</strong> å¼ºè°äºä»åä¸å®éææ åç»åäººç±»è¯ä¼°åæ´å¨é¢çãæç¥å¯¹é½çè¯ä¼°ç­ç¥ï¼å¦VBenchï¼è½¬åçéè¦æ§ï¼ä»¥æ´å¥½å°ææè§é¢è´¨éçç»å¾®å·®å«åä¸»è§æç¥ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææç¡®æåºäºT2Vé¢åå½åé¢ä¸´çå å¤§å±éæ§ï¼
*   <strong>æ°æ®å¯ç¨æ§éå¶ï¼</strong> ç¼ºä¹å¤§è§æ¨¡ãé«è´¨éçææ¬-è§é¢éå¯¹æ°æ®éï¼è¿éå¶äºæ¨¡åçæ³åè½ååçæè§é¢çè´¨éã
*   <strong>è®¡ç®ææ¬é«æï¼</strong> è§é¢çæä»»å¡çè®¡ç®ææ¬è¿é«äºå¾åçæï¼å¯¹ç¡¬ä»¶èµæºåè®­ç»æ¶é´æåºäºå·¨å¤§ææã
*   <strong>æ¶é´ä¸è´æ§å»ºæ¨¡å°é¾ï¼</strong> ç¡®ä¿çæè§é¢çé¿ææ¶é´è¿è´¯æ§ãé¿åè§è§è·³ååä¸èªç¶è¿æ¸¡ä»ç¶æ¯ä¸ä¸ªæ ¸å¿ææã
*   <strong>è¯­ä¹å¯¹é½ä¸è¶³ï¼</strong> å°¤å¶æ¯å¨å¤å¯¹è±¡æå¨ä½ä¸°å¯çåºæ¯ä¸­ï¼ææ¬æè¿°ä¸è§é¢åå®¹ä¹é´çè¯­ä¹å¯¹é½ä»éæ¹è¿ã
*   <strong>ç°æè¯ä¼°ææ çå±éæ§ï¼</strong> ä¼ ç»çå®éææ ï¼å¦ISãFIDï¼å¾å¾æ æ³å¨é¢ææäººç±»å¯¹è§é¢çå®æãè¯­ä¹å¯¹é½åæ¶é´è¿è´¯æ§çæç¥ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºå¯¹å½åææçåæï¼è®ºææåºäºå ä¸ªæåæ¯çæªæ¥ç ç©¶æ¹åï¼
*   <strong>æ°æ®éä¸°å¯ï¼</strong> æ¢ç´¢ä½¿ç¨æ¸¸æå¼æï¼å¦UnityæUnreal Engineï¼åæå¤§è§æ¨¡ãé«åè¾¨çãå¤æ ·åçæ°æ®éï¼ä»¥åæçæéå¶åæ°æ®ç¨ç¼ºé®é¢ãå¼åéç¨çæç¤ºæ¡æ¶ï¼éè¿ç»æåæç¤ºçæè§é¢ï¼ä»¥å®ç°è¯­ä¹æ­£ç¡®åè§è§çå®çè§é¢ã
*   <strong>æ¨¡åæ¶æåä¼åï¼</strong> ç åæ´é«æçT2Væ¨¡åæ¶æåç®æ³ï¼ä»¥ä¼åè®¡ç®æçï¼æ´å¥½å°å¤çæ¶é´åºåæ°æ®ãæ¹è¿æ¶é´å»ºæ¨¡æºå¶ï¼ä»¥çææ´é¿ãæ´è¿è´¯çè§é¢ã
*   <strong>å¢å¼ºå¯¹é½åæ§å¶ï¼</strong> è¿ä¸æ­¥åå±æ³¨æåæºå¶ãå¤æ¨¡ææ°æ®èååæå¤±å½æ°ï¼ä»¥æ´ææå°å³æ³¨è¿è´¯æ§åçå®æï¼å¹¶æ´å¥½å°æ¨¡æç©çäº¤äºã
*   <strong>æ´å¨é¢çè¯ä¼°ç­ç¥ï¼</strong> æ¨å¹¿åæ åååVBenchè¿æ ·å¤ç»´åº¦çãç»åäººç±»åå¥½æ³¨éçè¯ä¼°åºåï¼ä»¥æä¾æ´ç»ç²åº¦ãæ´ç¬¦åäººç±»æç¥çæ¨¡åæ§è½è¯ä¼°ã
*   <strong>å®éåºç¨æå±ï¼</strong> æ¢ç´¢T2Vææ¯å¨æè²ãè¾å©ææ¯ãåå®¹åä½ãè¥éãæåéäº§ä¿æ¤ãæ³å¾åè¯ãåææ°æ®çæåæ¸¸æ/VRç­é¢åçæ´å¹¿æ³åºç¨ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºææ¬å°è§é¢çæé¢åæä¾äºä¸ä¸ªå¨é¢çå¿«ç§ï¼ä¸ä»æ»ç»äºè¯¥é¢åçåå²åå±åå½åæå°±ï¼è¿æç¡®æåºäºå¶é¢ä¸´çææï¼å¹¶ä¸ºæªæ¥çç ç©¶ææäºæ¹åãå¯¹äºå¸ææ·±å¥äºè§£T2Vææ¯ç°ç¶åæªæ¥è¶å¿çç ç©¶äººåæ¥è¯´ï¼è¿æ¯ä¸ä»½å®è´µçèµæºã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04999v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04999v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06219v1'></a></p>
<h2 id="human3r-everyone-everywhere-all-at-once"><a href="https://arxiv.org/abs/2510.06219v1">Human3R: Everyone Everywhere All at Once</a></h2>
<p><strong>Authors:</strong> Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, Gerard Pons-Moll</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos. Unlike previous approaches that rely on multi-stage
pipelines, iterative contact-aware refinement between humans and scenes, and
heavy dependencies, e.g., human detection, depth estimation, and SLAM
pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies
("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a
single forward pass ("all-at-once"). Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates
heavy dependencies and iterative refinement. After being trained on the
relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it
achieves superior performance with remarkable efficiency: it reconstructs
multiple humans in a one-shot manner, along with 3D scenes, in one stage, at
real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model. We hope that Human3R will serve as a simple yet strong
baseline, be easily extended for downstream applications.Code available in
https://fanegg.github.io/Human3R</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yue Chenç­äººæ°åçè®ºæâHuman3R: Everyone Everywhere All at Onceâçå¨é¢æè¦ã</p>
<hr />
<h3 id="human3r-everyone-everywhere-all-at-once_1">è®ºææè¦ï¼Human3R: Everyone Everywhere All at Once</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä»éææè·çåç®è§é¢ä¸­ï¼å¨ä¸çåæ ç³»ä¸è¿è¡å¨çº¿4Däººä½-åºæ¯éå»ºçææãä¼ ç»æ¹æ³éå¸¸ä¾èµå¤é¶æ®µæµç¨ãè¿­ä»£ä¼åä»¥åå¯¹äººä½æ£æµãæ·±åº¦ä¼°è®¡åSLAMé¢å¤çç­éåº¦ä¾èµï¼è¿éå¶äºå®ä»¬å¨å®æ¶ãç»ä¸åå¯æ©å±åºç¨ä¸­çæ§è½ãHuman3Rå¯»æ±æä¾ä¸ä¸ªç»ä¸çãååä¼ æ­çæ¡æ¶ï¼è½å¤åæ¶ãå®æ¶å°éå»ºå¤ä¸ªäººä½ç½æ ¼ãå¯é3Dåºæ¯åç¸æºè½¨è¿¹ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»ä¸çãååä¼ æ­æ¡æ¶ï¼</strong> Human3Ræ¯ç¬¬ä¸ä¸ªå®ç°âä¸æ¬¡æ§âï¼all-at-onceï¼éå»ºçç»ä¸æ¨¡åï¼å®å¨ä¸ä¸ªåä¸çååä¼ æ­è¿ç¨ä¸­å±åæ¢å¤å¨å±å¤ä¸ªäººä½SMPL-Xæ¨¡åï¼âæ¯ä¸ªäººâï¼ãå¯é3Dåºæ¯ï¼âæ å¤ä¸å¨âï¼åç¸æºè½¨è¿¹ãè¿æ¶é¤äºå¯¹å¤é¶æ®µç®¡éãè¿­ä»£ç»ååéåº¦ä¾èµçéæ±ã
*   <strong>åºäºCUT3Rç4Då¨çº¿éå»ºï¼</strong> è¯¥æ¹æ³å»ºç«å¨4Då¨çº¿éå»ºåºç¡æ¨¡åCUT3Rä¹ä¸ï¼è¯¥æ¨¡åç¼ç äºä¸°å¯çæ¶ç©ºåéªãHuman3Réè¿åæ°é«æçè§è§æç¤ºå¾®è°ï¼Visual Prompt Tuning, VPTï¼æ¥æ©å±CUT3Rï¼ä»¥ä¿çå¶å¼ºå¤§çæ¶ç©ºåéªï¼åæ¶ç´æ¥è¯»åå¤ä¸ªäººä½SMPL-Xæ¨¡åã
*   <strong>äººç±»åéªçæ³¨å¥ï¼</strong> ä¸ºäºå¢å¼ºéå»ºäººä½å§¿æåå½¢ç¶çç»èï¼Human3Rå°æ¥èªMulti-HMR [3] ViT DINOç¼ç å¨ï¼å¨ä»¥äººä¸ºä¸­å¿çæ°æ®éä¸é¢è®­ç»ï¼çäººç±»ç¹å®ç¹å¾æ³¨å¥å°å¤´é¨tokenä¸­ï¼ä½ä¸ºäººç±»åéªãè¿æå©äºæé«å¯¹ç²¾ç»äººä½å§¿æåå½¢ç¶çé¢æµã
*   <strong>å¨çº¿äººä½åå²åè·è¸ªï¼</strong> Human3Rè¿æ¯æè®­ç»åçäººä½åå²åè·è¸ªï¼éè¿é¢æµæ¯ä¸ªå¾ååæ¯å¦åå«äººä½é¨ä½æ¥çæåç´ å¯¹é½çå¯éæ©ç ï¼å¹¶éè¿å¹éç²¾ç¼çäººä½tokenç¹å¾å®ç°è·è¸ªã
*   <strong>æµè¯æ¶åºåé¿åº¦èªéåºï¼TTT3Rï¼ï¼</strong> ä¸ºäºè§£å³RNNæ¨¡åå¨åºåé¿åº¦è¶åºè®­ç»ä¸ä¸ææ¶æ§è½ä¸éçé®é¢ï¼Human3Réç¨äºTTT3R [12]ï¼éè¿å¨æå­¦ä¹ çèªéåºå°å°å½åè§æµå¼ç¼ç å°è®°å¿ç¶æä¸­ï¼å¹³è¡¡åå²ä¸ä¸æçä¿çåæ°è§æµå¼çæ´åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæçï¼</strong> Human3Rå¨ä»ä½¿ç¨ä¸å48GB GPUè®­ç»ä¸å¤©åï¼å¨å®æ¶ï¼15 FPSï¼ä¸ä»¥ä½åå­å ç¨ï¼8 GBï¼éå»ºå¤ä¸ªäººä½å3Dåºæ¯ï¼å®ç°äºæ¾èçæçã
*   <strong>æåè¿ææç«äºåçæ§è½ï¼</strong> è®ºæéè¿å¹¿æ³å®éªè¯æï¼Human3Rå¨å¤é¡¹ä»»å¡ä¸­å®ç°äºæåè¿ææç«äºåçæ§è½ï¼åæ¬å¨å±äººä½è¿å¨ä¼°è®¡ãå±é¨äººä½ç½æ ¼æ¢å¤ãè§é¢æ·±åº¦ä¼°è®¡åç¸æºå§¿æä¼°è®¡ï¼ææè¿äºé½éè¿ä¸ä¸ªç»ä¸çæ¨¡åå®æã
*   <strong>é²æ£æ§æåï¼</strong> ç¸æ¯Multi-HMRï¼Human3Rå¨å¤çä¸åå¾åå®½é«æ¯æ¶è¡¨ç°åºæ´å¼ºçä¸è´æ§ï¼å¹¶ä¸æ éç¸æºååï¼è¿å¾çäºCUT3Ræä¾ç3Dåºæ¯æç¥è½åã
*   <strong>ç¸äºåçï¼</strong> å®éªè¡¨æï¼éè¿å¯¹äººä½éå»ºè¿è¡å¾®è°ï¼3Dåºæ¯éå»ºä¹å¾å°äºæ¹åï¼è¿çªæ¾äºå¯¹äººä½ååºæ¯è¿è¡èåæ¨ççç¸äºçå¤ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¤´é¨ä½ä¸ºå³é®ç¹ï¼</strong> è¯¥æ¹æ³ä¾èµå¤´é¨ä½ä¸ºæ£æµäººç±»çå¤å«æ§å³é®ç¹ï¼å½å¤´é¨ä¸å¯è§æ¶å¯è½å¯¼è´å¤±è´¥ã
*   <strong>ä»£çSMPLç½æ ¼ï¼</strong> ç®åä½¿ç¨ä»£çSMPLç½æ ¼è¡¨ç¤ºäººç±»ï¼æªè½å»ºæ¨¡è¡£ç©æå¤è§ç»èã
*   <strong>äº¤äºåç©çéå¶ï¼</strong> Human3Rè½ç¶éå¼å»ºæ¨¡äºäººç±»äº¤äºï¼ä½å°æªå®å¨è§£å³å®ä»¬ï¼å¹¶ä¸å¨éå»ºç²¾åº¦ä¸æªè½å¹éä¸äºå¼ºå¤§çç¦»çº¿æ¹æ³ï¼å¦JOSH [53]ï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´ååç´ å¯¹é½çèº«ä½ç¹å®ä½å¨ï¼</strong> å¼å¥åç´ å¯¹é½çèº«ä½ç¹å®ä½å¨ï¼å¦[40, 76]ï¼å¯ä»¥ç¼è§£å¤´é¨ä¸å¯è§æ¶çæ£æµå¤±è´¥é®é¢ã
*   <strong>æ©å±å°3DGSéå®çSMPLï¼</strong> å°æ¡æ¶æ©å±å°åºäº3DGSï¼Gaussian Splattingï¼éå®çSMPLï¼å¯ä»¥å®ç°æ´ä¸°å¯ãæ´å¨é¢çéå»ºï¼åæ¬è¡£ç©åå¤è§ã
*   <strong>ä½ä¸ºä¼åæ¹æ³çåå§åï¼</strong> Human3Rå¯ä»¥ä½ä¸ºä¼åæ¹æ³ï¼å¦[53]ï¼çææåå§åï¼ä»¥å¨éè¦æ´é«ç²¾åº¦æ¶æé«åç¡®æ§ï¼å°½ç®¡ä¼å¢å è®¡ç®ææ¬ã
*   <strong>æ©å±å°å¶ä»å¨æå®ä½ï¼</strong> è®ºææåºçåºå±åçå¯ä»¥æ©å±å°éå»ºå¨ç©ãè½¦è¾æå¶ä»å·æå®æ´6Då§¿æçç§»å¨ç©ä½ï¼ä»èæ¯æéçå¨ç©çæµãäº¤éåæãäºº-ç©äº¤äºåæºå¨äººç­åºç¨ã</p>
<hr />
<p>æ»èè¨ä¹ï¼Human3Réè¿å¼å¥ä¸ä¸ªç»ä¸çãååä¼ æ­çæ¡æ¶ï¼ç»ååæ°é«æçè§è§æç¤ºå¾®è°åäººç±»åéªæ³¨å¥ï¼æ¾èæ¨å¨äºå¨çº¿4Däººä½-åºæ¯éå»ºé¢åçåå±ãå®å¨æçåæ§è½ä¸é½è¡¨ç°åºè²ï¼ä¸ºæªæ¥çå®æ¶åºç¨åæ´å¹¿æ³çå¨æå®ä½éå»ºå¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos.</li>
<li>Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies.</li>
<li>Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06218v1'></a></p>
<h2 id="egonight-towards-egocentric-vision-understanding-at-night-with-a-challenging-benchmark"><a href="https://arxiv.org/abs/2510.06218v1">EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</a></h2>
<p><strong>Authors:</strong> Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, Yu-Gang Jiang, Luc Van Gool, Danda Pani Paudel</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Most existing benchmarks for egocentric vision understanding focus primarily
on daytime scenarios, overlooking the low-light conditions that are inevitable
in real-world applications. To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task. A key feature of EgoNight is the
introduction of day-night aligned videos, which enhance night annotation
quality using the daytime data and reveal clear performance gaps between
lighting conditions. To achieve this, we collect both synthetic videos rendered
by Blender and real-world recordings, ensuring that scenes and actions are
visually and temporally aligned. Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification. Each QA pair is double-checked
by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs
across 90 videos, spanning 12 diverse QA types, with more than 300 hours of
human work. Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.
Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night
correspondence retrieval and egocentric depth estimation at night, that further
explore the boundaries of existing models. We believe EgoNight-VQA provides a
strong foundation for advancing application-driven egocentric vision research
and for developing models that generalize across illumination domains. All the
data and code will be made available upon acceptance.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯å¯¹Deheng Zhangç­äººæ°åçè®ºæâEgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmarkâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçå¤§å¤æ°ä»¥èªæä¸ºä¸­å¿çè§è§çè§£åºåä¸»è¦å³æ³¨ç½å¤©åºæ¯ï¼å¿½ç¥äºç°å®ä¸çåºç¨ä¸­ä¸å¯é¿åçä½åç§æ¡ä»¶ãè¿ç¯è®ºææ¨å¨è§£å³è¿ä¸ç©ºç½ï¼å³å¨å¤é´ä½åç§æ¡ä»¶ä¸ï¼ä»¥èªæä¸ºä¸­å¿çè§è§ç³»ç»å¦ä½è¿è¡å¤æçåºæ¯çè§£åæ¨çã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
*   <strong>EgoNightåºåæ°æ®éï¼</strong> é¦æ¬¡æåºäºä¸ä¸ªå¨é¢è§£å³å¤é´æ¡ä»¶çä»¥èªæä¸ºä¸­å¿çåºåæ°æ®éï¼å¶æ ¸å¿æ¯è§è§é®ç­ï¼VQAï¼ãè¯¥æ°æ®éåå«åæè§é¢ï¼ç±Blenderæ¸²æï¼åçå®ä¸çå½åï¼ç¡®ä¿åºæ¯åå¨ä½å¨è§è§åæ¶é´ä¸å¯¹é½ï¼å¹¶æ¶µçäºç½å¤©åå¤é´ä¸¤ç§æ¡ä»¶ã
*   <strong>æ¥å¤å¯¹é½è§é¢ï¼</strong> å¼å¥äºæ¥å¤å¯¹é½è§é¢ï¼å©ç¨ç½å¤©æ°æ®æé«å¤é´æ æ³¨è´¨éï¼å¹¶æ­ç¤ºäºä¸ååç§æ¡ä»¶ä¸çæ¨¡åæ§è½å·®è·ã
*   <strong>EgoNight-VQAï¼</strong> æå»ºäºä¸ä¸ªåå«3658ä¸ªQAå¯¹çVQAä»»å¡ï¼æ¶µç12ç§ä¸åçQAç±»åï¼éè¿æ°é¢çâæ¥é´å¢å¼ºå¤é´èªå¨æ æ³¨å¼æâåå¹¿æ³çäººå·¥éªè¯è¿è¡æå»ºã
*   <strong>è¾å©ä»»å¡ï¼</strong> é¤äºVQAï¼EgoNightè¿å¼å¥äºæ¥å¤å¯¹åºæ£ç´¢åå¤é´ä»¥èªæä¸ºä¸­å¿çæ·±åº¦ä¼°è®¡ä¸¤ä¸ªè¾å©ä»»å¡ï¼ä»¥è¿ä¸æ­¥æ¢ç´¢ç°ææ¨¡åçè¾¹çã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   å¯¹æåè¿çå¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼çè¯ä¼°æ¾ç¤ºï¼ä»ç½å¤©å°å¤é´è½¬æ¢æ¶ï¼æ¨¡åæ§è½æ¾èä¸éï¼çªæ¾äºå¨ä½åç§æ¡ä»¶ä¸è¿è¡æ¨ççææã
*   è¿è¡¨æç°æMLLMså¨å¤é´ä»¥èªæä¸ºä¸­å¿çè§è§çè§£æ¹é¢å­å¨å±éæ§ï¼éè¦å¼åæ´å·é²æ£æ§ãè½å¤æ³åå°ä¸ååç§é¢åçæ¨¡åã
*   æ°æåºçQAç±»åï¼å¦åç§è¯å«/å¨æãåºæ¯åºåæ¨çãå¯¼èªåéå¸¸è§æ¨çï¼æ¯ä¼ ç»ç±»å«æ´å·æææ§ï¼æ­ç¤ºäºMLLMsé¢ä¸´çæ°é¾é¢ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®éè§æ¨¡ï¼</strong> EgoNightæ°æ®éçè§æ¨¡ä¸å¤§åè§è§è¯­è¨è¯­æåºç¸æ¯ä»ç¶éä¸­ãå°½ç®¡ä½èè®¤ä¸ºç°æè§æ¨¡å·²è¶³ä»¥è¿è¡åºåæµè¯ï¼ä½æªæ¥è®¡åéè¿åææ´å¤æ°æ®åå½å¶æ´å¤çå®ä¸çè§é¢æ¥è¿ä¸æ­¥æ©å¤§è§æ¨¡ï¼ä»¥æ¯æé¢è®­ç»åå¾®è°ã
*   <strong>ç¯å¢æ¡ä»¶ï¼</strong> EgoNightä¸»è¦å³æ³¨æ¥å¤åç§ååï¼èå¶ä»ç°å®ä¸çææï¼å¦å¤©æ°ååãæç«¯ç¸æºè¿å¨ï¼å°æªæ¶µçã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   è¿ä¸æ­¥æ©å¤§å¤é´è§é¢æ°æ®éçè§æ¨¡ï¼ä»¥æ¯æMLLMçé¢è®­ç»åå¾®è°ï¼ä»èæé«å¶æ§è½ã
*   å°ä»¥èªæä¸ºä¸­å¿çè§è§çè§£ç ç©¶æ©å±å°å¶ä»ç°å®ä¸çææï¼å¦é¨ãé¾ç­å¤©æ°æ¡ä»¶ä»¥åæç«¯ç¸æºè¿å¨ã
*   å¼åæ´å·åç§é²æ£æ§çæ¨¡åï¼ä»¥å¼¥åç½å¤©åå¤é´æ¡ä»¶ä¸çæ§è½å·®è·ã</p>
<p>æ»èè¨ä¹ï¼EgoNightåºåä¸ºæ¨å¨ä»¥èªæä¸ºä¸­å¿çå¤é´è§è§çè§£ç ç©¶æä¾äºä¸ä¸ªå¼ºå¤§èåæ¶çåºç¡ï¼æ¨å¨ä¿è¿å¼åè½å¤æ³åå°åç§åç§æ¡ä»¶ä¸çå¯é AIå©æã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task.</li>
<li>Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification.</li>
<li>Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06218v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06218v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06194v1'></a></p>
<h2 id="overlap-aware-segmentation-for-topological-reconstruction-of-obscured-objects"><a href="https://arxiv.org/abs/2510.06194v1">Overlap-aware segmentation for topological reconstruction of obscured objects</a></h2>
<p><strong>Authors:</strong> J. Schueler, H. M. AraÃºjo, S. N. Balashov, J. E. Borg, C. Brew, F. M. Brunbauer, C. Cazzaniga, A. Cottle, D. Edgeman, C. D. Frost, F. Garcia, D. Hunt, M. Kastriotou, P. Knights, H. Kraus, A. Lindote, M. Lisowska, D. Loomba, E. Lopez Asamar, P. A. Majewski, T. Marley, C. McCabe, L. Millins, R. Nandakumar, T. Neep, F. Neves, K. Nikolopoulos, E. Oliveri, A. Roy, T. J. Sumner, E. Tilly, W. Thompson, M. A. Vogiatzi</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> hep-ex, astro-ph.IM, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The separation of overlapping objects presents a significant challenge in
scientific imaging. While deep learning segmentation-regression algorithms can
predict pixel-wise intensities, they typically treat all regions equally rather
than prioritizing overlap regions where attribution is most ambiguous. Recent
advances in instance segmentation show that weighting regions of pixel overlap
in training can improve segmentation boundary predictions in regions of
overlap, but this idea has not yet been extended to segmentation regression. We
address this with Overlap-Aware Segmentation of ImageS (OASIS): a new
segmentation-regression framework with a weighted loss function designed to
prioritize regions of object-overlap during training, enabling extraction of
pixel intensities and topological features from heavily obscured objects. We
demonstrate OASIS in the context of the MIGDAL experiment, which aims to
directly image the Migdal effect--a rare process where electron emission is
induced by nuclear scattering--in a low-pressure optical time projection
chamber. This setting poses an extreme test case, as the target for
reconstruction is a faint electron recoil track which is often heavily-buried
within the orders-of-magnitude brighter nuclear recoil track. Compared to
unweighted training, OASIS improves median intensity reconstruction errors from
-32% to -14% for low-energy electron tracks (4-5 keV) and improves topological
intersection-over-union scores from 0.828 to 0.855. These performance gains
demonstrate OASIS's ability to recover obscured signals in overlap-dominated
regions. The framework provides a generalizable methodology for scientific
imaging where pixels represent physical quantities and overlap obscures
features of interest. All code is openly available to facilitate cross-domain
adoption.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾J. Schuelerç­äººæ°åçè®ºæâOverlap-aware segmentation for topological reconstruction of obscured objectsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Overlap-aware segmentation for topological reconstruction of obscured objects</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºæä¸»è¦è§£å³ç§å­¦æåä¸­éå ç©ä½åç¦»çéå¤§ææãå·ä½èè¨ï¼å®å³æ³¨çæ¯æ·±åº¦å­¦ä¹ åå²-åå½ç®æ³å¨å¤çåç´ çº§å¼ºåº¦é¢æµæ¶ï¼éå¸¸å¯¹ææåºåä¸è§åä»ï¼èæªè½ä¼åå¤çå½å ææ¨¡ç³çéå åºåãè¿å¯¼è´å¨éå ä¸¥éãå¼ºåº¦å·®å¼å·¨å¤§çæåµä¸ï¼é¾ä»¥åç¡®æåè¢«é®æ¡ç©ä½çåç´ å¼ºåº¦åææç¹å¾ãè®ºæä»¥MIGDALå®éªä¸ºä¾ï¼è¯¥å®éªæ¨å¨ç´æ¥æåMigdalæåºï¼ä¸ç§ç±æ ¸æ£å°å¼èµ·ççµå­åå°è¿ç¨ï¼ï¼å¶ä¸­éå»ºç®æ æ¯å¾®å¼±ççµå­åå²å¾è¿¹ï¼è¿äºå¾è¿¹éå¸¸è¢«äº®åº¦é«åºå ä¸ªæ°éçº§çæ ¸åå²å¾è¿¹ä¸¥éæ©çã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæå¼å¥äº<strong>Overlap-Aware Segmentation of ImageS (OASIS)</strong> æ¡æ¶ï¼è¿æ¯ä¸ä¸ªæ°çåå²-åå½æ¡æ¶ï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>å ææå¤±å½æ°ï¼</strong> OASISè®¾è®¡äºä¸ä¸ªå®å¶çå ææå¤±å½æ°ï¼è¯¥å½æ°å¨è®­ç»è¿ç¨ä¸­ä¼åå¤çç©ä½éå åºåãéè¿ä¸ºä¸åééï¼ä¾å¦ï¼å¾®å¼±ççµå­åå²å¾è¿¹ï¼åéå åºååéç¹å®çæéï¼æ¨¡åè½å¤å°è®­ç»æ³¨æåéä¸­å¨å½å æå³é®çåºåã
*   <strong>æ©å±éå æç¥è®­ç»èå¼ï¼</strong> ç°æå®ä¾åå²æ¹æ³ï¼å¦MultiStaråBCNetï¼å·²è¯æéå åºåå æå¯ä»¥æ¹ååå²è¾¹çé¢æµï¼ä½OASISé¦æ¬¡å°è¿ä¸ææ³æ©å±å°åå²åå½ä»»å¡ï¼ä½¿å¶è½å¤é¢æµç©çéï¼åç´ å¼ºåº¦ï¼èéä»ä»æ¯ç±»å«ææ©ç ã
*   <strong>éç¨æ§ï¼</strong> è¯¥æ¡æ¶æä¾äºä¸ç§å¯æ¨å¹¿çæ¹æ³ï¼éç¨äºåç´ ä»£è¡¨ç©çéä¸éå é®æ¡æå´è¶£ç¹å¾çç§å­¦æåé¢åãå®å¨è®¡ç®ä¸é«æï¼ä¸éè¦å¯¹æ ååå²-åå½ç½ç»çæ¶æè¿è¡ä¿®æ¹ï¼å¹¶ä¸ä¸ç¹å®çéª¨å¹²ç½ç»æ å³ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
OASISå¨MIGDALå®éªèæ¯ä¸çæ§è½è¯ä¼°æ¾ç¤ºåºæ¾èæ¹è¿ï¼
*   <strong>å¼ºåº¦éå»ºè¯¯å·®éä½ï¼</strong> å¯¹äºä½è½éçµå­å¾è¿¹ï¼4-5 keVï¼ï¼OASISå°ä¸­ä½å¼ºåº¦éå»ºè¯¯å·®ä»-32%æé«å°-14%ãè¿è¡¨æå¨éå åºåä¸­ï¼æ¨¡åè½å¤æ´åç¡®å°å½å å¾®å¼±ä¿¡å·çå¼ºåº¦ã
*   <strong>ææäº¤å¹¶æ¯ï¼IoUï¼åæ°æé«ï¼</strong> ææäº¤å¹¶æ¯åæ°ä»0.828æé«å°0.855ãè¿è¡¨æOASISå¨éå»ºè¢«é®æ¡ä¿¡å·çææç»ææ¹é¢ä¹è¡¨ç°åºæ´å¥½çæ§è½ã
*   <strong>ä½è½éåºåçæ¹è¿ï¼</strong> å¨Migdalæåºæªé¢éè½ééä½åææ°å¢é¿çä½è½éåºåï¼4-6 keVï¼ï¼OASISçæ¹è¿å°¤ä¸ºå³é®ï¼å ä¸ºè¿äºåºåçåç¡®éå»ºå¯¹äºéªè¯çè®ºé¢æµè³å³éè¦ã
*   <strong>è§åº¦ä¸è´æ§ï¼</strong> è®ºæè¿å±ç¤ºäºOASISå¨ä½è½éåºååï¼éè¿ä¸»æ²çº¿æåå¾å°çè§åº¦ä¸è´æ§å¨20Â°ä»¥åï¼è¿å¯¹äºæµéMigdalæåºçè§åå¸å·æéè¦æä¹ã
*   <strong>åé³æ§çï¼</strong> å°½ç®¡å æè®­ç»æ¾èæé«äºä½è½éERéå»ºæ§è½ï¼ä½ä»£ä»·æ¯åé³æ§çç¥æå¢å ï¼ä»0.5%å°1.5%ï¼ãç¶èï¼å¯¹äºMIGDALå®éªèè¨ï¼ç±äºOASISå¹¶éç¨äºæç´¢Migdalæåºåéèï¼èæ¯ç¨äºæåå·²æ£æµå°çERçåç¡®è¡¨ç¤ºï¼å æ­¤è¿ä¸æ¯ä¸ä¸ªä¸»è¦é®é¢ã</p>
<p>è¿äºæ§è½æåè¯æäºOASISå¨éå ä¸»å¯¼åºåæ¢å¤è¢«é®æ¡ä¿¡å·çè½åï¼ä¸ºç§å­¦æåä¸­åç´ ä»£è¡¨ç©çéä¸éå é®æ¡æå´è¶£ç¹å¾çé®é¢æä¾äºä¸ç§éç¨æ¹æ³ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åé³æ§çï¼</strong> å°½ç®¡å æè®­ç»æ¾èæé«äºä½è½éERéå»ºæ§è½ï¼ä½ä¸æªå æè®­ç»ç¸æ¯ï¼å®å¯¼è´äºæ´é«çåé³æ§çï¼1.5% vs 0.5%ï¼ï¼å³æ´å®¹æéå»ºä¸å­å¨çä¿¡å·ãè®ºææåºï¼å¯¹äºMIGDALå®éªï¼è¿å¹¶éä¸»è¦å³æ³¨ç¹ï¼å ä¸ºOASISå¹¶éç¨äºæç´¢Migdalæåºåéèï¼èæ¯ç¨äºæåå·²æ£æµå°çERçåç¡®è¡¨ç¤ºã
*   <strong>ERé¡¶ç¹éå»ºï¼</strong> è®ºææå°ï¼ERé¡¶ç¹é¾ä»¥ç²¾ç¡®ç¡®å®ãè½ç¶ä»ä»¬å©ç¨NRé¡¶ç¹ä½ç½®æ¥æå»ºä¸»è½´ï¼ä½ERé¡¶ç¹æ¬èº«çç²¾ç¡®éå»ºä»æ¯ä¸ä¸ªææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è·¨é¢ååºç¨ï¼</strong> OASISçéç¨æ§ä½¿å¶å¯ä»¥åºç¨äºå¶ä»ç§å­¦æåé¢åï¼ä¾å¦å¤©æå­¦ä¸­çæç³»å»æ··åï¼deblendingï¼åå»å­¦æåä¸­çä½ç§¯æ°æ®åæãè®ºæç¹å«æå°äºGalaxy Zoo DECALSæ°æ®åºï¼è¿æ¯ä¸ä¸ªå¬å¼å¯ç¨çæ°æ®éï¼éå¸¸éååºç¨OASISã
*   <strong>å¤æ³¢æ®µè¾å¥ï¼</strong> æ¡æ¶å¯ä»¥æ©å±ä»¥æ¯æå¤ééè¾å¥ï¼ä¾å¦å¤©æå­¦ä¸­çå¤æ³¢æ®µå¾åã
*   <strong>ä¸ç»´éå»ºï¼</strong> æ¯æå®æ´ç3Dä½ç´ ï¼3ä¸ªç©ºé´ç»´åº¦+å¼ºåº¦ï¼å°ä½¿å¶è½å¤åºç¨äºä½ç§¯å»å­¦æååå·æå®æ´3Déå»ºè½åçåçº§ç²å­æ¢æµå¨ã
*   <strong>Migdalæåºè§åå¸æµéï¼</strong> è®ºææåºï¼OASISå¨ä½è½éåºåçè§æææ§ææç¨äºå½åä¸ä»£MIGDALæ¢æµå¨æµè¯Migdalæåºè§äº§ççè®ºæ¨¡åçè½åï¼è¿å°å¨æªæ¥çå·¥ä½ä¸­è¿è¡æ¥åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We
address this with Overlap-Aware Segmentation of ImageS (OASIS): a new
segmentation-regression framework with a weighted loss function designed to
prioritize regions of object-overlap during training, enabling extraction of
pixel intensities and topological features from heavily obscured objects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06194v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06194v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06040v1'></a></p>
<h2 id="videominer-iteratively-grounding-key-frames-of-hour-long-videos-via-tree-based-group-relative-policy-optimization"><a href="https://arxiv.org/abs/2510.06040v1">VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</a></h2>
<p><strong>Authors:</strong> Xinye Cao, Hongcan Guo, Jiawen Qian, Guoshun Nan, Chao Wang, Yuqi Pan, Tianhao Hou, Xiaojuan Wang, Yutong Gao</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Understanding hour-long videos with multi-modal large language models
(MM-LLMs) enriches the landscape of human-centered AI applications. However,
for end-to-end video understanding with LLMs, uniformly sampling video frames
results in LLMs being overwhelmed by a vast amount of irrelevant information as
video length increases. Existing hierarchical key frame extraction methods
improve the accuracy of video understanding but still face two critical
challenges. 1) How can the interference of extensive redundant information in
long videos be mitigated? 2) How can a model dynamically adapt to complex
hierarchical structures while accurately identifying key frames? To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure. The proposed
VideoMiner progresses from long videos to events to frames while preserving
temporal coherence, effectively addressing the first challenge. To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner. The proposed T-GRPO is specifically designed for tree
structures, integrating spatiotemporal information at the event level while
being guided by the question, thus solving the second challenge. We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights. Our proposed T-GRPO surprisingly incentivizes the model
to spontaneously generate a reasoning chain. Additionally, the designed tree
growth auxin dynamically adjusts the expansion depth, obtaining accuracy and
efficiency gains. The code is publicly available at
https://github.com/caoxinye/VideoMiner.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xinye Caoç­äººæ°åçè®ºæâVideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimizationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼VideoMinerï¼éè¿åºäºæ çç»ç¸å¯¹ç­ç¥ä¼åè¿­ä»£å°å®ä½é¿è§é¢ä¸­çå³é®å¸§</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³é¿è§é¢çè§£ä¸­å­å¨çä¸¤ä¸ªå³é®ææï¼
1. <strong>å¦ä½åè½»é¿è§é¢ä¸­å¤§éåä½ä¿¡æ¯çå¹²æ°ï¼</strong> éçè§é¢é¿åº¦çå¢å ï¼å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MM-LLMsï¼å¨ç«¯å°ç«¯è§é¢çè§£ä¸­ï¼ç±äºååéæ ·å¸§å¯¼è´æ¨¡åè¢«å¤§éä¸ç¸å³ä¿¡æ¯æ·¹æ²¡ã
2. <strong>æ¨¡åå¦ä½å¨æéåºå¤æçå±æ¬¡ç»æï¼åæ¶åç¡®è¯å«å³é®å¸§ï¼</strong> ç°æçå±æ¬¡åå³é®å¸§æåæ¹æ³è½ç¶æé«äºè§é¢çè§£çåç¡®æ§ï¼ä½å¨å¤çå¤æå±æ¬¡ç»æåç²¾ç¡®å³é®å¸§å®ä½æ¹é¢ä»é¢ä¸´ææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼è®ºææåºäº<strong>VideoMiner</strong>ï¼ä¸ä¸ªæ°é¢çåºäºå¼ºåå­¦ä¹ çè§é¢çè§£æ¡æ¶ï¼å¶æ ¸å¿åæ°åæ¬ï¼
*   <strong>å±æ¬¡åæ ç»ææå»ºï¼</strong> VideoMineréè¿è¿­ä»£å°å¯¹é¿è§é¢è¿è¡åå²ãçæå­å¹åèç±»ï¼æå»ºäºä¸ä¸ªå±æ¬¡åçæ ç»æãå®ä»é¿è§é¢å±é¢éæ­¥æ·±å¥å°äºä»¶å±é¢ï¼åå°å¸§å±é¢ï¼åæ¶ä¿ææ¶é´è¿è´¯æ§ï¼ææç¼è§£äºåä½ä¿¡æ¯å¹²æ°çé®é¢ã
*   <strong>T-GRPOï¼Tree-based Group Relative Policy Optimizationï¼ï¼</strong> ä¸ºäºç²¾ç¡®å°å®ä½å³é®å¸§ï¼è®ºæå¼å¥äºT-GRPOï¼è¿æ¯ä¸ç§åºäºæ ç»æçç»ç¸å¯¹ç­ç¥ä¼åå¼ºåå­¦ä¹ æ¹æ³ï¼ç¨äºæå¯¼VideoMinerçæ¢ç´¢è¿ç¨ãT-GRPOä¸é¨ä¸ºæ ç»æè®¾è®¡ï¼å®å¨äºä»¶å±é¢æ´åäºæ¶ç©ºä¿¡æ¯ï¼å¹¶ç±é®é¢å¼å¯¼ï¼ä»èè§£å³äºå¨æéåºå¤æå±æ¬¡ç»æååç¡®è¯å«å³é®å¸§çé®é¢ã
*   <strong>å¥å±å½æ°è®¾è®¡ï¼</strong> T-GRPOçå¥å±å½æ°è¢«åè§£ä¸ºèç¹çº§å¥å±ï¼è¯ä¼°åä¸ªèç¹å³ç­è´¨éï¼åæ çº§å¥å±ï¼åæ æç»æ çº§ç»æçæ­£ç¡®æ§ï¼ï¼ä»¥æå¯¼ç­ç¥æ¨¡åååºæ´ç»æåãè¯¦ç»ååç¡®çå³é®å¸§å³ç­ã
*   <strong>æ çé¿ç´ ï¼Tree Growth Auxinï¼æºå¶ï¼</strong> å¼å¥äºç±»ä¼¼æ¤ç©çé¿ç´ çæºå¶<code>Aauxin</code>ï¼å¨æè°æ´æ çæ©å±æ·±åº¦ï¼ä»¥å¹³è¡¡åç¡®æ§åæçï¼å¹¶è°èç­ç¥æ¨¡åçæ¢ç´¢å¾åã
*   <strong>æ¨çé¾çèªåçæï¼</strong> T-GRPOåºäººææå°æ¿å±æ¨¡åèªåçææ¨çé¾ï¼æ¾èæåäºæ¨¡åçæ¨çè½åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> VideoMinerå¨ææé¿è§é¢çè§£ä»»å¡ä¸­ååå¾äºåè¶çæ§è½ï¼å¹¶å¨ç­è§é¢åºåæµè¯ä¸­ä¹è¡¨ç°åºè²ï¼æ¾èä¼äºå¤ç§åºçº¿æ¹æ³ã
*   <strong>é¿è§é¢çè§£çä¼å¿ï¼</strong> éçè§é¢é¿åº¦çå¢å ï¼VideoMinerä¸åºçº¿æ¹æ³ä¹é´çæ§è½å·®è·éæ¸æ©å¤§ï¼è¡¨æå¶å¨é¿è§é¢çè§£ä»»å¡ä¸­çä¼è¶æ§ãè¿å¾çäºå¶åºæ¯åå²åèç±»æ¹æ³æå¤§éåº¦å°ä¿çäºæ¶é´ä¿¡æ¯ï¼ä»¥åå¼ºåå­¦ä¹ è®­ç»çç­ç¥æ¨¡åå·å¤èªå®åå³ç­è½åã
*   <strong>äºä»¶èç±»çæææ§ï¼</strong> äºä»¶èç±»ç¸æ¯å¸§èç±»è½ä¿çæ´ä¸°å¯çæ¶é´ä¿¡æ¯ï¼å¹¶ä¿è¿æ ç»æçæææå»ºï¼å¨å¤§å¤æ°åºåæµè¯ä¸­å®ç°äºæç­çè¿è¡æ¶é´åæé«çåç¡®æ§ã
*   <strong>T-GRPOçæææ§ï¼</strong> T-GRPOå¼å¥çæ çº§å¥å±è®¾è®¡æ¾èå¢å¼ºäºç­ç¥æ¨¡åçæ¨çè½åï¼ä½¿å¶è½å¤èèå½åå³ç­å¯¹æªæ¥ç»æçå½±åï¼ä»èæé«äºåç¡®æ§ã
*   <strong>è¡¥å¨é¿åº¦åçé¿éççå½±åï¼</strong> å®éªè¡¨æï¼æ´é¿çè¡¥å¨é¿åº¦ï¼å³æ©å±çè¡¥å¨è¿ç¨ï¼ä¼å¸¦æ¥æ´é«çåç¡®æ§ï¼å ä¸ºå¼ºåå­¦ä¹ è¿ç¨ä¼èªç¶å°è¯±å¯¼æç»´é¾è¡ä¸ºãæ­¤å¤ï¼æ çé¿éç<code>Aauxin</code>å¨æè°èæ©å±æ·±åº¦ï¼å¹³è¡¡äºåç¡®æ§åæçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç­è§é¢ä»»å¡çæ§è½å·®è·ï¼</strong> å°½ç®¡VideoMinerå¨é¿è§é¢ä»»å¡ä¸­è¡¨ç°ä¼å¼ï¼ä½å¨ç­è§é¢ä»»å¡ä¸­ï¼ä¸ç«¯å°ç«¯æ¹æ³ç¸æ¯ä»å­å¨ä¸å®çæ§è½å·®è·ãè¿ä¸»è¦æ¯å ä¸ºVideoMineråç«¯å°ç«¯æ¹æ³ä½¿ç¨äºä¸åçåºç¡æ¨¡åï¼ä¸å¶æ¨¡åæ¯ä¸é¨ä¸ºè§é¢ä»»å¡è®­ç»åå¢å¼ºçãVideoMinerä¸»è¦ä¸ºå³é®å¸§éæ©è³å³éè¦çé¿è§é¢çè§£èè®¾è®¡ï¼èç­è§é¢åä¸ä¸å®éè¦ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææªæç¡®æåºæªæ¥ç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åå±éæ§å¯ä»¥æ¨æ­ï¼
*   <strong>ä¼åç­è§é¢çè§£ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½è°æ´VideoMinerçæ¶ææè®­ç»ç­ç¥ï¼ä»¥ç¼©å°å¶å¨ç­è§é¢ä»»å¡ä¸­ä¸ç«¯å°ç«¯æ¹æ³çæ§è½å·®è·ã
*   <strong>æ´å¹¿æ³çåºç¨åºæ¯ï¼</strong> æ¢ç´¢VideoMinerå¨å¶ä»éè¦ç²¾ç¡®å³é®å¸§å®ä½åå±æ¬¡åçè§£çé¿è§é¢åºç¨ä¸­çæ½åï¼ä¾å¦è§é¢ç¼è¾ãåå®¹æ£ç´¢ææè²é¢åã
*   <strong>å¤æ¨¡æä¿¡æ¯èåçæ·±åº¦æ¢ç´¢ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æ´ææå°æ´ååå©ç¨è§é¢ä¸­çåç§æ¨¡æä¿¡æ¯ï¼å¦é³é¢ãææ¬ãè§è§ï¼ï¼ä»¥æåçè§£è½åã
*   <strong>å¯è§£éæ§ä¸éæåº¦ï¼</strong> é´äºT-GRPOè½å¤èªåçææ¨çé¾ï¼æªæ¥å¯ä»¥æ·±å¥ç ç©¶å¦ä½å©ç¨è¿äºæ¨çé¾æ¥æé«æ¨¡åçéæåº¦åå¯è§£éæ§ï¼å¸®å©ç¨æ·æ´å¥½å°çè§£æ¨¡åçå³ç­è¿ç¨ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure.</li>
<li>To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner.</li>
<li>We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06040v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06040v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.06009v1'></a></p>
<h2 id="continual-learning-for-image-captioning-through-improved-image-text-alignment"><a href="https://arxiv.org/abs/2510.06009v1">Continual Learning for Image Captioning through Improved Image-Text Alignment</a></h2>
<p><strong>Authors:</strong> Bertram Taetz, Gal Bordelius</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating accurate and coherent image captions in a continual learning
setting remains a major challenge due to catastrophic forgetting and the
difficulty of aligning evolving visual concepts with language over time. In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks. Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation. We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods. The code can
be found via the following link https://github.com/
Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Bertram TaetzåGal Bordeliusæ°åçè®ºæâContinual Learning for Image Captioning through Improved Image-Text Alignmentâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼æç»­å­¦ä¹ å¾åå­å¹çæä¸­çæ¹è¿å¾å-ææ¬å¯¹é½</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æç»­å­¦ä¹ ï¼Continual Learningï¼ç¯å¢ä¸å¾åå­å¹çææé¢ä¸´çæ ¸å¿ææãå·ä½æ¥è¯´ï¼å®å³æ³¨ä¸¤ä¸ªä¸»è¦é®é¢ï¼
* <strong>ç¾é¾æ§éå¿ï¼Catastrophic Forgettingï¼ï¼</strong> å¨æ¨¡åå­¦ä¹ æ°ä»»å¡æ¶ï¼å¦ä½é²æ­¢å¶éå¿ååå­¦ä¹ å°çç¥è¯ã
* <strong>è§è§æ¦å¿µä¸è¯­è¨çå¯¹é½ï¼</strong> å¦ä½ææå°å°ä¸æ­æ¼åçè§è§æ¦å¿µä¸è¯­è¨æè¿°è¿è¡å¯¹é½ï¼ä»¥çæåç¡®ä¸è¿è´¯çå¾åå­å¹ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä½èæåºäºä¸ç§æ°é¢ç<strong>å¤æå¤±æ¡æ¶ï¼multi-loss frameworkï¼</strong>ï¼ç¨äºæç»­å­¦ä¹ å¾åå­å¹çæï¼å¶æ ¸å¿å¨äºéè¿åºäºæç¤ºçæç»­å­¦ä¹ ï¼prompt-based continual learningï¼åå¯¹æ¯å¯¹é½ï¼contrastive alignmentï¼æ¥æ´åè¯­ä¹æå¯¼ãè¯¥æ¹æ³åºäºé¢è®­ç»çViT-GPT-2éª¨å¹²ç½ç»ï¼å¹¶ç»åäºä»¥ä¸ä¸ä¸ªé¢å¤çæå¤±ç»ä»¶ï¼</p>
<ul>
<li><strong>åºäºæç¤ºçä½å¼¦ç¸ä¼¼åº¦æå¤±ï¼Prompt-based Cosine Similarity Loss - <script type="math/tex">L_{nouns}</script>ï¼ï¼</strong> æ¨å¨å°å¾ååµå¥ï¼image embeddingsï¼ä¸äººå·¥æå»ºçãç¼ç äºå¯¹è±¡ãå±æ§åå¨ä½çæç¤ºåµå¥ï¼synthetically constructed promptsï¼è¿è¡å¯¹é½ãè¿æå©äºæ¨¡åæ´å¥½å°çè§£åºæ¯ä¸­çå³é®è§è§åç´ ã</li>
<li><strong>CLIPé£æ ¼çæå¤±ï¼CLIP-style Loss - <script type="math/tex">L_{CLIP}</script>ï¼ï¼</strong> ä¿è¿å¾ååµå¥ä¸ç®æ å­å¹åµå¥ï¼target caption embeddingï¼ä¹é´çå¯¹é½ï¼ä»èå¢å¼ºè§è§åææ¬æ¨¡æå¨å±äº«è¯­ä¹ç©ºé´ä¸­çä¸è´æ§ã</li>
<li><strong>è¯­è¨å¼å¯¼çå¯¹æ¯æå¤±ï¼Language-Guided Contrastive Loss - <script type="math/tex">L_{LGCL}</script>ï¼ï¼</strong> éç¨ä¸åç»æå¤±ï¼triplet lossï¼æ¥å¢å¼ºä»»å¡ä¹é´ç±»çº§å«çå¯åºåæ§ãå®é¼å±å¾ååµå¥ä¸å¶æ­£ç¡®çè¯­è¨å¯¹åºç©ä¿ææ¥è¿ï¼åæ¶ä¸ä¸å¹éçè´æ ·æ¬ä¿æä¸å®è·ç¦»ã</li>
</ul>
<p>å¼å¾æ³¨æçæ¯ï¼è¯¥æ¹æ³å¨æ¨çæ¶<strong>ä¸å¼å¥é¢å¤çå¼é</strong>ï¼å¹¶ä¸å¨å­å¹çæè¿ç¨ä¸­<strong>ä¸éè¦æç¤º</strong>ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªç»æè¡¨æï¼è¯¥æ¹æ³å¨ç¼è§£ç¾é¾æ§éå¿æ¹é¢è¡¨ç°åºè²ï¼å¹¶ä¸ä¸ç°ææåè¿çæ¹æ³ç¸æ¯ï¼å®ç°äºæ´å¥½çè¯­ä¹å­å¹å¯¹é½ã</p>
<ul>
<li><strong>ç¼è§£ç¾é¾æ§éå¿ï¼</strong> å¨ContCapåRATTæ°æ®éä¸çå®éªæ¾ç¤ºï¼CLICITAæ¹æ³å¨ç¥è¯ä¿çæ¹é¢ä¼äºé¢è®­ç»åºçº¿æ¨¡åï¼å¹³åéå¿çæ´ä½ã</li>
<li><strong>æ¹è¿è¯­ä¹å¯¹é½ï¼</strong> ä¸ç°ææ¹æ³ç¸æ¯ï¼CLICITAå¨METEORç­è¯­ä¹ææ ä¸è¡¨ç°åºæ¾èæåï¼è¡¨æå¶çæçå­å¹å¨è¯­ä¹ä¸æ´åç¡®ãæ´è¿è´¯ã</li>
<li><strong>æ¨çæçï¼</strong> è¯¥æ¹æ³å¨æ¨çæ¶æ éé¢å¤å¼éåæç¤ºï¼ä½¿å¶éç¨äºèµæºåéçç¯å¢ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåå½åæ¹æ³çå·ä½å±éæ§ï¼ä½ä½ä¸ºæç»­å­¦ä¹ é¢åçç ç©¶ï¼éå¸¸ä¼é¢ä¸´ä»¥ä¸æ½å¨ææï¼å°½ç®¡è®ºææªç´æ¥æåºï¼ï¼
*   <strong>æ¨¡åå¤ææ§ï¼</strong> å¼å¥å¤ä¸ªæå¤±å½æ°å¯è½ä¼å¢å è®­ç»çå¤ææ§ã
*   <strong>è¶åæ°è°ä¼ï¼</strong> å¤ä¸ªæå¤±ç»ä»¶çæéå¹³è¡¡å¯è½éè¦ç²¾ç»çè¶åæ°è°ä¼ã
*   <strong>æ³åè½åï¼</strong> å°½ç®¡å¨ç¹å®æ°æ®éä¸è¡¨ç°è¯å¥½ï¼ä½å¨æ´å¹¿æ³ãæ´å¤æ ·åçæç»­å­¦ä¹ åºæ¯ä¸­çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä½èæåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>æ´å¼ºå¤§åé²æ£çæ¨¡åï¼</strong> æ¢ç´¢å°è¯¥æ¹æ³åºç¨äºæ´å¼ºå¤§åé²æ£çå¾åå­å¹çææ¨¡åã
*   <strong>æ´å¤§çæç»­å­¦ä¹ å¾åå­å¹æ°æ®éï¼</strong> å¨æ´å¤§çæç»­å­¦ä¹ å¾åå­å¹æ°æ®éä¸è¿è¡å®éªï¼ä»¥è¿ä¸æ­¥éªè¯åæåæ¹æ³çæ§è½ã</p>
<hr />
<p>è¿ä»½æè¦æ¨å¨æ¸æ°ãç®æ´å°ä¼ è¾¾è®ºæçæ ¸å¿åå®¹ï¼çªåºå¶å¨æç»­å­¦ä¹ å¾åå­å¹çæé¢åçè´¡ç®ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment.</li>
<li>Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks.</li>
<li>Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation.</li>
<li>We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.06009v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.06009v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05949v1'></a></p>
<h2 id="gaussian-embeddings-how-jepas-secretly-learn-your-data-density"><a href="https://arxiv.org/abs/2510.05949v1">Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</a></h2>
<p><strong>Authors:</strong> Randall Balestriero, Nicolas Ballas, Mike Rabbat, Yann LeCun</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, stat.ML</p>
<p><strong>Abstract:</strong></p>
<p>Joint Embedding Predictive Architectures (JEPAs) learn representations able
to solve numerous downstream tasks out-of-the-box. JEPAs combine two
objectives: (i) a latent-space prediction term, i.e., the representation of a
slightly perturbed sample must be predictable from the original sample's
representation, and (ii) an anti-collapse term, i.e., not all samples should
have the same representation. While (ii) is often considered as an obvious
remedy to representation collapse, we uncover that JEPAs' anti-collapse term
does much more--it provably estimates the data density. In short, any
successfully trained JEPA can be used to get sample probabilities, e.g., for
data curation, outlier detection, or simply for density estimation. Our
theoretical finding is agnostic of the dataset and architecture used--in any
case one can compute the learned probabilities of sample <script type="math/tex">x</script> efficiently and in
closed-form using the model's Jacobian matrix at <script type="math/tex">x</script>. Our findings are
empirically validated across datasets (synthetic, controlled, and Imagenet) and
across different Self Supervised Learning methods falling under the JEPA family
(I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the
method extracting the JEPA learned density as {\bf JEPA-SCORE}.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<p><strong>è®ºææè¦åæï¼Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºæ­ç¤ºäºèååµå¥é¢æµæ¶æï¼JEPAsï¼ä¸­çâååç¼©é¡¹âä¸ä»è½é²æ­¢è¡¨ç¤ºåç¼©ï¼è¿è½<strong>å¯è¯æå°ä¼°è®¡æ°æ®å¯åº¦</strong>ãè¿æå³çä»»ä½æåè®­ç»çJEPAæ¨¡åï¼å¶åé¨æºå¶å®éä¸å·²ç»å­¦ä¹ å°äºæ°æ®åå¸çæ¦çä¿¡æ¯ãä½èæåºäºä¸ç§åä¸ºJEPA-SCOREçæ¹æ³ï¼è½å¤é«æä¸ä»¥é­åå½¢å¼ï¼å©ç¨æ¨¡åå¨æ ·æ¬å¤çéå¯æ¯ç©éµï¼ä»JEPAä¸­æåè¿äºå­¦ä¹ å°çæ ·æ¬æ¦çã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>å³é®åæ°å¨äº<strong>çè®ºä¸è¯æäºJEPAçååç¼©é¡¹ä¸æ°æ®å¯åº¦ä¼°è®¡ä¹é´çåå¨èç³»</strong>ãè¿é¢ è¦äºä¼ ç»ä¸è®¤ä¸ºååç¼©é¡¹ä»ä»æ¯é²æ­¢æ¨¡åéåçè§ç¹ãæ¹æ³è®ºä¸ï¼è®ºææåºäºä¸ç§<strong>åºäºæ¨¡åéå¯æ¯ç©éµçé­åå½¢å¼è®¡ç®æ¹æ³</strong>ï¼ç¨äºä»å·²è®­ç»çJEPAä¸­é«æå°æåæ ·æ¬æ¦çï¼å¹¶å°å¶å½åä¸ºJEPA-SCOREãè¿ç§æ¹æ³å·ææ®éæ§ï¼ä¸ä¾èµäºå·ä½æ°æ®éææ¶æã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>ç»ä¸äºèªçç£å­¦ä¹ ä¸å¯åº¦ä¼°è®¡ï¼</strong> è¿é¡¹å·¥ä½å°èªçç£å­¦ä¹ ï¼ç¹å«æ¯JEPAå®¶æï¼ä¸çææ¨¡ååå¯åº¦ä¼°è®¡é¢åèç³»èµ·æ¥ï¼ä¸ºçè§£èªçç£å­¦ä¹ çæ·±å±æºå¶æä¾äºæ°çè§è§ã</li>
<li><strong>æåJEPAæ¨¡åçä»·å¼ï¼</strong> JEPAæ¨¡åä¸åä»ä»æ¯ç¨äºä¸æ¸¸ä»»å¡çç¹å¾æåå¨ï¼å®ä»¬ç°å¨è¢«è¯æåå¨å°åå«äºæ°æ®åå¸ä¿¡æ¯ï¼æå¤§å°æ©å±äºå¶åºç¨æ½åã</li>
<li><strong>æ°çæ°æ®çè§£å·¥å·ï¼</strong> JEPA-SCOREæä¾äºä¸ç§ä»ç°æèªçç£æ¨¡åä¸­âåè´¹âè·åæ°æ®å¯åº¦ä¿¡æ¯çæ¹æ³ï¼æ éé¢å¤è®­ç»ä¸é¨çå¯åº¦ä¼°è®¡æ¨¡åã</li>
<li><strong>æ¨å¨çè®ºç ç©¶ï¼</strong> è¿ä¸åç°å¯è½ä¼æ¿åæ´å¤å³äºèªçç£å­¦ä¹ ä¸­ä¸åç»ä»¶åè½åå¶ä¸ç»è®¡å­¦åçä¹é´èç³»ççè®ºç ç©¶ã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>æ°æ®ç­å± (Data Curation)ï¼</strong> å¯ä»¥å©ç¨å­¦ä¹ å°çæ ·æ¬æ¦çæ¥è¯å«æä»£è¡¨æ§çæ ·æ¬ï¼æè¿æ»¤æä½è´¨éãåä½çæ°æ®ï¼ä»èä¼åè®­ç»æ°æ®éã</li>
<li><strong>å¼å¸¸æ£æµ (Outlier Detection)ï¼</strong> ä½æ¦ççæ ·æ¬å¾å¯è½æ¯å¼å¸¸å¼ï¼JEPA-SCOREå¯ä»¥ç´æ¥ç¨äºè¯å«è¿äºå¼å¸¸ã</li>
<li><strong>å¯åº¦ä¼°è®¡ (Density Estimation)ï¼</strong> æä¾äºä¸ç§æ°çãå¯è½æ´é«æçå¯åº¦ä¼°è®¡æ¹æ³ï¼å°¤å¶æ¯å¨èªçç£å­¦ä¹ å·²ç»å¹¿æ³åºç¨çç¯å¢ä¸­ã</li>
<li><strong>çææ¨¡å (Generative Models)ï¼</strong> å¯¹æ°æ®å¯åº¦ççè§£æ¯çææ¨¡åçåºç¡ï¼è¿é¡¹ç ç©¶å¯è½ä¸ºæ¡ä»¶çæãæ ·æ¬è´¨éè¯ä¼°ç­æä¾æ°çæè·¯ã</li>
<li><strong>å¤æ¨¡æå­¦ä¹  (Multimodal Learning)ï¼</strong> æè¦ä¸­æå°å¨MetaCLIPç­å¤æ¨¡ææ¨¡åä¸çéªè¯ï¼è¡¨æè¯¥æ¹æ³å¨å¤çå¤æå¤æ¨¡ææ°æ®åå¸æ¹é¢ä¹å·ææ½åã</li>
<li><strong>èªçç£å­¦ä¹ çå¯è§£éæ§ï¼</strong> æ·±å¥çè§£JEPAçå·¥ä½åçï¼æå©äºæé«èªçç£å­¦ä¹ æ¨¡åçå¯è§£éæ§ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>è®¡ç®ææ¬ï¼</strong> è½ç¶æè¦å£°ç§°âé«æä¸ä»¥é­åå½¢å¼âè®¡ç®éå¯æ¯ç©éµï¼ä½å¨éå¸¸å¤§çæ¨¡ååé«ç»´æ°æ®ä¸ï¼è®¡ç®éå¯æ¯ç©éµçææ¬ä»ç¶å¯è½æ¯ä¸ä¸ªå®éçèèå ç´ ã</li>
<li><strong>âæåè®­ç»âçå®ä¹ï¼</strong> è®ºæå¼ºè°âä»»ä½æåè®­ç»çJEPAâï¼ä½âæåâçå®ä¹å¯è½éè¦è¿ä¸æ­¥æç¡®ãä¾å¦ï¼å¦æJEPAè®­ç»ä¸ååææ¶æå°æ¬¡ä¼è§£ï¼å¶å­¦ä¹ å°çå¯åº¦ä¼°è®¡çåç¡®æ§å¦ä½ï¼</li>
<li><strong>å¯åº¦ä¼°è®¡çè´¨éï¼</strong> å°½ç®¡è½å¤ä¼°è®¡å¯åº¦ï¼ä½å¶ä¼°è®¡çåç¡®æ§ãé²æ£æ§ä»¥åä¸ä¸é¨çå¯åº¦ä¼°è®¡æ¹æ³ï¼å¦æµæ¨¡åãæ©æ£æ¨¡åï¼ç¸æ¯çä¼å£ï¼ä»éå¨æ´å¹¿æ³çåºæ¯ä¸è¿è¡æ·±å¥è¯ä¼°ã</li>
<li><strong>çè®ºä¸å®è·µçå·®è·ï¼</strong> çè®ºè¯æäºèç³»ï¼ä½å®éåºç¨ä¸­ï¼è¿ç§å¯åº¦ä¼°è®¡å¨åç§ä¸æ¸¸ä»»å¡ä¸­çè¡¨ç°ï¼ä¾å¦ï¼å¼å¸¸æ£æµçF1åæ°ï¼æ¯å¦è½è¶è¶ç°æSOTAæ¹æ³ï¼ä»éå¤§éå®éªéªè¯ã</li>
<li><strong>ä»éäºJEPAå®¶æï¼</strong> å°½ç®¡JEPAå®¶ææ¶µçäºI-JEPAåDINOv2ç­éè¦æ¹æ³ï¼ä½è¯¥çè®ºæ¯å¦è½æ¨å¹¿å°å¶ä»éJEPAç±»çèªçç£å­¦ä¹ æ¹æ³ï¼å¦å¯¹æ¯å­¦ä¹ ãæ©ç å¾åå»ºæ¨¡ç­ï¼ï¼æè¦ä¸­å¹¶æªæåã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºèªçç£å­¦ä¹ é¢åå¸¦æ¥äºä»¤äººå´å¥çæ°è§è§ï¼å®ä¸ä»æ­ç¤ºäºJEPAæ¨¡åçä¸ä¸ªâéèâè½åï¼è¿æä¾äºä¸ç§å®ç¨çå·¥å·æ¥å©ç¨è¿ä¸è½åãå¦æå¶çè®ºåå®éªç»æè½å¤å¾å°å¹¿æ³éªè¯ï¼å®å°å¯¹æä»¬çè§£ååºç¨èªçç£å­¦ä¹ æ¨¡åäº§çæ·±è¿å½±åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our findings are
empirically validated across datasets (synthetic, controlled, and Imagenet) and
across different Self Supervised Learning methods falling under the JEPA family
(I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP.</li>
<li>We denote the
method extracting the JEPA learned density as {\bf JEPA-SCORE}.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05949v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05949v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05903v1'></a></p>
<h2 id="kaputt-a-large-scale-dataset-for-visual-defect-detection"><a href="https://arxiv.org/abs/2510.05903v1">Kaputt: A Large-Scale Dataset for Visual Defect Detection</a></h2>
<p><strong>Authors:</strong> Sebastian HÃ¶fer, Dorian Henning, Artemij Amiranashvili, Douglas Morrison, Mariliza Tzes, Ingmar Posner, Marc Matvienko, Alessandro Rennola, Anton Milan</p>
<p><strong>Published:</strong> 2025-10-07</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡è¯¦ç»åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Kaputt: A Large-Scale Dataset for Visual Defect Detection</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçä¸»è¦è´¡ç®æ¯å¼å¥äºä¸ä¸ªåä¸ºâKaputtâçå¨æ°å¤§è§æ¨¡æ°æ®éï¼ç¨äºé¶å®ç©æµç¯å¢ä¸­çè§è§ç¼ºé·æ£æµãè¯¥æ°æ®éæ¨å¨è§£å³ç°æå·¥ä¸å¼å¸¸æ£æµåºåï¼å¦MVTec-ADåVisAï¼å¨å¤çç©ä½å§¿æåå¤è§å¤æ ·æ§æ¹é¢çå±éæ§ï¼è¿äºç°æåºåå¨åæ§å¶é åºæ¯ä¸­å·²æ¥è¿é¥±åãKaputtæ°æ®éçè§æ¨¡æ¯MVTec-ADç40åï¼åå«è¶è¿23ä¸å¼ å¾åå2.9ä¸ä¸ªç¼ºé·å®ä¾ï¼å¹¶æç¡®æåºå½åæåè¿çå¼å¸¸æ£æµæ¹æ³å¨è¯¥æ°æ®éä¸çè¡¨ç°è¿ä½äºé¢æï¼AUROCä»ä¸º56.96%ï¼ï¼ä»èä¸ºè¯¥é¢åè®¾å®äºæ°çææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>è¿ç¯è®ºæçå³é®åæ°å¨äº<strong>æ°æ®éçæå»ºåå¶æéå¯¹çæ°é¢ä¸æ´å·æææ§çé®é¢è®¾å®</strong>ãå®æ²¡ææåºæ°çç®æ³ï¼èæ¯éè¿åå»ºä¸ä¸ªå¤§è§æ¨¡ãé«å¤æ ·æ§çæ°æ®éæ¥æ¨å¨é¢ååå±ãå·ä½æ¥è¯´ï¼</p>
<ul>
<li><strong>é®é¢åçè½¬ç§»åæ©å±ï¼</strong> ä»é«åº¦åæ§çå¶é åºæ¯è½¬åé¶å®ç©æµï¼å¼å¥äºç©ä½å§¿æåå¤è§çå·¨å¤§å¤æ ·æ§ï¼è¿å¨ç°æåºåä¸­æ¯ç¼ºå¤±çã</li>
<li><strong>æ°æ®éè§æ¨¡åå¤ææ§ï¼</strong> Kaputtæ°æ®éçè§æ¨¡ï¼23ä¸å¼ å¾åï¼2.9ä¸ä¸ªç¼ºé·å®ä¾ï¼4.8ä¸ä¸ªç¬ç«ç©ä½ï¼è¿è¶ç°æåºåï¼æä¾äºæ´ä¸°å¯çè®­ç»åæµè¯æ°æ®ã</li>
<li><strong>æç¡®çææè®¾å®ï¼</strong> éè¿å±ç¤ºç°æSOTAæ¹æ³å¨è¯¥æ°æ®éä¸çä½æ§è½ï¼56.96% AUROCï¼ï¼æç¡®æåºäºå½åæ¹æ³çå±éæ§ï¼å¹¶ä¸ºæªæ¥çç ç©¶è®¾å®äºæç¡®çæ¹è¿ç®æ ã</li>
<li><strong>å¼ºè°âæ­£å¸¸æ ·æ¬âçå©ç¨ææï¼</strong> æè¦ä¸­æå°âç°ææ¹æ³é¾ä»¥å¨ä¸¥éçå§¿æåå¤è§ååä¸å©ç¨æ­£å¸¸æ ·æ¬âï¼è¿æç¤ºäºè¯¥æ°æ®éçå¤ææ§ä¸ä»å¨äºç¼ºé·æ¬èº«ï¼è¿å¨äºå¦ä½ææå­¦ä¹ âæ­£å¸¸âçåå¸ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨å¼å¸¸æ£æµç ç©¶çæ°æ¹åï¼</strong> è¯¥æ°æ®éå°ä¿ä½¿ç ç©¶äººåå¼åæ´é²æ£ãæ´æ³åçå¼å¸¸æ£æµç®æ³ï¼ä»¥åºå¯¹é«å§¿æåå¤è§å¤æ ·æ§ã</li>
<li><strong>å éé¶å®ç©æµèªå¨åï¼</strong> è§£å³é¶å®ç©æµä¸­çç¼ºé·æ£æµé®é¢ï¼å°ç´æ¥ä¿è¿è¯¥é¢åçèªå¨ååæçæåï¼åå°äººå·¥æ£æ¥çææ¬åéè¯¯ã</li>
<li><strong>ä¿è¿èªçç£/æ çç£å­¦ä¹ çåå±ï¼</strong> å¼å¸¸æ£æµæ¬è´¨ä¸æ¯æ çç£æåçç£é®é¢ï¼è¯¥æ°æ®éçæææ§å°æ¿å±å¨è¿äºé¢åè¿è¡æ´æ·±å¥çç ç©¶ï¼å°¤å¶æ¯å¨å¦ä½ä»å¤§éâæ­£å¸¸âä½é«åº¦ååçæ ·æ¬ä¸­å­¦ä¹ ææè¡¨ç¤ºæ¹é¢ã</li>
<li><strong>æ°çåºååæ¯è¾å¹³å°ï¼</strong> Kaputtå°æä¸ºæªæ¥å¼å¸¸æ£æµç®æ³æ§è½è¯ä¼°çæ°æ åï¼å°¤å¶æ¯å¨éåæ§å·¥ä¸åºæ¯ä¸­ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨å¯è½åçäºè¿é¡¹ç ç©¶</strong></p>
<ul>
<li><strong>æºè½ä»å¨åç©æµï¼</strong> èªå¨è¯å«åè£¹ãååæè®¾å¤å¨è¿è¾ãå­å¨è¿ç¨ä¸­çæåæç¼ºé·ã</li>
<li><strong>è´¨éæ§å¶ï¼éåæ§ç¯å¢ï¼ï¼</strong> ä¾å¦ï¼å¨åæ¶åç±»ãåäº§ååæ£ç­åºæ¯ä¸­ï¼ç©ä½å§¿æåå¤è§ååå¤§ã</li>
<li><strong>æºå¨äººæååæä½ï¼</strong> æºå¨äººéè¦è¯å«å¹¶é¿åæåæç¼ºé·çç©ä½ï¼æå¨å¤æç¯å¢ä¸­è¿è¡æä½ã</li>
<li><strong>è®¡ç®æºè§è§ä¸­çåæ³åï¼Domain Generalizationï¼åå°æ ·æ¬å­¦ä¹ ï¼Few-Shot Learningï¼ï¼</strong> åºå¯¹é«å¤æ ·æ§æ°æ®ï¼éè¦æ¨¡åå·å¤æ´å¥½çæ³åè½åï¼å¹¶å¯è½éè¦ä»å°éç¼ºé·æ ·æ¬ä¸­å­¦ä¹ ã</li>
<li><strong>æ°æ®å¢å¼ºååææ°æ®çæï¼</strong> é¢å¯¹å¦æ­¤å¤æçæ°æ®ï¼å¦ä½ææå°è¿è¡æ°æ®å¢å¼ºæçæåææ°æ®ä»¥æé«æ¨¡åæ§è½å°æä¸ºä¸ä¸ªç ç©¶æ¹åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>æ°æ®éçç¼ºé·ç±»åæªæç¡®è¯´æï¼</strong> æè¦ä¸­åªæå°âç¼ºé·å®ä¾âï¼ä½æ²¡æè¯¦ç»è¯´æç¼ºé·çç§ç±»ãå¤§å°ãä¸¥éç¨åº¦ç­ï¼è¿å¯è½ä¼å½±åç ç©¶äººåå¯¹æ°æ®éå¤ææ§ççè§£åç®æ³è®¾è®¡ã</li>
<li><strong>âé¶å®ç©æµâçå·ä½åºæ¯æªå®å¨å±å¼ï¼</strong> è½ç¶æå°äºé¶å®ç©æµï¼ä½å·ä½æ¯åªç§ååãåªç§åè£ãåªç§ç©æµç¯èï¼å¦å¥åºãåºåºãåæ£ï¼ç­ç»èç¼ºå¤±ï¼è¿å¯è½å½±åç ç©¶äººåå¯¹é®é¢èæ¯çæ·±å¥çè§£ã</li>
<li><strong>ç¼ºä¹å¯¹âæ­£å¸¸æ ·æ¬âå¤æ ·æ§çéåï¼</strong> æè¦å¼ºè°äºâå§¿æåå¤è§ååâï¼ä½æ²¡ææä¾éåçææ æ¥æè¿°è¿ç§å¤æ ·æ§ï¼ä¾å¦å§¿æååçèå´ãåç§ååçç¨åº¦ç­ã</li>
<li><strong>ç°ææ¹æ³å¤±è´¥çå·ä½åå åææéï¼</strong> æè¦ä¸­æå°âç°ææ¹æ³é¾ä»¥å¨ä¸¥éçå§¿æåå¤è§ååä¸å©ç¨æ­£å¸¸æ ·æ¬âï¼ä½æ´æ·±å¥çå®æ§æå®éåæï¼ä¾å¦ï¼æ¯ç¹å¾æåä¸è¶³ï¼è¿æ¯å¼å¸¸åæ°è®¡ç®æºå¶ä¸éåºï¼ï¼å¨æè¦ä¸­å¹¶æªè¯¦ç»å±å¼ã</li>
<li><strong>æ°æ®éçæ æ³¨è´¨éåä¸è´æ§ï¼</strong> ä½ä¸ºä¸ä¸ªå¤§è§æ¨¡æ°æ®éï¼å¶æ æ³¨çåç¡®æ§åä¸è´æ§è³å³éè¦ï¼ä½æè¦ä¸­æ²¡ææåç¸å³çæ æ³¨åè®®æè´¨éæ§å¶æªæ½ã</li>
<li><strong>ä»å³æ³¨AUROCææ ï¼</strong> è½ç¶AUROCæ¯å¼å¸¸æ£æµçå¸¸ç¨ææ ï¼ä½å¨æäºå®éåºç¨ä¸­ï¼å¶ä»ææ å¦F1åæ°ãç²¾ç¡®ç-å¬åçæ²çº¿ä¸çé¢ç§¯ï¼AUPRCï¼ç­ä¹å¯è½å¾éè¦ï¼å°¤å¶æ¯å¨ç¼ºé·æ ·æ¬ç¨å°çæåµä¸ãæè¦ä¸­æªæåå¶ä»ææ ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥ä¸ä¸ªå·ææææ§çæ°æ°æ®éï¼ä¸ºè®¡ç®æºè§è§é¢åçå¼å¸¸æ£æµç ç©¶æ³¨å¥äºæ°çæ´»åãå®æç¡®æåºäºå½åSOTAæ¹æ³å¨å¤ççå®ä¸çå¤ææ§æ¹é¢çå±éæ§ï¼å¹¶ä¸ºæªæ¥çç ç©¶è®¾å®äºæ¸æ°çæ¹åãå¶å¯¹é¶å®ç©æµé¢åçå³æ³¨ä¹ä½¿å¶å·æéè¦çå®éåºç¨ä»·å¼ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a novel large-scale dataset for defect detection in a logistics
setting.</li>
<li>Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores.</li>
<li>In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance.</li>
<li>Leading anomaly detection methods fall short when applied
to this new setting.</li>
<li>To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets.</li>
<li>To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset.</li>
<li>With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05903v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05903v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-08 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
