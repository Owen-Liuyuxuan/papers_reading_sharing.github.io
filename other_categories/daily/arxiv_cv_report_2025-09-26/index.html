<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-26 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-24/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-29/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-26">Arxiv Computer Vision Papers - 2025-09-26</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-models-are-zero-shot-learners-and-reasoners" class="nav-link">Video models are zero-shot learners and reasoners</a>
                </li>
                <li class="nav-item">
                    <a href="#sd35-flash-distribution-guided-distillation-of-generative-flows" class="nav-link">SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</a>
                </li>
                <li class="nav-item">
                    <a href="#quantized-visual-geometry-grounded-transformer" class="nav-link">Quantized Visual Geometry Grounded Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#does-flux-already-know-how-to-perform-physically-plausible-image-composition" class="nav-link">Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a>
                </li>
                <li class="nav-item">
                    <a href="#mmr1-enhancing-multimodal-reasoning-with-variance-aware-sampling-and-open-resources" class="nav-link">MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</a>
                </li>
                <li class="nav-item">
                    <a href="#hunyuan3d-omni-a-unified-framework-for-controllable-generation-of-3d-assets" class="nav-link">Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</a>
                </li>
                <li class="nav-item">
                    <a href="#evaluating-the-evaluators-metrics-for-compositional-text-to-image-generation" class="nav-link">Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#unitransfer-video-concept-transfer-via-progressive-spatial-and-timestep-decomposition" class="nav-link">UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</a>
                </li>
                <li class="nav-item">
                    <a href="#background-prompt-for-few-shot-out-of-distribution-detection" class="nav-link">Background Prompt for Few-Shot Out-of-Distribution Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#keyworld-key-frame-reasoning-enables-effective-and-efficient-world-models" class="nav-link">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-26">Arxiv Computer Vision Papers - 2025-09-26</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月24日Arxiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解最新进展。</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文执行摘要 (2025-09-24)</strong></p>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文展示了计算机视觉领域几个关键方向的持续快速发展：</p>
<ul>
<li><strong>多模态与视频理解的深化：</strong> 视频模型作为零样本学习器和推理器的能力正在被积极探索，同时多模态推理的鲁素性也在通过新的采样和资源整合方法得到提升。</li>
<li><strong>生成模型与3D内容的进步：</strong> 扩散模型（特别是其蒸馏和效率提升）以及可控的3D资产生成是显著的焦点。对生成模型输出的评估指标也受到了关注。</li>
<li><strong>模型效率与鲁棒性：</strong> 量化技术被应用于Transformer模型以提高效率，同时对分布外检测（OOD）和物理真实性合成的关注表明了对模型在复杂现实世界场景中鲁棒性的追求。</li>
<li><strong>世界模型与具身智能的交汇：</strong> 关键帧推理被引入世界模型，预示着在具身智能和强化学习背景下对更有效和高效环境建模的兴趣。</li>
</ul>
<p><strong>2. 特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"Video models are zero-shot learners and reasoners" by Thaddäus Wiedemer et et al. (1号论文):</strong> 这篇论文如果能成功证明视频模型在零样本学习和推理方面的强大能力，将对多模态AI的未来发展产生深远影响，可能预示着视频基础模型在通用智能方面的新突破。</li>
<li><strong>"Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets" by Team Hunyuan3D et al. (6号论文):</strong> 统一的可控3D资产生成框架是工业界和研究界都高度关注的方向。如果该框架能实现高质量和高可控性，将极大地推动3D内容创作和虚拟现实应用。</li>
<li><strong>"SD3.5-Flash: Distribution-Guided Distillation of Generative Flows" by Hmrishav Bandyopadhyay et al. (2号论文):</strong> 扩散模型的高效性一直是其应用的关键瓶颈。SD3.5-Flash通过分布引导蒸馏来提高生成流的效率，这对于实时生成和资源受限环境下的部署至关重要。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>视频模型作为通用推理引擎：</strong> 1号论文强调了视频模型超越简单识别，向更高级的零样本推理能力发展。</li>
<li><strong>高效生成模型蒸馏：</strong> 2号论文的“分布引导蒸馏”是提升扩散模型效率的有效途径，预示着未来更多关于模型压缩和加速的研究。</li>
<li><strong>3D资产的统一可控生成：</strong> 6号论文的“统一框架”表明研究正从单一3D生成任务向更全面、可控的3D内容创作平台发展。</li>
<li><strong>世界模型中的关键帧推理：</strong> 10号论文将“关键帧推理”引入世界模型，这是一种在具身智能和强化学习中提高环境建模效率和有效性的新策略。</li>
<li><strong>背景提示（Background Prompt）用于OOD检测：</strong> 9号论文提出了一种新颖的少样本OOD检测方法，利用背景信息来增强模型的泛化能力。</li>
</ul>
<p><strong>4. 建议完整阅读的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对于关注基础模型和通用AI的研究人员：</strong><ul>
<li><strong>1. "Video models are zero-shot learners and reasoners"</strong></li>
<li><strong>5. "MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources"</strong></li>
</ul>
</li>
<li><strong>对于关注生成模型、3D内容和效率的研究人员：</strong><ul>
<li><strong>2. "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows"</strong></li>
<li><strong>6. "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets"</strong></li>
<li><strong>7. "Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation"</strong> (对于评估生成模型输出至关重要)</li>
</ul>
</li>
<li><strong>对于关注模型鲁棒性、效率和具身智能的研究人员：</strong><ul>
<li><strong>3. "Quantized Visual Geometry Grounded Transformer"</strong></li>
<li><strong>9. "Background Prompt for Few-Shot Out-of-Distribution Detection"</strong></li>
<li><strong>10. "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models"</strong></li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究方向最相关的论文。建议根据您的具体兴趣进一步深入阅读。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.20328v1">Video models are zero-shot learners and reasoners</a></li>
<li><a href="#2509.21318v1">SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</a></li>
<li><a href="#2509.21302v1">Quantized Visual Geometry Grounded Transformer</a></li>
<li><a href="#2509.21278v1">Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a></li>
<li><a href="#2509.21268v1">MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</a></li>
<li><a href="#2509.21245v1">Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</a></li>
<li><a href="#2509.21227v1">Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</a></li>
<li><a href="#2509.21086v1">UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</a></li>
<li><a href="#2509.21055v1">Background Prompt for Few-Shot Out-of-Distribution Detection</a></li>
<li><a href="#2509.21027v1">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.20328v1'></a></p>
<h2 id="video-models-are-zero-shot-learners-and-reasoners"><a href="https://arxiv.org/abs/2509.20328v1">Video models are zero-shot learners and reasoners</a></h2>
<p><strong>Authors:</strong> Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos</p>
<p><strong>Published:</strong> 2025-09-24</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The remarkable zero-shot capabilities of Large Language Models (LLMs) have
propelled natural language processing from task-specific models to unified,
generalist foundation models. This transformation emerged from simple
primitives: large, generative models trained on web-scale data. Curiously, the
same primitives apply to today's generative video models. Could video models be
on a trajectory towards general-purpose vision understanding, much like LLMs
developed general-purpose language understanding? We demonstrate that Veo 3 can
solve a broad variety of tasks it wasn't explicitly trained for: segmenting
objects, detecting edges, editing images, understanding physical properties,
recognizing object affordances, simulating tool use, and more. These abilities
to perceive, model, and manipulate the visual world enable early forms of
visual reasoning like maze and symmetry solving. Veo's emergent zero-shot
capabilities indicate that video models are on a path to becoming unified,
generalist vision foundation models.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文《视频模型是零样本学习者和推理者》探讨了视频模型在通用视觉理解方面是否能像大型语言模型（LLMs）在语言理解方面一样，发展出零样本能力。</p>
<p><strong>1. 论文主要问题或研究问题：</strong>
论文的核心问题是：视频模型能否像LLMs一样，通过大规模生成式训练和网络规模数据，发展出通用的视觉理解能力，并展现出零样本学习和推理能力，从而成为统一的、通用型视觉基础模型？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>零样本能力演示：</strong> 论文通过对Veo 3模型进行广泛的定性（62项任务）和定量（7项任务）评估，展示了其在未经明确训练的任务上解决问题的能力。这些任务涵盖了感知（如图像分割、边缘检测）、建模（如理解物理属性）、操作（如图像编辑、工具使用模拟）和推理（如迷宫和对称性求解）等视觉堆栈的各个层面。
*   <strong>“帧链（Chain-of-Frames）”视觉推理概念：</strong> 论文提出，视频模型通过逐帧生成视频来模拟LLMs的“思维链（Chain-of-Thought）”推理过程，从而在时间和空间上进行视觉推理。
*   <strong>性能提升的证据：</strong> 论文通过比较Veo 3与其前身Veo 2的性能，展示了视频模型能力的快速进步，表明其正朝着通用视觉基础模型发展。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>广泛的零样本能力：</strong> Veo 3在感知、建模和操作视觉世界方面展现出显著的零样本能力，能够执行对象分割、边缘检测、图像编辑、理解物理属性、识别对象功能、模拟工具使用等多种任务。
*   <strong>早期视觉推理能力：</strong> 模型能够进行迷宫求解和对称性求解等早期形式的视觉推理，这表明视频模型不仅能处理静态图像，还能理解动态场景并进行序列决策。
*   <strong>向通用视觉基础模型迈进：</strong> Veo 3的这些新兴零样本能力预示着视频模型有望成为统一的、通用型视觉基础模型，就像LLMs在自然语言处理领域所做的那样，取代许多任务专用模型。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>性能仍低于任务专用模型：</strong> 尽管Veo 3表现出色，但在许多任务上，其性能仍低于专门为这些任务训练的定制模型。这与LLMs早期的情况类似。
*   <strong>视频生成成本高昂：</strong> 目前生成视频的成本高于运行任务专用模型，但论文认为随着技术发展，成本会迅速下降。
*   <strong>对提示的敏感性：</strong> 模型的性能对提示的精确描述高度敏感，需要精心设计的提示才能获得最佳结果。
*   <strong>模型偏差：</strong> 在某些任务（如视觉类比的反射和旋转）中，模型存在系统性的错误偏差。
*   <strong>物理模拟的局限性：</strong> 在某些物理推理任务中，模型可能违反物理定律或产生不切实际的运动。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提升零样本性能：</strong> 进一步提高视频模型在各种任务上的零样本性能，使其能够与任务专用模型相媲美。
*   <strong>推理和规划：</strong> 探索更高级的视觉推理和规划能力，例如通过更复杂的“帧链”机制来解决多步骤问题。
*   <strong>成本优化：</strong> 降低视频生成和推理的计算成本，使其更具实用性。
*   <strong>提示工程：</strong> 发展更强大的视觉提示工程技术，以更好地指导视频模型执行复杂任务。
*   <strong>消除模型偏差：</strong> 解决模型在某些任务中存在的系统性偏差，提高其泛化能力和准确性。
*   <strong>多模态融合：</strong> 进一步探索视频模型与语言模型等其他模态的融合，以实现更全面的通用智能。</p>
<p>总而言之，这篇论文通过对Veo 3的广泛评估，有力地论证了视频模型在零样本学习和视觉推理方面的巨大潜力，并将其定位为未来通用视觉基础模型的重要发展方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that Veo 3 can
solve a broad variety of tasks it wasn't explicitly trained for: segmenting
objects, detecting edges, editing images, understanding physical properties,
recognizing object affordances, simulating tool use, and more.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.20328v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.20328v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21318v1'></a></p>
<h2 id="sd35-flash-distribution-guided-distillation-of-generative-flows"><a href="https://arxiv.org/abs/2509.21318v1">SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</a></h2>
<p><strong>Authors:</strong> Hmrishav Bandyopadhyay, Rahim Entezari, Jim Scott, Reshinth Adithyan, Yi-Zhe Song, Varun Jampani</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices. Our
approach distills computationally prohibitive rectified flow models through a
reformulated distribution matching objective tailored specifically for few-step
generation. We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.
Combined with comprehensive pipeline optimizations like text encoder
restructuring and specialized quantization, our system enables both rapid
generation and memory-efficient deployment across different hardware
configurations. This democratizes access across the full spectrum of devices,
from mobile phones to desktop computers. Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“SD3.5-Flash: Distribution-Guided Distillation of Generative Flows”的全面摘要，由Hmrishav Bandyopadhyay等人撰写：</p>
<p><strong>论文摘要：SD3.5-Flash: 分布引导的生成流蒸馏</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前最先进的图像生成模型（特别是基于修正流的模型）计算成本过高的问题，这些模型通常需要大量计算资源（如25+步推理、16GB+VRAM、30+秒/图像），导致它们无法在普通消费级设备（如手机、桌面电脑）上高效运行。核心研究问题是如何通过高效的少步蒸馏框架，将高质量图像生成能力带到更广泛、更易访问的消费级设备上。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
SD3.5-Flash引入了以下关键创新和方法论贡献：</p>
<ul>
<li><strong>重新制定的分布匹配目标（Reformulated Distribution Matching Objective）：</strong> 针对少步生成场景，作者重新设计了分布匹配目标，以蒸馏计算成本高昂的修正流模型。</li>
<li><strong>时间步共享（Timestep Sharing）：</strong> 为了减少梯度噪声并提高训练稳定性，该方法在计算分布匹配时，使用学生模型轨迹上的样本而非随机轨迹点进行估计。这确保了在已知噪声水平下稳定的梯度信号和可靠的ODE轨迹流预测。</li>
<li><strong>分步时间步微调（Split-Timestep Fine-Tuning）：</strong> 为解决少步蒸馏中模型容量与图像质量（特别是提示对齐）之间的权衡问题，作者在训练期间暂时扩展模型容量。通过将预训练模型复制到不同的分支，并在不相交的时间步范围内进行训练，然后将它们合并为一个统一的检查点，以提高提示对齐和语义保真度。</li>
<li><strong>全面的管道优化（Comprehensive Pipeline Optimizations）：</strong> 包括文本编码器重构（利用编码器dropout预训练替代T5-XXL）和专门的量化方案（从16位到6位精度），以平衡内存占用和推理速度，实现快速生成和内存高效部署。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
SD3.5-Flash通过广泛的评估（包括大规模用户研究）展示了显著的成果：</p>
<ul>
<li><strong>性能超越现有少步方法：</strong> SD3.5-Flash在图像质量和提示对齐方面持续优于现有的少步生成方法，甚至在某些指标上超越了多步教师模型。</li>
<li><strong>高效部署：</strong> 结合管道优化，该系统实现了快速生成和内存高效部署，使其能够在各种硬件配置（从移动设备到桌面电脑）上运行，从而真正普及了先进的生成式AI。</li>
<li><strong>高质量图像生成：</strong> 即使在少步推理（如4步）下，模型也能生成高保真图像，并展现出卓越的提示遵循性和构图理解能力，尤其在处理解剖结构和多对象构图等传统蒸馏方法难以处理的场景中表现出色。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到了以下局限性：</p>
<ul>
<li><strong>质量与多样性的权衡：</strong> 像所有蒸馏过程一样，SD3.5-Flash在复杂生成任务中，需要在质量和多样性方面做出一定的权衡。</li>
<li><strong>T5-XXL移除的影响：</strong> 为了实现更快的推理速度和更低的内存占用而移除T5-XXL文本编码器，可能会导致模型在构建复杂构图时面临挑战，因为条件上下文的质量会下降。</li>
<li><strong>少步模型的固有局限性：</strong> 这些局限性并非SD3.5-Flash独有，而是少步模型近似扩散轨迹的自然结果。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文没有明确提出未来的研究方向，但从其工作内容可以推断出以下潜在方向：</p>
<ul>
<li><strong>进一步优化少步蒸馏：</strong> 探索更先进的蒸馏技术，以在更少的推理步数下进一步缩小与多步教师模型之间的性能差距，尤其是在复杂构图和语义理解方面。</li>
<li><strong>更广泛的硬件适配：</strong> 进一步优化模型和管道，以支持更广泛的边缘设备和低功耗平台，实现更广泛的普及。</li>
<li><strong>动态适应性蒸馏：</strong> 研究如何根据不同的输入提示或用户需求，动态调整蒸馏策略或模型配置，以在质量和速度之间取得最佳平衡。</li>
<li><strong>结合其他生成范式：</strong> 探索将修正流蒸馏与其他生成范式（如GANs或VAE）结合，以进一步提升生成质量或效率。</li>
<li><strong>提升FID指标：</strong> 尽管在其他指标上表现出色，但FID指标仍有提升空间，这可能需要更深入地研究生成样本与真实图像分布的匹配。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices.</li>
<li>We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.</li>
<li>Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21318v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21318v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21302v1'></a></p>
<h2 id="quantized-visual-geometry-grounded-transformer"><a href="https://arxiv.org/abs/2509.21302v1">Quantized Visual Geometry Grounded Transformer</a></h2>
<p><strong>Authors:</strong> Weilun Feng, Haotong Qin, Mingqiang Wu, Chuanguang Yang, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs), have made remarkable progress with the use of
large-scale transformers. Their prohibitive computational and memory costs
severely hinder real-world deployment. Post-Training Quantization (PTQ) has
become a common practice for compressing and accelerating models. However, we
empirically observe that PTQ faces unique obstacles when compressing
billion-scale VGGTs: the data-independent special tokens induce heavy-tailed
activation distributions, while the multi-view nature of 3D data makes
calibration sample selection highly unstable. This paper proposes the first
Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly. Second, we design Noise-Filtered Diverse Sampling, which
filters outliers via deep-layer statistics and constructs frame-aware diverse
calibration clusters to ensure stable quantization ranges. Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin. We highlight
that our 4-bit QuantVGGT can deliver a 3.7<script type="math/tex">\times</script> memory reduction and
2.5<script type="math/tex">\times</script> acceleration in real-hardware inference, while maintaining
reconstruction accuracy above 98\% of its full-precision counterpart. This
demonstrates the vast advantages and practicality of QuantVGGT in
resource-constrained scenarios. Our code is released in
https://github.com/wlfeng0509/QuantVGGT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Weilun Feng等人撰写的论文“Quantized Visual Geometry Grounded Transformer”的全面摘要。</p>
<hr />
<h3 id="quantized-visual-geometry-grounded-transformer_1">论文摘要：Quantized Visual Geometry Grounded Transformer</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决基于学习的3D重建模型，特别是视觉几何基础Transformer (VGGT) 在实际部署中面临的计算和内存成本过高的问题。尽管VGGT在3D任务中取得了显著进展，但其数十亿参数的规模严重阻碍了其在资源受限环境中的应用。现有的训练后量化（PTQ）方法在压缩VGGT这类大规模模型时面临独特挑战：数据独立的特殊token（如相机和注册token）导致激活分布呈重尾（heavy-tailed），以及3D数据的多视角特性使得校准样本选择高度不稳定。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
为了应对上述挑战，论文提出了首个专门针对VGGT的量化框架——QuantVGGT，其主要贡献包括：</p>
<ul>
<li><strong>双平滑细粒度量化（Dual-Smoothed Fine-Grained Quantization, DSFQ）：</strong> 这种方法结合了预全局Hadamard旋转和后局部通道平滑。Hadamard旋转用于分散异常值并平滑重尾分布，而局部通道平滑则在旋转空间中标准化通道级方差，从而鲁棒地缓解了重尾分布和通道间方差问题，显著降低了量化误差。</li>
<li><strong>噪声过滤多样化采样（Noise-Filtered Diverse Sampling, NFDS）：</strong> 为了克服校准不稳定性，该方法通过深度层统计过滤异常值，并构建帧感知的多样化校准簇。这确保了校准样本集具有代表性和稳定性，从而保证了稳定的量化范围。</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
综合实验结果表明，QuantVGGT在不同基准和位宽下均取得了最先进的性能，显著超越了现有通用的量化方法。</p>
<ul>
<li><strong>性能保持：</strong> 在W4A4（4比特权重和4比特激活）设置下，QuantVGGT在Co3Dv2数据集上的相机姿态估计任务中，其重建精度仍能保持在全精度对应模型的98%以上。</li>
<li><strong>效率提升：</strong> 4比特的QuantVGGT在真实硬件推理中实现了3.7倍的内存减少和2.5倍的加速。</li>
<li><strong>泛化能力：</strong> 即使在W8A8（8比特权重和8比特激活）设置下，QuantVGGT在DTU数据集上的点云图估计任务中也表现出良好的泛化能力，甚至在某些指标上超越了全精度模型。</li>
</ul>
<p>这些结果充分证明了QuantVGGT在资源受限场景下的巨大优势和实用性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文主要关注PTQ方法，并未深入探讨量化感知训练（QAT）的潜力，尽管QAT通常能在极低位宽下提供更好的性能，但其需要大量的训练资源。此外，论文主要针对VGGT模型，其提出的特殊token和多视角数据特性是VGGT独有的，因此QuantVGGT的某些组件可能不直接适用于其他类型的模型。尽管论文展示了在W4A4设置下的优异性能，但更低位宽（如2比特）的量化可能仍需进一步探索。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文并未明确提出未来研究方向，但从其内容和当前领域趋势可以推断出以下几点：</p>
<ul>
<li><strong>更低位宽量化：</strong> 探索如何将QuantVGGT扩展到更低位宽（如2比特），同时保持甚至提高重建精度。</li>
<li><strong>跨模型泛化：</strong> 研究QuantVGGT中的核心思想（如双平滑和噪声过滤采样）如何泛化到其他大规模3D重建模型或更广泛的Transformer架构。</li>
<li><strong>硬件协同设计：</strong> 进一步优化QuantVGGT以更好地利用特定硬件加速器的特性，实现更大的推理速度和能效提升。</li>
<li><strong>动态量化策略：</strong> 论文提到了动态token-wise量化带来的性能提升，未来可以深入研究更复杂的动态量化策略，以适应不同场景和数据分布。</li>
</ul>
<hr />
<p>这份摘要旨在清晰、简洁地传达论文的核心贡献和发现，并突出其在计算机视觉领域的重要性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly.</li>
<li>Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21302v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21302v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21278v1'></a></p>
<h2 id="does-flux-already-know-how-to-perform-physically-plausible-image-composition"><a href="https://arxiv.org/abs/2509.21278v1">Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a></h2>
<p><strong>Authors:</strong> Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs. Modern
text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential
physical and resolution priors, yet lack a framework to unleash them without
resorting to latent inversion, which often locks object poses into contextually
inappropriate orientations, or brittle attention surgery. We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained
customization adapters (e.g., IP-Adapter) to guide latents for faithful subject
representation while preserving background integrity. Degradation-suppression
guidance and adaptive background blending are proposed to further eliminate
low-quality outputs and visible seams. To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces. Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).
Code and benchmark will be publicly available upon publication.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供这篇题为“Does FLUX Already Know How to Perform Physically Plausible Image Composition?”的论文的全面摘要。</p>
<hr />
<h3 id="does-flux-already-know-how-to-perform-physically-plausible-image-composition_1">论文摘要：Does FLUX Already Know How to Perform Physically Plausible Image Composition?</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
图像合成旨在将用户指定的对象无缝地插入新场景。然而，现有模型在处理复杂光照（如准确的阴影、水面反射）和多样化高分辨率输入时面临挑战。尽管现代文本到图像扩散模型（如SD3.5、FLUX）已经编码了基本的物理和分辨率先验知识，但它们缺乏一个框架来释放这些能力，往往需要依赖潜在反演（导致对象姿态不当）或脆弱的注意力操作。本研究旨在解决如何实现物理上合理、高保真且无缝的图像合成，同时避免这些现有方法的局限性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
作者提出了一个名为 <strong>SHINE</strong> 的免训练框架，用于实现无缝、高保真且错误中和的图像插入。SHINE框架包含三项主要创新：
*   <strong>流形引导锚定（Manifold-Steered Anchor, MSA）损失：</strong> 利用预训练的开放域定制适配器（如IP-Adapter）来引导潜在表示，使其忠实地再现参考主体，同时保持背景的结构完整性。
*   <strong>降级抑制引导（Degradation-Suppression Guidance, DSG）：</strong> 通过模糊查询图像（Q_img）来构建负速度，引导采样远离低质量分布，从而消除输出中的低质量伪影，如过饱和颜色和身份不一致。
*   <strong>自适应背景混合（Adaptive Background Blending, ABB）：</strong> 引入了一种语义引导的掩码（通过二值化交叉注意力图获得），取代了传统的刚性用户掩码，以消除掩码边界处的可见接缝，实现更平滑的过渡和场景连贯性。</p>
<p>此外，为了解决现有基准的不足，作者还引入了 <strong>ComplexCompo</strong>，这是一个包含多样化分辨率和挑战性条件（如低光照、强光照、复杂阴影和反射表面）的新基准。</p>
<p><strong>3. 主要结果及其意义：</strong>
SHINE在ComplexCompo和DreamEditBench基准上取得了最先进的性能。在标准指标（如DINOv2）和与人类偏好对齐的指标（如DreamSim、ImageReward、VisionReward）上，SHINE均超越了现有基线。特别是，SHINE在复杂场景（如低光照、水面反射和复杂阴影）中表现出色，能够自然地合成对象，而现有方法（如AnyDoor）往往会复制粘贴主体，导致不自然的合成和较低的图像质量分数。消融研究进一步证实了MSA损失、DSG和ABB对提高主体身份一致性、图像质量和消除接缝的有效性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>颜色继承问题：</strong> 尽管SHINE通过MSA优化能够可靠地收敛到正确的主体身份，但如果图像修复提示指定了不正确的颜色，最终的合成结果可能会继承并保留这种错误的颜色。
*   <strong>定制适配器质量依赖：</strong> 插入对象与用户提供对象之间的相似性取决于所用定制适配器的质量。虽然LoRA在测试时针对个体概念进行微调，可以生成与目标更相似的主体，但预训练的开放域定制适配器在某些情况下可能表现出较低的身份一致性指标。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进颜色一致性：</strong> 解决图像修复提示中颜色错误导致合成结果颜色继承的问题，可能需要更鲁棒的颜色校正或更智能的提示理解机制。
*   <strong>增强开放域定制适配器：</strong> 随着开放域定制适配器领域的进步，SHINE方法的潜力将继续提高，未来的研究可以探索如何进一步提升这些适配器的性能，以实现更高质量的身份保留。
*   <strong>扩展到更复杂的交互：</strong> 尽管SHINE在复杂光照和表面条件下表现良好，但可以探索如何处理更复杂、多对象之间的交互，例如物理碰撞、遮挡关系等。</p>
<hr />
<p>这篇论文通过提出SHINE框架，为图像合成领域带来了显著的进步，特别是在处理复杂光照和高分辨率输入方面。其免训练的特性和对现有扩散模型的通用适用性，使其成为一个有前景的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs.</li>
<li>We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors.</li>
<li>To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces.</li>
<li>Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21278v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21278v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21268v1'></a></p>
<h2 id="mmr1-enhancing-multimodal-reasoning-with-variance-aware-sampling-and-open-resources"><a href="https://arxiv.org/abs/2509.21268v1">MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</a></h2>
<p><strong>Authors:</strong> Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Lidong Bing, Deli Zhao, Wei Lu, Yu Rong, Aixin Sun, Shijian Lu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large multimodal reasoning models have achieved rapid progress, but their
advancement is constrained by two major limitations: the absence of open,
large-scale, high-quality long chain-of-thought (CoT) data, and the instability
of reinforcement learning (RL) algorithms in post-training. Group Relative
Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone
to gradient vanishing when reward variance is low, which weakens optimization
signals and impairs convergence. This work makes three contributions: (1) We
propose Variance-Aware Sampling (VAS), a data selection strategy guided by
Variance Promotion Score (VPS) that combines outcome variance and trajectory
diversity to promote reward variance and stabilize policy optimization. (2) We
release large-scale, carefully curated resources containing ~1.6M long CoT
cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,
and diversity, along with a fully reproducible end-to-end training codebase.
(3) We open-source a family of multimodal reasoning models in multiple scales,
establishing standardized baselines for the community. Experiments across
mathematical reasoning benchmarks demonstrate the effectiveness of both the
curated data and the proposed VAS. Comprehensive ablation studies and analyses
provide further insight into the contributions of each component. In addition,
we theoretically establish that reward variance lower-bounds the expected
policy gradient magnitude, with VAS serving as a practical mechanism to realize
this guarantee. Our code, data, and checkpoints are available at
https://github.com/LengSicong/MMR1.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Sicong Leng等人撰写的论文“MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
大型多模态推理模型在后训练阶段面临两大限制：缺乏开放、大规模、高质量的长链式思考（CoT）数据，以及强化学习（RL）算法的不稳定性。特别是，用于RL微调的标准框架——群组相对策略优化（GRPO）在奖励方差较低时容易出现梯度消失，从而削弱优化信号并损害收敛。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了三项主要贡献：
*   <strong>方差感知采样（Variance-Aware Sampling, VAS）：</strong> 一种动态数据选择策略，由方差促进分数（Variance Promotion Score, VPS）指导。VPS结合了结果方差和轨迹多样性，旨在提高奖励方差并稳定策略优化。
*   <strong>大规模开放资源：</strong> 发布了精心策划的大规模资源，包括约1.6M条长CoT冷启动数据和约15k条RL问答对。这些数据旨在确保质量、难度和多样性，并附带一个完全可复现的端到端训练代码库。
*   <strong>多模态推理模型开源：</strong> 开源了一系列多尺度多模态推理模型，为社区建立了标准化基线。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   在数学推理基准上的实验证明了所策划数据和提出的VAS策略的有效性。
*   全面的消融研究和分析深入揭示了每个组件的贡献。
*   理论上证明了奖励方差对预期策略梯度幅度的下界，而VAS是实现这一保证的实用机制。
*   VAS提高了收敛性、稳定性和下游性能。OVS和TDS提供了互补的优势：OVS通过平衡结果来增强预期奖励方差，而TDS通过鼓励轨迹多样性来支持更一致的梯度更新。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   尽管VAS缓解了梯度消失问题，但它未能完全解决多模态强化学习中固有的所有训练不稳定性。
*   基于方差的提示分数（VPS）计算会带来额外的开销，尽管可以通过增加更新间隔或选择性更新部分样本来缓解。
*   该方法主要侧重于数据采样；虽然预计它将补充强化学习算法的进步，但对其整合的系统性研究仍有待未来进行。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   将VAS扩展到更广泛的领域。
*   研究VAS与不同奖励设计之间的相互作用。
*   将VAS与更先进的强化学习算法相结合，以进一步提高样本效率和鲁棒性。</p>
<p>该论文通过引入方差感知采样和开放大规模高质量数据资源，为解决多模态推理模型在强化学习训练中的梯度消失和数据稀缺问题提供了新颖且全面的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This work makes three contributions: (1) We
propose Variance-Aware Sampling (VAS), a data selection strategy guided by
Variance Promotion Score (VPS) that combines outcome variance and trajectory
diversity to promote reward variance and stabilize policy optimization.</li>
<li>Comprehensive ablation studies and analyses
provide further insight into the contributions of each component.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21245v1'></a></p>
<h2 id="hunyuan3d-omni-a-unified-framework-for-controllable-generation-of-3d-assets"><a href="https://arxiv.org/abs/2509.21245v1">Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</a></h2>
<p><strong>Authors:</strong> Team Hunyuan3D, :, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in 3D-native generative models have accelerated asset
creation for games, film, and design. However, most methods still rely
primarily on image or text conditioning and lack fine-grained, cross-modal
controls, which limits controllability and practical adoption. To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose. Instead of separate heads for each modality, our model
unifies all signals in a single cross-modal architecture. We train with a
progressive, difficulty-aware sampling strategy that selects one control
modality per example and biases sampling toward harder signals (e.g., skeletal
pose) while downweighting easier ones (e.g., point clouds), encouraging robust
multi-modal fusion and graceful handling of missing inputs. Experiments show
that these additional controls improve generation accuracy, enable
geometry-aware transformations, and increase robustness for production
workflows.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文“Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets”的全面摘要。</p>
<p><strong>论文摘要：Hunyuan3D-Omni: 可控3D资产生成统一框架</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
当前3D原生生成模型在游戏、电影和设计等领域加速了资产创建，但大多数方法主要依赖图像或文本作为条件，缺乏细粒度的跨模态控制，这限制了其可控性和实际应用。论文旨在解决这一问题，即如何实现对3D资产生成更精细、更可控的跨模态控制。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>统一框架 Hunyuan3D-Omni：</strong> 论文提出了一个基于Hunyuan3D 2.1的统一框架，用于细粒度、可控的3D资产生成。
*   <strong>多模态控制信号集成：</strong> 除了图像，Hunyuan3D-Omni还接受点云、体素、包围盒和骨骼姿态先验作为条件信号，从而实现对几何、拓扑和姿态的精确控制。
*   <strong>单一跨模态架构：</strong> 与为每种模态设置独立头部不同，该模型将所有控制信号统一到一个单一的跨模态架构中。
*   <strong>渐进式、难度感知采样策略：</strong> 训练过程中采用了一种创新的采样策略，每个示例选择一种控制模态，并偏向于采样难度更大的信号（例如骨骼姿态），同时降低较简单信号（例如点云）的权重。这鼓励了鲁棒的多模态融合，并能优雅地处理缺失输入。
*   <strong>统一控制编码器：</strong> 设计了一个统一的控制编码器，将点云、体素、包围盒和骨骼等多种额外条件整合到单一生成模型中，以区分这些信号并获取相应的嵌入。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>提高生成精度：</strong> 实验证明，这些额外的控制信号显著提高了生成精度，能够解决原生3D生成中常见的扭曲、扁平化、细节缺失和长宽比不一致等问题。
*   <strong>实现几何感知转换：</strong> 模型能够根据控制信号进行几何形状的调整和变换，例如根据包围盒调整物体比例，或根据骨骼姿态生成准确的角色几何。
*   <strong>增强生产工作流的鲁棒性：</strong> 额外的控制增加了模型在生产环境中的鲁棒性，能够更好地处理复杂和多样的输入条件。
*   <strong>支持角色姿态标准化和生成输出风格化：</strong> 骨骼条件有助于实现角色姿态的标准化，而其他条件则有助于生成具有特定风格的输出。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文的摘要和正文并未明确列出当前工作的具体局限性。然而，从其强调“缺乏细粒度的跨模态控制”是现有方法的问题来看，可以推断出：
*   <strong>现有方法的局限性：</strong> 现有3D生成模型主要依赖图像或文本，难以实现对几何、拓扑和姿态的精确控制。
*   <strong>数据稀疏性挑战：</strong> 论文提到骨骼姿态条件的数据相对较少且更难学习，这暗示了在某些特定控制模态下，数据量和学习难度可能仍然是挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文的摘要和正文并未直接提出未来的研究方向，但从其贡献和解决的问题来看，可以推断出以下潜在方向：
*   <strong>更广泛的控制模态集成：</strong> 探索集成更多类型的细粒度控制信号，以进一步提升3D资产生成的可控性。
*   <strong>动态和交互式控制：</strong> 研究如何实现3D资产的动态和交互式控制，例如实时调整几何或姿态。
*   <strong>效率和可扩展性：</strong> 进一步优化模型的训练和推理效率，使其能够处理更大规模的数据和更复杂的场景。
*   <strong>用户友好型界面：</strong> 开发更直观、用户友好的界面，将这些高级控制功能集成到实际的3D建模工具中。
*   <strong>结合其他生成范式：</strong> 探索将Hunyuan3D-Omni与最新的3D生成范式（如神经辐射场、高斯泼溅等）结合，以实现更高质量、更逼真的3D资产生成。</p>
<p>总而言之，Hunyuan3D-Omni通过引入统一的多模态控制框架，显著提升了3D资产生成的可控性和精度，为计算机视觉和图形学领域在3D内容创作方面带来了重要的进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21245v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21245v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21227v1'></a></p>
<h2 id="evaluating-the-evaluators-metrics-for-compositional-text-to-image-generation"><a href="https://arxiv.org/abs/2509.21227v1">Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Seyed Amir Kasaei, Ali Aghayari, Arash Marioriyad, Niki Sepasian, MohammadAmin Fazli, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Seyed Amir Kasaei等人撰写的论文“Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：评估评估器：组合式文本到图像生成的度量标准</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前文本到图像生成领域中一个核心挑战：如何准确评估生成图像与文本提示之间，特别是组合性对齐（即图像是否真实反映了提示中描述的对象、属性和关系）的程度。研究人员指出，现有评估严重依赖自动化度量标准，但这些标准往往是基于惯例或流行度而非经过人类判断验证的，因此，理解这些度量标准在多大程度上反映了人类偏好至关重要。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>全面评估框架：</strong> 论文对12种广泛使用的组合式文本到图像评估度量标准进行了广泛研究，涵盖了嵌入式、基于VQA和仅图像三大家族。
*   <strong>超越简单相关性分析：</strong> 研究不仅限于简单的相关性分析，还深入探讨了这些度量标准在不同组合性挑战（如颜色、形状、纹理、2D/3D空间关系、非空间关系、复杂提示和数字计数）下的行为表现。
*   <strong>回归分析：</strong> 除了Spearman相关性分析外，论文还进行了回归分析，以人类评分作为目标，所有度量标准输出作为预测因子，揭示了每个度量标准对人类判断的联合贡献。
*   <strong>度量标准分数分布模式分析：</strong> 论文分析了不同度量标准家族的分数分布模式，揭示了它们在区分质量差异和饱和度方面的局限性。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>无单一最佳度量标准：</strong> 研究发现，没有单一的度量标准能在所有组合性任务中始终表现出色，其性能因组合性问题的类型而异。这强调了仅依赖单一信号的不足。
*   <strong>VQA和嵌入式度量标准的表现：</strong> 尽管基于VQA的度量标准（如VQAScore、DA Score）很受欢迎，但并非总是最优的；某些嵌入式度量标准（如ImageReward、HPS）在特定情况下表现更强。例如，DA Score在颜色属性上表现最佳，ImageReward在形状和纹理上表现突出，VQA Score在2D空间关系和复杂提示上表现最佳。
*   <strong>仅图像度量标准的局限性：</strong> 仅图像度量标准（如CLIP-IQA、Aesthetic Score）对组合性评估的贡献很小，因为它们主要关注感知质量而非文本-图像对齐。
*   <strong>度量标准分布问题：</strong> 嵌入式度量标准（如CLIPScore）常产生中等范围分数，难以区分质量差异；而基于VQA的度量标准则倾向于高分饱和，限制了它们区分更强候选者的能力。
*   <strong>综合评估的必要性：</strong> 结果表明，VQA-based方法和embedding-based方法都对评估有贡献，且具有不同的优势，这暗示了结合互补度量标准的重要性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   论文主要基于T2I-CompBench++数据集进行评估，该数据集虽然提供了多样化的组合性挑战，但其范围可能仍有限。
*   人类判断作为黄金标准，其本身也可能存在一定的主观性和变异性。
*   研究主要关注了现有度量标准的评估能力，并未提出全新的度量标准。
*   度量标准在作为生成模型奖励模型时的具体影响，虽然有所提及，但未深入探讨其优化机制。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>开发更鲁棒的组合性评估度量标准：</strong> 鉴于没有单一度量标准能始终表现最佳，未来的研究可以探索结合不同度量标准家族优势的新方法，或开发能更全面捕捉人类偏好的新型度量标准。
*   <strong>改进度量标准的分数分布：</strong> 解决嵌入式度量标准分数范围受限和VQA度量标准分数饱和的问题，以提高其区分能力。
*   <strong>度量标准作为奖励模型的优化：</strong> 深入研究如何更有效地利用这些度量标准作为奖励信号，以指导扩散模型在生成过程中更好地实现组合性对齐。
*   <strong>更广泛的数据集和场景验证：</strong> 在更多样化、更具挑战性的数据集和真实世界场景中验证评估度量标准的有效性。
*   <strong>可解释性评估：</strong> 除了量化对齐程度，未来的工作还可以探索如何使评估度量标准更具可解释性，从而提供关于生成失败原因的洞察。</p>
<hr />
<p>这篇论文通过对现有评估度量标准的深入分析，为文本到图像生成领域的评估实践提供了宝贵的见解。它强调了在选择评估工具时需要谨慎和透明，并为未来开发更有效、更可靠的评估方法指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.</li>
<li>The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21227v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21227v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21086v1'></a></p>
<h2 id="unitransfer-video-concept-transfer-via-progressive-spatial-and-timestep-decomposition"><a href="https://arxiv.org/abs/2509.21086v1">UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</a></h2>
<p><strong>Authors:</strong> Guojun Lei, Rong Zhang, Chi Wang, Tianhang Liu, Hong Li, Zhiyuan Ma, Weiwei Xu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We propose a novel architecture UniTransfer, which introduces both spatial
and diffusion timestep decomposition in a progressive paradigm, achieving
precise and controllable video concept transfer. Specifically, in terms of
spatial decomposition, we decouple videos into three key components: the
foreground subject, the background, and the motion flow. Building upon this
decomposed formulation, we further introduce a dual-to-single-stream DiT-based
architecture for supporting fine-grained control over different components in
the videos. We also introduce a self-supervised pretraining strategy based on
random masking to enhance the decomposed representation learning from
large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning
paradigm, we further revisit the denoising diffusion process and propose a
Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We
decompose the denoising process into three stages of different granularity and
leverage large language models (LLMs) for stage-specific instructions to guide
the generation progressively. We also curate an animal-centric video dataset
called OpenAnimal to facilitate the advancement and benchmarking of research in
video concept transfer. Extensive experiments demonstrate that our method
achieves high-quality and controllable video concept transfer across diverse
reference images and scenes, surpassing existing baselines in both visual
fidelity and editability. Web Page:
https://yu-shaonian.github.io/UniTransfer-Web/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Guojun Lei等人撰写的论文“UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition”的全面摘要。</p>
<hr />
<h3 id="unitransfer">论文摘要：UniTransfer: 通过渐进式空间和时间步分解实现视频概念迁移</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
视频生成模型中的视频概念迁移（Video Concept Transfer, VCT）是一项重要但极具挑战性的任务。现有方法通常将视频作为一个整体进行建模，导致在仅编辑特定区域或概念时灵活性和精确性受限。这使得实现高质量、可控的视频概念迁移，特别是针对视频中特定对象、角色、背景或运动流的精细化操作变得困难。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
UniTransfer 引入了一种新颖的架构，通过渐进式空间和扩散时间步分解来解决上述问题，实现了精确和可控的视频概念迁移。其主要创新包括：</p>
<ul>
<li><strong>渐进式空间分解：</strong> 将视频解耦为三个关键组成部分：前景主体、背景和运动流。这种分解允许模型灵活地适应通用概念迁移任务。<ul>
<li><strong>双流到单流DiT架构：</strong> 基于这种分解，论文进一步引入了一个双流到单流的Diffusion Transformer (DiT) 架构，以支持视频中不同组件的精细控制。</li>
<li><strong>基于随机掩码的自监督预训练策略：</strong> 为了增强从大规模未标注视频数据中学习到的分解表示，论文引入了自监督预训练策略。这使得模型能够在没有精细标注的情况下捕获解耦特征。</li>
</ul>
</li>
<li><strong>渐进式时间步分解（Chain-of-Prompt, CoP）：</strong> 重新审视了去噪扩散过程，并提出了Chain-of-Prompt (CoP) 机制来实现时间步分解。<ul>
<li><strong>三阶段去噪过程：</strong> 将去噪过程分解为粗粒度、中粒度和细粒度三个阶段。</li>
<li><strong>LLM引导的指令：</strong> 利用大型语言模型（LLMs）为每个阶段生成特定指令，逐步引导生成过程，从噪声到详细纹理进行渐进式细化。</li>
</ul>
</li>
<li><strong>OpenAnimal数据集：</strong> 论文还整理了一个以动物为中心的视频数据集OpenAnimal，以促进视频概念迁移研究的进展和基准测试。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
广泛的实验证明，UniTransfer 方法在各种视频概念迁移场景中实现了高质量和可控的视频概念迁移，超越了现有基线在视觉保真度和可编辑性方面的表现。</p>
<ul>
<li><strong>高质量和可控性：</strong> UniTransfer 在角色、服装、背景和运动等多种参考组件的迁移中表现出色，合成的新视频具有卓越的视觉质量和帧间一致性。</li>
<li><strong>超越基线：</strong> 在TikTok和UBC数据集上的定量评估显示，UniTransfer在FID、LPIPS、主体一致性、美学质量等多个指标上均优于现有方法，证明了其在复杂视频角色迁移任务中的鲁棒性和泛化能力。</li>
<li><strong>多功能性：</strong> 该框架支持多种视频概念迁移任务，包括运动迁移、背景迁移、动物迁移和区域前景迁移（如服装替换），展示了其处理多样化转换的灵活性和适应性。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
尽管UniTransfer模型在视频中实现了主体迁移和背景替换，但论文也指出了一些局限性：</p>
<ul>
<li><strong>潜在的伪影：</strong> 在某些情况下，主体和背景可能会出现伪影。</li>
<li><strong>分割模型的限制：</strong> 这种伪影问题可能源于当前的分割模型无法完全分离前景和背景元素，导致不完美的复合结果。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
为了解决现有局限性并进一步提升模型性能，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>利用大规模模型：</strong> 未来计划通过利用大规模模型来增强视频场景理解，进一步提高生成视频的质量。</li>
<li><strong>解决分割挑战：</strong> 改进分割模型，以更精确地分离前景和背景元素，从而减少伪影。</li>
</ul>
<hr />
<p>总而言之，UniTransfer通过其创新的渐进式空间和时间步分解策略，为视频概念迁移领域带来了显著的进步，实现了前所未有的精细控制和高质量输出。尽管存在一些局限性，但其强大的性能和多功能性为未来的视频生成和编辑研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a novel architecture UniTransfer, which introduces both spatial
and diffusion timestep decomposition in a progressive paradigm, achieving
precise and controllable video concept transfer.</li>
<li>Extensive experiments demonstrate that our method
achieves high-quality and controllable video concept transfer across diverse
reference images and scenes, surpassing existing baselines in both visual
fidelity and editability.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21086v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21086v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21055v1'></a></p>
<h2 id="background-prompt-for-few-shot-out-of-distribution-detection"><a href="https://arxiv.org/abs/2509.21055v1">Background Prompt for Few-Shot Out-of-Distribution Detection</a></h2>
<p><strong>Authors:</strong> Songyue Cai, Zongqian Wu, Yujie Mo, Liang Peng, Ping Hu, Xiaoshuang Shi, Xiaofeng Zhu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing foreground-background (FG-BG) decomposition methods for the few-shot
out-of-distribution (FS-OOD) detection often suffer from low robustness due to
over-reliance on the local class similarity and a fixed background patch
extraction strategy. To address these challenges, we propose a new FG-BG
decomposition framework, namely Mambo, for FS-OOD detection. Specifically, we
propose to first learn a background prompt to obtain the local background
similarity containing both the background and image semantic information, and
then refine the local background similarity using the local class similarity.
As a result, we use both the refined local background similarity and the local
class similarity to conduct background extraction, reducing the dependence of
the local class similarity in previous methods. Furthermore, we propose the
patch self-calibrated tuning to consider the sample diversity to flexibly
select numbers of background patches for different samples, and thus exploring
the issue of fixed background extraction strategies in previous methods.
Extensive experiments on real-world datasets demonstrate that our proposed
Mambo achieves the best performance, compared to SOTA methods in terms of OOD
detection and near OOD detection setting. The source code will be released at
https://github.com/YuzunoKawori/Mambo.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Songyue Cai等人撰写的论文“Background Prompt for Few-Shot Out-of-Distribution Detection”的全面摘要。</p>
<hr />
<p><strong>论文摘要：基于背景提示的少样本分布外检测</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
现有的少样本分布外检测（FS-OOD）中的前景-背景（FG-BG）分解方法存在两个主要问题：
a) <strong>鲁棒性低：</strong> 过度依赖局部类别相似性，导致在局部类别相似性不准确时性能下降。
b) <strong>背景补丁提取策略固定：</strong> 采用固定的背景补丁提取策略，忽略了样本多样性，容易引入错误的背景信息。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述挑战，作者提出了一个名为 <strong>Mambo</strong> 的新型FG-BG分解框架，其核心创新包括：
a) <strong>背景提示学习（Background Prompt Learning）：</strong> 首次引入背景提示（background prompt），通过CLIP的文本编码器学习背景文本特征。这使得模型能够独立捕获背景语义信息，从而获得更鲁棒的局部背景相似性，减轻了对局部类别相似性的过度依赖。
b) <strong>局部相似性细化（Local Similarity Refinement）：</strong> 提出了一种机制，利用局部类别相似性中的前景语义信息来细化局部背景相似性。通过自适应地结合基于真实类别预测概率的局部类别相似性，提高了背景提取的准确性。
c) <strong>补丁自校准调整（Patch Self-Calibrated Tuning）：</strong> 引入了一种动态调整背景补丁数量的策略。根据真实类别的预测概率，灵活地选择不同样本的背景补丁数量，避免了固定策略的局限性，减少了错误背景信息的引入。
d) <strong>综合利用两种相似性：</strong> Mambo同时利用细化后的局部背景相似性和局部类别相似性进行背景提取，降低了对单一局部类别相似性的依赖。</p>
<p><strong>3. 主要结果及其意义：</strong>
a) <strong>卓越的OOD检测性能：</strong> 在ImageNet-1K和ImageNet-100等真实世界数据集上的广泛实验表明，Mambo在OOD检测和近OOD检测设置下均优于现有的最先进方法（SOTA）。
b) <strong>鲁棒性和灵活性：</strong> 实验结果验证了Mambo在不同数据集和不同OOD检测类型下均能保持最佳性能，证明了其方法的鲁棒性和灵活性。
c) <strong>组件有效性：</strong> 消融研究证实了局部相似性细化和补丁自校准调整这两个核心组件的有效性，它们共同促进了OOD检测性能的提升。
d) <strong>效率：</strong> 尽管引入了新组件，Mambo在参数数量与现有方法相当的情况下，仍能实现更好的性能，并且在训练时间和GPU内存消耗方面表现出较高的效率。</p>
<p><strong>4. 论文中提及的局限性：</strong>
a) <strong>挑战性样本：</strong> 论文指出，在某些挑战性样本（例如，目标特征与背景区域高度相似或缺乏独特特征的样本）面前，Mambo仍可能表现出次优性能。背景提示有时会错误地将OOD特征识别为背景。
b) <strong>跨领域OOD检测：</strong> 尽管Mambo在2D自然图像分类任务的FS-OOD检测中表现出色，但现有方法仍无法同时实现跨领域的FS-OOD检测（例如自动驾驶和医学成像）。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
a) <strong>增强模型处理挑战性数据的能力：</strong> 探索区分不同类型特征的方法，以提高模型在挑战性数据上的性能。
b) <strong>扩展到开放世界场景：</strong> 将Mambo应用于自动驾驶和医学成像等开放世界场景，以解决这些领域中对FS-OOD检测的迫切需求。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose a new FG-BG
decomposition framework, namely Mambo, for FS-OOD detection.</li>
<li>Furthermore, we propose the
patch self-calibrated tuning to consider the sample diversity to flexibly
select numbers of background patches for different samples, and thus exploring
the issue of fixed background extraction strategies in previous methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21055v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21055v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21027v1'></a></p>
<h2 id="keyworld-key-frame-reasoning-enables-effective-and-efficient-world-models"><a href="https://arxiv.org/abs/2509.21027v1">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a></h2>
<p><strong>Authors:</strong> Sibo Li, Qianyue Hao, Yu Shang, Yong Li</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Robotic world models are a promising paradigm for forecasting future
environment states, yet their inference speed and the physical plausibility of
generated trajectories remain critical bottlenecks, limiting their real-world
applications. This stems from the redundancy of the prevailing frame-to-frame
generation approach, where the model conducts costly computation on similar
frames, as well as neglecting the semantic importance of key transitions. To
address this inefficiency, we propose KeyWorld, a framework that improves
text-conditioned robotic world models by concentrating transformers computation
on a few semantic key frames while employing a lightweight convolutional model
to fill the intermediate frames. Specifically, KeyWorld first identifies
significant transitions by iteratively simplifying the robot's motion
trajectories, obtaining the ground truth key frames. Then, a DiT model is
trained to reason and generate these physically meaningful key frames from
textual task descriptions. Finally, a lightweight interpolator efficiently
reconstructs the full video by inpainting all intermediate frames. Evaluations
on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68<script type="math/tex">\times</script>
acceleration compared to the frame-to-frame generation baseline, and focusing
on the motion-aware key frames further contributes to the physical validity of
the generated videos, especially on complex tasks. Our approach highlights a
practical path toward deploying world models in real-time robotic control and
other domains requiring both efficient and effective world models. Code is
released at https://anonymous.4open.science/r/Keyworld-E43D.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Sibo Li, Qianyue Hao, Yu Shang, Yong Li撰写的论文“KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models”的全面摘要。</p>
<hr />
<h3 id="keyworld">KeyWorld: 关键帧推理实现高效有效的世界模型</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决机器人世界模型在预测未来环境状态时面临的两个关键瓶颈：推理速度慢和生成轨迹的物理合理性不足。现有的逐帧生成方法存在计算冗余（对相似帧进行昂贵计算）和忽略关键语义转换的问题，这限制了世界模型在实际机器人应用中的部署。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
KeyWorld 框架通过以下创新点解决了上述问题：
*   <strong>高效模块化框架：</strong> KeyWorld 将文本条件机器人世界模型的推理过程解耦为基于扩散的关键帧生成和轻量级中间帧插值，显著降低了视频生成成本并增强了关键帧的语义理解。
*   <strong>运动感知关键帧检测：</strong> 论文引入了一种运动感知关键帧检测范式。它利用 Ramer-Douglas-Peucker (RDP) 算法从机器人姿态轨迹中识别语义上重要的状态转换点作为关键帧。这种方法确保了选定的关键帧与有意义的物理转换对齐，为模型提供了更清晰的物理动力学表示。
*   <strong>两阶段生成模型：</strong>
    *   <strong>关键帧生成：</strong> 训练一个 Diffusion Transformer (DiT) 模型（基于 CogVideoX），从文本任务描述和初始状态推理并生成这些物理上有意义的关键帧。通过在运动感知关键帧上进行微调，模型能够专注于生成语义关键的锚点，从而减少计算负担并增强对关键物理交互的关注。
    *   <strong>中间帧插值：</strong> 使用一个轻量级卷积神经网络 (CNN) 模型（基于 FILM）作为插值器，通过预测帧间隙并生成中间帧来高效地重建完整的视频序列。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的效率提升：</strong> 在 LIBERO 基准测试上，KeyWorld 实现了相比逐帧生成基线 <strong>5.68 倍的加速</strong>。关键帧生成占据了绝大部分计算成本（超过 90%），而帧插值模块的开销可忽略不计。
*   <strong>更高的物理合理性：</strong> 专注于运动感知关键帧显著提高了生成视频的物理有效性，尤其是在复杂任务中。模型能够更准确地识别目标对象并生成与真实情况高度相似的机器人运动。
*   <strong>优越的视频质量：</strong> KeyWorld 在 PSNR、SSIM 和对象级准确性等多个指标上保持了优越的视频质量。特别是在对象级准确性方面，KeyWorld 在 LIBERO-goal 和 LIBERO-object 等任务上表现出显著提升。
*   <strong>复杂任务中的优势：</strong> 论文分析表明，运动感知关键帧带来的性能提升在轨迹复杂度较高的场景中最为显著，这表明该方法在需要精确建模重要状态转换的复杂任务中特别有效。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   论文未明确提及具体的局限性，但从其方法学和实验设计中可以推断出一些潜在的方面：
    *   <strong>RDP 算法的阈值选择：</strong> RDP 算法中的阈值 <script type="math/tex">\epsilon</script> 需要通过二分搜索来控制关键帧的数量，这可能需要一定的调优。
    *   <strong>模型泛化能力：</strong> 尽管在 LIBERO 基准测试上表现良好，但 KeyWorld 在更广泛、更多样化的机器人任务或真实世界场景中的泛化能力仍需进一步验证。
    *   <strong>关键帧密度对性能的影响：</strong> 论文指出最佳关键帧密度可能因任务而异，这意味着在不同应用中可能需要针对性地选择关键帧比例。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>自适应关键帧选择：</strong> 探索更智能、自适应的关键帧选择机制，能够根据任务的动态特性和复杂性自动调整关键帧密度，而不仅仅是依赖固定的比例或手动调优。
*   <strong>更复杂的插值模型：</strong> 尽管目前轻量级插值器表现良好，但对于极端复杂或快速变化的中间运动，可能需要研究更先进的插值技术，以进一步提高物理一致性和视觉质量。
*   <strong>多模态关键帧：</strong> 结合除了机器人姿态之外的其他模态信息（如力、触觉、声音等）来识别关键帧，可能能捕捉到更丰富、更细致的语义转换。
*   <strong>实时部署的进一步优化：</strong> 尽管 KeyWorld 实现了显著加速，但进一步优化模型架构和推理流程，以满足更严格的实时机器人控制需求，仍是一个重要的研究方向。
*   <strong>与其他世界模型的集成：</strong> 探索 KeyWorld 框架与不同类型的世界模型（例如，基于状态空间模型或强化学习的世界模型）的集成，以期结合各自优势，实现更强大、更通用的机器人智能。</p>
<hr />
<p>总而言之，KeyWorld 论文提出了一种新颖且高效的机器人世界模型框架，通过将关键帧推理与轻量级插值相结合，显著提升了生成效率和物理合理性。其运动感知关键帧检测和两阶段生成策略为实时机器人控制及其他需要高效有效世界模型的领域开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
address this inefficiency, we propose KeyWorld, a framework that improves
text-conditioned robotic world models by concentrating transformers computation
on a few semantic key frames while employing a lightweight convolutional model
to fill the intermediate frames.</li>
<li>Our approach highlights a
practical path toward deploying world models in real-time robotic control and
other domains requiring both efficient and effective world models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21027v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21027v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-26 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
