<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-26 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-24/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-29/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-26">Arxiv Computer Vision Papers - 2025-09-26</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-models-are-zero-shot-learners-and-reasoners" class="nav-link">Video models are zero-shot learners and reasoners</a>
                </li>
                <li class="nav-item">
                    <a href="#sd35-flash-distribution-guided-distillation-of-generative-flows" class="nav-link">SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</a>
                </li>
                <li class="nav-item">
                    <a href="#quantized-visual-geometry-grounded-transformer" class="nav-link">Quantized Visual Geometry Grounded Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#does-flux-already-know-how-to-perform-physically-plausible-image-composition" class="nav-link">Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a>
                </li>
                <li class="nav-item">
                    <a href="#mmr1-enhancing-multimodal-reasoning-with-variance-aware-sampling-and-open-resources" class="nav-link">MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</a>
                </li>
                <li class="nav-item">
                    <a href="#hunyuan3d-omni-a-unified-framework-for-controllable-generation-of-3d-assets" class="nav-link">Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</a>
                </li>
                <li class="nav-item">
                    <a href="#evaluating-the-evaluators-metrics-for-compositional-text-to-image-generation" class="nav-link">Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#unitransfer-video-concept-transfer-via-progressive-spatial-and-timestep-decomposition" class="nav-link">UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</a>
                </li>
                <li class="nav-item">
                    <a href="#background-prompt-for-few-shot-out-of-distribution-detection" class="nav-link">Background Prompt for Few-Shot Out-of-Distribution Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#keyworld-key-frame-reasoning-enables-effective-and-efficient-world-models" class="nav-link">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-26">Arxiv Computer Vision Papers - 2025-09-26</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ24æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ§è¡æè¦ (2025-09-24)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæå±ç¤ºäºè®¡ç®æºè§è§é¢åå ä¸ªå³é®æ¹åçæç»­å¿«éåå±ï¼</p>
<ul>
<li><strong>å¤æ¨¡æä¸è§é¢çè§£çæ·±åï¼</strong> è§é¢æ¨¡åä½ä¸ºé¶æ ·æ¬å­¦ä¹ å¨åæ¨çå¨çè½åæ­£å¨è¢«ç§¯ææ¢ç´¢ï¼åæ¶å¤æ¨¡ææ¨ççé²ç´ æ§ä¹å¨éè¿æ°çéæ ·åèµæºæ´åæ¹æ³å¾å°æåã</li>
<li><strong>çææ¨¡åä¸3Dåå®¹çè¿æ­¥ï¼</strong> æ©æ£æ¨¡åï¼ç¹å«æ¯å¶è¸é¦åæçæåï¼ä»¥åå¯æ§ç3Dèµäº§çææ¯æ¾èçç¦ç¹ãå¯¹çææ¨¡åè¾åºçè¯ä¼°ææ ä¹åå°äºå³æ³¨ã</li>
<li><strong>æ¨¡åæçä¸é²æ£æ§ï¼</strong> éåææ¯è¢«åºç¨äºTransformeræ¨¡åä»¥æé«æçï¼åæ¶å¯¹åå¸å¤æ£æµï¼OODï¼åç©ççå®æ§åæçå³æ³¨è¡¨æäºå¯¹æ¨¡åå¨å¤æç°å®ä¸çåºæ¯ä¸­é²æ£æ§çè¿½æ±ã</li>
<li><strong>ä¸çæ¨¡åä¸å·èº«æºè½çäº¤æ±ï¼</strong> å³é®å¸§æ¨çè¢«å¼å¥ä¸çæ¨¡åï¼é¢ç¤ºçå¨å·èº«æºè½åå¼ºåå­¦ä¹ èæ¯ä¸å¯¹æ´ææåé«æç¯å¢å»ºæ¨¡çå´è¶£ã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"Video models are zero-shot learners and reasoners" by ThaddÃ¤us Wiedemer et et al. (1å·è®ºæ):</strong> è¿ç¯è®ºæå¦æè½æåè¯æè§é¢æ¨¡åå¨é¶æ ·æ¬å­¦ä¹ åæ¨çæ¹é¢çå¼ºå¤§è½åï¼å°å¯¹å¤æ¨¡æAIçæªæ¥åå±äº§çæ·±è¿å½±åï¼å¯è½é¢ç¤ºçè§é¢åºç¡æ¨¡åå¨éç¨æºè½æ¹é¢çæ°çªç ´ã</li>
<li><strong>"Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets" by Team Hunyuan3D et al. (6å·è®ºæ):</strong> ç»ä¸çå¯æ§3Dèµäº§çææ¡æ¶æ¯å·¥ä¸çåç ç©¶çé½é«åº¦å³æ³¨çæ¹åãå¦æè¯¥æ¡æ¶è½å®ç°é«è´¨éåé«å¯æ§æ§ï¼å°æå¤§å°æ¨å¨3Dåå®¹åä½åèæç°å®åºç¨ã</li>
<li><strong>"SD3.5-Flash: Distribution-Guided Distillation of Generative Flows" by Hmrishav Bandyopadhyay et al. (2å·è®ºæ):</strong> æ©æ£æ¨¡åçé«ææ§ä¸ç´æ¯å¶åºç¨çå³é®ç¶é¢ãSD3.5-Flashéè¿åå¸å¼å¯¼è¸é¦æ¥æé«çææµçæçï¼è¿å¯¹äºå®æ¶çæåèµæºåéç¯å¢ä¸çé¨ç½²è³å³éè¦ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>è§é¢æ¨¡åä½ä¸ºéç¨æ¨çå¼æï¼</strong> 1å·è®ºæå¼ºè°äºè§é¢æ¨¡åè¶è¶ç®åè¯å«ï¼åæ´é«çº§çé¶æ ·æ¬æ¨çè½ååå±ã</li>
<li><strong>é«æçææ¨¡åè¸é¦ï¼</strong> 2å·è®ºæçâåå¸å¼å¯¼è¸é¦âæ¯æåæ©æ£æ¨¡åæççææéå¾ï¼é¢ç¤ºçæªæ¥æ´å¤å³äºæ¨¡ååç¼©åå éçç ç©¶ã</li>
<li><strong>3Dèµäº§çç»ä¸å¯æ§çæï¼</strong> 6å·è®ºæçâç»ä¸æ¡æ¶âè¡¨æç ç©¶æ­£ä»åä¸3Dçæä»»å¡åæ´å¨é¢ãå¯æ§ç3Dåå®¹åä½å¹³å°åå±ã</li>
<li><strong>ä¸çæ¨¡åä¸­çå³é®å¸§æ¨çï¼</strong> 10å·è®ºæå°âå³é®å¸§æ¨çâå¼å¥ä¸çæ¨¡åï¼è¿æ¯ä¸ç§å¨å·èº«æºè½åå¼ºåå­¦ä¹ ä¸­æé«ç¯å¢å»ºæ¨¡æçåæææ§çæ°ç­ç¥ã</li>
<li><strong>èæ¯æç¤ºï¼Background Promptï¼ç¨äºOODæ£æµï¼</strong> 9å·è®ºææåºäºä¸ç§æ°é¢çå°æ ·æ¬OODæ£æµæ¹æ³ï¼å©ç¨èæ¯ä¿¡æ¯æ¥å¢å¼ºæ¨¡åçæ³åè½åã</li>
</ul>
<p><strong>4. å»ºè®®å®æ´éè¯»çè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºå³æ³¨åºç¡æ¨¡ååéç¨AIçç ç©¶äººåï¼</strong><ul>
<li><strong>1. "Video models are zero-shot learners and reasoners"</strong></li>
<li><strong>5. "MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources"</strong></li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨çææ¨¡åã3Dåå®¹åæççç ç©¶äººåï¼</strong><ul>
<li><strong>2. "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows"</strong></li>
<li><strong>6. "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets"</strong></li>
<li><strong>7. "Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation"</strong> (å¯¹äºè¯ä¼°çææ¨¡åè¾åºè³å³éè¦)</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨æ¨¡åé²æ£æ§ãæçåå·èº«æºè½çç ç©¶äººåï¼</strong><ul>
<li><strong>3. "Quantized Visual Geometry Grounded Transformer"</strong></li>
<li><strong>9. "Background Prompt for Few-Shot Out-of-Distribution Detection"</strong></li>
<li><strong>10. "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models"</strong></li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæãå»ºè®®æ ¹æ®æ¨çå·ä½å´è¶£è¿ä¸æ­¥æ·±å¥éè¯»ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.20328v1">Video models are zero-shot learners and reasoners</a></li>
<li><a href="#2509.21318v1">SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</a></li>
<li><a href="#2509.21302v1">Quantized Visual Geometry Grounded Transformer</a></li>
<li><a href="#2509.21278v1">Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a></li>
<li><a href="#2509.21268v1">MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</a></li>
<li><a href="#2509.21245v1">Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</a></li>
<li><a href="#2509.21227v1">Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</a></li>
<li><a href="#2509.21086v1">UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</a></li>
<li><a href="#2509.21055v1">Background Prompt for Few-Shot Out-of-Distribution Detection</a></li>
<li><a href="#2509.21027v1">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.20328v1'></a></p>
<h2 id="video-models-are-zero-shot-learners-and-reasoners"><a href="https://arxiv.org/abs/2509.20328v1">Video models are zero-shot learners and reasoners</a></h2>
<p><strong>Authors:</strong> ThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos</p>
<p><strong>Published:</strong> 2025-09-24</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The remarkable zero-shot capabilities of Large Language Models (LLMs) have
propelled natural language processing from task-specific models to unified,
generalist foundation models. This transformation emerged from simple
primitives: large, generative models trained on web-scale data. Curiously, the
same primitives apply to today's generative video models. Could video models be
on a trajectory towards general-purpose vision understanding, much like LLMs
developed general-purpose language understanding? We demonstrate that Veo 3 can
solve a broad variety of tasks it wasn't explicitly trained for: segmenting
objects, detecting edges, editing images, understanding physical properties,
recognizing object affordances, simulating tool use, and more. These abilities
to perceive, model, and manipulate the visual world enable early forms of
visual reasoning like maze and symmetry solving. Veo's emergent zero-shot
capabilities indicate that video models are on a path to becoming unified,
generalist vision foundation models.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæãè§é¢æ¨¡åæ¯é¶æ ·æ¬å­¦ä¹ èåæ¨çèãæ¢è®¨äºè§é¢æ¨¡åå¨éç¨è§è§çè§£æ¹é¢æ¯å¦è½åå¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨è¯­è¨çè§£æ¹é¢ä¸æ ·ï¼åå±åºé¶æ ·æ¬è½åã</p>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è®ºæçæ ¸å¿é®é¢æ¯ï¼è§é¢æ¨¡åè½å¦åLLMsä¸æ ·ï¼éè¿å¤§è§æ¨¡çæå¼è®­ç»åç½ç»è§æ¨¡æ°æ®ï¼åå±åºéç¨çè§è§çè§£è½åï¼å¹¶å±ç°åºé¶æ ·æ¬å­¦ä¹ åæ¨çè½åï¼ä»èæä¸ºç»ä¸çãéç¨åè§è§åºç¡æ¨¡åï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>é¶æ ·æ¬è½åæ¼ç¤ºï¼</strong> è®ºæéè¿å¯¹Veo 3æ¨¡åè¿è¡å¹¿æ³çå®æ§ï¼62é¡¹ä»»å¡ï¼åå®éï¼7é¡¹ä»»å¡ï¼è¯ä¼°ï¼å±ç¤ºäºå¶å¨æªç»æç¡®è®­ç»çä»»å¡ä¸è§£å³é®é¢çè½åãè¿äºä»»å¡æ¶µçäºæç¥ï¼å¦å¾ååå²ãè¾¹ç¼æ£æµï¼ãå»ºæ¨¡ï¼å¦çè§£ç©çå±æ§ï¼ãæä½ï¼å¦å¾åç¼è¾ãå·¥å·ä½¿ç¨æ¨¡æï¼åæ¨çï¼å¦è¿·å®«åå¯¹ç§°æ§æ±è§£ï¼ç­è§è§å æ çåä¸ªå±é¢ã
*   <strong>âå¸§é¾ï¼Chain-of-Framesï¼âè§è§æ¨çæ¦å¿µï¼</strong> è®ºææåºï¼è§é¢æ¨¡åéè¿éå¸§çæè§é¢æ¥æ¨¡æLLMsçâæç»´é¾ï¼Chain-of-Thoughtï¼âæ¨çè¿ç¨ï¼ä»èå¨æ¶é´åç©ºé´ä¸è¿è¡è§è§æ¨çã
*   <strong>æ§è½æåçè¯æ®ï¼</strong> è®ºæéè¿æ¯è¾Veo 3ä¸å¶åèº«Veo 2çæ§è½ï¼å±ç¤ºäºè§é¢æ¨¡åè½åçå¿«éè¿æ­¥ï¼è¡¨æå¶æ­£æçéç¨è§è§åºç¡æ¨¡ååå±ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å¹¿æ³çé¶æ ·æ¬è½åï¼</strong> Veo 3å¨æç¥ãå»ºæ¨¡åæä½è§è§ä¸çæ¹é¢å±ç°åºæ¾èçé¶æ ·æ¬è½åï¼è½å¤æ§è¡å¯¹è±¡åå²ãè¾¹ç¼æ£æµãå¾åç¼è¾ãçè§£ç©çå±æ§ãè¯å«å¯¹è±¡åè½ãæ¨¡æå·¥å·ä½¿ç¨ç­å¤ç§ä»»å¡ã
*   <strong>æ©æè§è§æ¨çè½åï¼</strong> æ¨¡åè½å¤è¿è¡è¿·å®«æ±è§£åå¯¹ç§°æ§æ±è§£ç­æ©æå½¢å¼çè§è§æ¨çï¼è¿è¡¨æè§é¢æ¨¡åä¸ä»è½å¤çéæå¾åï¼è¿è½çè§£å¨æåºæ¯å¹¶è¿è¡åºåå³ç­ã
*   <strong>åéç¨è§è§åºç¡æ¨¡åè¿è¿ï¼</strong> Veo 3çè¿äºæ°å´é¶æ ·æ¬è½åé¢ç¤ºçè§é¢æ¨¡åæææä¸ºç»ä¸çãéç¨åè§è§åºç¡æ¨¡åï¼å°±åLLMså¨èªç¶è¯­è¨å¤çé¢åæåçé£æ ·ï¼åä»£è®¸å¤ä»»å¡ä¸ç¨æ¨¡åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ§è½ä»ä½äºä»»å¡ä¸ç¨æ¨¡åï¼</strong> å°½ç®¡Veo 3è¡¨ç°åºè²ï¼ä½å¨è®¸å¤ä»»å¡ä¸ï¼å¶æ§è½ä»ä½äºä¸é¨ä¸ºè¿äºä»»å¡è®­ç»çå®å¶æ¨¡åãè¿ä¸LLMsæ©æçæåµç±»ä¼¼ã
*   <strong>è§é¢çæææ¬é«æï¼</strong> ç®åçæè§é¢çææ¬é«äºè¿è¡ä»»å¡ä¸ç¨æ¨¡åï¼ä½è®ºæè®¤ä¸ºéçææ¯åå±ï¼ææ¬ä¼è¿éä¸éã
*   <strong>å¯¹æç¤ºçæææ§ï¼</strong> æ¨¡åçæ§è½å¯¹æç¤ºçç²¾ç¡®æè¿°é«åº¦ææï¼éè¦ç²¾å¿è®¾è®¡çæç¤ºæè½è·å¾æä½³ç»æã
*   <strong>æ¨¡ååå·®ï¼</strong> å¨æäºä»»å¡ï¼å¦è§è§ç±»æ¯çåå°åæè½¬ï¼ä¸­ï¼æ¨¡åå­å¨ç³»ç»æ§çéè¯¯åå·®ã
*   <strong>ç©çæ¨¡æçå±éæ§ï¼</strong> å¨æäºç©çæ¨çä»»å¡ä¸­ï¼æ¨¡åå¯è½è¿åç©çå®å¾æäº§çä¸åå®éçè¿å¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æåé¶æ ·æ¬æ§è½ï¼</strong> è¿ä¸æ­¥æé«è§é¢æ¨¡åå¨åç§ä»»å¡ä¸çé¶æ ·æ¬æ§è½ï¼ä½¿å¶è½å¤ä¸ä»»å¡ä¸ç¨æ¨¡åç¸åª²ç¾ã
*   <strong>æ¨çåè§åï¼</strong> æ¢ç´¢æ´é«çº§çè§è§æ¨çåè§åè½åï¼ä¾å¦éè¿æ´å¤æçâå¸§é¾âæºå¶æ¥è§£å³å¤æ­¥éª¤é®é¢ã
*   <strong>ææ¬ä¼åï¼</strong> éä½è§é¢çæåæ¨ççè®¡ç®ææ¬ï¼ä½¿å¶æ´å·å®ç¨æ§ã
*   <strong>æç¤ºå·¥ç¨ï¼</strong> åå±æ´å¼ºå¤§çè§è§æç¤ºå·¥ç¨ææ¯ï¼ä»¥æ´å¥½å°æå¯¼è§é¢æ¨¡åæ§è¡å¤æä»»å¡ã
*   <strong>æ¶é¤æ¨¡ååå·®ï¼</strong> è§£å³æ¨¡åå¨æäºä»»å¡ä¸­å­å¨çç³»ç»æ§åå·®ï¼æé«å¶æ³åè½åååç¡®æ§ã
*   <strong>å¤æ¨¡æèåï¼</strong> è¿ä¸æ­¥æ¢ç´¢è§é¢æ¨¡åä¸è¯­è¨æ¨¡åç­å¶ä»æ¨¡æçèåï¼ä»¥å®ç°æ´å¨é¢çéç¨æºè½ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¯¹Veo 3çå¹¿æ³è¯ä¼°ï¼æåå°è®ºè¯äºè§é¢æ¨¡åå¨é¶æ ·æ¬å­¦ä¹ åè§è§æ¨çæ¹é¢çå·¨å¤§æ½åï¼å¹¶å°å¶å®ä½ä¸ºæªæ¥éç¨è§è§åºç¡æ¨¡åçéè¦åå±æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that Veo 3 can
solve a broad variety of tasks it wasn't explicitly trained for: segmenting
objects, detecting edges, editing images, understanding physical properties,
recognizing object affordances, simulating tool use, and more.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.20328v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.20328v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21318v1'></a></p>
<h2 id="sd35-flash-distribution-guided-distillation-of-generative-flows"><a href="https://arxiv.org/abs/2509.21318v1">SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</a></h2>
<p><strong>Authors:</strong> Hmrishav Bandyopadhyay, Rahim Entezari, Jim Scott, Reshinth Adithyan, Yi-Zhe Song, Varun Jampani</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices. Our
approach distills computationally prohibitive rectified flow models through a
reformulated distribution matching objective tailored specifically for few-step
generation. We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.
Combined with comprehensive pipeline optimizations like text encoder
restructuring and specialized quantization, our system enables both rapid
generation and memory-efficient deployment across different hardware
configurations. This democratizes access across the full spectrum of devices,
from mobile phones to desktop computers. Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâSD3.5-Flash: Distribution-Guided Distillation of Generative Flowsâçå¨é¢æè¦ï¼ç±Hmrishav Bandyopadhyayç­äººæ°åï¼</p>
<p><strong>è®ºææè¦ï¼SD3.5-Flash: åå¸å¼å¯¼ççææµè¸é¦</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åæåè¿çå¾åçææ¨¡åï¼ç¹å«æ¯åºäºä¿®æ­£æµçæ¨¡åï¼è®¡ç®ææ¬è¿é«çé®é¢ï¼è¿äºæ¨¡åéå¸¸éè¦å¤§éè®¡ç®èµæºï¼å¦25+æ­¥æ¨çã16GB+VRAMã30+ç§/å¾åï¼ï¼å¯¼è´å®ä»¬æ æ³å¨æ®éæ¶è´¹çº§è®¾å¤ï¼å¦ææºãæ¡é¢çµèï¼ä¸é«æè¿è¡ãæ ¸å¿ç ç©¶é®é¢æ¯å¦ä½éè¿é«æçå°æ­¥è¸é¦æ¡æ¶ï¼å°é«è´¨éå¾åçæè½åå¸¦å°æ´å¹¿æ³ãæ´æè®¿é®çæ¶è´¹çº§è®¾å¤ä¸ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
SD3.5-Flashå¼å¥äºä»¥ä¸å³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼</p>
<ul>
<li><strong>éæ°å¶å®çåå¸å¹éç®æ ï¼Reformulated Distribution Matching Objectiveï¼ï¼</strong> éå¯¹å°æ­¥çæåºæ¯ï¼ä½èéæ°è®¾è®¡äºåå¸å¹éç®æ ï¼ä»¥è¸é¦è®¡ç®ææ¬é«æçä¿®æ­£æµæ¨¡åã</li>
<li><strong>æ¶é´æ­¥å±äº«ï¼Timestep Sharingï¼ï¼</strong> ä¸ºäºåå°æ¢¯åº¦åªå£°å¹¶æé«è®­ç»ç¨³å®æ§ï¼è¯¥æ¹æ³å¨è®¡ç®åå¸å¹éæ¶ï¼ä½¿ç¨å­¦çæ¨¡åè½¨è¿¹ä¸çæ ·æ¬èééæºè½¨è¿¹ç¹è¿è¡ä¼°è®¡ãè¿ç¡®ä¿äºå¨å·²ç¥åªå£°æ°´å¹³ä¸ç¨³å®çæ¢¯åº¦ä¿¡å·åå¯é çODEè½¨è¿¹æµé¢æµã</li>
<li><strong>åæ­¥æ¶é´æ­¥å¾®è°ï¼Split-Timestep Fine-Tuningï¼ï¼</strong> ä¸ºè§£å³å°æ­¥è¸é¦ä¸­æ¨¡åå®¹éä¸å¾åè´¨éï¼ç¹å«æ¯æç¤ºå¯¹é½ï¼ä¹é´çæè¡¡é®é¢ï¼ä½èå¨è®­ç»æé´ææ¶æ©å±æ¨¡åå®¹éãéè¿å°é¢è®­ç»æ¨¡åå¤å¶å°ä¸åçåæ¯ï¼å¹¶å¨ä¸ç¸äº¤çæ¶é´æ­¥èå´åè¿è¡è®­ç»ï¼ç¶åå°å®ä»¬åå¹¶ä¸ºä¸ä¸ªç»ä¸çæ£æ¥ç¹ï¼ä»¥æé«æç¤ºå¯¹é½åè¯­ä¹ä¿çåº¦ã</li>
<li><strong>å¨é¢çç®¡éä¼åï¼Comprehensive Pipeline Optimizationsï¼ï¼</strong> åæ¬ææ¬ç¼ç å¨éæï¼å©ç¨ç¼ç å¨dropouté¢è®­ç»æ¿ä»£T5-XXLï¼åä¸é¨çéåæ¹æ¡ï¼ä»16ä½å°6ä½ç²¾åº¦ï¼ï¼ä»¥å¹³è¡¡åå­å ç¨åæ¨çéåº¦ï¼å®ç°å¿«éçæååå­é«æé¨ç½²ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
SD3.5-Flashéè¿å¹¿æ³çè¯ä¼°ï¼åæ¬å¤§è§æ¨¡ç¨æ·ç ç©¶ï¼å±ç¤ºäºæ¾èçææï¼</p>
<ul>
<li><strong>æ§è½è¶è¶ç°æå°æ­¥æ¹æ³ï¼</strong> SD3.5-Flashå¨å¾åè´¨éåæç¤ºå¯¹é½æ¹é¢æç»­ä¼äºç°æçå°æ­¥çææ¹æ³ï¼çè³å¨æäºææ ä¸è¶è¶äºå¤æ­¥æå¸æ¨¡åã</li>
<li><strong>é«æé¨ç½²ï¼</strong> ç»åç®¡éä¼åï¼è¯¥ç³»ç»å®ç°äºå¿«éçæååå­é«æé¨ç½²ï¼ä½¿å¶è½å¤å¨åç§ç¡¬ä»¶éç½®ï¼ä»ç§»å¨è®¾å¤å°æ¡é¢çµèï¼ä¸è¿è¡ï¼ä»èçæ­£æ®åäºåè¿ççæå¼AIã</li>
<li><strong>é«è´¨éå¾åçæï¼</strong> å³ä½¿å¨å°æ­¥æ¨çï¼å¦4æ­¥ï¼ä¸ï¼æ¨¡åä¹è½çæé«ä¿çå¾åï¼å¹¶å±ç°åºåè¶çæç¤ºéµå¾ªæ§åæå¾çè§£è½åï¼å°¤å¶å¨å¤çè§£åç»æåå¤å¯¹è±¡æå¾ç­ä¼ ç»è¸é¦æ¹æ³é¾ä»¥å¤ççåºæ¯ä¸­è¡¨ç°åºè²ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æå°äºä»¥ä¸å±éæ§ï¼</p>
<ul>
<li><strong>è´¨éä¸å¤æ ·æ§çæè¡¡ï¼</strong> åææè¸é¦è¿ç¨ä¸æ ·ï¼SD3.5-Flashå¨å¤æçæä»»å¡ä¸­ï¼éè¦å¨è´¨éåå¤æ ·æ§æ¹é¢ååºä¸å®çæè¡¡ã</li>
<li><strong>T5-XXLç§»é¤çå½±åï¼</strong> ä¸ºäºå®ç°æ´å¿«çæ¨çéåº¦åæ´ä½çåå­å ç¨èç§»é¤T5-XXLææ¬ç¼ç å¨ï¼å¯è½ä¼å¯¼è´æ¨¡åå¨æå»ºå¤ææå¾æ¶é¢ä¸´ææï¼å ä¸ºæ¡ä»¶ä¸ä¸æçè´¨éä¼ä¸éã</li>
<li><strong>å°æ­¥æ¨¡åçåºæå±éæ§ï¼</strong> è¿äºå±éæ§å¹¶éSD3.5-Flashç¬æï¼èæ¯å°æ­¥æ¨¡åè¿ä¼¼æ©æ£è½¨è¿¹çèªç¶ç»æã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶å·¥ä½åå®¹å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼</p>
<ul>
<li><strong>è¿ä¸æ­¥ä¼åå°æ­¥è¸é¦ï¼</strong> æ¢ç´¢æ´åè¿çè¸é¦ææ¯ï¼ä»¥å¨æ´å°çæ¨çæ­¥æ°ä¸è¿ä¸æ­¥ç¼©å°ä¸å¤æ­¥æå¸æ¨¡åä¹é´çæ§è½å·®è·ï¼å°¤å¶æ¯å¨å¤ææå¾åè¯­ä¹çè§£æ¹é¢ã</li>
<li><strong>æ´å¹¿æ³çç¡¬ä»¶ééï¼</strong> è¿ä¸æ­¥ä¼åæ¨¡ååç®¡éï¼ä»¥æ¯ææ´å¹¿æ³çè¾¹ç¼è®¾å¤åä½åèå¹³å°ï¼å®ç°æ´å¹¿æ³çæ®åã</li>
<li><strong>å¨æéåºæ§è¸é¦ï¼</strong> ç ç©¶å¦ä½æ ¹æ®ä¸åçè¾å¥æç¤ºæç¨æ·éæ±ï¼å¨æè°æ´è¸é¦ç­ç¥ææ¨¡åéç½®ï¼ä»¥å¨è´¨éåéåº¦ä¹é´åå¾æä½³å¹³è¡¡ã</li>
<li><strong>ç»åå¶ä»çæèå¼ï¼</strong> æ¢ç´¢å°ä¿®æ­£æµè¸é¦ä¸å¶ä»çæèå¼ï¼å¦GANsæVAEï¼ç»åï¼ä»¥è¿ä¸æ­¥æåçæè´¨éææçã</li>
<li><strong>æåFIDææ ï¼</strong> å°½ç®¡å¨å¶ä»ææ ä¸è¡¨ç°åºè²ï¼ä½FIDææ ä»ææåç©ºé´ï¼è¿å¯è½éè¦æ´æ·±å¥å°ç ç©¶çææ ·æ¬ä¸çå®å¾ååå¸çå¹éã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices.</li>
<li>We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.</li>
<li>Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21318v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21318v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21302v1'></a></p>
<h2 id="quantized-visual-geometry-grounded-transformer"><a href="https://arxiv.org/abs/2509.21302v1">Quantized Visual Geometry Grounded Transformer</a></h2>
<p><strong>Authors:</strong> Weilun Feng, Haotong Qin, Mingqiang Wu, Chuanguang Yang, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs), have made remarkable progress with the use of
large-scale transformers. Their prohibitive computational and memory costs
severely hinder real-world deployment. Post-Training Quantization (PTQ) has
become a common practice for compressing and accelerating models. However, we
empirically observe that PTQ faces unique obstacles when compressing
billion-scale VGGTs: the data-independent special tokens induce heavy-tailed
activation distributions, while the multi-view nature of 3D data makes
calibration sample selection highly unstable. This paper proposes the first
Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly. Second, we design Noise-Filtered Diverse Sampling, which
filters outliers via deep-layer statistics and constructs frame-aware diverse
calibration clusters to ensure stable quantization ranges. Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin. We highlight
that our 4-bit QuantVGGT can deliver a 3.7<script type="math/tex">\times</script> memory reduction and
2.5<script type="math/tex">\times</script> acceleration in real-hardware inference, while maintaining
reconstruction accuracy above 98\% of its full-precision counterpart. This
demonstrates the vast advantages and practicality of QuantVGGT in
resource-constrained scenarios. Our code is released in
https://github.com/wlfeng0509/QuantVGGT.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Weilun Fengç­äººæ°åçè®ºæâQuantized Visual Geometry Grounded Transformerâçå¨é¢æè¦ã</p>
<hr />
<h3 id="quantized-visual-geometry-grounded-transformer_1">è®ºææè¦ï¼Quantized Visual Geometry Grounded Transformer</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åºäºå­¦ä¹ ç3Déå»ºæ¨¡åï¼ç¹å«æ¯è§è§å ä½åºç¡Transformer (VGGT) å¨å®éé¨ç½²ä¸­é¢ä¸´çè®¡ç®ååå­ææ¬è¿é«çé®é¢ãå°½ç®¡VGGTå¨3Dä»»å¡ä¸­åå¾äºæ¾èè¿å±ï¼ä½å¶æ°åäº¿åæ°çè§æ¨¡ä¸¥éé»ç¢äºå¶å¨èµæºåéç¯å¢ä¸­çåºç¨ãç°æçè®­ç»åéåï¼PTQï¼æ¹æ³å¨åç¼©VGGTè¿ç±»å¤§è§æ¨¡æ¨¡åæ¶é¢ä¸´ç¬ç¹ææï¼æ°æ®ç¬ç«çç¹æ®tokenï¼å¦ç¸æºåæ³¨åtokenï¼å¯¼è´æ¿æ´»åå¸åéå°¾ï¼heavy-tailedï¼ï¼ä»¥å3Dæ°æ®çå¤è§è§ç¹æ§ä½¿å¾æ ¡åæ ·æ¬éæ©é«åº¦ä¸ç¨³å®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
ä¸ºäºåºå¯¹ä¸è¿°ææï¼è®ºææåºäºé¦ä¸ªä¸é¨éå¯¹VGGTçéåæ¡æ¶ââQuantVGGTï¼å¶ä¸»è¦è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>åå¹³æ»ç»ç²åº¦éåï¼Dual-Smoothed Fine-Grained Quantization, DSFQï¼ï¼</strong> è¿ç§æ¹æ³ç»åäºé¢å¨å±Hadamardæè½¬ååå±é¨ééå¹³æ»ãHadamardæè½¬ç¨äºåæ£å¼å¸¸å¼å¹¶å¹³æ»éå°¾åå¸ï¼èå±é¨ééå¹³æ»åå¨æè½¬ç©ºé´ä¸­æ ååééçº§æ¹å·®ï¼ä»èé²æ£å°ç¼è§£äºéå°¾åå¸åééé´æ¹å·®é®é¢ï¼æ¾èéä½äºéåè¯¯å·®ã</li>
<li><strong>åªå£°è¿æ»¤å¤æ ·åéæ ·ï¼Noise-Filtered Diverse Sampling, NFDSï¼ï¼</strong> ä¸ºäºåææ ¡åä¸ç¨³å®æ§ï¼è¯¥æ¹æ³éè¿æ·±åº¦å±ç»è®¡è¿æ»¤å¼å¸¸å¼ï¼å¹¶æå»ºå¸§æç¥çå¤æ ·åæ ¡åç°ãè¿ç¡®ä¿äºæ ¡åæ ·æ¬éå·æä»£è¡¨æ§åç¨³å®æ§ï¼ä»èä¿è¯äºç¨³å®çéåèå´ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
ç»¼åå®éªç»æè¡¨æï¼QuantVGGTå¨ä¸ååºååä½å®½ä¸ååå¾äºæåè¿çæ§è½ï¼æ¾èè¶è¶äºç°æéç¨çéåæ¹æ³ã</p>
<ul>
<li><strong>æ§è½ä¿æï¼</strong> å¨W4A4ï¼4æ¯ç¹æéå4æ¯ç¹æ¿æ´»ï¼è®¾ç½®ä¸ï¼QuantVGGTå¨Co3Dv2æ°æ®éä¸çç¸æºå§¿æä¼°è®¡ä»»å¡ä¸­ï¼å¶éå»ºç²¾åº¦ä»è½ä¿æå¨å¨ç²¾åº¦å¯¹åºæ¨¡åç98%ä»¥ä¸ã</li>
<li><strong>æçæåï¼</strong> 4æ¯ç¹çQuantVGGTå¨çå®ç¡¬ä»¶æ¨çä¸­å®ç°äº3.7åçåå­åå°å2.5åçå éã</li>
<li><strong>æ³åè½åï¼</strong> å³ä½¿å¨W8A8ï¼8æ¯ç¹æéå8æ¯ç¹æ¿æ´»ï¼è®¾ç½®ä¸ï¼QuantVGGTå¨DTUæ°æ®éä¸çç¹äºå¾ä¼°è®¡ä»»å¡ä¸­ä¹è¡¨ç°åºè¯å¥½çæ³åè½åï¼çè³å¨æäºææ ä¸è¶è¶äºå¨ç²¾åº¦æ¨¡åã</li>
</ul>
<p>è¿äºç»æååè¯æäºQuantVGGTå¨èµæºåéåºæ¯ä¸çå·¨å¤§ä¼å¿åå®ç¨æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸»è¦å³æ³¨PTQæ¹æ³ï¼å¹¶æªæ·±å¥æ¢è®¨éåæç¥è®­ç»ï¼QATï¼çæ½åï¼å°½ç®¡QATéå¸¸è½å¨æä½ä½å®½ä¸æä¾æ´å¥½çæ§è½ï¼ä½å¶éè¦å¤§éçè®­ç»èµæºãæ­¤å¤ï¼è®ºæä¸»è¦éå¯¹VGGTæ¨¡åï¼å¶æåºçç¹æ®tokenåå¤è§è§æ°æ®ç¹æ§æ¯VGGTç¬æçï¼å æ­¤QuantVGGTçæäºç»ä»¶å¯è½ä¸ç´æ¥éç¨äºå¶ä»ç±»åçæ¨¡åãå°½ç®¡è®ºæå±ç¤ºäºå¨W4A4è®¾ç½®ä¸çä¼å¼æ§è½ï¼ä½æ´ä½ä½å®½ï¼å¦2æ¯ç¹ï¼çéåå¯è½ä»éè¿ä¸æ­¥æ¢ç´¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæå¹¶æªæç¡®æåºæªæ¥ç ç©¶æ¹åï¼ä½ä»å¶åå®¹åå½åé¢åè¶å¿å¯ä»¥æ¨æ­åºä»¥ä¸å ç¹ï¼</p>
<ul>
<li><strong>æ´ä½ä½å®½éåï¼</strong> æ¢ç´¢å¦ä½å°QuantVGGTæ©å±å°æ´ä½ä½å®½ï¼å¦2æ¯ç¹ï¼ï¼åæ¶ä¿æçè³æé«éå»ºç²¾åº¦ã</li>
<li><strong>è·¨æ¨¡åæ³åï¼</strong> ç ç©¶QuantVGGTä¸­çæ ¸å¿ææ³ï¼å¦åå¹³æ»ååªå£°è¿æ»¤éæ ·ï¼å¦ä½æ³åå°å¶ä»å¤§è§æ¨¡3Déå»ºæ¨¡åææ´å¹¿æ³çTransformeræ¶æã</li>
<li><strong>ç¡¬ä»¶ååè®¾è®¡ï¼</strong> è¿ä¸æ­¥ä¼åQuantVGGTä»¥æ´å¥½å°å©ç¨ç¹å®ç¡¬ä»¶å éå¨çç¹æ§ï¼å®ç°æ´å¤§çæ¨çéåº¦åè½ææåã</li>
<li><strong>å¨æéåç­ç¥ï¼</strong> è®ºææå°äºå¨ætoken-wiseéåå¸¦æ¥çæ§è½æåï¼æªæ¥å¯ä»¥æ·±å¥ç ç©¶æ´å¤æçå¨æéåç­ç¥ï¼ä»¥éåºä¸ååºæ¯åæ°æ®åå¸ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æ¸æ°ãç®æ´å°ä¼ è¾¾è®ºæçæ ¸å¿è´¡ç®ååç°ï¼å¹¶çªåºå¶å¨è®¡ç®æºè§è§é¢åçéè¦æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly.</li>
<li>Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21302v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21302v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21278v1'></a></p>
<h2 id="does-flux-already-know-how-to-perform-physically-plausible-image-composition"><a href="https://arxiv.org/abs/2509.21278v1">Does FLUX Already Know How to Perform Physically Plausible Image Composition?</a></h2>
<p><strong>Authors:</strong> Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs. Modern
text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential
physical and resolution priors, yet lack a framework to unleash them without
resorting to latent inversion, which often locks object poses into contextually
inappropriate orientations, or brittle attention surgery. We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained
customization adapters (e.g., IP-Adapter) to guide latents for faithful subject
representation while preserving background integrity. Degradation-suppression
guidance and adaptive background blending are proposed to further eliminate
low-quality outputs and visible seams. To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces. Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).
Code and benchmark will be publicly available upon publication.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è¿ç¯é¢ä¸ºâDoes FLUX Already Know How to Perform Physically Plausible Image Composition?âçè®ºæçå¨é¢æè¦ã</p>
<hr />
<h3 id="does-flux-already-know-how-to-perform-physically-plausible-image-composition_1">è®ºææè¦ï¼Does FLUX Already Know How to Perform Physically Plausible Image Composition?</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¾ååææ¨å¨å°ç¨æ·æå®çå¯¹è±¡æ ç¼å°æå¥æ°åºæ¯ãç¶èï¼ç°ææ¨¡åå¨å¤çå¤æåç§ï¼å¦åç¡®çé´å½±ãæ°´é¢åå°ï¼åå¤æ ·åé«åè¾¨çè¾å¥æ¶é¢ä¸´ææãå°½ç®¡ç°ä»£ææ¬å°å¾åæ©æ£æ¨¡åï¼å¦SD3.5ãFLUXï¼å·²ç»ç¼ç äºåºæ¬çç©çååè¾¨çåéªç¥è¯ï¼ä½å®ä»¬ç¼ºä¹ä¸ä¸ªæ¡æ¶æ¥éæ¾è¿äºè½åï¼å¾å¾éè¦ä¾èµæ½å¨åæ¼ï¼å¯¼è´å¯¹è±¡å§¿æä¸å½ï¼æèå¼±çæ³¨æåæä½ãæ¬ç ç©¶æ¨å¨è§£å³å¦ä½å®ç°ç©çä¸åçãé«ä¿çä¸æ ç¼çå¾ååæï¼åæ¶é¿åè¿äºç°ææ¹æ³çå±éæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä½èæåºäºä¸ä¸ªåä¸º <strong>SHINE</strong> çåè®­ç»æ¡æ¶ï¼ç¨äºå®ç°æ ç¼ãé«ä¿çä¸éè¯¯ä¸­åçå¾åæå¥ãSHINEæ¡æ¶åå«ä¸é¡¹ä¸»è¦åæ°ï¼
*   <strong>æµå½¢å¼å¯¼éå®ï¼Manifold-Steered Anchor, MSAï¼æå¤±ï¼</strong> å©ç¨é¢è®­ç»çå¼æ¾åå®å¶ééå¨ï¼å¦IP-Adapterï¼æ¥å¼å¯¼æ½å¨è¡¨ç¤ºï¼ä½¿å¶å¿ å®å°åç°åèä¸»ä½ï¼åæ¶ä¿æèæ¯çç»æå®æ´æ§ã
*   <strong>éçº§æå¶å¼å¯¼ï¼Degradation-Suppression Guidance, DSGï¼ï¼</strong> éè¿æ¨¡ç³æ¥è¯¢å¾åï¼Q_imgï¼æ¥æå»ºè´éåº¦ï¼å¼å¯¼éæ ·è¿ç¦»ä½è´¨éåå¸ï¼ä»èæ¶é¤è¾åºä¸­çä½è´¨éä¼ªå½±ï¼å¦è¿é¥±åé¢è²åèº«ä»½ä¸ä¸è´ã
*   <strong>èªéåºèæ¯æ··åï¼Adaptive Background Blending, ABBï¼ï¼</strong> å¼å¥äºä¸ç§è¯­ä¹å¼å¯¼çæ©ç ï¼éè¿äºå¼åäº¤åæ³¨æåå¾è·å¾ï¼ï¼åä»£äºä¼ ç»çåæ§ç¨æ·æ©ç ï¼ä»¥æ¶é¤æ©ç è¾¹çå¤çå¯è§æ¥ç¼ï¼å®ç°æ´å¹³æ»çè¿æ¸¡ååºæ¯è¿è´¯æ§ã</p>
<p>æ­¤å¤ï¼ä¸ºäºè§£å³ç°æåºåçä¸è¶³ï¼ä½èè¿å¼å¥äº <strong>ComplexCompo</strong>ï¼è¿æ¯ä¸ä¸ªåå«å¤æ ·ååè¾¨çåæææ§æ¡ä»¶ï¼å¦ä½åç§ãå¼ºåç§ãå¤æé´å½±ååå°è¡¨é¢ï¼çæ°åºåã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
SHINEå¨ComplexCompoåDreamEditBenchåºåä¸åå¾äºæåè¿çæ§è½ãå¨æ åææ ï¼å¦DINOv2ï¼åä¸äººç±»åå¥½å¯¹é½çææ ï¼å¦DreamSimãImageRewardãVisionRewardï¼ä¸ï¼SHINEåè¶è¶äºç°æåºçº¿ãç¹å«æ¯ï¼SHINEå¨å¤æåºæ¯ï¼å¦ä½åç§ãæ°´é¢åå°åå¤æé´å½±ï¼ä¸­è¡¨ç°åºè²ï¼è½å¤èªç¶å°åæå¯¹è±¡ï¼èç°ææ¹æ³ï¼å¦AnyDoorï¼å¾å¾ä¼å¤å¶ç²è´´ä¸»ä½ï¼å¯¼è´ä¸èªç¶çåæåè¾ä½çå¾åè´¨éåæ°ãæ¶èç ç©¶è¿ä¸æ­¥è¯å®äºMSAæå¤±ãDSGåABBå¯¹æé«ä¸»ä½èº«ä»½ä¸è´æ§ãå¾åè´¨éåæ¶é¤æ¥ç¼çæææ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>é¢è²ç»§æ¿é®é¢ï¼</strong> å°½ç®¡SHINEéè¿MSAä¼åè½å¤å¯é å°æ¶æå°æ­£ç¡®çä¸»ä½èº«ä»½ï¼ä½å¦æå¾åä¿®å¤æç¤ºæå®äºä¸æ­£ç¡®çé¢è²ï¼æç»çåæç»æå¯è½ä¼ç»§æ¿å¹¶ä¿çè¿ç§éè¯¯çé¢è²ã
*   <strong>å®å¶ééå¨è´¨éä¾èµï¼</strong> æå¥å¯¹è±¡ä¸ç¨æ·æä¾å¯¹è±¡ä¹é´çç¸ä¼¼æ§åå³äºæç¨å®å¶ééå¨çè´¨éãè½ç¶LoRAå¨æµè¯æ¶éå¯¹ä¸ªä½æ¦å¿µè¿è¡å¾®è°ï¼å¯ä»¥çæä¸ç®æ æ´ç¸ä¼¼çä¸»ä½ï¼ä½é¢è®­ç»çå¼æ¾åå®å¶ééå¨å¨æäºæåµä¸å¯è½è¡¨ç°åºè¾ä½çèº«ä»½ä¸è´æ§ææ ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¹è¿é¢è²ä¸è´æ§ï¼</strong> è§£å³å¾åä¿®å¤æç¤ºä¸­é¢è²éè¯¯å¯¼è´åæç»æé¢è²ç»§æ¿çé®é¢ï¼å¯è½éè¦æ´é²æ£çé¢è²æ ¡æ­£ææ´æºè½çæç¤ºçè§£æºå¶ã
*   <strong>å¢å¼ºå¼æ¾åå®å¶ééå¨ï¼</strong> éçå¼æ¾åå®å¶ééå¨é¢åçè¿æ­¥ï¼SHINEæ¹æ³çæ½åå°ç»§ç»­æé«ï¼æªæ¥çç ç©¶å¯ä»¥æ¢ç´¢å¦ä½è¿ä¸æ­¥æåè¿äºééå¨çæ§è½ï¼ä»¥å®ç°æ´é«è´¨éçèº«ä»½ä¿çã
*   <strong>æ©å±å°æ´å¤æçäº¤äºï¼</strong> å°½ç®¡SHINEå¨å¤æåç§åè¡¨é¢æ¡ä»¶ä¸è¡¨ç°è¯å¥½ï¼ä½å¯ä»¥æ¢ç´¢å¦ä½å¤çæ´å¤æãå¤å¯¹è±¡ä¹é´çäº¤äºï¼ä¾å¦ç©çç¢°æãé®æ¡å³ç³»ç­ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿æåºSHINEæ¡æ¶ï¼ä¸ºå¾ååæé¢åå¸¦æ¥äºæ¾èçè¿æ­¥ï¼ç¹å«æ¯å¨å¤çå¤æåç§åé«åè¾¨çè¾å¥æ¹é¢ãå¶åè®­ç»çç¹æ§åå¯¹ç°ææ©æ£æ¨¡åçéç¨éç¨æ§ï¼ä½¿å¶æä¸ºä¸ä¸ªæåæ¯çè§£å³æ¹æ¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs.</li>
<li>We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors.</li>
<li>To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces.</li>
<li>Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21278v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21278v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21268v1'></a></p>
<h2 id="mmr1-enhancing-multimodal-reasoning-with-variance-aware-sampling-and-open-resources"><a href="https://arxiv.org/abs/2509.21268v1">MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</a></h2>
<p><strong>Authors:</strong> Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Lidong Bing, Deli Zhao, Wei Lu, Yu Rong, Aixin Sun, Shijian Lu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large multimodal reasoning models have achieved rapid progress, but their
advancement is constrained by two major limitations: the absence of open,
large-scale, high-quality long chain-of-thought (CoT) data, and the instability
of reinforcement learning (RL) algorithms in post-training. Group Relative
Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone
to gradient vanishing when reward variance is low, which weakens optimization
signals and impairs convergence. This work makes three contributions: (1) We
propose Variance-Aware Sampling (VAS), a data selection strategy guided by
Variance Promotion Score (VPS) that combines outcome variance and trajectory
diversity to promote reward variance and stabilize policy optimization. (2) We
release large-scale, carefully curated resources containing ~1.6M long CoT
cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,
and diversity, along with a fully reproducible end-to-end training codebase.
(3) We open-source a family of multimodal reasoning models in multiple scales,
establishing standardized baselines for the community. Experiments across
mathematical reasoning benchmarks demonstrate the effectiveness of both the
curated data and the proposed VAS. Comprehensive ablation studies and analyses
provide further insight into the contributions of each component. In addition,
we theoretically establish that reward variance lower-bounds the expected
policy gradient magnitude, with VAS serving as a practical mechanism to realize
this guarantee. Our code, data, and checkpoints are available at
https://github.com/LengSicong/MMR1.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Sicong Lengç­äººæ°åçè®ºæâMMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resourcesâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¤§åå¤æ¨¡ææ¨çæ¨¡åå¨åè®­ç»é¶æ®µé¢ä¸´ä¸¤å¤§éå¶ï¼ç¼ºä¹å¼æ¾ãå¤§è§æ¨¡ãé«è´¨éçé¿é¾å¼æèï¼CoTï¼æ°æ®ï¼ä»¥åå¼ºåå­¦ä¹ ï¼RLï¼ç®æ³çä¸ç¨³å®æ§ãç¹å«æ¯ï¼ç¨äºRLå¾®è°çæ åæ¡æ¶ââç¾¤ç»ç¸å¯¹ç­ç¥ä¼åï¼GRPOï¼å¨å¥å±æ¹å·®è¾ä½æ¶å®¹æåºç°æ¢¯åº¦æ¶å¤±ï¼ä»èåå¼±ä¼åä¿¡å·å¹¶æå®³æ¶æã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸é¡¹ä¸»è¦è´¡ç®ï¼
*   <strong>æ¹å·®æç¥éæ ·ï¼Variance-Aware Sampling, VASï¼ï¼</strong> ä¸ç§å¨ææ°æ®éæ©ç­ç¥ï¼ç±æ¹å·®ä¿è¿åæ°ï¼Variance Promotion Score, VPSï¼æå¯¼ãVPSç»åäºç»ææ¹å·®åè½¨è¿¹å¤æ ·æ§ï¼æ¨å¨æé«å¥å±æ¹å·®å¹¶ç¨³å®ç­ç¥ä¼åã
*   <strong>å¤§è§æ¨¡å¼æ¾èµæºï¼</strong> åå¸äºç²¾å¿ç­åçå¤§è§æ¨¡èµæºï¼åæ¬çº¦1.6Mæ¡é¿CoTå·å¯å¨æ°æ®åçº¦15kæ¡RLé®ç­å¯¹ãè¿äºæ°æ®æ¨å¨ç¡®ä¿è´¨éãé¾åº¦åå¤æ ·æ§ï¼å¹¶éå¸¦ä¸ä¸ªå®å¨å¯å¤ç°çç«¯å°ç«¯è®­ç»ä»£ç åºã
*   <strong>å¤æ¨¡ææ¨çæ¨¡åå¼æºï¼</strong> å¼æºäºä¸ç³»åå¤å°ºåº¦å¤æ¨¡ææ¨çæ¨¡åï¼ä¸ºç¤¾åºå»ºç«äºæ åååºçº¿ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   å¨æ°å­¦æ¨çåºåä¸çå®éªè¯æäºæç­åæ°æ®åæåºçVASç­ç¥çæææ§ã
*   å¨é¢çæ¶èç ç©¶ååææ·±å¥æ­ç¤ºäºæ¯ä¸ªç»ä»¶çè´¡ç®ã
*   çè®ºä¸è¯æäºå¥å±æ¹å·®å¯¹é¢æç­ç¥æ¢¯åº¦å¹åº¦çä¸çï¼èVASæ¯å®ç°è¿ä¸ä¿è¯çå®ç¨æºå¶ã
*   VASæé«äºæ¶ææ§ãç¨³å®æ§åä¸æ¸¸æ§è½ãOVSåTDSæä¾äºäºè¡¥çä¼å¿ï¼OVSéè¿å¹³è¡¡ç»ææ¥å¢å¼ºé¢æå¥å±æ¹å·®ï¼èTDSéè¿é¼å±è½¨è¿¹å¤æ ·æ§æ¥æ¯ææ´ä¸è´çæ¢¯åº¦æ´æ°ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   å°½ç®¡VASç¼è§£äºæ¢¯åº¦æ¶å¤±é®é¢ï¼ä½å®æªè½å®å¨è§£å³å¤æ¨¡æå¼ºåå­¦ä¹ ä¸­åºæçææè®­ç»ä¸ç¨³å®æ§ã
*   åºäºæ¹å·®çæç¤ºåæ°ï¼VPSï¼è®¡ç®ä¼å¸¦æ¥é¢å¤çå¼éï¼å°½ç®¡å¯ä»¥éè¿å¢å æ´æ°é´éæéæ©æ§æ´æ°é¨åæ ·æ¬æ¥ç¼è§£ã
*   è¯¥æ¹æ³ä¸»è¦ä¾§éäºæ°æ®éæ ·ï¼è½ç¶é¢è®¡å®å°è¡¥åå¼ºåå­¦ä¹ ç®æ³çè¿æ­¥ï¼ä½å¯¹å¶æ´åçç³»ç»æ§ç ç©¶ä»æå¾æªæ¥è¿è¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å°VASæ©å±å°æ´å¹¿æ³çé¢åã
*   ç ç©¶VASä¸ä¸åå¥å±è®¾è®¡ä¹é´çç¸äºä½ç¨ã
*   å°VASä¸æ´åè¿çå¼ºåå­¦ä¹ ç®æ³ç¸ç»åï¼ä»¥è¿ä¸æ­¥æé«æ ·æ¬æçåé²æ£æ§ã</p>
<p>è¯¥è®ºæéè¿å¼å¥æ¹å·®æç¥éæ ·åå¼æ¾å¤§è§æ¨¡é«è´¨éæ°æ®èµæºï¼ä¸ºè§£å³å¤æ¨¡ææ¨çæ¨¡åå¨å¼ºåå­¦ä¹ è®­ç»ä¸­çæ¢¯åº¦æ¶å¤±åæ°æ®ç¨ç¼ºé®é¢æä¾äºæ°é¢ä¸å¨é¢çè§£å³æ¹æ¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This work makes three contributions: (1) We
propose Variance-Aware Sampling (VAS), a data selection strategy guided by
Variance Promotion Score (VPS) that combines outcome variance and trajectory
diversity to promote reward variance and stabilize policy optimization.</li>
<li>Comprehensive ablation studies and analyses
provide further insight into the contributions of each component.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21245v1'></a></p>
<h2 id="hunyuan3d-omni-a-unified-framework-for-controllable-generation-of-3d-assets"><a href="https://arxiv.org/abs/2509.21245v1">Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</a></h2>
<p><strong>Authors:</strong> Team Hunyuan3D, :, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in 3D-native generative models have accelerated asset
creation for games, film, and design. However, most methods still rely
primarily on image or text conditioning and lack fine-grained, cross-modal
controls, which limits controllability and practical adoption. To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose. Instead of separate heads for each modality, our model
unifies all signals in a single cross-modal architecture. We train with a
progressive, difficulty-aware sampling strategy that selects one control
modality per example and biases sampling toward harder signals (e.g., skeletal
pose) while downweighting easier ones (e.g., point clouds), encouraging robust
multi-modal fusion and graceful handling of missing inputs. Experiments show
that these additional controls improve generation accuracy, enable
geometry-aware transformations, and increase robustness for production
workflows.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæâHunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assetsâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼Hunyuan3D-Omni: å¯æ§3Dèµäº§çæç»ä¸æ¡æ¶</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½å3Dåççææ¨¡åå¨æ¸¸æãçµå½±åè®¾è®¡ç­é¢åå éäºèµäº§åå»ºï¼ä½å¤§å¤æ°æ¹æ³ä¸»è¦ä¾èµå¾åæææ¬ä½ä¸ºæ¡ä»¶ï¼ç¼ºä¹ç»ç²åº¦çè·¨æ¨¡ææ§å¶ï¼è¿éå¶äºå¶å¯æ§æ§åå®éåºç¨ãè®ºææ¨å¨è§£å³è¿ä¸é®é¢ï¼å³å¦ä½å®ç°å¯¹3Dèµäº§çææ´ç²¾ç»ãæ´å¯æ§çè·¨æ¨¡ææ§å¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»ä¸æ¡æ¶ Hunyuan3D-Omniï¼</strong> è®ºææåºäºä¸ä¸ªåºäºHunyuan3D 2.1çç»ä¸æ¡æ¶ï¼ç¨äºç»ç²åº¦ãå¯æ§ç3Dèµäº§çæã
*   <strong>å¤æ¨¡ææ§å¶ä¿¡å·éæï¼</strong> é¤äºå¾åï¼Hunyuan3D-Omniè¿æ¥åç¹äºãä½ç´ ãåå´çåéª¨éª¼å§¿æåéªä½ä¸ºæ¡ä»¶ä¿¡å·ï¼ä»èå®ç°å¯¹å ä½ãææåå§¿æçç²¾ç¡®æ§å¶ã
*   <strong>åä¸è·¨æ¨¡ææ¶æï¼</strong> ä¸ä¸ºæ¯ç§æ¨¡æè®¾ç½®ç¬ç«å¤´é¨ä¸åï¼è¯¥æ¨¡åå°æææ§å¶ä¿¡å·ç»ä¸å°ä¸ä¸ªåä¸çè·¨æ¨¡ææ¶æä¸­ã
*   <strong>æ¸è¿å¼ãé¾åº¦æç¥éæ ·ç­ç¥ï¼</strong> è®­ç»è¿ç¨ä¸­éç¨äºä¸ç§åæ°çéæ ·ç­ç¥ï¼æ¯ä¸ªç¤ºä¾éæ©ä¸ç§æ§å¶æ¨¡æï¼å¹¶ååäºéæ ·é¾åº¦æ´å¤§çä¿¡å·ï¼ä¾å¦éª¨éª¼å§¿æï¼ï¼åæ¶éä½è¾ç®åä¿¡å·ï¼ä¾å¦ç¹äºï¼çæéãè¿é¼å±äºé²æ£çå¤æ¨¡æèåï¼å¹¶è½ä¼éå°å¤çç¼ºå¤±è¾å¥ã
*   <strong>ç»ä¸æ§å¶ç¼ç å¨ï¼</strong> è®¾è®¡äºä¸ä¸ªç»ä¸çæ§å¶ç¼ç å¨ï¼å°ç¹äºãä½ç´ ãåå´çåéª¨éª¼ç­å¤ç§é¢å¤æ¡ä»¶æ´åå°åä¸çææ¨¡åä¸­ï¼ä»¥åºåè¿äºä¿¡å·å¹¶è·åç¸åºçåµå¥ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æé«çæç²¾åº¦ï¼</strong> å®éªè¯æï¼è¿äºé¢å¤çæ§å¶ä¿¡å·æ¾èæé«äºçæç²¾åº¦ï¼è½å¤è§£å³åç3Dçæä¸­å¸¸è§çæ­æ²ãæå¹³åãç»èç¼ºå¤±åé¿å®½æ¯ä¸ä¸è´ç­é®é¢ã
*   <strong>å®ç°å ä½æç¥è½¬æ¢ï¼</strong> æ¨¡åè½å¤æ ¹æ®æ§å¶ä¿¡å·è¿è¡å ä½å½¢ç¶çè°æ´ååæ¢ï¼ä¾å¦æ ¹æ®åå´çè°æ´ç©ä½æ¯ä¾ï¼ææ ¹æ®éª¨éª¼å§¿æçæåç¡®çè§è²å ä½ã
*   <strong>å¢å¼ºçäº§å·¥ä½æµçé²æ£æ§ï¼</strong> é¢å¤çæ§å¶å¢å äºæ¨¡åå¨çäº§ç¯å¢ä¸­çé²æ£æ§ï¼è½å¤æ´å¥½å°å¤çå¤æåå¤æ ·çè¾å¥æ¡ä»¶ã
*   <strong>æ¯æè§è²å§¿ææ åååçæè¾åºé£æ ¼åï¼</strong> éª¨éª¼æ¡ä»¶æå©äºå®ç°è§è²å§¿æçæ ååï¼èå¶ä»æ¡ä»¶åæå©äºçæå·æç¹å®é£æ ¼çè¾åºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæçæè¦åæ­£æå¹¶æªæç¡®ååºå½åå·¥ä½çå·ä½å±éæ§ãç¶èï¼ä»å¶å¼ºè°âç¼ºä¹ç»ç²åº¦çè·¨æ¨¡ææ§å¶âæ¯ç°ææ¹æ³çé®é¢æ¥çï¼å¯ä»¥æ¨æ­åºï¼
*   <strong>ç°ææ¹æ³çå±éæ§ï¼</strong> ç°æ3Dçææ¨¡åä¸»è¦ä¾èµå¾åæææ¬ï¼é¾ä»¥å®ç°å¯¹å ä½ãææåå§¿æçç²¾ç¡®æ§å¶ã
*   <strong>æ°æ®ç¨çæ§ææï¼</strong> è®ºææå°éª¨éª¼å§¿ææ¡ä»¶çæ°æ®ç¸å¯¹è¾å°ä¸æ´é¾å­¦ä¹ ï¼è¿æç¤ºäºå¨æäºç¹å®æ§å¶æ¨¡æä¸ï¼æ°æ®éåå­¦ä¹ é¾åº¦å¯è½ä»ç¶æ¯ææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæçæè¦åæ­£æå¹¶æªç´æ¥æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åè§£å³çé®é¢æ¥çï¼å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼
*   <strong>æ´å¹¿æ³çæ§å¶æ¨¡æéæï¼</strong> æ¢ç´¢éææ´å¤ç±»åçç»ç²åº¦æ§å¶ä¿¡å·ï¼ä»¥è¿ä¸æ­¥æå3Dèµäº§çæçå¯æ§æ§ã
*   <strong>å¨æåäº¤äºå¼æ§å¶ï¼</strong> ç ç©¶å¦ä½å®ç°3Dèµäº§çå¨æåäº¤äºå¼æ§å¶ï¼ä¾å¦å®æ¶è°æ´å ä½æå§¿æã
*   <strong>æçåå¯æ©å±æ§ï¼</strong> è¿ä¸æ­¥ä¼åæ¨¡åçè®­ç»åæ¨çæçï¼ä½¿å¶è½å¤å¤çæ´å¤§è§æ¨¡çæ°æ®åæ´å¤æçåºæ¯ã
*   <strong>ç¨æ·åå¥½åçé¢ï¼</strong> å¼åæ´ç´è§ãç¨æ·åå¥½ççé¢ï¼å°è¿äºé«çº§æ§å¶åè½éæå°å®éç3Då»ºæ¨¡å·¥å·ä¸­ã
*   <strong>ç»åå¶ä»çæèå¼ï¼</strong> æ¢ç´¢å°Hunyuan3D-Omniä¸ææ°ç3Dçæèå¼ï¼å¦ç¥ç»è¾å°åºãé«æ¯æ³¼æºç­ï¼ç»åï¼ä»¥å®ç°æ´é«è´¨éãæ´é¼çç3Dèµäº§çæã</p>
<p>æ»èè¨ä¹ï¼Hunyuan3D-Omniéè¿å¼å¥ç»ä¸çå¤æ¨¡ææ§å¶æ¡æ¶ï¼æ¾èæåäº3Dèµäº§çæçå¯æ§æ§åç²¾åº¦ï¼ä¸ºè®¡ç®æºè§è§åå¾å½¢å­¦é¢åå¨3Dåå®¹åä½æ¹é¢å¸¦æ¥äºéè¦çè¿å±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21245v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21245v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21227v1'></a></p>
<h2 id="evaluating-the-evaluators-metrics-for-compositional-text-to-image-generation"><a href="https://arxiv.org/abs/2509.21227v1">Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Seyed Amir Kasaei, Ali Aghayari, Arash Marioriyad, Niki Sepasian, MohammadAmin Fazli, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Seyed Amir Kasaeiç­äººæ°åçè®ºæâEvaluating the Evaluators: Metrics for Compositional Text-to-Image Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼è¯ä¼°è¯ä¼°å¨ï¼ç»åå¼ææ¬å°å¾åçæçåº¦éæ å</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åææ¬å°å¾åçæé¢åä¸­ä¸ä¸ªæ ¸å¿ææï¼å¦ä½åç¡®è¯ä¼°çæå¾åä¸ææ¬æç¤ºä¹é´ï¼ç¹å«æ¯ç»åæ§å¯¹é½ï¼å³å¾åæ¯å¦çå®åæ äºæç¤ºä¸­æè¿°çå¯¹è±¡ãå±æ§åå³ç³»ï¼çç¨åº¦ãç ç©¶äººåæåºï¼ç°æè¯ä¼°ä¸¥éä¾èµèªå¨ååº¦éæ åï¼ä½è¿äºæ åå¾å¾æ¯åºäºæ¯ä¾ææµè¡åº¦èéç»è¿äººç±»å¤æ­éªè¯çï¼å æ­¤ï¼çè§£è¿äºåº¦éæ åå¨å¤å¤§ç¨åº¦ä¸åæ äºäººç±»åå¥½è³å³éè¦ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¨é¢è¯ä¼°æ¡æ¶ï¼</strong> è®ºæå¯¹12ç§å¹¿æ³ä½¿ç¨çç»åå¼ææ¬å°å¾åè¯ä¼°åº¦éæ åè¿è¡äºå¹¿æ³ç ç©¶ï¼æ¶µçäºåµå¥å¼ãåºäºVQAåä»å¾åä¸å¤§å®¶æã
*   <strong>è¶è¶ç®åç¸å³æ§åæï¼</strong> ç ç©¶ä¸ä»éäºç®åçç¸å³æ§åæï¼è¿æ·±å¥æ¢è®¨äºè¿äºåº¦éæ åå¨ä¸åç»åæ§ææï¼å¦é¢è²ãå½¢ç¶ãçº¹çã2D/3Dç©ºé´å³ç³»ãéç©ºé´å³ç³»ãå¤ææç¤ºåæ°å­è®¡æ°ï¼ä¸çè¡ä¸ºè¡¨ç°ã
*   <strong>åå½åæï¼</strong> é¤äºSpearmanç¸å³æ§åæå¤ï¼è®ºæè¿è¿è¡äºåå½åæï¼ä»¥äººç±»è¯åä½ä¸ºç®æ ï¼ææåº¦éæ åè¾åºä½ä¸ºé¢æµå å­ï¼æ­ç¤ºäºæ¯ä¸ªåº¦éæ åå¯¹äººç±»å¤æ­çèåè´¡ç®ã
*   <strong>åº¦éæ ååæ°åå¸æ¨¡å¼åæï¼</strong> è®ºæåæäºä¸ååº¦éæ åå®¶æçåæ°åå¸æ¨¡å¼ï¼æ­ç¤ºäºå®ä»¬å¨åºåè´¨éå·®å¼åé¥±ååº¦æ¹é¢çå±éæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>æ åä¸æä½³åº¦éæ åï¼</strong> ç ç©¶åç°ï¼æ²¡æåä¸çåº¦éæ åè½å¨ææç»åæ§ä»»å¡ä¸­å§ç»è¡¨ç°åºè²ï¼å¶æ§è½å ç»åæ§é®é¢çç±»åèå¼ãè¿å¼ºè°äºä»ä¾èµåä¸ä¿¡å·çä¸è¶³ã
*   <strong>VQAååµå¥å¼åº¦éæ åçè¡¨ç°ï¼</strong> å°½ç®¡åºäºVQAçåº¦éæ åï¼å¦VQAScoreãDA Scoreï¼å¾åæ¬¢è¿ï¼ä½å¹¶éæ»æ¯æä¼çï¼æäºåµå¥å¼åº¦éæ åï¼å¦ImageRewardãHPSï¼å¨ç¹å®æåµä¸è¡¨ç°æ´å¼ºãä¾å¦ï¼DA Scoreå¨é¢è²å±æ§ä¸è¡¨ç°æä½³ï¼ImageRewardå¨å½¢ç¶åçº¹çä¸è¡¨ç°çªåºï¼VQA Scoreå¨2Dç©ºé´å³ç³»åå¤ææç¤ºä¸è¡¨ç°æä½³ã
*   <strong>ä»å¾ååº¦éæ åçå±éæ§ï¼</strong> ä»å¾ååº¦éæ åï¼å¦CLIP-IQAãAesthetic Scoreï¼å¯¹ç»åæ§è¯ä¼°çè´¡ç®å¾å°ï¼å ä¸ºå®ä»¬ä¸»è¦å³æ³¨æç¥è´¨éèéææ¬-å¾åå¯¹é½ã
*   <strong>åº¦éæ ååå¸é®é¢ï¼</strong> åµå¥å¼åº¦éæ åï¼å¦CLIPScoreï¼å¸¸äº§çä¸­ç­èå´åæ°ï¼é¾ä»¥åºåè´¨éå·®å¼ï¼èåºäºVQAçåº¦éæ ååå¾åäºé«åé¥±åï¼éå¶äºå®ä»¬åºåæ´å¼ºåéèçè½åã
*   <strong>ç»¼åè¯ä¼°çå¿è¦æ§ï¼</strong> ç»æè¡¨æï¼VQA-basedæ¹æ³åembedding-basedæ¹æ³é½å¯¹è¯ä¼°æè´¡ç®ï¼ä¸å·æä¸åçä¼å¿ï¼è¿æç¤ºäºç»åäºè¡¥åº¦éæ åçéè¦æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   è®ºæä¸»è¦åºäºT2I-CompBench++æ°æ®éè¿è¡è¯ä¼°ï¼è¯¥æ°æ®éè½ç¶æä¾äºå¤æ ·åçç»åæ§ææï¼ä½å¶èå´å¯è½ä»æéã
*   äººç±»å¤æ­ä½ä¸ºé»éæ åï¼å¶æ¬èº«ä¹å¯è½å­å¨ä¸å®çä¸»è§æ§ååå¼æ§ã
*   ç ç©¶ä¸»è¦å³æ³¨äºç°æåº¦éæ åçè¯ä¼°è½åï¼å¹¶æªæåºå¨æ°çåº¦éæ åã
*   åº¦éæ åå¨ä½ä¸ºçææ¨¡åå¥å±æ¨¡åæ¶çå·ä½å½±åï¼è½ç¶æææåï¼ä½æªæ·±å¥æ¢è®¨å¶ä¼åæºå¶ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¼åæ´é²æ£çç»åæ§è¯ä¼°åº¦éæ åï¼</strong> é´äºæ²¡æåä¸åº¦éæ åè½å§ç»è¡¨ç°æä½³ï¼æªæ¥çç ç©¶å¯ä»¥æ¢ç´¢ç»åä¸ååº¦éæ åå®¶æä¼å¿çæ°æ¹æ³ï¼æå¼åè½æ´å¨é¢ææäººç±»åå¥½çæ°ååº¦éæ åã
*   <strong>æ¹è¿åº¦éæ åçåæ°åå¸ï¼</strong> è§£å³åµå¥å¼åº¦éæ ååæ°èå´åéåVQAåº¦éæ ååæ°é¥±åçé®é¢ï¼ä»¥æé«å¶åºåè½åã
*   <strong>åº¦éæ åä½ä¸ºå¥å±æ¨¡åçä¼åï¼</strong> æ·±å¥ç ç©¶å¦ä½æ´ææå°å©ç¨è¿äºåº¦éæ åä½ä¸ºå¥å±ä¿¡å·ï¼ä»¥æå¯¼æ©æ£æ¨¡åå¨çæè¿ç¨ä¸­æ´å¥½å°å®ç°ç»åæ§å¯¹é½ã
*   <strong>æ´å¹¿æ³çæ°æ®éååºæ¯éªè¯ï¼</strong> å¨æ´å¤æ ·åãæ´å·æææ§çæ°æ®éåçå®ä¸çåºæ¯ä¸­éªè¯è¯ä¼°åº¦éæ åçæææ§ã
*   <strong>å¯è§£éæ§è¯ä¼°ï¼</strong> é¤äºéåå¯¹é½ç¨åº¦ï¼æªæ¥çå·¥ä½è¿å¯ä»¥æ¢ç´¢å¦ä½ä½¿è¯ä¼°åº¦éæ åæ´å·å¯è§£éæ§ï¼ä»èæä¾å³äºçæå¤±è´¥åå çæ´å¯ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¯¹ç°æè¯ä¼°åº¦éæ åçæ·±å¥åæï¼ä¸ºææ¬å°å¾åçæé¢åçè¯ä¼°å®è·µæä¾äºå®è´µçè§è§£ãå®å¼ºè°äºå¨éæ©è¯ä¼°å·¥å·æ¶éè¦è°¨æåéæï¼å¹¶ä¸ºæªæ¥å¼åæ´ææãæ´å¯é çè¯ä¼°æ¹æ³ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.</li>
<li>The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21227v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21227v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21086v1'></a></p>
<h2 id="unitransfer-video-concept-transfer-via-progressive-spatial-and-timestep-decomposition"><a href="https://arxiv.org/abs/2509.21086v1">UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</a></h2>
<p><strong>Authors:</strong> Guojun Lei, Rong Zhang, Chi Wang, Tianhang Liu, Hong Li, Zhiyuan Ma, Weiwei Xu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We propose a novel architecture UniTransfer, which introduces both spatial
and diffusion timestep decomposition in a progressive paradigm, achieving
precise and controllable video concept transfer. Specifically, in terms of
spatial decomposition, we decouple videos into three key components: the
foreground subject, the background, and the motion flow. Building upon this
decomposed formulation, we further introduce a dual-to-single-stream DiT-based
architecture for supporting fine-grained control over different components in
the videos. We also introduce a self-supervised pretraining strategy based on
random masking to enhance the decomposed representation learning from
large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning
paradigm, we further revisit the denoising diffusion process and propose a
Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We
decompose the denoising process into three stages of different granularity and
leverage large language models (LLMs) for stage-specific instructions to guide
the generation progressively. We also curate an animal-centric video dataset
called OpenAnimal to facilitate the advancement and benchmarking of research in
video concept transfer. Extensive experiments demonstrate that our method
achieves high-quality and controllable video concept transfer across diverse
reference images and scenes, surpassing existing baselines in both visual
fidelity and editability. Web Page:
https://yu-shaonian.github.io/UniTransfer-Web/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Guojun Leiç­äººæ°åçè®ºæâUniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decompositionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="unitransfer">è®ºææè¦ï¼UniTransfer: éè¿æ¸è¿å¼ç©ºé´åæ¶é´æ­¥åè§£å®ç°è§é¢æ¦å¿µè¿ç§»</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è§é¢çææ¨¡åä¸­çè§é¢æ¦å¿µè¿ç§»ï¼Video Concept Transfer, VCTï¼æ¯ä¸é¡¹éè¦ä½æå·æææ§çä»»å¡ãç°ææ¹æ³éå¸¸å°è§é¢ä½ä¸ºä¸ä¸ªæ´ä½è¿è¡å»ºæ¨¡ï¼å¯¼è´å¨ä»ç¼è¾ç¹å®åºåææ¦å¿µæ¶çµæ´»æ§åç²¾ç¡®æ§åéãè¿ä½¿å¾å®ç°é«è´¨éãå¯æ§çè§é¢æ¦å¿µè¿ç§»ï¼ç¹å«æ¯éå¯¹è§é¢ä¸­ç¹å®å¯¹è±¡ãè§è²ãèæ¯æè¿å¨æµçç²¾ç»åæä½åå¾å°é¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
UniTransfer å¼å¥äºä¸ç§æ°é¢çæ¶æï¼éè¿æ¸è¿å¼ç©ºé´åæ©æ£æ¶é´æ­¥åè§£æ¥è§£å³ä¸è¿°é®é¢ï¼å®ç°äºç²¾ç¡®åå¯æ§çè§é¢æ¦å¿µè¿ç§»ãå¶ä¸»è¦åæ°åæ¬ï¼</p>
<ul>
<li><strong>æ¸è¿å¼ç©ºé´åè§£ï¼</strong> å°è§é¢è§£è¦ä¸ºä¸ä¸ªå³é®ç»æé¨åï¼åæ¯ä¸»ä½ãèæ¯åè¿å¨æµãè¿ç§åè§£åè®¸æ¨¡åçµæ´»å°éåºéç¨æ¦å¿µè¿ç§»ä»»å¡ã<ul>
<li><strong>åæµå°åæµDiTæ¶æï¼</strong> åºäºè¿ç§åè§£ï¼è®ºæè¿ä¸æ­¥å¼å¥äºä¸ä¸ªåæµå°åæµçDiffusion Transformer (DiT) æ¶æï¼ä»¥æ¯æè§é¢ä¸­ä¸åç»ä»¶çç²¾ç»æ§å¶ã</li>
<li><strong>åºäºéæºæ©ç çèªçç£é¢è®­ç»ç­ç¥ï¼</strong> ä¸ºäºå¢å¼ºä»å¤§è§æ¨¡æªæ æ³¨è§é¢æ°æ®ä¸­å­¦ä¹ å°çåè§£è¡¨ç¤ºï¼è®ºæå¼å¥äºèªçç£é¢è®­ç»ç­ç¥ãè¿ä½¿å¾æ¨¡åè½å¤å¨æ²¡æç²¾ç»æ æ³¨çæåµä¸æè·è§£è¦ç¹å¾ã</li>
</ul>
</li>
<li><strong>æ¸è¿å¼æ¶é´æ­¥åè§£ï¼Chain-of-Prompt, CoPï¼ï¼</strong> éæ°å®¡è§äºå»åªæ©æ£è¿ç¨ï¼å¹¶æåºäºChain-of-Prompt (CoP) æºå¶æ¥å®ç°æ¶é´æ­¥åè§£ã<ul>
<li><strong>ä¸é¶æ®µå»åªè¿ç¨ï¼</strong> å°å»åªè¿ç¨åè§£ä¸ºç²ç²åº¦ãä¸­ç²åº¦åç»ç²åº¦ä¸ä¸ªé¶æ®µã</li>
<li><strong>LLMå¼å¯¼çæä»¤ï¼</strong> å©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMsï¼ä¸ºæ¯ä¸ªé¶æ®µçæç¹å®æä»¤ï¼éæ­¥å¼å¯¼çæè¿ç¨ï¼ä»åªå£°å°è¯¦ç»çº¹çè¿è¡æ¸è¿å¼ç»åã</li>
</ul>
</li>
<li><strong>OpenAnimalæ°æ®éï¼</strong> è®ºæè¿æ´çäºä¸ä¸ªä»¥å¨ç©ä¸ºä¸­å¿çè§é¢æ°æ®éOpenAnimalï¼ä»¥ä¿è¿è§é¢æ¦å¿µè¿ç§»ç ç©¶çè¿å±ååºåæµè¯ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éªè¯æï¼UniTransfer æ¹æ³å¨åç§è§é¢æ¦å¿µè¿ç§»åºæ¯ä¸­å®ç°äºé«è´¨éåå¯æ§çè§é¢æ¦å¿µè¿ç§»ï¼è¶è¶äºç°æåºçº¿å¨è§è§ä¿çåº¦åå¯ç¼è¾æ§æ¹é¢çè¡¨ç°ã</p>
<ul>
<li><strong>é«è´¨éåå¯æ§æ§ï¼</strong> UniTransfer å¨è§è²ãæè£ãèæ¯åè¿å¨ç­å¤ç§åèç»ä»¶çè¿ç§»ä¸­è¡¨ç°åºè²ï¼åæçæ°è§é¢å·æåè¶çè§è§è´¨éåå¸§é´ä¸è´æ§ã</li>
<li><strong>è¶è¶åºçº¿ï¼</strong> å¨TikTokåUBCæ°æ®éä¸çå®éè¯ä¼°æ¾ç¤ºï¼UniTransferå¨FIDãLPIPSãä¸»ä½ä¸è´æ§ãç¾å­¦è´¨éç­å¤ä¸ªææ ä¸åä¼äºç°ææ¹æ³ï¼è¯æäºå¶å¨å¤æè§é¢è§è²è¿ç§»ä»»å¡ä¸­çé²æ£æ§åæ³åè½åã</li>
<li><strong>å¤åè½æ§ï¼</strong> è¯¥æ¡æ¶æ¯æå¤ç§è§é¢æ¦å¿µè¿ç§»ä»»å¡ï¼åæ¬è¿å¨è¿ç§»ãèæ¯è¿ç§»ãå¨ç©è¿ç§»ååºååæ¯è¿ç§»ï¼å¦æè£æ¿æ¢ï¼ï¼å±ç¤ºäºå¶å¤çå¤æ ·åè½¬æ¢ççµæ´»æ§åéåºæ§ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡UniTransferæ¨¡åå¨è§é¢ä¸­å®ç°äºä¸»ä½è¿ç§»åèæ¯æ¿æ¢ï¼ä½è®ºæä¹æåºäºä¸äºå±éæ§ï¼</p>
<ul>
<li><strong>æ½å¨çä¼ªå½±ï¼</strong> å¨æäºæåµä¸ï¼ä¸»ä½åèæ¯å¯è½ä¼åºç°ä¼ªå½±ã</li>
<li><strong>åå²æ¨¡åçéå¶ï¼</strong> è¿ç§ä¼ªå½±é®é¢å¯è½æºäºå½åçåå²æ¨¡åæ æ³å®å¨åç¦»åæ¯åèæ¯åç´ ï¼å¯¼è´ä¸å®ç¾çå¤åç»æã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä¸ºäºè§£å³ç°æå±éæ§å¹¶è¿ä¸æ­¥æåæ¨¡åæ§è½ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>å©ç¨å¤§è§æ¨¡æ¨¡åï¼</strong> æªæ¥è®¡åéè¿å©ç¨å¤§è§æ¨¡æ¨¡åæ¥å¢å¼ºè§é¢åºæ¯çè§£ï¼è¿ä¸æ­¥æé«çæè§é¢çè´¨éã</li>
<li><strong>è§£å³åå²ææï¼</strong> æ¹è¿åå²æ¨¡åï¼ä»¥æ´ç²¾ç¡®å°åç¦»åæ¯åèæ¯åç´ ï¼ä»èåå°ä¼ªå½±ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼UniTransferéè¿å¶åæ°çæ¸è¿å¼ç©ºé´åæ¶é´æ­¥åè§£ç­ç¥ï¼ä¸ºè§é¢æ¦å¿µè¿ç§»é¢åå¸¦æ¥äºæ¾èçè¿æ­¥ï¼å®ç°äºåææªæçç²¾ç»æ§å¶åé«è´¨éè¾åºãå°½ç®¡å­å¨ä¸äºå±éæ§ï¼ä½å¶å¼ºå¤§çæ§è½åå¤åè½æ§ä¸ºæªæ¥çè§é¢çæåç¼è¾ç ç©¶å¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a novel architecture UniTransfer, which introduces both spatial
and diffusion timestep decomposition in a progressive paradigm, achieving
precise and controllable video concept transfer.</li>
<li>Extensive experiments demonstrate that our method
achieves high-quality and controllable video concept transfer across diverse
reference images and scenes, surpassing existing baselines in both visual
fidelity and editability.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21086v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21086v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21055v1'></a></p>
<h2 id="background-prompt-for-few-shot-out-of-distribution-detection"><a href="https://arxiv.org/abs/2509.21055v1">Background Prompt for Few-Shot Out-of-Distribution Detection</a></h2>
<p><strong>Authors:</strong> Songyue Cai, Zongqian Wu, Yujie Mo, Liang Peng, Ping Hu, Xiaoshuang Shi, Xiaofeng Zhu</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing foreground-background (FG-BG) decomposition methods for the few-shot
out-of-distribution (FS-OOD) detection often suffer from low robustness due to
over-reliance on the local class similarity and a fixed background patch
extraction strategy. To address these challenges, we propose a new FG-BG
decomposition framework, namely Mambo, for FS-OOD detection. Specifically, we
propose to first learn a background prompt to obtain the local background
similarity containing both the background and image semantic information, and
then refine the local background similarity using the local class similarity.
As a result, we use both the refined local background similarity and the local
class similarity to conduct background extraction, reducing the dependence of
the local class similarity in previous methods. Furthermore, we propose the
patch self-calibrated tuning to consider the sample diversity to flexibly
select numbers of background patches for different samples, and thus exploring
the issue of fixed background extraction strategies in previous methods.
Extensive experiments on real-world datasets demonstrate that our proposed
Mambo achieves the best performance, compared to SOTA methods in terms of OOD
detection and near OOD detection setting. The source code will be released at
https://github.com/YuzunoKawori/Mambo.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Songyue Caiç­äººæ°åçè®ºæâBackground Prompt for Few-Shot Out-of-Distribution Detectionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼åºäºèæ¯æç¤ºçå°æ ·æ¬åå¸å¤æ£æµ</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçå°æ ·æ¬åå¸å¤æ£æµï¼FS-OODï¼ä¸­çåæ¯-èæ¯ï¼FG-BGï¼åè§£æ¹æ³å­å¨ä¸¤ä¸ªä¸»è¦é®é¢ï¼
a) <strong>é²æ£æ§ä½ï¼</strong> è¿åº¦ä¾èµå±é¨ç±»å«ç¸ä¼¼æ§ï¼å¯¼è´å¨å±é¨ç±»å«ç¸ä¼¼æ§ä¸åç¡®æ¶æ§è½ä¸éã
b) <strong>èæ¯è¡¥ä¸æåç­ç¥åºå®ï¼</strong> éç¨åºå®çèæ¯è¡¥ä¸æåç­ç¥ï¼å¿½ç¥äºæ ·æ¬å¤æ ·æ§ï¼å®¹æå¼å¥éè¯¯çèæ¯ä¿¡æ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼ä½èæåºäºä¸ä¸ªåä¸º <strong>Mambo</strong> çæ°åFG-BGåè§£æ¡æ¶ï¼å¶æ ¸å¿åæ°åæ¬ï¼
a) <strong>èæ¯æç¤ºå­¦ä¹ ï¼Background Prompt Learningï¼ï¼</strong> é¦æ¬¡å¼å¥èæ¯æç¤ºï¼background promptï¼ï¼éè¿CLIPçææ¬ç¼ç å¨å­¦ä¹ èæ¯ææ¬ç¹å¾ãè¿ä½¿å¾æ¨¡åè½å¤ç¬ç«æè·èæ¯è¯­ä¹ä¿¡æ¯ï¼ä»èè·å¾æ´é²æ£çå±é¨èæ¯ç¸ä¼¼æ§ï¼åè½»äºå¯¹å±é¨ç±»å«ç¸ä¼¼æ§çè¿åº¦ä¾èµã
b) <strong>å±é¨ç¸ä¼¼æ§ç»åï¼Local Similarity Refinementï¼ï¼</strong> æåºäºä¸ç§æºå¶ï¼å©ç¨å±é¨ç±»å«ç¸ä¼¼æ§ä¸­çåæ¯è¯­ä¹ä¿¡æ¯æ¥ç»åå±é¨èæ¯ç¸ä¼¼æ§ãéè¿èªéåºå°ç»ååºäºçå®ç±»å«é¢æµæ¦ççå±é¨ç±»å«ç¸ä¼¼æ§ï¼æé«äºèæ¯æåçåç¡®æ§ã
c) <strong>è¡¥ä¸èªæ ¡åè°æ´ï¼Patch Self-Calibrated Tuningï¼ï¼</strong> å¼å¥äºä¸ç§å¨æè°æ´èæ¯è¡¥ä¸æ°éçç­ç¥ãæ ¹æ®çå®ç±»å«çé¢æµæ¦çï¼çµæ´»å°éæ©ä¸åæ ·æ¬çèæ¯è¡¥ä¸æ°éï¼é¿åäºåºå®ç­ç¥çå±éæ§ï¼åå°äºéè¯¯èæ¯ä¿¡æ¯çå¼å¥ã
d) <strong>ç»¼åå©ç¨ä¸¤ç§ç¸ä¼¼æ§ï¼</strong> Mamboåæ¶å©ç¨ç»ååçå±é¨èæ¯ç¸ä¼¼æ§åå±é¨ç±»å«ç¸ä¼¼æ§è¿è¡èæ¯æåï¼éä½äºå¯¹åä¸å±é¨ç±»å«ç¸ä¼¼æ§çä¾èµã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
a) <strong>åè¶çOODæ£æµæ§è½ï¼</strong> å¨ImageNet-1KåImageNet-100ç­çå®ä¸çæ°æ®éä¸çå¹¿æ³å®éªè¡¨æï¼Mamboå¨OODæ£æµåè¿OODæ£æµè®¾ç½®ä¸åä¼äºç°æçæåè¿æ¹æ³ï¼SOTAï¼ã
b) <strong>é²æ£æ§åçµæ´»æ§ï¼</strong> å®éªç»æéªè¯äºMamboå¨ä¸åæ°æ®éåä¸åOODæ£æµç±»åä¸åè½ä¿ææä½³æ§è½ï¼è¯æäºå¶æ¹æ³çé²æ£æ§åçµæ´»æ§ã
c) <strong>ç»ä»¶æææ§ï¼</strong> æ¶èç ç©¶è¯å®äºå±é¨ç¸ä¼¼æ§ç»ååè¡¥ä¸èªæ ¡åè°æ´è¿ä¸¤ä¸ªæ ¸å¿ç»ä»¶çæææ§ï¼å®ä»¬å±åä¿è¿äºOODæ£æµæ§è½çæåã
d) <strong>æçï¼</strong> å°½ç®¡å¼å¥äºæ°ç»ä»¶ï¼Mamboå¨åæ°æ°éä¸ç°ææ¹æ³ç¸å½çæåµä¸ï¼ä»è½å®ç°æ´å¥½çæ§è½ï¼å¹¶ä¸å¨è®­ç»æ¶é´åGPUåå­æ¶èæ¹é¢è¡¨ç°åºè¾é«çæçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
a) <strong>æææ§æ ·æ¬ï¼</strong> è®ºææåºï¼å¨æäºæææ§æ ·æ¬ï¼ä¾å¦ï¼ç®æ ç¹å¾ä¸èæ¯åºåé«åº¦ç¸ä¼¼æç¼ºä¹ç¬ç¹ç¹å¾çæ ·æ¬ï¼é¢åï¼Mamboä»å¯è½è¡¨ç°åºæ¬¡ä¼æ§è½ãèæ¯æç¤ºææ¶ä¼éè¯¯å°å°OODç¹å¾è¯å«ä¸ºèæ¯ã
b) <strong>è·¨é¢åOODæ£æµï¼</strong> å°½ç®¡Mamboå¨2Dèªç¶å¾ååç±»ä»»å¡çFS-OODæ£æµä¸­è¡¨ç°åºè²ï¼ä½ç°ææ¹æ³ä»æ æ³åæ¶å®ç°è·¨é¢åçFS-OODæ£æµï¼ä¾å¦èªå¨é©¾é©¶åå»å­¦æåï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
a) <strong>å¢å¼ºæ¨¡åå¤çæææ§æ°æ®çè½åï¼</strong> æ¢ç´¢åºåä¸åç±»åç¹å¾çæ¹æ³ï¼ä»¥æé«æ¨¡åå¨æææ§æ°æ®ä¸çæ§è½ã
b) <strong>æ©å±å°å¼æ¾ä¸çåºæ¯ï¼</strong> å°Mamboåºç¨äºèªå¨é©¾é©¶åå»å­¦æåç­å¼æ¾ä¸çåºæ¯ï¼ä»¥è§£å³è¿äºé¢åä¸­å¯¹FS-OODæ£æµçè¿«åéæ±ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose a new FG-BG
decomposition framework, namely Mambo, for FS-OOD detection.</li>
<li>Furthermore, we propose the
patch self-calibrated tuning to consider the sample diversity to flexibly
select numbers of background patches for different samples, and thus exploring
the issue of fixed background extraction strategies in previous methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21055v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21055v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.21027v1'></a></p>
<h2 id="keyworld-key-frame-reasoning-enables-effective-and-efficient-world-models"><a href="https://arxiv.org/abs/2509.21027v1">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a></h2>
<p><strong>Authors:</strong> Sibo Li, Qianyue Hao, Yu Shang, Yong Li</p>
<p><strong>Published:</strong> 2025-09-25</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Robotic world models are a promising paradigm for forecasting future
environment states, yet their inference speed and the physical plausibility of
generated trajectories remain critical bottlenecks, limiting their real-world
applications. This stems from the redundancy of the prevailing frame-to-frame
generation approach, where the model conducts costly computation on similar
frames, as well as neglecting the semantic importance of key transitions. To
address this inefficiency, we propose KeyWorld, a framework that improves
text-conditioned robotic world models by concentrating transformers computation
on a few semantic key frames while employing a lightweight convolutional model
to fill the intermediate frames. Specifically, KeyWorld first identifies
significant transitions by iteratively simplifying the robot's motion
trajectories, obtaining the ground truth key frames. Then, a DiT model is
trained to reason and generate these physically meaningful key frames from
textual task descriptions. Finally, a lightweight interpolator efficiently
reconstructs the full video by inpainting all intermediate frames. Evaluations
on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68<script type="math/tex">\times</script>
acceleration compared to the frame-to-frame generation baseline, and focusing
on the motion-aware key frames further contributes to the physical validity of
the generated videos, especially on complex tasks. Our approach highlights a
practical path toward deploying world models in real-time robotic control and
other domains requiring both efficient and effective world models. Code is
released at https://anonymous.4open.science/r/Keyworld-E43D.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Sibo Li, Qianyue Hao, Yu Shang, Yong Liæ°åçè®ºæâKeyWorld: Key Frame Reasoning Enables Effective and Efficient World Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="keyworld">KeyWorld: å³é®å¸§æ¨çå®ç°é«æææçä¸çæ¨¡å</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººä¸çæ¨¡åå¨é¢æµæªæ¥ç¯å¢ç¶ææ¶é¢ä¸´çä¸¤ä¸ªå³é®ç¶é¢ï¼æ¨çéåº¦æ¢åçæè½¨è¿¹çç©çåçæ§ä¸è¶³ãç°æçéå¸§çææ¹æ³å­å¨è®¡ç®åä½ï¼å¯¹ç¸ä¼¼å¸§è¿è¡æè´µè®¡ç®ï¼åå¿½ç¥å³é®è¯­ä¹è½¬æ¢çé®é¢ï¼è¿éå¶äºä¸çæ¨¡åå¨å®éæºå¨äººåºç¨ä¸­çé¨ç½²ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
KeyWorld æ¡æ¶éè¿ä»¥ä¸åæ°ç¹è§£å³äºä¸è¿°é®é¢ï¼
*   <strong>é«ææ¨¡ååæ¡æ¶ï¼</strong> KeyWorld å°ææ¬æ¡ä»¶æºå¨äººä¸çæ¨¡åçæ¨çè¿ç¨è§£è¦ä¸ºåºäºæ©æ£çå³é®å¸§çæåè½»éçº§ä¸­é´å¸§æå¼ï¼æ¾èéä½äºè§é¢çæææ¬å¹¶å¢å¼ºäºå³é®å¸§çè¯­ä¹çè§£ã
*   <strong>è¿å¨æç¥å³é®å¸§æ£æµï¼</strong> è®ºæå¼å¥äºä¸ç§è¿å¨æç¥å³é®å¸§æ£æµèå¼ãå®å©ç¨ Ramer-Douglas-Peucker (RDP) ç®æ³ä»æºå¨äººå§¿æè½¨è¿¹ä¸­è¯å«è¯­ä¹ä¸éè¦çç¶æè½¬æ¢ç¹ä½ä¸ºå³é®å¸§ãè¿ç§æ¹æ³ç¡®ä¿äºéå®çå³é®å¸§ä¸ææä¹çç©çè½¬æ¢å¯¹é½ï¼ä¸ºæ¨¡åæä¾äºæ´æ¸æ°çç©çå¨åå­¦è¡¨ç¤ºã
*   <strong>ä¸¤é¶æ®µçææ¨¡åï¼</strong>
    *   <strong>å³é®å¸§çæï¼</strong> è®­ç»ä¸ä¸ª Diffusion Transformer (DiT) æ¨¡åï¼åºäº CogVideoXï¼ï¼ä»ææ¬ä»»å¡æè¿°ååå§ç¶ææ¨çå¹¶çæè¿äºç©çä¸ææä¹çå³é®å¸§ãéè¿å¨è¿å¨æç¥å³é®å¸§ä¸è¿è¡å¾®è°ï¼æ¨¡åè½å¤ä¸æ³¨äºçæè¯­ä¹å³é®çéç¹ï¼ä»èåå°è®¡ç®è´æå¹¶å¢å¼ºå¯¹å³é®ç©çäº¤äºçå³æ³¨ã
    *   <strong>ä¸­é´å¸§æå¼ï¼</strong> ä½¿ç¨ä¸ä¸ªè½»éçº§å·ç§¯ç¥ç»ç½ç» (CNN) æ¨¡åï¼åºäº FILMï¼ä½ä¸ºæå¼å¨ï¼éè¿é¢æµå¸§é´éå¹¶çæä¸­é´å¸§æ¥é«æå°éå»ºå®æ´çè§é¢åºåã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçæçæåï¼</strong> å¨ LIBERO åºåæµè¯ä¸ï¼KeyWorld å®ç°äºç¸æ¯éå¸§çæåºçº¿ <strong>5.68 åçå é</strong>ãå³é®å¸§çæå æ®äºç»å¤§é¨åè®¡ç®ææ¬ï¼è¶è¿ 90%ï¼ï¼èå¸§æå¼æ¨¡åçå¼éå¯å¿½ç¥ä¸è®¡ã
*   <strong>æ´é«çç©çåçæ§ï¼</strong> ä¸æ³¨äºè¿å¨æç¥å³é®å¸§æ¾èæé«äºçæè§é¢çç©çæææ§ï¼å°¤å¶æ¯å¨å¤æä»»å¡ä¸­ãæ¨¡åè½å¤æ´åç¡®å°è¯å«ç®æ å¯¹è±¡å¹¶çæä¸çå®æåµé«åº¦ç¸ä¼¼çæºå¨äººè¿å¨ã
*   <strong>ä¼è¶çè§é¢è´¨éï¼</strong> KeyWorld å¨ PSNRãSSIM åå¯¹è±¡çº§åç¡®æ§ç­å¤ä¸ªææ ä¸ä¿æäºä¼è¶çè§é¢è´¨éãç¹å«æ¯å¨å¯¹è±¡çº§åç¡®æ§æ¹é¢ï¼KeyWorld å¨ LIBERO-goal å LIBERO-object ç­ä»»å¡ä¸è¡¨ç°åºæ¾èæåã
*   <strong>å¤æä»»å¡ä¸­çä¼å¿ï¼</strong> è®ºæåæè¡¨æï¼è¿å¨æç¥å³é®å¸§å¸¦æ¥çæ§è½æåå¨è½¨è¿¹å¤æåº¦è¾é«çåºæ¯ä¸­æä¸ºæ¾èï¼è¿è¡¨æè¯¥æ¹æ³å¨éè¦ç²¾ç¡®å»ºæ¨¡éè¦ç¶æè½¬æ¢çå¤æä»»å¡ä¸­ç¹å«ææã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   è®ºææªæç¡®æåå·ä½çå±éæ§ï¼ä½ä»å¶æ¹æ³å­¦åå®éªè®¾è®¡ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼
    *   <strong>RDP ç®æ³çéå¼éæ©ï¼</strong> RDP ç®æ³ä¸­çéå¼ <script type="math/tex">\epsilon</script> éè¦éè¿äºåæç´¢æ¥æ§å¶å³é®å¸§çæ°éï¼è¿å¯è½éè¦ä¸å®çè°ä¼ã
    *   <strong>æ¨¡åæ³åè½åï¼</strong> å°½ç®¡å¨ LIBERO åºåæµè¯ä¸è¡¨ç°è¯å¥½ï¼ä½ KeyWorld å¨æ´å¹¿æ³ãæ´å¤æ ·åçæºå¨äººä»»å¡æçå®ä¸çåºæ¯ä¸­çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã
    *   <strong>å³é®å¸§å¯åº¦å¯¹æ§è½çå½±åï¼</strong> è®ºææåºæä½³å³é®å¸§å¯åº¦å¯è½å ä»»å¡èå¼ï¼è¿æå³çå¨ä¸ååºç¨ä¸­å¯è½éè¦éå¯¹æ§å°éæ©å³é®å¸§æ¯ä¾ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>èªéåºå³é®å¸§éæ©ï¼</strong> æ¢ç´¢æ´æºè½ãèªéåºçå³é®å¸§éæ©æºå¶ï¼è½å¤æ ¹æ®ä»»å¡çå¨æç¹æ§åå¤ææ§èªå¨è°æ´å³é®å¸§å¯åº¦ï¼èä¸ä»ä»æ¯ä¾èµåºå®çæ¯ä¾ææå¨è°ä¼ã
*   <strong>æ´å¤æçæå¼æ¨¡åï¼</strong> å°½ç®¡ç®åè½»éçº§æå¼å¨è¡¨ç°è¯å¥½ï¼ä½å¯¹äºæç«¯å¤ææå¿«éååçä¸­é´è¿å¨ï¼å¯è½éè¦ç ç©¶æ´åè¿çæå¼ææ¯ï¼ä»¥è¿ä¸æ­¥æé«ç©çä¸è´æ§åè§è§è´¨éã
*   <strong>å¤æ¨¡æå³é®å¸§ï¼</strong> ç»åé¤äºæºå¨äººå§¿æä¹å¤çå¶ä»æ¨¡æä¿¡æ¯ï¼å¦åãè§¦è§ãå£°é³ç­ï¼æ¥è¯å«å³é®å¸§ï¼å¯è½è½ææå°æ´ä¸°å¯ãæ´ç»è´çè¯­ä¹è½¬æ¢ã
*   <strong>å®æ¶é¨ç½²çè¿ä¸æ­¥ä¼åï¼</strong> å°½ç®¡ KeyWorld å®ç°äºæ¾èå éï¼ä½è¿ä¸æ­¥ä¼åæ¨¡åæ¶æåæ¨çæµç¨ï¼ä»¥æ»¡è¶³æ´ä¸¥æ ¼çå®æ¶æºå¨äººæ§å¶éæ±ï¼ä»æ¯ä¸ä¸ªéè¦çç ç©¶æ¹åã
*   <strong>ä¸å¶ä»ä¸çæ¨¡åçéæï¼</strong> æ¢ç´¢ KeyWorld æ¡æ¶ä¸ä¸åç±»åçä¸çæ¨¡åï¼ä¾å¦ï¼åºäºç¶æç©ºé´æ¨¡åæå¼ºåå­¦ä¹ çä¸çæ¨¡åï¼çéæï¼ä»¥æç»ååèªä¼å¿ï¼å®ç°æ´å¼ºå¤§ãæ´éç¨çæºå¨äººæºè½ã</p>
<hr />
<p>æ»èè¨ä¹ï¼KeyWorld è®ºææåºäºä¸ç§æ°é¢ä¸é«æçæºå¨äººä¸çæ¨¡åæ¡æ¶ï¼éè¿å°å³é®å¸§æ¨çä¸è½»éçº§æå¼ç¸ç»åï¼æ¾èæåäºçææçåç©çåçæ§ãå¶è¿å¨æç¥å³é®å¸§æ£æµåä¸¤é¶æ®µçæç­ç¥ä¸ºå®æ¶æºå¨äººæ§å¶åå¶ä»éè¦é«æææä¸çæ¨¡åçé¢åå¼è¾äºæ°çéè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
address this inefficiency, we propose KeyWorld, a framework that improves
text-conditioned robotic world models by concentrating transformers computation
on a few semantic key frames while employing a lightweight convolutional model
to fill the intermediate frames.</li>
<li>Our approach highlights a
practical path toward deploying world models in real-time robotic control and
other domains requiring both efficient and effective world models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.21027v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.21027v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-26 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
