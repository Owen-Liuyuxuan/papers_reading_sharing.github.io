<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-16 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-15/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-17/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-16">Arxiv Computer Vision Papers - 2025-10-16</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#prompt-based-adaptation-in-large-scale-vision-models-a-survey" class="nav-link">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda" class="nav-link">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a>
                </li>
                <li class="nav-item">
                    <a href="#generative-universal-verifier-as-multimodal-meta-reasoner" class="nav-link">Generative Universal Verifier as Multimodal Meta-Reasoner</a>
                </li>
                <li class="nav-item">
                    <a href="#trace-anything-representing-any-video-in-4d-via-trajectory-fields" class="nav-link">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a>
                </li>
                <li class="nav-item">
                    <a href="#bee-a-high-quality-corpus-and-full-stack-suite-to-unlock-advanced-fully-open-mllms" class="nav-link">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#internvla-m1-a-spatially-guided-vision-language-action-framework-for-generalist-robot-policy" class="nav-link">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a>
                </li>
                <li class="nav-item">
                    <a href="#uni-mmmu-a-massive-multi-discipline-multimodal-unified-benchmark" class="nav-link">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#interactiveomni-a-unified-omni-modal-model-for-audio-visual-multi-turn-dialogue" class="nav-link">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</a>
                </li>
                <li class="nav-item">
                    <a href="#multi-scale-high-resolution-logarithmic-grapher-module-for-efficient-vision-gnns" class="nav-link">Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</a>
                </li>
                <li class="nav-item">
                    <a href="#next-omni-towards-any-to-any-omnimodal-foundation-models-with-discrete-flow-matching" class="nav-link">NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-16">Arxiv Computer Vision Papers - 2025-10-16</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月15日Arxiv计算机视觉领域论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速掌握关键信息。</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文报告执行摘要 (2025年10月15日)</strong></p>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文集清晰地展示了计算机视觉领域向<strong>多模态、通用性、大型模型适应与应用</strong>的强劲发展趋势。核心主题包括：</p>
<ul>
<li><strong>大型视觉-语言模型 (VLMs) 的泛化与应用：</strong> 大量工作聚焦于VLMs在不同任务（如城市监控、机器人策略、多模态对话）中的能力拓展、评估和实际部署。</li>
<li><strong>多模态数据与基准：</strong> 为了支持通用模型的发展，高质量、大规模的多模态数据集和统一基准的构建成为重要研究方向。</li>
<li><strong>通用智能体与元推理：</strong> 出现了旨在构建能够处理多种模态、执行复杂推理任务的“通用验证器”或“元推理器”的尝试。</li>
<li><strong>视频理解与4D表示：</strong> 视频内容的高效、细粒度表示方法（如轨迹场）是持续关注的焦点。</li>
<li><strong>模型效率与架构创新：</strong> 尽管大型模型是主流，但也有工作关注如何通过图神经网络等架构创新提升效率。</li>
</ul>
<p><strong>2. 特别重要或创新的论文亮点：</strong></p>
<ul>
<li><strong>"Generative Universal Verifier as Multimodal Meta-Reasoner" (Xinchen Zhang et al.)：</strong> 这篇论文提出了一个“生成式通用验证器”的概念，旨在作为多模态的元推理器。其目标是构建一个能够跨模态进行通用验证和推理的系统，这代表了迈向更高级通用人工智能的重要一步。</li>
<li><strong>"Trace Anything: Representing Any Video in 4D via Trajectory Fields" (Xinhang Liu et al.)：</strong> 提出通过轨迹场来表示视频中的任何内容，这是一种新颖且强大的4D视频表示方法，有望极大地提升视频理解和编辑的精细度与灵活性。</li>
<li><strong>"NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching" (Run Luo et al.)：</strong> 致力于构建“任意到任意”的全模态基础模型，并引入了离散流匹配技术。这代表了在统一不同模态输入和输出方面的前沿探索，具有巨大的潜力。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>“全模态 (Omnimodal)”模型：</strong> 不仅仅是多模态，而是追求能够处理和生成任意模态（文本、图像、音频、视频、动作等）之间转换的模型，如"NExT-OMNI"和"InteractiveOmni"。</li>
<li><strong>轨迹场 (Trajectory Fields) 进行视频表示：</strong> 作为一种细粒度的4D视频表示方法，有望超越传统的帧级或稀疏点表示。</li>
<li><strong>离散流匹配 (Discrete Flow Matching)：</strong> 在全模态生成模型中被提出，可能成为处理复杂多模态数据生成的新范式。</li>
<li><strong>Prompt-based Adaptation 的系统性研究：</strong> 随着大型模型成为主流，如何高效、鲁棒地进行模型适应（尤其是基于Prompt）成为关键，"Prompt-based Adaptation in Large-scale Vision Models: A Survey"提供了全面的视角。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对于关注通用AI和多模态推理的研究者：</strong><ul>
<li><strong>"Generative Universal Verifier as Multimodal Meta-Reasoner" (Xinchen Zhang et al.)</strong></li>
<li><strong>"NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching" (Run Luo et al.)</strong></li>
</ul>
</li>
<li><strong>对于关注大型模型应用和适应性的研究者：</strong><ul>
<li><strong>"Prompt-based Adaptation in Large-scale Vision Models: A Survey" (Xi Xiao et al.)</strong> - 提供全面背景和未来方向。</li>
<li><strong>"Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda" (André Torneiro et al.)</strong> - 结合实际应用场景，具有很强的参考价值。</li>
<li><strong>"InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy" (Xinyi Chen et al.)</strong> - 机器人领域的重要进展。</li>
</ul>
</li>
<li><strong>对于关注视频理解和新表示方法的研究者：</strong><ul>
<li><strong>"Trace Anything: Representing Any Video in 4D via Trajectory Fields" (Xinhang Liu et al.)</strong></li>
</ul>
</li>
<li><strong>对于关注多模态基准和数据构建的研究者：</strong><ul>
<li><strong>"Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark" (Kai Zou et al.)</strong></li>
<li><strong>"Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs" (Yi Zhang et al.)</strong></li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究方向最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></li>
<li><a href="#2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></li>
<li><a href="#2510.13804v1">Generative Universal Verifier as Multimodal Meta-Reasoner</a></li>
<li><a href="#2510.13802v1">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a></li>
<li><a href="#2510.13795v1">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a></li>
<li><a href="#2510.13778v1">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a></li>
<li><a href="#2510.13759v1">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a></li>
<li><a href="#2510.13747v1">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</a></li>
<li><a href="#2510.13740v1">Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</a></li>
<li><a href="#2510.13721v1">NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.13219v1'></a></p>
<h2 id="prompt-based-adaptation-in-large-scale-vision-models-a-survey"><a href="https://arxiv.org/abs/2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></h2>
<p><strong>Authors:</strong> Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Prompt-based Adaptation in Large-scale Vision Models: A Survey”的中文摘要，涵盖了您要求的所有要点：</p>
<p><strong>论文摘要：大型视觉模型中的基于提示的适应：一项综述</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该综述旨在解决计算机视觉领域中，视觉提示（Visual Prompting, VP）和视觉提示微调（Visual Prompt Tuning, VPT）这两种新兴的轻量级模型适应方法之间概念边界模糊的问题。尽管它们在“预训练-微调”范式中取得了快速进展，但当前研究中经常互换使用这些术语，缺乏对它们各自技术和应用的系统性区分。因此，核心问题是建立一个统一的框架，清晰地定义、分类并概述基于提示的适应（Prompt-based Adaptation, PA）方法及其在大型视觉模型中的应用。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
* <strong>统一框架与分类：</strong> 论文将VP和VPT从第一性原理出发，概念化为一个统一的“基于提示的适应（PA）”框架。
* <strong>详细分类法：</strong> 提出了一种全面的分类法，将现有方法分为可学习、生成式和不可学习提示，并根据注入粒度（像素级和令牌级）进一步组织。
* <strong>应用领域整合：</strong> 深入探讨了PA在各种不同领域中的集成，包括医学影像、3D点云、视觉-语言任务，以及其在测试时间适应和可信赖AI中的作用。
* <strong>首次全面综述：</strong> 据作者所知，这是第一篇专门针对PA方法论及其应用，并结合其独特特征的全面综述。</p>
<p><strong>3. 主要结果及其意义：</strong>
* <strong>PA的有效性：</strong> 综述表明，PA作为全量微调的轻量级且有效替代方案，在各种受限学习场景（如数据稀缺、数据分布非平稳、模型内部不可访问或计算资源有限）中展现出显著的有效性。
* <strong>VP与VPT的区分：</strong> VP通过修改输入空间来适应模型，而VPT则通过在模型内部注入可学习的提示令牌来调整模型行为。这种区分对于理解它们的效率特点和适用场景至关重要。
* <strong>广泛的应用潜力：</strong> PA在基础CV任务（如分割、图像恢复、压缩）和特定领域（如医疗、机器人、遥感）中都显示出强大的潜力，能够桥接预训练模型与下游任务。
* <strong>可信赖AI的贡献：</strong> PA在提升模型鲁棒性、缓解公平性与偏见、以及保障隐私与安全方面发挥着重要作用，为构建可信赖的AI系统提供了轻量级解决方案。</p>
<p><strong>4. 论文中提及的局限性：</strong>
* <strong>训练开销与稳定性：</strong> 尽管PA提高了参数效率，但训练开销（如超参数搜索）和训练结果的不稳定性仍然是挑战。
* <strong>推理延迟：</strong> 额外的提示组件可能增加推理延迟和内存消耗。
* <strong>真实世界环境评估不足：</strong> 当前PA方法的评估主要依赖标准化基准数据集，未能充分反映真实世界场景的复杂性和分布漂移。
* <strong>理论分析有限：</strong> 相关的理论分析在当前研究社区中仍然有限，对PA如何引发模型行为变化、提示学习到什么以及在不同适应设置下的有效性等问题缺乏深入理解。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
* <strong>解决训练开销与稳定性：</strong> 需要进一步研究训练捷径、检测并纠正训练不稳定性的策略。
* <strong>优化推理延迟：</strong> 探索剪枝、知识蒸馏、量化和内存高效微调等技术以减少推理延迟。
* <strong>加强真实世界评估：</strong> 优先发展能够应对多样化、复杂和异构视觉上下文的鲁棒方法，以弥合与真实世界部署之间的差距。
* <strong>深入理论分析：</strong> 进一步探索PA如何诱导模型行为变化、视觉提示学习的内容以及PA方法在不同适应设置下的有效性。
* <strong>安全对齐：</strong> 将安全对齐（包括可解释性、治理和严格验证）融入PA方法的开发和部署中，以应对恶意行为、有害内容生成和偏见放大等问题。
* <strong>混合方法：</strong> 结合VP和VPT的优势，例如使用生成器提供初始空间提示，同时使用条件感知令牌进行内部调整，以应对环境退化。</p>
<p>这篇综述为研究人员和实践者提供了一个清晰的路线图，以理解和探索PA相关研究的不断发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics.</li>
<li>Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12400v1'></a></p>
<h2 id="towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda"><a href="https://arxiv.org/abs/2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></h2>
<p><strong>Authors:</strong> André Torneiro, Diogo Monteiro, Paulo Novais, Pedro Rangel Henriques, Nuno F. Rodrigues</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Urban monitoring of public infrastructure (such as waste bins, road signs,
vegetation, sidewalks, and construction sites) poses significant challenges due
to the diversity of objects, environments, and contextual conditions involved.
Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.
This raises a critical question: Can machines now "see" like citizens and infer
informed opinions about the condition of urban infrastructure? Vision-Language
Models (VLMs), which integrate visual understanding with natural language
reasoning, have recently demonstrated impressive capabilities in processing
complex visual information, turning them into a promising technology to address
this challenge. This systematic review investigates the role of VLMs in urban
monitoring, with particular emphasis on zero-shot applications. Following the
PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021
and 2025 to address four core research questions: (1) What urban monitoring
tasks have been effectively addressed using VLMs? (2) Which VLM architectures
and frameworks are most commonly used and demonstrate superior performance? (3)
What datasets and resources support this emerging field? (4) How are VLM-based
applications evaluated, and what performance levels have been reported?</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于André Torneiro等人的论文“Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda”的全面摘要：</p>
<p><strong>论文摘要：</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决城市公共基础设施（如垃圾桶、路标、植被、人行道和建筑工地）监测所面临的重大挑战。传统的物联网传感器和人工检查方法成本高昂、难以扩展，且往往与市民的直观视觉感知不符。因此，核心研究问题是：机器能否像市民一样“看”，并对城市基础设施的状况形成知情判断？具体而言，论文通过系统综述，探讨了视觉-语言模型（VLMs）在城市监测中的作用，尤其侧重于零样本应用，并回答了以下四个核心问题：
1. VLMs有效解决了哪些城市监测任务？
2. 哪些VLM架构和框架最常用且表现优异？
3. 哪些数据集和资源支持这一新兴领域？
4. 基于VLM的应用如何评估，报告了哪些性能水平？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
* <strong>系统综述与分类法：</strong> 论文采用PRISMA方法论，对2021年至2025年间发表的32篇同行评审研究进行了分析。在此基础上，提出了一套功能性VLM城市应用分类法，将现有研究划分为七个主要领域：目标检测与分割、城市规划与土地利用分类、导航与路径规划、交通分析与运输、城市场景理解与感知、地理定位与位置查找、以及城市监控与安全。
* <strong>模型生态系统分析：</strong> 论文将模型生态系统分为四类：纯视觉骨干网络、独立语言模型、多模态VLM和混合集成，并分析了它们在城市应用中的操作模式、集成策略和局限性。
* <strong>数据集使用模式分析：</strong> 论文详细分析了城市监测中数据集的使用情况，包括街景图像、合成数据、航空/俯视图以及专有数据集，揭示了现有数据集在泛化能力、可复现性和实际应用方面的结构性缺陷。</p>
<p><strong>3. 主要结果及其意义：</strong>
* <strong>VLM在城市监测中的潜力：</strong> VLMs通过整合视觉理解和自然语言推理，在处理复杂视觉信息方面展现出强大能力，为解决城市监测挑战提供了有前景的技术。
* <strong>性能多样性：</strong> 在不同任务中，VLMs表现出不同的性能。例如，在目标检测中，SAM和Grounding DINO结合取得了高IoU分数；在城市规划中，UrbanCLIP在住宅区分类中F1分数达到0.82；在地理定位中，IM2City通过线性探测达到了85.9%的Top-1准确率。
* <strong>模型和数据集趋势：</strong> CLIP、Grounding DINO和GPT-3.5是最常用的模型，反映了对模块化、通用骨干网络的偏好。街景图像数据集（如Google Street View、Mapillary Vistas）占据主导地位，但合成数据集（如CARLA、SYNTHIA）在模拟稀有或危险条件方面也得到广泛应用。
* <strong>领域适应与泛化挑战：</strong> 尽管在有限监督下，mIoU和上下文感知准确性有所提高，但在跨域（如合成到真实场景）和跨城市泛化方面仍存在显著挑战。</p>
<p><strong>4. 论文中提及的局限性：</strong>
* <strong>评估标准不一致：</strong> 性能报告差异大，缺乏统一的基准协议、标准化指标、置信区间和详细的错误分析，使得不同研究间的直接比较困难。
* <strong>部署可行性不足：</strong> 很少有研究评估模型的运行时、硬件兼容性、能耗或内存占用等部署相关因素，也未充分考虑对抗性攻击、遮挡或时间漂移的鲁棒性。
* <strong>模态差距与上下文缺失：</strong> 大多数城市VLM管道过度依赖静态图像-文本对，忽略了时间序列、深度图、地理定位和环境声音等丰富模态，限制了其在动态城市环境中的推理能力。
* <strong>对资源密集型架构的过度依赖：</strong> 许多最先进的VLM模型计算成本高昂，不适合实时、移动或嵌入式部署。
* <strong>伦理盲点与法律疏忽：</strong> 很少有研究将算法公平性、知情同意、数据来源和隐私保护等伦理维度整合到模型开发流程中。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
* <strong>SLM-VLM混合架构：</strong> 结合小型语言模型（SLMs）与模块化视觉编码器和解码器，实现高效的多模态推理，适用于边缘硬件部署。
* <strong>统一的城市基准：</strong> 开发集成多语言提示、多模态传感器流（图像、视频、音频、LiDAR）和文化多样性地理数据的评估套件，以确保可复现性、跨域可比性和鲁棒泛化。
* <strong>以部署为中心的设计：</strong> 将硬件限制、延迟要求、热预算和隐私考虑等部署约束嵌入模型开发周期。
* <strong>嵌入式伦理与合规性：</strong> 将文化鲁棒性检查、算法公平性评估、数据集同意追踪和偏见审计整合到核心基准测试和评估生命周期中。
* <strong>可复现的开放生态系统：</strong> 培养透明文化，通过版本化数据集、Docker化基线、公共排行榜和共享评估代码，促进社区驱动的倡议。</p>
<p>总而言之，这篇论文全面回顾了视觉-语言模型在城市监测领域的应用现状，揭示了其巨大潜力，同时也指出了当前研究在泛化能力、部署可行性和伦理考量方面的局限性，并提出了一个多维度的研究议程，旨在推动城市AI系统向更强大、可部署、包容、可解释和符合伦理的方向发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12400v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12400v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13804v1'></a></p>
<h2 id="generative-universal-verifier-as-multimodal-meta-reasoner"><a href="https://arxiv.org/abs/2510.13804v1">Generative Universal Verifier as Multimodal Meta-Reasoner</a></h2>
<p><strong>Authors:</strong> Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xinchen Zhang等人撰写的论文“Generative Universal Verifier as Multimodal Meta-Reasoner”的全面摘要。</p>
<hr />
<p><strong>论文摘要：多模态元推理器——生成式通用验证器</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决下一代多模态推理中视觉结果验证的根本性挑战。现有的视觉-语言模型（VLMs）在多模态推理和生成过程中，对视觉结果进行可靠的反射和细化能力不足，导致其在验证视觉输出方面与人类水平存在显著差距。具体来说，论文探讨了如何系统地评估VLMs在视觉结果验证方面的表现，如何开发一个强大的生成式通用验证器，以及如何利用视觉验证来增强推理或生成能力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>ViVerBench基准的构建：</strong> 论文引入了一个全面且具有挑战性的基准ViVerBench，涵盖16类关键任务，用于评估多模态推理中的视觉结果。该基准通过人工标注精心构建，包含3,594个多样化且具有挑战性的验证问题，要求模型提供二元判断和详细解释。
*   <strong>OmniVerifier-7B的训练与原子能力识别：</strong> 论文设计了两种自动化数据构建流程，用于生成大规模视觉验证数据，并在此基础上训练了OmniVerifier-7B。这是首个为通用视觉验证而训练的全能生成式验证器。通过训练，论文识别出视觉验证的三种原子能力：显式对齐、关系验证和整合推理，并展示了它们如何协同泛化和交互。
*   <strong>OmniVerifier-TTS的提出：</strong> 论文提出了OmniVerifier-TTS，一种顺序测试时缩放范式。它利用通用验证器在统一模型中桥接图像生成和编辑，通过迭代细粒度优化来提升生成能力上限。该范式还扩展了通用验证器在更广泛的世界建模交错推理场景中的应用。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>现有VLMs的不足：</strong> ViVerBench上的实验结果表明，现有VLMs在视觉结果验证任务上表现不佳，与人类水平存在显著差距。具体表现为：在细粒度图像-提示对齐方面的弱点、世界知识表示不匹配以及视觉推理任务中批评能力不足。
*   <strong>OmniVerifier-7B的卓越性能：</strong> OmniVerifier-7B在ViVerBench上取得了显著的性能提升（+8.3），超越了GPT-4o，并在显式对齐和关系验证等任务上表现出显著改进。这表明通过针对原子能力进行强化学习训练，可以有效地构建更强大、更具泛化性的视觉验证器。
*   <strong>OmniVerifier-TTS的生成增强：</strong> OmniVerifier-TTS在T2I-ReasonBench（+3.7）和GenEval++（+4.3）上均实现了改进，优于现有的并行测试时缩放方法（如Best-of-N）。这证明了通过可靠的视觉验证，OmniVerifier能够实现生成过程中的可靠反射和可扩展的测试时细化，从而推动下一代推理系统更值得信赖和可控。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>任务泛化能力：</strong> 某些任务（如迷宫）由于领域差距较大，泛化效果不佳，需要任务特定的数据进行优化。
*   <strong>骨干模型的影响：</strong> OmniVerifier-TTS的性能受其骨干模型的影响。目前，统一多模态模型对生成或编辑的图像分布敏感，在多步自细化过程中可能表现出异常行为（例如，GPT-Image-1在迭代编辑后倾向于生成偏黄的图像）。尽管这些伪影不影响验证性能，但它们是骨干模型本身的局限性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>增强通用验证器的泛化能力：</strong> 未来的工作将探索训练和数据构建策略，以增强OmniVerifier的泛化能力，使其更接近真正的通用验证器。
*   <strong>改进多模态后训练：</strong> 进一步扩展通用验证器，并研究其在改进多模态后训练方面的潜力。
*   <strong>解决骨干模型的风格一致性问题：</strong> 鼓励进一步努力，在多步自细化下增强风格一致性。</p>
<hr />
<p>这篇论文通过引入生成式通用验证器，为多模态推理和生成领域带来了重要的进展。它不仅提供了一个评估现有模型能力的全面基准，还提出了一种有效的方法来训练能够进行可靠视觉验证的模型，并将其应用于增强生成质量和推理能力，为构建更智能、更可控的AI系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.</li>
<li>This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning.</li>
<li>Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13804v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13804v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13802v1'></a></p>
<h2 id="trace-anything-representing-any-video-in-4d-via-trajectory-fields"><a href="https://arxiv.org/abs/2510.13802v1">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a></h2>
<p><strong>Authors:</strong> Xinhang Liu, Yuxi Xiao, Donny Y. Chen, Jiashi Feng, Yu-Wing Tai, Chi-Keung Tang, Bingyi Kang</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Effective spatio-temporal representation is fundamental to modeling,
understanding, and predicting dynamics in videos. The atomic unit of a video,
the pixel, traces a continuous 3D trajectory over time, serving as the
primitive element of dynamics. Based on this principle, we propose representing
any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame. With this
representation, we introduce Trace Anything, a neural network that predicts the
entire trajectory field in a single feed-forward pass. Specifically, for each
pixel in each frame, our model predicts a set of control points that
parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at
arbitrary query time instants. We trained the Trace Anything model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
our new benchmark for trajectory field estimation and performs competitively on
established point-tracking benchmarks; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.
Project page: https://trace-anything.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：Trace Anything: Representing Any Video in 4D via Trajectory Fields</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种新颖的视频表示方法——“轨迹场”（Trajectory Field），它将视频中的每个像素映射为一个连续的3D时间轨迹函数。基于此，作者引入了“Trace Anything”神经网络，该网络能够通过单次前向传播预测整个轨迹场，从而实现对视频中任何像素在任意时间点的3D位置的追踪和预测。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>核心创新在于将视频的原子单元（像素）视为在时间上连续的3D轨迹，并提出了一种<strong>轨迹场</strong>的表示范式。传统方法通常关注离散的特征点追踪或光流估计，而轨迹场则提供了一种<strong>稠密且连续的4D（3D空间+1D时间）表示</strong>。</p>
<p>具体方法论是：
*   <strong>轨迹场表示：</strong> 将视频中的每个像素在每个帧中都关联一个连续的3D轨迹函数。
*   <strong>参数化轨迹：</strong> 使用一组控制点来参数化这些轨迹（例如，B样条），使得模型能够预测任意查询时间点的3D位置。
*   <strong>单次前向传播预测：</strong> “Trace Anything”神经网络通过一次前向传播直接预测所有像素的轨迹控制点，避免了迭代优化或依赖辅助估计器，显著提高了效率。
*   <strong>大规模4D数据训练：</strong> 模型在包括自建平台数据在内的大规模4D数据上进行训练，这暗示了对高质量、稠密轨迹标注数据的需求和获取能力。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>统一的视频表示：</strong> 轨迹场提供了一种更基础、更统一的视频动态表示，可能成为未来视频理解和生成任务的基石。</li>
<li><strong>效率提升：</strong> 单次前向传播的范式显著提高了轨迹预测的效率，使其更适用于实时应用或大规模视频处理。</li>
<li><strong>新能力涌现：</strong> 摘要中提到的“目标条件操作”、“运动预测”和“时空融合”等新兴能力，表明这种稠密、连续的轨迹表示能够解锁更高级的视频理解和交互任务，超越了传统点追踪或光流的范畴。</li>
<li><strong>新的基准和研究方向：</strong> 引入新的轨迹场估计基准，将推动该领域的研究进展，并可能激发更多基于轨迹场的新算法和应用。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>视频理解与分析：</strong> 更精确的运动理解、行为识别、事件检测。</li>
<li><strong>视频生成与编辑：</strong> 运动风格迁移、视频插帧、物体移除与填充、视频内容创作。</li>
<li><strong>机器人学与自主驾驶：</strong> 目标跟踪、运动预测、路径规划、场景理解。</li>
<li><strong>增强现实/虚拟现实 (AR/VR)：</strong> 场景重建、运动追踪、虚拟物体与真实场景的融合。</li>
<li><strong>医学影像分析：</strong> 器官运动追踪、细胞动力学分析。</li>
<li><strong>物理模拟与动画：</strong> 更真实的物体运动模拟、角色动画。</li>
<li><strong>人机交互：</strong> 手势识别、眼动追踪。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>数据依赖性：</strong> 摘要强调了在“大规模4D数据”上进行训练，包括“新平台”的数据。这暗示了高质量、稠密、带有3D轨迹标注的数据集对于模型训练至关重要，获取和标注这类数据可能是一个巨大的挑战和成本。</li>
<li><strong>计算复杂度：</strong> 尽管是单次前向传播，但为视频中“每个像素在每个帧”预测“一组控制点”来参数化轨迹，其计算量和内存消耗可能仍然非常大，尤其对于高分辨率、长时序视频。</li>
<li><strong>轨迹的平滑性和准确性：</strong> B样条等参数化方法在表示复杂、非刚体或快速变化的运动时，其准确性和细节捕捉能力可能受到限制。模型预测的控制点数量和轨迹的阶数会影响其表达能力。</li>
<li><strong>泛化能力：</strong> 模型在特定大规模4D数据上训练，其在与训练数据分布差异较大的真实世界复杂场景（如遮挡、光照变化、模糊、快速运动）下的泛化能力有待进一步验证。</li>
<li><strong>“新兴能力”的实现细节：</strong> 摘要中提到的“目标条件操作”、“运动预测”等能力，其具体实现机制（例如，如何将目标条件融入轨迹预测）在摘要中并未详细说明，可能需要额外的模块或训练策略。</li>
</ul>
<hr />
<p>总的来说，这篇论文提出了一种非常具有前瞻性和潜力的视频表示方法，将视频理解推向了更深层次的4D时空连续性。其单次前向传播的效率优势和所展现出的新兴能力，使其在计算机视觉领域具有重要的研究价值和广泛的应用前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on this principle, we propose representing
any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame.</li>
<li>With this
representation, we introduce Trace Anything, a neural network that predicts the
entire trajectory field in a single feed-forward pass.</li>
<li>We trained the Trace Anything model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
our new benchmark for trajectory field estimation and performs competitively on
established point-tracking benchmarks; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13802v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13802v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13795v1'></a></p>
<h2 id="bee-a-high-quality-corpus-and-full-stack-suite-to-unlock-advanced-fully-open-mllms"><a href="https://arxiv.org/abs/2510.13795v1">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a></h2>
<p><strong>Authors:</strong> Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文《Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs》的全面摘要。</p>
<p><strong>论文摘要：</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前完全开源多模态大语言模型（MLLMs）在性能上落后于专有模型的核心问题。作者指出，这种差距主要源于监督微调（SFT）阶段数据质量的显著不足，具体表现为现有开源数据集普遍存在的噪声和复杂推理数据（如思维链CoT）的严重缺乏，这阻碍了模型高级能力的开发。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述挑战，论文提出了三项主要贡献：
*   <strong>Honey-Data-15M 数据集：</strong> 引入了一个新的SFT数据集，包含约1500万个问答对。该数据集经过多重清洗技术处理，并通过新颖的双层（短CoT和长CoT）CoT富集策略进行增强，以提供不同深度的推理路径。
*   <strong>HoneyPipe 数据策展流程与 DataStudio 框架：</strong> 提出了一个透明且可适应的数据策展方法论，超越了静态数据集发布模式。HoneyPipe利用MLLMs自动化整个策展工作流，从清洗到富集，为开源社区提供了一种可扩展且经济高效的高质量数据构建方案。
*   <strong>Bee-8B 模型：</strong> 训练了一个8B参数模型Bee-8B，用于验证Honey-Data-15M数据集和HoneyPipe流程的有效性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能突破：</strong> 实验结果表明，Bee-8B在完全开源MLLMs中建立了新的最先进（SOTA）性能，并且在某些情况下甚至超越了InternVL3.5-8B等近期半开源模型。
*   <strong>数据策展的有效性：</strong> 广泛的消融研究证实，数据策展过程（包括噪声过滤和CoT富集）对模型性能提升具有显著影响，尤其是在推理密集型基准测试上。这直接验证了高质量数据策略在弥合完全开源与半开源MLLMs性能差距方面的关键作用。
*   <strong>资源发布：</strong> 论文向社区提供了包括Honey-Data-15M语料库、包含HoneyPipe和DataStudio的全栈套件、训练方案、评估工具以及模型权重在内的基础资源。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中未明确提及当前工作的具体局限性，但其核心研究问题本身就暗示了现有开源MLLMs在数据质量和复杂推理能力上的不足。此外，论文强调了数据策展的成本效益和可扩展性，这可能间接说明了大规模人工标注的局限性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文明确指出，未来的研究方向应继续关注数据质量，并通过透明、可复现的方法进行数据策展，而非仅仅追求数据量。这为开源社区开发具有高度竞争力的MLLMs指明了方向。此外，DataStudio框架的灵活性也为社区提供了不断迭代和改进数据策展方法的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Addressing these challenges, our work makes three primary
contributions.</li>
<li>First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy.</li>
<li>Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases.</li>
<li>Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13795v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13795v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13778v1'></a></p>
<h2 id="internvla-m1-a-spatially-guided-vision-language-action-framework-for-generalist-robot-policy"><a href="https://arxiv.org/abs/2510.13778v1">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a></h2>
<p><strong>Authors:</strong> Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, Yangkun Zhu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine <code>where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide</code>how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.</p>
<p><strong>Analysis:</strong></p>
<p>以下是论文“InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy”的摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决通用机器人领域的一个核心挑战：如何使机器人能够理解高级语言指令，并将其转化为在3D物理世界中精确、具身化的动作。现有方法在将文本抽象的指令与实际的、连续的机器人动作联系起来时存在根本性差距，尤其是在面对大规模、多样化任务时，难以实现可扩展的、通用的机器人智能。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
InternVLA-M1引入了一个统一的、空间引导的视觉-语言-动作（VLA）框架，其核心创新在于“空间引导的视觉-语言-动作训练”范式。该框架采用两阶段训练流程：
*   <strong>空间接地预训练（Spatial Grounding Pre-training）：</strong> 在超过230万的空间推理数据上进行预训练，以确定“在哪里行动”（where to act），通过将指令与视觉的、与具身无关的位置对齐。这建立了一个可迁移的空间先验知识。
*   <strong>空间引导的动作后训练（Spatially Guided Action Post-training）：</strong> 通过即插即用的空间提示（spatial prompting）生成具身感知的动作，以决定“如何行动”（how to act）。这种方法将空间先验知识转化为具体的运动控制。
*   <strong>双系统架构：</strong> InternVLA-M1采用双系统架构，包含一个VLM规划器（作为慢速但可靠的System 2推理器）和一个动作专家（作为快速的System 1控制器）。VLM规划器通过空间提示生成潜在规划令牌，指导动作专家生成控制信号。
*   <strong>大规模合成数据引擎：</strong> 构建了一个可扩展的模拟引擎，收集了24.4万个可泛化的抓取-放置（pick-and-place）任务片段，以进一步扩展指令遵循能力，并增强视觉多样性。</p>
<p><strong>3. 主要结果及其意义</strong>
InternVLA-M1在多项基准测试和真实世界场景中展现出显著的性能提升和泛化能力：
*   <strong>SimplerEnv基准：</strong> 在SimplerEnv Google Robot上，性能比没有空间引导的变体提高了14.6%；在WidowX上提高了17%；在LIBERO Franka上提高了4.3%。同时，在盒子、点和轨迹预测方面展示了更强的空间推理能力。
*   <strong>泛化抓取-放置任务：</strong> 在200个任务和3000多个对象上，平均性能提高了6.2%。
*   <strong>真实世界场景：</strong> 在真实世界的聚类抓取-放置任务中，性能提高了7.3%；通过合成数据协同训练，在未见对象和新颖配置上实现了20.6%的提升。
*   <strong>长时程推理任务：</strong> 在长时程、推理密集型场景中，性能超越现有工作10%以上。
这些结果强调了空间引导训练作为实现可扩展和弹性通用机器人统一原则的有效性。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中没有明确列出InternVLA-M1的局限性。然而，从其方法论和实验设置中可以推断出一些潜在的方面：
*   <strong>数据依赖性：</strong> 尽管使用了大规模合成数据，但模型的性能仍然依赖于训练数据的质量和多样性。对于某些高度复杂的、未充分表示的真实世界场景，可能仍存在泛化挑战。
*   <strong>计算资源需求：</strong> 模型的训练需要16块NVIDIA A100 GPU，这表明其训练过程对计算资源要求较高。
*   <strong>推理速度：</strong> 尽管通过FlashAttention和KV缓存进行了优化，VLM组件的推理速度约为10 FPS，对于某些需要极低延迟的实时交互任务，可能仍有提升空间。
*   <strong>任务范围：</strong> 尽管在抓取-放置、排序和长时程操作任务中表现出色，但其在更广泛、更开放的机器人任务（例如，需要复杂物理交互或精细操作的任务）中的表现仍需进一步探索。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文中没有明确列出未来研究方向，但基于其贡献和潜在局限性，可以推断出以下方向：
*   <strong>更广泛的任务泛化：</strong> 探索InternVLA-M1在更多样化、更复杂的机器人任务中的应用，包括需要更精细运动控制、更复杂物理交互或更抽象推理的任务。
*   <strong>效率优化：</strong> 进一步优化模型的训练和推理效率，使其能够在更少的计算资源下运行，并实现更快的实时响应。
*   <strong>数据效率：</strong> 研究如何通过更少的数据（尤其是真实世界数据）实现相似的性能，例如通过更先进的自监督学习或领域适应技术。
*   <strong>可解释性和鲁棒性：</strong> 深入研究空间引导训练如何增强模型的可解释性和在未知环境中的鲁棒性，并探索如何进一步提升这些特性。
*   <strong>多模态融合：</strong> 探索除了视觉和语言之外，结合更多模态（如触觉、听觉）来增强空间接地和动作生成。
*   <strong>持续学习和适应：</strong> 研究如何使InternVLA-M1能够在新环境中持续学习和适应，而无需从头开始重新训练。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence.</li>
<li>This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction.</li>
<li>In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13778v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13778v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13759v1'></a></p>
<h2 id="uni-mmmu-a-massive-multi-discipline-multimodal-unified-benchmark"><a href="https://arxiv.org/abs/2510.13759v1">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a></h2>
<p><strong>Authors:</strong> Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Unified multimodal models aim to jointly enable visual understanding and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
conceptual understanding to guide precise visual synthesis, or (ii) utilize
generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
incorporates verifiable intermediate reasoning steps, unique ground truths, and
a reproducible scoring protocol for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.</p>
<p><strong>Analysis:</strong></p>
<p>以下是论文“Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark”的全面摘要：</p>
<p><strong>1. 论文主要问题或研究问题</strong>
当前统一多模态模型旨在同时实现视觉理解和生成，但现有基准测试未能充分评估这两种能力之间的真正整合。现有评估要么孤立地处理这两种能力，要么忽视了它们之间固有的耦合任务。论文旨在解决这一评估空白，系统地探究生成与理解之间的双向协同作用。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>Uni-MMMU基准的提出：</strong> 论文提出了Uni-MMMU，一个全面且跨学科的基准，系统地揭示了生成与理解在八个以推理为中心的领域（包括科学、编码、数学和谜题）中的双向协同作用。每个任务都是双向耦合的，要求模型利用概念理解指导精确的视觉合成，或利用生成作为分析推理的认知支架。
*   <strong>双层评估协议：</strong> Uni-MMMU整合了可验证的中间推理步骤、独特的真实值以及可重现的文本和视觉输出评分协议。这使得对最终结果和中间步骤进行双层评估成为可能，从而实现细粒度的错误归因。
*   <strong>自动化和可重现的评估流程：</strong> 评估流程采用程序化解析器、感知度量和LLM-as-a-Judge相结合的方式，确保了客观、一致和可解释的结果。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>性能差异和跨模态依赖性：</strong> 对最先进的统一模型、仅生成模型和仅理解模型进行广泛评估后，揭示了显著的性能差异和跨模态依赖性。
*   <strong>协同作用的有效性：</strong> 结果表明，生成与理解之间的协同作用在具有严格逻辑依赖性的任务中最为有效。即使是不完美的中间生成结果，也能显著提高最终准确性。
*   <strong>当前模型的局限性：</strong> 分析揭示了当前统一模型存在明显不平衡，它们严重偏向理解能力，而生成能力是主要瓶颈。常见的失败点包括不精确的图像编辑、示意图的合成以及细粒度的空间推理。
*   <strong>评估方法的有效性：</strong> LLM-as-a-Judge评估组件的有效性通过与人类标注者和更强大的商业模型（Gemini-2.5-pro）的比较得到验证，Cohen's Kappa系数在0.6到0.8之间，表明高度一致性。</p>
<p><strong>4. 论文中提到的局限性</strong>
*   <strong>任务范围：</strong> Uni-MMMU的任务主要集中在具有确定性和可验证解决方案的推理中心学科，这使得评估客观且可重现，但未能涵盖更广泛的真实世界场景，例如需要开放式创造力、主观判断或细微常识推理的任务。
*   <strong>静态图像限制：</strong> 当前基准完全基于静态图像，未来工作可以扩展到涉及视频或长期时间交互的任务。
*   <strong>数据策展方法：</strong> 迷宫导航、滑动拼图和代码渲染等任务采用程序化生成，这确保了唯一解决方案和客观解析，但可能导致数据缺乏真实世界图像的复杂性、噪声和视觉多样性。科学任务采用LLM驱动的流程和手动策展，可能引入生成模型或人类策展者的细微偏差。
*   <strong>评估管道的局限性：</strong> 某些任务依赖于“模型即评判者”框架，尽管已验证其与人类标注者的高度一致性，但这些评判模型并非万无一失，可能存在自身的偏见或知识空白，从而影响评估准确性。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>增强生成能力：</strong> 解决当前统一模型在生成方面的瓶颈，特别是图像编辑、示意图合成和细粒度空间推理。
*   <strong>更紧密的控制性：</strong> 探索更紧密的控制机制（例如，程序或约束引导的生成）、跨编辑的更强空间/状态不变性，以及使可执行中间表示成为推理-生成循环中的一等公民的接口。
*   <strong>扩展任务范围：</strong> 将基准扩展到需要开放式创造力、主观判断或细微常识推理的真实世界场景。
*   <strong>引入视频和时间交互：</strong> 将评估扩展到涉及视频或长期时间交互的任务。
*   <strong>改进评估方法：</strong> 进一步完善“模型即评判者”框架，减少潜在偏差，提高评估准确性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.</li>
<li>Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13759v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13759v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13747v1'></a></p>
<h2 id="interactiveomni-a-unified-omni-modal-model-for-audio-visual-multi-turn-dialogue"><a href="https://arxiv.org/abs/2510.13747v1">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</a></h2>
<p><strong>Authors:</strong> Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Wenwen Tong等人撰写的论文“InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
当前的多模态大语言模型（MLLMs）在处理复杂、多轮的音频-视觉交互任务时，缺乏类人（human-like）的长期对话能力和无缝集成用户体验。它们主要关注单轮理解能力，未能提供端到端的全模态输入理解和语音响应生成能力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>统一的全模态模型架构：</strong> InteractiveOmni是一个统一的、开源的全模态大语言模型，能够同时接收图像、音频、文本和视频等全模态输入，并直接生成连贯的文本和语音流，实现真正的集成多轮交互。它集成了视觉编码器、音频编码器、大语言模型和语音解码器。
*   <strong>多阶段训练策略：</strong> 采用全模态预训练（用于跨模态理解）和后训练（用于语音对话和音频-视觉交互）的多阶段训练策略，以确保强大的跨模态能力。
*   <strong>精心策划的多轮训练数据集：</strong> 为了实现类人的长期对话能力，论文精心策划了一个多轮训练数据集，以增强模型处理复杂多轮交互的能力。
*   <strong>新型多轮基准测试：</strong> 构建了多模态多轮记忆基准（MMMB）和多轮语音交互基准（MSIB），以有效评估多轮记忆和语音交互能力，弥补现有基准的不足。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> InteractiveOmni在图像、音频、视频理解和语音生成任务中，相对于同等规模的模型取得了最先进（state-of-the-art）的结果。
*   <strong>轻量级模型的竞争力：</strong> InteractiveOmni-4B在通用基准上与更大的Qwen2.5-Omni-7B模型相当，并且在模型尺寸仅为一半的情况下，仍能保持InteractiveOmni-8B 97%的性能。
*   <strong>强大的长期记忆能力：</strong> 实验证明InteractiveOmni显著优于领先的开源模型，提供了更智能的多轮音频-视觉体验，尤其在长期记忆能力方面表现突出。
*   <strong>开源基础：</strong> InteractiveOmni作为一个可访问的开源基础模型，为下一代智能交互系统奠定了基础。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文摘要中并未明确提及InteractiveOmni模型的具体局限性。然而，从其研究背景和目标来看，当前MLLMs的普遍局限性（如多模态对齐的复杂性、端到端统一理解和生成框架的构建挑战、以及多轮对话中长期记忆和情感表达的不足）是InteractiveOmni旨在解决的问题。因此，可以推断这些是现有技术面临的挑战，而InteractiveOmni正努力克服它们。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提高模型效率：</strong> 增强模型在实时交互中的效率。
*   <strong>扩展理解复杂抽象跨模态关系的能力：</strong> 进一步扩展模型理解更复杂、更抽象的跨模态关系的能力，为更真实、更类人的用户体验铺平道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities.</li>
<li>Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities.</li>
<li>Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13747v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13747v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13740v1'></a></p>
<h2 id="multi-scale-high-resolution-logarithmic-grapher-module-for-efficient-vision-gnns"><a href="https://arxiv.org/abs/2510.13740v1">Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</a></h2>
<p><strong>Authors:</strong> Mustafa Munir, Alex Zhang, Radu Marculescu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Vision graph neural networks (ViG) have demonstrated promise in vision tasks
as a competitive alternative to conventional convolutional neural nets (CNN)
and transformers (ViTs); however, common graph construction methods, such as
k-nearest neighbor (KNN), can be expensive on larger images. While methods such
as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step
scale can lead to over-squashing and missing multiple connections to gain the
same information that could be gained from a long-range link. Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network. Extensive experiments show that
LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,
GMACs, and parameters on image classification and semantic segmentation tasks.
Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Mustafa Munir、Alex Zhang和Radu Marculescu撰写的论文《Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs》的全面摘要。</p>
<hr />
<h3 id="multi-scale-high-resolution-logarithmic-grapher-module-for-efficient-vision-gnns_1">论文摘要：Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现有视觉图神经网络（ViG）在处理高分辨率图像时面临的计算效率和信息丢失问题。传统的图构建方法，如K近邻（KNN），在处理大型图像时计算成本高昂。而静态图构建方法，如稀疏视觉图注意力（SVGA），虽然简化了计算，但其固定的步长尺度会导致“过度压缩”（over-squashing），即在长距离连接中丢失重要信息，以及无法有效获取全局上下文。这限制了ViG在计算机视觉任务中的性能和可扩展性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了以下关键创新：</p>
<ul>
<li><strong>对数可伸缩图构建（Logarithmic Scalable Graph Construction, LSGC）：</strong> 针对KNN和SVGA的局限性，LSGC提出了一种新的图构建方法。它利用图像尺寸的位深度（bit-depth）进行对数缩放，而非线性缩放。这使得图构建在处理高分辨率图像时能够生成更少的连接，从而减轻过度压缩并降低计算复杂度，同时通过优先考虑近距离连接来保持局部性，并通过长距离连接建立全局上下文。</li>
<li><strong>LogViG 混合CNN-GNN 模型：</strong> 论文引入了一种新颖的混合CNN-GNN架构LogViG，它集成了LSGC。该模型在所有四个阶段都使用了卷积层和图层，以实现局部和全局处理。</li>
<li><strong>多尺度高分辨率架构：</strong> LogViG借鉴了多尺度和高分辨率架构的成功经验，引入了一个高分辨率分支，并通过高分辨率快捷连接（High-Resolution Shortcut, HRS）在不同分辨率分支之间融合特征，以实现多尺度高分辨率视觉GNN网络。HRS通过两个3x3卷积（步长分别为2和1）将高分辨率特征注入模型的后期阶段，并通过双线性插值、逐点卷积和特征求和来融合低分辨率和高分辨率特征。</li>
<li><strong>网络深度与宽度优化：</strong> 论文通过实验证明，更深更窄的网络架构（如Ti-LogViG）相比更宽更浅的网络能带来更好的性能提升。</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
论文通过在ImageNet-1K图像分类和ADE20K语义分割任务上的广泛实验，展示了LogViG的优越性能：</p>
<ul>
<li><strong>图像分类性能：</strong> 最小模型Ti-LogViG在ImageNet-1K上实现了79.9%的平均Top-1准确率（标准差±0.2%），比现有Vision GNN高出1.7%，同时参数减少了24.3%，GMACs减少了35.3%。S-LogViG在参数和GMACs相似的情况下，性能优于HRViT-b2、DeiT和PViG等模型。B-LogViG也显著优于EfficientFormer系列模型。</li>
<li><strong>语义分割性能：</strong> S-LogViG在ADE20K上表现优于PoolFormer-S12、FastViT-SA12、EfficientFormer-L1和MobileViG-M，mIoU分别高出6.9、6.1、5.2和2.3。B-LogViG也优于FastViT-SA36和EfficientFormer-L3。</li>
<li><strong>效率提升：</strong> LogViG在准确性、GMACs和参数方面均优于现有的ViG、CNN和ViT架构，证明了LSGC和混合架构在ViG设计上的显著进步。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
论文中没有明确列出LogViG或LSGC的局限性。然而，可以从其设计和比较中推断出一些潜在的方面：</p>
<ul>
<li><strong>计算复杂性：</strong> 尽管LSGC旨在降低图构建的计算成本，但GNN固有的图操作仍然可能比纯CNN或ViT在某些场景下更复杂，尤其是在非常规硬件上。</li>
<li><strong>超参数敏感性：</strong> LSGC中的扩展率K以及LogViG架构中的其他设计选择（如各阶段的通道维度和块数量）可能需要仔细调整以达到最佳性能。</li>
<li><strong>泛化能力：</strong> 尽管在ImageNet-1K和ADE20K上表现出色，但LogViG在其他更广泛或更具挑战性的视觉任务上的泛化能力仍需进一步验证。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文没有明确提出未来的研究方向，但基于其贡献，可以推断出以下几点：</p>
<ul>
<li><strong>LSGC的进一步优化：</strong> 探索LSGC中对数缩放策略的更高级形式，或者结合自适应机制，使其能够根据图像内容或任务需求动态调整连接。</li>
<li><strong>LogViG架构的扩展：</strong> 将LogViG应用于更广泛的计算机视觉任务，如目标检测、实例分割或视频理解，并探索其在这些任务中的性能。</li>
<li><strong>轻量化和部署：</strong> 进一步优化LogViG模型，使其在资源受限的设备（如移动设备）上更高效地运行，可能通过剪枝、量化或知识蒸馏等技术。</li>
<li><strong>理论分析：</strong> 对LSGC的图属性（如连通性、直径）进行更深入的理论分析，以更好地理解其在信息传播和全局上下文捕获方面的优势。</li>
<li><strong>与其他图构建方法的结合：</strong> 探索将LSGC与其他先进的图构建或注意力机制相结合，以进一步提升性能或解决特定挑战。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links.</li>
<li>To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC.</li>
<li>Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network.</li>
<li>Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13740v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13740v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13721v1'></a></p>
<h2 id="next-omni-towards-any-to-any-omnimodal-foundation-models-with-discrete-flow-matching"><a href="https://arxiv.org/abs/2510.13721v1">NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</a></h2>
<p><strong>Authors:</strong> Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms. By leveraging metric-induced probability paths and kinetic optimal
velocities, NExT-OMNI natively supports any-to-any understanding and generation
with enhanced response efficiency, while enabling broader application scenarios
through concise unified representations rather than task-decoupled designs.
Trained on large-scale interleaved text, image, video, and audio data,
NExT-OMNI delivers competitive performance on multimodal generation and
understanding benchmarks, while outperforming prior unified models in
multi-turn multimodal interaction and cross-modal retrieval, highlighting its
architectural advantages as a next-generation multimodal foundation model. To
advance further research, we release training details, data protocols, and
open-source both the code and model checkpoints.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Run Luo等人撰写的论文“NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong></p>
<p>当前多模态基础模型在实现任意模态间的生成和多轮交互方面面临挑战。现有模型大多受限于自回归（AR）架构，这导致理解和生成能力难以平衡整合。尽管混合和解耦策略已被探索，但其冗余、非集成设计限制了它们在跨模态检索等更广泛场景中的应用。论文旨在解决这一问题，提出一个能够统一建模、实现任意模态理解和生成，并提升响应效率的多模态基础模型。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<ul>
<li><strong>引入NEXT-OMNI模型：</strong> 论文提出了NEXT-OMNI，一个基于离散流匹配（DFM）技术的开源全模态基础模型，首次实现了任意模态间的生成，并具有更快的推理速度。</li>
<li><strong>统一建模与离散流匹配范式：</strong> NEXT-OMNI通过离散流匹配范式实现统一建模，利用度量诱导概率路径和动力学最优速度，原生支持任意模态的理解和生成，提高了响应效率。</li>
<li><strong>重建增强的统一表示：</strong> 模型设计了带有中间特征融合的重建增强统一表示，这不仅实现了精确的跨模态检索，还支持多轮任意模态交互，避免了任务解耦的设计。</li>
<li><strong>动态长度生成策略与自适应缓存：</strong> 为了提升理解任务的性能和加速推理，模型引入了动态长度生成策略和自适应缓存设计，显著提高了文本生成能力和推理速度。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<ul>
<li><strong>卓越的性能：</strong> NEXT-OMNI在多模态生成和理解基准测试中表现出竞争性或卓越的性能，并在多轮多模态交互和跨模态检索方面超越了现有统一模型。例如，在OmniBench、WorldSense和AV-Odyssey等全模态理解基准测试中，NEXT-OMNI的平均性能比OpenOmni高出3.2个百分点。</li>
<li><strong>架构优势：</strong> 实验结果突显了DFM架构作为下一代多模态基础模型的优势，尤其是在处理复杂的多模态交互和跨模态检索任务时。</li>
<li><strong>更快的推理速度：</strong> 结合并行解码和自适应缓存机制，NEXT-OMNI的推理响应速度比AR架构提高了1.2倍。</li>
<li><strong>统一建模的潜力：</strong> 结果验证了基于DFM的架构在统一多模态建模方面的巨大潜力，能够实现更广泛的应用场景。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong></p>
<ul>
<li><strong>资源限制：</strong> 由于资源限制，模型仅在7B参数规模和2T token数据上进行训练和验证。因此，NEXT-OMNI的全部潜力尚未完全展现，尤其是在缺乏相应大型语言模型基础支持的情况下。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>扩展应用场景：</strong> 未来研究计划将NEXT-OMNI扩展到更广泛的领域，例如视觉-语言-动作模型中的动作轨迹生成，以及物理AI理解中的视频生成，其中视觉生成可以辅助物理感知。</li>
<li><strong>模型规模化：</strong> 进一步探索模型在更大规模数据和参数上的可扩展性，以充分发挥DFM架构的潜力。</li>
<li><strong>“世界大脑”愿景：</strong> 论文展望统一多模态模型将成为与现实世界交互的“世界大脑”，通过持续的多模态数据交互不断增强其通用能力，最终实现通用人工智能（AGI）。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13721v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13721v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-16 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
