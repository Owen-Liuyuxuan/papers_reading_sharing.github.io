<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-16 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-15/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-17/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-16">Arxiv Computer Vision Papers - 2025-10-16</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#prompt-based-adaptation-in-large-scale-vision-models-a-survey" class="nav-link">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda" class="nav-link">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a>
                </li>
                <li class="nav-item">
                    <a href="#generative-universal-verifier-as-multimodal-meta-reasoner" class="nav-link">Generative Universal Verifier as Multimodal Meta-Reasoner</a>
                </li>
                <li class="nav-item">
                    <a href="#trace-anything-representing-any-video-in-4d-via-trajectory-fields" class="nav-link">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a>
                </li>
                <li class="nav-item">
                    <a href="#bee-a-high-quality-corpus-and-full-stack-suite-to-unlock-advanced-fully-open-mllms" class="nav-link">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#internvla-m1-a-spatially-guided-vision-language-action-framework-for-generalist-robot-policy" class="nav-link">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a>
                </li>
                <li class="nav-item">
                    <a href="#uni-mmmu-a-massive-multi-discipline-multimodal-unified-benchmark" class="nav-link">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#interactiveomni-a-unified-omni-modal-model-for-audio-visual-multi-turn-dialogue" class="nav-link">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</a>
                </li>
                <li class="nav-item">
                    <a href="#multi-scale-high-resolution-logarithmic-grapher-module-for-efficient-vision-gnns" class="nav-link">Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</a>
                </li>
                <li class="nav-item">
                    <a href="#next-omni-towards-any-to-any-omnimodal-foundation-models-with-discrete-flow-matching" class="nav-link">NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-16">Arxiv Computer Vision Papers - 2025-10-16</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ15æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éææ¡å³é®ä¿¡æ¯ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ¥åæ§è¡æè¦ (2025å¹´10æ15æ¥)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæéæ¸æ°å°å±ç¤ºäºè®¡ç®æºè§è§é¢åå<strong>å¤æ¨¡æãéç¨æ§ãå¤§åæ¨¡åéåºä¸åºç¨</strong>çå¼ºå²åå±è¶å¿ãæ ¸å¿ä¸»é¢åæ¬ï¼</p>
<ul>
<li><strong>å¤§åè§è§-è¯­è¨æ¨¡å (VLMs) çæ³åä¸åºç¨ï¼</strong> å¤§éå·¥ä½èç¦äºVLMså¨ä¸åä»»å¡ï¼å¦åå¸çæ§ãæºå¨äººç­ç¥ãå¤æ¨¡æå¯¹è¯ï¼ä¸­çè½åæå±ãè¯ä¼°åå®éé¨ç½²ã</li>
<li><strong>å¤æ¨¡ææ°æ®ä¸åºåï¼</strong> ä¸ºäºæ¯æéç¨æ¨¡åçåå±ï¼é«è´¨éãå¤§è§æ¨¡çå¤æ¨¡ææ°æ®éåç»ä¸åºåçæå»ºæä¸ºéè¦ç ç©¶æ¹åã</li>
<li><strong>éç¨æºè½ä½ä¸åæ¨çï¼</strong> åºç°äºæ¨å¨æå»ºè½å¤å¤çå¤ç§æ¨¡æãæ§è¡å¤ææ¨çä»»å¡çâéç¨éªè¯å¨âæâåæ¨çå¨âçå°è¯ã</li>
<li><strong>è§é¢çè§£ä¸4Dè¡¨ç¤ºï¼</strong> è§é¢åå®¹çé«æãç»ç²åº¦è¡¨ç¤ºæ¹æ³ï¼å¦è½¨è¿¹åºï¼æ¯æç»­å³æ³¨çç¦ç¹ã</li>
<li><strong>æ¨¡åæçä¸æ¶æåæ°ï¼</strong> å°½ç®¡å¤§åæ¨¡åæ¯ä¸»æµï¼ä½ä¹æå·¥ä½å³æ³¨å¦ä½éè¿å¾ç¥ç»ç½ç»ç­æ¶æåæ°æåæçã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"Generative Universal Verifier as Multimodal Meta-Reasoner" (Xinchen Zhang et al.)ï¼</strong> è¿ç¯è®ºææåºäºä¸ä¸ªâçæå¼éç¨éªè¯å¨âçæ¦å¿µï¼æ¨å¨ä½ä¸ºå¤æ¨¡æçåæ¨çå¨ãå¶ç®æ æ¯æå»ºä¸ä¸ªè½å¤è·¨æ¨¡æè¿è¡éç¨éªè¯åæ¨ççç³»ç»ï¼è¿ä»£è¡¨äºè¿åæ´é«çº§éç¨äººå·¥æºè½çéè¦ä¸æ­¥ã</li>
<li><strong>"Trace Anything: Representing Any Video in 4D via Trajectory Fields" (Xinhang Liu et al.)ï¼</strong> æåºéè¿è½¨è¿¹åºæ¥è¡¨ç¤ºè§é¢ä¸­çä»»ä½åå®¹ï¼è¿æ¯ä¸ç§æ°é¢ä¸å¼ºå¤§ç4Dè§é¢è¡¨ç¤ºæ¹æ³ï¼æææå¤§å°æåè§é¢çè§£åç¼è¾çç²¾ç»åº¦ä¸çµæ´»æ§ã</li>
<li><strong>"NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching" (Run Luo et al.)ï¼</strong> è´åäºæå»ºâä»»æå°ä»»æâçå¨æ¨¡æåºç¡æ¨¡åï¼å¹¶å¼å¥äºç¦»æ£æµå¹éææ¯ãè¿ä»£è¡¨äºå¨ç»ä¸ä¸åæ¨¡æè¾å¥åè¾åºæ¹é¢çåæ²¿æ¢ç´¢ï¼å·æå·¨å¤§çæ½åã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>âå¨æ¨¡æ (Omnimodal)âæ¨¡åï¼</strong> ä¸ä»ä»æ¯å¤æ¨¡æï¼èæ¯è¿½æ±è½å¤å¤çåçæä»»ææ¨¡æï¼ææ¬ãå¾åãé³é¢ãè§é¢ãå¨ä½ç­ï¼ä¹é´è½¬æ¢çæ¨¡åï¼å¦"NExT-OMNI"å"InteractiveOmni"ã</li>
<li><strong>è½¨è¿¹åº (Trajectory Fields) è¿è¡è§é¢è¡¨ç¤ºï¼</strong> ä½ä¸ºä¸ç§ç»ç²åº¦ç4Dè§é¢è¡¨ç¤ºæ¹æ³ï¼ææè¶è¶ä¼ ç»çå¸§çº§æç¨çç¹è¡¨ç¤ºã</li>
<li><strong>ç¦»æ£æµå¹é (Discrete Flow Matching)ï¼</strong> å¨å¨æ¨¡æçææ¨¡åä¸­è¢«æåºï¼å¯è½æä¸ºå¤çå¤æå¤æ¨¡ææ°æ®çæçæ°èå¼ã</li>
<li><strong>Prompt-based Adaptation çç³»ç»æ§ç ç©¶ï¼</strong> éçå¤§åæ¨¡åæä¸ºä¸»æµï¼å¦ä½é«æãé²æ£å°è¿è¡æ¨¡åéåºï¼å°¤å¶æ¯åºäºPromptï¼æä¸ºå³é®ï¼"Prompt-based Adaptation in Large-scale Vision Models: A Survey"æä¾äºå¨é¢çè§è§ã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºå³æ³¨éç¨AIåå¤æ¨¡ææ¨ççç ç©¶èï¼</strong><ul>
<li><strong>"Generative Universal Verifier as Multimodal Meta-Reasoner" (Xinchen Zhang et al.)</strong></li>
<li><strong>"NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching" (Run Luo et al.)</strong></li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨å¤§åæ¨¡ååºç¨åéåºæ§çç ç©¶èï¼</strong><ul>
<li><strong>"Prompt-based Adaptation in Large-scale Vision Models: A Survey" (Xi Xiao et al.)</strong> - æä¾å¨é¢èæ¯åæªæ¥æ¹åã</li>
<li><strong>"Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda" (AndrÃ© Torneiro et al.)</strong> - ç»åå®éåºç¨åºæ¯ï¼å·æå¾å¼ºçåèä»·å¼ã</li>
<li><strong>"InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy" (Xinyi Chen et al.)</strong> - æºå¨äººé¢åçéè¦è¿å±ã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨è§é¢çè§£åæ°è¡¨ç¤ºæ¹æ³çç ç©¶èï¼</strong><ul>
<li><strong>"Trace Anything: Representing Any Video in 4D via Trajectory Fields" (Xinhang Liu et al.)</strong></li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨å¤æ¨¡æåºååæ°æ®æå»ºçç ç©¶èï¼</strong><ul>
<li><strong>"Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark" (Kai Zou et al.)</strong></li>
<li><strong>"Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs" (Yi Zhang et al.)</strong></li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></li>
<li><a href="#2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></li>
<li><a href="#2510.13804v1">Generative Universal Verifier as Multimodal Meta-Reasoner</a></li>
<li><a href="#2510.13802v1">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a></li>
<li><a href="#2510.13795v1">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a></li>
<li><a href="#2510.13778v1">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a></li>
<li><a href="#2510.13759v1">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a></li>
<li><a href="#2510.13747v1">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</a></li>
<li><a href="#2510.13740v1">Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</a></li>
<li><a href="#2510.13721v1">NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.13219v1'></a></p>
<h2 id="prompt-based-adaptation-in-large-scale-vision-models-a-survey"><a href="https://arxiv.org/abs/2510.13219v1">Prompt-based Adaptation in Large-scale Vision Models: A Survey</a></h2>
<p><strong>Authors:</strong> Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâPrompt-based Adaptation in Large-scale Vision Models: A Surveyâçä¸­ææè¦ï¼æ¶µçäºæ¨è¦æ±çææè¦ç¹ï¼</p>
<p><strong>è®ºææè¦ï¼å¤§åè§è§æ¨¡åä¸­çåºäºæç¤ºçéåºï¼ä¸é¡¹ç»¼è¿°</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç»¼è¿°æ¨å¨è§£å³è®¡ç®æºè§è§é¢åä¸­ï¼è§è§æç¤ºï¼Visual Prompting, VPï¼åè§è§æç¤ºå¾®è°ï¼Visual Prompt Tuning, VPTï¼è¿ä¸¤ç§æ°å´çè½»éçº§æ¨¡åéåºæ¹æ³ä¹é´æ¦å¿µè¾¹çæ¨¡ç³çé®é¢ãå°½ç®¡å®ä»¬å¨âé¢è®­ç»-å¾®è°âèå¼ä¸­åå¾äºå¿«éè¿å±ï¼ä½å½åç ç©¶ä¸­ç»å¸¸äºæ¢ä½¿ç¨è¿äºæ¯è¯­ï¼ç¼ºä¹å¯¹å®ä»¬åèªææ¯ååºç¨çç³»ç»æ§åºåãå æ­¤ï¼æ ¸å¿é®é¢æ¯å»ºç«ä¸ä¸ªç»ä¸çæ¡æ¶ï¼æ¸æ°å°å®ä¹ãåç±»å¹¶æ¦è¿°åºäºæç¤ºçéåºï¼Prompt-based Adaptation, PAï¼æ¹æ³åå¶å¨å¤§åè§è§æ¨¡åä¸­çåºç¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
* <strong>ç»ä¸æ¡æ¶ä¸åç±»ï¼</strong> è®ºæå°VPåVPTä»ç¬¬ä¸æ§åçåºåï¼æ¦å¿µåä¸ºä¸ä¸ªç»ä¸çâåºäºæç¤ºçéåºï¼PAï¼âæ¡æ¶ã
* <strong>è¯¦ç»åç±»æ³ï¼</strong> æåºäºä¸ç§å¨é¢çåç±»æ³ï¼å°ç°ææ¹æ³åä¸ºå¯å­¦ä¹ ãçæå¼åä¸å¯å­¦ä¹ æç¤ºï¼å¹¶æ ¹æ®æ³¨å¥ç²åº¦ï¼åç´ çº§åä»¤ççº§ï¼è¿ä¸æ­¥ç»ç»ã
* <strong>åºç¨é¢åæ´åï¼</strong> æ·±å¥æ¢è®¨äºPAå¨åç§ä¸åé¢åä¸­çéæï¼åæ¬å»å­¦å½±åã3Dç¹äºãè§è§-è¯­è¨ä»»å¡ï¼ä»¥åå¶å¨æµè¯æ¶é´éåºåå¯ä¿¡èµAIä¸­çä½ç¨ã
* <strong>é¦æ¬¡å¨é¢ç»¼è¿°ï¼</strong> æ®ä½èæç¥ï¼è¿æ¯ç¬¬ä¸ç¯ä¸é¨éå¯¹PAæ¹æ³è®ºåå¶åºç¨ï¼å¹¶ç»åå¶ç¬ç¹ç¹å¾çå¨é¢ç»¼è¿°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
* <strong>PAçæææ§ï¼</strong> ç»¼è¿°è¡¨æï¼PAä½ä¸ºå¨éå¾®è°çè½»éçº§ä¸æææ¿ä»£æ¹æ¡ï¼å¨åç§åéå­¦ä¹ åºæ¯ï¼å¦æ°æ®ç¨ç¼ºãæ°æ®åå¸éå¹³ç¨³ãæ¨¡ååé¨ä¸å¯è®¿é®æè®¡ç®èµæºæéï¼ä¸­å±ç°åºæ¾èçæææ§ã
* <strong>VPä¸VPTçåºåï¼</strong> VPéè¿ä¿®æ¹è¾å¥ç©ºé´æ¥éåºæ¨¡åï¼èVPTåéè¿å¨æ¨¡ååé¨æ³¨å¥å¯å­¦ä¹ çæç¤ºä»¤çæ¥è°æ´æ¨¡åè¡ä¸ºãè¿ç§åºåå¯¹äºçè§£å®ä»¬çæçç¹ç¹åéç¨åºæ¯è³å³éè¦ã
* <strong>å¹¿æ³çåºç¨æ½åï¼</strong> PAå¨åºç¡CVä»»å¡ï¼å¦åå²ãå¾åæ¢å¤ãåç¼©ï¼åç¹å®é¢åï¼å¦å»çãæºå¨äººãé¥æï¼ä¸­é½æ¾ç¤ºåºå¼ºå¤§çæ½åï¼è½å¤æ¡¥æ¥é¢è®­ç»æ¨¡åä¸ä¸æ¸¸ä»»å¡ã
* <strong>å¯ä¿¡èµAIçè´¡ç®ï¼</strong> PAå¨æåæ¨¡åé²æ£æ§ãç¼è§£å¬å¹³æ§ä¸åè§ãä»¥åä¿ééç§ä¸å®å¨æ¹é¢åæ¥çéè¦ä½ç¨ï¼ä¸ºæå»ºå¯ä¿¡èµçAIç³»ç»æä¾äºè½»éçº§è§£å³æ¹æ¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
* <strong>è®­ç»å¼éä¸ç¨³å®æ§ï¼</strong> å°½ç®¡PAæé«äºåæ°æçï¼ä½è®­ç»å¼éï¼å¦è¶åæ°æç´¢ï¼åè®­ç»ç»æçä¸ç¨³å®æ§ä»ç¶æ¯ææã
* <strong>æ¨çå»¶è¿ï¼</strong> é¢å¤çæç¤ºç»ä»¶å¯è½å¢å æ¨çå»¶è¿ååå­æ¶èã
* <strong>çå®ä¸çç¯å¢è¯ä¼°ä¸è¶³ï¼</strong> å½åPAæ¹æ³çè¯ä¼°ä¸»è¦ä¾èµæ åååºåæ°æ®éï¼æªè½åååæ çå®ä¸çåºæ¯çå¤ææ§ååå¸æ¼ç§»ã
* <strong>çè®ºåææéï¼</strong> ç¸å³ççè®ºåæå¨å½åç ç©¶ç¤¾åºä¸­ä»ç¶æéï¼å¯¹PAå¦ä½å¼åæ¨¡åè¡ä¸ºååãæç¤ºå­¦ä¹ å°ä»ä¹ä»¥åå¨ä¸åéåºè®¾ç½®ä¸çæææ§ç­é®é¢ç¼ºä¹æ·±å¥çè§£ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
* <strong>è§£å³è®­ç»å¼éä¸ç¨³å®æ§ï¼</strong> éè¦è¿ä¸æ­¥ç ç©¶è®­ç»æ·å¾ãæ£æµå¹¶çº æ­£è®­ç»ä¸ç¨³å®æ§çç­ç¥ã
* <strong>ä¼åæ¨çå»¶è¿ï¼</strong> æ¢ç´¢åªæãç¥è¯è¸é¦ãéåååå­é«æå¾®è°ç­ææ¯ä»¥åå°æ¨çå»¶è¿ã
* <strong>å å¼ºçå®ä¸çè¯ä¼°ï¼</strong> ä¼ååå±è½å¤åºå¯¹å¤æ ·åãå¤æåå¼æè§è§ä¸ä¸æçé²æ£æ¹æ³ï¼ä»¥å¼¥åä¸çå®ä¸çé¨ç½²ä¹é´çå·®è·ã
* <strong>æ·±å¥çè®ºåæï¼</strong> è¿ä¸æ­¥æ¢ç´¢PAå¦ä½è¯±å¯¼æ¨¡åè¡ä¸ºååãè§è§æç¤ºå­¦ä¹ çåå®¹ä»¥åPAæ¹æ³å¨ä¸åéåºè®¾ç½®ä¸çæææ§ã
* <strong>å®å¨å¯¹é½ï¼</strong> å°å®å¨å¯¹é½ï¼åæ¬å¯è§£éæ§ãæ²»çåä¸¥æ ¼éªè¯ï¼èå¥PAæ¹æ³çå¼ååé¨ç½²ä¸­ï¼ä»¥åºå¯¹æ¶æè¡ä¸ºãæå®³åå®¹çæååè§æ¾å¤§ç­é®é¢ã
* <strong>æ··åæ¹æ³ï¼</strong> ç»åVPåVPTçä¼å¿ï¼ä¾å¦ä½¿ç¨çæå¨æä¾åå§ç©ºé´æç¤ºï¼åæ¶ä½¿ç¨æ¡ä»¶æç¥ä»¤çè¿è¡åé¨è°æ´ï¼ä»¥åºå¯¹ç¯å¢éåã</p>
<p>è¿ç¯ç»¼è¿°ä¸ºç ç©¶äººååå®è·µèæä¾äºä¸ä¸ªæ¸æ°çè·¯çº¿å¾ï¼ä»¥çè§£åæ¢ç´¢PAç¸å³ç ç©¶çä¸æ­åå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics.</li>
<li>Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13219v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13219v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12400v1'></a></p>
<h2 id="towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda"><a href="https://arxiv.org/abs/2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></h2>
<p><strong>Authors:</strong> AndrÃ© Torneiro, Diogo Monteiro, Paulo Novais, Pedro Rangel Henriques, Nuno F. Rodrigues</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Urban monitoring of public infrastructure (such as waste bins, road signs,
vegetation, sidewalks, and construction sites) poses significant challenges due
to the diversity of objects, environments, and contextual conditions involved.
Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.
This raises a critical question: Can machines now "see" like citizens and infer
informed opinions about the condition of urban infrastructure? Vision-Language
Models (VLMs), which integrate visual understanding with natural language
reasoning, have recently demonstrated impressive capabilities in processing
complex visual information, turning them into a promising technology to address
this challenge. This systematic review investigates the role of VLMs in urban
monitoring, with particular emphasis on zero-shot applications. Following the
PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021
and 2025 to address four core research questions: (1) What urban monitoring
tasks have been effectively addressed using VLMs? (2) Which VLM architectures
and frameworks are most commonly used and demonstrate superior performance? (3)
What datasets and resources support this emerging field? (4) How are VLM-based
applications evaluated, and what performance levels have been reported?</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºAndrÃ© Torneiroç­äººçè®ºæâTowards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agendaâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åå¸å¬å±åºç¡è®¾æ½ï¼å¦åå¾æ¡¶ãè·¯æ ãæ¤è¢«ãäººè¡éåå»ºç­å·¥å°ï¼çæµæé¢ä¸´çéå¤§ææãä¼ ç»çç©èç½ä¼ æå¨åäººå·¥æ£æ¥æ¹æ³ææ¬é«æãé¾ä»¥æ©å±ï¼ä¸å¾å¾ä¸å¸æ°çç´è§è§è§æç¥ä¸ç¬¦ãå æ­¤ï¼æ ¸å¿ç ç©¶é®é¢æ¯ï¼æºå¨è½å¦åå¸æ°ä¸æ ·âçâï¼å¹¶å¯¹åå¸åºç¡è®¾æ½çç¶åµå½¢æç¥æå¤æ­ï¼å·ä½èè¨ï¼è®ºæéè¿ç³»ç»ç»¼è¿°ï¼æ¢è®¨äºè§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨åå¸çæµä¸­çä½ç¨ï¼å°¤å¶ä¾§éäºé¶æ ·æ¬åºç¨ï¼å¹¶åç­äºä»¥ä¸åä¸ªæ ¸å¿é®é¢ï¼
1. VLMsææè§£å³äºåªäºåå¸çæµä»»å¡ï¼
2. åªäºVLMæ¶æåæ¡æ¶æå¸¸ç¨ä¸è¡¨ç°ä¼å¼ï¼
3. åªäºæ°æ®éåèµæºæ¯æè¿ä¸æ°å´é¢åï¼
4. åºäºVLMçåºç¨å¦ä½è¯ä¼°ï¼æ¥åäºåªäºæ§è½æ°´å¹³ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
* <strong>ç³»ç»ç»¼è¿°ä¸åç±»æ³ï¼</strong> è®ºæéç¨PRISMAæ¹æ³è®ºï¼å¯¹2021å¹´è³2025å¹´é´åè¡¨ç32ç¯åè¡è¯å®¡ç ç©¶è¿è¡äºåæãå¨æ­¤åºç¡ä¸ï¼æåºäºä¸å¥åè½æ§VLMåå¸åºç¨åç±»æ³ï¼å°ç°æç ç©¶ååä¸ºä¸ä¸ªä¸»è¦é¢åï¼ç®æ æ£æµä¸åå²ãåå¸è§åä¸åå°å©ç¨åç±»ãå¯¼èªä¸è·¯å¾è§åãäº¤éåæä¸è¿è¾ãåå¸åºæ¯çè§£ä¸æç¥ãå°çå®ä½ä¸ä½ç½®æ¥æ¾ãä»¥ååå¸çæ§ä¸å®å¨ã
* <strong>æ¨¡åçæç³»ç»åæï¼</strong> è®ºæå°æ¨¡åçæç³»ç»åä¸ºåç±»ï¼çº¯è§è§éª¨å¹²ç½ç»ãç¬ç«è¯­è¨æ¨¡åãå¤æ¨¡æVLMåæ··åéæï¼å¹¶åæäºå®ä»¬å¨åå¸åºç¨ä¸­çæä½æ¨¡å¼ãéæç­ç¥åå±éæ§ã
* <strong>æ°æ®éä½¿ç¨æ¨¡å¼åæï¼</strong> è®ºæè¯¦ç»åæäºåå¸çæµä¸­æ°æ®éçä½¿ç¨æåµï¼åæ¬è¡æ¯å¾åãåææ°æ®ãèªç©º/ä¿¯è§å¾ä»¥åä¸ææ°æ®éï¼æ­ç¤ºäºç°ææ°æ®éå¨æ³åè½åãå¯å¤ç°æ§åå®éåºç¨æ¹é¢çç»ææ§ç¼ºé·ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
* <strong>VLMå¨åå¸çæµä¸­çæ½åï¼</strong> VLMséè¿æ´åè§è§çè§£åèªç¶è¯­è¨æ¨çï¼å¨å¤çå¤æè§è§ä¿¡æ¯æ¹é¢å±ç°åºå¼ºå¤§è½åï¼ä¸ºè§£å³åå¸çæµæææä¾äºæåæ¯çææ¯ã
* <strong>æ§è½å¤æ ·æ§ï¼</strong> å¨ä¸åä»»å¡ä¸­ï¼VLMsè¡¨ç°åºä¸åçæ§è½ãä¾å¦ï¼å¨ç®æ æ£æµä¸­ï¼SAMåGrounding DINOç»ååå¾äºé«IoUåæ°ï¼å¨åå¸è§åä¸­ï¼UrbanCLIPå¨ä½å®åºåç±»ä¸­F1åæ°è¾¾å°0.82ï¼å¨å°çå®ä½ä¸­ï¼IM2Cityéè¿çº¿æ§æ¢æµè¾¾å°äº85.9%çTop-1åç¡®çã
* <strong>æ¨¡ååæ°æ®éè¶å¿ï¼</strong> CLIPãGrounding DINOåGPT-3.5æ¯æå¸¸ç¨çæ¨¡åï¼åæ äºå¯¹æ¨¡ååãéç¨éª¨å¹²ç½ç»çåå¥½ãè¡æ¯å¾åæ°æ®éï¼å¦Google Street ViewãMapillary Vistasï¼å æ®ä¸»å¯¼å°ä½ï¼ä½åææ°æ®éï¼å¦CARLAãSYNTHIAï¼å¨æ¨¡æç¨ææå±é©æ¡ä»¶æ¹é¢ä¹å¾å°å¹¿æ³åºç¨ã
* <strong>é¢åéåºä¸æ³åææï¼</strong> å°½ç®¡å¨æéçç£ä¸ï¼mIoUåä¸ä¸ææç¥åç¡®æ§æææé«ï¼ä½å¨è·¨åï¼å¦åæå°çå®åºæ¯ï¼åè·¨åå¸æ³åæ¹é¢ä»å­å¨æ¾èææã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
* <strong>è¯ä¼°æ åä¸ä¸è´ï¼</strong> æ§è½æ¥åå·®å¼å¤§ï¼ç¼ºä¹ç»ä¸çåºååè®®ãæ ååææ ãç½®ä¿¡åºé´åè¯¦ç»çéè¯¯åæï¼ä½¿å¾ä¸åç ç©¶é´çç´æ¥æ¯è¾å°é¾ã
* <strong>é¨ç½²å¯è¡æ§ä¸è¶³ï¼</strong> å¾å°æç ç©¶è¯ä¼°æ¨¡åçè¿è¡æ¶ãç¡¬ä»¶å¼å®¹æ§ãè½èæåå­å ç¨ç­é¨ç½²ç¸å³å ç´ ï¼ä¹æªååèèå¯¹ææ§æ»å»ãé®æ¡ææ¶é´æ¼ç§»çé²æ£æ§ã
* <strong>æ¨¡æå·®è·ä¸ä¸ä¸æç¼ºå¤±ï¼</strong> å¤§å¤æ°åå¸VLMç®¡éè¿åº¦ä¾èµéæå¾å-ææ¬å¯¹ï¼å¿½ç¥äºæ¶é´åºåãæ·±åº¦å¾ãå°çå®ä½åç¯å¢å£°é³ç­ä¸°å¯æ¨¡æï¼éå¶äºå¶å¨å¨æåå¸ç¯å¢ä¸­çæ¨çè½åã
* <strong>å¯¹èµæºå¯éåæ¶æçè¿åº¦ä¾èµï¼</strong> è®¸å¤æåè¿çVLMæ¨¡åè®¡ç®ææ¬é«æï¼ä¸éåå®æ¶ãç§»å¨æåµå¥å¼é¨ç½²ã
* <strong>ä¼¦çç²ç¹ä¸æ³å¾çå¿½ï¼</strong> å¾å°æç ç©¶å°ç®æ³å¬å¹³æ§ãç¥æåæãæ°æ®æ¥æºåéç§ä¿æ¤ç­ä¼¦çç»´åº¦æ´åå°æ¨¡åå¼åæµç¨ä¸­ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
* <strong>SLM-VLMæ··åæ¶æï¼</strong> ç»åå°åè¯­è¨æ¨¡åï¼SLMsï¼ä¸æ¨¡ååè§è§ç¼ç å¨åè§£ç å¨ï¼å®ç°é«æçå¤æ¨¡ææ¨çï¼éç¨äºè¾¹ç¼ç¡¬ä»¶é¨ç½²ã
* <strong>ç»ä¸çåå¸åºåï¼</strong> å¼åéæå¤è¯­è¨æç¤ºãå¤æ¨¡æä¼ æå¨æµï¼å¾åãè§é¢ãé³é¢ãLiDARï¼åæåå¤æ ·æ§å°çæ°æ®çè¯ä¼°å¥ä»¶ï¼ä»¥ç¡®ä¿å¯å¤ç°æ§ãè·¨åå¯æ¯æ§åé²æ£æ³åã
* <strong>ä»¥é¨ç½²ä¸ºä¸­å¿çè®¾è®¡ï¼</strong> å°ç¡¬ä»¶éå¶ãå»¶è¿è¦æ±ãç­é¢ç®åéç§èèç­é¨ç½²çº¦æåµå¥æ¨¡åå¼åå¨æã
* <strong>åµå¥å¼ä¼¦çä¸åè§æ§ï¼</strong> å°æåé²æ£æ§æ£æ¥ãç®æ³å¬å¹³æ§è¯ä¼°ãæ°æ®éåæè¿½è¸ªååè§å®¡è®¡æ´åå°æ ¸å¿åºåæµè¯åè¯ä¼°çå½å¨æä¸­ã
* <strong>å¯å¤ç°çå¼æ¾çæç³»ç»ï¼</strong> å¹å»éææåï¼éè¿çæ¬åæ°æ®éãDockerååºçº¿ãå¬å±æè¡æ¦åå±äº«è¯ä¼°ä»£ç ï¼ä¿è¿ç¤¾åºé©±å¨çå¡è®®ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæå¨é¢åé¡¾äºè§è§-è¯­è¨æ¨¡åå¨åå¸çæµé¢åçåºç¨ç°ç¶ï¼æ­ç¤ºäºå¶å·¨å¤§æ½åï¼åæ¶ä¹æåºäºå½åç ç©¶å¨æ³åè½åãé¨ç½²å¯è¡æ§åä¼¦çèéæ¹é¢çå±éæ§ï¼å¹¶æåºäºä¸ä¸ªå¤ç»´åº¦çç ç©¶è®®ç¨ï¼æ¨å¨æ¨å¨åå¸AIç³»ç»åæ´å¼ºå¤§ãå¯é¨ç½²ãåå®¹ãå¯è§£éåç¬¦åä¼¦ççæ¹ååå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12400v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12400v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13804v1'></a></p>
<h2 id="generative-universal-verifier-as-multimodal-meta-reasoner"><a href="https://arxiv.org/abs/2510.13804v1">Generative Universal Verifier as Multimodal Meta-Reasoner</a></h2>
<p><strong>Authors:</strong> Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xinchen Zhangç­äººæ°åçè®ºæâGenerative Universal Verifier as Multimodal Meta-Reasonerâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼å¤æ¨¡æåæ¨çå¨ââçæå¼éç¨éªè¯å¨</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä¸ä¸ä»£å¤æ¨¡ææ¨çä¸­è§è§ç»æéªè¯çæ ¹æ¬æ§ææãç°æçè§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨å¤æ¨¡ææ¨çåçæè¿ç¨ä¸­ï¼å¯¹è§è§ç»æè¿è¡å¯é çåå°åç»åè½åä¸è¶³ï¼å¯¼è´å¶å¨éªè¯è§è§è¾åºæ¹é¢ä¸äººç±»æ°´å¹³å­å¨æ¾èå·®è·ãå·ä½æ¥è¯´ï¼è®ºææ¢è®¨äºå¦ä½ç³»ç»å°è¯ä¼°VLMså¨è§è§ç»æéªè¯æ¹é¢çè¡¨ç°ï¼å¦ä½å¼åä¸ä¸ªå¼ºå¤§ççæå¼éç¨éªè¯å¨ï¼ä»¥åå¦ä½å©ç¨è§è§éªè¯æ¥å¢å¼ºæ¨çæçæè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ViVerBenchåºåçæå»ºï¼</strong> è®ºæå¼å¥äºä¸ä¸ªå¨é¢ä¸å·ææææ§çåºåViVerBenchï¼æ¶µç16ç±»å³é®ä»»å¡ï¼ç¨äºè¯ä¼°å¤æ¨¡ææ¨çä¸­çè§è§ç»æãè¯¥åºåéè¿äººå·¥æ æ³¨ç²¾å¿æå»ºï¼åå«3,594ä¸ªå¤æ ·åä¸å·ææææ§çéªè¯é®é¢ï¼è¦æ±æ¨¡åæä¾äºåå¤æ­åè¯¦ç»è§£éã
*   <strong>OmniVerifier-7Bçè®­ç»ä¸åå­è½åè¯å«ï¼</strong> è®ºæè®¾è®¡äºä¸¤ç§èªå¨åæ°æ®æå»ºæµç¨ï¼ç¨äºçæå¤§è§æ¨¡è§è§éªè¯æ°æ®ï¼å¹¶å¨æ­¤åºç¡ä¸è®­ç»äºOmniVerifier-7Bãè¿æ¯é¦ä¸ªä¸ºéç¨è§è§éªè¯èè®­ç»çå¨è½çæå¼éªè¯å¨ãéè¿è®­ç»ï¼è®ºæè¯å«åºè§è§éªè¯çä¸ç§åå­è½åï¼æ¾å¼å¯¹é½ãå³ç³»éªè¯åæ´åæ¨çï¼å¹¶å±ç¤ºäºå®ä»¬å¦ä½ååæ³ååäº¤äºã
*   <strong>OmniVerifier-TTSçæåºï¼</strong> è®ºææåºäºOmniVerifier-TTSï¼ä¸ç§é¡ºåºæµè¯æ¶ç¼©æ¾èå¼ãå®å©ç¨éç¨éªè¯å¨å¨ç»ä¸æ¨¡åä¸­æ¡¥æ¥å¾åçæåç¼è¾ï¼éè¿è¿­ä»£ç»ç²åº¦ä¼åæ¥æåçæè½åä¸éãè¯¥èå¼è¿æ©å±äºéç¨éªè¯å¨å¨æ´å¹¿æ³çä¸çå»ºæ¨¡äº¤éæ¨çåºæ¯ä¸­çåºç¨ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ç°æVLMsçä¸è¶³ï¼</strong> ViVerBenchä¸çå®éªç»æè¡¨æï¼ç°æVLMså¨è§è§ç»æéªè¯ä»»å¡ä¸è¡¨ç°ä¸ä½³ï¼ä¸äººç±»æ°´å¹³å­å¨æ¾èå·®è·ãå·ä½è¡¨ç°ä¸ºï¼å¨ç»ç²åº¦å¾å-æç¤ºå¯¹é½æ¹é¢çå¼±ç¹ãä¸çç¥è¯è¡¨ç¤ºä¸å¹éä»¥åè§è§æ¨çä»»å¡ä¸­æ¹è¯è½åä¸è¶³ã
*   <strong>OmniVerifier-7Bçåè¶æ§è½ï¼</strong> OmniVerifier-7Bå¨ViVerBenchä¸åå¾äºæ¾èçæ§è½æåï¼+8.3ï¼ï¼è¶è¶äºGPT-4oï¼å¹¶å¨æ¾å¼å¯¹é½åå³ç³»éªè¯ç­ä»»å¡ä¸è¡¨ç°åºæ¾èæ¹è¿ãè¿è¡¨æéè¿éå¯¹åå­è½åè¿è¡å¼ºåå­¦ä¹ è®­ç»ï¼å¯ä»¥ææå°æå»ºæ´å¼ºå¤§ãæ´å·æ³åæ§çè§è§éªè¯å¨ã
*   <strong>OmniVerifier-TTSççæå¢å¼ºï¼</strong> OmniVerifier-TTSå¨T2I-ReasonBenchï¼+3.7ï¼åGenEval++ï¼+4.3ï¼ä¸åå®ç°äºæ¹è¿ï¼ä¼äºç°æçå¹¶è¡æµè¯æ¶ç¼©æ¾æ¹æ³ï¼å¦Best-of-Nï¼ãè¿è¯æäºéè¿å¯é çè§è§éªè¯ï¼OmniVerifierè½å¤å®ç°çæè¿ç¨ä¸­çå¯é åå°åå¯æ©å±çæµè¯æ¶ç»åï¼ä»èæ¨å¨ä¸ä¸ä»£æ¨çç³»ç»æ´å¼å¾ä¿¡èµåå¯æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ä»»å¡æ³åè½åï¼</strong> æäºä»»å¡ï¼å¦è¿·å®«ï¼ç±äºé¢åå·®è·è¾å¤§ï¼æ³åææä¸ä½³ï¼éè¦ä»»å¡ç¹å®çæ°æ®è¿è¡ä¼åã
*   <strong>éª¨å¹²æ¨¡åçå½±åï¼</strong> OmniVerifier-TTSçæ§è½åå¶éª¨å¹²æ¨¡åçå½±åãç®åï¼ç»ä¸å¤æ¨¡ææ¨¡åå¯¹çææç¼è¾çå¾ååå¸ææï¼å¨å¤æ­¥èªç»åè¿ç¨ä¸­å¯è½è¡¨ç°åºå¼å¸¸è¡ä¸ºï¼ä¾å¦ï¼GPT-Image-1å¨è¿­ä»£ç¼è¾åå¾åäºçæåé»çå¾åï¼ãå°½ç®¡è¿äºä¼ªå½±ä¸å½±åéªè¯æ§è½ï¼ä½å®ä»¬æ¯éª¨å¹²æ¨¡åæ¬èº«çå±éæ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¢å¼ºéç¨éªè¯å¨çæ³åè½åï¼</strong> æªæ¥çå·¥ä½å°æ¢ç´¢è®­ç»åæ°æ®æå»ºç­ç¥ï¼ä»¥å¢å¼ºOmniVerifierçæ³åè½åï¼ä½¿å¶æ´æ¥è¿çæ­£çéç¨éªè¯å¨ã
*   <strong>æ¹è¿å¤æ¨¡æåè®­ç»ï¼</strong> è¿ä¸æ­¥æ©å±éç¨éªè¯å¨ï¼å¹¶ç ç©¶å¶å¨æ¹è¿å¤æ¨¡æåè®­ç»æ¹é¢çæ½åã
*   <strong>è§£å³éª¨å¹²æ¨¡åçé£æ ¼ä¸è´æ§é®é¢ï¼</strong> é¼å±è¿ä¸æ­¥åªåï¼å¨å¤æ­¥èªç»åä¸å¢å¼ºé£æ ¼ä¸è´æ§ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥çæå¼éç¨éªè¯å¨ï¼ä¸ºå¤æ¨¡ææ¨çåçæé¢åå¸¦æ¥äºéè¦çè¿å±ãå®ä¸ä»æä¾äºä¸ä¸ªè¯ä¼°ç°ææ¨¡åè½åçå¨é¢åºåï¼è¿æåºäºä¸ç§ææçæ¹æ³æ¥è®­ç»è½å¤è¿è¡å¯é è§è§éªè¯çæ¨¡åï¼å¹¶å°å¶åºç¨äºå¢å¼ºçæè´¨éåæ¨çè½åï¼ä¸ºæå»ºæ´æºè½ãæ´å¯æ§çAIç³»ç»å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.</li>
<li>This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning.</li>
<li>Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13804v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13804v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13802v1'></a></p>
<h2 id="trace-anything-representing-any-video-in-4d-via-trajectory-fields"><a href="https://arxiv.org/abs/2510.13802v1">Trace Anything: Representing Any Video in 4D via Trajectory Fields</a></h2>
<p><strong>Authors:</strong> Xinhang Liu, Yuxi Xiao, Donny Y. Chen, Jiashi Feng, Yu-Wing Tai, Chi-Keung Tang, Bingyi Kang</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Effective spatio-temporal representation is fundamental to modeling,
understanding, and predicting dynamics in videos. The atomic unit of a video,
the pixel, traces a continuous 3D trajectory over time, serving as the
primitive element of dynamics. Based on this principle, we propose representing
any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame. With this
representation, we introduce Trace Anything, a neural network that predicts the
entire trajectory field in a single feed-forward pass. Specifically, for each
pixel in each frame, our model predicts a set of control points that
parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at
arbitrary query time instants. We trained the Trace Anything model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
our new benchmark for trajectory field estimation and performs competitively on
established point-tracking benchmarks; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.
Project page: https://trace-anything.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Trace Anything: Representing Any Video in 4D via Trajectory Fields</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§æ°é¢çè§é¢è¡¨ç¤ºæ¹æ³âââè½¨è¿¹åºâï¼Trajectory Fieldï¼ï¼å®å°è§é¢ä¸­çæ¯ä¸ªåç´ æ å°ä¸ºä¸ä¸ªè¿ç»­ç3Dæ¶é´è½¨è¿¹å½æ°ãåºäºæ­¤ï¼ä½èå¼å¥äºâTrace Anythingâç¥ç»ç½ç»ï¼è¯¥ç½ç»è½å¤éè¿åæ¬¡ååä¼ æ­é¢æµæ´ä¸ªè½¨è¿¹åºï¼ä»èå®ç°å¯¹è§é¢ä¸­ä»»ä½åç´ å¨ä»»ææ¶é´ç¹ç3Dä½ç½®çè¿½è¸ªåé¢æµã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>æ ¸å¿åæ°å¨äºå°è§é¢çåå­ååï¼åç´ ï¼è§ä¸ºå¨æ¶é´ä¸è¿ç»­ç3Dè½¨è¿¹ï¼å¹¶æåºäºä¸ç§<strong>è½¨è¿¹åº</strong>çè¡¨ç¤ºèå¼ãä¼ ç»æ¹æ³éå¸¸å³æ³¨ç¦»æ£çç¹å¾ç¹è¿½è¸ªæåæµä¼°è®¡ï¼èè½¨è¿¹åºåæä¾äºä¸ç§<strong>ç¨ å¯ä¸è¿ç»­ç4Dï¼3Dç©ºé´+1Dæ¶é´ï¼è¡¨ç¤º</strong>ã</p>
<p>å·ä½æ¹æ³è®ºæ¯ï¼
*   <strong>è½¨è¿¹åºè¡¨ç¤ºï¼</strong> å°è§é¢ä¸­çæ¯ä¸ªåç´ å¨æ¯ä¸ªå¸§ä¸­é½å³èä¸ä¸ªè¿ç»­ç3Dè½¨è¿¹å½æ°ã
*   <strong>åæ°åè½¨è¿¹ï¼</strong> ä½¿ç¨ä¸ç»æ§å¶ç¹æ¥åæ°åè¿äºè½¨è¿¹ï¼ä¾å¦ï¼Bæ ·æ¡ï¼ï¼ä½¿å¾æ¨¡åè½å¤é¢æµä»»ææ¥è¯¢æ¶é´ç¹ç3Dä½ç½®ã
*   <strong>åæ¬¡ååä¼ æ­é¢æµï¼</strong> âTrace Anythingâç¥ç»ç½ç»éè¿ä¸æ¬¡ååä¼ æ­ç´æ¥é¢æµææåç´ çè½¨è¿¹æ§å¶ç¹ï¼é¿åäºè¿­ä»£ä¼åæä¾èµè¾å©ä¼°è®¡å¨ï¼æ¾èæé«äºæçã
*   <strong>å¤§è§æ¨¡4Dæ°æ®è®­ç»ï¼</strong> æ¨¡åå¨åæ¬èªå»ºå¹³å°æ°æ®å¨åçå¤§è§æ¨¡4Dæ°æ®ä¸è¿è¡è®­ç»ï¼è¿æç¤ºäºå¯¹é«è´¨éãç¨ å¯è½¨è¿¹æ æ³¨æ°æ®çéæ±åè·åè½åã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>ç»ä¸çè§é¢è¡¨ç¤ºï¼</strong> è½¨è¿¹åºæä¾äºä¸ç§æ´åºç¡ãæ´ç»ä¸çè§é¢å¨æè¡¨ç¤ºï¼å¯è½æä¸ºæªæ¥è§é¢çè§£åçæä»»å¡çåºç³ã</li>
<li><strong>æçæåï¼</strong> åæ¬¡ååä¼ æ­çèå¼æ¾èæé«äºè½¨è¿¹é¢æµçæçï¼ä½¿å¶æ´éç¨äºå®æ¶åºç¨æå¤§è§æ¨¡è§é¢å¤çã</li>
<li><strong>æ°è½åæ¶ç°ï¼</strong> æè¦ä¸­æå°çâç®æ æ¡ä»¶æä½âãâè¿å¨é¢æµâåâæ¶ç©ºèåâç­æ°å´è½åï¼è¡¨æè¿ç§ç¨ å¯ãè¿ç»­çè½¨è¿¹è¡¨ç¤ºè½å¤è§£éæ´é«çº§çè§é¢çè§£åäº¤äºä»»å¡ï¼è¶è¶äºä¼ ç»ç¹è¿½è¸ªæåæµçèç´ã</li>
<li><strong>æ°çåºååç ç©¶æ¹åï¼</strong> å¼å¥æ°çè½¨è¿¹åºä¼°è®¡åºåï¼å°æ¨å¨è¯¥é¢åçç ç©¶è¿å±ï¼å¹¶å¯è½æ¿åæ´å¤åºäºè½¨è¿¹åºçæ°ç®æ³ååºç¨ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>è§é¢çè§£ä¸åæï¼</strong> æ´ç²¾ç¡®çè¿å¨çè§£ãè¡ä¸ºè¯å«ãäºä»¶æ£æµã</li>
<li><strong>è§é¢çæä¸ç¼è¾ï¼</strong> è¿å¨é£æ ¼è¿ç§»ãè§é¢æå¸§ãç©ä½ç§»é¤ä¸å¡«åãè§é¢åå®¹åä½ã</li>
<li><strong>æºå¨äººå­¦ä¸èªä¸»é©¾é©¶ï¼</strong> ç®æ è·è¸ªãè¿å¨é¢æµãè·¯å¾è§åãåºæ¯çè§£ã</li>
<li><strong>å¢å¼ºç°å®/èæç°å® (AR/VR)ï¼</strong> åºæ¯éå»ºãè¿å¨è¿½è¸ªãèæç©ä½ä¸çå®åºæ¯çèåã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å¨å®è¿å¨è¿½è¸ªãç»èå¨åå­¦åæã</li>
<li><strong>ç©çæ¨¡æä¸å¨ç»ï¼</strong> æ´çå®çç©ä½è¿å¨æ¨¡æãè§è²å¨ç»ã</li>
<li><strong>äººæºäº¤äºï¼</strong> æå¿è¯å«ãç¼å¨è¿½è¸ªã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>æ°æ®ä¾èµæ§ï¼</strong> æè¦å¼ºè°äºå¨âå¤§è§æ¨¡4Dæ°æ®âä¸è¿è¡è®­ç»ï¼åæ¬âæ°å¹³å°âçæ°æ®ãè¿æç¤ºäºé«è´¨éãç¨ å¯ãå¸¦æ3Dè½¨è¿¹æ æ³¨çæ°æ®éå¯¹äºæ¨¡åè®­ç»è³å³éè¦ï¼è·ååæ æ³¨è¿ç±»æ°æ®å¯è½æ¯ä¸ä¸ªå·¨å¤§çææåææ¬ã</li>
<li><strong>è®¡ç®å¤æåº¦ï¼</strong> å°½ç®¡æ¯åæ¬¡ååä¼ æ­ï¼ä½ä¸ºè§é¢ä¸­âæ¯ä¸ªåç´ å¨æ¯ä¸ªå¸§âé¢æµâä¸ç»æ§å¶ç¹âæ¥åæ°åè½¨è¿¹ï¼å¶è®¡ç®éååå­æ¶èå¯è½ä»ç¶éå¸¸å¤§ï¼å°¤å¶å¯¹äºé«åè¾¨çãé¿æ¶åºè§é¢ã</li>
<li><strong>è½¨è¿¹çå¹³æ»æ§ååç¡®æ§ï¼</strong> Bæ ·æ¡ç­åæ°åæ¹æ³å¨è¡¨ç¤ºå¤æãéåä½æå¿«éååçè¿å¨æ¶ï¼å¶åç¡®æ§åç»èææè½åå¯è½åå°éå¶ãæ¨¡åé¢æµçæ§å¶ç¹æ°éåè½¨è¿¹çé¶æ°ä¼å½±åå¶è¡¨è¾¾è½åã</li>
<li><strong>æ³åè½åï¼</strong> æ¨¡åå¨ç¹å®å¤§è§æ¨¡4Dæ°æ®ä¸è®­ç»ï¼å¶å¨ä¸è®­ç»æ°æ®åå¸å·®å¼è¾å¤§ççå®ä¸çå¤æåºæ¯ï¼å¦é®æ¡ãåç§ååãæ¨¡ç³ãå¿«éè¿å¨ï¼ä¸çæ³åè½åæå¾è¿ä¸æ­¥éªè¯ã</li>
<li><strong>âæ°å´è½åâçå®ç°ç»èï¼</strong> æè¦ä¸­æå°çâç®æ æ¡ä»¶æä½âãâè¿å¨é¢æµâç­è½åï¼å¶å·ä½å®ç°æºå¶ï¼ä¾å¦ï¼å¦ä½å°ç®æ æ¡ä»¶èå¥è½¨è¿¹é¢æµï¼å¨æè¦ä¸­å¹¶æªè¯¦ç»è¯´æï¼å¯è½éè¦é¢å¤çæ¨¡åæè®­ç»ç­ç¥ã</li>
</ul>
<hr />
<p>æ»çæ¥è¯´ï¼è¿ç¯è®ºææåºäºä¸ç§éå¸¸å·æåç»æ§åæ½åçè§é¢è¡¨ç¤ºæ¹æ³ï¼å°è§é¢çè§£æ¨åäºæ´æ·±å±æ¬¡ç4Dæ¶ç©ºè¿ç»­æ§ãå¶åæ¬¡ååä¼ æ­çæçä¼å¿åæå±ç°åºçæ°å´è½åï¼ä½¿å¶å¨è®¡ç®æºè§è§é¢åå·æéè¦çç ç©¶ä»·å¼åå¹¿æ³çåºç¨åæ¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on this principle, we propose representing
any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame.</li>
<li>With this
representation, we introduce Trace Anything, a neural network that predicts the
entire trajectory field in a single feed-forward pass.</li>
<li>We trained the Trace Anything model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
our new benchmark for trajectory field estimation and performs competitively on
established point-tracking benchmarks; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13802v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13802v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13795v1'></a></p>
<h2 id="bee-a-high-quality-corpus-and-full-stack-suite-to-unlock-advanced-fully-open-mllms"><a href="https://arxiv.org/abs/2510.13795v1">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a></h2>
<p><strong>Authors:</strong> Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæãBee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMsãçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå®å¨å¼æºå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼å¨æ§è½ä¸è½åäºä¸ææ¨¡åçæ ¸å¿é®é¢ãä½èæåºï¼è¿ç§å·®è·ä¸»è¦æºäºçç£å¾®è°ï¼SFTï¼é¶æ®µæ°æ®è´¨éçæ¾èä¸è¶³ï¼å·ä½è¡¨ç°ä¸ºç°æå¼æºæ°æ®éæ®éå­å¨çåªå£°åå¤ææ¨çæ°æ®ï¼å¦æç»´é¾CoTï¼çä¸¥éç¼ºä¹ï¼è¿é»ç¢äºæ¨¡åé«çº§è½åçå¼åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼è®ºææåºäºä¸é¡¹ä¸»è¦è´¡ç®ï¼
*   <strong>Honey-Data-15M æ°æ®éï¼</strong> å¼å¥äºä¸ä¸ªæ°çSFTæ°æ®éï¼åå«çº¦1500ä¸ä¸ªé®ç­å¯¹ãè¯¥æ°æ®éç»è¿å¤éæ¸æ´ææ¯å¤çï¼å¹¶éè¿æ°é¢çåå±ï¼ç­CoTåé¿CoTï¼CoTå¯éç­ç¥è¿è¡å¢å¼ºï¼ä»¥æä¾ä¸åæ·±åº¦çæ¨çè·¯å¾ã
*   <strong>HoneyPipe æ°æ®ç­å±æµç¨ä¸ DataStudio æ¡æ¶ï¼</strong> æåºäºä¸ä¸ªéæä¸å¯éåºçæ°æ®ç­å±æ¹æ³è®ºï¼è¶è¶äºéææ°æ®éåå¸æ¨¡å¼ãHoneyPipeå©ç¨MLLMsèªå¨åæ´ä¸ªç­å±å·¥ä½æµï¼ä»æ¸æ´å°å¯éï¼ä¸ºå¼æºç¤¾åºæä¾äºä¸ç§å¯æ©å±ä¸ç»æµé«æçé«è´¨éæ°æ®æå»ºæ¹æ¡ã
*   <strong>Bee-8B æ¨¡åï¼</strong> è®­ç»äºä¸ä¸ª8Båæ°æ¨¡åBee-8Bï¼ç¨äºéªè¯Honey-Data-15Mæ°æ®éåHoneyPipeæµç¨çæææ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½çªç ´ï¼</strong> å®éªç»æè¡¨æï¼Bee-8Bå¨å®å¨å¼æºMLLMsä¸­å»ºç«äºæ°çæåè¿ï¼SOTAï¼æ§è½ï¼å¹¶ä¸å¨æäºæåµä¸çè³è¶è¶äºInternVL3.5-8Bç­è¿æåå¼æºæ¨¡åã
*   <strong>æ°æ®ç­å±çæææ§ï¼</strong> å¹¿æ³çæ¶èç ç©¶è¯å®ï¼æ°æ®ç­å±è¿ç¨ï¼åæ¬åªå£°è¿æ»¤åCoTå¯éï¼å¯¹æ¨¡åæ§è½æåå·ææ¾èå½±åï¼å°¤å¶æ¯å¨æ¨çå¯éååºåæµè¯ä¸ãè¿ç´æ¥éªè¯äºé«è´¨éæ°æ®ç­ç¥å¨å¼¥åå®å¨å¼æºä¸åå¼æºMLLMsæ§è½å·®è·æ¹é¢çå³é®ä½ç¨ã
*   <strong>èµæºåå¸ï¼</strong> è®ºæåç¤¾åºæä¾äºåæ¬Honey-Data-15Mè¯­æåºãåå«HoneyPipeåDataStudioçå¨æ å¥ä»¶ãè®­ç»æ¹æ¡ãè¯ä¼°å·¥å·ä»¥åæ¨¡åæéå¨åçåºç¡èµæºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æªæç¡®æåå½åå·¥ä½çå·ä½å±éæ§ï¼ä½å¶æ ¸å¿ç ç©¶é®é¢æ¬èº«å°±æç¤ºäºç°æå¼æºMLLMså¨æ°æ®è´¨éåå¤ææ¨çè½åä¸çä¸è¶³ãæ­¤å¤ï¼è®ºæå¼ºè°äºæ°æ®ç­å±çææ¬æçåå¯æ©å±æ§ï¼è¿å¯è½é´æ¥è¯´æäºå¤§è§æ¨¡äººå·¥æ æ³¨çå±éæ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææç¡®æåºï¼æªæ¥çç ç©¶æ¹ååºç»§ç»­å³æ³¨æ°æ®è´¨éï¼å¹¶éè¿éæãå¯å¤ç°çæ¹æ³è¿è¡æ°æ®ç­å±ï¼èéä»ä»è¿½æ±æ°æ®éãè¿ä¸ºå¼æºç¤¾åºå¼åå·æé«åº¦ç«äºåçMLLMsææäºæ¹åãæ­¤å¤ï¼DataStudioæ¡æ¶ççµæ´»æ§ä¹ä¸ºç¤¾åºæä¾äºä¸æ­è¿­ä»£åæ¹è¿æ°æ®ç­å±æ¹æ³çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Addressing these challenges, our work makes three primary
contributions.</li>
<li>First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy.</li>
<li>Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases.</li>
<li>Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13795v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13795v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13778v1'></a></p>
<h2 id="internvla-m1-a-spatially-guided-vision-language-action-framework-for-generalist-robot-policy"><a href="https://arxiv.org/abs/2510.13778v1">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a></h2>
<p><strong>Authors:</strong> Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, Yangkun Zhu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine <code>where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide</code>how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯è®ºæâInternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policyâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³éç¨æºå¨äººé¢åçä¸ä¸ªæ ¸å¿ææï¼å¦ä½ä½¿æºå¨äººè½å¤çè§£é«çº§è¯­è¨æä»¤ï¼å¹¶å°å¶è½¬åä¸ºå¨3Dç©çä¸çä¸­ç²¾ç¡®ãå·èº«åçå¨ä½ãç°ææ¹æ³å¨å°ææ¬æ½è±¡çæä»¤ä¸å®éçãè¿ç»­çæºå¨äººå¨ä½èç³»èµ·æ¥æ¶å­å¨æ ¹æ¬æ§å·®è·ï¼å°¤å¶æ¯å¨é¢å¯¹å¤§è§æ¨¡ãå¤æ ·åä»»å¡æ¶ï¼é¾ä»¥å®ç°å¯æ©å±çãéç¨çæºå¨äººæºè½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
InternVLA-M1å¼å¥äºä¸ä¸ªç»ä¸çãç©ºé´å¼å¯¼çè§è§-è¯­è¨-å¨ä½ï¼VLAï¼æ¡æ¶ï¼å¶æ ¸å¿åæ°å¨äºâç©ºé´å¼å¯¼çè§è§-è¯­è¨-å¨ä½è®­ç»âèå¼ãè¯¥æ¡æ¶éç¨ä¸¤é¶æ®µè®­ç»æµç¨ï¼
*   <strong>ç©ºé´æ¥å°é¢è®­ç»ï¼Spatial Grounding Pre-trainingï¼ï¼</strong> å¨è¶è¿230ä¸çç©ºé´æ¨çæ°æ®ä¸è¿è¡é¢è®­ç»ï¼ä»¥ç¡®å®âå¨åªéè¡å¨âï¼where to actï¼ï¼éè¿å°æä»¤ä¸è§è§çãä¸å·èº«æ å³çä½ç½®å¯¹é½ãè¿å»ºç«äºä¸ä¸ªå¯è¿ç§»çç©ºé´åéªç¥è¯ã
*   <strong>ç©ºé´å¼å¯¼çå¨ä½åè®­ç»ï¼Spatially Guided Action Post-trainingï¼ï¼</strong> éè¿å³æå³ç¨çç©ºé´æç¤ºï¼spatial promptingï¼çæå·èº«æç¥çå¨ä½ï¼ä»¥å³å®âå¦ä½è¡å¨âï¼how to actï¼ãè¿ç§æ¹æ³å°ç©ºé´åéªç¥è¯è½¬åä¸ºå·ä½çè¿å¨æ§å¶ã
*   <strong>åç³»ç»æ¶æï¼</strong> InternVLA-M1éç¨åç³»ç»æ¶æï¼åå«ä¸ä¸ªVLMè§åå¨ï¼ä½ä¸ºæ¢éä½å¯é çSystem 2æ¨çå¨ï¼åä¸ä¸ªå¨ä½ä¸å®¶ï¼ä½ä¸ºå¿«éçSystem 1æ§å¶å¨ï¼ãVLMè§åå¨éè¿ç©ºé´æç¤ºçææ½å¨è§åä»¤çï¼æå¯¼å¨ä½ä¸å®¶çææ§å¶ä¿¡å·ã
*   <strong>å¤§è§æ¨¡åææ°æ®å¼æï¼</strong> æå»ºäºä¸ä¸ªå¯æ©å±çæ¨¡æå¼æï¼æ¶éäº24.4ä¸ä¸ªå¯æ³åçæå-æ¾ç½®ï¼pick-and-placeï¼ä»»å¡çæ®µï¼ä»¥è¿ä¸æ­¥æ©å±æä»¤éµå¾ªè½åï¼å¹¶å¢å¼ºè§è§å¤æ ·æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
InternVLA-M1å¨å¤é¡¹åºåæµè¯åçå®ä¸çåºæ¯ä¸­å±ç°åºæ¾èçæ§è½æååæ³åè½åï¼
*   <strong>SimplerEnvåºåï¼</strong> å¨SimplerEnv Google Robotä¸ï¼æ§è½æ¯æ²¡æç©ºé´å¼å¯¼çåä½æé«äº14.6%ï¼å¨WidowXä¸æé«äº17%ï¼å¨LIBERO Frankaä¸æé«äº4.3%ãåæ¶ï¼å¨çå­ãç¹åè½¨è¿¹é¢æµæ¹é¢å±ç¤ºäºæ´å¼ºçç©ºé´æ¨çè½åã
*   <strong>æ³åæå-æ¾ç½®ä»»å¡ï¼</strong> å¨200ä¸ªä»»å¡å3000å¤ä¸ªå¯¹è±¡ä¸ï¼å¹³åæ§è½æé«äº6.2%ã
*   <strong>çå®ä¸çåºæ¯ï¼</strong> å¨çå®ä¸ççèç±»æå-æ¾ç½®ä»»å¡ä¸­ï¼æ§è½æé«äº7.3%ï¼éè¿åææ°æ®ååè®­ç»ï¼å¨æªè§å¯¹è±¡åæ°é¢éç½®ä¸å®ç°äº20.6%çæåã
*   <strong>é¿æ¶ç¨æ¨çä»»å¡ï¼</strong> å¨é¿æ¶ç¨ãæ¨çå¯éååºæ¯ä¸­ï¼æ§è½è¶è¶ç°æå·¥ä½10%ä»¥ä¸ã
è¿äºç»æå¼ºè°äºç©ºé´å¼å¯¼è®­ç»ä½ä¸ºå®ç°å¯æ©å±åå¼¹æ§éç¨æºå¨äººç»ä¸ååçæææ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
è®ºæä¸­æ²¡ææç¡®ååºInternVLA-M1çå±éæ§ãç¶èï¼ä»å¶æ¹æ³è®ºåå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼
*   <strong>æ°æ®ä¾èµæ§ï¼</strong> å°½ç®¡ä½¿ç¨äºå¤§è§æ¨¡åææ°æ®ï¼ä½æ¨¡åçæ§è½ä»ç¶ä¾èµäºè®­ç»æ°æ®çè´¨éåå¤æ ·æ§ãå¯¹äºæäºé«åº¦å¤æçãæªååè¡¨ç¤ºççå®ä¸çåºæ¯ï¼å¯è½ä»å­å¨æ³åææã
*   <strong>è®¡ç®èµæºéæ±ï¼</strong> æ¨¡åçè®­ç»éè¦16åNVIDIA A100 GPUï¼è¿è¡¨æå¶è®­ç»è¿ç¨å¯¹è®¡ç®èµæºè¦æ±è¾é«ã
*   <strong>æ¨çéåº¦ï¼</strong> å°½ç®¡éè¿FlashAttentionåKVç¼å­è¿è¡äºä¼åï¼VLMç»ä»¶çæ¨çéåº¦çº¦ä¸º10 FPSï¼å¯¹äºæäºéè¦æä½å»¶è¿çå®æ¶äº¤äºä»»å¡ï¼å¯è½ä»ææåç©ºé´ã
*   <strong>ä»»å¡èå´ï¼</strong> å°½ç®¡å¨æå-æ¾ç½®ãæåºåé¿æ¶ç¨æä½ä»»å¡ä¸­è¡¨ç°åºè²ï¼ä½å¶å¨æ´å¹¿æ³ãæ´å¼æ¾çæºå¨äººä»»å¡ï¼ä¾å¦ï¼éè¦å¤æç©çäº¤äºæç²¾ç»æä½çä»»å¡ï¼ä¸­çè¡¨ç°ä»éè¿ä¸æ­¥æ¢ç´¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºæä¸­æ²¡ææç¡®ååºæªæ¥ç ç©¶æ¹åï¼ä½åºäºå¶è´¡ç®åæ½å¨å±éæ§ï¼å¯ä»¥æ¨æ­åºä»¥ä¸æ¹åï¼
*   <strong>æ´å¹¿æ³çä»»å¡æ³åï¼</strong> æ¢ç´¢InternVLA-M1å¨æ´å¤æ ·åãæ´å¤æçæºå¨äººä»»å¡ä¸­çåºç¨ï¼åæ¬éè¦æ´ç²¾ç»è¿å¨æ§å¶ãæ´å¤æç©çäº¤äºææ´æ½è±¡æ¨ççä»»å¡ã
*   <strong>æçä¼åï¼</strong> è¿ä¸æ­¥ä¼åæ¨¡åçè®­ç»åæ¨çæçï¼ä½¿å¶è½å¤å¨æ´å°çè®¡ç®èµæºä¸è¿è¡ï¼å¹¶å®ç°æ´å¿«çå®æ¶ååºã
*   <strong>æ°æ®æçï¼</strong> ç ç©¶å¦ä½éè¿æ´å°çæ°æ®ï¼å°¤å¶æ¯çå®ä¸çæ°æ®ï¼å®ç°ç¸ä¼¼çæ§è½ï¼ä¾å¦éè¿æ´åè¿çèªçç£å­¦ä¹ æé¢åéåºææ¯ã
*   <strong>å¯è§£éæ§åé²æ£æ§ï¼</strong> æ·±å¥ç ç©¶ç©ºé´å¼å¯¼è®­ç»å¦ä½å¢å¼ºæ¨¡åçå¯è§£éæ§åå¨æªç¥ç¯å¢ä¸­çé²æ£æ§ï¼å¹¶æ¢ç´¢å¦ä½è¿ä¸æ­¥æåè¿äºç¹æ§ã
*   <strong>å¤æ¨¡æèåï¼</strong> æ¢ç´¢é¤äºè§è§åè¯­è¨ä¹å¤ï¼ç»åæ´å¤æ¨¡æï¼å¦è§¦è§ãå¬è§ï¼æ¥å¢å¼ºç©ºé´æ¥å°åå¨ä½çæã
*   <strong>æç»­å­¦ä¹ åéåºï¼</strong> ç ç©¶å¦ä½ä½¿InternVLA-M1è½å¤å¨æ°ç¯å¢ä¸­æç»­å­¦ä¹ åéåºï¼èæ éä»å¤´å¼å§éæ°è®­ç»ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence.</li>
<li>This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction.</li>
<li>In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13778v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13778v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13759v1'></a></p>
<h2 id="uni-mmmu-a-massive-multi-discipline-multimodal-unified-benchmark"><a href="https://arxiv.org/abs/2510.13759v1">Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark</a></h2>
<p><strong>Authors:</strong> Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Unified multimodal models aim to jointly enable visual understanding and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
conceptual understanding to guide precise visual synthesis, or (ii) utilize
generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
incorporates verifiable intermediate reasoning steps, unique ground truths, and
a reproducible scoring protocol for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯è®ºæâUni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmarkâçå¨é¢æè¦ï¼</p>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢</strong>
å½åç»ä¸å¤æ¨¡ææ¨¡åæ¨å¨åæ¶å®ç°è§è§çè§£åçæï¼ä½ç°æåºåæµè¯æªè½ååè¯ä¼°è¿ä¸¤ç§è½åä¹é´ççæ­£æ´åãç°æè¯ä¼°è¦ä¹å­¤ç«å°å¤çè¿ä¸¤ç§è½åï¼è¦ä¹å¿½è§äºå®ä»¬ä¹é´åºæçè¦åä»»å¡ãè®ºææ¨å¨è§£å³è¿ä¸è¯ä¼°ç©ºç½ï¼ç³»ç»å°æ¢ç©¶çæä¸çè§£ä¹é´çååååä½ç¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
*   <strong>Uni-MMMUåºåçæåºï¼</strong> è®ºææåºäºUni-MMMUï¼ä¸ä¸ªå¨é¢ä¸è·¨å­¦ç§çåºåï¼ç³»ç»å°æ­ç¤ºäºçæä¸çè§£å¨å«ä¸ªä»¥æ¨çä¸ºä¸­å¿çé¢åï¼åæ¬ç§å­¦ãç¼ç ãæ°å­¦åè°é¢ï¼ä¸­çååååä½ç¨ãæ¯ä¸ªä»»å¡é½æ¯ååè¦åçï¼è¦æ±æ¨¡åå©ç¨æ¦å¿µçè§£æå¯¼ç²¾ç¡®çè§è§åæï¼æå©ç¨çæä½ä¸ºåææ¨ççè®¤ç¥æ¯æ¶ã
*   <strong>åå±è¯ä¼°åè®®ï¼</strong> Uni-MMMUæ´åäºå¯éªè¯çä¸­é´æ¨çæ­¥éª¤ãç¬ç¹ççå®å¼ä»¥åå¯éç°çææ¬åè§è§è¾åºè¯ååè®®ãè¿ä½¿å¾å¯¹æç»ç»æåä¸­é´æ­¥éª¤è¿è¡åå±è¯ä¼°æä¸ºå¯è½ï¼ä»èå®ç°ç»ç²åº¦çéè¯¯å½å ã
*   <strong>èªå¨ååå¯éç°çè¯ä¼°æµç¨ï¼</strong> è¯ä¼°æµç¨éç¨ç¨åºåè§£æå¨ãæç¥åº¦éåLLM-as-a-Judgeç¸ç»åçæ¹å¼ï¼ç¡®ä¿äºå®¢è§ãä¸è´åå¯è§£éçç»æã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>æ§è½å·®å¼åè·¨æ¨¡æä¾èµæ§ï¼</strong> å¯¹æåè¿çç»ä¸æ¨¡åãä»çææ¨¡ååä»çè§£æ¨¡åè¿è¡å¹¿æ³è¯ä¼°åï¼æ­ç¤ºäºæ¾èçæ§è½å·®å¼åè·¨æ¨¡æä¾èµæ§ã
*   <strong>ååä½ç¨çæææ§ï¼</strong> ç»æè¡¨æï¼çæä¸çè§£ä¹é´çååä½ç¨å¨å·æä¸¥æ ¼é»è¾ä¾èµæ§çä»»å¡ä¸­æä¸ºææãå³ä½¿æ¯ä¸å®ç¾çä¸­é´çæç»æï¼ä¹è½æ¾èæé«æç»åç¡®æ§ã
*   <strong>å½åæ¨¡åçå±éæ§ï¼</strong> åææ­ç¤ºäºå½åç»ä¸æ¨¡åå­å¨ææ¾ä¸å¹³è¡¡ï¼å®ä»¬ä¸¥éååçè§£è½åï¼èçæè½åæ¯ä¸»è¦ç¶é¢ãå¸¸è§çå¤±è´¥ç¹åæ¬ä¸ç²¾ç¡®çå¾åç¼è¾ãç¤ºæå¾çåæä»¥åç»ç²åº¦çç©ºé´æ¨çã
*   <strong>è¯ä¼°æ¹æ³çæææ§ï¼</strong> LLM-as-a-Judgeè¯ä¼°ç»ä»¶çæææ§éè¿ä¸äººç±»æ æ³¨èåæ´å¼ºå¤§çåä¸æ¨¡åï¼Gemini-2.5-proï¼çæ¯è¾å¾å°éªè¯ï¼Cohen's Kappaç³»æ°å¨0.6å°0.8ä¹é´ï¼è¡¨æé«åº¦ä¸è´æ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
*   <strong>ä»»å¡èå´ï¼</strong> Uni-MMMUçä»»å¡ä¸»è¦éä¸­å¨å·æç¡®å®æ§åå¯éªè¯è§£å³æ¹æ¡çæ¨çä¸­å¿å­¦ç§ï¼è¿ä½¿å¾è¯ä¼°å®¢è§ä¸å¯éç°ï¼ä½æªè½æ¶µçæ´å¹¿æ³ççå®ä¸çåºæ¯ï¼ä¾å¦éè¦å¼æ¾å¼åé åãä¸»è§å¤æ­æç»å¾®å¸¸è¯æ¨ççä»»å¡ã
*   <strong>éæå¾åéå¶ï¼</strong> å½ååºåå®å¨åºäºéæå¾åï¼æªæ¥å·¥ä½å¯ä»¥æ©å±å°æ¶åè§é¢æé¿ææ¶é´äº¤äºçä»»å¡ã
*   <strong>æ°æ®ç­å±æ¹æ³ï¼</strong> è¿·å®«å¯¼èªãæ»å¨æ¼å¾åä»£ç æ¸²æç­ä»»å¡éç¨ç¨åºåçæï¼è¿ç¡®ä¿äºå¯ä¸è§£å³æ¹æ¡åå®¢è§è§£æï¼ä½å¯è½å¯¼è´æ°æ®ç¼ºä¹çå®ä¸çå¾åçå¤ææ§ãåªå£°åè§è§å¤æ ·æ§ãç§å­¦ä»»å¡éç¨LLMé©±å¨çæµç¨åæå¨ç­å±ï¼å¯è½å¼å¥çææ¨¡åæäººç±»ç­å±èçç»å¾®åå·®ã
*   <strong>è¯ä¼°ç®¡éçå±éæ§ï¼</strong> æäºä»»å¡ä¾èµäºâæ¨¡åå³è¯å¤èâæ¡æ¶ï¼å°½ç®¡å·²éªè¯å¶ä¸äººç±»æ æ³¨èçé«åº¦ä¸è´æ§ï¼ä½è¿äºè¯å¤æ¨¡åå¹¶éä¸æ ä¸å¤±ï¼å¯è½å­å¨èªèº«çåè§æç¥è¯ç©ºç½ï¼ä»èå½±åè¯ä¼°åç¡®æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>å¢å¼ºçæè½åï¼</strong> è§£å³å½åç»ä¸æ¨¡åå¨çææ¹é¢çç¶é¢ï¼ç¹å«æ¯å¾åç¼è¾ãç¤ºæå¾åæåç»ç²åº¦ç©ºé´æ¨çã
*   <strong>æ´ç´§å¯çæ§å¶æ§ï¼</strong> æ¢ç´¢æ´ç´§å¯çæ§å¶æºå¶ï¼ä¾å¦ï¼ç¨åºæçº¦æå¼å¯¼ççæï¼ãè·¨ç¼è¾çæ´å¼ºç©ºé´/ç¶æä¸åæ§ï¼ä»¥åä½¿å¯æ§è¡ä¸­é´è¡¨ç¤ºæä¸ºæ¨ç-çæå¾ªç¯ä¸­çä¸ç­å¬æ°çæ¥å£ã
*   <strong>æ©å±ä»»å¡èå´ï¼</strong> å°åºåæ©å±å°éè¦å¼æ¾å¼åé åãä¸»è§å¤æ­æç»å¾®å¸¸è¯æ¨çççå®ä¸çåºæ¯ã
*   <strong>å¼å¥è§é¢åæ¶é´äº¤äºï¼</strong> å°è¯ä¼°æ©å±å°æ¶åè§é¢æé¿ææ¶é´äº¤äºçä»»å¡ã
*   <strong>æ¹è¿è¯ä¼°æ¹æ³ï¼</strong> è¿ä¸æ­¥å®åâæ¨¡åå³è¯å¤èâæ¡æ¶ï¼åå°æ½å¨åå·®ï¼æé«è¯ä¼°åç¡®æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.</li>
<li>Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13759v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13759v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13747v1'></a></p>
<h2 id="interactiveomni-a-unified-omni-modal-model-for-audio-visual-multi-turn-dialogue"><a href="https://arxiv.org/abs/2510.13747v1">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</a></h2>
<p><strong>Authors:</strong> Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Wenwen Tongç­äººæ°åçè®ºæâInteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogueâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åçå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼å¨å¤çå¤æãå¤è½®çé³é¢-è§è§äº¤äºä»»å¡æ¶ï¼ç¼ºä¹ç±»äººï¼human-likeï¼çé¿æå¯¹è¯è½ååæ ç¼éæç¨æ·ä½éªãå®ä»¬ä¸»è¦å³æ³¨åè½®çè§£è½åï¼æªè½æä¾ç«¯å°ç«¯çå¨æ¨¡æè¾å¥çè§£åè¯­é³ååºçæè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»ä¸çå¨æ¨¡ææ¨¡åæ¶æï¼</strong> InteractiveOmniæ¯ä¸ä¸ªç»ä¸çãå¼æºçå¨æ¨¡æå¤§è¯­è¨æ¨¡åï¼è½å¤åæ¶æ¥æ¶å¾åãé³é¢ãææ¬åè§é¢ç­å¨æ¨¡æè¾å¥ï¼å¹¶ç´æ¥çæè¿è´¯çææ¬åè¯­é³æµï¼å®ç°çæ­£çéæå¤è½®äº¤äºãå®éæäºè§è§ç¼ç å¨ãé³é¢ç¼ç å¨ãå¤§è¯­è¨æ¨¡ååè¯­é³è§£ç å¨ã
*   <strong>å¤é¶æ®µè®­ç»ç­ç¥ï¼</strong> éç¨å¨æ¨¡æé¢è®­ç»ï¼ç¨äºè·¨æ¨¡æçè§£ï¼ååè®­ç»ï¼ç¨äºè¯­é³å¯¹è¯åé³é¢-è§è§äº¤äºï¼çå¤é¶æ®µè®­ç»ç­ç¥ï¼ä»¥ç¡®ä¿å¼ºå¤§çè·¨æ¨¡æè½åã
*   <strong>ç²¾å¿ç­åçå¤è½®è®­ç»æ°æ®éï¼</strong> ä¸ºäºå®ç°ç±»äººçé¿æå¯¹è¯è½åï¼è®ºæç²¾å¿ç­åäºä¸ä¸ªå¤è½®è®­ç»æ°æ®éï¼ä»¥å¢å¼ºæ¨¡åå¤çå¤æå¤è½®äº¤äºçè½åã
*   <strong>æ°åå¤è½®åºåæµè¯ï¼</strong> æå»ºäºå¤æ¨¡æå¤è½®è®°å¿åºåï¼MMMBï¼åå¤è½®è¯­é³äº¤äºåºåï¼MSIBï¼ï¼ä»¥ææè¯ä¼°å¤è½®è®°å¿åè¯­é³äº¤äºè½åï¼å¼¥è¡¥ç°æåºåçä¸è¶³ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> InteractiveOmniå¨å¾åãé³é¢ãè§é¢çè§£åè¯­é³çæä»»å¡ä¸­ï¼ç¸å¯¹äºåç­è§æ¨¡çæ¨¡ååå¾äºæåè¿ï¼state-of-the-artï¼çç»æã
*   <strong>è½»éçº§æ¨¡åçç«äºåï¼</strong> InteractiveOmni-4Bå¨éç¨åºåä¸ä¸æ´å¤§çQwen2.5-Omni-7Bæ¨¡åç¸å½ï¼å¹¶ä¸å¨æ¨¡åå°ºå¯¸ä»ä¸ºä¸åçæåµä¸ï¼ä»è½ä¿æInteractiveOmni-8B 97%çæ§è½ã
*   <strong>å¼ºå¤§çé¿æè®°å¿è½åï¼</strong> å®éªè¯æInteractiveOmniæ¾èä¼äºé¢åçå¼æºæ¨¡åï¼æä¾äºæ´æºè½çå¤è½®é³é¢-è§è§ä½éªï¼å°¤å¶å¨é¿æè®°å¿è½åæ¹é¢è¡¨ç°çªåºã
*   <strong>å¼æºåºç¡ï¼</strong> InteractiveOmniä½ä¸ºä¸ä¸ªå¯è®¿é®çå¼æºåºç¡æ¨¡åï¼ä¸ºä¸ä¸ä»£æºè½äº¤äºç³»ç»å¥ å®äºåºç¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææè¦ä¸­å¹¶æªæç¡®æåInteractiveOmniæ¨¡åçå·ä½å±éæ§ãç¶èï¼ä»å¶ç ç©¶èæ¯åç®æ æ¥çï¼å½åMLLMsçæ®éå±éæ§ï¼å¦å¤æ¨¡æå¯¹é½çå¤ææ§ãç«¯å°ç«¯ç»ä¸çè§£åçææ¡æ¶çæå»ºææãä»¥åå¤è½®å¯¹è¯ä¸­é¿æè®°å¿åææè¡¨è¾¾çä¸è¶³ï¼æ¯InteractiveOmniæ¨å¨è§£å³çé®é¢ãå æ­¤ï¼å¯ä»¥æ¨æ­è¿äºæ¯ç°æææ¯é¢ä¸´çææï¼èInteractiveOmniæ­£åªååæå®ä»¬ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æé«æ¨¡åæçï¼</strong> å¢å¼ºæ¨¡åå¨å®æ¶äº¤äºä¸­çæçã
*   <strong>æ©å±çè§£å¤ææ½è±¡è·¨æ¨¡æå³ç³»çè½åï¼</strong> è¿ä¸æ­¥æ©å±æ¨¡åçè§£æ´å¤æãæ´æ½è±¡çè·¨æ¨¡æå³ç³»çè½åï¼ä¸ºæ´çå®ãæ´ç±»äººçç¨æ·ä½éªéºå¹³éè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities.</li>
<li>Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities.</li>
<li>Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13747v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13747v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13740v1'></a></p>
<h2 id="multi-scale-high-resolution-logarithmic-grapher-module-for-efficient-vision-gnns"><a href="https://arxiv.org/abs/2510.13740v1">Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</a></h2>
<p><strong>Authors:</strong> Mustafa Munir, Alex Zhang, Radu Marculescu</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Vision graph neural networks (ViG) have demonstrated promise in vision tasks
as a competitive alternative to conventional convolutional neural nets (CNN)
and transformers (ViTs); however, common graph construction methods, such as
k-nearest neighbor (KNN), can be expensive on larger images. While methods such
as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step
scale can lead to over-squashing and missing multiple connections to gain the
same information that could be gained from a long-range link. Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network. Extensive experiments show that
LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,
GMACs, and parameters on image classification and semantic segmentation tasks.
Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Mustafa MunirãAlex ZhangåRadu Marculescuæ°åçè®ºæãMulti-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNsãçå¨é¢æè¦ã</p>
<hr />
<h3 id="multi-scale-high-resolution-logarithmic-grapher-module-for-efficient-vision-gnns_1">è®ºææè¦ï¼Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æè§è§å¾ç¥ç»ç½ç»ï¼ViGï¼å¨å¤çé«åè¾¨çå¾åæ¶é¢ä¸´çè®¡ç®æçåä¿¡æ¯ä¸¢å¤±é®é¢ãä¼ ç»çå¾æå»ºæ¹æ³ï¼å¦Kè¿é»ï¼KNNï¼ï¼å¨å¤çå¤§åå¾åæ¶è®¡ç®ææ¬é«æãèéæå¾æå»ºæ¹æ³ï¼å¦ç¨çè§è§å¾æ³¨æåï¼SVGAï¼ï¼è½ç¶ç®åäºè®¡ç®ï¼ä½å¶åºå®çæ­¥é¿å°ºåº¦ä¼å¯¼è´âè¿åº¦åç¼©âï¼over-squashingï¼ï¼å³å¨é¿è·ç¦»è¿æ¥ä¸­ä¸¢å¤±éè¦ä¿¡æ¯ï¼ä»¥åæ æ³ææè·åå¨å±ä¸ä¸æãè¿éå¶äºViGå¨è®¡ç®æºè§è§ä»»å¡ä¸­çæ§è½åå¯æ©å±æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäºä»¥ä¸å³é®åæ°ï¼</p>
<ul>
<li><strong>å¯¹æ°å¯ä¼¸ç¼©å¾æå»ºï¼Logarithmic Scalable Graph Construction, LSGCï¼ï¼</strong> éå¯¹KNNåSVGAçå±éæ§ï¼LSGCæåºäºä¸ç§æ°çå¾æå»ºæ¹æ³ãå®å©ç¨å¾åå°ºå¯¸çä½æ·±åº¦ï¼bit-depthï¼è¿è¡å¯¹æ°ç¼©æ¾ï¼èéçº¿æ§ç¼©æ¾ãè¿ä½¿å¾å¾æå»ºå¨å¤çé«åè¾¨çå¾åæ¶è½å¤çææ´å°çè¿æ¥ï¼ä»èåè½»è¿åº¦åç¼©å¹¶éä½è®¡ç®å¤æåº¦ï¼åæ¶éè¿ä¼åèèè¿è·ç¦»è¿æ¥æ¥ä¿æå±é¨æ§ï¼å¹¶éè¿é¿è·ç¦»è¿æ¥å»ºç«å¨å±ä¸ä¸æã</li>
<li><strong>LogViG æ··åCNN-GNN æ¨¡åï¼</strong> è®ºæå¼å¥äºä¸ç§æ°é¢çæ··åCNN-GNNæ¶æLogViGï¼å®éæäºLSGCãè¯¥æ¨¡åå¨ææåä¸ªé¶æ®µé½ä½¿ç¨äºå·ç§¯å±åå¾å±ï¼ä»¥å®ç°å±é¨åå¨å±å¤çã</li>
<li><strong>å¤å°ºåº¦é«åè¾¨çæ¶æï¼</strong> LogViGåé´äºå¤å°ºåº¦åé«åè¾¨çæ¶æçæåç»éªï¼å¼å¥äºä¸ä¸ªé«åè¾¨çåæ¯ï¼å¹¶éè¿é«åè¾¨çå¿«æ·è¿æ¥ï¼High-Resolution Shortcut, HRSï¼å¨ä¸ååè¾¨çåæ¯ä¹é´èåç¹å¾ï¼ä»¥å®ç°å¤å°ºåº¦é«åè¾¨çè§è§GNNç½ç»ãHRSéè¿ä¸¤ä¸ª3x3å·ç§¯ï¼æ­¥é¿åå«ä¸º2å1ï¼å°é«åè¾¨çç¹å¾æ³¨å¥æ¨¡åçåæé¶æ®µï¼å¹¶éè¿åçº¿æ§æå¼ãéç¹å·ç§¯åç¹å¾æ±åæ¥èåä½åè¾¨çåé«åè¾¨çç¹å¾ã</li>
<li><strong>ç½ç»æ·±åº¦ä¸å®½åº¦ä¼åï¼</strong> è®ºæéè¿å®éªè¯æï¼æ´æ·±æ´çªçç½ç»æ¶æï¼å¦Ti-LogViGï¼ç¸æ¯æ´å®½æ´æµçç½ç»è½å¸¦æ¥æ´å¥½çæ§è½æåã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
è®ºæéè¿å¨ImageNet-1Kå¾ååç±»åADE20Kè¯­ä¹åå²ä»»å¡ä¸çå¹¿æ³å®éªï¼å±ç¤ºäºLogViGçä¼è¶æ§è½ï¼</p>
<ul>
<li><strong>å¾ååç±»æ§è½ï¼</strong> æå°æ¨¡åTi-LogViGå¨ImageNet-1Kä¸å®ç°äº79.9%çå¹³åTop-1åç¡®çï¼æ åå·®Â±0.2%ï¼ï¼æ¯ç°æVision GNNé«åº1.7%ï¼åæ¶åæ°åå°äº24.3%ï¼GMACsåå°äº35.3%ãS-LogViGå¨åæ°åGMACsç¸ä¼¼çæåµä¸ï¼æ§è½ä¼äºHRViT-b2ãDeiTåPViGç­æ¨¡åãB-LogViGä¹æ¾èä¼äºEfficientFormerç³»åæ¨¡åã</li>
<li><strong>è¯­ä¹åå²æ§è½ï¼</strong> S-LogViGå¨ADE20Kä¸è¡¨ç°ä¼äºPoolFormer-S12ãFastViT-SA12ãEfficientFormer-L1åMobileViG-Mï¼mIoUåå«é«åº6.9ã6.1ã5.2å2.3ãB-LogViGä¹ä¼äºFastViT-SA36åEfficientFormer-L3ã</li>
<li><strong>æçæåï¼</strong> LogViGå¨åç¡®æ§ãGMACsååæ°æ¹é¢åä¼äºç°æçViGãCNNåViTæ¶æï¼è¯æäºLSGCåæ··åæ¶æå¨ViGè®¾è®¡ä¸çæ¾èè¿æ­¥ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®ååºLogViGæLSGCçå±éæ§ãç¶èï¼å¯ä»¥ä»å¶è®¾è®¡åæ¯è¾ä¸­æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼</p>
<ul>
<li><strong>è®¡ç®å¤ææ§ï¼</strong> å°½ç®¡LSGCæ¨å¨éä½å¾æå»ºçè®¡ç®ææ¬ï¼ä½GNNåºæçå¾æä½ä»ç¶å¯è½æ¯çº¯CNNæViTå¨æäºåºæ¯ä¸æ´å¤æï¼å°¤å¶æ¯å¨éå¸¸è§ç¡¬ä»¶ä¸ã</li>
<li><strong>è¶åæ°æææ§ï¼</strong> LSGCä¸­çæ©å±çKä»¥åLogViGæ¶æä¸­çå¶ä»è®¾è®¡éæ©ï¼å¦åé¶æ®µçééç»´åº¦ååæ°éï¼å¯è½éè¦ä»ç»è°æ´ä»¥è¾¾å°æä½³æ§è½ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡å¨ImageNet-1KåADE20Kä¸è¡¨ç°åºè²ï¼ä½LogViGå¨å¶ä»æ´å¹¿æ³ææ´å·æææ§çè§è§ä»»å¡ä¸çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½åºäºå¶è´¡ç®ï¼å¯ä»¥æ¨æ­åºä»¥ä¸å ç¹ï¼</p>
<ul>
<li><strong>LSGCçè¿ä¸æ­¥ä¼åï¼</strong> æ¢ç´¢LSGCä¸­å¯¹æ°ç¼©æ¾ç­ç¥çæ´é«çº§å½¢å¼ï¼æèç»åèªéåºæºå¶ï¼ä½¿å¶è½å¤æ ¹æ®å¾ååå®¹æä»»å¡éæ±å¨æè°æ´è¿æ¥ã</li>
<li><strong>LogViGæ¶æçæ©å±ï¼</strong> å°LogViGåºç¨äºæ´å¹¿æ³çè®¡ç®æºè§è§ä»»å¡ï¼å¦ç®æ æ£æµãå®ä¾åå²æè§é¢çè§£ï¼å¹¶æ¢ç´¢å¶å¨è¿äºä»»å¡ä¸­çæ§è½ã</li>
<li><strong>è½»éååé¨ç½²ï¼</strong> è¿ä¸æ­¥ä¼åLogViGæ¨¡åï¼ä½¿å¶å¨èµæºåéçè®¾å¤ï¼å¦ç§»å¨è®¾å¤ï¼ä¸æ´é«æå°è¿è¡ï¼å¯è½éè¿åªæãéåæç¥è¯è¸é¦ç­ææ¯ã</li>
<li><strong>çè®ºåæï¼</strong> å¯¹LSGCçå¾å±æ§ï¼å¦è¿éæ§ãç´å¾ï¼è¿è¡æ´æ·±å¥ççè®ºåæï¼ä»¥æ´å¥½å°çè§£å¶å¨ä¿¡æ¯ä¼ æ­åå¨å±ä¸ä¸ææè·æ¹é¢çä¼å¿ã</li>
<li><strong>ä¸å¶ä»å¾æå»ºæ¹æ³çç»åï¼</strong> æ¢ç´¢å°LSGCä¸å¶ä»åè¿çå¾æå»ºææ³¨æåæºå¶ç¸ç»åï¼ä»¥è¿ä¸æ­¥æåæ§è½æè§£å³ç¹å®ææã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links.</li>
<li>To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC.</li>
<li>Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network.</li>
<li>Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13740v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13740v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.13721v1'></a></p>
<h2 id="next-omni-towards-any-to-any-omnimodal-foundation-models-with-discrete-flow-matching"><a href="https://arxiv.org/abs/2510.13721v1">NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</a></h2>
<p><strong>Authors:</strong> Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2025-10-15</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms. By leveraging metric-induced probability paths and kinetic optimal
velocities, NExT-OMNI natively supports any-to-any understanding and generation
with enhanced response efficiency, while enabling broader application scenarios
through concise unified representations rather than task-decoupled designs.
Trained on large-scale interleaved text, image, video, and audio data,
NExT-OMNI delivers competitive performance on multimodal generation and
understanding benchmarks, while outperforming prior unified models in
multi-turn multimodal interaction and cross-modal retrieval, highlighting its
architectural advantages as a next-generation multimodal foundation model. To
advance further research, we release training details, data protocols, and
open-source both the code and model checkpoints.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Run Luoç­äººæ°åçè®ºæâNExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matchingâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>å½åå¤æ¨¡æåºç¡æ¨¡åå¨å®ç°ä»»ææ¨¡æé´ççæåå¤è½®äº¤äºæ¹é¢é¢ä¸´ææãç°ææ¨¡åå¤§å¤åéäºèªåå½ï¼ARï¼æ¶æï¼è¿å¯¼è´çè§£åçæè½åé¾ä»¥å¹³è¡¡æ´åãå°½ç®¡æ··ååè§£è¦ç­ç¥å·²è¢«æ¢ç´¢ï¼ä½å¶åä½ãééæè®¾è®¡éå¶äºå®ä»¬å¨è·¨æ¨¡ææ£ç´¢ç­æ´å¹¿æ³åºæ¯ä¸­çåºç¨ãè®ºææ¨å¨è§£å³è¿ä¸é®é¢ï¼æåºä¸ä¸ªè½å¤ç»ä¸å»ºæ¨¡ãå®ç°ä»»ææ¨¡æçè§£åçæï¼å¹¶æåååºæççå¤æ¨¡æåºç¡æ¨¡åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<ul>
<li><strong>å¼å¥NEXT-OMNIæ¨¡åï¼</strong> è®ºææåºäºNEXT-OMNIï¼ä¸ä¸ªåºäºç¦»æ£æµå¹éï¼DFMï¼ææ¯çå¼æºå¨æ¨¡æåºç¡æ¨¡åï¼é¦æ¬¡å®ç°äºä»»ææ¨¡æé´ççæï¼å¹¶å·ææ´å¿«çæ¨çéåº¦ã</li>
<li><strong>ç»ä¸å»ºæ¨¡ä¸ç¦»æ£æµå¹éèå¼ï¼</strong> NEXT-OMNIéè¿ç¦»æ£æµå¹éèå¼å®ç°ç»ä¸å»ºæ¨¡ï¼å©ç¨åº¦éè¯±å¯¼æ¦çè·¯å¾åå¨åå­¦æä¼éåº¦ï¼åçæ¯æä»»ææ¨¡æççè§£åçæï¼æé«äºååºæçã</li>
<li><strong>éå»ºå¢å¼ºçç»ä¸è¡¨ç¤ºï¼</strong> æ¨¡åè®¾è®¡äºå¸¦æä¸­é´ç¹å¾èåçéå»ºå¢å¼ºç»ä¸è¡¨ç¤ºï¼è¿ä¸ä»å®ç°äºç²¾ç¡®çè·¨æ¨¡ææ£ç´¢ï¼è¿æ¯æå¤è½®ä»»ææ¨¡æäº¤äºï¼é¿åäºä»»å¡è§£è¦çè®¾è®¡ã</li>
<li><strong>å¨æé¿åº¦çæç­ç¥ä¸èªéåºç¼å­ï¼</strong> ä¸ºäºæåçè§£ä»»å¡çæ§è½åå éæ¨çï¼æ¨¡åå¼å¥äºå¨æé¿åº¦çæç­ç¥åèªéåºç¼å­è®¾è®¡ï¼æ¾èæé«äºææ¬çæè½ååæ¨çéåº¦ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<ul>
<li><strong>åè¶çæ§è½ï¼</strong> NEXT-OMNIå¨å¤æ¨¡æçæåçè§£åºåæµè¯ä¸­è¡¨ç°åºç«äºæ§æåè¶çæ§è½ï¼å¹¶å¨å¤è½®å¤æ¨¡æäº¤äºåè·¨æ¨¡ææ£ç´¢æ¹é¢è¶è¶äºç°æç»ä¸æ¨¡åãä¾å¦ï¼å¨OmniBenchãWorldSenseåAV-Odysseyç­å¨æ¨¡æçè§£åºåæµè¯ä¸­ï¼NEXT-OMNIçå¹³åæ§è½æ¯OpenOmnié«åº3.2ä¸ªç¾åç¹ã</li>
<li><strong>æ¶æä¼å¿ï¼</strong> å®éªç»æçªæ¾äºDFMæ¶æä½ä¸ºä¸ä¸ä»£å¤æ¨¡æåºç¡æ¨¡åçä¼å¿ï¼å°¤å¶æ¯å¨å¤çå¤æçå¤æ¨¡æäº¤äºåè·¨æ¨¡ææ£ç´¢ä»»å¡æ¶ã</li>
<li><strong>æ´å¿«çæ¨çéåº¦ï¼</strong> ç»åå¹¶è¡è§£ç åèªéåºç¼å­æºå¶ï¼NEXT-OMNIçæ¨çååºéåº¦æ¯ARæ¶ææé«äº1.2åã</li>
<li><strong>ç»ä¸å»ºæ¨¡çæ½åï¼</strong> ç»æéªè¯äºåºäºDFMçæ¶æå¨ç»ä¸å¤æ¨¡æå»ºæ¨¡æ¹é¢çå·¨å¤§æ½åï¼è½å¤å®ç°æ´å¹¿æ³çåºç¨åºæ¯ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<ul>
<li><strong>èµæºéå¶ï¼</strong> ç±äºèµæºéå¶ï¼æ¨¡åä»å¨7Båæ°è§æ¨¡å2T tokenæ°æ®ä¸è¿è¡è®­ç»åéªè¯ãå æ­¤ï¼NEXT-OMNIçå¨é¨æ½åå°æªå®å¨å±ç°ï¼å°¤å¶æ¯å¨ç¼ºä¹ç¸åºå¤§åè¯­è¨æ¨¡ååºç¡æ¯æçæåµä¸ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<ul>
<li><strong>æ©å±åºç¨åºæ¯ï¼</strong> æªæ¥ç ç©¶è®¡åå°NEXT-OMNIæ©å±å°æ´å¹¿æ³çé¢åï¼ä¾å¦è§è§-è¯­è¨-å¨ä½æ¨¡åä¸­çå¨ä½è½¨è¿¹çæï¼ä»¥åç©çAIçè§£ä¸­çè§é¢çæï¼å¶ä¸­è§è§çæå¯ä»¥è¾å©ç©çæç¥ã</li>
<li><strong>æ¨¡åè§æ¨¡åï¼</strong> è¿ä¸æ­¥æ¢ç´¢æ¨¡åå¨æ´å¤§è§æ¨¡æ°æ®ååæ°ä¸çå¯æ©å±æ§ï¼ä»¥åååæ¥DFMæ¶æçæ½åã</li>
<li><strong>âä¸çå¤§èâæ¿æ¯ï¼</strong> è®ºæå±æç»ä¸å¤æ¨¡ææ¨¡åå°æä¸ºä¸ç°å®ä¸çäº¤äºçâä¸çå¤§èâï¼éè¿æç»­çå¤æ¨¡ææ°æ®äº¤äºä¸æ­å¢å¼ºå¶éç¨è½åï¼æç»å®ç°éç¨äººå·¥æºè½ï¼AGIï¼ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.13721v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.13721v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-16 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
