<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-07 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-06/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-08/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-07">Arxiv Computer Vision Papers - 2025-10-07</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-lmm-post-training-a-deep-dive-into-video-reasoning-with-large-multimodal-models" class="nav-link">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a>
                </li>
                <li class="nav-item">
                    <a href="#bridging-text-and-video-generation-a-survey" class="nav-link">Bridging Text and Video Generation: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-based-hashing-for-ann-search-foundations-and-early-advances" class="nav-link">Learning-Based Hashing for ANN Search: Foundations and Early Advances</a>
                </li>
                <li class="nav-item">
                    <a href="#pulp-motion-framing-aware-multimodal-camera-and-human-motion-generation" class="nav-link">Pulp Motion: Framing-aware multimodal camera and human motion generation</a>
                </li>
                <li class="nav-item">
                    <a href="#segmast3r-geometry-grounded-segment-matching" class="nav-link">SegMASt3R: Geometry Grounded Segment Matching</a>
                </li>
                <li class="nav-item">
                    <a href="#benthicat-an-opti-acoustic-dataset-for-advancing-benthic-classification-and-habitat-mapping" class="nav-link">BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping</a>
                </li>
                <li class="nav-item">
                    <a href="#progressive-gaussian-transformer-with-anisotropy-aware-sampling-for-open-vocabulary-occupancy-prediction" class="nav-link">Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#exposureengine-oriented-logo-detection-and-sponsor-visibility-analytics-in-sports-broadcasts" class="nav-link">ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts</a>
                </li>
                <li class="nav-item">
                    <a href="#object-centric-representation-learning-for-enhanced-3d-scene-graph-prediction" class="nav-link">Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#id-consistent-precise-expression-generation-with-blendshape-guided-diffusion" class="nav-link">ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-07">Arxiv Computer Vision Papers - 2025-10-07</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ6æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éææ¡å³é®ä¿¡æ¯ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ¥åæ§è¡æè¦ (2025å¹´10æ06æ¥)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæéå±ç°äºè®¡ç®æºè§è§é¢åå ä¸ªæ´»è·ä¸ç¸äºå³èçè¶å¿ï¼</p>
<ul>
<li><strong>å¤æ¨¡æå¤§æ¨¡å (LMM) çæ·±å¥æ¢ç´¢ä¸åºç¨ï¼</strong> å¤ç¯è®ºæèç¦äºè§é¢ä¸ææ¬ç­å¤æ¨¡ææ°æ®çèåï¼ç¹å«æ¯å¤§åå¤æ¨¡ææ¨¡åå¨è§é¢çè§£ãçæåæ¨çæ¹é¢çåºç¨ï¼æ¾ç¤ºåºLMMå¨å¤æåºæ¯çè§£ä¸­çå·¨å¤§æ½åã</li>
<li><strong>3D åºæ¯çè§£ä¸è¡¨ç¤ºï¼</strong> 3D æ°æ®å¤çãåºæ¯å¾é¢æµãå ç¨é¢æµä»¥åä¸å ä½ç¸å³çå¹éä»ç¶æ¯éè¦ç ç©¶æ¹åï¼å¼ºè°äºä»2Då¾ååæ´ä¸°å¯3Dä¸ççè§£çæ¼è¿ã</li>
<li><strong>çææ¨¡åä¸åå®¹åä½ï¼</strong> è§é¢çæãäººä½ä¸ç¸æºè¿å¨çæãä»¥åç²¾ç»åçäººè¸è¡¨æçæç­åå®¹åä½æ¹åæç»­åå±ï¼çææ¨¡åå¨æ§å¶æ§ãçå®æåä¸è´æ§æ¹é¢ä¸æ­æåã</li>
<li><strong>ç¹å®åºç¨ä¸æ°æ®éï¼</strong> éå¯¹æ°´ä¸ç¯å¢åç±»ãä½è²èµäºåæç­ç¹å®åºç¨åºæ¯çæ°æ®éæå»ºåæ¹æ³å¼åï¼ä½ç°äºCVææ¯å¨å®éé®é¢è§£å³ä¸­çè½å°ã</li>
<li><strong>åºç¡ææ¯ä¼åï¼</strong> æ£åï¼Hashingï¼å¨è¿ä¼¼æè¿é»æç´¢ï¼ANNï¼ä¸­çåºç¨ï¼ä»¥åå ä½å¹éç­åºç¡ç®æ³çæ¹è¿ï¼ä¸ºä¸å±åºç¨æä¾äºæ´é«æææ´é²æ£çæ¯æã</li>
</ul>
<p><strong>2. æ¾èæåæ°æ§è®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models" (Yunlong Tang et al.)ï¼</strong> è¿ç¯è®ºææ·±å¥æ¢è®¨äºè§é¢æ¨çä¸­å¤§åå¤æ¨¡ææ¨¡åçåè®­ç»ç­ç¥ï¼å¯è½æ­ç¤ºäºå¦ä½æææåLMMå¨å¤æè§é¢çè§£ä»»å¡ä¸çæ§è½ï¼å¯¹äºLMMçå®éåºç¨å·ææå¯¼æä¹ã</li>
<li><strong>"Pulp Motion: Framing-aware multimodal camera and human motion generation" (Robin Courant et al.)ï¼</strong> åæ°æ§å°ç»åäºç¸æºåäººä½è¿å¨çæï¼å¹¶èèäºâæå¾æç¥âï¼è¿å¯¹äºçµå½±å¶ä½ãèæç°å®åå¨ç»ç­é¢åçåå®¹åä½å·æçªç ´æ§ï¼è½çææ´èªç¶ãæ´å·åäºæçå¨æåºæ¯ã</li>
<li><strong>"Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction" (Chi Yan, Dan Xu)ï¼</strong> ç»åäºTransformeråé«æ¯è¡¨ç¤ºï¼å¹¶å¼å¥äºååå¼æ§éæ ·ï¼ä»¥å®ç°å¼æ¾è¯æ±çå ç¨é¢æµãè¿å¨3Dåºæ¯çè§£ä¸­æ¯ä¸ä¸ªéè¦è¿æ­¥ï¼å°¤å¶æ¯å¨å¤çæªç¥ç©ä½åå¤æå ä½ç»ææ¶ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>LMMå¨è§é¢é¢åçç²¾ç»ååºç¨ï¼</strong> ä¸åä»ä»æ¯ç®åçè§é¢-ææ¬å¯¹åºï¼èæ¯æ·±å¥å°è§é¢æ¨çãäºä»¶çè§£ç­æ´é«çº§çè®¤ç¥ä»»å¡ã</li>
<li><strong>å¤æ¨¡æçæä¸­çååæ§å¶ï¼</strong> å¦âPulp Motionâæç¤ºï¼åæ¶æ§å¶ç¸æºåäººä½è¿å¨ï¼å®ç°æ´åè°ãæ´å·è¡¨ç°åççæã</li>
<li><strong>3D åºæ¯çè§£çå¼æ¾è¯æ±è½åï¼</strong> è½å¤è¯å«åé¢æµæªè§è¿ç©ä½çå ç¨ä¿¡æ¯ï¼æ¯è¿åéç¨3Dçè§£çå³é®ä¸æ­¥ã</li>
<li><strong>ç»åå ä½åéªçæ·±åº¦å­¦ä¹ ï¼</strong> âSegMASt3RâåâObject-Centric Representation Learningâé½å¼ºè°äºå°å ä½ä¿¡æ¯èå¥æ·±åº¦å­¦ä¹ æ¨¡åçéè¦æ§ï¼ä»¥æé«é²æ£æ§åè§£éæ§ã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºå³æ³¨å¤æ¨¡æå¤§æ¨¡ååè§é¢çè§£çç ç©¶äººåï¼</strong><ul>
<li><strong>"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models" (Yunlong Tang et al.)</strong>ï¼äºè§£LMMå¨è§é¢æ¨çä¸­çææ°è¿å±åè®­ç»ç­ç¥ã</li>
<li><strong>"Bridging Text and Video Generation: A Survey" (Nilay Kumar et al.)</strong>ï¼å¨é¢äºè§£ææ¬å°è§é¢çæé¢åçç°ç¶åææã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨çææ¨¡åååå®¹åä½çç ç©¶äººåï¼</strong><ul>
<li><strong>"Pulp Motion: Framing-aware multimodal camera and human motion generation" (Robin Courant et al.)</strong>ï¼æ¢ç´¢åæ°çå¤æ¨¡æè¿å¨çæææ¯ã</li>
<li><strong>"ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion" (Foivos Paraperas Papantoniou, Stefanos Zafeiriou)</strong>ï¼äºè§£é«ä¿çäººè¸è¡¨æçæã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨3Dè§è§ååºæ¯çè§£çç ç©¶äººåï¼</strong><ul>
<li><strong>"Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction" (Chi Yan, Dan Xu)</strong>ï¼æ·±å¥äºè§£å¼æ¾è¯æ±3Då ç¨é¢æµã</li>
<li><strong>"Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction" (KunHo Heo et al.)</strong>ï¼æ¢ç´¢å¦ä½éè¿ä»¥å¯¹è±¡ä¸ºä¸­å¿çè¡¨ç¤ºæ¹è¿3Dåºæ¯å¾é¢æµã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨ç¹å®åºç¨æåºç¡ç®æ³çç ç©¶äººåï¼</strong><ul>
<li><strong>"BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping" (Hayat Rajani et al.)</strong>ï¼å¦æå¯¹æ°´ä¸æºå¨äººæç¯å¢çæµæå´è¶£ã</li>
<li><strong>"Learning-Based Hashing for ANN Search: Foundations and Early Advances" (Sean Moran)</strong>ï¼å¦æå¯¹é«ææ£ç´¢ååºç¡ç®æ³ä¼åæå´è¶£ã</li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦å¸æè½å¸®å©æ¨å¿«éææ¡ä»æ¥Arxivè®¡ç®æºè§è§é¢åçå³é®å¨æã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.05034v1">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></li>
<li><a href="#2510.04999v1">Bridging Text and Video Generation: A Survey</a></li>
<li><a href="#2510.04127v1">Learning-Based Hashing for ANN Search: Foundations and Early Advances</a></li>
<li><a href="#2510.05097v1">Pulp Motion: Framing-aware multimodal camera and human motion generation</a></li>
<li><a href="#2510.05051v1">SegMASt3R: Geometry Grounded Segment Matching</a></li>
<li><a href="#2510.04876v1">BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping</a></li>
<li><a href="#2510.04759v1">Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction</a></li>
<li><a href="#2510.04739v1">ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts</a></li>
<li><a href="#2510.04714v1">Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction</a></li>
<li><a href="#2510.04706v1">ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.05034v1'></a></p>
<h2 id="video-lmm-post-training-a-deep-dive-into-video-reasoning-with-large-multimodal-models"><a href="https://arxiv.org/abs/2510.05034v1">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a></h2>
<p><strong>Authors:</strong> Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yunlong Tangç­äººæ°åçè®ºæâVideo-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Modelsâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼Video-LMM åè®­ç»ï¼æ·±å¥æ¢ç´¢å¤§åå¤æ¨¡ææ¨¡åä¸­çè§é¢æ¨ç</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è§é¢çè§£é¢åçæ ¸å¿ææï¼å³å¦ä½å°æ°å´çè§é¢å¤§åå¤æ¨¡ææ¨¡åï¼Video-LMMsï¼ä»åºæ¬çæç¥ç³»ç»æåä¸ºå¤æçæ¨çå¼æãå·ä½èè¨ï¼ç ç©¶é®é¢éä¸­å¨Video-LMMsçâåè®­ç»âé¶æ®µï¼è¿ä¸é¶æ®µå¯¹äºæ¨¡åçè§£å¤æçæ¶ç©ºå³ç³»ãé¿æä¾èµåå¤æ¨¡æè¯æ®è³å³éè¦ï¼ä½ç®åæç®ä¸­å¯¹è¿ä¸é¶æ®µçç³»ç»æ§ç ç©¶å°ä¸å®åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿è´¡ç®å¨äºé¦æ¬¡å¯¹Video-LMMsçåè®­ç»æ¹æ³è¿è¡äºå¨é¢çç³»ç»æ§å®¡æ¥ååç±»ãå®å°åè®­ç»æ¹æ³åä¸ºä¸å¤§æ¯æ±ï¼
*   <strong>å¸¦æç»´é¾ççç£å¾®è°ï¼SFT with Chain-of-Thought, CoT-SFTï¼ï¼</strong> å¼ºè°éè¿æ¨¡ä»¿æ¨çæ¨¡å¼æ¥å¼å¯¼æ¨¡åï¼å¹¶ä½ä¸ºå¼ºåå­¦ä¹ ï¼RLï¼çå·å¯å¨é¶æ®µï¼æä¾ç»æåçæ¨çæ ¼å¼åç¨³å®çåå§åã
*   <strong>åºäºå¯éªè¯ç®æ çå¼ºåå­¦ä¹ ï¼RL from Verifiable Objectivesï¼ï¼</strong> æ¢è®¨äºå¦ä½å©ç¨å¯éªè¯çè¾åºï¼å¦ç­æ¡æ­£ç¡®æ§ãæ¶ç©ºå®ä½ç²¾åº¦ï¼æ¥ä¼åæ¨¡åï¼é¿åå¯¹äººå·¥åå¥½æ°æ®çä¾èµï¼å¹¶å¼å¥äºGRPOï¼Group Relative Policy Optimizationï¼ç­R1é£æ ¼çRLç®æ³ã
*   <strong>éè¿å¢å¼ºæ¨çè®¡ç®è¿è¡æµè¯æ¶ç¼©æ¾ï¼Test-Time Scaling, TTSï¼ï¼</strong> æ¶µçäºæ¨çé¶æ®µçè®¡ç®åéï¼ä»¥æé«å¯é æ§ï¼åæ¬æç»´é¾æç¤ºãèªæ´½æ§è§£ç ãåºäºç½®ä¿¡åº¦çè¿­ä»£æ¨çãèªæ¹è¿å¾ªç¯ãèç¹å¡æ´æ æç´¢ï¼MCTSï¼ä»¥åå·¥å·å¢å¼ºæ¨çã</p>
<p>è®ºæè¿æåºäºä¸ä¸ªç»æåçåç±»æ³ï¼éæäºè¿äºææ¯å¨è§£å³è§é¢ç¹æææï¼å¦æ¶é´å®ä½ãæ¶ç©ºå®ä½ãé¿è§é¢æçåå¤æ¨¡æè¯æ®æ´åï¼ä¸­çä½ç¨ãç¸äºèç³»åè§é¢ç¹å®éåºæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥è°æ¥éè¿å¯¹ä»£è¡¨æ§æ¹æ³çç³»ç»åæï¼ç»¼åäºå³é®è®¾è®¡ååãè§è§£åè¯ä¼°åè®®ãä¸»è¦åç°åæä¹åæ¬ï¼
*   <strong>SFTä½ä¸ºåºç¡ï¼</strong> CoT-SFTè½å¤ææå°å°ç»æåæ¨çè¡ä¸ºæ³¨å¥Video-LMMsï¼å¹¶ä¸ºåç»­çRLè®­ç»æä¾ç¨³å®çèµ·ç¹ã
*   <strong>RLçæææ§ï¼</strong> åºäºå¯éªè¯å¥å±çRLï¼ç¹å«æ¯GRPOï¼å¨æåVideo-LMMsçæ¨çè½åæ¹é¢è¡¨ç°åºæ¾èææï¼å°¤å¶æ¯å¨å¤çå¤ææ¶ç©ºä»»å¡åé¿è§é¢çè§£æ¹é¢ãRLæ¹æ³è¢«è¯ææ¯æ°æ®é«æçï¼å°éé«è´¨éæ°æ®å³å¯åª²ç¾å¤§è§æ¨¡çç£å¾®è°çæ§è½ã
*   <strong>TTSçå¯é æ§æåï¼</strong> TTSç­ç¥éè¿å¨æ¨çæ¶åéé¢å¤è®¡ç®èµæºï¼æ¾èæé«äºVideo-LMMsçå¯é æ§ãæ¨çæ·±åº¦åè·¯å¾å¤æ ·æ§ï¼æå©äºåå°å¹»è§å¹¶æé«ç­æ¡åç¡®æ§ã
*   <strong>ç»ä¸æ¡æ¶ï¼</strong> è®ºææä¾äºä¸ä¸ªç»ä¸çæ¡æ¶ï¼å°SFTãRLåTTSè§ä¸ºæ¨¡åä¼åçä¸å¯æç¼ºçç»æé¨åï¼è¿å¯¹äºæ¨å¨Video-LMMsè½åçåå±å·æéè¦æä¹ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæå¨è®¨è®ºå¼æ¾æææ¶æåäºå½åæ¹æ³çå±éæ§ï¼
*   <strong>å¥å±è®¾è®¡ææï¼</strong> ç°æå¥å±è®¾è®¡å¨å¤çå¤æãå¯ç»åçå¥å±ï¼å¦å®ä½é¾æ¥ãæåºãå¯¹è±¡-å¨ä½ç»å®ï¼æ¶ä»é¢ä¸´ææï¼éè¦æ´ç²¾ç»çè¿ç¨å¥å±æ¨¡åï¼PRMsï¼æ¥æä¾å¯éçä¿¡ç¨åéã
*   <strong>å¯æ©å±æ§é®é¢ï¼</strong> å°½ç®¡RLæ°æ®é«æï¼ä½å¨é¿è§é¢ä¸æ©å±RLä»ç¶é¢ä¸´é¢ç®éå¶ï¼éè¦æ´é«æçå¸§éæ©åç¼å­æºå¶ã
*   <strong>ææ¬-æ§è½ä¼åï¼</strong> å¸§ä¼åååç¼©æ¡æ¶ä»ç¶ææ¬é«æï¼éè¦æªæ¥çå·¥ä½ä½¿å¶å¨æ°æ®åè®¡ç®ä¸æ´é«æã
*   <strong>æ°æ®ç¨ç¼ºååå·®ï¼</strong> é«è´¨éãå¯éªè¯çCoTæ°æ®æå»ºææ¬é«æï¼ä¸å­å¨æ¨¡æ¿ååæ¨¡ååå·®ãRLè®­ç»ä¸­ä¹å­å¨è¯ä¼°åå·®åé¿åº¦åå·®ï¼å¯è½å¯¼è´æ¨¡ååºç°âå¥æ¿âæâå­å¹æ³é²âç­é®é¢ã
*   <strong>æ¢ç´¢è½åä¸è¶³ï¼</strong> RLçæ¢ç´¢è½åä»éæåï¼è¶è¶æå¸æ¨¡åæè½æä¾çç­ç¥ï¼éè¦å¤æ ·æ§é©±å¨çç®æ åèªåå¼æºå¶ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸å ä¸ªæªæ¥ç ç©¶æ¹åï¼
*   <strong>ç»æåæ¥å£åæ¥å°CoTï¼</strong> è§èæ¨çæ ¼å¼ï¼å°æ¨çæ­¥éª¤ä¸è¯æ®ï¼æ¶é´æ³ãå¸§IDãåºåï¼ç»å®ï¼ä»¥æé«å¿ å®åº¦å¹¶ç®åéªè¯å¨è®¾è®¡ã
*   <strong>å¤§è§æ¨¡éªè¯å¨å¨ç¯CoTåæï¼</strong> èªå¨åèç¨¿-ç»å-å®¡è®¡æµç¨ï¼ä»ASR/OCR/éå¤´åæ°æ®å¼å§ï¼éè¿è½»éçº§æ£æ¥å¨è¿è¡ç»ååè¿æ»¤ï¼ä»¥åå°å¹»è§ã
*   <strong>ä¸æ¨¡æçç£åå­å¹æ§å¶ï¼</strong> å°SFTæ©å±å°å¯¹è¯­é³ãäºä»¶åè§è§è¯æ®çå¯¹é½ï¼å¹¶å§ç»æ¥åå¸¦/ä¸å¸¦è½¬å½çç»æï¼ä»¥é¿åASRå¿«æ·æ¹å¼ã
*   <strong>å¹»è§æç¥æä»¤å¾®è°ï¼</strong> ç»ååäºå®åç¼ºå¤±æ¡ä¾ï¼è®­ç»æ¨¡åè¿è¡æ ¡åçå¼æåéªè¯è¡ä¸ºï¼åå°è¿åº¦è¯å®ã
*   <strong>å¤è¯­è¨ãOCRååäºç»æï¼</strong> æ©å±SFTä»¥å¤çå¤è¯­è¨ãéåææ¬åé¿è·¨åº¦åäºæ¨çã
*   <strong>å¯ç»åãå¯éªè¯çå¥å±ï¼</strong> å¼åæ´ç²¾ç»çå¥å±æºå¶ï¼è½å¤å¤çå¤æçæ¶ç©ºè¯­ä¹æ£æ¥ï¼å¹¶æ§å¶PRMsçææ¬ååå·®ã
*   <strong>è¶è¶æå¸çæ¢ç´¢ï¼</strong> å¼åå¤æ ·æ§é©±å¨çç®æ åèªåå¼æºå¶ï¼ä½¿RLè½å¤åç°è¶è¶æå¸è½åçç­ç¥ã
*   <strong>ç½®ä¿¡åº¦æç¥ãéªè¯å¨å¼å¯¼çTTSï¼</strong> å°åæ­¢è§åä¸ä¸ç¡®å®æ§ç»åï¼å¹¶å¨éè¦æ¶æ·±åæ¨çæå¯éåè§å¾ï¼å®ç°éæ¶å¯ç¨çåç¡®æ§ã
*   <strong>å·¥å·å¢å¼ºæ¨çåè¸é¦ï¼</strong> å°å·¥å·è°ç¨ï¼æ£ç´¢ãè·è¸ªãASRå¯¹é½ï¼ä¸æ¨çç¸ç»åï¼å¹¶éè¿åéªè¸é¦å°è¿äºçå¤è½¬ç§»å°åºç¡æ¨¡åä¸­ã
*   <strong>å¸¦è®°å¿çæµå¼ä»£çï¼</strong> å¼åè½å¤å³å®ä½æ¶è§çãè§çä»ä¹ï¼å¹¶ç»´æ¤ä»»å¡æç¥å·¥ä½è®°å¿çä»£çè§åå¨ï¼ä»¥å¤çé¿è§é¢ææµå¼è§é¢ã
*   <strong>æ ååæ¥ååæ³æ¼æ§å¶ï¼</strong> æ¥åæ¥çé¢ç®ãæ¨çé¿åº¦ãè·¯å¾è®¡æ°ãå»¶è¿/ååéåå­å¹ä½¿ç¨æåµï¼å¹¶è¿è¡âå¥æ¿âåâå¤æ­åå·®âè¯æ­ã
*   <strong>åéè§å¾ä¸çè®¡ç®-åç¡®æ§æè¡¡ï¼</strong> ååè°æ´å¸§éæ©ååç¼©ä¸æ¨çè´¨éï¼ä»¥å¨ä»å¤çå°éå¸§æ¶ä¿æç³»ç»æ§è½ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºVideo-LMMsçåè®­ç»æä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»æ»ç»äºç°æææ¯ï¼è¿ææäºæªæ¥ç ç©¶çå³é®æ¹åï¼å¯¹äºæ¨å¨è§é¢çè§£é¢åçåå±å·æéè¦çæå¯¼æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05034v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05034v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04999v1'></a></p>
<h2 id="bridging-text-and-video-generation-a-survey"><a href="https://arxiv.org/abs/2510.04999v1">Bridging Text and Video Generation: A Survey</a></h2>
<p><strong>Authors:</strong> Nilay Kumar, Priyansh Bhandari, G. Maragatham</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.GR, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Nilay Kumar, Priyansh Bhandari, G. Maragathamæ°åçè®ºæâBridging Text and Video Generation: A Surveyâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼ææ¬å°è§é¢çæææ¯ç»¼è¿°</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨å¨é¢ç»¼è¿°ææ¬å°è§é¢ï¼Text-to-Video, T2Vï¼çæé¢åï¼è§£å³å¦ä½ä»èªç¶è¯­è¨æç¤ºä¸­åå»ºè¿è´¯çè§è§åå®¹è¿ä¸æ ¸å¿é®é¢ãå°½ç®¡T2Vææ¯å¨æè²ãè¥éãå¨±ä¹åè¾å©ææ¯ç­å¤ä¸ªé¢åå·æå·¨å¤§æ½åï¼ä½è¯¥é¢åä»é¢ä¸´è¯¸å¤ææï¼åæ¬çæè§é¢çè¯­ä¹å¯¹é½ãé¿æè¿è´¯æ§ãè§è§è´¨éä»¥åè®¡ç®æçãæ¬ç»¼è¿°æ¨å¨ç³»ç»å°è¿½è¸ªè¯¥é¢åçåå±ï¼è¯å«å³é®æ¨¡åãæ°æ®éãè¯ä¼°æ¹æ³ãç°æå±éæ§ä»¥åæªæ¥çç ç©¶æ¹åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææ¬èº«æ¯ä¸ç¯ç»¼è¿°ï¼å¶ä¸»è¦è´¡ç®å¨äºå¯¹T2Vé¢åè¿è¡äºç³»ç»æ§ãç»æåçæ¢³çååæï¼èéæåºæ°çæ¨¡åãå¶æ¹æ³è®ºè´¡ç®ä½ç°å¨ï¼
*   <strong>åå±è·¯å¾æ¢³çï¼</strong> è¯¦ç»è¿½æº¯äºT2Væ¨¡åä»æ©æççæå¯¹æç½ç»ï¼GANsï¼åååèªç¼ç å¨ï¼VAEsï¼å°å½åæ··åæ©æ£-Transformerï¼DiTï¼æ¶æçæ¼åè¿ç¨ãè§£éäºè¿äºæ¨¡åçå·¥ä½åçãå®ä»¬å¦ä½è§£å³åä»£æ¨¡åçå±éæ§ï¼ä»¥åä¸ºä½éè¦è½¬åæ°çæ¶æèå¼ä»¥åæè´¨éãè¿è´¯æ§åæ§å¶æ¹é¢çææã
*   <strong>æ ¸å¿ææ¯åé¡¾ï¼</strong> è¯¦ç»ä»ç»äºT2Væ¨¡åæä¾èµçåºç¡æ§æ¶æï¼åæ¬U-NetãVAEsãGANsåå»åªæ©æ£æ¦çæ¨¡åï¼DDPMsï¼ï¼ä»¥åTransformeræºå¶ãè¿ä¸ºçè§£T2Væ¨¡åçåé¨æºå¶æä¾äºå¿è¦çèæ¯ç¥è¯ã
*   <strong>æ°æ®éåè®­ç»éç½®çç³»ç»åï¼</strong> æä¾äºT2Væ¨¡åè®­ç»åè¯ä¼°æç¨æ°æ®éçè¯¦ç»æ¸åï¼å¹¶ä¸ºäºæ¯æå¯å¤ç°æ§åè¯ä¼°æ¨¡åè®­ç»çå¯è®¿é®æ§ï¼è¯¦ç»ååºäºç¡¬ä»¶è§æ ¼ãGPUæ°éãæ¹æ¬¡å¤§å°ãå­¦ä¹ çãä¼åå¨ãè®­ç»å¨æç­å³é®è¶åæ°ã
*   <strong>è¯ä¼°ææ ååºåçå¨é¢åæï¼</strong> æ¦è¿°äºT2Væ¨¡åå¸¸ç¨çå®éè¯ä¼°ææ ï¼å¦ISãFIDãFVDãCLIP-SIMãKVDï¼åå¶å¨æ ååºåä¸çè¡¨ç°ãåæ¶ï¼è®¨è®ºäºè¿äºææ çå±éæ§ï¼å¹¶å¼ºè°äºåæ´å¨é¢ãæç¥å¯¹é½çè¯ä¼°ç­ç¥ï¼å¦VBenchï¼è½¬åçå¿è¦æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ææ¯æ¼è¿çæ¸æ°å¾æ¯ï¼</strong> ç»¼è¿°å±ç¤ºäºT2Vææ¯ä»æ©æå¯¹ææ¨¡åå°åºäºæ©æ£æ¨¡åçæ¾èè¿æ­¥ï¼è¿äºè¿æ­¥å¸¦æ¥äºæ´é«ä¿çåº¦ãæ´å·æ¶é´è¿è´¯æ§çè¾åºãè¿è¡¨ææ©æ£æ¨¡åå·²æä¸ºå½åT2Vçæçä¸»æµèå¼ã
*   <strong>å¯¹ç°ææ¨¡åä¼ç¼ºç¹çæ·±å¥çè§£ï¼</strong> è®ºæè¯¦ç»åæäºä¸åæ¶æï¼GANsãVAEsãæ©æ£æ¨¡åï¼å¨è§£å³è§é¢çæåºæé®é¢ï¼å¦æ¶é´ä¸è´æ§ãè§è§è´¨éåææ¬-è§é¢å¯¹é½ï¼æ¹é¢çä¼å¿åå±éæ§ã
*   <strong>æ ååè¯ä¼°çå¿è¦æ§ï¼</strong> å¼ºè°äºç°æå®éææ å¨ææäººç±»æç¥è´¨éæ¹é¢çä¸è¶³ï¼å¹¶ä»ç»äºVBenchç­æ°å´åºåå¦ä½éè¿å¤ç»´åº¦è¯ä¼°åäººç±»åå¥½æ æ³¨æ¥æä¾æ´ç»è´ãæ´å¨é¢çæ¨¡åæ§è½è¯ä¼°ãè¿å¯¹äºæ¨å¨é¢ååå±åç¡®ä¿æ¨¡åä¸äººç±»ææå¯¹é½è³å³éè¦ã
*   <strong>å¯å¤ç°æ§åå¯è®¿é®æ§çä¿è¿ï¼</strong> éè¿è¯¦ç»ååºè®­ç»éç½®ï¼ä¸ºç ç©¶äººåæä¾äºå®è´µçå®è·µæå¯¼ï¼æå©äºè¯ä¼°å®ç°åæ¹è¿ç°ææ¹æ³çå¯è½æ§ï¼å¹¶è¯å«æ½å¨çç¶é¢åæ¹è¿æºä¼ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>è®¡ç®æçï¼</strong> ç°ææ¨¡åå¨å¤çé¿è§é¢åºåæ¶è®¡ç®ææ¬é«æï¼ä¸è®­ç»æ¶é´é¿ã
*   <strong>æ°æ®ééå¶ï¼</strong> ç°æT2Væ°æ®éè§æ¨¡æéãè´¨éä¸è¶³ï¼ä¸å­å¨çæéå¶ï¼è¿é»ç¢äºæ¨¡åæ³åè½ååçæé«è´¨éè§é¢çè½åã
*   <strong>è¾åºè´¨éåè¿è´¯æ§ï¼</strong> å°½ç®¡ææè¿æ­¥ï¼ä½çæçè§é¢ä»å¯è½å­å¨æ¶é´æç©ºé´ä¸çä¸è¿è´¯æ§ï¼ç©ä½å¯è½ä¸èªç¶å°ç§»å¨ãåºç°ææ¶å¤±ï¼ç©çäº¤äºä¸çå®ï¼ä»¥åé¾ä»¥ææå¤æåºæ¯åå¤äº¤äºåç´ ã
*   <strong>å¤æ ·æ§ä¸è¶³ï¼</strong> çæåå®¹çå¤æ ·æ§æéï¼é¨ååå å¨äºè®­ç»æ°æ®æ¬èº«ç¼ºä¹å¤æ ·æ§ã
*   <strong>åè¾¨çéå¶ï¼</strong> ç»´æé¿åºåé«åè¾¨çè¾åºä»ç¶æ¯ä¸ä¸ªææã
*   <strong>è¯ä¼°ææ çå±éæ§ï¼</strong> ä¼ ç»å®éææ æ æ³å¨é¢ææäººç±»æç¥å°çè§é¢è´¨éï¼å¦èº«ä»½ä¿æãè¿å¨æµçæ§åæ¶é´ç¨³å®æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ°æ®éä¸°å¯ï¼</strong>
    *   å©ç¨æ¸¸æå¼æï¼å¦UnityãUnreal Engineï¼åæå¤§è§æ¨¡ãé«åè¾¨çãå¤æ ·åä¸æ çæéå¶çæ°æ®éã
    *   å¼åå¹¿ä¹çæç¤ºæ¡æ¶ï¼éè¿ç»æåæç¤ºï¼åæ¬ä¸»ä½ãå±æ§ãå¨ä½ãèæ¯ãé£æ ¼ç­ï¼èªå¨åè§é¢çæï¼ä»¥æé«æ°æ®éçè´¨éåæ°éã
*   <strong>æ¨¡åæ¶æåä¼åï¼</strong>
    *   å¼åæ°çæ¨¡åæ¶ææç®æ³ï¼ä»¥æ´ææå°å¤çè§é¢æ°æ®ï¼æé«è®¡ç®æçåå¯æ©å±æ§ã
    *   å¢å¼ºæ¨¡åç<strong>æ¶é´å»ºæ¨¡è½å</strong>ï¼ä»¥çææ´é¿ãæ´è¿è´¯çè§é¢ã
    *   æ´ååè¿ç<strong>æ³¨æåæºå¶</strong>ï¼å©ç¨å¤æ¨¡ææ°æ®ï¼å¹¶ä¿®æ¹æå¤±å½æ°ï¼ä»¥æ´ææå°å³æ³¨è¿è´¯æ§åçå®æã
    *   æ¹è¿<strong>ç©ççº¦æå»ºæ¨¡</strong>ï¼ä½¿çæçè§é¢æ´å·ç©çåçæ§ã
    *   éè¿æ´å¤æ ·åçæ°æ®éååè¿ç<strong>æ°æ®å¢å¼ºææ¯</strong>ï¼æé«çæåå®¹çå¤æ ·æ§ã
*   <strong>åºç¨åå½±åï¼</strong>
    *   æ¢ç´¢T2Vææ¯å¨æè²ãè¥éãå¨±ä¹ãè¾å©ææ¯ãæåéäº§ä¿æ¤ãæ³å¾åè¯ãåææ°æ®çæãæ¸¸æåèæç°å®ç­é¢åçæ´å¹¿æ³åºç¨ã</p>
<hr />
<p>è¿ç¯ç»¼è¿°ä¸ºT2Vé¢åçç ç©¶äººåæä¾äºä¸ä¸ªå®è´µçèµæºï¼ä¸ä»æ»ç»äºç°æææ¯ï¼è¿ææäºæªæ¥çåå±æ¹åï¼æå©äºæ¨å¨è¯¥é¢åå¨è´¨éãæçååºç¨æ¹é¢çè¿ä¸æ­¥çªç ´ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04999v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04999v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04127v1'></a></p>
<h2 id="learning-based-hashing-for-ann-search-foundations-and-early-advances"><a href="https://arxiv.org/abs/2510.04127v1">Learning-Based Hashing for ANN Search: Foundations and Early Advances</a></h2>
<p><strong>Authors:</strong> Sean Moran</p>
<p><strong>Published:</strong> 2025-10-05</p>
<p><strong>Categories:</strong> cs.IR, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Sean Moranæ°åçè®ºæâLearning-Based Hashing for ANN Search: Foundations and Early Advancesâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Learning-Based Hashing for ANN Search: Foundations and Early Advances</strong></p>
<p>è¿ç¯è®ºæç±Sean Moranæ°åï¼æ¨å¨å¯¹æ©æåºäºå­¦ä¹ çåå¸æ¹æ³è¿è¡åºç¡æ§ç»¼è¿°ï¼è¿äºæ¹æ³ä¸»è¦ç¨äºè¿ä¼¼æè¿é»ï¼ANNï¼æç´¢ãANNæç´¢æ¯ä¿¡æ¯æ£ç´¢ä¸­çä¸ä¸ªæ ¸å¿é®é¢ï¼å¹¿æ³åºç¨äºè®¡ç®æºè§è§ãèªç¶è¯­è¨å¤çåè·¨æ¨¡ææç´¢ç­å¤§è§æ¨¡åºç¨ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è®ºæä¸»è¦æ¢è®¨äºå¦ä½éè¿å°é«ç»´æ°æ®æ å°å°ç´§åçäºè¿å¶ä»£ç ï¼åå¸ç ï¼æ¥é«æå°è§£å³å¤§è§æ¨¡æ°æ®éä¸­çANNæç´¢é®é¢ï¼ä»èå¨æ±æç©ºé´ä¸­å®ç°å¿«éç¸ä¼¼æ§è®¡ç®ãæ ¸å¿ç ç©¶é®é¢å¨äºï¼ä¼ ç»çåå¸æ¹æ³ï¼å¦å±é¨ææåå¸LSHï¼éå¸¸æ¯æ°æ®æ å³çï¼å¶æå½±åéåå½æ°æ¯éæºéæ©çï¼è¿å¯¼è´äºä¿¡æ¯æå¤±åæ¬¡ä¼çæ£ç´¢æ§è½ãå æ­¤ï¼è®ºææ¨å¨åé¡¾ååæé£äºéè¿æ°æ®ä¼åæå½±åéåå½æ°æ¥æé«åå¸ç è´¨éåæ£ç´¢æççæ©æâå­¦ä¹ åå¸âæ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæåé¡¾äºæ©æå­¦ä¹ åå¸æ¹æ³å¨ä»¥ä¸å ä¸ªæ¹é¢çåæ°ï¼</p>
<ul>
<li><strong>æ°æ®é©±å¨çæå½±å½æ°å­¦ä¹ ï¼</strong> å¼ºè°äºéè¿æ°æ®ä¼åæå½±å½æ°ï¼å³åå¸è¶å¹³é¢ï¼çéè¦æ§ï¼èééæºéæ©ãè¿åæ¬æ çç£æ¹æ³ï¼å¦PCAHãSHãITQãAGHï¼åæçç£æ¹æ³ï¼å¦ITQ+CCAãKSHãBREãSTHï¼ï¼å®ä»¬æ¨å¨å­¦ä¹ è½å¤æ´å¥½å°ä¿çåå§ç©ºé´ç¸ä¼¼æ§çè¶å¹³é¢ã</li>
<li><strong>å¤ä½åå¤éå¼éåç­ç¥ï¼</strong> æ¢è®¨äºè¶è¶ç®åç¬¦å·å½æ°ï¼åéå¼ï¼çéåæ¹æ³ï¼ä»¥åå°ä¿¡æ¯æå¤±ãè¿åæ¬åå±éåï¼HQï¼ãåä½éåï¼DBQï¼åæ¼åé¡¿åå¸éåï¼MHQï¼ï¼å®ä»¬éè¿å¼å¥å¤ä¸ªéå¼åæ´å¤æçç¼ç æ¹æ¡æ¥æé«åå¸ç çå¤å«åã</li>
<li><strong>è·¨æ¨¡ææ£ç´¢çæ©æè¿å±ï¼</strong> ä»ç»äºå°å­¦ä¹ åå¸æ©å±å°è·¨ä¸åæ°æ®æ¨¡æï¼å¦å¾ååææ¬ï¼çæ£ç´¢ä»»å¡ï¼ä¾å¦è·¨è§å¾åå¸ï¼CVHï¼ãååæ­£åååå¸ï¼CRHï¼ç­ã</li>
<li><strong>è¯ä¼°èå¼åææ ï¼</strong> è®ºæè¯¦ç»ä»ç»äºè¯ä¼°åå¸æ¹æ³æ§è½çæ åèå¼ï¼æ±ææåºååå¸è¡¨æ¡¶è¯ä¼°ï¼ä»¥åå¸¸ç¨ææ ï¼AUPRCãmAPï¼ï¼å¹¶è®¨è®ºäºæ°æ®éååç­ç¥ä»¥ç¡®ä¿è¯ä¼°çç¨³å¥æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¯¹æ©ææ¨¡åçåå²èæ¯è¿è¡æ¢³çï¼æ­ç¤ºäºä»¥ä¸éè¦è§è§£ï¼</p>
<ul>
<li><strong>æ°æ®æç¥äºå¼åä¼äºéæè§åï¼</strong> ä¼åéåéå¼éå¸¸è½äº§çæ¯åä¸éæéå¼æ´å·å¤å«åçä»£ç ã</li>
<li><strong>ä¿¡æ¯éä¸åçæå½±ï¼</strong> éåååééå¼ï¼å³å¯¹ä¿¡æ¯éæ´å¤§çæå½±è¿è¡æ´ç²¾ç»çéåï¼è½æé«æ£ç´¢æçã</li>
<li><strong>å­¦ä¹ æå½±ä¼äºéæºæå½±ï¼</strong> å¼å¥çç£ä¿¡æ¯æ¥æå¯¼åå¸è¶å¹³é¢çæ¾ç½®ï¼éå¸¸è½æé«æçã</li>
<li><strong>è·¨æ¨¡æåå¸çå¯è¡æ§åä»·å¼ï¼</strong> è·¨æ¨¡ææå½±å­¦ä¹ è½å¤å®ç°æåè¿çæ£ç´¢æ§è½ã</li>
<li><strong>æµæ°´çº¿è¦åçéè¦æ§ï¼</strong> èåå­¦ä¹ æå½±åéåæ¯å­¤ç«å¤çæ¯ä¸ªæ­¥éª¤è½å¸¦æ¥æ¹è¿ã</li>
</ul>
<p>è¿äºåç°ä¸ºåç»­å­¦ä¹ åå¸é¢åçç ç©¶å¥ å®äºåºç¡ï¼å¼ºè°äºæ°æ®ä¾èµæ§æ¹æ³å¨æé«ANNæç´¢æçååç¡®æ§æ¹é¢çæ½åã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¹æåºäºæ©ææ¹æ³çä¸äºå±éæ§ï¼</p>
<ul>
<li><strong>éåæ¨¡åç¼ºä¹çç£ä¿¡æ¯ï¼</strong> æ©æå¤éå¼éåæ¨¡åï¼å¦HQãDBQãMHQï¼éå¸¸ä»¥æ çç£æ¹å¼å­¦ä¹ ï¼æªè½ååå©ç¨æ ç­¾ä¿¡æ¯ã</li>
<li><strong>éå¼åéçååæ§ï¼</strong> éå¼éå¸¸åååéå¨æå½±ç»´åº¦ä¸ï¼å¿½ç¥äºå¤å«åçååã</li>
<li><strong>è®¡ç®ææ¬é«æï¼</strong> æçç£æå½±å½æ°ï¼å¦ITQ+CCAãKSHãBREãSTHï¼éå¸¸ä¾èµäºè®¡ç®ææ¬é«æçç¹å¾å¼åè§£ææ ¸æ¹æ³ï¼éå¶äºå¯æ©å±æ§ã</li>
<li><strong>ç¼ºä¹ç»ä¸æ¡æ¶ï¼</strong> æ©æå·¥ä½æªè½å°æå½±è¶å¹³é¢å­¦ä¹ ä¸å¤éåéå¼å­¦ä¹ ç»åå¨ä¸ä¸ªç»ä¸çæ¡æ¶ä¸­ã</li>
<li><strong>è¯ä¼°æ¹æ³çä¸ä¸è´æ§ï¼</strong> ä¸åçç ç©¶å¨å°é¢çå¼å®ä¹ï¼ç±»æ ç­¾ä¸åº¦ée-ballï¼ãæ°æ®éåååè®®ï¼å¯è½å¯¼è´è¿æåï¼åæ¥åææ ï¼mAPä¸AUPRCï¼æ¹é¢å­å¨ä¸ä¸è´ï¼å½±åäºç»æçå¯æ¯æ§ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§åæ©æç ç©¶çç»éªï¼è®ºææåºäºå ä¸ªæåæ¯çæªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>äººç±»å¯¹é½ï¼</strong> éªè¯åº¦éæ¹è¿ï¼AUPRC/mAPï¼æ¯å¦ä¸ç¨æ·æ»¡æåº¦ç¸å³ã</li>
<li><strong>å¨çº¿/æµå¼åå¸ï¼</strong> éåºçç£æå½±å­¦ä¹ ä»¥å¤çééææ°æ®æµã</li>
<li><strong>è·¨è¯­è¨æ£ç´¢ï¼</strong> å°è·¨æ¨¡æåå¸æ©å±å°å¤è¯­è¨ææ¡£éåï¼ä¸ä¾èµç¿»è¯ã</li>
<li><strong>ä½é´ä¾èµæ§ï¼</strong> æ¢ç´¢è·¨ç»´åº¦åè·¨è¶å¹³é¢çä¾èµæ§éå¼åå»ç¸å³ã</li>
<li><strong>ç«¯å°ç«¯ç®æ ï¼</strong> å¨åä¸è®­ç»ååä¸èåä¼åæå½±åéåã</li>
</ul>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºå­¦ä¹ åå¸é¢åæä¾äºä¸ä¸ªå®è´µçåå²è§è§ï¼æ»ç»äºæ©æç ç©¶çæå°±ãææåæªæ¥æ¹åï¼ä¸ºçè§£å½åæ·±åº¦å­¦ä¹ æ¶ä»£åå¸æ¹æ³çåå±æä¾äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search.</li>
<li>By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04127v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04127v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05097v1'></a></p>
<h2 id="pulp-motion-framing-aware-multimodal-camera-and-human-motion-generation"><a href="https://arxiv.org/abs/2510.05097v1">Pulp Motion: Framing-aware multimodal camera and human motion generation</a></h2>
<p><strong>Authors:</strong> Robin Courant, Xi Wang, David Loiseaux, Marc Christie, Vicky Kalogeiton</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Treating human motion and camera trajectory generation separately overlooks a
core principle of cinematography: the tight interplay between actor performance
and camera work in the screen space. In this paper, we are the first to cast
this task as a text-conditioned joint generation, aiming to maintain consistent
on-screen framing while producing two heterogeneous, yet intrinsically linked,
modalities: human motion and camera trajectories. We propose a simple,
model-agnostic framework that enforces multimodal coherence via an auxiliary
modality: the on-screen framing induced by projecting human joints onto the
camera. This on-screen framing provides a natural and effective bridge between
modalities, promoting consistency and leading to more precise joint
distribution. We first design a joint autoencoder that learns a shared latent
space, together with a lightweight linear transform from the human and camera
latents to a framing latent. We then introduce auxiliary sampling, which
exploits this linear transform to steer generation toward a coherent framing
modality. To support this task, we also introduce the PulpMotion dataset, a
human-motion and camera-trajectory dataset with rich captions, and high-quality
human motions. Extensive experiments across DiT- and MAR-based architectures
show the generality and effectiveness of our method in generating on-frame
coherent human-camera motions, while also achieving gains on textual alignment
for both modalities. Our qualitative results yield more cinematographically
meaningful framings setting the new state of the art for this task. Code,
models and data are available in our
\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project
page}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Robin Courantç­äººæ°åçè®ºæâPulp Motion: Framing-aware multimodal camera and human motion generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="pulp-motion-framing-aware-multimodal-camera-and-human-motion-generation_1">è®ºææè¦ï¼Pulp Motion: Framing-aware Multimodal Camera and Human Motion Generation</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°ææ¹æ³å°äººä½è¿å¨åæåæºè½¨è¿¹çæè§ä¸ºç¬ç«ä»»å¡çå±éæ§ãè¿ç§åç¦»å¿½ç¥äºçµå½±æå½±çæ ¸å¿ååââæ¼åè¡¨æ¼åæåæºå·¥ä½å¨å±å¹ç©ºé´ä¸­ç´§å¯äº¤ç»ãå æ­¤ï¼æ ¸å¿ç ç©¶é®é¢æ¯å¦ä½å®ç°ææ¬æ¡ä»¶ä¸çèåçæï¼ä»¥å¨çæäººä½è¿å¨åæåæºè½¨è¿¹è¿ä¸¤ç§å¼æä½åå¨å³èçæ¨¡ææ¶ï¼ä¿æä¸è´çå±å¹æå¾ï¼å³âæå¾æç¥âï¼ï¼å¹¶ç¡®ä¿å¤æ¨¡æè¿è´¯æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªç®åãæ¨¡åæ å³çæ¡æ¶ï¼éè¿å¼å¥ä¸ä¸ªè¾å©æ¨¡æââ<strong>å±å¹æå¾ï¼on-screen framingï¼</strong>æ¥å¼ºå¶å¤æ¨¡æè¿è´¯æ§ãå±å¹æå¾æ¯éè¿å°äººä½å³èæå½±å°æåæºä¸èäº§ççï¼å®ä¸ºä¸åæ¨¡æä¹é´æä¾äºä¸ä¸ªèªç¶ææçæ¡¥æ¢ãå·ä½åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>èåèªç¼ç å¨ä¸å±äº«æ½å¨ç©ºé´ï¼</strong> è®¾è®¡äºä¸ä¸ªèåèªç¼ç å¨ï¼å­¦ä¹ ä¸ä¸ªå±äº«çæ½å¨ç©ºé´æ¥è¡¨ç¤ºäººä½è¿å¨åæåæºè½¨è¿¹ã</li>
<li><strong>è½»éçº§çº¿æ§åæ¢ï¼</strong> å¼å¥äºä¸ä¸ªè½»éçº§ççº¿æ§åæ¢ï¼å°äººä½åæåæºæ½å¨è¡¨ç¤ºæ å°å°ä¸ä¸ªæå¾æ½å¨è¡¨ç¤ºãè¿ä¸ªåæ¢ç´æ¥å¨æ½å¨ç©ºé´ä¸­ææäºçææ¨¡æä¸è¾å©æ¨¡æä¹é´çå³ç³»ã</li>
<li><strong>è¾å©éæ ·æºå¶ï¼</strong> æåºäºä¸ç§è¾å©éæ ·ææ¯ï¼å©ç¨ä¸è¿°çº¿æ§åæ¢å¨æ¨çè¿ç¨ä¸­å¼å¯¼çæï¼ä½¿å¶è¶åäºè¿è´¯çæå¾æ¨¡æãè¿ç§æ¹æ³å¨è®­ç»æé´æ éæ¾å¼å°å°è¾å©æ¨¡æçº³å¥æ¡ä»¶ï¼éä½äºè®­ç»ææ¬å¹¶æé«äºéç¨æ§ã</li>
<li><strong>PulpMotionæ°æ®éï¼</strong> ä¸ºäºæ¯æè¿é¡¹ä»»å¡ï¼è®ºæè¿å¼å¥äºPulpMotionæ°æ®éï¼è¿æ¯ä¸ä¸ªåå«ä¸°å¯ææ¬æè¿°åé«è´¨éäººä½è¿å¨çäººä½è¿å¨åæåæºè½¨è¿¹æ°æ®éï¼æ¾èæ©å±äºç°ææ°æ®éçè§æ¨¡åæ¨¡æè¦çèå´ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
éè¿å¨DiTåMARä¸¤ç§ä¸åæ¶æä¸è¿è¡å¹¿æ³å®éªï¼è®ºæå±ç¤ºäºè¯¥æ¹æ³çéç¨æ§åæææ§ï¼</p>
<ul>
<li><strong>æ¾èæåå¤æ¨¡æè¿è´¯æ§ï¼</strong> è¾å©éæ ·æ¾èæ¹åäºçæè¿å¨åè½¨è¿¹ä¹é´çè¿è´¯æ§ï¼éä½äºæå¾è¯¯å·®ï¼FDframingï¼ååºç»çï¼Out-rateï¼ï¼åæ¶ä¿æäºå¼ºå¤§çåæ¨¡æçææ§è½ã</li>
<li><strong>æé«ææ¬å¯¹é½è´¨éï¼</strong> è¯¥æ¹æ³å¨äººä½è¿å¨åæåæºè½¨è¿¹çææ¬å¯¹é½æ¹é¢ä¹åå¾äºæåï¼TMR-ScoreåCLaTr-Scoreï¼ï¼è¡¨æçæåå®¹ä¸ææ¬æè¿°æ´å ä¸è´ã</li>
<li><strong>çµå½±æå½±æä¹ä¸çæå¾ï¼</strong> å®æ§ç»æè¡¨æï¼è¯¥æ¹æ³çæçæå¾å¨çµå½±æå½±ä¸æ´å·æä¹ï¼å°è¯¥ä»»å¡çç°æææ¯æ°´å¹³æ¨åäºæ°çé«åº¦ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåå½åæ¹æ³çå·ä½å±éæ§ï¼ä½å¯ä»¥ä»å¶æªæ¥å·¥ä½æ¹åä¸­æ¨æ­åºä¸äºæ½å¨çéå¶ï¼</p>
<ul>
<li><strong>æå¾ç²åº¦ï¼</strong> ç®åçæå¾å¯è½ä»æ¯ç¸å¯¹å®è§çï¼å°æªå®ç°æ´ç²¾ç»çæå¾æ§å¶ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ©å±è¾å©æ¨¡ææ¹æ³å°å¶ä»é¢åï¼</strong> å°è¾å©æ¨¡ææ¹æ³æ¨å¹¿å°å¶ä»å¤æ¨¡æçæä»»å¡ä¸­ã</li>
<li><strong>å®ç°æ´ç²¾ç»çæå¾æ§å¶ï¼</strong> æ¢ç´¢å¦ä½å®ç°æ´ç²¾ç»çæå¾æ§å¶ï¼ä¾å¦éå¯¹ç¹å®èº«ä½é¨ä½çæå¾ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥âå±å¹æå¾âä½ä¸ºè¾å©æ¨¡æï¼å¼åæ§å°å°äººä½è¿å¨åæåæºè½¨è¿¹ççæä»»å¡ç»ä¸ä¸ºææ¬æ¡ä»¶ä¸çèåçæé®é¢ãå¶æ¨¡åæ å³çæ¡æ¶åæ°é¢çè¾å©éæ ·æºå¶ï¼ç»åæ°åå¸çPulpMotionæ°æ®éï¼æ¾èæåäºçæåå®¹çè¿è´¯æ§åææ¬å¯¹é½è´¨éï¼ä¸ºçµå½±æå½±é¢åå¸¦æ¥äºæ´å·æä¹çæå¾ï¼ä¸ºå¤æ¨¡æçæé¢åæ ç«äºæ°çåºåã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a simple,
model-agnostic framework that enforces multimodal coherence via an auxiliary
modality: the on-screen framing induced by projecting human joints onto the
camera.</li>
<li>Extensive experiments across DiT- and MAR-based architectures
show the generality and effectiveness of our method in generating on-frame
coherent human-camera motions, while also achieving gains on textual alignment
for both modalities.</li>
<li>Our qualitative results yield more cinematographically
meaningful framings setting the new state of the art for this task.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05097v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05097v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.05051v1'></a></p>
<h2 id="segmast3r-geometry-grounded-segment-matching"><a href="https://arxiv.org/abs/2510.05051v1">SegMASt3R: Geometry Grounded Segment Matching</a></h2>
<p><strong>Authors:</strong> Rohit Jayanti, Swayam Agrawal, Vansh Garg, Siddharth Tourani, Muhammad Haris Khan, Sourav Garg, Madhava Krishna</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>SegMASt3R: Geometry Grounded Segment Matching è®ºææè¦</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®¡ç®æºè§è§é¢åä¸­âå®½åºçº¿å¾åå¯¹çè¯­ä¹æå ä½è¿è´¯åºåï¼å³å¾åçæ®µï¼å¹éâè¿ä¸å·ææææ§çé®é¢ãä¸ä¼ ç»çå³é®ç¹å¹éä¸åï¼çæ®µå¹ééè¦å»ºç«å¾åé´ç»æååºåçå¯¹åºå³ç³»ï¼è¿å¨å­å¨æç«¯è§è§ååï¼é«è¾¾180åº¦ï¼ãé®æ¡ãåç§åååè§è§ååçæåµä¸å°¤å¶å°é¾ãç°ææ¹æ³ï¼åæ¬ä¾èµ2Dçç£çå±é¨ç¹å¾å¹éåè§é¢ä¼ æ­å¨ï¼å¨è¿ç§å®½åºçº¿æ¡ä»¶ä¸è¡¨ç°ä¸ä½³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å©ç¨3Dåºç¡æ¨¡åï¼3DFMï¼çå½çº³åç½®ï¼</strong> è®ºæçæ ¸å¿åæ°å¨äºå©ç¨é¢è®­ç»ç3Dåºç¡æ¨¡åMASt3Rçå¼ºå¤§ç©ºé´çè§£è½åæ¥è§£å³å®½åºçº¿çæ®µå¹éé®é¢ãMASt3Rå¨åç§3Dè§è§æ°æ®éä¸è¿è¡è®­ç»ï¼è½å¤ææåºæ¯çæ·±åº¦ãå½¢ç¶åå§¿æç­ç©ºé´åç»æå±æ§ï¼å¶å ä½æç¥è¡¨ç¤ºå¯¹äºå¤çæç«¯è§è§ååè³å³éè¦ã
*   <strong>SegMASt3Ræ¶æï¼</strong> è®ºææåºäºä¸ç§åä¸ºSegMASt3Rçæ¶æï¼å®å¨MASt3Réª¨å¹²ç½ç»çåºç¡ä¸ï¼å¼å¥äºä¸ä¸ªè½»éçº§çâçæ®µç¹å¾å¤´âï¼Segment-Feature Headï¼ãè¯¥å¤´é¨å°MASt3Rè§£ç å¨è¾åºçè¡¥ä¸çº§ç¹å¾è½¬æ¢ä¸ºçæ®µçº§æè¿°ç¬¦ã
*   <strong>å¯å¾®åæä¼ä¼ è¾å±ï¼</strong> çæ®µæè¿°ç¬¦éè¿ä¸ä¸ªå¯å¾®åçæä¼ä¼ è¾å±è¿è¡å¹éï¼ä»¥å»ºç«å¾åé´ççæ®µå¯¹åºå³ç³»ï¼å¹¶éè¿è¡åargmaxæä½çææç»å¹éç»æã
*   <strong>å¯å­¦ä¹ çâåå¾æ¡¶âï¼Dustbinï¼ï¼</strong> ä¸ºäºå¤çå®½åºçº¿è®¾ç½®ä¸å¯è½å­å¨çæ å¹éçæ®µï¼æ¨¡åå¼å¥äºä¸ä¸ªå¯å­¦ä¹ çâåå¾æ¡¶âè¡ååå°äº²åç©éµä¸­ï¼ä»¥å¸æ¶éå¹éé¡¹ï¼ä»èæé«å¹éçé²æ£æ§ã
*   <strong>ç«¯å°ç«¯è®­ç»ï¼</strong> æ´ä¸ªå¹éå±ä¸ä¸æ¸¸çæ®µç¼ç å¨åä¸æ¸¸ä»»å¡æå¤±ä¸èµ·è¿è¡ç«¯å°ç«¯è®­ç»ï¼éç¨SuperGlueçäº¤åçµæå¤±å½æ°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èä¼äºç°æææ¯ï¼</strong> SegMASt3Rå¨ScanNet++åReplicaæ°æ®éä¸ï¼å¨AUPRCï¼ç²¾ç¡®åº¦-å¬åçæ²çº¿ä¸é¢ç§¯ï¼ææ ä¸ï¼æ¯åæ¬SAM2è§é¢ä¼ æ­å¨åå±é¨ç¹å¾å¹éæ¹æ³å¨åçæåè¿æ¹æ³é«åºé«è¾¾30%ãè¿è¡¨æå¶å¨å¤çå®½åºçº¿å¾åå¯¹æ¹é¢çåè¶æ§è½ã
*   <strong>å¨ä¸æ¸¸ä»»å¡ä¸­çå®ç¨æ§ï¼</strong> è®ºæè¿ä¸æ­¥è¯æäºSegMASt3Rå¨3Då®ä¾æ å°åç®æ ç¸å¯¹å¯¼èªç­ç¸å³ä¸æ¸¸ä»»å¡ä¸­çå®éæç¨ï¼å¹¶ä¸å¨è¿äºä»»å¡ä¸­ä¹ä¼äºç«äºå¯¹æã
*   <strong>å¼ºå¤§çæ³åè½åï¼</strong> æ¨¡åå¨æªè§è¿çå®¤åæ°æ®éReplicaä¸è¡¨ç°åºè¯å¥½çæ³åè½åãå³ä½¿å¨å·ææææ§çå®¤å¤MapFreeæ°æ®éä¸ï¼éè¿ç®åçå¯å­¦ä¹ âåå¾æ¡¶âåæ°æ ¡åï¼æ¨¡åä¹è½æ¾èç¼©å°ä¸é¢åè½¬ç§»ç¸å³çæ§è½å·®è·ï¼å±ç¤ºäºå¶å­¦ä¹ å°çå ä½ç¹å¾çå¼ºå¤§éåºæ§ã
*   <strong>å¯¹åªå£°åå²æ©ç çé²æ£æ§ï¼</strong> å³ä½¿å¨FastSAMçæçåªå£°åå²æ©ç æ¡ä»¶ä¸ï¼SegMASt3Rä¹è½ä¿ææ¾èçæ§è½ä¼å¿ï¼è¿è¯å®äºå¶å­¦ä¹ å°çå ä½åéªå¨ä¸ä¸è´ååªå£°è¾å¥ä¸çé²æ£æ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æªå¨ä¸»è®ºæä¸­æç¡®è®¨è®ºï¼</strong> è®ºæå¨æ­£æä¸­å¹¶æªè®¾ç½®ä¸é¨çâå±éæ§âç« èï¼ä½æ ¹æ®NeurIPSæ¸åçåç­ï¼ä½èè¡¨ç¤ºå±éæ§å¨è¡¥åææä¸­æææåã
*   <strong>è®¡ç®èµæºï¼</strong> è®ºææå°å¨ScanNet++ä¸è®­ç»æ¨¡åéè¦22å°æ¶ï¼åæ¬¡æ¨çï¼æ¹å¤çå¤§å°ä¸º1ï¼éè¦0.579ç§ï¼è¿å¯è½æç¤ºäºæ¨¡åå¨æäºåºç¨åºæ¯ä¸çè®¡ç®ææ¬ã
*   <strong>ä»£ç åæ°æ®åå¸ï¼</strong> è®ºæè¡¨ç¤ºå°å¨æ¥åååå¸ä»£ç åè®­ç»å¾åå¯¹ï¼è¿æå³çå¨è®ºææäº¤æ¶ï¼ä»£ç åæ°æ®å°æªå¬å¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæå¹¶æªæç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åå±éæ§ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨æ¹åï¼
*   <strong>è¿ä¸æ­¥æåå®½åºçº¿å¹éæ§è½ï¼</strong> å°½ç®¡SegMASt3Rå·²åå¾æ¾èè¿å±ï¼ä½ä»ææåç©ºé´ï¼å°¤å¶æ¯å¨æç«¯è§è§åååæç¥å®ä¾æ··å ç­å¤æåºæ¯ä¸ã
*   <strong>æ´å¹¿æ³çæ³åè½åï¼</strong> æ¢ç´¢å¦ä½è¿ä¸æ­¥æåæ¨¡åå¨æ´å¤æ ·åãæ´å·æææ§çå®¤å¤åºæ¯æè·¨é¢åæ°æ®éä¸çæ³åè½åï¼å¯è½æ¶åæ´å¤æçé¢åéåºææ¯ã
*   <strong>è®¡ç®æçä¼åï¼</strong> éå¯¹æ¨¡åå¨æ¨çåè®­ç»æ¶çè®¡ç®ææ¬è¿è¡ä¼åï¼ä½¿å¶æ´éç¨äºèµæºåéçå®æ¶åºç¨ã
*   <strong>ç»åå¶ä»3Dåºç¡æ¨¡åï¼</strong> æ¢ç´¢å°SegMASt3Rçæ¡æ¶åºç¨äºå¶ä»æ°å´ç3Dåºç¡æ¨¡åï¼ä»¥è¯ä¼°å¶æ§è½åéç¨æ§ã
*   <strong>æ´å¤æçä¸æ¸¸ä»»å¡ï¼</strong> å°SegMASt3Råºç¨äºæ´å¹¿æ³çæºå¨äººæç¥åå¯¼èªä»»å¡ï¼ä¾å¦æ´å¤æçåºæ¯çè§£ãäººæºäº¤äºç­ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change.</li>
<li>Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.05051v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.05051v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04876v1'></a></p>
<h2 id="benthicat-an-opti-acoustic-dataset-for-advancing-benthic-classification-and-habitat-mapping"><a href="https://arxiv.org/abs/2510.04876v1">BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping</a></h2>
<p><strong>Authors:</strong> Hayat Rajani, Valerio Franchi, Borja Martinez-Clavel Valles, Raimon Ramos, Rafael Garcia, Nuno Gracias</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV, cs.LG, I.2.6; I.4.6; I.5.1; I.5.4</p>
<p><strong>Abstract:</strong></p>
<p>Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®æ¯å¼å¥äºä¸ä¸ªåä¸º BenthiCat çå¤§è§æ¨¡å¤æ¨¡ææ°æ®éï¼ç¨äºæµ·åºæ æ¯å°æµç»ãè¯¥æ°æ®éåå«è¿ç¾ä¸å¼ ä¾§æ«å£°çº³ (SSS) å¾åï¼è¾ä»¥æ°´æ·±å¾åæ¥èªèªä¸»æ°´ä¸èªè¡å¨ (AUV) çå±éååå­¦å¾åï¼å¶ä¸­çº¦ 36,000 å¼  SSS å¾åå·²æå¨æ æ³¨åå²æ©ç ãéè¿åå¸åå§ä¼ æå¨æ°æ®ãé¶åµå¾ãé¢å¤çåæ æ³¨å·¥å·ï¼è¯¥å·¥ä½æ¨å¨ä¸ºæ°´ä¸æ æ¯å°æµç»æä¾ä¸ä¸ªæ åååºåï¼å¹¶ä¿è¿å¤ä¼ æå¨èååèªä¸»æµ·åºåç±»çç ç©¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºæå»ºå¹¶åå¸äºä¸ä¸ª<strong>å¤§è§æ¨¡ãå¤æ¨¡æãä¸é¨åæ æ³¨çâå£°å­¦-åå­¦âæ°æ®é</strong>ï¼ä¸é¨éå¯¹æµ·åºæ æ¯å°æµç»ãå¶æ¹æ³è®ºäº®ç¹åæ¬ï¼</p>
<ul>
<li><strong>å¤æ¨¡ææ°æ®éæï¼</strong> å°ä¾§æ«å£°çº³ (SSS) å¾åï¼æä¾å¹¿åè¦çååºè´¨ä¿¡æ¯ï¼ä¸åå­¦å¾åï¼æä¾é«åè¾¨ççº¹çåç»èï¼ä»¥åæ°´æ·±å¾ç»åï¼åæäºåä¸ä¼ æå¨æ°æ®çå±éæ§ã</li>
<li><strong>å¤§è§æ¨¡æ æ³¨ï¼</strong> æä¾äºçº¦ 36,000 å¼  SSS å¾åçæå¨åå²æ©ç æ æ³¨ï¼è¿å¯¹äºçç£å­¦ä¹ æ¨¡åçå¾®è°è³å³éè¦ï¼å°¤å¶æ¯å¨æ°æ®ç¨ç¼ºçæµ·æ´é¢åã</li>
<li><strong>ç©ºé´å³èä¸è·¨æ¨¡æå­¦ä¹ ï¼</strong> å¼ºè°äºå°åå­¦å¾åä¸ç¸åºç SSS ç¦çè¿è¡ç©ºé´å³èï¼ä»¥ä¿è¿èªçç£ãè·¨æ¨¡æè¡¨å¾å­¦ä¹ ï¼è¿å¯¹äºè§£å³ AUV å¤ä¼ æå¨æ°æ®èåçææè³å³éè¦ã</li>
<li><strong>å·¥å·é¾æ¯æï¼</strong> éæ°æ®éä¸ååå¸äºå¼æºçé¢å¤çåæ æ³¨å·¥å·ï¼æå¤§å°éä½äºç ç©¶äººåçä½¿ç¨é¨æ§ï¼é¼å±äºç¤¾åºåä¸åç®æ³å¼åã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>è¯¥ç ç©¶å¯¹è®¡ç®æºè§è§åæµ·æ´ç§å­¦é¢åå·ææ¾èçæ½å¨å½±åï¼</p>
<ul>
<li><strong>æ¨å¨æµ·åºåç±»æ¨¡ååå±ï¼</strong> BenthiCat æ°æ®éå°æä¸ºå¼åååºåæµè¯æ°çæºå¨å­¦ä¹ æ¨¡åï¼ç¹å«æ¯æ·±åº¦å­¦ä¹ æ¨¡åï¼çå®è´µèµæºï¼ç¨äºæµ·åºåºè´¨åç±»ãæ æ¯å°è¯å«åå¼å¸¸æ£æµã</li>
<li><strong>ä¿è¿å¤ä¼ æå¨èåç ç©¶ï¼</strong> å¶å¤æ¨¡æç¹æ§å°æ¿å±ç ç©¶äººåæ¢ç´¢æ´åè¿çä¼ æå¨èåææ¯ï¼ä»¥ç»åå£°å­¦ååå­¦æ°æ®çä¼å¿ï¼æé«æ°´ä¸ç¯å¢æç¥çé²æ£æ§ååç¡®æ§ã</li>
<li><strong>å éèªä¸»æ°´ä¸èªè¡å¨ (AUV) è½åï¼</strong> æ´å¥½çæµ·åºæµç»ååç±»è½åå°ç´æ¥æå AUV çèªä¸»å¯¼èªãç®æ è¯å«åç§å­¦èå¯è½åï¼ä½¿å¶è½æ´ææå°æ§è¡ä»»å¡ã</li>
<li><strong>å»ºç«æ åååºåï¼</strong> ä½ä¸ºç¬¬ä¸ä¸ªå¤§è§æ¨¡çå£°å­¦-åå­¦æµ·åºæ°æ®éï¼å®æææä¸ºè¯¥é¢åçæ åååºåï¼ä¿è¿ä¸åç®æ³åæ¹æ³çå¬å¹³æ¯è¾ï¼å éç ç©¶è¿å±ã</li>
<li><strong>æ¯ææµ·æ´çæä¿æ¤ï¼</strong> åç¡®çæ æ¯å°æµç»æ¯æµ·æ´ä¿æ¤ãèµæºç®¡çåç¯å¢çæµçåºç¡ï¼è¯¥æ°æ®éå°ç´æ¥æ¯æè¿äºåºç¨ã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>æµ·æ´çç©å­¦ä¸çæå­¦ï¼</strong> ç¨äºç ç©¶åºæ çç©åå¸ãæ æ¯å°å¥åº·è¯ä¼°ãçç©å¤æ ·æ§çæµã</li>
<li><strong>æµ·æ´å°è´¨å­¦ï¼</strong> ç¨äºæµ·åºå°è²åæãåºè´¨ç±»åè¯å«ãæ²ç§¯ç©ç ç©¶ã</li>
<li><strong>æ°´ä¸æºå¨äººä¸èªä¸»ç³»ç»ï¼</strong> æå AUV çç¯å¢æç¥ãè·¯å¾è§åãç®æ è¯å«åèªä¸»å³ç­è½åã</li>
<li><strong>æµ·æ´å·¥ç¨ï¼</strong> ç¨äºæµ·åºç®¡çº¿ãçµç¼éºè®¾è·¯å¾è§åãæ°´ä¸åºç¡è®¾æ½æ£æ¥ã</li>
<li><strong>æ¸ä¸ç®¡çï¼</strong> è¯ä¼°æ¸ä¸èµæºãè¯å«é±¼ç±»æ æ¯å°ã</li>
<li><strong>åäºä¸å®å¨ï¼</strong> æ°´ä¸ç®æ æ£æµãæ°´é·è¯å«ãæµ·åºå°å½¢åæã</li>
<li><strong>è®¡ç®æºè§è§ä¸æºå¨å­¦ä¹ ï¼</strong> ç¹å«æ¯å¤æ¨¡æå­¦ä¹ ãèªçç£å­¦ä¹ ãè¯­ä¹åå²ãç®æ æ£æµåä¼ æå¨èåç®æ³çå¼åä¸æµè¯ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>æ æ³¨èå´ï¼</strong> å°½ç®¡æ 36,000 å¼  SSS ç¦çè¿è¡äºæå¨åå²æ æ³¨ï¼ä½ç¸å¯¹äºè¿ç¾ä¸å¼ çæ»éï¼è¿ä»ç¶æ¯ç¸å¯¹è¾å°çä¸é¨åãè¿æå³çå¨æäºåºç¨ä¸­ï¼å¯è½éè¦è¿ä¸æ­¥çåçç£ææ çç£å­¦ä¹ æ¹æ³æ¥å©ç¨æªæ æ³¨æ°æ®ã</li>
<li><strong>åå­¦æ°æ®è¦çï¼</strong> æè¦æå°åå­¦å¾åæ¥èªâç®æ è°æ¥ (targeted surveys)âï¼è¿å¯è½æå³çåå­¦æ°æ®çè¦çèå´ä¸å¦ SSS æ°æ®å¹¿æ³ï¼ä¸å¯è½å­å¨ç©ºé´ä¸çç¨çæ§æä¸è¿ç»­æ§ãè¿ä¼å½±ååå­¦æ°æ®å¨å¹¿åæµç»ä¸­çç´æ¥åºç¨ï¼ä½å¯¹äºå±é¨é«ç²¾åº¦åæåè·¨æ¨¡æå­¦ä¹ ä»æå·¨å¤§ä»·å¼ã</li>
<li><strong>å°çå±éæ§ï¼</strong> æ°æ®éæ¶éäºâå æ³°ç½å°¼äºæµ·å²¸ (coast of Catalonia, Spain)âï¼è¿æå³çå¶å°çèå´æéãæµ·åºç¯å¢å·æé«åº¦å¤æ ·æ§ï¼è¯¥æ°æ®éçæ³åè½åå¯è½éè¦è¿ä¸æ­¥éªè¯ï¼ä»¥éåºå¨çä¸ååºåçåºè´¨ç±»ååå£°å­¦/åå­¦ç¹å¾ã</li>
<li><strong>æ æ³¨ç²åº¦/ç±»å«ï¼</strong> æè¦æªè¯¦ç»è¯´æåå²æ©ç çæ æ³¨ç²åº¦ï¼ä¾å¦ï¼æ¯ç²ç¥çåºè´¨ç±»åè¿æ¯ç²¾ç»ççç©ç¾¤è½ï¼ä»¥åå·ä½åå«çç±»å«æ°éãè¿ä¼å½±åæ¨¡åè½å¤è¯å«çç»èç¨åº¦ã</li>
<li><strong>æ°æ®è´¨éä¸åªå£°ï¼</strong> ä¾§æ«å£°çº³æ°æ®å®¹æåå°æ°´ä½æ¡ä»¶ãä¼ æå¨åæ°åæµ·åºå°å½¢å¤ææ§çå½±åèäº§çåªå£°åä¼ªå½±ãæè¦æªæåæ°æ®é¢å¤çä¸­å¯¹è¿äºææçå¤çç¨åº¦ã</li>
<li><strong>æ¶é´ç»´åº¦ï¼</strong> æè¦æªæåæ°æ®æ¶éçæ¶é´è·¨åº¦ãå¦ææ°æ®æ¯å¨ä¸åæ¶é´ç¹æ¶éçï¼å¯è½ä¼å­å¨ç¯å¢ååï¼å¦å­£èæ§çç©ç¾¤è½ååï¼å¸¦æ¥çææã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼BenthiCat æ°æ®éæ¯è®¡ç®æºè§è§åæµ·æ´ç§å­¦äº¤åé¢åçä¸ä¸ªéè¦éç¨ç¢ãå®éè¿æä¾ä¸ä¸ªåææªæçå¤§è§æ¨¡ãå¤æ¨¡ææ°æ®éï¼æææ¾èå éæµ·åºæ æ¯å°æµç»åèªä¸»æ°´ä¸æºå¨äººææ¯çåå±ï¼ä¸ºæµ·æ´çæç ç©¶åä¿æ¤æä¾å¼ºæåçå·¥å·ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research.</li>
<li>This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04876v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04876v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04759v1'></a></p>
<h2 id="progressive-gaussian-transformer-with-anisotropy-aware-sampling-for-open-vocabulary-occupancy-prediction"><a href="https://arxiv.org/abs/2510.04759v1">Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction</a></h2>
<p><strong>Authors:</strong> Chi Yan, Dan Xu</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chi YanåDan Xuæ°åçè®ºæãProgressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Predictionãçå¨é¢æè¦ã</p>
<hr />
<h3 id="progressive-gaussian-transformer-with-anisotropy-aware-sampling-for-open-vocabulary-occupancy-prediction_1">è®ºææè¦ï¼Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³3Då ç¨é¢æµä»»å¡ä¸­çä¸ä¸ªæ ¸å¿ææï¼å¦ä½å¨èªå¨é©¾é©¶ç³»ç»ä¸­å®ç°å¼æ¾è¯æ±ç3Då ç¨é¢æµï¼åæ¶åæç°ææ¹æ³å¨å¤çåºæ¯ç»èåè®¡ç®æçæ¹é¢çå±éæ§ãå·ä½æ¥è¯´ï¼ç°ææ¹æ³è¦ä¹ä½¿ç¨åºå®è¯­ä¹ç±»å«ï¼éå¶äºå¯¹æªç¥ç©ä½çæç¥ï¼è¦ä¹éç¨ç¨çé«æ¯è¡¨ç¤ºï¼é¾ä»¥ææåºæ¯ä¸­çå°ç©ä½ï¼æèéç¨å¯éè¡¨ç¤ºï¼å¯¼è´å·¨å¤§çè®¡ç®å¼éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäºPG-Occæ¡æ¶ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>æ¸è¿å¼é«æ¯Transformeræ¡æ¶ï¼Progressive Gaussian Transformer Frameworkï¼ï¼</strong> PG-Occéè¿ä¸ç§åæ°çæ¸è¿å¼å¨çº¿ç¨ å¯åï¼Progressive Online Densification, PODï¼ç­ç¥ï¼è¿­ä»£å°å¢å¼º3Dé«æ¯è¡¨ç¤ºãå®ä»ç²ç¥çåºç¡é«æ¯å¼å§å»ºæ¨¡å¨å±åºæ¯ç»æï¼ç¶åæ ¹æ®æç¥è¯¯å·®éæ­¥ç»åæªååæç¥çåºåï¼ä»èææç²¾ç»çåºæ¯ç»èãè¿ç§åé¦ç­ç¥é¿åäºæ¢¯åº¦ååä¼ æ­çè®¡ç®å¼éï¼æé«äºæçã</li>
<li><strong>ååå¼æ§æç¥éæ ·ç­ç¥ï¼Anisotropy-aware Sampling Strategy, AFSï¼ï¼</strong> éå¯¹é«æ¯è¡¨ç¤ºçååå¼æ§ç¹æ§ï¼PG-Occå¼å¥äºä¸ç§èªéåºéæ ·æ¹æ³ãè¯¥æ¹æ³æ ¹æ®æ¯ä¸ªé«æ¯çç©ºé´åå¸è°æ´å¶æåéï¼å¹¶å°å¶æå½±å°å·æä¸åæåéçç¹å¾å¹³é¢ä¸ï¼ä»èå®ç°æ´ææçæ¶ç©ºç¹å¾èååæ´ä¸°å¯çåºæ¯ä¿¡æ¯æè·ã</li>
<li><strong>éå¯¹ç§°èªæ³¨æåæºå¶ï¼Asymmetric Self-Attention, ASAï¼ï¼</strong> ä¸ºäºå¨æ¸è¿å¼å»ºæ¨¡ä¸­ä¿æè®­ç»ç¨³å®æ§ï¼ASAç¡®ä¿æ°æ·»å çé«æ¯ä¸ä¼å¹²æ°å·²ä¼åçé«æ¯ï¼åæ¶åè®¸æ°é«æ¯å©ç¨ç°æä¿¡æ¯è¿è¡èªæå®åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
PG-Occå¨Occ3D-nuScenesæ°æ®éä¸åå¾äºæåè¿çæ§è½ï¼ç¸å¯¹äºä¹åè¡¨ç°æä½³çæ¹æ³ï¼mIoUç¸å¯¹æåäº14.3%ãå¨nuScenesæ£ç´¢æ°æ®éä¸ï¼PG-Occä¹æ¾èä¼äºç°æåºäºè§è§çæ¹æ³ãè¿äºç»æè¡¨æï¼</p>
<ul>
<li><strong>åè¶çæç¥åç¡®æ§ï¼</strong> PG-Occè½å¤æ´åç¡®ãæ´è¿è´¯å°é¢æµ3Då ç¨ï¼æææ´ç²¾ç»çç»æç»èï¼å¹¶çææ´åãæ´çå®çè¡¨é¢ã</li>
<li><strong>å¼æ¾è¯æ±è½åï¼</strong> è¯¥æ¡æ¶è½å¤æ ¹æ®ä»»æææ¬æ¥è¯¢è¿è¡é¶æ ·æ¬è¯­ä¹3Då ç¨é¢æµï¼æææ¡¥æ¥è¯­è¨çè§£åç©ºé´æç¥ä¹é´çé¸¿æ²ã</li>
<li><strong>é«ææ§ï¼</strong> å°½ç®¡æ¨¡åå¤æåº¦å¢å ï¼ä½éè¿æ¸è¿å¼ç¨ å¯åååé¦ç­ç¥ï¼PG-Occå¨è®­ç»æ¶é´åæ¨çéåº¦ä¸ä»å·æç«äºåï¼å®ç°äºæçä¸åç¡®æ§çå¹³è¡¡ã</li>
<li><strong>å ä½ç²¾åº¦ï¼</strong> å¨æ·±åº¦ä¼°è®¡æ¹é¢ï¼PG-Occä¹è¡¨ç°åºè²ï¼çè³è¶è¶äºåå§çç£æ ç­¾çç²¾åº¦ï¼è¿å¾çäºå¤è§è§æ·±åº¦ä¸è´æ§åç¹å¾è¿è´¯æ§å¸¦æ¥çå ä½çº¦æã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­ä¹å¦è¯å°æåºäºPG-Occçå ä¸ªå±éæ§ï¼</p>
<ul>
<li><strong>ç¨çè§è§ä¸çé«æ¯å°ºåº¦çº¦æï¼</strong> å¨é©¾é©¶åºæ¯ä¸­ï¼ç±äºè§è§ç¨çï¼çº¦æé«æ¯å¨æ·±åº¦ä¸çå°ºåº¦å·ææææ§ï¼å¯è½å¯¼è´âå¼¹åºâä¼ªå½±ï¼popping artifactsï¼ã</li>
<li><strong>åå­åè®¡ç®ææ¬ï¼</strong> éçå»ºæ¨¡è¿ç¨ä¸­é«æ¯æ°éçå¢å ï¼åå­åè®¡ç®ææ¬ä¹ä¼éä¹å¢é¿ï¼å¯è½å½±åå®æ¶æ§è½ã</li>
<li><strong>å°ç©ä½æ§è½ï¼</strong> å°½ç®¡å¨æ£æµä¸­åç©ä½æ¹é¢è¡¨ç°åºè²ï¼ä½ç±äºç²ç³çä½ç´ åè¾¨çï¼0.4ç±³ï¼ï¼PG-Occå¨å°ç©ä½ä¸çæ§è½ç¥ä½ã</li>
<li><strong>é®æ¡é®é¢ï¼</strong> å¨ç¼ºä¹è§è§è§æµçåºåï¼èªçç£æ¹æ³å¨è¿è¡åç¡®é¢æµæ¶å¯è½é¢ä¸´ææã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä¸ºäºè§£å³ä¸è¿°å±éæ§ï¼ä½èæåºäºæªæ¥çç ç©¶æ¹åï¼</p>
<ul>
<li>æ¢ç´¢<strong>4Dé«æ¯æ¹æ³</strong>ï¼ä»¥æ´å¥½å°å¤çæ¶åºä¿¡æ¯åå¨æåºæ¯ã</li>
<li>ç ç©¶<strong>å¤è§è§çº¦æ</strong>ï¼ä»¥è¿ä¸æ­¥æé«å¨ç¨çè§è§ä¸çé«æ¯å°ºåº¦çº¦æåæ´ä½åºæ¯è¡¨ç¤ºçåç¡®æ§ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction.</li>
<li>Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture.</li>
<li>Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04759v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04759v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04739v1'></a></p>
<h2 id="exposureengine-oriented-logo-detection-and-sponsor-visibility-analytics-in-sports-broadcasts"><a href="https://arxiv.org/abs/2510.04739v1">ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts</a></h2>
<p><strong>Authors:</strong> Mehdi Houshmand Sarkhoosh, FrÃ¸y Ãye, Henrik Nestor SÃ¸rlie, Nam Hoang Vu, Dag Johansen, Cise Midoglu, Tomas Kupka, PÃ¥l Halvorsen</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æ»ç»Mehdi Houshmand Sarkhooshç­äººæ°åçè®ºæâExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcastsâã</p>
<hr />
<h3 id="exposureengine">ExposureEngine: ä½è²èµäºå¹¿æ­ä¸­å®åæ å¿æ£æµä¸èµå©åå¯è§æ§åæ</h3>
<p><strong>è®ºææè¦</strong></p>
<p>è¿ç¯è®ºæä»ç»äº <strong>ExposureEngine</strong>ï¼ä¸ä¸ªç«¯å°ç«¯ç³»ç»ï¼æ¨å¨è§£å³ä½è²èµäºå¹¿æ­ä¸­èµå©åå¯è§æ§åæçä¼ ç»ææãè¯¥ç³»ç»éè¿å¼å¥å®åè¾¹çæ¡ï¼OBBï¼æ£æµåè¯­è¨é©±å¨çåæå±ï¼æ¾èæé«äºèµå©åæååº¦éè¡¡çåç¡®æ§åå¯è§£éæ§ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>ä¼ ç»ä¸ï¼ä½è²èµäºå¹¿æ­ä¸­çèµå©åå¯è§æ§éåæ¯ä¸ä¸ªå³é®çè¥éä»»å¡ï¼ä½åéäºæå¨ãä¸»è§ä¸é¾ä»¥æ©å±çåææ¹æ³ãç°æçèªå¨åç³»ç»éå¸¸ä¾èµäºè½´å¯¹é½æ°´å¹³è¾¹çæ¡ï¼HBBï¼ï¼å½æ å¿å å¨ææåæºè§åº¦åéè§ç¸åèæè½¬æå¾ææ¶ï¼HBBä¼å¯¼è´ä¸åç¡®çæååº¦éãHBBä¼åå«éæ å¿èæ¯åºåï¼ä»èé«ä¼°æ å¿å°ºå¯¸åå±å¹çªåºåº¦ãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å®ç°ç²¾ç¡®ãæè½¬æç¥çæ å¿æ£æµï¼å¹¶å°å¶è½¬åä¸ºå¯å®¡è®¡åå¯è§£éçèµå©åå¯è§æ§åæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<ul>
<li><strong>å®åè¾¹çæ¡ï¼OBBï¼æ£æµæ¨¡åï¼</strong> ExposureEngineçæ ¸å¿åæ°å¨äºéç¨åºäºYOLOv11çOBBæ£æµæ¨¡åãä¸HBBä¸åï¼OBBè½å¤ä¸ºæ¯ä¸ªæ å¿æä¾å ä½ä¸ç²¾ç¡®çæåï¼æ è®ºå¶å¨å±å¹ä¸çæ¹åå¦ä½ï¼ä»èé¿åäºèæ¯åºåçè¿åº¦åå«ï¼æé«äºå°ºå¯¸åä½ç½®ä¼°è®¡çåç¡®æ§ã</li>
<li><strong>æ°æ°æ®éçåå»ºï¼</strong> ä¸ºäºè®­ç»åè¯ä¼°å¶æ£æµå¨ï¼ä½èæå»ºäºä¸ä¸ªåå«1,103å¸§çå¸ç²¾è±è¶³çæ¯èµçæ°æ°æ®éï¼å¶ä¸­åå«670ä¸ªç¬ç¹çèµå©åæ å¿ï¼å¹¶ä½¿ç¨OBBè¿è¡æ æ³¨ãè¿æ¯é¦ä¸ªæä¾åºäºOBBçè¶³çå¹¿æ­èµå©åæ å¿æ æ³¨çå¼æ¾æ°æ®éã</li>
<li><strong>è´¨éæç¥æå¤±å½æ°ï¼Varifocal Loss, VFLï¼ï¼</strong> ä¸ºäºè§£å³é¿å°¾ç±»å«åå¸åç±»å«ä¸å¹³è¡¡é®é¢ï¼æ¨¡åéç¨äºVFLä½ä¸ºåç±»æå¤±ï¼å®è½ææéä½ç®åè´æ ·æ¬çæéï¼å¹¶æ ¹æ®å®ä½è´¨éæé«æ­£æ ·æ¬çæéï¼ä»èæ´å¥½å°æ ¡åç±»å«ç½®ä¿¡åº¦ä¸è¾¹çæ¡ç²¾åº¦ã</li>
<li><strong>ç«¯å°ç«¯åæç®¡éï¼</strong> ç³»ç»å°OBBæ£æµç»æéæå°ä¸ä¸ªåæç®¡éä¸­ï¼ç¨äºè®¡ç®ç²¾ç¡®çå¯è§æ§ææ ï¼å¦æåæ¶é¿åå±å¹è¦ççã</li>
<li><strong>è¯­è¨é©±å¨çæºè½ä½å±ï¼</strong> å¼å¥äºä¸ä¸ªåºäºå¤§åè¯­è¨æ¨¡åï¼LLMï¼çæºè½ä½å±ï¼ä½¿ç¨æ·è½å¤éè¿èªç¶è¯­è¨æ¥è¯¢çææ¥åãæè¦ååªä½åå®¹ï¼æ¯ææåãç»è®¡æ¥è¯¢ååå®¹çæç­æä½ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<ul>
<li><strong>é«æ£æµæ§è½ï¼</strong> OBBæ£æµæ¨¡åå¨æµè¯éä¸å®ç°äº0.859çå¹³åç²¾åº¦ï¼mAP@0.5ï¼ï¼ç²¾ç¡®åº¦ä¸º0.96ï¼å¬åçä¸º0.87ãè¿è¡¨æå¨åç§å¹¿æ­æ¡ä»¶ä¸ï¼æ¨¡åå¨å®ä½æ å¿æ¹é¢è¡¨ç°åºå¼ºå¤§çé²æ£æ§ã</li>
<li><strong>OBBçå ä½ç²¾åº¦ä¼å¿ï¼</strong> ä¸HBBç¸æ¯ï¼OBBè½å¤æ´ç´§å¯å°åå´æ å¿åºåï¼æå¤§éåº¦å°åå°åä½èæ¯ãç´§å¯åº¦æ¯çï¼TRï¼åæè¡¨æï¼OBBå¨æ å¿æè½¬æ¶è½æä¾æ´ç´§å¯çåå´ï¼ä»èå®ç°æ´åç¡®çå±å¹é¢ç§¯åä½ç½®ä¼°è®¡ã</li>
<li><strong>ç³»ç»æçï¼</strong> GPUå éä½¿å¾ç³»ç»è½å¤ä»¥æ¥è¿å®æ¶ï¼19.98 FPSï¼çéåº¦è¿è¡ï¼éç¨äºå®æ¶ä»ªè¡¨æ¿åèªå¨åäº®ç¹çæã</li>
<li><strong>å¯å®¡è®¡åå¯è§£éæ§ï¼</strong> å®æ´çç³»ç»ï¼åæ¬æ°æ®éååæä»ªè¡¨æ¿ï¼ä¸ºä½è²åªä½ä¸­èµå©åæµéæä¾äºä¸ä¸ªå¨é¢ãå¯å®¡è®¡åå¯è§£éçè§£å³æ¹æ¡ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong></p>
<ul>
<li><strong>æ°æ®ç¨çæ§ï¼</strong> æ¨¡åçåç¡®æ§æç»åéäºå¯ç¨æ°æ®ãçå¸è¶³çè¯­æåºçé¿å°¾ç±»å«åå¸å¯¼è´æ¯ç±»å«APçæ¹å·®å¢å ï¼å¹¶éå¶äºä¸å¸¸åºç°èµå©åçå¬åçãVFLè½ç¶ææå¸®å©ï¼ä½å¯¹äºç¨æç±»å«ï¼å¶è¡¨ç¤ºè½åä»ç¶æ¯ç¶é¢ã</li>
<li><strong>æ³åè½åï¼</strong> é²æ£çæ³åè½åéè¦è·¨ä¸åèèµãå¶ä½é£æ ¼ååºé¦è¿è¡æµè¯ã</li>
<li><strong>æ¶é´è¿è´¯æ§ï¼</strong> å¸§çº§æ£æµç»æå®¹æåºç°æ¼ç§»åéªçï¼éè¦OBBæç¥çè·è¸ªæºå¶æ¥å½¢æç¨³å®çå¯¹è±¡è½¨è¿¹ï¼ä»¥å¹³æ»ç¬æéè¯¯å¹¶é²æ­¢éå¤è®¡æ°ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<ul>
<li><strong>å¤ç®¡é½ä¸çæ°æ®ç­ç¥ï¼</strong> æ©å±è·¨æ´å¤èµå­£åèèµçè¦çèå´ï¼å¼å¥åå±æ ç­¾ï¼éç¨ç±»å«æç¥éæ ·åæéå¯¹æ§çæ°æ®å¢å¼ºï¼å¦å¤å¶-ç²è´´ï¼ï¼ä»¥åå©ç¨æªæ æ³¨å¹¿æ­çåçç£å­¦ä¹ ï¼ä»¥è§£å³ç¨æç±»å«é®é¢ã</li>
<li><strong>ä»å­å¨éåå°ä»·å¼è¯ä¼°ï¼</strong> æªæ¥çå·¥ä½åºä»éåå­å¨è½¬åè¯ä¼°ä»·å¼ãå¼åä¸ä¸ªäºä»¶å æç³»ç»ï¼å©ç¨æ¯èµåæ°æ®ï¼å¦è¿çãå°é¨ãé»çï¼ä¸ºå³é®æ¶å»åºç°çæ å¿èµäºæ´é«çä»·å¼ã</li>
<li><strong>åºåå´è¶£ï¼ROIï¼åæï¼</strong> ç»åROIåæï¼ç¹å«æ¯éå¯¹ç¤¾äº¤åªä½çåç´ROIï¼ä»¥è¡¡éæ å¿å¨ä¸åå¹³å°ååæ¸ éä¸­çå®ç¨æ§ã</li>
<li><strong>æºè½ä½å±çæ¼è¿ï¼</strong> å°æºè½ä½å±ä»ç®åçåå®¹æåå¨åå±ä¸ºæç¥å·¥å·ï¼è½å¤æ ¹æ®ä¸ä¸æåæ ¼å¼æç¥ææ èªå¨è¯å«ååç°é«ä»·å¼çæ®µã</li>
<li><strong>éªè¯åå®¡è®¡ï¼</strong> å¯¹èåææ çæç»åå·®åæ¹å·®è¿è¡ä¸¥æ ¼çå®¡è®¡ï¼ä»¥å¯¹æäººå·¥æ æ³¨ççå®æ°æ®ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼ExposureEngineéè¿å¼å¥OBBæ£æµåæºè½ä½é©±å¨çåæï¼ä¸ºä½è²èµäºå¹¿æ­ä¸­çèµå©åå¯è§æ§åææä¾äºä¸ä¸ªåæ°ä¸å¨é¢çè§£å³æ¹æ¡ãå®ä¸ä»æé«äºæ£æµçå ä½ç²¾åº¦ï¼è¿éè¿æºè½ä½å±å¢å¼ºäºç¨æ·äº¤äºåæ°æ®è§£éè½åï¼ä¸ºä½è²è¥éååªä½åæé¢åå¸¦æ¥äºæ¾èè¿æ­¥ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen.</li>
<li>To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04739v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04739v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04714v1'></a></p>
<h2 id="object-centric-representation-learning-for-enhanced-3d-scene-graph-prediction"><a href="https://arxiv.org/abs/2510.04714v1">Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction</a></h2>
<p><strong>Authors:</strong> KunHo Heo, GiHyun Kim, SuYeon Kim, MyeongAh Cho</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾KunHo Heo, GiHyun Kim, SuYeon Kim, MyeongAh Choæ°åçè®ºæâObject-Centric Representation Learning for Enhanced 3D Scene Graph Predictionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="object-centric-representation-learning-for-enhanced-3d-scene-graph-prediction_1">è®ºææè¦ï¼Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³3Dè¯­ä¹åºæ¯å¾é¢æµï¼3DSSGï¼ä¸­çæ ¸å¿ææãç°ææ¹æ³å¨ä¼åå¯¹è±¡åå³ç³»ç¹å¾çè¡¨å¾è½åæ¹é¢å­å¨ä¸è¶³ï¼è¿åº¦ä¾èµå¾ç¥ç»ç½ç»ï¼GNNï¼ä½å¤å«è½åä¸è¶³ï¼å¯¼è´å¯¹è±¡åç±»ä¸åç¡®ï¼è¿èå½±åå³ç³»é¢æµçåç¡®æ§ãå·ä½èè¨ï¼ç ç©¶åç°å¯¹è±¡ç¹å¾çè´¨éå¯¹æ´ä½åºæ¯å¾çåç¡®æ§èµ·çå³é®ä½ç¨ï¼ä¸ç°ææ¹æ³æªè½ååæ´åå³ç³»ä¿¡æ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäºä»¥ä¸å³é®åæ°ï¼
*   <strong>å¤å«æ§å¯¹è±¡ç¹å¾ç¼ç å¨ï¼Discriminative Object Feature Encoderï¼ï¼</strong> è®ºææåºå¹¶é¢è®­ç»äºä¸ä¸ªé«åº¦å¤å«æ§çå¯¹è±¡ç¹å¾ç¼ç å¨ï¼å°å¯¹è±¡è¡¨å¾å­¦ä¹ ä¸åºæ¯å¾é¢æµè§£è¦ãè¯¥ç¼ç å¨éè¿å¯¹æ¯é¢è®­ç»ç­ç¥ï¼å©ç¨3Då¯¹è±¡å®ä¾åå¶2Då¾åè§å¾åææ¬æè¿°ä¹é´çå¯¹åºå³ç³»ï¼å¢å¼ºè¯­ä¹è¡¨è¾¾è½åï¼åæ¶ä¿æå ä½ä¸åæ§ã
*   <strong>å³ç³»ç¹å¾ç¼ç å¨ï¼Relationship Feature Encoderï¼ï¼</strong> è¯¥ç¼ç å¨ææç»åäºå ä½åè¯­ä¹ç¹å¾ï¼ä»¥å®ç°æ´ä¼è¶çå³ç³»é¢æµãå®éè¿å¼å¥å±é¨ç©ºé´å¢å¼ºï¼Local Spatial Enhancement, LSEï¼æ¨¡åæ¥å¹³è¡¡é«ç»´å¯¹è±¡åµå¥åç¸å¯¹ç®åçå ä½æè¿°ç¬¦ä¹é´çä¿¡æ¯ä¸å¹³è¡¡ï¼å¹¶è®¾è®¡äºä¸ä¸ªååè¾¹ç¼é¨æ§ï¼Bidirectional Edge Gated, BEGï¼æºå¶çGNNï¼ä»¥æç¡®å»ºæ¨¡ä¸»ä½-å®¢ä½ä¸å¯¹ç§°æ§ã
*   <strong>å¨å±ç©ºé´å¢å¼ºï¼Global Spatial Enhancement, GSEï¼ï¼</strong> è¯¥æºå¶éè¿æ´åå¨å±å ä½ä½ç½®ä¿¡æ¯ï¼å°å¯¹è±¡å³ç³»æå¢åï¼ä»¥ææå¨å±ç©ºé´ä¾èµæ§ï¼è¿ä¸æ­¥æåå³ç³»é¢æµçåç¡®æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èæåå¯¹è±¡åç±»åå³ç³»é¢æµï¼</strong> è®ºæéè¿å¹¿æ³åæè¡¨æï¼å¯¹è±¡ç¹å¾çè´¨éå¯¹æ´ä½åºæ¯å¾åç¡®æ§è³å³éè¦ãææåºçå¤å«æ§å¯¹è±¡ç¹å¾ç¼ç å¨ä¸ä»æé«äºå¯¹è±¡åç±»åç¡®æ§ï¼è¿ç´æ¥æ¹åäºå³ç³»é¢æµã
*   <strong>è¶è¶ç°æSOTAæ¹æ³ï¼</strong> å°é¢è®­ç»çç¼ç å¨éæå°ç°ææ¡æ¶ä¸­æ¶ï¼å¨ææè¯ä¼°ææ ä¸é½è§å¯å°æ¾èçæ§è½æåãå¨3DSSGæ°æ®éä¸çç»¼åå®éªè¡¨æï¼è¯¥æ¹æ³æ¾èä¼äºååçæåè¿æ¹æ³ã
*   <strong>æææ´åå¤æ¨¡æä¿¡æ¯ï¼</strong> è®ºæå¼ºè°äºè§è§ãææ¬åå ä½ä¿¡å·çèåå©ç¨ï¼è½å¤äº§çæ´æ¸æ°çå¯¹è±¡åéªï¼ä»èæ¨å¨ä¸æ¸¸åºæ¯å¾ææ çæåã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç¼ºä¹3Då¯¹è±¡æ£æµè½åï¼</strong> æ¬ç ç©¶ä¸¥æ ¼éµå¾ª3DSSGçè¯ä¼°åè®®ï¼å æ­¤ä¸åå«3Då¯¹è±¡æ£æµè½åãè¿æå³çè¯¥æ¹æ³ä¸è½ç´æ¥åºç¨äºéè¦æ§è¡3Då¯¹è±¡æ£æµççå®ä¸çåºæ¯ã
*   <strong>ä¸æ¯æå¢éå¾æ´æ°ï¼</strong> è¯¥æ¹æ³éè¦æ´ä¸ªåºæ¯å¯ç¨æè½çæåºæ¯å¾ï¼ä¸æ¯æå¢éå¾æ´æ°ï¼è¿éå¶äºå¶å¨å®éåºæ¯é¨ç½²ä¸­çåºç¨ã
*   <strong>é­éè¯æ±è®¾ç½®ï¼</strong> æ¬ç ç©¶ä¸»è¦å³æ³¨é­éè¯æ±è®¾ç½®ï¼ä»¥éªè¯å¶æ ¸å¿åè®¾ï¼è¿ä¸å¼æ¾è¯æ±è®¾ç½®å­å¨å·®å¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>éæ3Då¯¹è±¡æ£æµï¼</strong> æªæ¥çå·¥ä½ç®æ æ¯å¼åä¸ä¸ªéææ¡æ¶ï¼å°3Då¯¹è±¡æ£æµä¸å¢éåºæ¯å¾çææ¨¡åç¸ç»åï¼ä»¥å®ç°æ´å®ç¨çåºæ¯å¾çæç®æ³ã
*   <strong>å©ç¨ç°æé«æ§è½å¯¹è±¡è¡¨å¾æ¹æ³ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½å©ç¨ç°æé«æ§è½å¯¹è±¡è¡¨å¾æ¹æ³ï¼ä»¥æ¾èæåæ´ä½åºæ¯å¾çææ§è½ã
*   <strong>æ¢ç´¢å¼æ¾è¯æ±è®¾ç½®ï¼</strong> å°å½åæ¡æ¶æ©å±å°å¼æ¾è¯æ±è®¾ç½®ï¼éè¿è§£è¦ç»ç²åº¦å¯¹è±¡åè°è¯ç±»å«ï¼å¹¶å¯¹é½åµå¥ç©ºé´ï¼ä»¥éåºå¼æ¾è¯æ±3Dåºæ¯å¾é¢æµã</p>
<hr />
<p>è¿ä»½æè¦çªåºäºè®ºæçæ ¸å¿è´¡ç®ï¼å³éè¿å¢å¼ºå¯¹è±¡ç¹å¾çå¤å«è½ååæææ´åå¤æ¨¡æå³ç³»ä¿¡æ¯æ¥æ¹è¿3Dåºæ¯å¾é¢æµï¼å¹¶æåºäºå¶å¨å®éåºç¨ä¸­çå±éæ§åæªæ¥çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy.</li>
<li>Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04714v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04714v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.04706v1'></a></p>
<h2 id="id-consistent-precise-expression-generation-with-blendshape-guided-diffusion"><a href="https://arxiv.org/abs/2510.04706v1">ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</a></h2>
<p><strong>Authors:</strong> Foivos Paraperas Papantoniou, Stefanos Zafeiriou</p>
<p><strong>Published:</strong> 2025-10-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Foivos Paraperas PapantoniouåStefanos Zafeiriouæ°åçè®ºæâID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="id-consistent-precise-expression-generation-with-blendshape-guided-diffusion_1">è®ºææè¦ï¼ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</h3>
<p>è¿ç¯è®ºææåºäºä¸ç§åºäºæ©æ£æ¨¡åçæ°é¢æ¡æ¶ï¼æ¨å¨å®ç°å¯¹äººè¸è¡¨æçç²¾ç»ãèº«ä»½ä¸è´ççæåç¼è¾ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
äººè¸çææ¨¡åå¨AIé©±å¨çæäºåè¿°ä¸­éè¦å·å¤ä¸¤ä¸ªæ ¸å¿è½åï¼èº«ä»½ä¸è´æ§ï¼identity consistencyï¼åå¯¹äººç±»è¡¨æ¼çç²¾ç¡®æ§å¶ï¼precise control over human performanceï¼ãå°½ç®¡è¿æåºäºæ©æ£çæ¹æ³å¨ä¿æé¢é¨èº«ä»½æ¹é¢åå¾äºæ¾èè¿å±ï¼ä½å¨ä¸æå®³èº«ä»½ä¸è´æ§çåæä¸å®ç°å¯¹ç²¾ç»è¡¨æçæ§å¶ä»ç¶æ¯ä¸ä¸ªææãç°ææ¹æ³å¾å¾é¾ä»¥ææè¡¨æçå¨é¨ç²¾åº¦åç»å¾®ä¹å¤ï¼å°¤å¶æ¯å¨å¤çæç«¯æä¸å¯¹ç§°è¡¨ææ¶ï¼å¹¶ä¸å¯è½å¼å¥èº«ä»½å¤±çã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿è´¡ç®å¨äºå¶æåºçæ©æ£æ¨¡åæ¡æ¶ï¼å®è½å¤å¿ å®å°éå¡ä»»ä½ä¸»ä½å¨ä»»ä½ç¹å®é¢é¨è¡¨æä¸çå½¢è±¡ãå·ä½åæ°åæ¬ï¼</p>
<ul>
<li><strong>åºäºIDä¸è´æ§åºç¡æ¨¡åçæå»ºï¼</strong> è¯¥æ¹æ³å»ºç«å¨Arc2Face [33] è¿ä¸IDä¸è´æ§äººè¸åºç¡æ¨¡åä¹ä¸ï¼è¯¥æ¨¡åè½å¤çæå·æé«åº¦èº«ä»½ç¸ä¼¼æ§çå¤æ ·åãé¼ççäººè¸å¾åã</li>
<li><strong>è¡¨æäº¤åæ³¨æåæ¨¡åï¼Expression Cross-Attention Moduleï¼ï¼</strong> å¼å¥äºä¸ä¸ªç»åå¼è®¾è®¡ï¼éè¿FLAME blendshapeåæ°æå¯¼çè¡¨æäº¤åæ³¨æåæ¨¡åï¼å®ç°äºå¯¹è¡¨æçæ¾å¼æ§å¶ãFLAME 3Däººè¸æ¨¡å [21] æä¾çåæ°åè¡¨ç¤ºï¼å®ç°äºå¯¹è¡¨æçè¿ç»­ãé«ç»´æ§å¶ï¼å¹¶å°è¡¨ææ§å¶é®é¢ä»å¾åç©ºé´æ å°å°ä¸ä¸»ä½æ å³ç3Dæ¨¡ååæ°ç©ºé´ã</li>
<li><strong>å¤æ ·åæ°æ®è®­ç»ï¼</strong> æ¨¡åå¨åå«ä¸°å¯è¡¨æååçå¾ååè§é¢æ°æ®æ··åéä¸è¿è¡è®­ç»ï¼ä½¿å¶è½å¤æ³åå°åºæ¬æç»ªä¹å¤çå¾®å¦å¾®è¡¨æåè¡¨æè¿æ¸¡ï¼è¿æ¯ä»¥å¾å·¥ä½æå¿½è§çã</li>
<li><strong>å¯ææåèééå¨ï¼Pluggable Reference Adapterï¼ï¼</strong> å¼å¥äºä¸ä¸ªåèééå¨ï¼éè¿å¨åæè¿ç¨ä¸­ä»åèå¸§è½¬ç§»å¤è§ï¼å®ç°äºå¨çå®å¾åä¸­è¿è¡è¡¨æç¼è¾ï¼åæ¶ä¸æ¹åä¸»ä½çå¤è§æèæ¯ãè¯¥ééå¨éè¿Reference UNetæåç©ºé´å¯¹é½çç¹å¾ï¼å¹¶ä¸ä¸»UNetçèªæ³¨æåå±èåï¼åæ¶ä½¿ç¨LoRAå±è¿è¡å¾®è°ä»¥è§£å³åèè¡¨æä¸ç®æ è¡¨æä¸ä¸è´å¯è½å¸¦æ¥çå²çªã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éåå®æ§è¯ä¼°è¡¨æï¼è¯¥æ¨¡åå¨å®å¶ååèº«ä»½ä¸è´çè¡¨æçææ¹é¢ä¼äºç°ææ¹æ³ã
*   <strong>è¡¨æä¿çåº¦ï¼</strong> å¨è¡¨æä¸è´æ§æ¹é¢æ¾èä¼äºåºäºAUï¼Action Unitsï¼åæ¸²æçæ¿ä»£æ¹æ³ï¼è½å¤åç¡®å°ä¼ éè¡¨æï¼åæ¬æç«¯åä¸å¯¹ç§°è¡¨æã
*   <strong>å¾åè´¨éåèº«ä»½ä¿æï¼</strong> å®ç°äºè¾ä½çFIDåæ°ï¼è¡¨ææ´é«çè§è§è´¨éï¼å¹¶ä¸å§ç»ä¿æä¸è¾å¥ä¸»ä½çé«åº¦èº«ä»½ç¸ä¼¼æ§ã
*   <strong>ç¨æ·ç ç©¶ï¼</strong> ç¨æ·ç ç©¶ç»ææ¾ç¤ºï¼è¯¥æ¹æ³å¨è¡¨æåç¡®æ§æ¹é¢è·å¾äº72%çæç¥¨ï¼è¯æäºå¶å¼ºå¤§çè¡¨æä¿çåº¦ã
*   <strong>åèé©±å¨ç¼è¾ï¼</strong> å¨åèé©±å¨çè¡¨æçæä»»å¡ä¸­ï¼ä¹åå¾äºä¼äºç°ææ¹æ³çè¡¨ç°ï¼è½å¤æ´å¿ å®å°è½¬ç§»è¡¨æï¼åæ¶æ´å¥½å°ä¿çèº«ä»½åè§è§ä¸è´æ§ã</p>
<p>è¿äºç»æçªåºäºå¶ç²¾ç¡®ãåæ°åè¡¨æè¡¨ç¤ºçæææ§ï¼ä»¥åå¨IDä¸è´æ§äººè¸çæèæ¯ä¸å®ç°ç²¾ç»è¡¨ææ§å¶çè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åæ°åè¡¨ç¤ºçè¯­ä¹å¯è§£éæ§ï¼</strong> æéç¨çåæ°åè¡¨ç¤ºï¼FLAME blendshapeï¼ç¼ºä¹è¯­ä¹å¯è§£éæ§ï¼å¯¼è´è¡¨ææä½éå¸¸ä¾èµäºä»åèå¾åä¸­æååæ°ï¼è¿å¯è½éå¶æäºåºç¨ã
*   <strong>å¯¹3Déå»ºæ¹æ³çä¾èµï¼</strong> è¡¨æåç°çåç¡®æ§åºæå°ä¾èµäºç¨äºæåblendshapeåæ°ç3Déå»ºæ¹æ³çè´¨éï¼å°½ç®¡è¯¥æ¹æ³æ¯SOTAï¼ä½ä»å¯è½å­å¨ç¼ºé·å¹¶å¼å¥å¶å°çéè¯¯ã
*   <strong>åèééå¨çä¸è´æ§é®é¢ï¼</strong> å¨æäºæåµä¸ï¼ä½¿ç¨åèééå¨è¿è¡è¡¨æç¼è¾å¯è½ä¸ä¸è´ãå½åèè¡¨æä¸ç®æ è¡¨æå·®å¼è¾å¤§æ¶ï¼æ¨¡åå¯è½è¿åº¦ä¾èµæºå¾åï¼å¯¼è´èæ¯æå§¿æä¸åèå¾åç¥æåå·®ãè½ç¶å¯ä»¥éè¿è°æ´LoRAå±çç¼©æ¾å å­æ¥ç¼è§£ï¼ä½è¿éè¦å¨å§¿æåèæ¯ä¸è´æ§ä¸è¡¨æä¿çåº¦ä¹é´è¿è¡æè¡¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶å±éæ§åè´¡ç®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨æ¹åï¼
*   <strong>æåè¯­ä¹å¯æ§æ§ï¼</strong> æ¢ç´¢å°FLAME blendshapeåæ°ä¸æ´å·è¯­ä¹å¯è§£éæ§çè¡¨ç¤ºï¼å¦èªç¶è¯­è¨æè¿°æé«çº§ææç±»å«ï¼ç¸ç»åï¼ä»¥å®ç°æ´ç´è§çè¡¨ææ§å¶ã
*   <strong>æ¹è¿3Déå»ºçé²æ£æ§ï¼</strong> è¿ä¸æ­¥ç ç©¶åéææ´é²æ£ãæ´ç²¾ç¡®ç3Däººè¸éå»ºæ¹æ³ï¼ä»¥åå°è¡¨ææåä¸­çéè¯¯ï¼ä»èæé«æ´ä½è¡¨æçæçåç¡®æ§ã
*   <strong>å¢å¼ºåèééå¨çä¸è´æ§ï¼</strong> æ¢ç´¢æ´åè¿çæºå¶æ¥è§£å³åèééå¨å¨å¤çå¤æè¡¨æå·®å¼æ¶å¯è½åºç°çâå¤å¶ç²è´´âè¡ä¸ºï¼ä¾å¦éè¿æ´æºè½çç¹å¾èåæèªéåºæéè°æ´ï¼ä»¥å¨ä¿æèæ¯åå§¿æä¸è´æ§çåæ¶ï¼ç¡®ä¿è¡¨æçåç¡®è½¬ç§»ã
*   <strong>ç¤¾ä¼å½±åä¸ä¼¦çèéï¼</strong> è®ºæå¼ºè°äºå¯æ§äººè¸çæææ¯å¯è½è¢«æ»¥ç¨çä¼¦çé®é¢ãæªæ¥çç ç©¶å¯ä»¥ä¸æ³¨äºå¼åæ£æµåæåå®¹çå¯¹ç­ï¼å¹¶ç¡®ä¿ææ¯ç¨äºç§¯æçé¢åï¼å¦å¯è®¿é®æ§ååææäºåè¿°ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression.</li>
<li>Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.04706v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.04706v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-07 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
