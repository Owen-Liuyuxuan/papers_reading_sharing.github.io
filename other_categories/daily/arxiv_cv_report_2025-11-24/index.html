<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-24 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-21/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-25/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-24">Arxiv Computer Vision Papers - 2025-11-24</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#rynnvla-002-a-unified-vision-language-action-and-world-model" class="nav-link">RynnVLA-002: A Unified Vision-Language-Action and World Model</a>
                </li>
                <li class="nav-item">
                    <a href="#native-3d-editing-with-full-attention" class="nav-link">Native 3D Editing with Full Attention</a>
                </li>
                <li class="nav-item">
                    <a href="#halo-high-altitude-language-conditioned-monocular-aerial-exploration-and-navigation" class="nav-link">HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation</a>
                </li>
                <li class="nav-item">
                    <a href="#mdg-masked-denoising-generation-for-multi-agent-behavior-modeling-in-traffic-environments" class="nav-link">MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments</a>
                </li>
                <li class="nav-item">
                    <a href="#video-r4-reinforcing-text-rich-video-reasoning-with-visual-rumination" class="nav-link">Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</a>
                </li>
                <li class="nav-item">
                    <a href="#downscaling-intelligence-exploring-perception-and-reasoning-bottlenecks-in-small-multimodal-models" class="nav-link">Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models</a>
                </li>
                <li class="nav-item">
                    <a href="#radar2shape-3d-shape-reconstruction-from-high-frequency-radar-using-multiresolution-signed-distance-functions" class="nav-link">Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions</a>
                </li>
                <li class="nav-item">
                    <a href="#counterfactual-world-models-via-digital-twin-conditioned-video-diffusion" class="nav-link">Counterfactual World Models via Digital Twin-conditioned Video Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#improving-multimodal-distillation-for-3d-semantic-segmentation-under-domain-shift" class="nav-link">Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift</a>
                </li>
                <li class="nav-item">
                    <a href="#planning-with-sketch-guided-verification-for-physics-aware-video-generation" class="nav-link">Planning with Sketch-Guided Verification for Physics-Aware Video Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-24">Arxiv Computer Vision Papers - 2025-11-24</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年11月21日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年11月21日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>日期：</strong> 2025年11月21日</p>
<p><strong>主要趋势与主题：</strong></p>
<p>本期 Arxiv 论文集呈现出几个显著的趋势：</p>
<ul>
<li><strong>多模态融合的深化：</strong> 视觉、语言和动作的统一模型（如 RynnVLA-002）以及语言引导的导航（HALO）表明，将不同模态的信息有效结合以实现更强大的理解和交互是当前研究的重点。</li>
<li><strong>生成模型在复杂场景中的应用：</strong> 论文涵盖了从多智能体行为建模（MDG）到物理感知视频生成（Planning with Sketch-Guided Verification）的广泛生成任务，显示出生成模型在模拟和创造真实世界场景方面的潜力。</li>
<li><strong>三维理解与重建的进步：</strong> 3D 编辑（Native 3D Editing）、雷达数据到3D形状重建（Radar2Shape）以及多模态蒸馏在3D语义分割中的应用（Improving Multimodal Distillation）都表明了对三维空间更深入理解和建模的需求。</li>
<li><strong>模型效率与鲁棒性：</strong> 对小型多模态模型的研究（Downscaling Intelligence）以及在领域迁移下的鲁棒性提升，反映了在资源受限或复杂环境下的模型优化需求。</li>
<li><strong>世界模型的探索：</strong> 通过数字孪生条件视频扩散（Counterfactual World Models）等方法，研究人员正在探索构建能够理解和预测世界动态的模型。</li>
</ul>
<p><strong>特别值得关注的论文：</strong></p>
<ul>
<li><strong>"RynnVLA-002: A Unified Vision-Language-Action and World Model"</strong>：这篇论文提出的统一模型，整合了视觉、语言和动作，并具备世界模型能力，预示着通用人工智能（AGI）方向的重大进展，能够实现更全面的理解和交互。</li>
<li><strong>"Native 3D Editing with Full Attention"</strong>：在3D编辑领域引入“全注意力”机制，有望显著提升3D内容的编辑精度和灵活性，为虚拟现实、游戏开发等领域带来新的可能性。</li>
<li><strong>"HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation"</strong>：将语言指令与单目航空影像相结合，实现高空探索和导航，是实现自主空中机器人和无人机应用的关键一步。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>统一的跨模态模型：</strong> 将视觉、语言、动作甚至世界模型整合到一个框架下，是实现更智能AI的关键。</li>
<li><strong>物理感知生成：</strong> 在视频生成中考虑物理规律，使得生成的视频更加真实可信。</li>
<li><strong>雷达数据在3D重建中的应用：</strong> 利用高频雷达数据进行高精度3D形状重建，为恶劣天气或低光照条件下的感知提供了新途径。</li>
<li><strong>小型化多模态模型：</strong> 探索在保持性能的同时减小模型规模，以适应边缘计算和移动设备的需求。</li>
<li><strong>反事实推理的世界模型：</strong> 通过数字孪生等技术，构建能够进行反事实推理的世界模型，增强AI的预测和规划能力。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的广泛影响和技术创新性，以下论文值得优先阅读全文：</p>
<ol>
<li><strong>"RynnVLA-002: A Unified Vision-Language-Action and World Model"</strong>：对于理解未来通用AI模型的发展方向至关重要。</li>
<li><strong>"Native 3D Editing with Full Attention"</strong>：在3D内容创作和编辑领域具有直接的应用价值和技术突破。</li>
<li><strong>"HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation"</strong>：对于无人机、机器人导航以及遥感应用的研究者具有重要意义。</li>
<li><strong>"Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models"</strong>：对于关注模型效率和部署的研究者来说，提供了关于如何优化小型多模态模型的宝贵见解。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速了解近期 Arxiv 计算机视觉领域的最新动态，并为您的进一步研究提供方向。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.17502v1">RynnVLA-002: A Unified Vision-Language-Action and World Model</a></li>
<li><a href="#2511.17501v1">Native 3D Editing with Full Attention</a></li>
<li><a href="#2511.17497v1">HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation</a></li>
<li><a href="#2511.17496v1">MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments</a></li>
<li><a href="#2511.17490v1">Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</a></li>
<li><a href="#2511.17487v1">Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models</a></li>
<li><a href="#2511.17484v1">Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions</a></li>
<li><a href="#2511.17481v1">Counterfactual World Models via Digital Twin-conditioned Video Diffusion</a></li>
<li><a href="#2511.17455v1">Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift</a></li>
<li><a href="#2511.17450v1">Planning with Sketch-Guided Verification for Physics-Aware Video Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.17502v1'></a></p>
<h2 id="rynnvla-002-a-unified-vision-language-action-and-world-model"><a href="https://arxiv.org/abs/2511.17502v1">RynnVLA-002: A Unified Vision-Language-Action and World Model</a></h2>
<p><strong>Authors:</strong> Jun Cen, Siteng Huang, Yuqian Yuan, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Kehan Li, Hao Luo, Fan Wang, Xin Li, Deli Zhao, Hao Chen</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于 RynnVLA-002 的全面摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> RynnVLA-002: A Unified Vision-Language-Action and World Model</p>
<p><strong>作者：</strong> Jun Cen, Siteng Huang, Yuqian Yuan, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Kehan Li, Hao Luo, Fan Wang, Xin Li, Deli Zhao, Hao Chen</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文旨在解决当前机器人控制领域中，独立的 Vision-Language-Action (VLA) 模型和世界模型各自存在的局限性。VLA 模型虽然能根据语言指令和视觉信息生成动作，但缺乏对动作动力学的内在理解和对未来状态的预测能力（缺乏“想象力”）。而世界模型能够预测未来图像状态，学习环境物理规律，但无法直接生成动作。因此，研究的核心问题是如何将 VLA 模型和世界模型进行有效融合，实现对环境的深度理解和更鲁棒的动作生成。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>统一的 VLA 和世界模型框架 (RynnVLA-002)：</strong> 论文提出了 RynnVLA-002，一个将 VLA 模型和世界模型统一在一个框架下的模型。该模型通过共享的 LLM 架构，利用三个独立的 Tokenizer（图像、文本、动作）将不同模态的信息编码成统一的 Token 序列，实现了跨模态的理解和生成。</li>
<li><strong>双向增强机制：</strong> RynnVLA-002 的核心在于其双向增强机制。世界模型利用动作和视觉输入预测未来图像状态，学习环境物理规律，从而优化动作生成；反之，VLA 模型从图像观察中生成动作，增强视觉理解，并支持世界模型的图像生成。这种协同作用使得模型更加强大和全面。</li>
<li><strong>离散动作生成中的注意力掩码策略：</strong> 为了解决离散动作生成中因自回归模型导致的误差累积问题，论文提出了一种创新的注意力掩码策略。该策略使得当前动作的生成仅依赖于文本和视觉输入，而忽略先前的动作，从而独立生成每个动作，有效缓解了误差传播。</li>
<li><strong>连续动作生成的 Action Transformer：</strong> 针对离散模型在真实世界机器人任务中泛化能力不足和推理速度慢的问题，论文引入了一个连续动作生成模块——Action Transformer。该模块能够并行处理整个上下文信息，生成平滑的连续动作轨迹，显著提升了泛化能力和推理效率。</li>
<li><strong>混合训练策略：</strong> RynnVLA-002 采用混合 VLA 模型数据和世界模型数据的训练方式，通过一个统一的损失函数（L = Ldis + αLconti）来同时优化动作预测和世界建模能力。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>性能超越：</strong> 实验结果表明，RynnVLA-002 在 LIBERO 模拟基准测试中取得了 97.4% 的成功率，并且在没有预训练的情况下，其性能与经过预训练的强基线模型相当。</li>
<li><strong>真实世界机器人任务的显著提升：</strong> 在真实世界的 LeRobot 实验中，RynnVLA-002 集成的世界模型将整体成功率提升了 50%。在具有挑战性的多目标和带干扰物的环境中，RynnVLA-002 的表现尤为突出，成功率比基线模型高出 10% 到 30%。</li>
<li><strong>双向增强的有效性：</strong> 消融实验证明了世界模型对 VLA 模型和 VLA 模型对世界模型的积极影响。世界模型帮助 VLA 模型更专注于目标物体，而 VLA 模型则增强了世界模型的图像生成能力，使其能更准确地预测不同视角的图像。</li>
<li><strong>意义：</strong> RynnVLA-002 的成功展示了将 VLA 和世界模型统一起来的巨大潜力，为构建更智能、更具泛化能力的机器人系统提供了新的途径。它证明了通过协同学习环境动力学和动作规划，可以实现超越单一模型的能力。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>离散动作生成在真实世界中的挑战：</strong> 论文提到，尽管离散动作生成在模拟环境中表现良好，但在真实世界机器人任务中泛化能力有限，这可能与真实世界动态变量（如光照、物体位置）的复杂性有关。</li>
<li><strong>预训练数据的需求：</strong> 虽然 RynnVLA-002 在没有预训练的情况下表现出色，但论文也探讨了世界模型预训练对 VLA 模型性能的提升作用，暗示了在某些场景下，预训练数据仍然是重要的。</li>
<li><strong>计算效率：</strong> 尽管引入了 Action Transformer 来加速推理，但对于非常长的动作序列，计算效率仍然是一个需要考虑的因素。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的机器人任务和环境：</strong> 将 RynnVLA-002 应用于更复杂、更多样化的机器人任务和真实世界环境，以进一步验证其泛化能力和鲁棒性。</li>
<li><strong>更精细的物理规律学习：</strong> 探索更先进的方法来学习和表示环境的物理动力学，例如更精细的接触力学、摩擦力等，以应对更具挑战性的操作。</li>
<li><strong>多模态融合的进一步探索：</strong> 除了视觉、语言和动作，还可以探索整合触觉、听觉等其他模态信息，构建更全面的机器人感知和决策系统。</li>
<li><strong>模型的可解释性：</strong> 进一步研究模型内部的决策过程，提高模型的可解释性，以便更好地理解其行为并进行调试。</li>
<li><strong>高效的预训练策略：</strong> 研究更有效的世界模型预训练策略，以在有限的真实世界数据下，快速获得高性能的 VLA 模型。</li>
</ul>
<p>总而言之，RynnVLA-002 是一项重要的研究工作，它成功地将 VLA 和世界模型融合，解决了各自的局限性，并在模拟和真实世界机器人任务中取得了显著的成果。该模型为实现更智能、更通用的机器人智能奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17502v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17502v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17501v1'></a></p>
<h2 id="native-3d-editing-with-full-attention"><a href="https://arxiv.org/abs/2511.17501v1">Native 3D Editing with Full Attention</a></h2>
<p><strong>Authors:</strong> Weiwei Cai, Shuangkang Fang, Weicai Ye, Xin Dong, Yunhan Yang, Xuanyang Zhang, Wei Cheng, Yanpei Cao, Gang Yu, Tao Chen</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.CV, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Native 3D Editing with Full Attention”的全面中文摘要，其中包含了您要求的各项内容：</p>
<p><strong>论文题目：</strong> Native 3D Editing with Full Attention</p>
<p><strong>作者：</strong> Weiwei Cai, Shuangkang Fang, Weicai Ye, Xin Dong, Yunhan Yang, Xuanyang Zhang, Wei Cheng, Yanpei Cao, Gang Yu, Tao Chen</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决当前指令引导式三维（3D）编辑领域面临的关键挑战。现有方法主要存在两大局限性：
*   <strong>优化类方法：</strong> 计算成本极高，编辑过程非常缓慢，不适用于实时或交互式应用。
*   <strong>基于2D提升（2D-lifting）的逐帧编辑方法：</strong> 这类方法通常先在2D图像上进行编辑，然后将编辑后的2D图像提升到3D。这种范式容易导致编辑后的3D模型在几何形状和视觉质量上出现不一致，并且难以精确地保留未编辑区域的细节。</p>
<p>因此，研究的核心问题是如何实现一种<strong>原生（native）的3D编辑框架</strong>，能够直接在3D表示上进行高效、精确且高质量的编辑，同时忠实遵循文本指令。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>为了解决上述问题，作者提出了以下关键创新：</p>
<ul>
<li><strong>原生3D编辑框架：</strong> 提出了一种新颖的原生3D编辑框架，能够直接在3D表示上进行操作，避免了2D提升带来的信息损失和不一致性。该框架采用<strong>单次高效的前馈（feed-forward）</strong>方式完成编辑。</li>
<li><strong>大规模、多模态的3D编辑数据集：</strong> 构建了一个<strong>首个大规模、多模态的指令引导式3D编辑数据集</strong>。该数据集涵盖了添加（addition）、删除（deletion）和修改（modification）等多种编辑任务。<ul>
<li><strong>删除任务数据：</strong> 利用Objaverse数据集中的3D资产，通过程序化移除特定部件来生成源-目标对。使用Gemini 2.5多模态大语言模型（MLLM）为每个编辑任务生成精确的文本指令。</li>
<li><strong>添加/修改任务数据：</strong> 利用现有的2D图像编辑数据集，通过Hunyuan3D 2.1等模型将其提升到3D，并经过严格的人工质量筛选，确保3D资产在几何和视觉上的一致性。</li>
</ul>
</li>
<li><strong>高效的3D条件化策略：</strong> 探索了两种模型条件化策略：<ul>
<li><strong>传统交叉注意力（Cross-Attention）：</strong> 将源3D对象和文本指令通过独立的交叉注意力层注入模型。</li>
<li><strong>3D Token Concatenation（3D Token拼接）：</strong> 提出了一种新颖且参数更高效的方法，将源3D对象的潜在表示（tokens）与目标对象的噪声潜在表示进行拼接，形成一个统一的序列，然后通过自注意力机制直接建模源、目标和指令之间的关系。作者证明了这种策略在参数效率和性能上均优于交叉注意力。</li>
</ul>
</li>
<li><strong>端到端前馈模型：</strong> 整个编辑过程是端到端的、前馈式的，无需迭代优化，大大提高了编辑效率。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能优越：</strong> 实验结果表明，作者提出的原生3D编辑框架在<strong>生成质量、3D几何一致性以及指令保真度</strong>方面均显著优于现有的2D提升方法。</li>
<li><strong>效率提升：</strong> 采用3D Token Concatenation策略的模型在参数效率上表现更佳，并且编辑速度快（20秒内完成）。</li>
<li><strong>新基准：</strong> 该工作为指令引导式3D编辑领域设定了一个新的基准，展示了直接在3D表示上进行编辑的巨大潜力。</li>
<li><strong>数据集贡献：</strong> 构建的大规模、高质量数据集为后续3D编辑研究提供了宝贵的资源。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong></p>
<ul>
<li><strong>数据构建的挑战：</strong> 尽管作者进行了细致的数据构建和人工筛选，但生成高质量、高一致性的3D数据（尤其是2D到3D提升部分）仍然存在挑战，现有图像到3D模型可能引入伪影或意外的修改。</li>
<li><strong>模型训练的依赖：</strong> 模型依赖于预训练的3D扩散模型（如TRELLIS架构），其性能受限于预训练模型的质量。</li>
<li><strong>部分方法仍需改进：</strong> 在消融实验中，虽然作者的方法表现优异，但交叉注意力策略在某些情况下仍可能导致几何失真和颜色不一致。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的编辑控制：</strong> 进一步探索更精细的编辑控制，例如局部区域的精确修改，或者更复杂的编辑操作。</li>
<li><strong>交互式编辑：</strong> 将该框架扩展到更具交互性的编辑场景，例如实时反馈和用户反馈的集成。</li>
<li><strong>多样化的3D表示：</strong> 探索该方法在其他3D表示（如NeRF、3D高斯溅射等）上的应用。</li>
<li><strong>更广泛的编辑任务：</strong> 扩展数据集和模型以支持更广泛、更复杂的3D编辑任务，例如场景编辑、材质编辑等。</li>
<li><strong>模型鲁棒性：</strong> 进一步提升模型在处理低质量输入、复杂场景以及模糊指令时的鲁棒性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文提出了一种创新的“原生3D编辑”框架，通过直接在3D表示上进行操作，克服了现有方法在速度、几何一致性和视觉质量上的瓶颈。其核心贡献在于构建了一个大规模、高质量的多模态3D编辑数据集，以及提出了一种高效的3D Token Concatenation条件化策略。实验结果表明，该方法在指令保真度、编辑质量和效率上均达到了新的水平，为未来的3D内容创作和编辑研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass.</li>
<li>Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach.</li>
<li>Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17501v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17501v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17497v1'></a></p>
<h2 id="halo-high-altitude-language-conditioned-monocular-aerial-exploration-and-navigation"><a href="https://arxiv.org/abs/2511.17497v1">HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation</a></h2>
<p><strong>Authors:</strong> Yuezhan Tao, Dexter Ong, Fernando Cladera, Jason Hughes, Camillo J. Taylor, Pratik Chaudhari, Vijay Kumar</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本研究提出了一个名为HALO的系统，实现了高空单目视觉下的实时度量-语义地图构建、探索与导航。HALO解决了远距离密集3D重建和大规模户外环境的精确几何与语义建图探索两大关键挑战，并能根据自然语言指令规划信息丰富的探索路径，高效完成多任务。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>HALO系统的核心创新在于其能够<strong>在远距离单目视觉条件下实现实时密集3D重建，并将其与大规模户外环境的精确几何和语义信息相结合，用于自主探索和导航。</strong> 具体来说，其方法论可能包含以下几个关键点（虽然摘要未详述，但可以推断）：</p>
<ul>
<li><strong>远距离密集3D重建：</strong> 这是该研究最突出的技术挑战。在远距离下，单目视觉的尺度模糊问题会加剧，并且由于像素分辨率的限制，获取密集3D信息更加困难。HALO可能采用了先进的视觉里程计（Visual Odometry）和场景流（Scene Flow）技术，结合了多帧信息，甚至可能利用了IMU数据来辅助尺度估计和运动补偿，以克服远距离带来的挑战。</li>
<li><strong>度量-语义地图构建：</strong> 系统不仅构建了环境的几何结构（度量地图），还赋予了地图语义信息（例如，识别出建筑、道路、植被等）。这通常需要结合语义分割模型（如DeepLab, Mask R-CNN等）与3D重建技术。</li>
<li><strong>语言条件下的探索与导航：</strong> HALO能够理解自然语言指令，并将其转化为具体的探索目标和路径规划。这表明系统集成了自然语言处理（NLP）模块，能够将文本指令映射到视觉和导航任务中。</li>
<li><strong>信息性路径规划：</strong> 为了高效完成任务，HALO能够规划“信息性”路径，这意味着它会优先探索那些能够提供最多新信息（几何或语义）的区域，从而加速地图构建和任务完成。这可能涉及到信息论或主动学习的原理。</li>
<li><strong>集成GPS和IMU：</strong> GPS提供了全局定位信息，而IMU提供了姿态和加速度信息。这些传感器数据的融合对于实现高精度的全局定位和运动估计至关重要，尤其是在大规模户外环境中。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<p>HALO的研究对计算机视觉和机器人领域具有重要意义，其潜在影响包括：</p>
<ul>
<li><strong>提升无人机自主能力：</strong> 显著提升无人机在高空进行自主探索、测绘和任务执行的能力，尤其是在GPS信号可能受限或需要高精度地图的场景。</li>
<li><strong>推动大规模3D重建技术：</strong> 挑战并可能突破现有大规模3D重建技术的局限性，尤其是在单目视觉和远距离条件下的性能。</li>
<li><strong>促进人机交互在机器人领域的应用：</strong> 为自然语言指令驱动的机器人探索和任务执行提供了新的范例，使得非专业用户也能更直观地控制机器人。</li>
<li><strong>加速地理空间信息获取：</strong> 为快速、高效地获取大规模区域的详细几何和语义信息提供了新的解决方案，可应用于城市规划、灾害评估、环境监测等领域。</li>
<li><strong>推动端到端机器人系统发展：</strong> 展示了一个高度集成的系统，将感知、理解、规划和执行紧密结合，为开发更复杂的自主机器人系统提供了参考。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>航空测绘与地理信息系统（GIS）：</strong> 快速生成高精度、高分辨率的数字高程模型（DEM）、数字表面模型（DSM）以及语义地图，用于城市规划、土地利用分析、基础设施监测等。</li>
<li><strong>灾害响应与应急管理：</strong> 在灾难发生后，无人机可以快速对受灾区域进行测绘和评估，识别危险区域和受损情况，并根据指令进行特定区域的详细勘察。</li>
<li><strong>农业监测与精准农业：</strong> 对大片农田进行高空测绘，识别作物生长状况、病虫害区域，并根据指令进行特定区域的重点监测。</li>
<li><strong>环境监测与保护：</strong> 监测森林覆盖率、水体变化、野生动物栖息地等，并能根据指令对特定区域进行详细调查。</li>
<li><strong>自动驾驶与机器人导航：</strong> 虽然论文聚焦于高空，但其在远距离3D重建和语义理解方面的技术可以迁移到地面机器人和自动驾驶车辆，用于构建更精细的环境模型和进行更鲁棒的导航。</li>
<li><strong>虚拟现实（VR）/增强现实（AR）内容生成：</strong> 为创建逼真的虚拟环境或叠加现实信息提供高质量的3D模型和语义数据。</li>
</ul>
<p><strong>5. 可推断的局限性：</strong></p>
<p>尽管摘要展示了令人印象深刻的成果，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>单目视觉的固有挑战：</strong> 尽管系统可能采取了多种策略，但单目视觉在尺度估计、深度不确定性以及在纹理稀疏或重复区域的鲁棒性方面仍然存在固有的挑战。在某些极端情况下，精度可能会受到影响。</li>
<li><strong>计算资源需求：</strong> 实时密集3D重建和语义理解通常需要大量的计算资源。虽然摘要提到“所有模块可以运行 onboard the robot”，但实际的计算负荷和对机器人硬件的要求可能很高，尤其是在资源受限的平台上。</li>
<li><strong>对环境的依赖性：</strong> 系统的性能可能在很大程度上依赖于环境的特征。例如，在光照条件剧烈变化、动态物体过多或缺乏明显纹理的区域，性能可能会下降。</li>
<li><strong>自然语言理解的复杂性：</strong> 尽管能够处理自然语言指令，但对于高度复杂、模糊或需要深层推理的指令，系统的理解和执行能力可能仍然有限。</li>
<li><strong>大规模环境下的累积误差：</strong> 在大规模环境中，即使是微小的里程计误差也可能随着时间的推移而累积，影响全局地图的精度。GPS和IMU的融合有助于缓解，但并非完全消除。</li>
<li><strong>“密集”3D重建的定义：</strong> 摘要中提到“密集3D重建”，但“密集”的程度和质量在不同场景下可能有所不同。在非常远的距离，重建的细节可能仍然有限。</li>
<li><strong>仿真与真实世界的差距：</strong> 尽管进行了真实世界实验，但仿真环境与真实世界环境之间总会存在差异，例如传感器噪声模型、环境动态性等，这些差异可能影响系统在真实世界中的泛化能力。</li>
</ul>
<p>总而言之，HALO系统在解决高空单目视觉下的3D重建和语义探索方面取得了显著进展，为无人机自主能力和人机交互在机器人领域的应用开辟了新的可能性。其技术创新点在于克服了远距离单目视觉的挑战，并将其与语言驱动的任务规划相结合。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU).</li>
<li>We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language.</li>
<li>m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17497v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17497v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17496v1'></a></p>
<h2 id="mdg-masked-denoising-generation-for-multi-agent-behavior-modeling-in-traffic-environments"><a href="https://arxiv.org/abs/2511.17496v1">MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments</a></h2>
<p><strong>Authors:</strong> Zhiyu Huang, Zewei Zhou, Tianhui Cai, Yun Zhang, Jiaqi Ma</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.RO, cs.MA</p>
<p><strong>Abstract:</strong></p>
<p>Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments”的全面中文摘要：</p>
<p><strong>论文题目：</strong> MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments (MDG：用于交通环境中多智能体行为建模的掩码去噪生成)</p>
<p><strong>作者：</strong> Zhiyu Huang, Zewei Zhou, Tianhui Cai, Yun Zhang, Jiaqi Ma</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
在自动驾驶和交通仿真领域，对真实且交互式的多智能体行为进行建模至关重要。然而，现有的扩散模型和自回归模型在效率、可复用性以及任务通用性方面存在局限性。具体来说，扩散模型常受限于迭代采样和计算成本，而自回归模型则依赖于离散的序列解码，难以捕捉连续动态并实现精细控制。这种模型在不同任务（如开环预测、闭环仿真、运动规划和条件生成）之间的分离，阻碍了通用性和可扩展性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了一种名为<strong>掩码去噪生成 (MDG)</strong> 的统一生成框架，将多智能体行为建模重新定义为对独立噪声化的时空张量进行重构。MDG的核心创新在于：</p>
<ul>
<li><strong>时空张量重构：</strong> 将多智能体轨迹建模为时空张量，并在此基础上进行噪声化和去噪。</li>
<li><strong>连续、按代理、按时间步的噪声掩码：</strong> MDG不依赖于扩散时间步或离散的token化，而是引入了连续的、针对每个代理和每个时间步的噪声掩码。这使得模型能够实现局部去噪和可控的轨迹生成，并且可以在单次或少数几次前向传播中完成。</li>
<li><strong>掩码驱动的通用框架：</strong> 这种掩码驱动的范式能够统一处理开环预测、闭环仿真、运动规划和条件生成等多种任务，无需针对不同任务设计特定的模型。</li>
<li><strong>灵活的推理模式：</strong> MDG支持多种推理模式，包括单步去噪、沿时间轴去噪、沿代理轴去噪，以及引导式生成和闭环复用。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能优越：</strong> 在Waymo Sim Agents和nuPlan Planning等大规模真实世界驾驶数据集上进行训练后，MDG在闭环仿真和规划任务上取得了具有竞争力的性能。
*   <strong>效率与可控性：</strong> MDG在提供高效、一致且可控的多智能体轨迹生成的同时，显著降低了计算开销，尤其是在引导式生成方面，相比于现有方法有显著提升。
*   <strong>通用性验证：</strong> 实验结果表明，MDG是一个简单而通用的多智能体行为建模范式，能够有效地处理多种下游任务。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>模型规模与数据限制：</strong> 论文提到，在增加模型规模时，性能提升会遇到瓶颈，这可能与训练数据的限制有关。
*   <strong>反应式代理的性能：</strong> 在nuPlan基准测试中，对于反应式代理的设置，MDG的性能略有下降，这归因于所使用的简单IDM模型产生的非真实行为和人工间隙。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>提高运行时效率：</strong> 论文指出未来的工作将专注于进一步提高MDG的运行时效率。
*   <strong>集成文本或其他条件输入：</strong> 探索将用户提供的文本或其他类型的条件输入集成到MDG框架中，以实现更丰富的场景控制和生成。</p>
<p><strong>总结：</strong>
MDG通过引入一种新颖的掩码去噪生成范式，成功地解决了多智能体行为建模中的效率、通用性和可控性挑战。它将多智能体轨迹建模为时空张量的重构问题，并利用连续的、按代理和按时间步的噪声掩码，实现了在单次或少数几次前向传播中完成多种下游任务。MDG在关键基准测试中展现出的优异性能和灵活性，使其成为多智能体行为建模领域一个有前景且通用的新范式。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17496v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17496v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17490v1'></a></p>
<h2 id="video-r4-reinforcing-text-rich-video-reasoning-with-visual-rumination"><a href="https://arxiv.org/abs/2511.17490v1">Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</a></h2>
<p><strong>Authors:</strong> Yolo Yunlong Tang, Daiki Shimada, Hang Hua, Chao Huang, Jing Bi, Rogerio Feris, Chenliang Xu</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>这篇论文提出了 Video-R4，一个创新的视频推理大型多模态模型（LMM），它通过“视觉反刍”（visual rumination）机制来解决文本丰富视频中细粒度证据识别的挑战。该模型能够迭代地选择帧、放大关键区域、重新编码像素并更新推理状态，从而显著提升了对包含大量文本信息的视频的理解能力。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>视觉反刍 (Visual Rumination)：</strong> 这是本文最核心的创新。它模仿人类在理解复杂信息时会反复审视、放大和重新阅读关键部分的行为。具体来说，Video-R4 能够：<ul>
<li><strong>迭代式帧选择 (Iterative Frame Selection)：</strong> 模型不是一次性处理所有帧，而是根据当前推理状态选择最相关的帧进行进一步分析。</li>
<li><strong>区域放大与重编码 (Region Zooming and Re-encoding)：</strong> 对于选定的帧，模型能够识别并放大包含关键文本信息的区域，然后对这些局部像素进行更精细的编码，以捕捉微小但重要的细节。</li>
<li><strong>推理状态更新 (Reasoning State Update)：</strong> 通过反刍过程获取的新信息会用来更新模型的内部推理状态，驱动下一轮的反刍决策。</li>
</ul>
</li>
<li><strong>多阶段反刍学习框架 (Multi-stage Rumination Learning Framework)：</strong><ul>
<li><strong>监督微调 (SFT)：</strong> 用于学习基础的原子视觉操作（如选择特定区域）和混合操作（如结合不同区域的信息）。</li>
<li><strong>基于 GRPO 的强化学习 (GRPO-based RL)：</strong> 用于训练模型学习更复杂的、序列化的反刍策略，以最大化推理的准确性。</li>
</ul>
</li>
<li><strong>新数据集的构建：</strong><ul>
<li><strong>Video-R4-CoT-17k：</strong> 包含可执行的反刍轨迹，用于监督学习。</li>
<li><strong>Video-R4-RL-30k：</strong> 用于强化学习，训练模型自主学习反刍策略。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升视频理解的精细度：</strong> Video-R4 的方法有望显著提高模型在处理包含大量文本信息的视频时的准确性，尤其是在需要精确识别和理解屏幕上文字的场景。</li>
<li><strong>解决“幻觉”问题：</strong> 通过迭代式地聚焦于证据，模型可以减少对模糊或不完整信息的猜测，从而降低产生“幻觉”的概率。</li>
<li><strong>推动更具交互性和动态性的多模态模型：</strong> 视觉反刍的范式为构建更智能、更具适应性的多模态模型提供了新的思路，模型不再是被动地接收信息，而是主动地探索和提炼信息。</li>
<li><strong>为细粒度视觉推理设定新标准：</strong> 该研究可能为视频中的细粒度视觉推理任务设定新的基准，并激发更多关于如何让模型进行更深入、更具策略性的视觉信息处理的研究。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>视频问答 (Video QA)：</strong> 特别是那些需要从视频中提取和理解屏幕文字的 VQA 任务。</li>
<li><strong>文档理解 (Document Understanding)：</strong> 如多页文档问答、幻灯片问答等，这些场景与文本丰富视频有相似之处，都需要精细的文本识别和推理。</li>
<li><strong>屏幕内容分析 (Screen Content Analysis)：</strong> 识别和理解用户界面、软件操作演示、教学视频中的文字信息。</li>
<li><strong>自动驾驶 (Autonomous Driving)：</strong> 理解路牌、交通标志等动态文本信息。</li>
<li><strong>视频内容审核与分析 (Video Content Moderation and Analysis)：</strong> 识别视频中的不当文字内容或提取关键信息。</li>
<li><strong>辅助技术 (Assistive Technologies)：</strong> 为视力障碍者提供更准确的视频内容描述。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>计算成本：</strong> 迭代式的帧选择、区域放大和重编码过程很可能带来显著的计算开销，尤其是在处理长视频或需要多次反刍的情况下。</li>
<li><strong>模型复杂度：</strong> 引入反刍机制和强化学习训练会增加模型的整体复杂性，可能需要更长的训练时间和更多的计算资源。</li>
<li><strong>对“反刍轨迹”的依赖：</strong> 虽然论文构建了数据集，但监督学习阶段对“可执行的反刍轨迹”的依赖，可能意味着模型在学习过程中需要明确的指导，其自主探索能力可能在初期受到限制。</li>
<li><strong>泛化性挑战：</strong> 尽管摘要提到模型在多个任务上表现良好，但其在处理“非文本丰富”视频或需要完全不同推理模式的视频上的泛化能力仍需进一步验证。</li>
<li><strong>“反刍”策略的优化：</strong> 如何最优地决定何时反刍、反刍哪些区域、反刍多少次，仍然是一个需要持续研究和优化的复杂问题。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Video-R4 的核心亮点在于其“视觉反刍”机制，这是一种模仿人类认知过程的创新方法，能够让 LMM 在处理文本丰富的视频时，像人一样“停下来、放大、再看一遍”。这种迭代式、聚焦式的证据提取方式，对于解决当前视频理解模型在细粒度文本信息识别上的痛点具有重要意义。论文通过构建新的数据集和提出多阶段学习框架，为实现这一目标提供了坚实的基础。该研究有望在多个领域产生深远影响，但其计算效率和策略优化仍是未来研究的关注点。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state.</li>
<li>We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL.</li>
<li>Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17490v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17490v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17487v1'></a></p>
<h2 id="downscaling-intelligence-exploring-perception-and-reasoning-bottlenecks-in-small-multimodal-models"><a href="https://arxiv.org/abs/2511.17487v1">Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models</a></h2>
<p><strong>Authors:</strong> Mark Endo, Serena Yeung-Levy</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models”的中文摘要，重点关注其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文摘要：</strong></p>
<p><strong>1. 研究问题与核心挑战：</strong></p>
<p>本文旨在解决一个关键问题：当大型多模态模型（MLLMs）的语言模型（LLM）部分被缩小（downscaling）以创建更小、更高效的模型时，哪些多模态能力会受到最严重的影响，以及为什么？研究人员发现，LLM的缩小不成比例地损害了模型的视觉能力，而非直接继承自LLM的能力。进一步的分析揭示，这种视觉能力的下降不仅是由于视觉推理能力的减弱，更根本的原因在于模型提取和理解视觉信息（感知能力）的根本性损失。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li><strong>系统性分析LLM缩小对多模态能力的影响：</strong> 作者通过解耦（decoupled）的框架，首次系统地分离了感知（perception）和推理（reasoning）在LLM缩小过程中的影响，揭示了感知能力是小型多模态模型的一个关键瓶颈。</li>
<li><strong>提出“视觉提取微调”（Visual Extraction Tuning）：</strong> 针对感知瓶颈，论文引入了一种新的训练范式。该范式显式地训练模型，使其能够跨不同任务一致地提取指令相关的视觉细节。这通过将视觉指令微调任务转化为视觉提取任务来实现，模型被引导去描述与指令相关的精细视觉信息。</li>
<li><strong>提出“提取+思考”（EXTRACT+THINK）框架：</strong> 结合了上述的视觉提取微调和逐步推理（step-by-step reasoning），形成了一个两阶段的框架。第一阶段（感知）通过视觉提取微调增强模型提取视觉信息的能力，第二阶段（推理）利用提取的视觉细节进行逐步思考以生成答案。</li>
</ul>
<p><strong>3. 主要结果与重要性：</strong></p>
<ul>
<li><strong>感知能力是关键瓶颈：</strong> 研究发现，LLM的缩小对视觉密集型任务的影响尤为显著，并且这种影响在很大程度上源于感知能力的退化，其严重程度甚至可以与推理能力的下降相媲美，甚至超过。这表明，即使在训练数据充足的情况下，小型模型在理解和提取视觉信息方面也存在根本性困难。</li>
<li><strong>EXTRACT+THINK的有效性：</strong> 提出的EXTRACT+THINK方法在参数和数据效率方面取得了显著的进步。即使是较小的模型变体，也能在各种任务上超越现有的大型模型（如PrismCaptioner），并且使用的视觉训练数据量大大减少。这为构建高效、高性能的小型多模态模型提供了新的途径。</li>
<li><strong>对现有假设的挑战：</strong> 研究结果挑战了以往认为感知能力对LLM规模不敏感的假设，强调了在小型多模态模型中，感知能力与推理能力同等重要，甚至可能是更基础的瓶颈。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模型规模范围：</strong> 分析主要集中在特定范围内的LLM模型尺寸缩小，未来可以探索更广泛的模型规模。</li>
<li><strong>视觉表示的缩小：</strong> 论文主要关注LLM的缩小，但视觉表示（vision encoder）的缩小也可能影响性能，这部分未深入探讨。</li>
<li><strong>数据规模的影响：</strong> 虽然EXTRACT+THINK在数据效率上表现出色，但数据规模对缩小行为的影响仍需进一步研究。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的模型规模分析：</strong> 探索不同模型规模下缩小行为的差异。</li>
<li><strong>视觉表示与语言模型缩小的对比：</strong> 比较视觉表示缩小和语言模型缩小对多模态能力的影响。</li>
<li><strong>数据规模与缩小行为的关系：</strong> 深入研究数据规模如何影响模型在缩小过程中的表现。</li>
<li><strong>视觉提取微调的进一步探索：</strong> 与传统的视觉指令微调进行更深入的比较，并将其应用于更大的语言模型。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文通过对小型多模态模型中LLM缩小的深入分析，揭示了感知能力作为关键瓶颈的重要性，并提出了创新的EXTRACT+THINK框架来解决这一问题。该框架通过视觉提取微调和逐步推理，显著提升了小型多模态模型的效率和性能，为在资源受限环境下部署强大的多模态AI提供了重要的理论和实践指导，对计算机视觉和多模态学习领域具有重要的理论和应用价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks.</li>
<li>Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17487v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17487v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17484v1'></a></p>
<h2 id="radar2shape-3d-shape-reconstruction-from-high-frequency-radar-using-multiresolution-signed-distance-functions"><a href="https://arxiv.org/abs/2511.17484v1">Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions</a></h2>
<p><strong>Authors:</strong> Neel Sortur, Justin Goodwin, Purvik Patel, Luis Enrique Martinez, Tzofi Klinghoffer, Rajmonda S. Caceres, Robin Walters</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Radar2Shape: 基于多分辨率符号距离函数的高频雷达三维形状重建</p>
<p><strong>作者：</strong> Neel Sortur, Justin Goodwin, Purvik Patel, Luis Enrique Martinez Jr, Tzofi Klinghoffer, Rajmonda S. Caceres, Robin Walters</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
该论文旨在解决一个核心挑战：如何从高频雷达信号中准确地重建三维（3D）物体的形状。传统方法在处理雷达信号时面临诸多困难，包括：
*   <strong>部分可观测性：</strong> 雷达传感器通常只能从有限的视角观测物体，导致测量中存在大量缺失信息（自遮挡）。
*   <strong>信号特性：</strong> 雷达信号的解析复杂，且与光学图像不同，雷达信号的几何投影关系不直接，使得现有光学3D重建方法难以直接应用。
*   <strong>任意形状表示：</strong> 现有方法难以表示任意复杂形状，并且在处理真实世界中收集到的、具有部分遮挡的雷达信号时表现不佳。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
作者提出了 <strong>Radar2Shape</strong>，一个新颖的<strong>去噪扩散模型</strong>，专门用于解决部分可观测的高频雷达信号3D重建问题。其核心创新在于：
*   <strong>多分辨率符号距离函数（SDF）表示：</strong> 论文引入了一种学习<strong>多分辨率、分层式潜在空间</strong>的方法，将3D形状分解为不同尺度的特征（例如，整体结构与精细结构如手臂和腿）。
*   <strong>粗到精的扩散过程：</strong> 模型采用两阶段方法：
    *   <strong>第一阶段（预训练）：</strong> 学习一个分层式的3D形状潜在空间，将点云映射到多分辨率的SDF表示。这通过一个<strong>多分辨率哈希编码器</strong>和一个<strong>分层式变分自编码器（VAE）</strong>实现，以解耦不同分辨率的形状特征。
    *   <strong>第二阶段（雷达条件生成）：</strong> 训练一个<strong>去噪扩散模型</strong>，该模型将雷达信号的频率信息与形状的潜在空间特征进行关联，并以<strong>粗到精</strong>的方式进行扩散（去噪），从而在潜在空间中生成形状。该阶段利用<strong>Transformer</strong>来处理雷达编码和形状编码的交织序列。
*   <strong>利用雷达频率与形状分辨率的对应关系：</strong> 模型的核心思想是将雷达信号的不同频率与其对应物体几何形状的不同分辨率（粗糙度）联系起来，从而利用雷达信号的频率信息来指导形状的重建。
*   <strong>引入领域特定先验：</strong> 论文还考虑了<strong>滚转对称形状先验</strong>，并提出了一种更高效的U-Net变体来处理低维度的形状空间。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能优越：</strong> Radar2Shape 在部分可观测和噪声较大的雷达信号条件下，能够成功重建任意3D形状，显著优于现有的3D重建方法（如TMNet、LIST、Diffusion-SDF）以及专门为雷达领域设计的基线方法（如InvRT）。
*   <strong>鲁棒性与泛化性：</strong> 模型在两种不同的模拟方法和真实世界数据上都表现出强大的泛化能力。
*   <strong>可解释性：</strong> 通过消融实验（图3），论文展示了模型学习到的分层式潜在空间能够几何地捕捉到形状的粗糙和精细特征，提高了模型的可解释性。
*   <strong>数据集贡献：</strong> 论文发布了两个新的合成基准数据集（Manifold40-PO 和 Manifold40-PO-SBR），包含多样化的网格模型和模拟的高频雷达响应，为该领域的研究提供了宝贵资源。
*   <strong>推动领域发展：</strong> 该工作为高频雷达3D形状重建提供了一个强大的新框架，并有望促进相关模型在实际雷达系统中的应用。</p>
<p><strong>4. 局限性：</strong>
*   <strong>尺度不变性：</strong> Radar2Shape 目前不直接学习重建物体的绝对尺度，因为其训练数据（ModelNet40）本身可能不包含准确的相对尺度信息。
*   <strong>姿态依赖性：</strong> 分层式特征在某些情况下可能与物体的姿态（pose）绑定，导致在姿态变化时表示的改变。
*   <strong>真实世界数据限制：</strong> 尽管论文使用了真实雷达数据，但获取高质量、多样化的真实高频雷达数据仍然具有挑战性，且受限于成本、安全和知识产权。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>姿态不变性：</strong> 探索如何通过引入旋转不变性来解决分层特征的姿态依赖性问题。
*   <strong>尺度学习：</strong> 改进模型以学习和预测重建物体的尺度。
*   <strong>更多样化的真实世界数据：</strong> 收集和利用更多样化的真实世界雷达数据，并进行模型微调，以进一步提升在实际应用中的性能。
*   <strong>多目标场景：</strong> 将模型扩展到处理包含多个物体的复杂场景。</p>
<p>总而言之，Radar2Shape 是一项重要的研究成果，它通过创新的多分辨率SDF表示和基于频率的粗到精扩散机制，显著提升了从部分可观测的高频雷达信号中进行3D形状重建的能力，并为该领域的研究和应用奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features.</li>
<li>Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner.</li>
<li>We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17484v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17484v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17481v1'></a></p>
<h2 id="counterfactual-world-models-via-digital-twin-conditioned-video-diffusion"><a href="https://arxiv.org/abs/2511.17481v1">Counterfactual World Models via Digital Twin-conditioned Video Diffusion</a></h2>
<p><strong>Authors:</strong> Yiqing Shen, Aiza Maksutova, Chenjia Li, Mathias Unberath</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as "what would happen if this object was removed?", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Counterfactual World Models via Digital Twin-conditioned Video Diffusion”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Counterfactual World Models via Digital Twin-conditioned Video Diffusion (通过数字孪生条件化的视频扩散模型实现反事实世界模型)</p>
<p><strong>作者：</strong> Yiqing Shen, Aiza Maksutova, Chenjia Li, Mathias Unberath</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>当前的世界模型主要关注基于事实的预测，即根据观察到的场景和动作来模拟未来的视觉演变。然而，在许多实际应用中，例如物理AI行为的全面评估，需要模型能够回答“反事实查询”，即“如果某个物体被移除会怎样？”。现有的世界模型（包括视频扩散模型）通常将场景中的物体属性和关系编码在纠缠的像素空间表示中，这使得对特定物体或关系的精确干预变得困难，并且缺乏对干预效果如何随时间传播的推理能力。因此，论文旨在解决如何使世界模型能够理解和生成在给定干预下的反事实场景。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>论文提出了 <strong>CWMDT (Counterfactual World Model with Digital Twin Representation Conditioned Diffusion Model)</strong> 框架，旨在将标准的视频扩散模型转化为有效的反事实世界模型。其核心创新在于引入了 <strong>数字孪生表示 (Digital Twin Representation)</strong> 作为中间层，实现了推理与合成的分离。具体方法包括三个阶段：</p>
<ul>
<li><strong>数字孪生表示构建 (Digital Twin Representation Construction)：</strong> 利用视觉基础模型（如SAM-2、DepthAnything、OWLv2）从视频帧中提取结构化的场景表示，这些表示以文本形式（JSON）编码了物体实例的标识符、类别、属性、空间位置和分割掩码。这使得场景因素变得显式且可分离。</li>
<li><strong>数字孪生表示上的反事实推理 (Counterfactual Reasoning over Digital Twin Representation)：</strong> 利用大型语言模型（LLM）来处理数字孪生表示和干预查询。LLM能够理解干预对物体和关系的影响，并预测这些变化在后续时间步中的传播，生成修改后的数字孪生表示序列。这实现了对干预效果的逻辑推理。</li>
<li><strong>数字孪生表示条件化的视频合成 (Video Synthesis Conditioned on Digital Twin Representation)：</strong> 将经过修改的数字孪生表示序列作为条件，输入到经过微调的视频扩散模型中，生成反事实的视频序列。通过编辑原始视频的第一帧以匹配修改后的数字孪生表示，确保了视觉与文本描述的一致性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>CWMDT在两个基准测试（RVEBench和FiVE）上进行了评估，结果显示：</p>
<ul>
<li><strong>性能优越：</strong> CWMDT在所有评估指标上均超越了现有的视频生成和编辑方法，尤其在需要复杂推理的反事实场景下表现突出。例如，在RVEBench的GroundingDINO和LLM-as-a-Judge指标上，CWMDT的得分显著高于基线方法。</li>
<li><strong>精确的干预执行：</strong> CWMDT能够准确地定位和修改干预目标，并生成与干预描述语义一致的视频。</li>
<li><strong>多样的反事实轨迹生成：</strong> CWMDT能够从单一干预生成多个合理且多样化的反事实视频轨迹，展示了其对不确定性传播的理解能力。</li>
<li><strong>数字孪生表示的重要性：</strong> 消融实验表明，数字孪生表示和LLM的推理能力对于实现准确的反事实世界模型至关重要。数字孪生表示使得场景因素可分离，LLM则提供了强大的推理能力。</li>
<li><strong>视觉-文本一致性：</strong> 使用编辑过的初始帧来匹配数字孪生表示，确保了生成视频的视觉一致性，从而提高了模型性能。</li>
</ul>
<p>这些结果表明，通过将视频表示分解为结构化的数字孪生，并结合LLM的推理能力，可以有效地增强视频扩散模型进行反事实推理的能力，为物理AI评估和决策提供更强大的工具。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>LLM规模的影响：</strong> 消融实验表明，LLM的规模对性能有显著影响，较小的LLM模型会导致性能下降。</li>
<li><strong>计算成本：</strong> 虽然论文没有明确指出，但使用LLM进行推理和视频扩散模型进行合成通常需要较高的计算资源。</li>
<li><strong>对“简单”场景的局限性：</strong> 在CausalVQA的“描述性”或“目标导向”推理问题上，CWMDT的提升幅度相对较小，表明在纯粹的事实性理解任务上，其优势不如反事实推理任务明显。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的物理属性捕捉：</strong> 未来工作可以探索将数字孪生表示扩展到捕捉更细粒度的物理属性，以实现更逼真的反事实模拟。</li>
<li><strong>自主系统中的决策引导：</strong> 将反事实世界模型应用于自主系统中，以指导决策制定，特别是在需要评估假设场景以确保安全操作的情况下。</li>
<li><strong>更高效的LLM集成：</strong> 研究如何更高效地集成LLM，以降低计算成本并可能进一步提升性能。</li>
<li><strong>更广泛的干预类型：</strong> 探索更广泛和复杂的干预类型，以及模型如何处理这些干预。</li>
</ul>
<p>总而言之，这篇论文提出了一种新颖的框架 CWMDT，通过引入数字孪生表示和LLM推理，成功地将视频扩散模型转化为能够理解和生成反事实场景的强大工具，为物理AI的评估和发展开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models.</li>
<li>Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17481v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17481v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17455v1'></a></p>
<h2 id="improving-multimodal-distillation-for-3d-semantic-segmentation-under-domain-shift"><a href="https://arxiv.org/abs/2511.17455v1">Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift</a></h2>
<p><strong>Authors:</strong> Björn Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet, Nicolas Courty</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift (改进多模态蒸馏以应对 3D 语义分割中的域偏移)</p>
<p><strong>作者：</strong> Björn Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet, Nicolas Courty</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决的是在自动驾驶和机器人领域中，3D 激光雷达（LiDAR）点云的语义分割模型在面对不同传感器（域偏移）时泛化能力不足的问题。在一种传感器上训练的完全监督模型，在部署到另一种传感器时性能会急剧下降。为了克服这一挑战，研究人员探索了利用视觉基础模型（VFMs）来增强模型在不同域之间的鲁棒性，并特别关注无监督域适应（UDA）技术，即在目标域没有标签的情况下进行模型适应。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>该研究的核心贡献在于对利用视觉基础模型（VFMs）进行 3D 激光雷达点云无监督域适应（UDA）的“配方”进行了详尽的探索和优化。其主要创新点包括：</p>
<ul>
<li><strong>系统性地研究了关键组件的影响：</strong> 作者深入分析了影响泛化性能的几个关键因素，包括：<ul>
<li><strong>激光雷达骨干网络架构的选择：</strong> 发现传统的 MinkowskiUNet (MUNet) 并非最佳选择，更现代的网络（如 WaffleIron - WI）能提供更好的泛化能力，并且增加骨干网络的容量（如从 WI-256 到 WI-768）能进一步提升性能。</li>
<li><strong>归一化层的选择：</strong> 证明了 Layernorms (LN) 相较于 Batchnorms (BN) 在域适应任务中表现更优。</li>
<li><strong>激光雷达强度特征的使用：</strong> 发现使用激光雷达强度作为输入特征在域适应任务中通常是有害的，因此在后续研究中将其移除。</li>
</ul>
</li>
<li><strong>提出了优化的预训练策略：</strong><ul>
<li><strong>多模态蒸馏：</strong> 利用图像领域的视觉基础模型（如 DINOv2）通过蒸馏技术为 3D 激光雷达骨干网络提供强大的预训练特征。研究表明 DINOv2 的特征比 SAM 更适合语义理解任务。</li>
<li><strong>一次预训练，多域适应：</strong> 证明了可以一次性在多个数据集上（包括源域和目标域）对骨干网络进行预训练，然后将该预训练好的骨干网络应用于多个不同的域迁移任务，而无需为每个域对重新训练。</li>
</ul>
</li>
<li><strong>优化了下游训练流程：</strong><ul>
<li><strong>冻结骨干网络，训练 MLP 头：</strong> 研究发现，在预训练骨干网络后，将其冻结，然后训练一个简单的多层感知机（MLP）分类头，能够获得最佳的性能，并且比完全微调（full finetuning）更简单、更稳定。</li>
<li><strong>顺序训练优于联合训练：</strong> 证明了先进行蒸馏预训练，再进行下游的语义分割训练，比同时进行蒸馏和语义分割训练效果更好。</li>
</ul>
</li>
<li><strong>提出了 MuDDoS 方法：</strong> 综合以上研究发现，作者提出了一个名为 MuDDoS (Multimodal Distillation for 3D Semantic Segmentation under Domain Shifts) 的方法，该方法在多个具有挑战性的数据集上取得了最先进的性能。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能大幅提升：</strong> MuDDoS 方法在四个广泛认可且具有挑战性的数据集对上取得了显著的性能提升，在某些设置下，与之前的最先进方法（如 Adapt-SAM）相比，平均 mIoU 提升高达 18.4 个百分点，甚至在 N+W 设置下提升了 24.2 个百分点。</li>
<li><strong>验证了关键设计选择的有效性：</strong> 研究结果有力地证明了骨干网络架构、归一化层选择、预训练策略（特别是多模态蒸馏和跨数据集预训练）以及下游训练流程（冻结骨干网络训练 MLP 头）对于提升 3D 激光雷达语义分割的域适应能力至关重要。</li>
<li><strong>提高了效率和可复用性：</strong> 一次预训练即可应对多个域迁移任务的发现，大大提高了模型的复用性和训练效率，避免了为每个新的域对进行昂贵的重新训练。</li>
<li><strong>为领域研究提供了指导：</strong> 该研究为未来在 3D 激光雷达语义分割领域进行域适应研究提供了宝贵的实践指导和“配方”。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>并非完美：</strong> 论文在定性结果部分（图 2）提到，尽管 MuDDoS 取得了显著进步，但其结果“并非完美”，在某些具有挑战性的场景下仍有改进空间。</li>
<li><strong>对特定类别的提升：</strong> 虽然整体性能提升显著，但论文在“Per class comparisons”部分指出，对于一些“简单”的类别（如汽车、植被、可行驶表面），所有方法表现都很好，而显著的提升主要体现在“困难”类别上（如自行车、摩托车、卡车、行人）。</li>
<li><strong>Adapt-SAM 的代码缺失：</strong> 在与 SOTA 方法 Adapt-SAM [38] 的对比中，作者提到由于 Adapt-SAM 的代码在投稿时不可用，因此无法直接在其方法上集成作者的研究发现。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>扩大骨干网络和预训练数据集的规模：</strong> 作者认为，由于蒸馏过程不需要标注数据，进一步扩大骨干网络的容量和预训练数据集的规模，有望获得更强大的泛化能力，从而为点云领域构建类似图像领域的“基础模型”。</li>
<li><strong>融合多个视觉基础模型：</strong> 探索结合不同视觉基础模型的优势，例如利用 DINOv2 的语义理解能力和 SAM 的实例分割能力，以进一步提升性能。</li>
<li><strong>探索更广泛的域适应场景：</strong> 将研究成果推广到更广泛的域适应场景，例如在恶劣天气条件下的适应，或更复杂的传感器组合。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过对 3D 激光雷达语义分割中的域适应问题进行深入的实证研究，系统地揭示了影响模型泛化能力的各个关键因素，并提出了一种名为 MuDDoS 的多模态蒸馏方法。该方法通过优化骨干网络架构、采用跨数据集预训练策略以及设计高效的下游训练流程，显著提升了模型在不同传感器之间的性能，为解决自动驾驶等领域的关键挑战提供了有效的解决方案和重要的研究启示。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17455v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17455v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.17450v1'></a></p>
<h2 id="planning-with-sketch-guided-verification-for-physics-aware-video-generation"><a href="https://arxiv.org/abs/2511.17450v1">Planning with Sketch-Guided Verification for Physics-Aware Video Generation</a></h2>
<p><strong>Authors:</strong> Yidong Huang, Zun Wang, Han Lin, Dong-Ki Kim, Shayegan Omidshafiei, Jaehong Yoon, Yue Zhang, Mohit Bansal</p>
<p><strong>Published:</strong> 2025-11-21</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Planning with Sketch-Guided Verification for Physics-Aware Video Generation”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Planning with Sketch-Guided Verification for Physics-Aware Video Generation (基于草图引导验证的物理感知视频生成规划)</p>
<p><strong>作者：</strong> Yidong Huang, Zun Wang, Han Lin, Dong-Ki Kim, Shayegan Omidshafiei, Jaehong Yoon, Yue Zhang, Mohit Bansal</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前先进的视频生成方法在生成具有时间连贯性和运动保真度的视频方面取得了显著进展。然而，它们在生成物理上真实且符合指令的精细运动方面仍然面临挑战。现有方法主要采用两种策略：单次规划（容易产生不准确或不切实际的运动）或迭代精炼（计算成本高昂，需要多次调用视频生成器）。这两种方法都无法在保证高质量运动规划的同时实现效率。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
本文提出了一种名为 <strong>SketchVerify</strong> 的新颖框架，旨在解决上述问题。SketchVerify 是一种<strong>无需训练、基于草图验证的规划框架</strong>，它在进行最终视频生成之前，通过引入一个<strong>测试时采样和验证循环</strong>来提高运动规划的质量，生成更具动态连贯性的轨迹（即物理上合理且符合指令的运动）。</p>
<p>其核心创新点包括：
*   <strong>草图代理渲染 (Sketch Proxy Rendering)：</strong> 为了高效地评估运动轨迹，SketchVerify 将每个轨迹渲染成一个轻量级的<strong>视频草图</strong>。这通过将分割出的对象叠加到静态背景上实现，避免了昂贵且重复的扩散模型合成过程，同时保留了关键的空间和时间结构信息。
*   <strong>多模态视觉语言验证器 (Multimodal Vision-Language Verifier)：</strong> 该验证器能够联合评估草图的<strong>语义一致性</strong>（是否符合指令）和<strong>物理合理性</strong>（是否符合物理定律）。它通过结构化推理，考虑了牛顿力学一致性、非穿透性、重力一致性以及形变一致性等物理原则。
*   <strong>测试时迭代精炼 (Test-Time Iterative Refinement)：</strong> SketchVerify 在测试时通过一个循环来迭代地改进运动规划。它首先预测多个候选运动计划，然后使用多模态验证器对其进行评分和排序。得分最高的轨迹被选为当前子指令的最终规划。这个过程会持续进行，直到找到满意的规划，然后传递给轨迹条件化的生成器进行最终合成。
*   <strong>高效的规划流程：</strong> 通过将验证过程从昂贵的完整视频生成转移到轻量级的草图上，SketchVerify 大幅降低了测试时的计算成本，实现了比迭代精炼方法快一个数量级的规划速度。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在 WorldModelBench 和 PhyWorldBench 这两个大型基准测试上，SketchVerify 在<strong>指令遵循、物理真实性、长期连贯性</strong>等方面均显著优于最先进的基线方法。
*   <strong>效率显著：</strong> 相较于需要多次完整视频生成的迭代精炼方法，SketchVerify 的规划时间<strong>缩短了近一个数量级</strong>（从约 12.5 分钟缩短到约 4.7 分钟），实现了约 93% 的速度提升。
*   <strong>消融实验验证：</strong> 消融研究表明，多模态验证器比仅基于语言的验证器效果更好；增加验证器的规模能提升轨迹合理性；草图验证与完整视频验证的质量相当，但效率高出近 10 倍；增加候选轨迹的数量能持续提升性能。
*   <strong>意义：</strong> SketchVerify 证明了在视频生成前进行<strong>高效、物理感知的规划</strong>是生成高质量、可控视频的关键。它为解决视频生成中的物理一致性和指令遵循难题提供了一个有效且高效的解决方案。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>粗粒度物理评估：</strong> 验证器主要评估的是<strong>粗粒度的物体运动和高层物理合理性</strong>。对于需要精细模拟的物理现象（如摩擦力、碰撞响应、流体动力学等），可能需要更复杂的模型。
*   <strong>外部 MLLM 的不确定性：</strong> 规划器和验证器都依赖于外部的大型多模态语言模型（MLLM），这些模型有时可能会做出不准确的判断，导致次优的候选轨迹选择。
*   <strong>2D 限制：</strong> 运动表示为 2D 边界框，这使得框架在处理<strong>精细的 3D 交互</strong>（如详细的物体交互或流体行为）时可能存在困难。
*   <strong>生成器能力限制：</strong> 最终视频的真实感仍然受限于底层视频生成模型的性能。</p>
<p><strong>5. 未来研究方向：</strong>
论文作者认为，随着更强大的视频生成器和验证模型的出现，上述局限性有望得到缓解。未来的研究可以集中在：
*   开发更精细、更准确的物理验证模型，以捕捉更复杂的物理动力学。
*   探索更鲁棒的 MLLM 集成方法，以减少外部模型带来的不确定性。
*   将运动表示扩展到 3D，以处理更复杂的空间交互。
*   结合更先进的视频生成模型，以进一步提升最终视频的视觉质量和真实感。</p>
<hr />
<p><strong>总结：</strong></p>
<p>这篇论文提出了一种名为 SketchVerify 的创新框架，通过在视频生成前引入一个<strong>草图引导的测试时规划和验证流程</strong>，显著提高了视频生成中运动的<strong>物理合理性</strong>和<strong>指令遵循能力</strong>。其核心在于利用轻量级的视频草图进行高效验证，避免了传统迭代精炼方法的高昂计算成本。实验结果表明，SketchVerify 在多个基准测试中取得了优于现有方法的性能，并且在效率上实现了数量级的提升。该工作为生成更真实、更可控的视频提供了一个有前景的解决方案，并指出了未来在精细物理模拟和 3D 交互方面的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop.</li>
<li>Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility.</li>
<li>Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.17450v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.17450v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-24 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
