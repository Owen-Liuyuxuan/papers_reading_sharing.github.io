<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-07 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-06/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-08/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-07">Arxiv Computer Vision Papers - 2026-01-07</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#muses-designing-composing-generating-nonexistent-fantasy-3d-creatures-without-training" class="nav-link">Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</a>
                </li>
                <li class="nav-item">
                    <a href="#infinidepth-arbitrary-resolution-and-fine-grained-depth-estimation-with-neural-implicit-fields" class="nav-link">InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</a>
                </li>
                <li class="nav-item">
                    <a href="#a-versatile-multimodal-agent-for-multimedia-content-generation" class="nav-link">A Versatile Multimodal Agent for Multimedia Content Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#ltx-2-efficient-joint-audio-visual-foundation-model" class="nav-link">LTX-2: Efficient Joint Audio-Visual Foundation Model</a>
                </li>
                <li class="nav-item">
                    <a href="#a-high-fidelity-digital-twin-for-robotic-manipulation-based-on-3d-gaussian-splatting" class="nav-link">A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#unicorn-towards-self-improving-unified-multimodal-models-through-self-generated-supervision" class="nav-link">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</a>
                </li>
                <li class="nav-item">
                    <a href="#multi-modal-data-enhanced-foundation-models-for-prediction-and-control-in-wireless-networks-a-survey" class="nav-link">Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#diffbench-meets-diffagent-end-to-end-llm-driven-diffusion-acceleration-code-generation" class="nav-link">DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#unified-thinker-a-general-reasoning-modular-core-for-image-generation" class="nav-link">Unified Thinker: A General Reasoning Modular Core for Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#transformers-self-organize-like-newborn-visual-systems-when-trained-in-prenatal-worlds" class="nav-link">Transformers self-organize like newborn visual systems when trained in prenatal worlds</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-07">Arxiv Computer Vision Papers - 2026-01-07</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提炼一份简明的 Arxiv 计算机视觉领域近期论文的执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文执行摘要 (2026-01-06)</strong></p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了以下几个关键主题和趋势：</p>
<ul>
<li><strong>多模态融合与生成：</strong> 跨越文本、图像、音频、3D 等多种模态的理解和生成是核心焦点。模型正朝着更通用、更强大的多模态能力发展，能够处理和生成复杂的内容。</li>
<li><strong>基础模型与通用性：</strong> 基础模型（Foundation Models）的构建和优化是另一大趋势，旨在通过自监督或自生成的方式提升模型的通用性和自我改进能力，减少对大规模标注数据的依赖。</li>
<li><strong>3D 内容生成与应用：</strong> 3D 内容的生成、表示和应用（如数字孪生、虚拟生物设计）正变得更加高效和逼真，特别是结合了神经隐式场和高斯溅射等新技术。</li>
<li><strong>效率与加速：</strong> 在保持高性能的同时，提升模型的效率和推理速度也是重要的研究方向，例如通过特定架构或与大型语言模型（LLM）的结合来实现。</li>
<li><strong>类比生物系统：</strong> 有研究开始探索将生物视觉系统的学习机制应用于人工神经网络，以期获得更高效、更鲁棒的学习能力。</li>
</ul>
<p><strong>2. 重点关注的创新性论文：</strong></p>
<ul>
<li><strong>"Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training"</strong> 极具创新性，它展示了在<strong>无需训练</strong>的情况下，仅通过设计和组合即可生成逼真的 3D 幻想生物，这可能颠覆传统的 3D 内容创作流程。</li>
<li><strong>"InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields"</strong> 在深度估计领域取得了显著进展，通过神经隐式场实现了任意分辨率和精细的深度估计，为 3D 重建和场景理解提供了更强大的工具。</li>
<li><strong>"UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision"</strong> 提出了一个<strong>自我改进</strong>的多模态模型框架，通过自生成监督信号来提升模型的性能，是迈向更自主、更高效基础模型的重要一步。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>无监督/自监督的 3D 内容生成：</strong> "Muses" 论文预示着未来在无需大量标注数据的情况下生成复杂 3D 内容的可能性。</li>
<li><strong>神经隐式场在 3D 任务中的深度应用：</strong> "InfiniDepth" 和 "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting" 都展示了神经隐式场和 3D 高斯溅射在精细化 3D 表示和应用中的强大潜力。</li>
<li><strong>LLM 驱动的生成模型加速：</strong> "DiffBench Meets DiffAgent" 探索了利用 LLM 来加速扩散模型代码生成，这可能成为提升生成模型开发效率的新途径。</li>
<li><strong>类比生物视觉系统的学习机制：</strong> "Transformers self-organize like newborn visual systems when trained in prenatal worlds" 引入了从生物学角度理解和设计神经网络的新视角。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>考虑到其创新性和对未来研究方向的潜在影响，以下论文值得深入阅读：</p>
<ul>
<li><strong>"Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training"</strong>: 如果您对 3D 内容生成、无监督学习或创意 AI 感兴趣，这篇论文提供了全新的思路。</li>
<li><strong>"InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields"</strong>: 对于任何从事 3D 视觉、SLAM、机器人感知或需要高精度深度信息的领域的研究者，这篇论文的技术细节至关重要。</li>
<li><strong>"UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision"</strong>: 如果您关注多模态基础模型、自监督学习或模型的可持续发展，这篇论文提出的框架值得深入研究。</li>
<li><strong>"Transformers self-organize like newborn visual systems when trained in prenatal worlds"</strong>: 这篇论文提供了对 Transformer 模型学习机制的独特见解，可能对理解和设计更高效的神经网络架构有启发。</li>
</ul>
<hr />
<p>这份摘要旨在为您提供一个快速了解最新研究动态的概览。希望它能帮助您高效地把握计算机视觉领域的最新进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.03256v1">Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</a></li>
<li><a href="#2601.03252v1">InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</a></li>
<li><a href="#2601.03250v1">A Versatile Multimodal Agent for Multimedia Content Generation</a></li>
<li><a href="#2601.03233v1">LTX-2: Efficient Joint Audio-Visual Foundation Model</a></li>
<li><a href="#2601.03200v1">A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting</a></li>
<li><a href="#2601.03193v1">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</a></li>
<li><a href="#2601.03181v1">Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey</a></li>
<li><a href="#2601.03178v1">DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation</a></li>
<li><a href="#2601.03127v1">Unified Thinker: A General Reasoning Modular Core for Image Generation</a></li>
<li><a href="#2601.03117v1">Transformers self-organize like newborn visual systems when trained in prenatal worlds</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.03256v1'></a></p>
<h2 id="muses-designing-composing-generating-nonexistent-fantasy-3d-creatures-without-training"><a href="https://arxiv.org/abs/2601.03256v1">Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</a></h2>
<p><strong>Authors:</strong> Hexiao Lu, Xiaokun Sun, Zeyu Cai, Hao Guo, Ying Tai, Jian Yang, Zhenyu Zhang</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training”论文的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training (Muses：在无需训练的情况下设计、组合、生成不存在的奇幻 3D 生物)</p>
<p><strong>作者：</strong> Hexiao Lu, Xiaokun Sun, Zeyu Cai, Hao Guo, Ying Tai, Jian Yang, Zhenyu Zhang</p>
<hr />
<h3 id="_1"><strong>论文摘要</strong></h3>
<p><strong>1. 研究问题/研究目标：</strong></p>
<p>该论文旨在解决当前 3D 内容生成领域，特别是生成不存在的、具有高度创造性的奇幻生物（如融合了不同动物、机械、神话生物特征的生物）所面临的挑战。现有方法，如依赖部件优化、手动组装或 2D 图像生成的方法，往往难以生成逼真、结构连贯且符合复杂创意描述的 3D 模型。研究的核心问题是如何在<strong>无需训练</strong>的情况下，高效且可控地生成这些高度复杂的、非领域内的 3D 生物。</p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<p>Muses 提出了一个<strong>首个无需训练的、前馈范式的奇幻 3D 生物生成框架</strong>。其核心创新在于：</p>
<ul>
<li><strong>以 3D 骨架为基础的生成范式：</strong> Muses 将 3D 骨架视为生物形态的基本表示，将其作为设计、组合和生成过程的根本依据。这克服了传统方法在精细部件操作和跨领域生成方面的局限性。</li>
<li><strong>设计-组合-生成（Design-Compose-Generate）流程：</strong><ul>
<li><strong>骨架引导的概念设计（Skeleton-guided Concept Design）：</strong> 利用图结构和大型语言模型（LLM）进行<strong>图约束推理</strong>，以生成具有合理布局和尺度的创意 3D 骨架。该方法能够将文本提示解析为概念，并生成相应的 3D 资产和骨架。</li>
<li><strong>结构化潜在空间（SLAT）的内容组合（SLAT-based Content Composition）：</strong> 利用骨架引导的<strong>蒙皮权重（skinning weights）</strong>，将骨架的各个部分映射到结构化潜在空间（SLAT）中的对应区域。然后，在 SLAT 中进行<strong>基于体素的几何和纹理插值</strong>，以融合不同对象的区域，确保组合后的形状在结构上与骨架对齐，并实现平滑连贯的几何和纹理。</li>
<li><strong>风格一致的纹理生成（Style-consistent Texture Generation）：</strong> 引入<strong>几何不变的纹理编辑</strong>方法，利用编辑后的图像和粗糙几何体，生成风格一致且和谐的纹理，从而提升最终 3D 生物的视觉保真度和多样性。</li>
</ul>
</li>
<li><strong>训练无关性：</strong> 整个流程是<strong>训练无关的（training-free）</strong>，这意味着它不需要在大量的 3D 数据集上进行预训练，从而能够处理更广泛的、非领域内的创意概念。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>生成高质量、高保真的奇幻 3D 生物：</strong> Muses 能够生成具有复杂结构、逼真几何和和谐纹理的奇幻 3D 生物，并且这些生物与文本描述高度一致。</li>
<li><strong>优于现有方法的性能：</strong> 通过与 DreamBeast、GaussianDreamer、UNO+Trellis、Trellis-Text-to-3D、OmniPart 等多种先进方法的比较，Muses 在视觉保真度、文本对齐度以及处理复杂组合描述的能力上均取得了显著优势。</li>
<li><strong>灵活的 3D 对象编辑能力：</strong> 该框架不仅能生成新生物，还展示了在几何和纹理方面的灵活编辑潜力，能够实现部件级别的 3D 编辑和风格迁移。</li>
<li><strong>意义：</strong> Muses 的工作为生成高度创意和非领域内的 3D 内容提供了一种新颖且有效的解决方案，尤其是在无需大量 3D 训练数据的情况下。它展示了 3D 骨架作为一种基础表示在复杂 3D 内容创作中的重要作用。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>依赖于底层 3D 生成模型：</strong> Muses 的生成质量在一定程度上依赖于其底层 3D 生成模型（如 Trellis）的能力。如果底层模型无法生成逼真的 3D 模型（例如，无法生成逼真的孔雀），那么 Muses 也难以从中提取有意义的骨架部分。</li>
<li><strong>骨架初始化问题：</strong> 骨架的初始生成（例如使用 Puppeteer）有时可能不理想，导致无法执行设计阶段。</li>
<li><strong>无法处理非骨架化对象：</strong> 该方法适用于具有骨架结构的生物，但无法处理无法用骨架形式化的抽象对象。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到更通用的 3D 对象编辑工具：</strong> 将 Muses 进一步发展为一个更灵活的 3D 对象编辑工具，以支持游戏、虚拟现实和动画等领域的交互式应用。</li>
<li><strong>改进底层 3D 生成和骨架建模方法：</strong> 结合更强大的 3D 生成模型和骨架建模技术，以克服当前方法在处理复杂或不常见对象时的局限性。</li>
<li><strong>处理更广泛的非骨架化对象：</strong> 探索将 Muses 的设计-组合-生成范式扩展到非骨架化对象的生成。</li>
</ul>
<hr />
<p>总而言之，Muses 论文提出了一种创新的、无需训练的 3D 生物生成框架，通过巧妙地利用 3D 骨架作为核心表示，实现了对复杂奇幻生物的有效设计、组合和生成，并在多个方面超越了现有技术，为未来 3D 内容创作开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm.</li>
<li>Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03256v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03256v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03252v1'></a></p>
<h2 id="infinidepth-arbitrary-resolution-and-fine-grained-depth-estimation-with-neural-implicit-fields"><a href="https://arxiv.org/abs/2601.03252v1">InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</a></h2>
<p><strong>Authors:</strong> Hao Yu, Haotong Lin, Jiawei Wang, Jiaxin Li, Yida Wang, Xueyang Zhang, Yue Wang, Xiaowei Zhou, Ruizhen Hu, Sida Peng</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields”的全面中文摘要：</p>
<p><strong>论文题目：</strong> InfiniDepth: 基于神经隐式场实现任意分辨率和精细化深度估计</p>
<p><strong>作者：</strong> Hao Yu, Haotong Lin, Jiawei Wang, Jiaxin Li, Yida Wang, Xueyang Zhang, Yue Wang, Xiaowei Zhou, Ruizhen Hu, Sida Peng</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
现有深度估计方法主要局限于在离散的图像网格上预测深度。这种表示方式限制了其扩展到任意输出分辨率的能力，并且难以恢复精细的几何细节。这导致在处理高分辨率场景或需要精确几何信息时效果不佳。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>神经隐式场深度表示：</strong> 论文的核心贡献是提出了一种新的深度表示方法——<strong>神经隐式场（Neural Implicit Fields）</strong>。与传统的离散网格表示不同，InfiniDepth将深度建模为一个连续的函数，允许在任意2D坐标点上查询深度值。
*   <strong>局部隐式解码器：</strong> 引入了一个简单而有效的<strong>局部隐式解码器</strong>，该解码器能够聚合来自多尺度特征的金字塔的特征，并将其输入到一个轻量级的MLP中，从而在连续的2D坐标上预测深度。这种连续和局部化的预测范式使得模型能够自然地生成任意分辨率的深度图，并捕捉精细的几何细节。
*   <strong>无限深度查询策略（Infinite Depth Query）：</strong> 为了解决传统逐像素深度预测在视角变化较大时导致的3D点云密度不均问题，论文设计了一种<strong>深度查询策略</strong>。该策略通过自适应地分配子像素查询预算，使得可见表面上的3D点云分布更加均匀，从而显著提高了在大视角偏移下的新视图合成（NVS）质量，减少了孔洞和伪影。
*   <strong>Synth4K数据集：</strong> 为了更好地评估模型在任意分辨率和精细化细节方面的能力，作者<strong>创建了一个高质量的4K合成数据集Synth4K</strong>。该数据集包含来自五个不同游戏的场景，具有丰富的几何和外观细节，并提供了4K分辨率的地面真实深度图。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>SOTA性能：</strong> 实验结果表明，InfiniDepth在Synth4K和真实世界数据集上的相对和度量深度估计任务中均取得了<strong>最先进（State-of-the-Art）的性能</strong>，尤其在精细细节区域表现出色。
*   <strong>任意分辨率和精细化：</strong> InfiniDepth能够生成任意分辨率的深度图，并且在恢复精细几何细节方面远超现有方法，这在图1(b)所示的精细化点云中得到了体现。
*   <strong>提升新视图合成：</strong> 结合无限深度查询策略，InfiniDepth在新视图合成任务中表现出显著优势，能够生成更完整、更稳定的新视图，尤其是在大视角偏移的情况下（如图1(c)和图8所示）。
*   <strong>数据集贡献：</strong> Synth4K数据集为高分辨率和精细化深度估计的研究提供了一个更具挑战性和更可靠的评估基准。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>时间一致性：</strong> 该方法主要针对单目深度估计，并且仅在单目深度数据上进行训练。当应用于视频时，它<strong>不显式地强制执行时间一致性</strong>，可能导致帧间出现闪烁现象。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>多视角和时间一致性：</strong> 将深度表示扩展到<strong>多视角设置</strong>，以提高时间一致性并减少视频中的闪烁。
*   <strong>3D感知和重建：</strong> 将InfiniDepth集成到更广泛的<strong>3D感知和重建流水线</strong>中。
*   <strong>更广泛的应用：</strong> 探索其在<strong>更广泛的3D感知和重建应用</strong>中的潜力。</p>
<p><strong>总结：</strong>
InfiniDepth通过引入神经隐式场作为深度表示，成功解决了现有深度估计方法在分辨率扩展性和细节恢复方面的根本性限制。其提出的局部隐式解码器和无限深度查询策略，不仅在深度估计任务上取得了显著的性能提升，尤其是在精细细节方面，还极大地改善了在大视角偏移下的新视图合成质量。Synth4K数据集的创建也为该领域的研究提供了重要资源。该工作为实现更高精度、更灵活的深度估计和3D场景理解开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details.</li>
<li>Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions.</li>
<li>It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03252v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03252v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03250v1'></a></p>
<h2 id="a-versatile-multimodal-agent-for-multimedia-content-generation"><a href="https://arxiv.org/abs/2601.03250v1">A Versatile Multimodal Agent for Multimedia Content Generation</a></h2>
<p><strong>Authors:</strong> Daoan Zhang, Wenlin Yao, Xiaoyang Wang, Yebowen Hu, Jiebo Luo, Dong Yu</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面中文摘要，重点突出其在多模态内容生成领域的贡献。</p>
<p><strong>论文题目：</strong> A Versatile Multimodal Agent for Multimedia Content Generation</p>
<p><strong>作者：</strong> Daoan Zhang, Wenlin Yao, Xiaoyang Wang, Yebowen Hu, Jiebo Luo, Dong Yu</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>当前AI生成内容（AIGC）技术虽然在视频编辑、音乐生成等领域取得了显著进展，但大多数模型仍局限于单一模态或作为特定应用场景的组件，无法有效处理现实世界中复杂、端到端的多模态内容生成任务。现实世界的媒体创作往往涉及多种模态（如图像、视频、音频、文本）的整合，而现有模型在有效整合这些模态以生成连贯、符合用户偏好的多模态内容方面存在不足。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>MultiMedia-Agent 框架：</strong> 论文提出了一种名为 MultiMedia-Agent 的系统，旨在自动化复杂的、端到端的多模态内容创作流程。</li>
<li><strong>技能获取理论驱动的训练：</strong> 核心创新在于将“技能获取理论”（Skill Acquisition Theory）应用于多模态内容生成代理的训练。该理论将学习过程分为认知阶段、联想阶段和自主阶段，论文据此设计了一个三阶段的训练流程，使代理能够从零开始学习复杂计划的生成和执行。</li>
<li><strong>两阶段层级化计划策定策略：</strong><ul>
<li><strong>数据生成与工具库：</strong> 构建了一个包含18种真实世界多模态任务的数据集，并设计了一个包含理解工具、生成/编辑工具和辅助工具的全面工具库。</li>
<li><strong>自相关与模型偏好相关：</strong> 引入了一个两阶段的计划优化策略。首先，利用GPT-4o进行“自相关”（self-correlation），通过自我反思和纠错来优化初始计划。接着，利用模型偏好相关（model preference correlation），通过基于人类偏好的评估指标来进一步精炼计划，确保生成的内容不仅功能上可行，而且在美学和情感上符合用户期望。</li>
</ul>
</li>
<li><strong>多模态内容评估指标：</strong> 设计了一系列针对图像、视频、音频和文本输出的评估指标，并引入了音频-视频对齐度量，以全面评估生成内容的质量和用户偏好对齐度。</li>
</ul>
<p><strong>3. 主要研究结果与意义：</strong></p>
<ul>
<li><strong>有效性验证：</strong> 通过实验表明，MultiMedia-Agent 在多模态内容生成任务上表现出有效性，能够生成比现有模型更优质的多模态内容。</li>
<li><strong>技能习得的体现：</strong> 三阶段的训练方法成功地使代理能够逐步掌握复杂计划的生成和执行，并在自主阶段实现了显著的性能提升，尤其是在满足人类偏好方面。</li>
<li><strong>对比优势：</strong> 与现有的一些多模态工具代理和内容生成代理相比，MultiMedia-Agent 在多模态理解、计划能力和偏好对齐方面展现出更强的综合能力（如Table 1所示）。</li>
<li><strong>可视化结果：</strong> 实验可视化结果（Figure 4）清晰地展示了代理在不同训练阶段生成内容的质量提升，从最初缺乏音频和特效，到后期能够整合字幕、特效和符合场景的音频，体现了方法的有效性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>工具选择方式：</strong> 目前的工具选择主要依赖于提示（prompt-based），对于海量工具库而言，效率和准确性有待提升。</li>
<li><strong>单代理局限：</strong> 在处理极其复杂的任务时，单代理系统可能不如多代理系统高效。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>检索增强生成（RAG）用于工具选择：</strong> 探索使用RAG技术来优化工具的选择过程，以应对大规模工具库。</li>
<li><strong>多代理系统：</strong> 研究和开发多代理系统，以更有效地解决复杂的多模态内容生成任务。</li>
</ul>
<p><strong>对计算机视觉领域的贡献：</strong></p>
<p>这篇论文为计算机视觉领域在<strong>多模态内容生成</strong>方面提供了重要的理论和实践贡献。它不仅提出了一种能够处理跨多种模态的复杂内容创作的通用代理框架，更重要的是，它成功地将<strong>技能获取理论</strong>这一认知科学的原理引入到AI代理的学习过程中，实现了代理从基础操作到复杂、偏好对齐的自主学习。这为构建更智能、更人性化的多模态内容生成系统开辟了新的途径，尤其是在<strong>视频生成、图像编辑与音频整合</strong>等视觉相关任务中，其方法论具有重要的参考价值和应用前景。论文提出的<strong>层级化计划策定策略</strong>和<strong>偏好对齐机制</strong>，也为提升生成内容的质量和用户满意度提供了有效的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation.</li>
<li>Notably, we introduce the skill acquisition theory to model the training data curation and agent training.</li>
<li>The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03250v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03250v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03233v1'></a></p>
<h2 id="ltx-2-efficient-joint-audio-visual-foundation-model"><a href="https://arxiv.org/abs/2601.03233v1">LTX-2: Efficient Joint Audio-Visual Foundation Model</a></h2>
<p><strong>Authors:</strong> Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LTX-2: Efficient Joint Audio-Visual Foundation Model”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的重要性：</p>
<p><strong>论文题目：</strong> LTX-2: Efficient Joint Audio-Visual Foundation Model</p>
<p><strong>作者：</strong> Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>当前文本到视频（Text-to-Video, T2V）扩散模型虽然能够生成引人入胜的视频序列，但普遍存在“沉默”的问题，即无法捕捉音频所提供的语义、情感和氛围线索。这使得生成的视频内容在沉浸感和实用性上大打折扣。同时，现有的文本到音频（Text-to-Audio, T2A）模型大多是任务特定的，缺乏统一、全面的音频生成能力。将两者结合的文本到音频+视频（Text-to-Audio+Video, T2AV）生成模型的研究仍处于早期阶段，并且现有方法往往采用解耦的流水线，未能充分建模音视频模态间的深层联合分布和双向依赖关系。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>LTX-2 提出了一种新颖的、开源的、高效的联合音频-视频基础模型，旨在生成高质量、时间同步的视听内容。其核心创新点包括：</p>
<ul>
<li><strong>非对称双流Transformer架构：</strong> 模型采用一个包含140亿参数的视频流和一个50亿参数的音频流的非对称双流Transformer架构。这种设计将更多计算资源分配给视觉生成，同时保持音频流的高效性，以适应两种模态信息密度的差异。</li>
<li><strong>跨模态AdaLN和时间位置嵌入：</strong> 通过双向音频-视频交叉注意力层、时间位置嵌入（视频使用3D RoPE，音频使用1D RoPE）以及跨模态AdaLN（Adaptive Layer Normalization）进行共享时间步长条件化，实现了紧密的时间对齐和模态间的有效信息交换。</li>
<li><strong>多语言文本编码器与“思考令牌”：</strong> 采用一个强大的多语言文本编码器（Gemma3-12B）和一种改进的文本处理管道，引入了“思考令牌”（Thinking Tokens），以增强对复杂提示的理解能力，提升语音生成的韵律、口音和情感准确性。</li>
<li><strong>模态感知分类器自由引导（Modality-Aware CFG）：</strong> 提出了一种新颖的Bimodal CFG机制，允许独立控制文本和跨模态引导的权重，从而显著改善了视听对齐和可控性。</li>
<li><strong>高效的音频VAE和潜在空间表示：</strong> 设计了一个紧凑的音频VAE，生成高保真度的1D潜在空间，并支持立体声输入，优化了音频生成效率。</li>
<li><strong>多尺度、多切片推理：</strong> 采用一种高效的推理策略，能够生成全高清（1080p）视听内容，同时显著降低了内存开销。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>状态级视听质量：</strong> LTX-2 在评估中展现了最先进的视听质量和提示遵循度，优于现有的开源系统。</li>
<li><strong>与专有模型媲美：</strong> 其生成结果在人类偏好研究中与领先的专有模型（如Veo 3和Sora 2）相当。</li>
<li><strong>极高的效率：</strong> LTX-2 在计算成本和推理时间上远低于同类模型，例如比Wan 2.2-14B快约18倍，这使其成为一个非常实用的基础模型。</li>
<li><strong>开源与可访问性：</strong> 模型权重和代码的公开，为社区提供了强大的T2AV生成基础，极大地推动了该领域的研究和应用。</li>
<li><strong>超越现有时间范围：</strong> LTX-2 能够生成长达20秒的连续视频和同步立体声，超过了许多现有模型。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>语言差异：</strong> 模型在不同语言上的表现存在差异，对于训练数据中代表性不足的语言或方言，可能导致语音合成或视听对齐不够准确。</li>
<li><strong>多说话人场景：</strong> 在多说话人场景下，模型可能无法始终准确地将语音内容分配给正确的角色，有时会混淆说话人。</li>
<li><strong>时间漂移：</strong> 生成超过20秒的连续视听序列可能导致时间漂移、同步性下降或场景多样性减少。</li>
<li><strong>缺乏显式推理能力：</strong> LTX-2 是一个生成模型，不具备显式的推理或世界建模能力，复杂的叙事连贯性、事实准确性或情境理解需要依赖外部大型语言模型。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>偏见缓解与可解释性：</strong> 探索方法来减轻模型中存在的偏见，并提高合成内容的真实性验证能力。</li>
<li><strong>跨语言和低资源语言支持：</strong> 进一步提升模型在不同语言上的表现，特别是针对低资源语言。</li>
<li><strong>更长序列生成：</strong> 克服长序列生成中的时间漂移问题，实现更长的叙事性内容生成。</li>
<li><strong>集成推理能力：</strong> 将LTX-2与外部推理系统结合，以生成更具逻辑性和事实准确性的内容。</li>
<li><strong>更精细的控制：</strong> 开发更精细的控制机制，允许用户在生成过程中对视听内容的各个方面进行更细致的调整。</li>
</ul>
<p><strong>在计算机视觉和机器学习领域的重要性：</strong></p>
<p>LTX-2 的发布标志着文本到音频+视频（T2AV）生成领域的一个重要里程碑。它不仅提供了一个<strong>高效、高质量且开源</strong>的基础模型，解决了现有T2V模型缺乏音频的痛点，而且通过其创新的<strong>非对称双流架构、跨模态交互机制和先进的文本理解能力</strong>，显著提升了视听内容的同步性、连贯性和表现力。其<strong>卓越的推理效率</strong>使得大规模、高分辨率的视听内容生成成为可能，为内容创作、教育、辅助技术等领域开辟了新的可能性。LTX-2 的开源性质将极大地促进该领域的研究和应用，并为未来更复杂的<strong>多模态生成模型</strong>奠定坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner.</li>
<li>In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03233v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03233v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03200v1'></a></p>
<h2 id="a-high-fidelity-digital-twin-for-robotic-manipulation-based-on-3d-gaussian-splatting"><a href="https://arxiv.org/abs/2601.03200v1">A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting</a></h2>
<p><strong>Authors:</strong> Ziyang Sun, Lingfan Bao, Tianhu Peng, Jingcheng Sun, Chengxu Zhou</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于使用3D高斯溅射（3D Gaussian Splatting, 3DGS）构建高保真数字孪生的论文的全面中文摘要，重点关注其在机器人操作中的应用。</p>
<p><strong>论文题目：</strong> A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting
<strong>作者：</strong> Ziyang Sun, Lingfan Bao, Tianhu Peng, Jingcheng Sun, Chengxu Zhou</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
机器人操作的自主性发展需要高保真、可交互的数字孪生，以实现闭环运动规划和可靠的现实世界执行，这对于推进“仿真到现实”（sim-to-real）的迁移至关重要。然而，现有方法在重建速度慢、视觉保真度有限以及将照片级真实感模型转换为可用于规划的碰撞几何体方面存在显著瓶颈。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
该论文提出了一种实用的端到端框架，能够从稀疏的RGB视图在几分钟内构建高质量的数字孪生。其核心贡献包括：</p>
<ul>
<li><strong>基于3DGS的高保真、快速重建：</strong> 利用3D高斯溅射（3DGS）作为统一的场景表示，实现了快速、照片级真实感的场景重建，解决了传统方法（如NeRFs）计算成本高、速度慢的问题。</li>
<li><strong>可见光感知语义融合（Visibility-Aware Semantic Fusion）：</strong> 引入了一种视图依赖的语义聚合方法，结合遮挡感知置信度加权策略，将2D分割线索提升到3D空间，确保了准确的3D标注，即使在遮挡情况下也能保持语义一致性。</li>
<li><strong>高效的碰撞几何体生成：</strong> 开发了一种基于过滤器的几何体转换方法，将原始的3DGS点云转换为精确、可用于规划的碰撞网格（collision meshes），无缝集成到Unity-ROS2-MoveIt物理引擎中。该过程包括多阶段的去噪和网格化处理，以去除伪影并生成水密网格。</li>
<li><strong>统一的“现实-仿真-现实”闭环流程：</strong> 构建了一个完整的闭环流程，将现实世界的场景捕获、数字孪生生成、仿真中的运动规划与验证，以及最终在真实机器人上的执行整合在一起。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>重建效率与质量：</strong> 该系统平均重建时间不到4分钟（229秒），比NeRFs方法快5倍，同时实现了更高的视觉保真度（PSNR 37.03 dB，SSIM 0.9821），优于NeRFs方法。
*   <strong>语义理解：</strong> 实现了0.87的2D分割mIoU和0.93的3D投影一致性，证明了其多视图融合方法能有效桥接2D感知和3D几何重建。
*   <strong>几何体质量：</strong> 通过消融实验证明，多阶段的去噪和网格化处理（启发式过滤+DBSCAN聚类）能够显著提高几何保真度，F1-Score达到近乎完美的0.9989，为机器人操作提供了可靠的碰撞几何体。
*   <strong>现实世界验证：</strong> 在Franka Emika机器人上进行的长期重排任务中，仿真验证成功率为100%，真实世界执行成功率为90%（9/10次），且成功执行的轨迹零碰撞，平均放置误差为0.83厘米。
*   <strong>意义：</strong> 该框架有效地缩小了“仿真到现实”的差距，为在非结构化环境中实现可靠的机器人操作提供了快速、可靠且可扩展的路径。它将神经渲染与传统运动规划框架的关键环节——可用于规划的碰撞几何体——有效结合。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>静态场景假设：</strong> 当前框架主要针对静态场景，并执行一次性重建。
*   <strong>仅模型几何和外观：</strong> 系统目前仅建模几何和外观，未包含物理属性的估计。
*   <strong>预定义抓取：</strong> 当前工作侧重于给定预定义抓取的运动规划，而非直接在高斯表示上进行抓取规划。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>动态场景处理：</strong> 集成动态3DGS变体或连续更新机制，使数字孪生能适应不断变化的环境。
*   <strong>物理属性估计：</strong> 整合在线物理属性估计方法，以支持更复杂的接触式交互。
*   <strong>集成抓取规划：</strong> 开发直接在3DGS表示上操作的鲁棒抓取规划模块。
*   <strong>作为学习策略的赋能者：</strong> 将数字孪生用作安全验证平台，或作为数据生成引擎，自主创建训练数据集，加速机器人学习模型（如视觉-语言-动作模型和强化学习）的开发。</p>
<hr />
<p><strong>总结：</strong></p>
<p>这篇论文提出了一种创新的方法，利用3D高斯溅射（3DGS）技术，在极短的时间内从稀疏的RGB图像构建出高保真、语义丰富的数字孪生。该框架通过引入可见光感知语义融合和高效的几何体清理及网格化流程，成功解决了现有方法在重建速度、视觉保真度和可规划性方面的不足。实验结果表明，该方法能够生成高质量的碰撞几何体，并成功应用于真实的机器人操作任务，显著推进了“仿真到现实”的迁移能力。该工作为未来在复杂非结构化环境中实现更自主、更可靠的机器人操作奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs.</li>
<li>In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03200v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03200v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03193v1'></a></p>
<h2 id="unicorn-towards-self-improving-unified-multimodal-models-through-self-generated-supervision"><a href="https://arxiv.org/abs/2601.03193v1">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</a></h2>
<p><strong>Authors:</strong> Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇题为“UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision”的论文进行了分析。以下是我的详细解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>这篇论文提出了一种名为 UniCorn 的自监督学习框架，旨在解决统一多模态模型（UMMs）在理解输入后难以生成高质量、可控输出的问题，即“传导失语症”。UniCorn 通过将模型分解为“提议者”、“解决者”和“评判者”三个角色，利用模型自身的交互和认知模式重构来实现自我改进，无需外部数据或教师监督，显著提升了文本到图像生成能力。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>“传导失语症”（Conduction Aphasia）的定义与解决：</strong> 论文首次提出了“传导失语症”这一概念，形象地描述了 UMMs 在跨模态理解和生成之间存在的鸿沟。UniCorn 的核心在于解决这一问题。</li>
<li><strong>自生成监督（Self-Generated Supervision）：</strong> 这是 UniCorn 最核心的创新点。它完全摆脱了对外部数据集或人工标注的依赖，通过模型自身的“自我玩耍”（self-play）和内部机制来产生训练信号。</li>
<li><strong>三角色协同框架（Proposer, Solver, Judge）：</strong><ul>
<li><strong>Proposer (提议者):</strong> 负责提出生成任务或生成初始内容。</li>
<li><strong>Solver (解决者):</strong> 负责根据提议者的输入进行多模态生成（例如，根据文本生成图像）。</li>
<li><strong>Judge (评判者):</strong> 负责评估 Solver 的生成结果是否符合 Proposer 的意图或多模态的连贯性。这个评判者可以看作是模型内部的一个“鉴别器”或“评估器”。</li>
</ul>
</li>
<li><strong>认知模式重构（Cognitive Pattern Reconstruction）：</strong> 通过这种机制，模型将潜在的、隐式的多模态理解转化为显式的、可用于指导生成的信号。这可以理解为模型在学习如何“思考”并将其“思考过程”转化为生成指令。</li>
<li><strong>UniCycle 基准测试：</strong> 为了量化和验证多模态连贯性的恢复，论文引入了一个新的循环一致性（cycle-consistency）基准测试。它通过“文本 -&gt; 图像 -&gt; 文本”的重建循环来评估模型的性能，这是一种非常直观且有力的评估方法。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动 UMMs 的生成能力：</strong> UniCorn 的成功将极大地推动 UMMs 在生成任务上的表现，使其不再仅仅是强大的理解工具，更能成为高质量的内容创作者。</li>
<li><strong>降低数据依赖，加速模型发展：</strong> 自监督学习是当前机器学习领域的重要趋势。UniCorn 的方法论如果被广泛接受和应用，将显著降低开发和训练 UMMs 的成本，加速相关领域的研究和应用落地。</li>
<li><strong>提升模型的可控性和忠实度：</strong> 通过“传导失语症”的解决，模型生成的输出将更符合用户的意图，更忠实于输入信息，从而提高用户体验和模型的可信度。</li>
<li><strong>为其他多模态任务提供通用框架：</strong> UniCorn 的自改进框架具有普适性，理论上可以应用于其他需要跨模态理解和生成相结合的任务，如视频生成、音频合成等。</li>
</ul>
<p><strong>4. 可能受益于该研究的相关领域或应用</strong></p>
<ul>
<li><strong>文本到图像生成（Text-to-Image Generation）：</strong> 这是论文直接验证并取得显著成果的领域，如 DALL-E, Stable Diffusion 等模型的进一步优化。</li>
<li><strong>多模态对话系统：</strong> 能够更好地理解用户意图并生成更具信息量和连贯性的回复。</li>
<li><strong>视觉问答（Visual Question Answering）与视觉推理：</strong> 提升模型在理解图像和文本关系后，进行更深层次推理和生成答案的能力。</li>
<li><strong>内容创作与辅助设计：</strong> 自动生成插画、设计草图、营销素材等。</li>
<li><strong>虚拟现实/增强现实（VR/AR）：</strong> 实时生成符合场景描述的虚拟内容。</li>
<li><strong>教育与培训：</strong> 生成教学材料、模拟场景等。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>“传导失语症”的普遍性与定义：</strong> 虽然论文提出了“传导失语症”的概念，但其在所有 UMMs 中的普遍程度以及其具体表现形式可能需要更深入的研究来界定。</li>
<li><strong>三角色协同的复杂性与计算成本：</strong> 将一个 UMM 分解为三个协同角色，虽然是自监督，但其训练过程可能比传统的监督学习模型更复杂，计算资源需求也可能更高。</li>
<li><strong>“认知模式重构”的内在机制：</strong> 摘要中对“认知模式重构”的描述较为抽象，其具体的实现细节和理论基础可能需要进一步的论文内容来阐述。</li>
<li><strong>UniCycle 基准的局限性：</strong> UniCycle 作为一种新的基准，虽然有意义，但其是否能完全捕捉到所有多模态连贯性的细微差别，以及其与现有基准的互补性，仍需进一步验证。</li>
<li><strong>泛化能力：</strong> 论文展示了在六个通用图像生成基准上的提升，但其在更广泛、更复杂的多模态任务上的泛化能力仍需观察。</li>
<li><strong>“简单而优雅”的实现：</strong> 尽管摘要称其“简单而优雅”，但实际实现过程中，如何有效地协调三个角色，如何设计有效的评判机制，可能仍然存在技术挑战。</li>
</ul>
<p><strong>总结来说，这篇论文的亮点在于其创新的自监督学习框架 UniCorn，通过“传导失语症”的定义和三角色协同机制，成功解决了 UMMs 在生成方面的短板，并且完全摆脱了外部数据依赖。这对于推动多模态人工智能向更自主、更强大的方向发展具有重要意义。</strong></p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision.</li>
<li>To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop.</li>
<li>These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03193v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03193v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03181v1'></a></p>
<h2 id="multi-modal-data-enhanced-foundation-models-for-prediction-and-control-in-wireless-networks-a-survey"><a href="https://arxiv.org/abs/2601.03181v1">Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey</a></h2>
<p><strong>Authors:</strong> Han Zhang, Mohammad Farzanullah, Mohammad Ghassemi, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.NI, cs.AI, cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Foundation models (FMs) are recognized as a transformative breakthrough that has started to reshape the future of artificial intelligence (AI) across both academia and industry. The integration of FMs into wireless networks is expected to enable the development of general-purpose AI agents capable of handling diverse network management requests and highly complex wireless-related tasks involving multi-modal data. Inspired by these ideas, this work discusses the utilization of FMs, especially multi-modal FMs in wireless networks. We focus on two important types of tasks in wireless network management: prediction tasks and control tasks. In particular, we first discuss FMs-enabled multi-modal contextual information understanding in wireless networks. Then, we explain how FMs can be applied to prediction and control tasks, respectively. Following this, we introduce the development of wireless-specific FMs from two perspectives: available datasets for development and the methodologies used. Finally, we conclude with a discussion of the challenges and future directions for FM-enhanced wireless networks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey”的全面中文摘要：</p>
<p><strong>论文摘要：多模态数据增强的无线网络预测与控制基础模型</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>随着人工智能（AI）的快速发展，基础模型（Foundation Models, FMs）已成为一股变革性力量，有望重塑AI的未来。在无线网络领域，将FMs，特别是多模态FMs，集成进来，能够开发出能够处理多样化网络管理请求和复杂无线相关任务的通用AI代理。然而，目前的研究在如何利用FMs处理多模态数据以实现无线网络管理中的预测和控制任务方面，以及如何开发专门针对无线网络的FMs方面，仍存在不足。本文旨在全面探讨FMs在无线网络中的应用，重点关注多模态数据处理、预测和控制任务，并为开发无线网络特有的FMs提供指导。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<ul>
<li><strong>系统性综述：</strong> 本文提供了对FMs在无线网络中应用的全面综述，重点关注多模态FMs。它详细介绍了FMs的演进、能力、典型模型以及部署方式，并阐述了如何有效利用它们处理上下文数据以实现网络管理。</li>
<li><strong>任务导向的分析：</strong> 论文将无线网络管理任务分为两大类：预测任务和控制任务，并深入探讨了FMs在这些任务中的应用。<ul>
<li><strong>预测任务：</strong> 涵盖了无线流量预测、信道状态信息（CSI）预测、无线链路故障预测和阻塞预测等。</li>
<li><strong>控制任务：</strong> 探讨了FMs如何指导强化学习（RL）代理的探索策略、提取观测的潜在表示，以及作为交互式代理在控制场景中发挥作用。</li>
</ul>
</li>
<li><strong>无线网络特有FMs的开发：</strong> 论文从两个关键视角介绍了开发无线网络特有FMs的方法：<ul>
<li><strong>数据集：</strong> 总结了可用于FM开发、涵盖无线流量、射频（RF）信号和各种传感模态的现有数据集。</li>
<li><strong>方法论：</strong> 探讨了预训练（从头开始或利用现有模型）和微调FMs以适应无线网络的方法，包括自监督学习、跨模态学习和参数高效微调（PEFT）技术。</li>
</ul>
</li>
<li><strong>多模态数据理解：</strong> 论文重点介绍了FMs如何理解和利用无线网络中的多模态上下文信息，包括视觉数据、图信息、3D点云数据以及其他模态（如ISAC数据、RF数据和网络信息）。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>通用AI代理的潜力：</strong> FMs能够处理多样化的无线网络任务，并利用多模态数据提供更丰富的上下文理解，这预示着通用AI代理在无线网络管理中的巨大潜力。</li>
<li><strong>提升预测和控制能力：</strong> FMs能够显著提高无线网络中预测任务（如流量预测、阻塞预测）和控制任务（如资源分配、波束管理）的准确性和效率。</li>
<li><strong>推动无线网络智能化：</strong> 通过利用FMs，无线网络可以实现更智能、更自适应的网络管理，例如主动的资源调度、更优化的用户体验和更强的网络韧性。</li>
<li><strong>为未来研究奠定基础：</strong> 本文为研究人员提供了关于FMs在无线网络中应用、开发和部署的全面视角，为未来的研究指明了方向。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>数据可用性与计算成本：</strong> 开发无线网络特有FMs面临数据稀缺和标注成本高昂的挑战。同时，FMs的巨大规模带来了高昂的计算成本和部署难题。</li>
<li><strong>隐私问题：</strong> 在联邦学习等协作式FM开发中，模型参数的传输可能暴露敏感信息，需要解决隐私保护问题。</li>
<li><strong>实时性与延迟：</strong> FMs的计算量大，可能导致推理延迟，这对于对延迟敏感的无线网络应用是一个挑战。</li>
<li><strong>模型泛化能力：</strong> 通用FMs可能缺乏对无线网络特有知识的深入理解，导致在特定任务上表现不佳，需要进行微调或领域适应。</li>
<li><strong>“幻觉”现象：</strong> FMs有时会生成不准确或无意义的输出（“幻觉”），这会影响其鲁棒性和可靠性。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>开发无线网络特有模态的FMs：</strong> 需要开发能够有效处理无线网络特有数据模态（如CSI、RF信号）的FMs。</li>
<li><strong>增强预训练FMs的无线网络知识：</strong> 通过微调、提示工程等技术，将无线网络特有知识注入预训练FMs，以提升其在无线通信任务中的性能。</li>
<li><strong>优化FMs的部署：</strong> 研究如何在资源受限的无线网络边缘设备上高效部署大型FMs，包括模型压缩、量化、分布式推理等技术。</li>
<li><strong>解决延迟问题：</strong> 探索低延迟的FM推理技术，如边缘计算、模型分区和硬件加速。</li>
<li><strong>提升模型鲁棒性与安全性：</strong> 研究如何通过引入约束、数据过滤、领域知识等方法来提高FMs的鲁棒性，并解决模型中毒、对抗性攻击等安全问题。</li>
<li><strong>实现Agentic AI：</strong> 将FMs作为智能代理，实现自主的决策和控制，以应对复杂多变的无线网络环境。</li>
<li><strong>克服多模态数据稀缺性：</strong> 探索数据增强、合成数据生成、联邦学习等方法，以解决多模态无线数据稀缺的问题。</li>
</ul>
<p>总而言之，这篇综述论文为理解和利用基础模型（FMs）在无线网络领域的潜力提供了全面的视角，特别强调了多模态数据在预测和控制任务中的关键作用，并为未来的研究和发展指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Following this, we introduce the development of wireless-specific FMs from two perspectives: available datasets for development and the methodologies used.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03181v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03181v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03178v1'></a></p>
<h2 id="diffbench-meets-diffagent-end-to-end-llm-driven-diffusion-acceleration-code-generation"><a href="https://arxiv.org/abs/2601.03178v1">DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation</a></h2>
<p><strong>Authors:</strong> Jiajun jiao, Haowei Zhu, Puyuan Yang, Jianghui Wang, Ji Liu, Ziqiong Liu, Dong Li, Yuejian Fang, Junhai Yong, Bin Wang, Emad Barsoum</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation”的全面摘要，重点关注其在计算机视觉领域的贡献：</p>
<p><strong>论文摘要</strong></p>
<p><strong>1. 要解决的主要问题/研究问题：</strong></p>
<p>扩散模型在图像和视频生成方面取得了巨大成功，但其多步推理过程带来了显著的计算开销，阻碍了其在现实世界中的广泛应用。如何有效地结合多种模型加速技术以降低推理成本，同时保持生成质量，是一个巨大的挑战。现有方法需要专家干预和针对特定模型及部署场景的定制化工程，这在扩散模型架构日益多样化和部署环境日益复杂的情况下变得尤为困难。因此，论文旨在解决如何自动化生成高质量、优化的扩散模型加速代码的问题。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>该论文提出了一个端到端的、由大型语言模型（LLM）驱动的框架，用于自动化扩散模型加速代码的生成和评估。其核心贡献包括：</p>
<ul>
<li><strong>DiffBench：</strong> 一个全面的基准测试套件，用于评估 LLM 生成扩散模型加速代码的能力。DiffBench 包含 604 个任务，涵盖了多样化的扩散模型架构、优化组合和部署场景，并实现了一个三阶段的自动化评估流程（静态参数评估、绝对性能评估和相对性能分析），以确保对生成代码的严格评估。</li>
<li><strong>DiffAgent：</strong> 一个基于 LLM 的智能体框架，用于生成任意扩散模型的最佳加速策略和代码。DiffAgent 采用闭环工作流程，集成了规划（Planning）、编码（Coding）和调试（Debugging）三个核心组件，并引入了一个遗传算法（Genetic Algorithm）选择器。该选择器利用执行环境提供的性能反馈，迭代地优化代码生成组件的输出，以满足用户指定的精度和效率目标。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>DiffBench 的有效性：</strong> 实验表明，DiffBench 提供了一个比现有基准更具挑战性的评估环境，能够有效区分不同 LLM 在扩散模型加速代码生成方面的能力。</li>
<li><strong>DiffAgent 的优越性：</strong> 实验证明，DiffAgent 显著优于现有的 LLM，在生成有效的扩散模型加速策略方面取得了显著进步。与直接代码生成相比，DiffAgent 在不同难度级别的任务上，将 Claude Sonnet 4 的平均通过率从 54.30% 提升到 81.59%。</li>
<li><strong>模块化贡献：</strong> 消融研究表明，DiffAgent 的三个核心模块（知识库、遗传算法和调试器）协同工作，共同实现了其卓越的性能。特别是遗传算法对于处理复杂性能约束的任务至关重要，知识库提供了广泛的优势，而调试器则增强了在高难度任务上的鲁棒性。</li>
<li><strong>自动化和效率提升：</strong> DiffAgent 能够自动化地生成满足用户需求的高质量加速代码，显著减少了对人工干预的需求，并实现了可观的推理速度提升，同时保持了生成质量的损失在可接受范围内。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 尽管 DiffAgent 旨在提高效率，但其迭代优化过程和 LLM 的调用仍然需要一定的计算资源。</li>
<li><strong>“硬”任务的挑战：</strong> 对于一些极具挑战性的任务（“hard” samples），论文提到可能并不总是有有效的解决方案，这表明在某些极端情况下，LLM 的能力仍然受到限制。</li>
<li><strong>LLM 的固有局限性：</strong> 尽管 DiffAgent 取得了显著进展，但 LLM 本身在理解复杂推理、处理细微的性能调优和避免级联错误方面仍可能存在局限性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的加速技术集成：</strong> 探索将更多新兴的扩散模型加速技术集成到 DiffAgent 框架中。</li>
<li><strong>更精细的性能调优：</strong> 研究更高级的优化策略，以在保持极低质量损失的情况下实现更高的速度提升。</li>
<li><strong>跨模型和跨硬件的泛化能力：</strong> 进一步提升 DiffAgent 在不同扩散模型架构和硬件平台上的泛化能力。</li>
<li><strong>用户交互和反馈机制：</strong> 探索更直观的用户交互方式，以及更有效的反馈机制，以进一步优化代码生成过程。</li>
<li><strong>LLM 自身能力的提升：</strong> 随着 LLM 技术的发展，其在代码理解、推理和优化方面的能力将不断增强，这将进一步推动 DiffAgent 的性能。</li>
</ul>
<p>总而言之，这篇论文通过引入 DiffBench 和 DiffAgent，为解决扩散模型推理效率低下这一关键问题提供了创新的自动化解决方案。DiffBench 提供了一个标准化的评估平台，而 DiffAgent 则通过智能的 LLM 驱动的框架，实现了高质量的扩散模型加速代码生成，为该领域的研究和应用开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation.</li>
<li>First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios.</li>
<li>Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models.</li>
<li>Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03178v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03178v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03127v1'></a></p>
<h2 id="unified-thinker-a-general-reasoning-modular-core-for-image-generation"><a href="https://arxiv.org/abs/2601.03127v1">Unified Thinker: A General Reasoning Modular Core for Image Generation</a></h2>
<p><strong>Authors:</strong> Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Unified Thinker: A General Reasoning Modular Core for Image Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Unified Thinker: A General Reasoning Modular Core for Image Generation
<strong>作者：</strong> Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
尽管高保真图像合成技术取得了显著进展，但当前的生成模型在遵循复杂、逻辑密集型指令方面仍存在困难，表现出“推理-执行”的鸿沟。闭源系统（如 Nano Banana）在推理驱动的图像生成方面展现出强大能力，而开源模型在这方面仍有差距。论文认为，弥合这一差距不仅需要更好的视觉生成器，还需要可执行的推理能力，即能够将高层意图分解为可验证的、具体的计划，并直接指导生成过程。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
*   <strong>Unified Thinker 框架：</strong> 提出了一种名为 Unified Thinker 的任务无关推理架构，作为通用的规划核心，可插入各种图像生成器和工作流。该框架将一个独立的“Thinker”（思考者）模块与“Generator”（生成器）模块解耦。
*   <strong>模块化设计：</strong> Thinker 负责指令理解和规划，Generator 负责像素合成。这种解耦允许独立升级推理能力，而无需重新训练整个生成模型，增强了模块化和可迁移性。
*   <strong>两阶段训练范式：</strong>
    *   <strong>结构化规划接口构建：</strong> 首先，通过构建一个名为 HieraReason-40K 的数据集（包含指令与结构化、可执行计划的配对），训练 Thinker 生成所需的规划格式和基本逻辑分解。
    *   <strong>执行导向的强化学习：</strong> 接着，采用两阶段强化学习（RL）来弥合推理与执行之间的差距。第一阶段（Reasoning-Oriented RL）优化 Thinker 的规划能力，使其生成对 Generator 更有效的计划。第二阶段（Generation-Oriented RL）则在 Generator 中引入随机性，以优化其执行计划的保真度。这种方法将 Thinker 的策略直接建立在像素级反馈之上，鼓励生成优化视觉正确性的计划。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>显著的性能提升：</strong> 在文本到图像生成和图像编辑任务上，Unified Thinker 显著提高了图像推理和生成质量，尤其在逻辑密集型指令遵循和约束满足方面表现出色。
*   <strong>跨模型可迁移性：</strong> 实验证明，解耦的 Thinker 模块能够学习可复用、可执行的推理模式，并能跨不同的生成器模型（如 Qwen-Image-Edit 和 BAGEL）进行迁移，验证了其通用性。
*   <strong>缩小与闭源模型的差距：</strong> 在 WiseBench 等基准测试中，Unified Thinker 显著缩小了与闭源模型（如 GPT-40）在推理能力上的差距。
*   <strong>平衡多任务能力：</strong> 尽管在某些低级编辑任务上可能存在轻微的性能权衡，但联合微调和两阶段 RL 训练能够有效缓解这种不匹配，并稳定多任务行为，最终在所有基准测试中取得一致的提升。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>对中间表示和训练数据的依赖：</strong> 方法的性能依赖于中间表示的质量、训练数据的覆盖范围以及 RL 中使用的奖励信号，这些都可能引入偏差并限制泛化能力。
*   <strong>执行的挑战：</strong> 尽管 Thinker 被设计为与生成器无关，但其可执行性并非完全独立于不同的生成器后端，尤其是在处理精细的几何变化、严格的局部性或精确的文本渲染等困难编辑任务时。
*   <strong>推理延迟和计算成本：</strong> 额外的规划阶段会增加推理延迟和计算成本，与直接提示单个生成器相比。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文并未明确列出未来研究方向，但基于其工作，可以推断以下潜在方向：
*   <strong>更强大的中间表示：</strong> 研究更丰富、更具表达力的中间表示，以更好地捕捉复杂视觉概念和逻辑关系。
*   <strong>更精细的推理-执行对齐：</strong> 探索更先进的对齐技术，以进一步缩小推理与像素级执行之间的差距，尤其是在处理更具挑战性的编辑任务时。
*   <strong>更广泛的生成器支持：</strong> 验证和扩展 Unified Thinker 在更多类型和架构的图像生成模型上的适用性。
*   <strong>实时推理优化：</strong> 探索减少推理延迟和计算成本的方法，以使其更适用于实时应用。
*   <strong>多模态推理的泛化：</strong> 将这种解耦的推理-生成框架扩展到其他多模态任务，如视频生成或3D内容创作。</p>
<p><strong>论文的独特性与重要性：</strong>
Unified Thinker 的核心贡献在于其提出的 <strong>解耦的“思考-执行”框架</strong> 和 <strong>两阶段的执行导向强化学习训练范式</strong>。它不仅解决了当前生成模型在逻辑推理方面的短板，而且通过模块化设计实现了推理能力的 <strong>高度可迁移性</strong>，这对于构建更智能、更通用的图像生成系统具有重要意义。该方法通过将复杂的推理过程显式地结构化并与生成器紧密对齐，为实现更可靠、更可控的图像生成开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03127v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03127v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.03117v1'></a></p>
<h2 id="transformers-self-organize-like-newborn-visual-systems-when-trained-in-prenatal-worlds"><a href="https://arxiv.org/abs/2601.03117v1">Transformers self-organize like newborn visual systems when trained in prenatal worlds</a></h2>
<p><strong>Authors:</strong> Lalit Pandey, Samantha M. W. Wood, Justin N. Wood</p>
<p><strong>Published:</strong> 2026-01-06</p>
<p><strong>Categories:</strong> q-bio.NC, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Do transformers learn like brains? A key challenge in addressing this question is that transformers and brains are trained on fundamentally different data. Brains are initially "trained" on prenatal sensory experiences (e.g., retinal waves), whereas transformers are typically trained on large datasets that are not biologically plausible. We reasoned that if transformers learn like brains, then they should develop the same structure as newborn brains when exposed to the same prenatal data. To test this prediction, we simulated prenatal visual input using a retinal wave generator. Then, using self-supervised temporal learning, we trained transformers to adapt to those retinal waves. During training, the transformers spontaneously developed the same structure as newborn visual systems: (1) early layers became sensitive to edges, (2) later layers became sensitive to shapes, and (3) the models developed larger receptive fields across layers. The organization of newborn visual systems emerges spontaneously when transformers adapt to a prenatal visual world. This developmental convergence suggests that brains and transformers learn in common ways and follow the same general fitting principles.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Transformers self-organize like newborn visual systems when trained in prenatal worlds”的全面中文摘要，重点关注其研究问题、方法创新、主要结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Transformers self-organize like newborn visual systems when trained in prenatal worlds</p>
<p><strong>作者：</strong> Lalit Pandey, Samantha M. W. Wood, Justin N. Wood</p>
<p><strong>摘要：</strong></p>
<p>这篇论文探讨了一个核心问题：<strong>人工智能中的Transformer模型是否像大脑一样学习？</strong> 论文指出，当前Transformer模型与大脑在学习数据上存在根本差异。大脑在出生前就通过产前感官体验（如视网膜波）进行“训练”，而Transformer模型通常使用不符合生物学原理的大型数据集进行训练。为了解决这一挑战，研究人员提出，如果Transformer模型真的像大脑一样学习，那么当它们暴露于与新生儿大脑相同的产前数据时，它们应该发展出相似的结构。</p>
<p><strong>关键创新与方法贡献：</strong></p>
<ol>
<li><strong>模拟产前视觉输入：</strong> 论文的核心创新在于使用<strong>视网膜波发生器</strong>来模拟新生儿大脑在出生前的视觉输入。这是为了提供一个生物学上更具可信度的数据集，用于训练Transformer模型。</li>
<li><strong>自监督时间学习：</strong> 研究人员采用了<strong>自监督时间学习</strong>（self-supervised temporal learning）的训练目标。具体来说，他们使用了一种名为<strong>ViT-CoT（Vision Transformer with Contrastive Learning through Time）</strong>的模型，该模型通过将同一时间窗口内的图像嵌入拉近，同时将不同时间窗口的图像嵌入推开，来学习时空信息。</li>
<li><strong>无先验硬编码：</strong> 与传统的卷积神经网络（CNNs）不同，Transformer模型被设计为<strong>通用学习者，不包含硬编码的领域特定先验</strong>。这使得它们能够更清晰地揭示学习经验在塑造视觉系统结构中的作用。</li>
<li><strong>多维度结构评估：</strong> 论文通过三种关键指标来评估模型结构是否与新生儿视觉系统相似：<ul>
<li><strong>早期层的边缘敏感性（Edge Sensitivity）：</strong> 模拟初级视觉皮层（V1）的特性。</li>
<li><strong>后期层的形状敏感性（Shape Sensitivity）：</strong> 模拟腹内侧颞叶皮层（inferior temporal cortex）的特性。</li>
<li><strong>跨层感受野大小的增长（Larger Receptive Fields Across Layers）：</strong> 模拟视觉系统层级化和视网膜拓扑结构的特征。</li>
</ul>
</li>
</ol>
<p><strong>主要结果及其意义：</strong></p>
<p>研究的主要结果表明，当Transformer模型（ViT-CoT）在模拟的产前视网膜波数据上进行训练时，它们<strong>自发地发展出了与新生儿视觉系统相似的三种结构特征</strong>：</p>
<ol>
<li><strong>早期层对边缘敏感：</strong> 模型在早期层学会了检测定向边缘。</li>
<li><strong>后期层对形状敏感：</strong> 模型在后期层学会了区分和识别形状。</li>
<li><strong>跨层感受野逐渐增大：</strong> 模型展现出层级化的结构，感受野在深层网络中变得更大。</li>
</ol>
<p>这些结果具有重要意义：</p>
<ul>
<li><strong>支持选择论（Selectional Theories）：</strong> 这一发现有力地支持了选择论的观点，即视觉系统的结构很大程度上是<strong>通过适应环境的产前经验而形成的</strong>，而无需基因的直接指令。</li>
<li><strong>揭示通用学习原则：</strong> 这表明大脑和Transformer模型在学习方式上存在<strong>共同的原则</strong>，即通过<strong>通用的时空拟合（space-time fitting）原理</strong>来适应环境。</li>
<li><strong>为AI和神经科学提供新视角：</strong> Transformer模型可以作为<strong>产前和产后大脑发育的基线模型</strong>，为理解视觉系统结构提供计算模型，并为AI工程提供新的思路。</li>
</ul>
<p><strong>论文提及的局限性：</strong></p>
<ol>
<li><strong>仅测试了三个关键特征：</strong> 研究仅关注了新生儿视觉系统的三个主要结构特征，未来需要探索其他特征是否也能自发形成。</li>
<li><strong>学习目标的多样性：</strong> 研究使用了特定的自监督时间学习目标，未来需要探索其他学习目标是否也能产生类似的组织结构。</li>
<li><strong>产前经验的单一性：</strong> 研究仅模拟了视觉领域的产前经验，而大脑的发育还受到听觉、本体感觉和触觉等多种感官输入的影响。</li>
</ol>
<p><strong>潜在的未来研究方向：</strong></p>
<ol>
<li><strong>探索其他结构特征：</strong> 研究其他新生儿视觉系统的特征，看它们是否也能在类似条件下自发形成。</li>
<li><strong>评估其他学习目标：</strong> 测试不同的无监督时间学习目标，以了解其对模型结构形成的影响。</li>
<li><strong>模拟多模态产前经验：</strong> 扩展研究范围，模拟听觉、本体感觉和触觉等其他领域的产前输入，以更全面地理解大脑发育。</li>
<li><strong>弥合人机学习鸿沟：</strong> 通过赋予Transformer模型产前发育阶段，使其更像大脑，从而缩小人类与机器之间的学习差距。</li>
</ol>
<p>总而言之，这篇论文通过创新的实验设计，证明了Transformer模型在模拟的产前视觉环境中，能够自发地发展出与新生儿视觉系统相似的结构。这一发现不仅为理解大脑发育提供了新的计算视角，也为构建更具生物学合理性的AI模型指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We reasoned that if transformers learn like brains, then they should develop the same structure as newborn brains when exposed to the same prenatal data.</li>
<li>During training, the transformers spontaneously developed the same structure as newborn visual systems: (1) early layers became sensitive to edges, (2) later layers became sensitive to shapes, and (3) the models developed larger receptive fields across layers.</li>
<li>The organization of newborn visual systems emerges spontaneously when transformers adapt to a prenatal visual world.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.03117v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.03117v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-07 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
