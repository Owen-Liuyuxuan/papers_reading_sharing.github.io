<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-04 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-03/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-05/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-04">Arxiv Computer Vision Papers - 2025-12-04</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#active-visual-perception-opportunities-and-challenges" class="nav-link">Active Visual Perception: Opportunities and Challenges</a>
                </li>
                <li class="nav-item">
                    <a href="#what-is-the-best-3d-scene-representation-for-robotics-from-geometric-to-foundation-models" class="nav-link">What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#unique-lives-shared-world-learning-from-single-life-videos" class="nav-link">Unique Lives, Shared World: Learning from Single-Life Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#radiance-meshes-for-volumetric-reconstruction" class="nav-link">Radiance Meshes for Volumetric Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#spacetools-tool-augmented-spatial-reasoning-via-double-interactive-rl" class="nav-link">SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</a>
                </li>
                <li class="nav-item">
                    <a href="#relic-interactive-video-world-model-with-long-horizon-memory" class="nav-link">RELIC: Interactive Video World Model with Long-Horizon Memory</a>
                </li>
                <li class="nav-item">
                    <a href="#jina-vlm-small-multilingual-vision-language-model" class="nav-link">Jina-VLM: Small Multilingual Vision Language Model</a>
                </li>
                <li class="nav-item">
                    <a href="#psa-pyramid-sparse-attention-for-efficient-video-understanding-and-generation" class="nav-link">PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#c3g-learning-compact-3d-representations-with-2k-gaussians" class="nav-link">C3G: Learning Compact 3D Representations with 2K Gaussians</a>
                </li>
                <li class="nav-item">
                    <a href="#emergent-outlier-view-rejection-in-visual-geometry-grounded-transformers" class="nav-link">Emergent Outlier View Rejection in Visual Geometry Grounded Transformers</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-04">Arxiv Computer Vision Papers - 2025-12-04</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月3日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月3日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>日期：</strong> 2025年12月3日</p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期论文集聚焦于<strong>三维场景理解与重建</strong>、<strong>长时序视频理解与生成</strong>以及<strong>高效多模态模型</strong>等关键领域。值得注意的是，<strong>基础模型（Foundation Models）</strong>在机器人和三维表示中的应用日益凸显，同时，<strong>交互式学习</strong>和<strong>紧凑型表示</strong>是提升模型效率和泛化能力的重要方向。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>三维场景表示的革新：</strong><ul>
<li><strong>"What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models"</strong> 探讨了从传统几何表示到新兴基础模型在机器人领域的三维场景表示的优劣，为机器人感知提供了重要的理论指导。</li>
<li><strong>"Radiance Meshes for Volumetric Reconstruction"</strong> 和 <strong>"C3G: Learning Compact 3D Representations with 2K Gaussians"</strong> 分别提出了新颖的体素重建方法和高效的紧凑型三维表示，有望在降低计算成本的同时提升重建质量。</li>
</ul>
</li>
<li><strong>长时序视频理解与交互：</strong><ul>
<li><strong>"RELIC: Interactive Video World Model with Long-Horizon Memory"</strong> 引入了一个具有长时域记忆的交互式视频世界模型，为理解和生成复杂视频序列提供了新的思路。</li>
<li><strong>"Unique Lives, Shared World: Learning from Single-Life Videos"</strong> 探索了从单一生长视频中学习通用世界模型，展现了从特定数据中提取普适性知识的潜力。</li>
</ul>
</li>
<li><strong>高效多模态模型：</strong><ul>
<li><strong>"Jina-VLM: Small Multilingual Vision Language Model"</strong> 提出了一种小型多语言视觉语言模型，在保持模型尺寸的同时实现了跨语言的视觉语言理解能力，对于资源受限的应用场景具有重要意义。</li>
<li><strong>"PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation"</strong> 通过金字塔稀疏注意力机制，显著提升了视频理解和生成任务的效率。</li>
</ul>
</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>基础模型在三维视觉中的应用：</strong> 论文表明，基础模型正逐渐成为三维场景表示和机器人感知的重要工具。</li>
<li><strong>交互式学习与主动感知：</strong> <strong>"SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL"</strong> 和 <strong>"Active Visual Perception: Opportunities and Challenges"</strong> 强调了通过交互式学习和主动感知来提升模型在复杂环境中的推理和适应能力。</li>
<li><strong>紧凑型与高效表示：</strong> 发展能够以更少参数和计算量实现高质量三维重建和视频理解的模型是当前的研究热点。</li>
<li><strong>鲁棒性与异常检测：</strong> <strong>"Emergent Outlier View Rejection in Visual Geometry Grounded Transformers"</strong> 揭示了在视觉几何Transformer中涌现出的异常视图拒绝能力，为提升模型在不完整或噪声数据下的鲁棒性提供了启示。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>为了快速了解该领域最重要的发展，建议重点阅读以下论文：</p>
<ol>
<li><strong>"What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models"</strong>: 对于理解三维表示的最新进展及其在机器人领域的应用至关重要。</li>
<li><strong>"RELIC: Interactive Video World Model with Long-Horizon Memory"</strong>: 代表了长时序视频理解和交互式学习的前沿。</li>
<li><strong>"Jina-VLM: Small Multilingual Vision Language Model"</strong>: 对于关注高效、多语言视觉语言模型的研究人员具有重要价值。</li>
<li><strong>"Radiance Meshes for Volumetric Reconstruction"</strong> 和 <strong>"C3G: Learning Compact 3D Representations with 2K Gaussians"</strong>: 对于对三维重建和表示技术感兴趣的研究人员，这两篇论文提供了创新的解决方案。</li>
</ol>
<hr />
<p>希望这份摘要能帮助您快速掌握近期 Arxiv 计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.03687v1">Active Visual Perception: Opportunities and Challenges</a></li>
<li><a href="#2512.03422v1">What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</a></li>
<li><a href="#2512.04085v1">Unique Lives, Shared World: Learning from Single-Life Videos</a></li>
<li><a href="#2512.04076v1">Radiance Meshes for Volumetric Reconstruction</a></li>
<li><a href="#2512.04069v1">SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</a></li>
<li><a href="#2512.04040v1">RELIC: Interactive Video World Model with Long-Horizon Memory</a></li>
<li><a href="#2512.04032v1">Jina-VLM: Small Multilingual Vision Language Model</a></li>
<li><a href="#2512.04025v1">PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</a></li>
<li><a href="#2512.04021v1">C3G: Learning Compact 3D Representations with 2K Gaussians</a></li>
<li><a href="#2512.04012v1">Emergent Outlier View Rejection in Visual Geometry Grounded Transformers</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.03687v1'></a></p>
<h2 id="active-visual-perception-opportunities-and-challenges"><a href="https://arxiv.org/abs/2512.03687v1">Active Visual Perception: Opportunities and Challenges</a></h2>
<p><strong>Authors:</strong> Yian Li, Xiaoyu Guo, Hao Zhang, Shuiwang Li, Xiaowei Dai</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“主动视觉感知：机遇与挑战”的论文的全面中文摘要，重点关注其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> 主动视觉感知：机遇与挑战 (Active Visual Perception: Opportunities and Challenges)
<strong>作者：</strong> Yian Li, Xiaoyu Guo, Hao Zhang, Shuiwang Li, Xiaowei Dai</p>
<p><strong>摘要：</strong></p>
<p>这篇论文深入探讨了<strong>主动视觉感知（Active Visual Perception）</strong>这一关键技术在人工智能和机器人领域的重要性、应用前景以及面临的挑战。论文的核心研究问题在于：<strong>如何使智能系统能够通过动态地与环境互动来优化其感知过程，从而在复杂多变的环境中做出更准确、更及时的决策。</strong></p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>论文主要关注的是如何克服传统<strong>被动视觉感知</strong>在处理复杂、动态和不确定环境时的局限性。被动系统仅依赖于静态数据，难以捕捉全局信息或应对突发情况。因此，研究的核心问题是如何设计和实现能够<strong>主动调整其感知策略、传感器姿态、注意力焦点以及与环境交互方式</strong>的系统，以获取更具信息量的数据，并最终提升决策的效率和准确性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<p>虽然本文是一篇综述性论文，但其主要贡献在于：</p>
<ul>
<li><strong>系统性地梳理了主动视觉感知的概念和优势：</strong> 论文清晰地定义了主动视觉感知，并将其与被动视觉感知进行了对比，强调了其在动态环境下的适应性和信息获取优化能力。</li>
<li><strong>全面概述了主动视觉感知的应用领域：</strong> 论文详细介绍了主动视觉感知在<strong>机器人与自主系统、人机交互、监控与安全、环境监测与保护</strong>等多个领域的广泛应用，并提供了具体的实例，展示了其巨大的潜力。</li>
<li><strong>深入分析了主动视觉感知面临的关键挑战：</strong> 论文系统地总结了当前主动视觉感知技术在<strong>实时决策、传感器集成与协调、计算开销、不确定性与鲁棒性、安全与伦理考量</strong>等方面存在的瓶颈。</li>
<li><strong>展望了主动视觉感知的未来发展方向：</strong> 论文提出了<strong>多模态传感器融合、协作系统、先进机器学习与人工智能技术、传感器技术改进以及伦理与安全标准</strong>等关键的未来研究方向，为该领域的进一步发展指明了道路。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>论文的主要“结果”体现在其对主动视觉感知领域现状的全面梳理和深刻洞察。其意义在于：</p>
<ul>
<li><strong>为主动视觉感知领域的研究者和从业者提供了一个清晰的路线图：</strong> 通过系统性的分析，论文帮助读者理解该技术的当前状态、潜在价值以及需要克服的障碍。</li>
<li><strong>强调了主动视觉感知在构建更智能、更自主的系统中的核心作用：</strong> 论文展示了主动视觉感知如何赋能机器人、自动驾驶汽车和人机交互系统，使其能够更有效地理解和响应真实世界。</li>
<li><strong>为解决现实世界中的复杂问题提供了技术视角：</strong> 例如，在自动驾驶中，主动感知能够提高对行人和障碍物的检测精度；在人机交互中，能够实现更自然、更直观的交互方式。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<p>论文中明确指出的主要局限性（即挑战）包括：</p>
<ul>
<li><strong>实时决策的困难：</strong> 在动态环境中，系统需要快速处理大量数据并做出准确决策，这对其算法的效率和鲁棒性提出了极高要求。</li>
<li><strong>传感器集成与协调的复杂性：</strong> 融合来自不同传感器（如摄像头、LiDAR、IMU）的数据，并处理其差异和潜在故障，是一个巨大的技术难题。</li>
<li><strong>计算开销巨大：</strong> 主动感知需要进行复杂的传感器调整、数据融合和决策过程，这需要强大的计算资源，尤其是在资源受限的嵌入式系统中。</li>
<li><strong>不确定性和鲁棒性问题：</strong> 真实世界环境充满噪声、光照变化、物体遮挡等不确定性，系统需要具备高度的适应性和鲁棒性才能稳定工作。</li>
<li><strong>安全与伦理考量：</strong> 在自动驾驶、医疗机器人等安全关键应用中，系统的可靠性和可预测性至关重要。同时，监控系统中的隐私侵犯和数据滥用也是亟待解决的伦理问题。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>论文提出了以下几个关键的未来研究方向：</p>
<ul>
<li><strong>先进机器学习与人工智能（ML/AI）：</strong> 利用深度学习、强化学习和无监督学习来提升系统的感知、理解和决策能力，使其能够更好地适应环境变化。</li>
<li><strong>改进的传感器技术：</strong> 发展更小型、更精确、更节能的传感器，并实现更高效的多模态传感器融合，以降低计算负担并提高数据质量。</li>
<li><strong>协作系统：</strong> 研究多智能体（如机器人、无人车）之间的协作感知和决策，以实现更广泛的覆盖和更强的任务执行能力。</li>
<li><strong>伦理与安全标准：</strong> 建立明确的伦理准则和安全标准，确保主动视觉感知系统的部署是透明、负责任且尊重隐私的，并具备完善的故障安全机制。</li>
<li><strong>可解释性与透明度：</strong> 提高 AI 驱动决策的可解释性，增强系统的透明度和可信度，尤其是在高风险应用中。</li>
</ul>
<p>总而言之，这篇论文为主动视觉感知领域的研究提供了一个全面而深入的视角，不仅阐述了其巨大的机遇和应用前景，也清晰地指出了当前面临的严峻挑战，并为未来的研究和发展提供了宝贵的指导。它强调了主动视觉感知是构建更智能、更适应性强的未来人工智能系统的关键技术之一。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs.</li>
<li>This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03687v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03687v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03422v1'></a></p>
<h2 id="what-is-the-best-3d-scene-representation-for-robotics-from-geometric-to-foundation-models"><a href="https://arxiv.org/abs/2512.03422v1">What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</a></h2>
<p><strong>Authors:</strong> Tianchen Deng, Yue Pan, Shenghai Yuan, Dong Li, Chen Wang, Mingrui Li, Long Chen, Lihua Xie, Danwei Wang, Jingchuan Wang, Javier Civera, Hesheng Wang, Weidong Chen</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models (机器人领域的最佳3D场景表示是什么？从几何到基础模型)</p>
<p><strong>作者：</strong> Tianchen Deng, Yue Pan, Shenghai Yuan, Dong Li, Chen Wang, Mingrui Li, Long Chen, Lihua Xie, Danwei Wang, Jingchuan Wang, Javier Civera, Hesheng Wang, Weidong Chen</p>
<p><strong>摘要：</strong></p>
<p>这篇论文全面回顾了机器人领域中现有的3D场景表示方法，涵盖了从传统的点云、体素、有符号距离场（SDF）和场景图，到新兴的神经表示方法，如神经辐射场（NeRF）、3D高斯溅射（3DGS）以及日益重要的基础模型（Foundation Models）。论文的核心研究问题是：<strong>“机器人领域的最佳3D场景表示是什么？”</strong></p>
<p><strong>主要贡献与方法：</strong></p>
<ol>
<li><strong>全面的分类与比较：</strong> 论文将3D场景表示方法根据其在机器人核心模块（感知、建图、定位、导航、操作）中的应用进行了详细分类和比较。通过图表（如图1和图4所示）展示了不同表示方法的演进时间线、优缺点以及在不同维度（如数据形式、连续性、内存效率、光照真实感、灵活性、几何表示能力）上的权衡。</li>
<li><strong>深入的模块化分析：</strong> 论文逐一分析了各种场景表示方法在机器人各个核心模块中的适用性，探讨了它们在实现精确几何表示、高保真渲染、语义理解以及与下游任务（如导航、避障、操作）的集成能力方面的优势和劣势。</li>
<li><strong>对新兴技术的关注：</strong> 论文重点关注了NeRF和3DGS等神经表示方法，分析了它们在提高场景表示的连续性、密度和光照真实感方面的潜力。同时，论文也深入探讨了基础模型（Foundation Models）在机器人领域的应用前景，特别是它们在整合高层语义特征和语言先验方面的能力，以及可能成为统一解决方案的潜力。</li>
<li><strong>未来趋势的探讨：</strong> 论文不仅回顾了过去和现在的技术，还对3D场景表示的未来发展趋势进行了展望，特别是基础模型如何可能取代现有方法，以及在完全实现这些模型时面临的挑战。</li>
</ol>
<p><strong>主要结果与意义：</strong></p>
<ul>
<li><strong>传统方法局限性：</strong> 论文指出，传统的稀疏表示方法（如点云、体素）在当前的SLAM和定位系统中仍占主导地位，但它们在生成密集和连续的3D表示方面存在不足，难以支持复杂的具身智能任务。</li>
<li><strong>神经表示的优势：</strong> NeRF和3DGS等神经表示方法在实现高保真渲染、连续几何表示以及整合语义信息方面展现出巨大潜力，为机器人提供了更丰富、更精细的环境理解能力。</li>
<li><strong>基础模型的颠覆性潜力：</strong> 论文强调，基础模型通过整合大规模数据和先进的Transformer架构，有望实现更强的泛化能力和零样本学习能力，为机器人提供更全面的3D场景理解和具身智能，甚至可能成为未来机器人应用的统一解决方案。</li>
<li><strong>为研究者提供资源：</strong> 论文旨在为机器人领域的新老研究者提供一个全面、最新的资源，帮助他们理解3D场景表示的现状、挑战和未来方向。作者还发布了一个GitHub项目，持续更新相关文献和技术。</li>
</ul>
<p><strong>论文提到的局限性：</strong></p>
<ul>
<li><strong>计算成本：</strong> 许多先进的神经表示方法（如NeRF）在计算上仍然非常昂贵，限制了其在实时机器人应用中的部署。</li>
<li><strong>数据稀缺性：</strong> 尽管基础模型在自然语言处理领域取得了巨大成功，但在机器人领域，特定于机器人任务的数据仍然稀缺，这阻碍了这些模型的充分发展和泛化。</li>
<li><strong>泛化能力与鲁棒性：</strong> 尽管基础模型展现出强大的潜力，但在处理复杂、动态或未知环境时，其泛化能力和鲁棒性仍需进一步提升。</li>
<li><strong>几何精度：</strong> 尽管神经表示在外观上表现出色，但在几何精度方面，尤其是在需要精确操作的任务中，可能仍不如传统方法。</li>
</ul>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>通用性与模块化：</strong> 探索如何构建更通用的基础模型，使其能够无缝集成到机器人系统的各个模块中，并实现端到端的智能。</li>
<li><strong>数据效率与生成模型：</strong> 利用生成模型和数据增强技术，克服机器人领域数据稀缺的挑战，提高模型的泛化能力。</li>
<li><strong>实时性能与硬件协同设计：</strong> 进一步优化算法，并探索硬件与算法协同设计，以实现高效的推理和实时部署。</li>
<li><strong>人机交互与语言理解：</strong> 深入研究如何将语言理解能力与3D场景表示相结合，实现更自然、更智能的人机交互和任务规划。</li>
<li><strong>可解释性与鲁棒性：</strong> 提高神经表示和基础模型的透明度和可解释性，增强其在复杂和不可预测环境中的鲁棒性。</li>
<li><strong>跨模态融合：</strong> 进一步融合视觉、触觉、听觉等多种感知模态，构建更全面的3D场景理解能力。</li>
</ul>
<p>总而言之，这篇论文为理解和推动机器人领域3D场景表示的发展提供了宝贵的框架和深刻的见解，特别强调了神经表示和基础模型在未来机器人智能化中的关键作用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics.</li>
<li>We have published an open-source project on GitHub and will continue to add new works and technologies to this project.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03422v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03422v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04085v1'></a></p>
<h2 id="unique-lives-shared-world-learning-from-single-life-videos"><a href="https://arxiv.org/abs/2512.04085v1">Unique Lives, Shared World: Learning from Single-Life Videos</a></h2>
<p><strong>Authors:</strong> Tengda Han, Sayna Ebrahimi, Dilara Gokay, Li Yang Ku, Maks Ovsjanikov, Iva Babukova, Daniel Zoran, Viorica Patraucean, Joao Carreira, Andrew Zisserman, Dima Damen</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Unique Lives, Shared World: Learning from Single-Life Videos”论文的全面摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Unique Lives, Shared World: Learning from Single-Life Videos</p>
<p><strong>作者：</strong> Tengda Han, Sayna Ebrahimi, Dilara Gokay, Li Yang Ku, Maks Ovsjanikov, Iva Babukova, Daniel Zoran, Viorica Patraucean, Joao Carreira, Andrew Zisserman, Dima Damen</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要研究问题/研究目标：</strong></p>
<p>该论文的核心研究问题是：<strong>能否仅通过一个人的“单一生”（single-life）的自我中心（egocentric）视频数据，训练出具有良好几何理解能力且能泛化到下游任务的视觉模型？</strong> 作者们挑战了当前视觉表示学习依赖于海量、多样化数据集的范式，提出了“单一生学习范式”，旨在探索个体经验的丰富性是否足以作为一种强大的自监督学习信号，来学习通用的视觉表示。</p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<ul>
<li><strong>“单一生学习范式” (Single-Life Learning Paradigm)：</strong> 这是论文的核心创新。作者们提出，不将来自不同个体的视频数据混合训练一个统一模型，而是为每个个体（“生命”）的数据训练一个独立的模型。这种范式强调了利用个体独特视角和经验的潜力。</li>
<li><strong>自监督几何表示学习：</strong> 利用单一生视频中自然存在的、因个体移动而产生的多视角信息，作者们采用了自监督的跨视图完成（Cross-View Completion, CroCo）方法来学习几何表示。这种方法不需要额外的标注数据。</li>
<li><strong>新的跨注意力对齐度量 (Correspondence Alignment Score, CAS)：</strong> 为了量化不同“单一生”模型之间学习到的几何表示的相似性，作者们引入了一种新颖的、基于跨注意力机制的度量方法。CAS能够评估模型在图像块（patch）层面的功能性对齐，克服了现有方法的局限性。</li>
<li><strong>多样的单一生数据集构建与分析：</strong> 作者们收集并分析了20个“单一生”数据集，涵盖了室内和室外环境，其中包含长达38小时的视频。他们还对这些数据集的属性（如相机姿态、深度、光流、亮度、物体分布等）进行了可视化分析，为理解个体经验的特性提供了基础。</li>
<li><strong>创新的配对策略：</strong> 为了更好地利用单一生视频进行CroCo训练，作者们探索了三种配对策略：时间邻近配对（temporal pairing）、空间重叠配对（spatial pairing）以及两者的结合。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>高度对齐的几何理解：</strong> 实验证明，即使是独立训练在不同“生命”数据上的模型，也能发展出高度一致且对齐的几何理解。这为论文提出的“共享世界假说”（Shared World Hypothesis）提供了直接证据，即世界固有的结构属性使得不同个体的视觉经验能够收敛到相似的几何表示。</li>
<li><strong>强大的泛化能力：</strong> 单一生模型学习到的几何表示能够有效地迁移到未见过的环境和下游任务，如单目深度估计和零样本对应匹配。这表明个体经验蕴含了丰富的、可泛化的几何先验知识。</li>
<li><strong>数据时长与性能的关系：</strong> 研究发现，大约30分钟到1小时的单一生数据足以使模型产生显著的几何对齐。随着数据时长的增加，单一生模型的性能能够稳健地提升，并且在约30小时的数据量下，其性能可以与同等时长但多样化的网络数据（如K400）相媲美，甚至在某些任务上超越。</li>
<li><strong>“非生命”视频的对照实验：</strong> 使用“非生命”视频（如屏幕录制、固定摄像头视角）进行训练的对照组模型，与“单一生”模型相比，在与CroCo基线模型的对齐度上表现极差，证明了自我中心视角、个体运动和交互的独特性对于学习共享世界至关重要。</li>
<li><strong>配对策略的有效性：</strong> 时间邻近配对策略被证明非常有效，而结合时间与空间配对策略能获得最佳性能，这与人类整合运动和视角变化来理解世界的直觉一致。</li>
</ul>
<p><strong>意义：</strong> 该研究的重要意义在于，它<strong>证明了仅凭个体丰富但相对有限的视觉经验，就可以学习到强大的、通用的几何表示，并且这种表示的质量可以与大规模、多样化数据集相媲美。</strong> 这为未来在数据获取受限或隐私敏感的场景下进行视觉表示学习提供了新的思路，并可能改变我们对视觉学习数据需求的认知。</p>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>数据时长：</strong> 虽然30小时的数据量已能达到可观的性能，但与真正海量的数据集相比，仍有差距。论文也指出，更长时长的单一生数据可能进一步缩小与多样化数据基线的差距。</li>
<li><strong>特定架构的依赖性：</strong> 主要实验集中在CroCo架构上，虽然在附录中也展示了DINOv2的实验，但关于“单一生学习范式”是否能普遍适用于所有自监督学习架构，仍需更广泛的探索。</li>
<li><strong>“非生命”视频的局限性：</strong> “非生命”视频的定义和选择可能存在一定的主观性，其对照实验结果虽然有力，但仍需进一步验证。</li>
<li><strong>对几何任务的侧重：</strong> 本研究主要关注几何表示的学习和评估，对于语义表示的学习和泛化能力，虽然在DINOv2实验中有所提及，但并非研究的重点。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>探索更广泛的自监督学习架构：</strong> 将“单一生学习范式”应用于更多不同类型的自监督学习方法（如对比学习、掩码自编码器等），以验证其普适性。</li>
<li><strong>学习更丰富的语义表示：</strong> 扩展“单一生学习范式”的应用范围，探索其在学习通用语义表示方面的潜力，例如物体识别、场景理解等。</li>
<li><strong>研究个体经验的独特性与普适性的权衡：</strong> 深入分析不同“生命”数据中，哪些是高度个体化的，哪些是共享世界的共性，以及如何更好地平衡这两者。</li>
<li><strong>大规模单一生数据的潜力：</strong> 探索收集和利用更长、更丰富、更多样化的单一生数据，以进一步提升模型性能，并可能实现更高级的视觉理解能力。</li>
<li><strong>跨生命迁移与个性化：</strong> 研究如何利用一个生命学习到的模型，来加速或改进另一个生命模型的学习，或者如何将通用模型进行个性化微调。</li>
</ul>
<hr />
<p>总而言之，这篇论文提出了一种新颖的“单一生学习范式”，通过利用个体独特的自我中心视频数据，在自监督的条件下学习到了高度对齐且泛化能力强的几何表示。研究结果有力地支持了“共享世界假说”，并为未来视觉表示学习的研究开辟了新的方向，尤其是在数据效率和个体经验利用方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual.</li>
<li>We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models.</li>
<li>Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments.</li>
<li>Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04085v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04085v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04076v1'></a></p>
<h2 id="radiance-meshes-for-volumetric-reconstruction"><a href="https://arxiv.org/abs/2512.04076v1">Radiance Meshes for Volumetric Reconstruction</a></h2>
<p><strong>Authors:</strong> Alexander Mai, Trevor Hedstrom, George Kopanas, Janne Kontkanen, Falko Kuester, Jonathan T. Barron</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization. Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware. As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing. We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms. Optimizing the positions of Delaunay vertices introduces topological discontinuities (edge flips). To solve this, we use a Zip-NeRF-style backbone which allows us to express a smoothly varying field even when the topology changes. Our rendering method exactly evaluates the volume rendering equation and enables high quality, real-time view synthesis on standard consumer hardware. Our tetrahedral meshes also lend themselves to a variety of exciting applications including fisheye lens distortion, physics-based simulation, editing, and mesh extraction.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析：</p>
<p><strong>论文标题：</strong> Radiance Meshes for Volumetric Reconstruction
<strong>作者：</strong> Alexander Mai, Trevor Hedstrom, George Kopanas, Janne Kontkanen, Falko Kuester, Jonathan T. Barron
<strong>分类：</strong> cs.GR, cs.CV
<strong>发表日期：</strong> 2025-12-03</p>
<hr />
<h3 id="_1">论文分析</h3>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）：</strong></p>
<p>本研究提出了一种名为“辐射网格”（Radiance Meshes）的新型体渲染技术，它利用恒定密度四面体单元和 Delaunay 四面体剖分来表示辐射场。该方法能够通过硬件原生支持的简单三角形实现精确且快速的体积渲染，并引入了一种创新的光栅化方法，在同等条件下实现了比现有辐射场表示更快的渲染速度。通过结合 Zip-NeRF 风格的骨干网络，该模型能够处理拓扑变化带来的不连续性，从而在标准消费级硬件上实现高质量、实时的视图合成。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>基于 Delaunay 四面体剖分的辐射网格表示：</strong> 这是最核心的创新。论文将辐射场表示为由 Delaunay 四面体剖分产生的恒定密度四面体单元。与 Voronoi 图不同，Delaunay 四面体剖分生成的是简单的三角形，这使得它们能够被现有硬件（GPU）原生支持，从而实现高效的渲染。</li>
<li><strong>硬件原生支持的精确体积渲染：</strong> 由于使用了简单的三角形作为基本单元，该方法能够直接利用现有光栅化和光线追踪硬件进行精确的体积渲染，避免了传统方法中可能存在的近似和采样误差。</li>
<li><strong>创新的光栅化渲染方法：</strong> 论文提出了一种新的光栅化方法，旨在实现比现有辐射场表示（如神经辐射场 NeRF 及其变体）更快的渲染速度，尤其是在同等原始数量和分辨率下。这对于实时应用至关重要。</li>
<li><strong>Zip-NeRF 风格的骨干网络处理拓扑变化：</strong> 在优化 Delaunay 四面体顶点位置时，可能会引入拓扑不连续性（如边翻转）。论文采用了类似 Zip-NeRF 的骨干网络来解决这个问题，使得即使在拓扑发生变化的情况下，模型也能表达出平滑变化的辐射场。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>加速体渲染和神经渲染：</strong> 该方法有望显著提升体渲染和神经渲染的效率，使其在实时应用中更具可行性。这可能推动 NeRF 等技术的普及和应用范围的扩大。</li>
<li><strong>降低硬件门槛：</strong> 通过充分利用现有硬件的优势，该技术可能降低对高性能计算资源的需求，使得高质量的 3D 重建和渲染在消费级硬件上成为可能。</li>
<li><strong>统一的表示和渲染框架：</strong> 将辐射场表示为基于网格的结构，并能直接利用硬件进行渲染，为体渲染提供了一个更统一、更高效的框架。</li>
<li><strong>推动新的应用：</strong> 论文中提到的“鱼眼镜头畸变、物理模拟、编辑和网格提取”等应用，表明该方法不仅限于视图合成，还可能为其他 3D 相关领域带来新的解决方案。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>计算机图形学：</strong> 实时渲染、游戏开发、虚拟现实（VR）和增强现实（AR）中的场景渲染。</li>
<li><strong>计算机视觉：</strong> 3D 重建、场景理解、动态场景的建模和渲染。</li>
<li><strong>机器人学：</strong> 机器人导航和感知中的环境建模。</li>
<li><strong>医学成像：</strong> 医学影像的体渲染和可视化。</li>
<li><strong>数字内容创作：</strong> 3D 内容的生成、编辑和可视化。</li>
<li><strong>物理模拟：</strong> 将辐射场与物理属性相结合进行模拟。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>Delaunay 四面体剖分的生成成本：</strong> 虽然 Delaunay 四面体剖分本身有成熟的算法，但在大规模或复杂场景下生成高质量的四面体网格可能仍然是一个计算密集型的过程。</li>
<li><strong>拓扑变化的处理：</strong> 虽然引入了 Zip-NeRF 风格的骨干网络来处理拓扑变化，但这种处理的鲁棒性、效率以及对最终渲染质量的影响程度仍需进一步验证。</li>
<li><strong>“等效数量的原语和分辨率”的假设：</strong> 摘要中提到“假设 an equivalent number of primitives and resolution”，这意味着在与其他方法进行速度比较时，需要仔细定义和控制这些参数，以确保公平性。实际应用中，如何最优地选择网格密度和分辨率以平衡质量和效率是一个挑战。</li>
<li><strong>对特定硬件的依赖性：</strong> 虽然强调了利用现有硬件，但其性能优势可能在不同硬件架构上有所差异。</li>
<li><strong>网格的质量和适应性：</strong> 恒定密度四面体单元可能在某些区域（如细节丰富的区域）需要非常精细的网格才能捕捉到足够的信息，这可能导致网格数量的爆炸式增长。如何自适应地生成网格以优化性能和质量是一个潜在的挑战。</li>
</ul>
<hr />
<p><strong>总结：</strong></p>
<p>这篇论文的核心吸引力在于它提供了一种<strong>将辐射场表示与硬件原生支持的几何结构（Delaunay 四面体）相结合</strong>的新范式。通过利用 Delaunay 四面体剖分产生的简单三角形，该方法有望实现比现有神经渲染技术<strong>更高效、更精确的体积渲染</strong>，并且能够<strong>在标准消费级硬件上实现实时视图合成</strong>。这种方法论上的突破，加上对拓扑变化的有效处理，使得该研究在计算机视觉和图形学领域具有重要的理论和应用价值，尤其是在推动神经渲染技术的普及和性能提升方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization.</li>
<li>We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04076v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04076v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04069v1'></a></p>
<h2 id="spacetools-tool-augmented-spatial-reasoning-via-double-interactive-rl"><a href="https://arxiv.org/abs/2512.04069v1">SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</a></h2>
<p><strong>Authors:</strong> Siyi Chen, Mikaela Angelina Uy, Chan Hee Song, Faisal Ladhak, Adithyavairavan Murali, Qing Qu, Stan Birchfield, Valts Blukis, Jonathan Tremblay</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SpaceTools: 通过双重交互式强化学习实现工具增强的空间推理</p>
<p><strong>作者：</strong> Siyi Chen, Mikaela Angelina Uy, Chan Hee Song, Faisal Ladhak, Adithyavairavan Murali, Qing Qu, Stan Birchfield, Valts Blukis, Jonathan Tremblay</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决视觉语言模型（VLMs）在理解和执行需要精确度量的空间推理任务时遇到的困难。尽管VLMs在定性视觉理解方面表现出色，但它们在需要精确空间关系、距离和姿态估计的任务上仍显不足，这限制了它们在机器人等具身应用中的集成。现有的方法要么依赖于手工设计的提示策略，要么采用固定的工具调用流程，这限制了模型发现最优工具使用模式的能力。虽然强化学习（RL）有望解决这一问题，但由于多工具推理中巨大的搜索空间，其应用仅限于单一视觉工具。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
作者提出了<strong>双重交互式强化学习（DIRL）</strong>，一个两阶段的训练框架，使VLMs能够通过交互式探索和反馈来协调使用多种工具。
*   <strong>教学阶段：</strong> 首先，通过交互式RL训练一个单一工具的专家模型，以建立基础的工具使用能力。然后，结合该专家模型的演示和使用所有工具的前沿模型的轨迹，生成一个教学数据集。最后，对基础模型进行监督微调（SFT），使其具备初步的工具使用行为。
*   <strong>探索阶段：</strong> 在此阶段，使用SFT初始化的模型，通过交互式RL在所有任务和所有可用工具上进行训练，从而进一步优化多工具协调能力。这种两阶段的IRL方法使得模型能够有效地学习复杂的工具链策略，克服了直接在大型多工具动作空间中进行RL探索的困难。
*   <strong>Toolshed 基础设施：</strong> 为了支持DIRL训练，作者开发了一个名为Toolshed的可扩展框架，用于部署和管理计算密集型的计算机视觉和机器人工具。Toolshed实现了工具的解耦执行、异步处理、资源隔离和弹性伸缩，从而实现了高效、可扩展的工具交互。</p>
<p><strong>3. 主要结果与意义：</strong>
作者提出的<strong>SpaceTools</strong>模型，通过工具增强的空间推理能力，在多个空间推理基准测试（如RoboSpatial-Home, BLINK, BOP-ASK）上取得了<strong>最先进的性能</strong>。更重要的是，SpaceTools展示了在真实世界中可靠的操纵能力，甚至能够将一个7自由度机器人作为工具使用。DIRL框架相比于传统的SFT（+12% on RoboSpatial）和RL（+16% on RoboSpatial）基线模型，在RoboSpatial基准上取得了显著的性能提升。SpaceTools能够动态地适应不同的任务，灵活地组合和调用各种工具（如指向、分割、深度估计、3D边界框拟合、抓取预测等），并学会了在工具失败时进行纠正和切换。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>抓取和姿态估计的准确性：</strong> 在杂乱和视觉复杂的场景中，抓取和姿态估计仍然是模型面临的挑战，这导致了较低的准确率。
*   <strong>工具协调和选择策略：</strong> 模型在选择工具和定位点时，有时会选择靠近边界的点，导致在真实世界操作中出现失败。
*   <strong>对精确几何推理和物理可行性的依赖：</strong> 模型的成功很大程度上依赖于精确的几何推理和对机器人操作物理可行性的理解。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>扩展应用范围：</strong> 将工具增强的空间推理应用于更长期的任务、更复杂的场景以及物理模拟环境。
*   <strong>增强方法灵活性：</strong> 探索更精细的工具输出（如视觉输出），以及更系统地改进模型从工具错误中恢复的能力。
*   <strong>改进RL方法：</strong> 研究更有效的奖励设计（如分步奖励）以提高大型多工具动作空间的学习效率。
*   <strong>优化Toolshed基础设施：</strong> 进一步提高Toolshed的效率和资源利用率，支持更大规模的训练和更复杂的工具工作流。
*   <strong>集成真实机器人反馈：</strong> 将真实机器人反馈集成到训练过程中，以提高模型的物理可行性。</p>
<p><strong>总结：</strong>
SpaceTools通过DIRL训练框架和Toolshed基础设施，成功地解决了VLMs在空间推理方面的挑战，实现了强大的工具协调能力和最先进的空间推理性能。该工作为VLMs通过学习工具协调来获取复杂空间推理能力提供了一种有效且可扩展的途径，并为具身智能在机器人和现实世界应用中的发展开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback.</li>
<li>Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04069v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04069v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04040v1'></a></p>
<h2 id="relic-interactive-video-world-model-with-long-horizon-memory"><a href="https://arxiv.org/abs/2512.04040v1">RELIC: Interactive Video World Model with Long-Horizon Memory</a></h2>
<p><strong>Authors:</strong> Yicong Hong, Yiqun Mei, Chongjian Ge, Yiran Xu, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, Kalyan Sunkavalli, Feng Liu, Zhengqi Li, Hao Tan</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RELIC: Interactive Video World Model with Long-Horizon Memory”的全面中文摘要：</p>
<p><strong>论文题目：</strong> RELIC: Interactive Video World Model with Long-Horizon Memory</p>
<p><strong>作者：</strong> Yicong Hong, Yiqun Mei, Chongjian Ge, Yiran Xu, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, Kalyan Sunkavalli, Feng Liu, Zhengqi Li, Hao Tan</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决构建一个真正交互式视频世界模型所面临的核心挑战：<strong>实时长时序视频生成（real-time long-horizon streaming）、一致的空间记忆（consistent spatial memory）和精确的用户控制（precise user control）</strong>。现有方法往往只能在其中一个方面取得进展，而同时实现这三者则非常困难，因为长时序记忆机制常常会损害实时性能。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了应对这些挑战，作者提出了 <strong>RELIC</strong>，一个统一的框架，实现了上述三个关键要素的协同工作。其核心创新包括：</p>
<ul>
<li><strong>长时序记忆的压缩表示：</strong> RELIC 使用高度压缩的历史潜在 token 来表示长时序记忆，这些 token 编码了相对动作和绝对相机位姿，并存储在 KV 缓存中。这种紧凑、相机感知的记忆结构支持隐式的 3D 一致性内容检索，并以最小的计算开销强制执行长时序连贯性。</li>
<li><strong>长时序教师模型微调与自强制蒸馏：</strong> 作者微调了一个双向教师视频模型，使其能够生成超过原始 5 秒训练时长的视频序列。然后，利用一种新颖的<strong>记忆高效自强制（memory-efficient self-forcing）范式</strong>，将该教师模型转化为因果学生生成器，实现了对长时序教师模型以及长时序学生自回溯（self-rollouts）的完整上下文蒸馏。</li>
<li><strong>重放反向传播（Replayed Back-Propagation）：</strong> 为了解决长视频蒸馏中内存消耗过大的问题，RELIC 引入了重放反向传播技术。该技术仅存储一小段计算图用于反向传播，从而显著降低了 GPU 内存需求，同时仍能捕获反映教师模型完整长视频分布的梯度。</li>
<li><strong>数据驱动的优化：</strong> 论文强调了高质量、多样化且包含精确动作和相机轨迹标注的数据集的重要性。他们构建了一个在虚幻引擎（Unreal Engine）中渲染的大规模合成数据集，其中包含大量室内外场景、多样的动作组合以及频繁的视点重访，以支持模型的训练。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
RELIC 是一个 14B 参数的模型，在虚幻引擎渲染的数据集上进行了训练。其主要成果包括：</p>
<ul>
<li><strong>实时生成能力：</strong> RELIC 实现了 <strong>16 FPS 的实时生成速度</strong>，在 480x832 的分辨率下生成长达 20 秒的视频。</li>
<li><strong>优越的性能：</strong> 相较于现有工作，RELIC 在<strong>更准确的动作跟随、更稳定的长时序视频生成以及更鲁棒的空间记忆检索</strong>方面表现出色。</li>
<li><strong>广泛的泛化能力：</strong> RELIC 能够泛化到各种艺术风格的场景，如油画、漫画、矢量艺术等，并能正确理解 3D 形状和距离感。</li>
<li><strong>可控的交互体验：</strong> 通过连续的动作控制和可调的相机速度，用户可以实现精确、灵活的虚拟场景探索。</li>
<li><strong>奠定基础：</strong> RELIC 被认为是下一代交互式视频世界建模的<strong>坚实基础</strong>，为具身 AI 和沉浸式虚拟内容创作等领域提供了潜力。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中也提到了 RELIC 的一些局限性：</p>
<ul>
<li><strong>生成视频的多样性和动态性有限：</strong> 主要由于训练数据集主要由静态场景组成，生成的视频在多样性和场景动态性方面仍有局限。</li>
<li><strong>长视频生成时长限制：</strong> 虽然支持 20 秒的视频生成，但对于长达数分钟的视频生成仍有挑战。</li>
<li><strong>推理延迟：</strong> 模型大小、KV 缓存需求以及多步迭代去噪过程显著影响了推理延迟，尤其是在资源受限的环境下。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
尽管存在局限性，作者认为这些问题可以通过<strong>改进流水线、调整数据和训练策略</strong>来缓解。未来的研究可以集中在：</p>
<ul>
<li><strong>提高生成视频的多样性和动态性：</strong> 探索更多样化的场景和更丰富的动态内容。</li>
<li><strong>扩展视频生成时长：</strong> 进一步提升模型生成更长视频的能力。</li>
<li><strong>优化推理效率：</strong> 探索更高效的模型架构和推理技术，以进一步降低延迟。</li>
<li><strong>更广泛的应用探索：</strong> 将 RELIC 应用于具身 AI、虚拟现实、游戏开发等更广泛的领域。</li>
</ul>
<p>总而言之，RELIC 是一个在交互式视频世界建模领域的重要进展，它通过创新的长时序记忆机制和高效的蒸馏方法，成功地实现了实时性、长时序连贯性和精确控制的结合，为构建更逼真、更具交互性的虚拟世界提供了强大的技术支撑。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present RELIC, a unified framework that tackles these three challenges altogether.</li>
<li>In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04040v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04040v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04032v1'></a></p>
<h2 id="jina-vlm-small-multilingual-vision-language-model"><a href="https://arxiv.org/abs/2512.04032v1">Jina-VLM: Small Multilingual Vision Language Model</a></h2>
<p><strong>Authors:</strong> Andreas Koukounas, Georgios Mastrapas, Florian Hönicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“JINA-VLM: Small Multilingual Vision Language Model”的全面中文摘要，重点关注其研究问题、创新点、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> JINA-VLM: Small Multilingual Vision Language Model</p>
<p><strong>作者：</strong> Andreas Koukounas, Georgios Mastrapas, Florian Hönicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要研究问题/挑战：</strong></p>
<p>该论文旨在解决当前多语言视觉语言模型（VLMs）面临的两个主要挑战：
*   <strong>多语言能力退化：</strong> 许多VLMs在英文基准测试上表现良好，但在其他语言上的性能会显著下降。
*   <strong>计算成本高昂：</strong> 高质量的VLMs通常需要大量的计算资源进行训练和部署，这限制了资源有限的研究者和开发者的可及性。</p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<p>JINA-VLM模型通过以下关键创新解决了上述挑战：</p>
<ul>
<li>
<p><strong>高效的多语言架构：</strong></p>
<ul>
<li><strong>SigLIP2视觉编码器与Qwen3语言骨干的结合：</strong> 模型采用了SigLIP2-So400M/14-384作为视觉编码器，并结合了Qwen3-1.7B-Base作为语言骨干。</li>
<li><strong>注意力池化连接器（Attention-Pooling Connector）：</strong> 这是一个创新的连接器，它能够将来自视觉编码器中间层的特征（第24层和第18层）进行注意力池化，从而在减少视觉token数量（降低4倍）的同时保留空间信息。这使得模型能够高效地处理任意分辨率的图像。</li>
<li><strong>两阶段训练策略：</strong> 模型采用了两阶段的训练流程，其中明确地融入了多语言数据，以提升其跨语言理解能力。</li>
</ul>
</li>
<li>
<p><strong>高效的任意分辨率图像处理：</strong></p>
<ul>
<li><strong>重叠瓦片（Overlapping Tiling）与注意力池化：</strong> 模型使用重叠的图像瓦片来处理任意分辨率的图像，并结合注意力池化技术来压缩视觉token序列，显著降低了计算复杂度。</li>
</ul>
</li>
<li>
<p><strong>保留文本理解能力：</strong></p>
<ul>
<li><strong>文本数据注入：</strong> 在多模态训练过程中，模型融入了文本数据（约15%），以缓解多模态训练通常会导致的文本理解能力退化问题。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>JINA-VLM在多项基准测试中取得了显著的成果：</p>
<ul>
<li><strong>多语言VQA的SOTA性能：</strong> 在2B参数规模的开源VLMs中，JINA-VLM在多语言多模态基准测试（如MMMB和Multilingual MMBench）上达到了最先进（state-of-the-art）的性能。这表明小型模型也能实现出色的跨语言视觉理解能力，而不会牺牲通用能力。</li>
<li><strong>通用VQA的竞争力：</strong> 在标准的英文VQA基准测试（涵盖图表、文档、OCR等）上，JINA-VLM的表现与同等规模的可比模型相当甚至更优，其在八个VQA基准测试上的平均得分达到了72.3。</li>
<li><strong>低幻觉率：</strong> 在POPE基准测试中，JINA-VLM取得了90.3的高分，表明其具有较低的幻觉（生成虚假信息）倾向。</li>
<li><strong>文本能力保持：</strong> 在文本能力测试中，JINA-VLM在ARC-C和HellaSwag等基准上匹配甚至超越了其语言骨干模型，显示出多模态训练对文本能力的损害最小化。</li>
</ul>
<p><strong>意义：</strong> JINA-VLM的成功证明了通过精巧的架构设计和训练策略，可以在保持模型规模较小的同时，实现强大的多语言视觉理解能力，并有效缓解计算成本和多语言性能退化的问题，从而提高VLMs的可及性和实用性。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>多瓦片处理的计算开销：</strong> 尽管模型采用了高效的瓦片处理方法，但多瓦片处理仍然会随着图像分辨率的增加而引入计算开销。</li>
<li><strong>未强调安全性和对齐：</strong> 论文中并未特别强调模型的安全性训练或与人类偏好的对齐。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更高效的分辨率处理：</strong> 探索更有效的方法来处理高分辨率图像，以进一步降低计算开销。</li>
<li><strong>多语言训练策略的泛化：</strong> 研究该多语言训练配方是否能成功迁移到更大规模的模型上。</li>
<li><strong>安全性和对齐研究：</strong> 探索如何将安全性和对齐机制集成到模型中。</li>
</ul>
<p>总而言之，JINA-VLM是一项重要的研究成果，它通过创新的注意力池化连接器和多阶段多语言训练策略，成功地构建了一个小巧但功能强大的多语言视觉语言模型，为未来更易于访问和部署的VLMs铺平了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images.</li>
<li>Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04032v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04032v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04025v1'></a></p>
<h2 id="psa-pyramid-sparse-attention-for-efficient-video-understanding-and-generation"><a href="https://arxiv.org/abs/2512.04025v1">PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</a></h2>
<p><strong>Authors:</strong> Xiaolong Li, Youping Gu, Xi Lin, Weijie Wang, Bohan Zhuang</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话)</strong></p>
<p>这篇论文提出了一种名为“金字塔稀疏注意力”（Pyramid Sparse Attention, PSA）的新型注意力机制，旨在解决现有稀疏注意力方法在处理视频任务时信息损失过多的问题。PSA通过引入多层次的键值（KV）表示和精细化的掩码策略，实现了更优的计算效率和信息保留能力，从而在视频理解和生成任务中取得了更好的性能。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>PSA的核心创新在于其<strong>多层次池化键值（KV）表示</strong>和<strong>精细化的掩码分配策略</strong>。</p>
<ul>
<li><strong>多层次池化KV表示：</strong> 与现有方法简单地保留或丢弃整个KV块（二元掩码）不同，PSA为KV块引入了不同程度的池化表示。这意味着即使一个KV块被部分“丢弃”，其低层次的池化表示（包含更粗粒度的信息）仍然可以被保留。</li>
<li><strong>动态掩码分配：</strong> 每个查询块会根据其重要性动态地将计算资源分配给不同层次的KV块。关键的KV块会获得较低的池化层（保留更多细节），而不那么重要的KV块则分配较高的池化层（更粗粒度）。这形成了一种“信息插值”，在完全保留和完全剪枝之间找到了一个更平滑、信息损失更少的折衷。</li>
<li><strong>硬件友好型内核：</strong> 论文还强调了PSA的实现采用了解耦的块-瓦片设计，以实现高效的硬件执行。</li>
</ul>
<p>这种方法借鉴了计算机视觉中固定点量化和特征金字塔网络的思想，通过多尺度表示来平衡信息量和计算成本。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>PSA的提出可能对以下方面产生重要影响：</p>
<ul>
<li><strong>高效视频模型：</strong> 显著提升了在视频理解和生成任务中构建高效、可扩展模型的可能性。这对于处理长视频序列、高分辨率视频以及资源受限的设备尤为重要。</li>
<li><strong>稀疏注意力研究方向：</strong> 挑战了当前稀疏注意力研究中普遍存在的二元掩码范式，开辟了新的研究思路，即如何通过多尺度表示和精细化控制来优化稀疏性与信息保留的平衡。</li>
<li><strong>通用性：</strong> PSA被设计为一种通用的模块，可以应用于多种视频任务，这可能促使其成为未来视频基础模型中的标准组件。</li>
<li><strong>性能与效率的权衡：</strong> 提供了更优的效率-质量权衡，使得在有限的计算预算下也能获得更好的性能，从而降低了研究和部署的门槛。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>视频理解：</strong><ul>
<li><strong>视频分类/识别：</strong> 更有效地捕捉长时序依赖关系。</li>
<li><strong>视频目标检测/跟踪：</strong> 实时性要求高，高效的注意力机制至关重要。</li>
<li><strong>视频问答（VQA）/视频字幕生成：</strong> 需要理解视频内容和上下文信息。</li>
</ul>
</li>
<li><strong>视频生成：</strong><ul>
<li><strong>视频生成模型（如文本到视频）：</strong> 生成更连贯、更高质量的视频内容，同时控制计算成本。</li>
<li><strong>视频编辑/增强：</strong> 实现更精细的视频操作。</li>
</ul>
</li>
<li><strong>其他多模态任务：</strong> 尽管论文聚焦视频，但其核心思想（多层次稀疏注意力）也可能推广到其他需要处理序列数据和捕捉长距离依赖的任务，如长文本处理、音频处理等。</li>
<li><strong>边缘计算和移动设备：</strong> 低计算预算的特性使其非常适合在资源受限的设备上部署。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要中强调了PSA的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>实现复杂度：</strong> 引入多层次池化和动态掩码分配，虽然在理论上更优，但其实现和优化可能比简单的二元掩码更复杂，需要精细的工程实现来保证硬件效率。</li>
<li><strong>超参数调优：</strong> 多层次池化的具体层数、池化策略以及掩码分配的阈值等可能需要额外的超参数调优，增加了模型设计的复杂性。</li>
<li><strong>信息损失的根本限制：</strong> 尽管PSA旨在减少信息损失，但任何形式的稀疏化都不可避免地会带来一定程度的信息损失。在极端稀疏的情况下，其性能可能仍然会受到影响。</li>
<li><strong>硬件依赖性：</strong> 论文提到“硬件友好型内核”，这可能意味着其最佳性能依赖于特定的硬件架构或优化技术，在通用硬件上的表现可能需要进一步验证。</li>
<li><strong>理论分析的深度：</strong> 摘要主要侧重于方法和实验结果，关于PSA在理论上为何能如此有效地保留信息，以及其信息论上的界限，可能需要阅读全文来深入了解。</li>
</ul>
<p>总而言之，PSA是一项非常有前景的研究，它通过创新的多层次稀疏注意力机制，有效地解决了现有方法的瓶颈，为高效的视频模型开发提供了新的强大工具。其在信息保留和计算效率之间的平衡，以及潜在的通用性，使其成为计算机视觉领域值得关注的重要进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04025v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04025v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04021v1'></a></p>
<h2 id="c3g-learning-compact-3d-representations-with-2k-gaussians"><a href="https://arxiv.org/abs/2512.04021v1">C3G: Learning Compact 3D Representations with 2K Gaussians</a></h2>
<p><strong>Authors:</strong> Honggyu An, Jaewoo Jung, Mungyeom Kim, Sunghwan Hong, Chaehyun Kim, Kazumi Fukuda, Minkyeong Jeon, Jisang Han, Takuya Narihira, Hyuna Ko, Junsu Kim, Yuki Mitsufuji, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于学习紧凑型3D高斯表示的论文的全面摘要，重点关注其创新、结果、局限性和未来方向。</p>
<p><strong>论文题目：</strong> C3G: Learning Compact 3D Representations with 2K Gaussians</p>
<p><strong>作者：</strong> Honggyu An, Jaewoo Jung, Mungyeom Kim, Sunghwan Hong, Chaehyun Kim, Kazumi Fukuda, Minkyeong Jeon, Jisang Han, Takuya Narihira, Hyuna Ko, Junsu Kim, Yuki Mitsufuji, Seungryong Kim</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 要解决的主要问题/研究问题：</strong></p>
<p>该论文旨在解决从无监督的稀疏多视图图像中以<strong>前馈方式</strong>重建和理解3D场景的挑战。现有方法通常采用<strong>逐像素3D高斯泼溅（Gaussian Splatting）</strong>进行重建，然后进行<strong>2D到3D的特征提升</strong>以实现场景理解。然而，这些方法会生成<strong>过多的冗余高斯</strong>，导致<strong>高内存开销</strong>和<strong>次优的多视图特征聚合</strong>，从而降低了<strong>新视图合成（Novel View Synthesis）</strong>和<strong>场景理解</strong>的性能。论文的核心研究问题是：<strong>是否需要像素对齐的高斯才能有效地重建和理解3D场景？</strong></p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<p>C3G（Compact 3D Gaussians）提出了一种新颖的前馈框架，其核心创新在于：</p>
<ul>
<li><strong>紧凑型3D高斯表示：</strong> C3G不生成逐像素的高斯，而是<strong>仅在关键的空间位置估计紧凑型3D高斯</strong>，从而最大限度地减少冗余。该方法仅使用约<strong>2K个高斯</strong>，比现有方法（如LSM）减少了约65倍。</li>
<li><strong>可学习的查询令牌（Learnable Query Tokens）：</strong> 引入了<strong>可学习的查询令牌</strong>，通过<strong>自注意力机制</strong>聚合多视图特征，以指导高斯的生成。这确保了每个高斯都能整合来自不同视图的相关视觉信息。</li>
<li><strong>基于查询的高斯解码：</strong> 利用这些经过精炼的可学习查询令牌，通过一个<strong>高斯头（Gaussian Head）</strong>高效地解码出紧凑的3D高斯。</li>
<li><strong>视图不变特征提升（View-Invariant Feature Lifting）：</strong> 利用C3G-G（高斯解码器）中学习到的<strong>注意力模式</strong>，设计了一个<strong>视图不变特征解码器（C3G-F）</strong>。该解码器能够高效地将任意2D特征提升到3D，并实现多视图特征的一致性聚合，而无需昂贵的后向映射操作。</li>
<li><strong>无需显式监督：</strong> 该框架仅通过<strong>光度重建目标</strong>进行训练，不需要地面真实深度或场景分解的监督。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>C3G在多个下游任务上取得了显著的成果：</p>
<ul>
<li><strong>新视图合成（Novel View Synthesis）：</strong> 尽管使用了少得多的高斯（约2K），C3G在PSNR、SSIM和LPIPS等指标上取得了与现有方法（如AnySplat）相当甚至更优的视觉质量。在测试时优化（TTO）后，C3G的性能进一步提升，并生成更少伪影的高质量渲染。</li>
<li><strong>3D场景理解（3D Scene Understanding）：</strong> C3G在开放词汇分割任务上表现出色，其紧凑型高斯与多视图聚合的语义特征相结合，显著优于现有的前馈方法。它在ScanNet和Replica数据集上取得了具有竞争力的分割性能，甚至优于一些需要更多输入图像的优化方法。</li>
<li><strong>内存效率和渲染速度：</strong> C3G实现了<strong>卓越的内存效率</strong>（4.1MB vs. 61.5MB），这对于带宽受限的应用至关重要。虽然FPS增益受限于硬件饱和，但其内存优势是关键的。</li>
<li><strong>特征保真度：</strong> C3G-F能够生成视图不变且语义上更具辨别力的特征，有效解决了多视图特征不一致的问题。</li>
</ul>
<p><strong>意义：</strong> 该研究表明，一个<strong>紧凑但几何上有意义的表示</strong>足以实现高质量的场景重建和理解。这为3D计算机视觉领域提供了一种更高效、更具可扩展性的解决方案，尤其是在处理稀疏和无监督的多视图数据时。</p>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>尺度模糊性：</strong> 论文提到，由于训练仅基于光度重建，生成的3D高斯可能无法完全与地面真实场景的尺度对齐。为了解决这个问题，在评估时需要进行目标视图相机姿态的优化，但这在实际应用中并非必需。</li>
<li><strong>动态场景：</strong> C3G目前仅限于<strong>静态场景重建</strong>。将其扩展到动态场景是一个潜在的局限性。</li>
<li><strong>对新颖基础模型的评估不足：</strong> 尽管C3G-F可以提升任意2D特征，但论文并未评估所有最新的基础模型（如SAM）。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>集成更先进的基础模型：</strong> 探索将像SAM这样的模型集成到C3G框架中，以实现更鲁棒的多视图一致性分割。</li>
<li><strong>3D场景问答（3D Scene Question Answering）：</strong> 将C3G的特征场与多模态大型语言模型（MLLMs）结合，以实现更全面的3D场景理解。</li>
<li><strong>动态场景重建：</strong> 将C3G的紧凑表示扩展到动态场景，以应对更广泛的自主应用。</li>
<li><strong>与视觉-语言-动作（VLA）模型或机器人技术的集成：</strong> 利用C3G的无信息损失特征渲染能力，将其应用于更复杂的机器人和VLA任务。</li>
</ul>
<hr />
<p>总而言之，C3G论文提出了一种创新的方法，通过学习紧凑型3D高斯表示来解决3D场景重建和理解中的效率和性能瓶颈。其核心贡献在于利用可学习的查询令牌和自注意力机制来生成稀疏但有意义的高斯，并在此基础上实现了高效的视图不变特征提升。该方法在多个任务上取得了优异的性能，同时显著降低了内存开销，为未来的3D视觉研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance.</li>
<li>We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting.</li>
<li>We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views.</li>
<li>Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness.</li>
<li>Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04021v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04021v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.04012v1'></a></p>
<h2 id="emergent-outlier-view-rejection-in-visual-geometry-grounded-transformers"><a href="https://arxiv.org/abs/2512.04012v1">Emergent Outlier View Rejection in Visual Geometry Grounded Transformers</a></h2>
<p><strong>Authors:</strong> Jisang Han, Sunghwan Hong, Jaewoo Jung, Wooseok Jang, Honggyu An, Qianqian Wang, Seungryong Kim, Chen Feng</p>
<p><strong>Published:</strong> 2025-12-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reliable 3D reconstruction from in-the-wild image collections is often hindered by "noisy" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> Emergent Outlier View Rejection in Visual Geometry Grounded Transformers
<strong>Authors:</strong> Jisang Han, Sunghwan Hong, Jaewoo Jung, Wooseok Jang, Honggyu An, Qianqian Wang, Seungryong Kim, Chen Feng
<strong>Categories:</strong> cs.CV
<strong>Published Date:</strong> 2025-12-03</p>
<p><strong>Abstract:</strong>
Reliable 3D reconstruction from in-the-wild image collections is often hindered by "noisy" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.</p>
<hr />
<p><strong>中文分析：</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong>
本研究的核心贡献在于，首次揭示了现有的前馈式3D重建模型（如VGGT）在缺乏显式异常值剔除机制的情况下，能够<strong>内生地（emergent）</strong>识别并抑制“干扰图像”（distractor images）。研究者通过深入分析，定位了模型中一个具有异常值抑制能力的特定层，并证明了该层编码的内部表征能够实现有效的噪声过滤，从而无需额外的微调或监督即可用于前馈式3D重建中的异常视图剔除。</p>
<p><strong>2. 关键创新点或方法论：</strong>
*   <strong>“涌现式”异常值剔除（Emergent Outlier View Rejection）：</strong> 这是最核心的创新。研究者并非设计新的模型或训练策略来处理异常值，而是发现现有模型中已存在这种能力，并将其“挖掘”出来。
*   <strong>深入的层级分析（In-depth Layer Analysis）：</strong> 通过在不同比例的合成干扰图像下进行实验，研究者精确定位了模型中负责异常值抑制的特定层。
*   <strong>利用内部表征进行噪声过滤（Leveraging Internal Representations for Noise Filtering）：</strong> 关键在于该特定层编码的“区分性内部表征”（discriminative internal representations），这些表征自然地赋予了模型过滤噪声的能力。
*   <strong>无监督、无微调的即插即用（Plug-and-Play without Fine-tuning or Supervision）：</strong> 研究者直接利用发现的机制，无需对模型进行任何修改或额外训练，即可实现异常视图的剔除，大大降低了应用门槛。</p>
<p><strong>3. 对该领域的潜在影响：</strong>
*   <strong>提升前馈式3D重建的鲁棒性：</strong> 这是最直接的影响。在“in-the-wild”等复杂、不可控的真实世界场景下，3D重建的性能往往受到噪声数据的影响。本研究提供了一种简单有效的方法来解决这一痛点，使得前馈模型在实际应用中更加可靠。
*   <strong>改变对现有模型的理解：</strong> 本研究挑战了“前馈模型缺乏异常值处理能力”的普遍认知，揭示了深度学习模型在复杂任务中可能涌现出的意想不到的能力，为理解和设计更强大的模型提供了新的视角。
*   <strong>简化3D重建流程：</strong> 传统SfM流程中的几何验证和异常值剔除是计算密集型且复杂的步骤。本研究表明，通过利用现有模型的内在能力，可以绕过这些显式步骤，实现更高效的重建。
*   <strong>推动“自适应”和“自愈”模型的研究：</strong> 这种“涌现式”能力的研究，可能启发更多关于模型如何自适应地处理噪声和不确定性的研究方向。</p>
<p><strong>4. 可能受益的相关领域或应用：</strong>
*   <strong>大规模3D场景重建：</strong> 例如，从用户上传的照片集（如Google Street View的早期数据、社交媒体图片）进行3D重建，这些数据往往包含大量低质量或无关的图像。
*   <strong>机器人导航与感知：</strong> 机器人需要在动态且充满不确定性的环境中进行3D感知，过滤掉错误的传感器数据（如相机捕捉到的瞬间干扰）至关重要。
*   <strong>虚拟现实（VR）/增强现实（AR）内容生成：</strong> 高质量的3D模型是VR/AR体验的基础，本研究有助于提高从真实世界数据生成3D内容的效率和质量。
*   <strong>自动驾驶中的场景理解：</strong> 车辆的3D环境感知需要高度的鲁棒性，剔除可能由光照变化、遮挡等引起的“干扰帧”是关键。
*   <strong>计算机视觉中的其他鲁棒性问题：</strong> 这种“涌现式”的噪声过滤能力，可能可以迁移或启发解决其他对噪声敏感的任务，如图像去噪、目标检测中的误检过滤等。</p>
<p><strong>5. 从摘要中可以推断出的局限性：</strong>
*   <strong>对“干扰图像”的定义和生成方式：</strong> 摘要提到使用“合成的干扰图像”（synthetic distractors）进行分析。这可能意味着该方法在处理真实世界中更复杂、更微妙的“噪声”或“干扰”时，其有效性需要进一步验证。真实世界的干扰可能不仅仅是“无视差”的图像，还可能包含模糊、曝光过度/不足、运动模糊等多种形式。
*   <strong>特定模型的依赖性：</strong> 虽然研究者声称“现有前馈重建模型，例如VGGT”，但其发现的特定层和内部表征的有效性，可能在多大程度上泛化到其他不同架构的前馈3D重建模型上，仍需进一步研究。
*   <strong>“涌现”的机制解释深度：</strong> 摘要描述了“发现”和“利用”，但对于该特定层为何会“自然地”产生这种异常值抑制行为的深层理论解释可能还需要更深入的研究。这可能与Transformer的自注意力机制、多视图特征的聚合方式等有关。
*   <strong>定量评估的范围：</strong> 摘要提到“控制和in-the-wild数据集”，但具体的数据集规模、多样性以及评估指标的详细情况并未在摘要中给出，这会影响对泛化能力的全面判断。
*   <strong>性能提升的幅度：</strong> 摘要表明“性能得到提升”，但具体的性能提升幅度（例如，与传统SfM或未处理异常值的前馈模型相比）并未量化。</p>
<p>总而言之，这篇论文的亮点在于其“发现式”的研究方法，即在现有模型中挖掘出隐藏的鲁棒性能力，并将其转化为实用的异常值剔除技术。这对于提升前馈式3D重建在真实世界场景下的可靠性具有重要意义，并且为理解深度学习模型的内在机制提供了新的视角。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision.</li>
<li>Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04012v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04012v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-04 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
