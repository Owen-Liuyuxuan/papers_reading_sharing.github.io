<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-01 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-30/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-01">Arxiv Computer Vision Papers - 2025-10-01</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#stitch-training-free-position-control-in-multimodal-diffusion-transformers" class="nav-link">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#ttt3r-3d-reconstruction-as-test-time-training" class="nav-link">TTT3R: 3D Reconstruction as Test-Time Training</a>
                </li>
                <li class="nav-item">
                    <a href="#benchmarking-egocentric-visual-inertial-slam-at-city-scale" class="nav-link">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-generalizable-shape-completion-with-sim3-equivariance" class="nav-link">Learning Generalizable Shape Completion with SIM(3) Equivariance</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training" class="nav-link">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a>
                </li>
                <li class="nav-item">
                    <a href="#hart-human-aligned-reconstruction-transformer" class="nav-link">HART: Human Aligned Reconstruction Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#da2-depth-anything-in-any-direction" class="nav-link">DA^2: Depth Anything in Any Direction</a>
                </li>
                <li class="nav-item">
                    <a href="#video-object-segmentation-aware-audio-generation" class="nav-link">Video Object Segmentation-Aware Audio Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#stable-cinemetrics-structured-taxonomy-and-evaluation-for-professional-video-generation" class="nav-link">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#automated-and-scalable-sem-image-analysis-of-perovskite-solar-cell-materials-via-a-deep-segmentation-framework" class="nav-link">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-01">Arxiv Computer Vision Papers - 2025-10-01</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月30日Arxiv计算机视觉领域论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解最新进展。</p>
<hr />
<p><strong>每日Arxiv计算机视觉报告执行摘要 (2025年9月30日)</strong></p>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文集展示了计算机视觉领域持续的多元化和快速发展。主要趋势包括：</p>
<ul>
<li><strong>多模态与扩散模型：</strong> 扩散模型在图像生成和控制方面持续演进，并与多模态输入（如文本、姿态）结合，实现更精细的控制。</li>
<li><strong>3D视觉与重建：</strong> 3D重建技术在效率、泛化性和特定场景（如城市规模SLAM、人体重建）的应用上取得了显著进展。</li>
<li><strong>基础模型与泛化能力：</strong> 探索如何利用大型语言模型（LLM）的先验知识来增强视觉理解，以及如何提升模型在未知环境或数据上的泛化能力。</li>
<li><strong>特定应用领域：</strong> 论文涵盖了从专业视频生成、视频对象分割到材料科学图像分析等多个具体应用场景，显示了CV技术在各行各业的渗透。</li>
<li><strong>效率与训练策略：</strong> 出现了免训练（training-free）方法和测试时训练（test-time training）等策略，旨在提高模型部署的灵活性和效率。</li>
</ul>
<p><strong>2. 特别重要或创新的论文亮点：</strong></p>
<ul>
<li><strong>"Stitch: Training-Free Position Control in Multimodal Diffusion Transformers" (Jessica Bader et et al.)：</strong> 这篇论文非常创新，提出了一个<strong>免训练</strong>的方法来控制多模态扩散模型中的物体位置，这对于生成式AI的实际应用具有重大意义，因为它大大降低了微调的成本和复杂性。</li>
<li><strong>"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training" (Junlin Han et al.)：</strong> 这篇论文深入探讨了LLM如何通过语言预训练获得视觉先验知识，并将其应用于视觉任务。这对于理解和构建更强大的<strong>多模态基础模型</strong>具有重要的理论和实践价值。</li>
<li><strong>"DA<script type="math/tex">^2</script>: Depth Anything in Any Direction" (Haodong Li et al.)：</strong> 延续了“Depth Anything”的泛化能力，这篇工作可能进一步提升了<strong>通用深度估计</strong>的鲁棒性和应用范围，使其在各种复杂场景下都能提供可靠的深度信息。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>免训练（Training-Free）方法：</strong> "Stitch"的出现表明，在特定控制任务中，通过巧妙的设计而非大量数据训练，也能实现高性能，这可能成为未来模型部署和定制化的一个重要方向。</li>
<li><strong>LLM视觉先验的利用：</strong> "Learning to See Before Seeing"强调了从语言模型中提取和利用视觉知识的潜力，预示着未来多模态模型将更紧密地融合语言和视觉信息。</li>
<li><strong>测试时训练（Test-Time Training, TTT）在3D重建中的应用：</strong> "TTT3R"将TTT引入3D重建，这是一种提高模型对未知场景适应性的有效策略，有望在资源受限或需要快速适应新环境的场景中发挥作用。</li>
<li><strong>结构化视频生成与评估：</strong> "Stable Cinemetrics"提出了专业视频生成的分类和评估框架，表明该领域正从单纯的生成质量转向更注重<strong>内容结构和专业性</strong>。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>为了更深入地了解这些进展，我建议您优先阅读以下论文：</p>
<ul>
<li><strong>"Stitch: Training-Free Position Control in Multimodal Diffusion Transformers" (Jessica Bader et al.)：</strong> 对于任何关注生成式AI和扩散模型控制的研究人员来说，这篇论文是必读的，其免训练的理念非常具有启发性。</li>
<li><strong>"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training" (Junlin Han et al.)：</strong> 如果您对多模态基础模型、LLM与视觉的结合感兴趣，这篇论文将提供深刻的见解。</li>
<li><strong>"DA<script type="math/tex">^2</script>: Depth Anything in Any Direction" (Haodong Li et al.)：</strong> 对于从事3D视觉、机器人或自动驾驶领域的研究人员，了解通用深度估计的最新进展至关重要。</li>
<li><strong>"HART: Human Aligned Reconstruction Transformer" (Xiyi Chen et al.)：</strong> 如果您专注于人体3D重建或虚拟人技术，这篇论文可能提供了新的SOTA方法。</li>
</ul>
<hr />
<p>这份摘要旨在为您提供一个快速概览，帮助您识别最相关的研究方向和论文，以便进一步深入阅读。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.26644v1">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></li>
<li><a href="#2509.26645v1">TTT3R: 3D Reconstruction as Test-Time Training</a></li>
<li><a href="#2509.26639v1">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a></li>
<li><a href="#2509.26631v1">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></li>
<li><a href="#2509.26625v1">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a></li>
<li><a href="#2509.26621v1">HART: Human Aligned Reconstruction Transformer</a></li>
<li><a href="#2509.26618v1">DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</a></li>
<li><a href="#2509.26604v1">Video Object Segmentation-Aware Audio Generation</a></li>
<li><a href="#2509.26555v1">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a></li>
<li><a href="#2509.26548v1">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.26644v1'></a></p>
<h2 id="stitch-training-free-position-control-in-multimodal-diffusion-transformers"><a href="https://arxiv.org/abs/2509.26644v1">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></h2>
<p><strong>Authors:</strong> Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-Image (T2I) generation models have advanced rapidly in recent years,
but accurately capturing spatial relationships like "above" or "to the right
of" poses a persistent challenge. Earlier methods improved spatial relationship
following with external position control. However, as architectures evolved to
enhance image quality, these techniques became incompatible with modern models.
We propose Stitch, a training-free method for incorporating external position
control into Multi-Modal Diffusion Transformers (MMDiT) via
automatically-generated bounding boxes. Stitch produces images that are both
spatially accurate and visually appealing by generating individual objects
within designated bounding boxes and seamlessly stitching them together. We
find that targeted attention heads capture the information necessary to isolate
and cut out individual objects mid-generation, without needing to fully
complete the image. We evaluate Stitch on PosEval, our benchmark for
position-based T2I generation. Featuring five new tasks that extend the concept
of Position beyond the basic GenEval task, PosEval demonstrates that even top
models still have significant room for improvement in position-based
generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances
base models, even improving FLUX by 218% on GenEval's Position task and by 206%
on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on
PosEval, improving over previous models by 54%, all accomplished while
integrating position control into leading models training-free. Code is
available at https://github.com/ExplainableML/Stitch.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Jessica Bader等人撰写的论文“Stitch: Training-Free Position Control in Multimodal Diffusion Transformers”的全面摘要：</p>
<p><strong>论文摘要：Stitch: 多模态扩散Transformer中的免训练位置控制</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
尽管文本到图像（T2I）生成模型近年来取得了显著进展，但在准确捕捉“上方”、“右侧”等空间关系方面仍然面临持续挑战。早期的外部位置控制方法虽然能改善空间关系，但随着模型架构演进以提升图像质量，这些技术与现代模型变得不兼容。因此，核心问题是如何在不牺牲图像质量和生成速度的前提下，为最新的多模态扩散Transformer（MMDiT）模型提供精确且免训练的位置控制。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了<strong>Stitch</strong>，一种免训练的方法，用于将外部位置控制集成到MMDiT架构中。其主要创新点包括：</p>
<ul>
<li><strong>LLM驱动的边界框分解与区域绑定（Region Binding）：</strong> Stitch利用大型语言模型（LLM）将完整的文本提示分解为多个子提示，每个子提示对应一个由LLM生成的边界框。在生成的前S步中，MMDiT模型在这些指定边界框内独立生成各个对象和背景，并通过注意力掩码约束（Region Binding）确保对象完全在各自的边界框内生成，并与周围上下文隔离。</li>
<li><strong>注意力引导的抠图（Cutout）：</strong> 为了避免背景不匹配导致的可见接缝，Stitch在生成中期（S步之后）通过分析特定注意力头的最高注意力权重，从潜在空间中提取前景对象。这种方法无需外部分割模型，且能在图像未完全生成时进行。提取出的前景潜在tokens与背景潜在tokens合并形成复合潜在表示，用于后续的无约束生成。</li>
<li><strong>PosEval基准测试：</strong> 论文引入了PosEval，一个扩展自GenEval的、针对基于位置的T2I生成能力的综合基准测试。PosEval包含五个新任务，旨在深入评估T2I模型的定位能力，超越了传统的“位置”概念，以揭示特定故障模式并应对更复杂的生成挑战。</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
Stitch在多个领先的MMDiT模型（如Qwen-Image、FLUX和SD3.5）上进行了评估，并取得了显著成果：</p>
<ul>
<li><strong>显著提升位置准确性：</strong> Stitch在PosEval基准测试中持续提升了基础模型的性能。例如，在GenEval的“位置”任务上，FLUX的性能提升了218%；在PosEval上，FLUX的性能提升了206%。</li>
<li><strong>实现最先进（SOTA）结果：</strong> Stitch在Qwen-Image模型上实现了PosEval的SOTA结果，比之前模型提升了54%。</li>
<li><strong>保持图像质量和多样性：</strong> Stitch在提升位置控制的同时，并未显著降低图像的视觉质量（通过美学分数评估）和多样性（通过DINOv2嵌入空间中的样本间距离评估）。</li>
<li><strong>免训练集成：</strong> 所有这些改进都是通过免训练的方式实现的，使得Stitch能够快速且经济地升级现有T2I模型。</li>
<li><strong>PosEval揭示现有模型局限：</strong> PosEval基准测试表明，即使是顶级的T2I模型在处理复杂的位置提示时仍有很大的改进空间，尤其是在多对象、相对关系和否定关系等任务上。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确指出Stitch方法的局限性，但提到了以下几点：</p>
<ul>
<li><strong>现有T2I模型在复杂位置提示上的挣扎：</strong> PosEval基准测试揭示，即使是SOTA模型在处理多对象、相对关系和否定关系等复杂位置任务时仍有显著不足。</li>
<li><strong>抠图掩码的窄边框：</strong> 抠图掩码有时会在对象周围留下一个窄边框，这可能是因为低级细节尚未完全固化。尽管这种行为有助于捕获完整对象并在后续生成步骤中重建缺失部分，但它也暗示了抠图的精确度可能仍有提升空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文没有明确提出未来研究方向，但从其贡献和局限性中可以推断出以下潜在方向：</p>
<ul>
<li><strong>优化抠图机制：</strong> 进一步研究和优化抠图机制，以实现更精确的对象分割，减少窄边框现象，并探索在更早的生成阶段进行更精细的潜在空间操作。</li>
<li><strong>扩展PosEval基准：</strong> 随着T2I模型能力的提升，可以进一步扩展PosEval，引入更多复杂、细致的位置任务，例如涉及三维空间关系、动态对象或更复杂的语言结构。</li>
<li><strong>探索更深层次的MMDiT集成：</strong> 尽管Stitch是免训练的，但未来可以探索将位置控制机制更深层次地集成到MMDiT的训练过程中，以期实现更原生和鲁棒的性能。</li>
<li><strong>多模态输入融合：</strong> 论文主要关注文本提示，未来可以探索如何将Stitch扩展到处理其他模态的输入（如草图、布局图），以实现更灵活和精细的位置控制。</li>
</ul>
<p>总而言之，Stitch为多模态扩散Transformer中的位置控制提供了一个强大且免训练的解决方案，显著提升了T2I模型在复杂空间关系生成方面的能力，并通过引入PosEval基准测试，为该领域未来的研究提供了重要的评估工具。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Stitch, a training-free method for incorporating external position
control into Multi-Modal Diffusion Transformers (MMDiT) via
automatically-generated bounding boxes.</li>
<li>Featuring five new tasks that extend the concept
of Position beyond the basic GenEval task, PosEval demonstrates that even top
models still have significant room for improvement in position-based
generation.</li>
<li>Stitch achieves state-of-the-art results with Qwen-Image on
PosEval, improving over previous models by 54%, all accomplished while
integrating position control into leading models training-free.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26644v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26644v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26645v1'></a></p>
<h2 id="ttt3r-3d-reconstruction-as-test-time-training"><a href="https://arxiv.org/abs/2509.26645v1">TTT3R: 3D Reconstruction as Test-Time Training</a></h2>
<p><strong>Authors:</strong> Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern Recurrent Neural Networks have become a competitive architecture for
3D reconstruction due to their linear-time complexity. However, their
performance degrades significantly when applied beyond the training context
length, revealing limited length generalization. In this work, we revisit the
3D reconstruction foundation models from a Test-Time Training perspective,
framing their designs as an online learning problem. Building on this
perspective, we leverage the alignment confidence between the memory state and
incoming observations to derive a closed-form learning rate for memory updates,
to balance between retaining historical information and adapting to new
observations. This training-free intervention, termed TTT3R, substantially
improves length generalization, achieving a <script type="math/tex">2\times</script> improvement in global
pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU
memory to process thousands of images. Code available in
https://rover-xingyu.github.io/TTT3R</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen撰写的论文“TTT3R: 3D Reconstruction as Test-Time Training”的全面摘要。</p>
<hr />
<h3 id="ttt3r-3d-reconstruction-as-test-time-training_1">论文摘要：TTT3R: 3D Reconstruction as Test-Time Training</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文主要关注现代循环神经网络（RNNs）在3D重建任务中的一个核心限制：尽管RNNs具有线性时间复杂度，但在训练上下文长度之外应用时，其性能会显著下降，表现出有限的长度泛化能力。具体来说，现有的基于RNN的3D重建基础模型（如CUT3R）在处理长序列图像时，由于“遗忘问题”和状态过拟合，性能会严重退化，无法有效保留历史信息。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
TTT3R的核心创新在于将3D重建基础模型的设计重新构想为<strong>测试时训练（Test-Time Training, TTT）</strong>问题，将其视为一个在线学习过程。在此视角下，论文提出了以下关键贡献：
*   <strong>在线学习框架：</strong> 将记忆状态（state）视为在测试时通过梯度下降学习的“快速权重”，而非训练阶段固定的“慢权重”，从而使其能够动态适应输入上下文。
*   <strong>置信度引导的状态更新规则：</strong> 论文利用记忆状态与传入观测值之间的对齐置信度，推导出一个<strong>闭式（closed-form）学习率</strong>来更新记忆。这个学习率能够平衡历史信息的保留和对新观测的适应，有效缓解了灾难性遗忘问题。具体而言，学习率<script type="math/tex">\beta_t</script>由状态查询<script type="math/tex">Q_{s_{t-1}}</script>和观测键<script type="math/tex">K_{x_t}</script>之间的对齐置信度（通过softmax加权）决定，从而实现每token自适应学习。
*   <strong>训练无关的干预：</strong> TTT3R是一种“训练无关、即插即用”的干预措施，无需对现有模型进行微调或添加额外参数，即可直接应用于下游任务，显著提升了长度泛化能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
TTT3R在多个3D重建基准测试中取得了显著成果：
*   <strong>长度泛化能力大幅提升：</strong> 相较于现有基线方法，TTT3R在全局姿态估计方面实现了<strong>2倍</strong>的性能提升，尤其在处理数千张图像的长序列时表现出强大的鲁化性。
*   <strong>高效的资源利用：</strong> 尽管处理数千张图像，TTT3R仍能以<strong>20 FPS</strong>的速度运行，且仅需<strong>6 GB的GPU内存</strong>，保持了与CUT3R基线相同的推理速度和内存效率。
*   <strong>定性改进：</strong> 在定性结果中，TTT3R实现了更准确的重建，有效缓解了CUT3R中出现的灾难性遗忘、相机姿态漂移、几何结构损坏和鬼影伪影等问题，并支持在线闭环。
*   <strong>与在线方法的竞争力：</strong> 在短序列评估中，TTT3R与最先进的在线重建模型（如CUT3R、Point3R）相比具有竞争力，在某些数据集上甚至表现最佳。</p>
<p>这些结果表明，TTT3R为解决RNNs在长序列3D重建中的泛化问题提供了一个有效且高效的解决方案，推动了在线3D重建技术的发展。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>未能完全解决状态遗忘：</strong> 尽管TTT3R显著缓解了状态遗忘问题，但并未完全解决。
*   <strong>与离线方法的差距：</strong> TTT3R的重建精度尚未完全匹配强大的离线方法（如VGGT），因为全注意力机制虽然速度慢、内存需求高，但能保留完整的历史上下文。
*   <strong>设计空间仍待探索：</strong> 作为一种测试时回归方法，TTT3R的设计空间仍有待进一步探索。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>开发更有效、稳定和可并行化的循环架构：</strong> 论文指出，未来研究应继续探索更有效的循环架构，以进一步提高3D重建的精度和长度泛化能力。
*   <strong>深入探索测试时回归的设计空间：</strong> 鉴于测试时回归在关联记忆方面展现出的潜力，未来可以更深入地研究其设计空间，以期发现更优的解决方案。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this
perspective, we leverage the alignment confidence between the memory state and
incoming observations to derive a closed-form learning rate for memory updates,
to balance between retaining historical information and adapting to new
observations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26645v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26645v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26639v1'></a></p>
<h2 id="benchmarking-egocentric-visual-inertial-slam-at-city-scale"><a href="https://arxiv.org/abs/2509.26639v1">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a></h2>
<p><strong>Authors:</strong> Anusha Krishnan, Shaohui Liu, Paul-Edouard Sarlin, Oscar Gentilhomme, David Caruso, Maurizio Monge, Richard Newcombe, Jakob Engel, Marc Pollefeys</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard
sensors is critical for wearable devices capturing egocentric data, which
exhibits specific challenges, such as a wider diversity of motions and
viewpoints, prevalent dynamic visual content, or long sessions affected by
time-varying sensor calibration. While recent progress on SLAM has been swift,
academic research is still driven by benchmarks that do not reflect these
challenges or do not offer sufficiently accurate ground truth poses. In this
paper, we introduce a new dataset and benchmark for visual-inertial SLAM with
egocentric, multi-modal data. We record hours and kilometers of trajectories
through a city center with glasses-like devices equipped with various sensors.
We leverage surveying tools to obtain control points as indirect pose
annotations that are metric, centimeter-accurate, and available at city scale.
This makes it possible to evaluate extreme trajectories that involve walking at
night or traveling in a vehicle. We show that state-of-the-art systems
developed by academia are not robust to these challenges and we identify
components that are responsible for this. In addition, we design tracks with
different levels of difficulty to ease in-depth analysis and evaluation of less
mature approaches. The dataset and benchmark are available at
https://www.lamaria.ethz.ch.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Anusha Krishnan等人撰写的论文“Benchmarking Egocentric Visual-Inertial SLAM at City Scale”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Benchmarking Egocentric Visual-Inertial SLAM at City Scale</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现有视觉惯性同步定位与建图（VI-SLAM）基准数据集在评估以自我为中心（egocentric）数据时存在的不足。具体来说，现有基准未能充分反映可穿戴设备捕获的自我中心数据的独特挑战，例如：运动和视角的多样性、动态视觉内容、长时间会话中时变传感器校准，以及缺乏足够精确的真值姿态。这导致学术界开发的SLAM系统在面对这些真实世界挑战时表现不佳。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>LaMAria数据集：</strong> 引入了一个新的、大规模、以城市为中心的自我中心VI-SLAM数据集和基准。该数据集使用Project Aria眼镜式设备记录了数小时、数十公里的轨迹，包含多传感器（双灰度全局快门相机、RGB卷帘快门相机、双IMU、磁力计、气压计、温度计、GNSS接收器、WiFi和蓝牙收发器）数据。
*   <strong>厘米级真值姿态：</strong> 利用测量工具（包括GNSS-RTK和全站仪）获取稀疏控制点（CPs）作为间接姿态标注，这些标注具有度量、厘米级精度，并覆盖整个城市范围。通过将AprilTag标记附着在CPs上，实现了CP的自动检测。
*   <strong>伪真值姿态生成：</strong> 通过融合视觉、惯性和CP信息，并进行联合优化，生成了更密集的伪真值相机姿态，以支持细粒度评估和3D重建任务。
*   <strong>挑战性场景覆盖：</strong> 数据集涵盖了自我中心数据的独特挑战，包括极低光照、曝光变化、移动平台（如电车、缆车）、时变校准、动态场景（如行人、车辆）以及室内外过渡。
*   <strong>分级难度轨道：</strong> 设计了不同难度级别的实验轨道（Level I-IV），从受控平台运动到不受限制的头戴式运动，以便深入分析和评估不同成熟度的方法。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>现有SOTA系统表现不佳：</strong> 评估结果表明，学术界开发的现有最先进（SOTA）VI-SLAM系统在面对LaMAria数据集中的自我中心挑战时，鲁棒性不足，并且与Project Aria设备的专有SLAM API之间存在显著差距。
*   <strong>多传感器融合的优势：</strong> 依赖多相机和惯性传感器显著提升了VI-SLAM系统的性能。
*   <strong>在线校准的重要性：</strong> Project Aria的SLAM系统（包含在线校准）表现优于所有学术解决方案，尤其是在长时间序列中，焦距变化范围更大，凸显了在线校准对于可穿戴设备全天候使用的重要性。
*   <strong>识别失败原因：</strong> 论文识别了导致现有系统失败的关键组件，例如在快速复杂运动模式下容易丢失跟踪，以及在低光照和移动平台等挑战性条件下视觉信息不可靠。
*   <strong>推动未来研究：</strong> LaMAria数据集和基准的发布为多传感器SLAM在不受控自我中心记录下的发展提供了新的研究方向和评估工具。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>伪真值精度：</strong> 尽管论文生成的密集伪真值姿态比现有数据集更准确，但其精度仍不如测量级的控制点，在某些情况下（如移动平台场景）其准确性保证有限。
*   <strong>视觉信息不可靠：</strong> 在某些最具挑战性的场景（如移动平台），视觉信息可能不可靠，导致视觉惯性系统难以初始化或跟踪失败。
*   <strong>现有系统对在线校准支持不足：</strong> 大多数评估的学术系统不支持在线内参校准，只实现了简单的相机模型，这限制了它们在时变校准场景下的性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>时变校准的在线优化：</strong> 开发能够适应可穿戴设备全天候使用特性的时变校准在线优化方法。
*   <strong>回环检测和VI捆集调整：</strong> 改进回环检测和视觉惯性捆集调整，以减少开环预测中的里程计漂移。
*   <strong>鲁棒的异常值去除和跟踪丢失处理：</strong> 针对移动平台等场景，开发更鲁棒的异常值去除策略和更好的跟踪丢失处理机制。
*   <strong>基于机器学习的图像匹配和点跟踪：</strong> 利用在大规模数据集上训练的机器学习模型，提升图像匹配和点跟踪的性能。</p>
<hr />
<p>这篇论文通过引入一个具有高精度真值和多样化挑战场景的大规模自我中心数据集，为VI-SLAM领域的研究做出了重要贡献。它不仅揭示了现有SOTA系统在真实世界自我中心数据下的局限性，也为未来的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
paper, we introduce a new dataset and benchmark for visual-inertial SLAM with
egocentric, multi-modal data.</li>
<li>We show that state-of-the-art systems
developed by academia are not robust to these challenges and we identify
components that are responsible for this.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26639v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26639v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26631v1'></a></p>
<h2 id="learning-generalizable-shape-completion-with-sim3-equivariance"><a href="https://arxiv.org/abs/2509.26631v1">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></h2>
<p><strong>Authors:</strong> Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>3D shape completion methods typically assume scans are pre-aligned to a
canonical frame. This leaks pose and scale cues that networks may exploit to
memorize absolute positions rather than inferring intrinsic geometry. When such
alignment is absent in real data, performance collapses. We argue that robust
generalization demands architectural equivariance to the similarity group,
SIM(3), so the model remains agnostic to pose and scale. Following this
principle, we introduce the first SIM(3)-equivariant shape completion network,
whose modular layers successively canonicalize features, reason over
similarity-invariant geometry, and restore the original frame. Under a
de-biased evaluation protocol that removes the hidden cues, our model
outperforms both equivariant and augmentation baselines on the PCN benchmark.
It also sets new cross-domain records on real driving and indoor scans,
lowering minimal matching distance on KITTI by 17% and Chamfer distance <script type="math/tex">\ell1</script>
on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol
still outperforms competitors under their biased settings. These results
establish full SIM(3) equivariance as an effective route to truly generalizable
shape completion. Project page: https://sime-completion.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu撰写的论文“Learning Generalizable Shape Completion with SIM(3) Equivariance”的全面摘要。</p>
<hr />
<h3 id="learning-generalizable-shape-completion-with-sim3-equivariance_1">论文摘要：Learning Generalizable Shape Completion with SIM(3) Equivariance</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前3D形状补全方法普遍存在一个核心问题：它们通常假设输入扫描已预先对齐到规范坐标系。这种做法导致网络倾向于记忆绝对位置和尺度信息，而非学习形状的内在几何特性。当在实际数据中缺乏这种预对齐时，模型的性能会急剧下降，导致泛化能力差。因此，论文旨在解决如何实现对任意姿态和尺度（即SIM(3)变换）具有鲁棒泛化能力的3D形状补全，从而避免对齐偏差。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
为了解决上述问题，论文提出了首个完全SIM(3)等变（SIM(3)-equivariant）的形状补全网络，其核心创新点在于：
*   <strong>SIM(3)等变架构：</strong> 论文主张鲁棒的泛化能力需要模型对相似变换群SIM(3)（包括旋转、平移和尺度）具有等变性。为此，作者设计了一个模块化的网络架构，其每一层都确保了SIM(3)等变性。
*   <strong>模块化设计：</strong> 网络由三个连续阶段组成：
    1.  <strong>特征规范化（Canonicalization）：</strong> 将特征转换为平移和尺度不变的形式，通过扩展层归一化（Layer Normalization）显式地去除全局平移和尺度信息，同时保持旋转等变性。
    2.  <strong>相似性不变几何推理（Shape Reasoning）：</strong> 在规范化特征空间中，通过SIM(3)不变的注意力机制进行形状推理，确保模型学习到内在几何。
    3.  <strong>变换恢复（Transform Restoration）：</strong> 引入一个轻量级的变换恢复路径，通过残差连接将姿态和尺度信息重新注入到特征中，以将补全后的形状恢复到原始传感器坐标系。
*   <strong>去偏置评估协议：</strong> 论文建立了一个严格的评估协议，移除了传统基准测试中存在的隐藏姿态和尺度线索，以公平地测试模型的真实泛化能力。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>超越基线：</strong> 在去偏置评估协议下，该模型在PCN基准测试上显著优于现有的等变和数据增强基线方法。
*   <strong>跨域泛化能力：</strong> 在真实驾驶场景（KITTI）和室内场景（OmniObject3D）扫描数据上，模型取得了新的跨域泛化记录，将KITTI上的最小匹配距离（MMD）降低了17%，将OmniObject3D上的Chamfer距离<script type="math/tex">\ell1</script>降低了14%。
*   <strong>鲁棒性：</strong> 即使在更严格的协议下（即没有预对齐），该模型仍能超越在偏置设置下（即有预对齐）训练的竞争对手。这证明了完全SIM(3)等变性是实现真正可泛化形状补全的有效途径。
*   <strong>对齐偏差的揭示：</strong> 论文通过实验证实了现有方法中姿态和尺度偏差的存在，并强调了SIM(3)等变性对于“野外”泛化的重要性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>姿态和尺度依赖特征的丢失：</strong> 尽管模型对任意相似变换具有鲁棒性，但它也可能丢弃当物体始终出现在规范坐标系中时有用的线索。例如，没有可见腿的椅子背面可能被误认为是沙发。
*   <strong>部分观测的对称性：</strong> 框架中的等变性是针对单个部分扫描定义的。对于同一物体的不同部分观测，初始化变异性无法完全消除，因此跨视图对称性必须隐式学习。
*   <strong>复杂场景和可动部件：</strong> 目前的方法擅长补全静态形状，但尚未明确处理独立移动的子部件（如人体关节、机械臂或多物体场景）。
*   <strong>计算开销：</strong> 向量值特征和完全等变模块相比标量值层会带来三倍的计算开销，导致运行时延迟较高，可能限制实时或资源受限部署。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   将当前框架扩展到多物体和大规模场景建模，以处理更复杂的场景。
*   结合特定类别形状先验或允许多个局部变换，以更好地处理可动部件和复杂场景。
*   进一步优化计算效率，以降低等变模块带来的运行时开销，使其适用于实时应用。</p>
<hr />
<p>这篇论文通过引入SIM(3)等变性，为3D形状补全领域带来了重要的突破，解决了长期存在的对齐偏差问题，并显著提升了模型在真实世界场景中的泛化能力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Following this
principle, we introduce the first SIM(3)-equivariant shape completion network,
whose modular layers successively canonicalize features, reason over
similarity-invariant geometry, and restore the original frame.</li>
<li>Under a
de-biased evaluation protocol that removes the hidden cues, our model
outperforms both equivariant and augmentation baselines on the PCN benchmark.</li>
<li>It also sets new cross-domain records on real driving and indoor scans,
lowering minimal matching distance on KITTI by 17% and Chamfer distance <script type="math/tex">\ell1</script>
on OmniObject3D by 14%.</li>
<li>Perhaps surprisingly, ours under the stricter protocol
still outperforms competitors under their biased settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26631v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26631v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26625v1'></a></p>
<h2 id="learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training"><a href="https://arxiv.org/abs/2509.26625v1">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a></h2>
<p><strong>Authors:</strong> Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Large Language Models (LLMs), despite being trained on text alone,
surprisingly develop rich visual priors. These priors allow latent visual
capabilities to be unlocked for vision tasks with a relatively small amount of
multimodal data, and in some cases, to perform visual tasks without ever having
seen an image. Through systematic analysis, we reveal that visual priors-the
implicit, emergent knowledge about the visual world acquired during language
pre-training-are composed of separable perception and reasoning priors with
unique scaling trends and origins. We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.
This reasoning prior acquired from language pre-training is transferable and
universally applicable to visual reasoning. In contrast, a perception prior
emerges more diffusely from broad corpora, and perception ability is more
sensitive to the vision encoder and visual instruction tuning data. In
parallel, text describing the visual world proves crucial, though its
performance impact saturates rapidly. Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training. Our findings are grounded in over 100 controlled
experiments consuming 500,000 GPU-hours, spanning the full MLLM construction
pipeline-from LLM pre-training to visual alignment and supervised multimodal
fine-tuning-across five model scales, a wide range of data categories and
mixtures, and multiple adaptation setups. Along with our main findings, we
propose and investigate several hypotheses, and introduce the Multi-Level
Existence Bench (MLE-Bench). Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Junlin Han等人撰写的论文“Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training”的全面摘要：</p>
<p><strong>1. 论文主要问题或研究问题</strong>
该论文旨在系统地探究大型语言模型（LLMs）在仅通过文本训练后，如何出人意料地发展出丰富的视觉先验知识。具体来说，它试图揭示这些视觉先验的构成、来源、演变规律，以及如何利用这些先验来构建更强大的多模态LLMs（MLLMs）。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>视觉先验的结构分解：</strong> 论文首次系统地将LLM的视觉先验分解为可分离的“感知先验”和“推理先验”，并揭示了它们独特的扩展趋势和起源。
*   <strong>数据中心化预训练配方：</strong> 基于对视觉先验起源的深入分析，论文提出了一种数据中心化的预训练配方，用于有意识地培养视觉感知LLMs，并在1T token规模的预训练中得到了验证。
*   <strong>多层次存在基准（MLE-Bench）：</strong> 引入了一个新的基准测试，专门用于细粒度评估模型的感知能力，尤其是在不同尺寸对象（小、中、大）上的感知性能。
*   <strong>盲视觉指令微调（Blind Visual Instruction Tuning）：</strong> 提出了一种“盲视觉指令微调”技巧，作为提高视觉适应性的实用工具，并揭示了模型如何通过语言“捷径”解决视觉任务。
*   <strong>系统性消融研究：</strong> 通过超过100个受控实验（消耗500,000 GPU小时），涵盖LLM预训练到视觉对齐和监督多模态微调的整个MLLM构建流程，跨越五种模型规模、多种数据类别和混合比例以及多种适应设置，对视觉先验的起源进行了深入分析。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>视觉先验的构成与起源：</strong> 论文发现LLM的潜在视觉推理能力主要通过推理中心数据（如代码、数学、学术论文）的预训练逐步发展和扩展。这种推理先验具有可迁移性和普遍适用性。相比之下，感知先验更广泛地从通用语料库中出现，并且感知能力对视觉编码器和视觉指令微调数据更敏感。描述视觉世界的文本虽然重要，但其性能影响迅速饱和。
*   <strong>数据混合策略：</strong> 实验表明，最大化MLLM VQA性能的最佳数据混合是严重偏向推理中心内容，但同时包含必要的视觉世界知识。在1T token规模的预训练中，平衡模型在语言能力保持竞争力的同时，在所有视觉任务上均优于语言偏好模型。
*   <strong>感知先验的尺度依赖性：</strong> MLE-Bench的评估结果显示，感知先验确实具有尺度依赖性，其优势在感知中小尺寸对象时最为显著，表明多样化的网络爬取数据对于获取感知先验至关重要。
*   <strong>推理能力的跨模态通用性：</strong> 论文通过定性分析和评估模型推理质量的实验证实，LLM从文本中获得的推理能力是模态无关的，可以有效地迁移到视觉问题解决中，表现出更强的逻辑性和更深的推理深度。
*   <strong>语言数据结构与视觉对齐：</strong> 结构化语言数据（如代码和数学）的比例增加，通常能提高LLM-视觉对齐分数，表明学习抽象结构有助于形成更一致的潜在空间，但这种趋势并非单调线性。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>架构限制：</strong> 研究主要集中在适配器风格的MLLM架构，其发现可能无法完全推广到其他方法，如离散视觉token化或端到端联合训练视觉和语言组件的模型。
*   <strong>安全与伦理问题：</strong> 论文未深入探讨这些学习到的视觉先验可能带来的安全和伦理影响，例如文本语料库中存在的社会偏见、刻板印象是否会通过视觉先验体现在下游MLLM的有害生成或分类行为中。
*   <strong>模态限制：</strong> 研究仅限于静态图像领域，未探索动态模态（如视频理解）的视觉先验。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>其他MLLM架构的视觉先验研究：</strong> 探索离散视觉token化或端到端联合训练模型中视觉先验的形成和利用动态。
*   <strong>视觉先验的安全与伦理审计：</strong> 对LLM中学习到的视觉先验进行彻底的公平性和安全性审计，以识别和缓解潜在的偏见和有害行为。
*   <strong>动态模态的视觉先验探索：</strong> 研究不同文本源如何促进视频理解中时间推理、动作识别和因果关系等先验的形成。
*   <strong>抽象结构与语义基础的相互作用：</strong> 进一步探究抽象结构和语义基础在形成跨模态表示中的精确相互作用。
*   <strong>更精细的视觉先验结构：</strong> 进一步细化感知先验的内部结构，例如其在不同视觉属性（颜色、形状、纹理）上的表现。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.</li>
<li>Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training.</li>
<li>Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26625v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26625v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26621v1'></a></p>
<h2 id="hart-human-aligned-reconstruction-transformer"><a href="https://arxiv.org/abs/2509.26621v1">HART: Human Aligned Reconstruction Transformer</a></h2>
<p><strong>Authors:</strong> Xiyi Chen, Shaofei Wang, Marko Mihajlovic, Taewon Kang, Sergey Prokudin, Ming Lin</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce HART, a unified framework for sparse-view human reconstruction.
Given a small set of uncalibrated RGB images of a person as input, it outputs a
watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat
representation for photorealistic novel-view rendering. Prior methods for
clothed human reconstruction either optimize parametric templates, which
overlook loose garments and human-object interactions, or train implicit
functions under simplified camera assumptions, limiting applicability in real
scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body
correspondences, and employs an occlusion-aware Poisson reconstruction to
recover complete geometry, even in self-occluded regions. These predictions
also align with a parametric SMPL-X body model, ensuring that reconstructed
geometry remains consistent with human structure while capturing loose clothing
and interactions. These human-aligned meshes initialize Gaussian splats to
further enable sparse-view rendering. While trained on only 2.3K synthetic
scans, HART achieves state-of-the-art results: Chamfer Distance improves by
18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for
SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on
a wide range of datasets. These results suggest that feed-forward transformers
can serve as a scalable model for robust human reconstruction in real-world
settings. Code and models will be released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xiyi Chen等人撰写的论文“HART: Human Aligned Reconstruction Transformer”的全面摘要。</p>
<p><strong>论文摘要：HART: Human Aligned Reconstruction Transformer</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决从少量未校准的RGB图像中进行稀疏视角人体重建的挑战。现有的方法在处理宽松衣物、人-物体交互以及在真实场景中应用时存在局限性，例如优化参数模板会忽略衣物细节，或在简化相机假设下训练隐式函数。核心问题是如何在存在自遮挡的情况下，从有限的输入图像中准确、完整地重建出穿着衣物的人体几何形状，并实现逼真的新视角渲染。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
HART（Human Aligned Reconstruction Transformer）是一个统一的框架，其主要创新包括：
*   <strong>统一框架：</strong> HART能够同时重建出穿着衣物的水密网格、对齐的SMPL-X身体网格，以及用于逼真新视角渲染的高斯溅射（Gaussian-splat）表示。
*   <strong>像素级3D属性预测：</strong> HART预测每像素的3D点图、法线和身体对应关系，这比传统方法更精细，能够更好地捕捉细节和自遮挡区域。
*   <strong>遮挡感知泊松重建（Occlusion-aware Poisson Reconstruction）：</strong> 论文引入了一个3D U-Net来细化指示网格，以解决点图方法在处理自遮挡区域时的局限性，从而恢复完整且水密的几何形状。
*   <strong>人体对齐的几何重建：</strong> 预测结果与参数化的SMPL-X身体模型对齐，确保重建的几何形状与人体结构保持一致，同时捕捉宽松衣物和交互。
*   <strong>几何引导的新视角合成：</strong> 重建的人体对齐网格被用作高斯溅射的初始化和正则化，从而实现稀疏视角渲染。</p>
<p><strong>3. 主要结果及其意义：</strong>
尽管仅在2.3K合成扫描数据上进行训练，HART仍取得了最先进的（state-of-the-art）结果：
*   <strong>穿着衣物网格重建：</strong> Chamfer Distance（倒角距离）提高了18-23%。
*   <strong>SMPL-X估计：</strong> PA-V2V（顶点到顶点误差）降低了6-27%。
*   <strong>新视角合成：</strong> LPIPS（感知距离）在各种数据集上降低了15-27%。
这些结果表明，前馈Transformer可以作为一种可扩展的模型，用于在真实世界环境中进行鲁棒的人体重建。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>细节恢复：</strong> 重建结果在精细尺度细节（如手指、头发）方面仍有不足，这可能受限于指示网格的分辨率。
*   <strong>稀疏视角和挑战性光照：</strong> 在非常稀疏的视角（例如3个视角）或挑战性光照条件下，渲染质量会显著下降。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>分层或多尺度架构：</strong> 探索分层或多尺度架构以提高细节恢复能力。
*   <strong>扩散先验：</strong> 利用扩散先验来改进自遮挡区域的渲染。
*   <strong>基于视频的训练：</strong> 采用基于视频的训练方法，以增强时间一致性并实现可动画的重建。</p>
<p>总而言之，HART论文提出了一种新颖且统一的框架，通过结合Transformer的强大预测能力、遮挡感知几何重建和SMPL-X模型对齐，显著提升了稀疏视角人体重建的质量和鲁棒性。其在多个任务上的优异表现，特别是其在真实世界图像上的泛化能力，为未来的人体3D重建研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce HART, a unified framework for sparse-view human reconstruction.</li>
<li>Given a small set of uncalibrated RGB images of a person as input, it outputs a
watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat
representation for photorealistic novel-view rendering.</li>
<li>While trained on only 2.3K synthetic
scans, HART achieves state-of-the-art results: Chamfer Distance improves by
18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for
SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on
a wide range of datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26621v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26621v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26618v1'></a></p>
<h2 id="da2-depth-anything-in-any-direction"><a href="https://arxiv.org/abs/2509.26618v1">DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</a></h2>
<p><strong>Authors:</strong> Haodong Li, Wangguangdong Zheng, Jing He, Yuhao Liu, Xin Lin, Xin Yang, Ying-Cong Chen, Chunchao Guo</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Panorama has a full FoV (360<script type="math/tex">^\circ\times</script>180<script type="math/tex">^\circ</script>), offering a more
complete visual description than perspective images. Thanks to this
characteristic, panoramic depth estimation is gaining increasing traction in 3D
vision. However, due to the scarcity of panoramic data, previous methods are
often restricted to in-domain settings, leading to poor zero-shot
generalization. Furthermore, due to the spherical distortions inherent in
panoramas, many approaches rely on perspective splitting (e.g., cubemaps),
which leads to suboptimal efficiency. To address these challenges, we propose
<script type="math/tex">\textbf{DA}</script>
<script type="math/tex">^{\textbf{2}}</script>: <script type="math/tex">\textbf{D}</script>epth <script type="math/tex">\textbf{A}</script>nything in
<script type="math/tex">\textbf{A}</script>ny <script type="math/tex">\textbf{D}</script>irection, an accurate, zero-shot generalizable, and
fully end-to-end panoramic depth estimator. Specifically, for scaling up
panoramic data, we introduce a data curation engine for generating high-quality
panoramic depth data from perspective, and create <script type="math/tex">\sim</script>543K panoramic
RGB-depth pairs, bringing the total to <script type="math/tex">\sim</script>607K. To further mitigate the
spherical distortions, we present SphereViT, which explicitly leverages
spherical coordinates to enforce the spherical geometric consistency in
panoramic image features, yielding improved performance. A comprehensive
benchmark on multiple datasets clearly demonstrates DA<script type="math/tex">^{2}</script>'s SoTA
performance, with an average 38% improvement on AbsRel over the strongest
zero-shot baseline. Surprisingly, DA<script type="math/tex">^{2}</script> even outperforms prior in-domain
methods, highlighting its superior zero-shot generalization. Moreover, as an
end-to-end solution, DA<script type="math/tex">^{2}</script> exhibits much higher efficiency over fusion-based
approaches. Both the code and the curated panoramic data will be released.
Project page: https://depth-any-in-any-dir.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Haodong Li等人撰写的论文“DA<script type="math/tex">^2</script>: Depth Anything in Any Direction”的全面摘要。</p>
<hr />
<p><strong>论文摘要：DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决全景深度估计领域面临的两个核心挑战：
1. <strong>全景数据稀缺性导致泛化能力差：</strong> 现有全景深度估计方法由于缺乏大规模高质量的全景数据，往往局限于特定领域，导致零样本泛化能力不足。
2. <strong>球形畸变带来的效率和准确性问题：</strong> 全景图像固有的球形畸变（尤其是在两极附近）使得许多方法依赖于透视分割（如立方体贴图），这不仅效率低下，也可能导致几何不一致。</p>
<p><strong>2. 关键创新或方法贡献</strong>
为了解决上述问题，论文提出了DA<script type="math/tex">^2</script>（Depth Anything in Any Direction），一个准确、零样本泛化且完全端到端的全景深度估计器，其主要创新包括：</p>
<ul>
<li><strong>全景数据策展引擎（Panoramic Data Curation Engine）：</strong> 针对全景数据稀缺问题，DA<script type="math/tex">^2</script>引入了一个数据策展引擎，能够从现有的高质量透视深度数据中生成全景深度数据。通过透视到等距柱状投影（P2E）和全景图像外绘（使用FLUX-I2P模型），该引擎生成了约543K新的全景RGB-深度对，使总数据集达到约607K，极大地扩展了训练数据量和多样性。</li>
<li><strong>SphereViT架构：</strong> 为缓解球形畸变的影响，DA<script type="math/tex">^2</script>提出了SphereViT作为其主要骨干网络。SphereViT通过显式利用全景图像的球形坐标（方位角和极角）来构建球形嵌入（Spherical Embedding）。这些嵌入通过交叉注意力机制与图像特征融合，从而在全景图像特征中强制执行球形几何一致性，生成畸变感知（distortion-aware）的表示，显著提高了性能。</li>
<li><strong>综合基准测试：</strong> 论文构建了一个全面的基准测试，比较了零样本/域内、全景/透视方法，为全景深度估计领域提供了统一的评估框架。</li>
<li><strong>端到端高效解决方案：</strong> DA<script type="math/tex">^2</script>作为一个完全端到端的解决方案，相比于基于融合的方法，展现出更高的效率。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
DA<script type="math/tex">^2</script>在多个数据集上的综合基准测试中取得了最先进（SoTA）的性能，具体表现为：</p>
<ul>
<li><strong>卓越的零样本泛化能力：</strong> DA<script type="math/tex">^2</script>在最强的零样本基线上，AbsRel指标平均提高了38%。令人惊讶的是，DA<script type="math/tex">^2</script>甚至超越了先前的域内方法，这凸显了其卓越的零样本泛化能力。</li>
<li><strong>几何保真度显著提升：</strong> SphereViT通过强制球形几何一致性，使得DA<script type="math/tex">^2</script>能够生成具有出色几何保真度的深度估计，重建的3D结构展现出清晰的几何细节，并在不同场景中表现出鲁棒性。</li>
<li><strong>数据规模化的重要性：</strong> 实验结果（如缩放定律曲线）清晰地表明，随着从透视数据转换而来的全景深度数据量的增加，DA<script type="math/tex">^2</script>的性能稳步提升，验证了数据策展引擎的有效性。</li>
<li><strong>高效推理：</strong> 作为端到端方法，DA<script type="math/tex">^2</script>在推理效率上远超基于融合的方法。</li>
</ul>
<p>这些结果表明，通过大规模全景数据和显式建模球形几何，可以实现高质量和鲁棒的360°×180°几何估计，为沉浸式3D场景创建、AR/VR、机器人仿真和物理仿真等应用铺平了道路。</p>
<p><strong>4. 论文中提到的局限性</strong>
尽管DA<script type="math/tex">^2</script>表现出色，论文也提到了其存在的局限性：</p>
<ul>
<li><strong>分辨率限制：</strong> 训练分辨率（1024x512）低于更高清晰度格式（如2K或4K），可能导致DA<script type="math/tex">^2</script>偶尔会遗漏精细细节。</li>
<li><strong>GT深度可用性：</strong> 策展的透视数据在球形空间中仅提供部分可用的GT深度，这可能导致DA<script type="math/tex">^2</script>在预测中出现可见的接缝，尤其是在全景图像的左右边界处，这些边界理想情况下应无缝对齐。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
虽然论文没有明确列出未来的研究方向，但从其局限性中可以推断出以下几点：</p>
<ul>
<li><strong>更高分辨率的全景深度估计：</strong> 探索如何将DA<script type="math/tex">^2</script>扩展到更高分辨率的输入，以捕捉更精细的几何细节。</li>
<li><strong>改进全景边界的无缝对齐：</strong> 研究新的方法或损失函数，以更好地处理全景图像的左右边界，确保预测的深度图在这些区域无缝对齐。</li>
<li><strong>更完善的GT深度生成：</strong> 进一步优化数据策展引擎，特别是外绘深度图的绝对精度，以减少对GT深度可用性的依赖。</li>
<li><strong>探索其他球形表示：</strong> 除了等距柱状投影，可以探索其他球形表示（如立方体贴图的更高效集成或更复杂的网格表示），并将其与SphereViT结合，以进一步优化性能。</li>
<li><strong>多模态融合：</strong> 结合其他模态（如LiDAR、惯性测量单元等）来进一步提高全景深度估计的精度和鲁棒性。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose
<script type="math/tex">\textbf{DA}</script>
<script type="math/tex">^{\textbf{2}}</script>: <script type="math/tex">\textbf{D}</script>epth <script type="math/tex">\textbf{A}</script>nything in
<script type="math/tex">\textbf{A}</script>ny <script type="math/tex">\textbf{D}</script>irection, an accurate, zero-shot generalizable, and
fully end-to-end panoramic depth estimator.</li>
<li>Specifically, for scaling up
panoramic data, we introduce a data curation engine for generating high-quality
panoramic depth data from perspective, and create <script type="math/tex">\sim</script>543K panoramic
RGB-depth pairs, bringing the total to <script type="math/tex">\sim</script>607K.</li>
<li>To further mitigate the
spherical distortions, we present SphereViT, which explicitly leverages
spherical coordinates to enforce the spherical geometric consistency in
panoramic image features, yielding improved performance.</li>
<li>Surprisingly, DA<script type="math/tex">^{2}</script> even outperforms prior in-domain
methods, highlighting its superior zero-shot generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26618v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26618v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26604v1'></a></p>
<h2 id="video-object-segmentation-aware-audio-generation"><a href="https://arxiv.org/abs/2509.26604v1">Video Object Segmentation-Aware Audio Generation</a></h2>
<p><strong>Authors:</strong> Ilpo Viertola, Vladimir Iashin, Esa Rahtu</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing multimodal audio generation models often lack precise user control,
which limits their applicability in professional Foley workflows. In
particular, these models focus on the entire video and do not provide precise
methods for prioritizing a specific object within a scene, generating
unnecessary background sounds, or focusing on the wrong objects. To address
this gap, we introduce the novel task of video object segmentation-aware audio
generation, which explicitly conditions sound synthesis on object-level
segmentation maps. We present SAGANet, a new multimodal generative model that
enables controllable audio generation by leveraging visual segmentation masks
along with video and textual cues. Our model provides users with fine-grained
and visually localized control over audio generation. To support this task and
further research on segmentation-aware Foley, we propose Segmented Music Solos,
a benchmark dataset of musical instrument performance videos with segmentation
information. Our method demonstrates substantial improvements over current
state-of-the-art methods and sets a new standard for controllable,
high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are
available at https://saganet.notion.site</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ilpo Viertola, Vladimir Iashin, Esa Rahtu撰写的论文“Video Object Segmentation-Aware Audio Generation”的全面摘要。</p>
<hr />
<h3 id="video-object-segmentation-aware-audio-generation_1">论文《Video Object Segmentation-Aware Audio Generation》摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有多模态音频生成模型在专业拟音（Foley）工作流程中缺乏精确的用户控制。这些模型通常关注整个视频，无法提供精确的方法来优先处理场景中的特定对象、避免生成不必要的背景声音或错误地聚焦于非目标对象。这限制了它们在需要精细控制的音频合成场景中的应用。因此，该论文旨在解决如何实现对视频中特定对象声音的精细、视觉局部化控制，从而生成高质量、高保真的拟音。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，论文提出了以下关键创新和贡献：</p>
<ul>
<li><strong>新型任务：视频对象分割感知音频生成（Video Object Segmentation-aware Audio Generation）</strong>：该任务明确地将声音合成与对象级别的分割图进行条件化，从而实现对特定对象声音的生成。</li>
<li><strong>SAGANet 模型</strong>：提出了一种新的多模态生成模型SAGANet，它通过利用视觉分割掩码、视频和文本线索，实现可控的音频生成。SAGANet在预训练的MMAudio模型基础上，引入了一个自监督控制模块，该模块融合了全局和局部视觉信息，并通过门控交叉注意力（Gated Cross-Attention）机制实现精细的集成。</li>
<li><strong>Focal Prompt 机制</strong>：为了提供详细信息并结合全局上下文和分割信息，SAGANet引入了Focal Prompt，它包含原始未修改的视觉流及其掩码流，以及围绕感兴趣区域裁剪的视频流及其掩码流，从而提供全局概览和目标区域的详细视图。</li>
<li><strong>Localized Vision Backbone with Temporal Mask Embedding</strong>：通过将视频和掩码流嵌入共享的时空表示中，并应用可学习的位置编码，SAGANet能够捕获精细的空间线索及其时间动态，这对于生成语义对齐的音频至关重要。</li>
<li><strong>Segmented Music Solos 数据集</strong>：为了支持这项新任务和进一步研究分割感知拟音，论文构建了一个基准数据集，包含带有分割信息的乐器演奏视频。该数据集通过结合Solos、AVSBench和MUSIC21等数据集，并经过严格的视觉和听觉验证以及掩码生成流程（利用GroundedSAM2框架），确保了高质量的带声音对象分割图和高音频-视频对应关系。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
SAGANet在Segmented Music Solos数据集上的实验结果表明，它在所有评估指标上均显著优于基线模型MMAudio：</p>
<ul>
<li><strong>分布匹配</strong>：SAGANet在Fréchet Distance (FD) 和Kullback-Leibler Distance (KL) 等指标上表现更好，表明生成音频的分布更接近真实音频。</li>
<li><strong>音频质量</strong>：Inception Score (IS) 评分更高，表明生成的音频具有更高的客观质量和多样性。</li>
<li><strong>语义对齐</strong>：ImageBind (IB-score) 评分更高，证明模型能够更好地聚焦于正确的对象，实现更强的语义相似性。</li>
<li><strong>时间对齐</strong>：DeSync（绝对偏移预测）指标显著降低，表明SAGANet在时间同步方面表现出色，解决了现有模型在复杂场景中难以对齐的问题。</li>
</ul>
<p>这些结果的意义在于，SAGANet通过引入对象级分割控制，极大地提升了拟音合成的精确性和可控性，使其在多源复杂场景中能够有效聚焦于目标对象，即使在仅使用单源样本进行训练的情况下也能泛化到多源场景。LoRA微调进一步提升了模型的性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提及的局限性主要包括：</p>
<ul>
<li><strong>数据生成中的手动标注限制</strong>：在测试数据中，目标对象的定位坐标是手动提供的，尽管这产生了更连贯的掩码，但有限的资源阻碍了在训练数据中手动标注所有目标对象。</li>
<li><strong>基线模型在复杂场景中的局限性</strong>：基线MMAudio模型在复杂场景中，仅通过文本描述和视觉输入，难以提供足够强的指导来生成时间上和语义上对齐的音频，因为它会聚焦于场景中的其他乐器。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文为未来的研究方向奠定了基础，包括：</p>
<ul>
<li><strong>更用户友好的拟音模型开发</strong>：通过提供更精细、视觉局部化的控制，SAGANet为开发更易于使用和更强大的拟音模型铺平了道路，这些模型可以更好地集成到视频后期制作工作流程中。</li>
<li><strong>扩展到更广泛的场景和对象类型</strong>：目前数据集主要关注乐器演奏，未来可以探索将分割感知音频生成扩展到更广泛的日常场景和对象类型。</li>
<li><strong>减少对手动标注的依赖</strong>：进一步研究如何减少在数据生成过程中对目标对象手动标注的依赖，例如通过更先进的零样本或少样本分割方法。</li>
<li><strong>探索其他控制信号</strong>：除了分割掩码、视频和文本，还可以探索其他模态或控制信号，以进一步增强音频生成的精细控制能力。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
this gap, we introduce the novel task of video object segmentation-aware audio
generation, which explicitly conditions sound synthesis on object-level
segmentation maps.</li>
<li>We present SAGANet, a new multimodal generative model that
enables controllable audio generation by leveraging visual segmentation masks
along with video and textual cues.</li>
<li>To support this task and
further research on segmentation-aware Foley, we propose Segmented Music Solos,
a benchmark dataset of musical instrument performance videos with segmentation
information.</li>
<li>Our method demonstrates substantial improvements over current
state-of-the-art methods and sets a new standard for controllable,
high-fidelity Foley synthesis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26604v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26604v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26555v1'></a></p>
<h2 id="stable-cinemetrics-structured-taxonomy-and-evaluation-for-professional-video-generation"><a href="https://arxiv.org/abs/2509.26555v1">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a></h2>
<p><strong>Authors:</strong> Agneet Chatterjee, Rahim Entezari, Maksym Zhuravinskyi, Maksim Lapin, Reshinth Adithyan, Amit Raj, Chitta Baral, Yezhou Yang, Varun Jampani</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in video generation have enabled high-fidelity video
synthesis from user provided prompts. However, existing models and benchmarks
fail to capture the complexity and requirements of professional video
generation. Towards that goal, we introduce Stable Cinemetrics, a structured
evaluation framework that formalizes filmmaking controls into four
disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.
Together, these taxonomies define 76 fine-grained control nodes grounded in
industry practices. Using these taxonomies, we construct a benchmark of prompts
aligned with professional use cases and develop an automated pipeline for
prompt categorization and question generation, enabling independent evaluation
of each control dimension. We conduct a large-scale human study spanning 10+
models and 20K videos, annotated by a pool of 80+ film professionals. Our
analysis, both coarse and fine-grained reveal that even the strongest current
models exhibit significant gaps, particularly in Events and Camera-related
controls. To enable scalable evaluation, we train an automatic evaluator, a
vision-language model aligned with expert annotations that outperforms existing
zero-shot baselines. SCINE is the first approach to situate professional video
generation within the landscape of video generative models, introducing
taxonomies centered around cinematic controls and supporting them with
structured evaluation pipelines and detailed analyses to guide future research.</p>
<p><strong>Analysis:</strong></p>
<p>以下是对Agneet Chatterjee等人撰写的论文“Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
当前视频生成模型和基准未能捕捉专业视频生成所需的复杂性和要求。该研究旨在解决一个核心问题：“当前的视频生成模型是否已为专业用途做好准备？”换句话说，论文旨在弥合休闲探索性视频合成与支持专业级、可控电影输出的媒体之间的差距。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>Stable Cinemetrics (SCINE) 框架：</strong> 引入了一个结构化的评估框架，将电影制作控制形式化为四个解耦的、分层的分类法：<strong>设置 (Setup)、事件 (Event)、灯光 (Lighting) 和摄像机 (Camera)</strong>。这些分类法共定义了76个基于行业实践的细粒度控制节点。
*   <strong>专业对齐的基准提示：</strong> 利用这些分类法构建了一个与专业用例对齐的提示基准，包括“故事驱动型”和“视觉阐述型”两种提示类型，以模拟实际的电影制作流程。
*   <strong>自动化评估流程：</strong> 开发了一个自动化流程，用于提示分类和问题生成，从而能够独立评估每个控制维度。
*   <strong>大规模人工研究：</strong> 对10多个模型和2万个视频进行了大规模人工研究，由80多位电影专业人士进行标注，确保了评估的高质量和专业性。
*   <strong>自动评估器：</strong> 训练了一个视觉-语言模型（VLM）作为自动评估器，该模型与专家标注对齐，性能优于现有的零样本基线，实现了可扩展的评估。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>当前模型的显著差距：</strong> 粗粒度和细粒度分析均显示，即使是最强大的当前模型也存在显著差距，尤其是在<strong>事件 (Events)</strong> 和<strong>摄像机 (Camera)</strong> 相关控制方面。
*   <strong>不同控制维度的性能差异：</strong> 模型在“设置 (Setup)”和“灯光 (Lighting)”方面表现相对较好，但在“事件 (Events)”和“摄像机 (Camera)”方面表现较弱。例如，模型在处理原子动作方面表现良好，但在因果和重叠事件方面表现不佳；在灯光方面，模型在自然光源（如阳光、频闪）方面表现较好，但在HMI、荧光灯和钨丝灯方面表现较差。
*   <strong>VLM评估器的有效性：</strong> 训练的VLM在与人类标注对齐方面表现出一致性，优于零样本基线，证明了其在专业视频生成评估中的可扩展性潜力。
*   <strong>对未来研究的指导：</strong> SCINE是第一个将专业视频生成置于视频生成模型领域的方法，通过引入以电影控制为中心的分类法、结构化评估流程和详细分析来指导未来的研究方向。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>分类法的范围：</strong> 尽管分类法是与领域专家协商开发的，但其范围受限于合作者网络。电影制作术语和解释性细微差别因地区和文化而异，更广泛的专家多样性将有助于纳入全球电影控制。
*   <strong>某些节点的抽象：</strong> 某些分类法节点（例如色温、ISO）被抽象化以进行评估，因为标注者难以始终感知细粒度值。
*   <strong>提示生成中的LLM偏见：</strong> 提示生成依赖于LLM，其专有性质和潜在偏见可能会影响提示的语言和结构。
*   <strong>VLM评估的计算和数据限制：</strong> 零样本VLM评估受限于计算和数据资源，限制了实验的规模和范围。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展分类法：</strong> 进一步扩展分类法，纳入更广泛的全球电影控制和更细粒度的控制值。
*   <strong>VLM的应用：</strong> 将VLM应用于分析视频数据集的电影多样性或作为视频字幕的结构。
*   <strong>模型改进：</strong> 解决当前模型在事件和摄像机控制方面的显著差距，特别是通过微调和定制化，使生成模型更接近实际生产需求。
*   <strong>更深入的探索：</strong> 鼓励在电影制作和视频生成模型交叉领域进行更深入的探索，促进艺术家和模型之间更紧密的合作。</p>
<p>总而言之，这篇论文通过引入Stable Cinemetrics框架，为专业视频生成提供了一个急需的结构化评估方法，揭示了当前模型在满足专业级电影制作要求方面的不足，并为未来的研究和模型开发指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Towards that goal, we introduce Stable Cinemetrics, a structured
evaluation framework that formalizes filmmaking controls into four
disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.</li>
<li>To enable scalable evaluation, we train an automatic evaluator, a
vision-language model aligned with expert annotations that outperforms existing
zero-shot baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26555v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26555v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26548v1'></a></p>
<h2 id="automated-and-scalable-sem-image-analysis-of-perovskite-solar-cell-materials-via-a-deep-segmentation-framework"><a href="https://arxiv.org/abs/2509.26548v1">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a></h2>
<p><strong>Authors:</strong> Jian Guo Pan, Lin Wang, Xia Cai</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cond-mat.mtrl-sci, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Scanning Electron Microscopy (SEM) is indispensable for characterizing the
microstructure of thin films during perovskite solar cell fabrication. Accurate
identification and quantification of lead iodide and perovskite phases are
critical because residual lead iodide strongly influences crystallization
pathways and defect formation, while the morphology of perovskite grains
governs carrier transport and device stability. Yet current SEM image analysis
is still largely manual, limiting throughput and consistency. Here, we present
an automated deep learning-based framework for SEM image segmentation that
enables precise and efficient identification of lead iodide, perovskite and
defect domains across diverse morphologies. Built upon an improved YOLOv8x
architecture, our model named PerovSegNet incorporates two novel modules: (i)
Adaptive Shuffle Dilated Convolution Block, which enhances multi-scale and
fine-grained feature extraction through group convolutions and channel mixing;
and (ii) Separable Adaptive Downsampling module, which jointly preserves
fine-scale textures and large-scale structures for more robust boundary
recognition. Trained on an augmented dataset of 10,994 SEM images, PerovSegNet
achieves a mean Average Precision of 87.25% with 265.4 Giga Floating Point
Operations, outperforming the baseline YOLOv8x-seg by 4.08%, while reducing
model size and computational load by 24.43% and 25.22%, respectively. Beyond
segmentation, the framework provides quantitative grain-level metrics, such as
lead iodide/perovskite area and count, which can serve as reliable indicators
of crystallization efficiency and microstructural quality. These capabilities
establish PerovSegNet as a scalable tool for real-time process monitoring and
data-driven optimization of perovskite thin-film fabrication.The source code is
available at:https://github.com/wlyyj/PerovSegNet/tree/master.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jian Guo Pan, Lin Wang, Xia Cai撰写的论文“Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework”的全面摘要。</p>
<p><strong>论文摘要：</strong></p>
<ol>
<li>
<p><strong>主要问题或研究问题：</strong>
    钙钛矿太阳能电池（PSCs）薄膜的微观结构表征对于其性能和稳定性至关重要。扫描电子显微镜（SEM）是分析这些薄膜微观结构（如碘化铅、钙钛矿相和缺陷）的不可或缺的工具。然而，当前的SEM图像分析主要依赖手动操作，这限制了分析的吞吐量、一致性，并且难以准确识别细粒度特征和复杂背景。因此，论文旨在解决如何实现钙钛矿太阳能电池材料SEM图像的自动化、精确且可扩展的分割和量化分析，以克服手动分析的局限性。</p>
</li>
<li>
<p><strong>关键创新或方法贡献：</strong>
    为了解决上述问题，作者提出了一个名为 <strong>PerovSegNet</strong> 的自动化深度学习分割框架。该框架基于改进的YOLOv8x架构，并引入了两个新颖的模块：</p>
<ul>
<li><strong>(i) 自适应混洗膨胀卷积块 (Adaptive Shuffle Dilated Convolution Block, ASDCB)：</strong> 该模块通过组卷积和通道混合增强多尺度和细粒度特征提取，从而提高网络区分晶界、小颗粒和缺陷区域的能力。</li>
<li><strong>(ii) 可分离自适应下采样模块 (Separable Adaptive Downsampling module, SAD)：</strong> 该模块通过结合深度可分离卷积和自适应池化机制，共同保留细尺度纹理和大尺度结构，以实现更鲁棒的边界识别，有效缓解传统下采样方法中常见的混叠和对小尺度纹理敏感度有限的问题。
此外，为了克服带注释SEM数据稀缺的挑战，作者构建了一个包含10,994张增强SEM图像的 <strong>PerovData 数据集</strong>，并使用UMAP分析验证了特征的可分离性和形态多样性。</li>
</ul>
</li>
<li>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>卓越的分割性能：</strong> PerovSegNet 在增强的PerovData数据集上实现了87.25%的平均精度（mAP@0.5），显著优于基线YOLOv8x-seg模型4.08%。</li>
<li><strong>计算效率提升：</strong> 模型尺寸和计算负载分别减少了24.43%和25.22%，同时保持了高性能，使其成为一个轻量级且高效的解决方案。</li>
<li><strong>定性分析优势：</strong> 相较于Mask R-CNN和Cascade InternImage-XL等基线模型，PerovSegNet能够更清晰地描绘边界，更可靠地分离密集堆积的晶粒，并显著减少漏检的碘化铅簇和缺陷区域。</li>
<li><strong>定量微观结构指标：</strong> 除了分割，该框架还提供了定量的晶粒级指标，如碘化铅/钙钛矿的面积和数量，这些指标可作为结晶效率和微观结构质量的可靠指示器。</li>
<li><strong>与器件性能的关联：</strong> 论文通过相关性分析（Pearson r）展示了图像衍生的微观结构描述符（如钙钛矿面积、缺陷密度）与器件光伏转换效率（PCE）之间的关系，为数据驱动的工艺优化提供了基础。</li>
<li><strong>尺度不变性分析：</strong> PerovSegNet在不同SEM分辨率下（从200 nm到2 µm）均表现出良好的分割能力，证明了其在不同实验条件下适应SEM数据的鲁棒性。</li>
</ul>
<p>这些结果表明，PerovSegNet是一个可扩展的工具，可用于实时过程监控和数据驱动的钙钛矿薄膜制造优化。</p>
</li>
<li>
<p><strong>论文中提及的局限性：</strong></p>
<ul>
<li><strong>相关性非因果性：</strong> 报告的相关性是观察性的，不暗示因果关系。其他因素（如成分或界面工程）可能同时影响形态和PCE。</li>
<li><strong>误差传播：</strong> 分割误差和类别不平衡（特别是缺陷类别）可能传播到派生指标中。</li>
<li><strong>成像条件影响：</strong> 成像条件（电压、放大倍数、校准）也会影响定量输出。</li>
<li><strong>数据集规模和多样性：</strong> 解决上述问题需要更大、更多样化的数据集，以及不确定性量化和跨放大倍数验证。</li>
</ul>
</li>
<li>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到其他材料系统：</strong> 将该框架扩展到其他材料系统。</li>
<li><strong>进一步加速推理：</strong> 进一步加速推理，以实现原位（in situ）或在线（inline）应用。</li>
<li><strong>数据驱动的工艺优化：</strong> 利用PerovSegNet提供的定量微观结构指标，通过数据驱动的反馈循环，调整工艺参数以实现目标形态。</li>
<li><strong>不确定性量化和跨放大倍数验证：</strong> 进一步研究以提高模型的鲁棒性和泛化能力。</li>
</ul>
</li>
</ol>
<p>总而言之，PerovSegNet通过其创新的模块设计和对大规模数据集的训练，在钙钛矿太阳能电池材料的SEM图像分析方面取得了显著进展，为材料科学领域的自动化微观结构表征和优化提供了强大且高效的工具。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Here, we present
an automated deep learning-based framework for SEM image segmentation that
enables precise and efficient identification of lead iodide, perovskite and
defect domains across diverse morphologies.</li>
<li>Built upon an improved YOLOv8x
architecture, our model named PerovSegNet incorporates two novel modules: (i)
Adaptive Shuffle Dilated Convolution Block, which enhances multi-scale and
fine-grained feature extraction through group convolutions and channel mixing;
and (ii) Separable Adaptive Downsampling module, which jointly preserves
fine-scale textures and large-scale structures for more robust boundary
recognition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26548v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26548v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-01 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
