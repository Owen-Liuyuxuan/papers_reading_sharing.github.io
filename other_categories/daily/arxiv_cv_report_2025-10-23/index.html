<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-23 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-22/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-24/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-23">Arxiv Computer Vision Papers - 2025-10-23</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-22" class="nav-link">Arxiv 计算机视觉每日报告执行摘要 (2025-10-22)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#a-survey-on-cache-methods-in-diffusion-models-toward-efficient-multi-modal-generation" class="nav-link">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#advances-in-4d-representation-geometry-motion-and-interaction" class="nav-link">Advances in 4D Representation: Geometry, Motion, and Interaction</a>
                </li>
                <li class="nav-item">
                    <a href="#how-to-evaluate-monocular-depth-estimation" class="nav-link">How to Evaluate Monocular Depth Estimation?</a>
                </li>
                <li class="nav-item">
                    <a href="#from-forecasting-to-planning-policy-world-model-for-collaborative-state-action-prediction" class="nav-link">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#pragmatic-heterogeneous-collaborative-perception-via-generative-communication-mechanism" class="nav-link">Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</a>
                </li>
                <li class="nav-item">
                    <a href="#decomposed-attention-fusion-in-mllms-for-training-free-video-reasoning-segmentation" class="nav-link">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#a-matter-of-time-revealing-the-structure-of-time-in-vision-language-models" class="nav-link">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-across-views-benchmarking-spatial-reasoning-of-vision-language-models-in-robotic-scenes" class="nav-link">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a>
                </li>
                <li class="nav-item">
                    <a href="#aegisrf-adversarial-perturbations-guided-with-sensitivity-for-protecting-intellectual-property-of-neural-radiance-fields" class="nav-link">AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</a>
                </li>
                <li class="nav-item">
                    <a href="#damo-data-mixing-optimizer-in-fine-tuning-multimodal-llms-for-mobile-phone-agents" class="nav-link">DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-23">Arxiv Computer Vision Papers - 2025-10-23</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-22">Arxiv 计算机视觉每日报告执行摘要 (2025-10-22)</h2>
<p><strong>概述：</strong></p>
<p>今天的 Arxiv 计算机视觉论文主要围绕<strong>多模态学习、3D/4D 视觉理解、高效模型设计以及机器人感知与规划</strong>展开。显著趋势包括将大型语言模型 (LLMs) 的能力扩展到视觉领域，以实现更复杂的推理和交互，同时关注模型效率和实际应用。</p>
<p><strong>主要主题和趋势：</strong></p>
<ol>
<li><strong>多模态学习与大型语言模型 (MLLMs) 的融合：</strong> 多篇论文探索了 MLLMs 在视频理解、空间推理和机器人代理中的应用，强调了视觉-语言模型的强大潜力。</li>
<li><strong>3D/4D 视觉理解与表示：</strong> 对 4D 几何、运动和交互的表示以及神经辐射场 (NeRF) 的保护和应用是重要的研究方向。</li>
<li><strong>模型效率与优化：</strong> 扩散模型中的缓存方法以及多模态 LLMs 的数据混合优化表明了对模型效率和微调策略的持续关注。</li>
<li><strong>评估与基准：</strong> 如何有效评估单目深度估计和视觉-语言模型的空间推理能力是关键的挑战。</li>
<li><strong>机器人感知与规划：</strong> 从预测到规划的策略世界模型以及异构协作感知展示了计算机视觉在机器人领域的实际应用。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation" by Jiacheng Liu et al. (1)</strong>：这是一篇及时且重要的综述，它系统地梳理了扩散模型中的缓存技术，对于提升多模态生成效率具有指导意义。考虑到扩散模型在生成领域的广泛应用，这篇综述将对研究人员产生深远影响。</li>
<li><strong>"From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction" by Zhida Zhao et al. (4)</strong>：这篇论文提出了一个从预测转向规划的策略世界模型，对于机器人协作和复杂任务执行具有突破性意义。它将视觉感知与决策制定更紧密地结合起来，是迈向更智能机器人系统的重要一步。</li>
<li><strong>"A Matter of Time: Revealing the Structure of Time in Vision-Language Models" by Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer (7)</strong>：这篇论文深入探讨了视觉-语言模型中时间结构的理解，这是一个被相对忽视但至关重要的方面。对时间推理的更好理解将极大地提升模型在视频理解和动态场景分析中的能力。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>4D 几何、运动和交互的统一表示 (2)</strong>：超越传统的 3D 建模，将时间维度纳入表示中，以更好地捕捉动态场景。</li>
<li><strong>基于生成式通信机制的异构协作感知 (5)</strong>：利用生成模型促进不同传感器或智能体之间的有效信息共享，以克服异构性挑战。</li>
<li><strong>训练无关的视频推理分割 (6)</strong>：通过分解注意力融合，在不进行额外训练的情况下实现视频推理分割，这预示着更灵活和通用的模型应用。</li>
<li><strong>神经辐射场 (NeRF) 的知识产权保护 (9)</strong>：随着 NeRF 的普及，对其进行对抗性扰动以保护知识产权是一个新颖且重要的研究方向。</li>
<li><strong>多模态 LLMs 的数据混合优化 (10)</strong>：针对特定应用（如移动手机代理）对 MLLMs 进行高效微调，强调了实际部署中的优化策略。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于希望深入了解特定领域的忙碌研究人员，我建议优先阅读以下论文：</p>
<ul>
<li><strong>对于生成模型和效率感兴趣的研究人员：</strong><ul>
<li><strong>1. "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation"</strong> (提供全面的背景和未来方向)</li>
</ul>
</li>
<li><strong>对于机器人和具身智能感兴趣的研究人员：</strong><ul>
<li><strong>4. "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction"</strong> (代表了机器人规划的最新进展)</li>
<li><strong>8. "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes"</strong> (提供了评估 MLLMs 在机器人场景中空间推理能力的关键基准)</li>
</ul>
</li>
<li><strong>对于多模态学习和视频理解感兴趣的研究人员：</strong><ul>
<li><strong>7. "A Matter of Time: Revealing the Structure of Time in Vision-Language Models"</strong> (深入探讨了视频理解中的核心挑战)</li>
<li><strong>6. "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation"</strong> (展示了 MLLMs 在视频任务中的新颖应用)</li>
</ul>
</li>
<li><strong>对于 3D 视觉和模型安全感兴趣的研究人员：</strong><ul>
<li><strong>9. "AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields"</strong> (提出了 NeRF 安全性的新问题和解决方案)</li>
</ul>
</li>
</ul>
<p>这份摘要旨在帮助您快速掌握今日 Arxiv 计算机视觉领域的关键进展，并指导您进行更深入的阅读。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.19755v1">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a></li>
<li><a href="#2510.19255v1">Advances in 4D Representation: Geometry, Motion, and Interaction</a></li>
<li><a href="#2510.19814v1">How to Evaluate Monocular Depth Estimation?</a></li>
<li><a href="#2510.19654v1">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a></li>
<li><a href="#2510.19618v1">Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</a></li>
<li><a href="#2510.19592v1">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</a></li>
<li><a href="#2510.19559v1">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a></li>
<li><a href="#2510.19400v1">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a></li>
<li><a href="#2510.19371v1">AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</a></li>
<li><a href="#2510.19336v1">DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.19755v1'></a></p>
<h2 id="a-survey-on-cache-methods-in-diffusion-models-toward-efficient-multi-modal-generation"><a href="https://arxiv.org/abs/2510.19755v1">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a></h2>
<p><strong>Authors:</strong> Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion Models have become a cornerstone of modern generative AI for their
exceptional generation quality and controllability. However, their inherent
\textit{multi-step iterations} and \textit{complex backbone networks} lead to
prohibitive computational overhead and generation latency, forming a major
bottleneck for real-time applications. Although existing acceleration
techniques have made progress, they still face challenges such as limited
applicability, high training costs, or quality degradation.
  Against this backdrop, \textbf{Diffusion Caching} offers a promising
training-free, architecture-agnostic, and efficient inference paradigm. Its
core mechanism identifies and reuses intrinsic computational redundancies in
the diffusion process. By enabling feature-level cross-step reuse and
inter-layer scheduling, it reduces computation without modifying model
parameters. This paper systematically reviews the theoretical foundations and
evolution of Diffusion Caching and proposes a unified framework for its
classification and analysis.
  Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}. This trend enhances caching flexibility across diverse tasks and
enables integration with other acceleration techniques such as sampling
optimization and model distillation, paving the way for a unified, efficient
inference framework for future multimodal and interactive applications. We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jiacheng Liu等人撰写的论文《A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation》的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：扩散模型中的缓存方法：迈向高效多模态生成</h3>
<p><strong>1. 主要问题或研究问题：</strong>
扩散模型（DMs）在生成式AI领域取得了突破性进展，但其固有的“多步迭代”和“复杂骨干网络”导致了巨大的计算开销和生成延迟。这严重阻碍了扩散模型在实时交互应用中的部署。尽管现有加速技术取得了一些进展，但它们仍面临适用性有限、训练成本高昂或生成质量下降等挑战。因此，论文旨在解决如何在不牺牲生成质量的前提下，高效降低扩散模型的推理延迟这一关键问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文的核心创新在于提出并系统性地综述了<strong>扩散缓存（Diffusion Caching）</strong>这一训练无关、架构无关且高效的推理范式。其主要方法论贡献包括：
*   <strong>识别和重用计算冗余：</strong> 扩散缓存的核心机制在于识别并重用扩散推理过程中固有的计算冗余。通过特征层面的跨步重用和层间调度，在不修改模型参数的情况下有效降低了计算负荷。
*   <strong>统一分类和分析框架：</strong> 论文构建了首个扩散缓存的系统性分类和分析框架，从触发条件、重用粒度和更新策略三个维度揭示了不同方法的内在逻辑和技术演进。
*   <strong>演进轨迹分析：</strong> 论文通过对代表性方法的比较分析，揭示了扩散缓存从“静态重用”向“动态预测”的演进轨迹。
    *   <strong>静态缓存（Static Caching）：</strong> 采用固定重用策略，在预定义层或时间步进行缓存，且在所有推理运行中保持不变。例如DeepCache利用U-Net上采样层特征变化小的特点，FasterDiffusion通过编码器传播重用编码器特征。
    *   <strong>动态缓存（Dynamic Caching）：</strong> 引入错误检查机制，根据预定义指标（如特征相似度）动态决定何时进行计算、更新缓存或直接使用缓存。
        *   <strong>时间步自适应缓存（Timestep-Adaptive Caching）：</strong> 根据特征在扩散阶段的不同稳定性和动态性，动态调整缓存激活和刷新时机。例如TeaCache通过计算相邻输出的相对L1差异来预测输出变化。
        *   <strong>层自适应缓存（Layer-Adaptive Caching）：</strong> 根据网络中各层特征的动态特性（如梯度幅度、特征差异），自适应调整各层的缓存和更新频率。例如Block Caching根据块输出的相对绝对变化来决定缓存策略。
        *   <strong>预测缓存（Predictive Caching）：</strong> 将缓存视为数值预测问题，利用历史缓存表示显式预测未来多步特征。例如TaylorSeer通过泰勒级数展开来预测特征轨迹，HiCache采用Hermite多项式作为预测基础。
        *   <strong>混合缓存（Hybrid Caching）：</strong> 结合多种维度（时间步、网络层级、特征动态）的策略，实现跨空间和时间域的计算/重用协调。例如ClusCa引入空间token相似性，SpeCa采用预测-验证闭环框架。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的推理加速：</strong> 扩散缓存方法在不牺牲生成质量的前提下，显著降低了扩散模型的计算开销和推理延迟。例如，TaylorSeer实现了近5倍的加速，FreqCa在Qwen-Image上实现了高达7.14倍的推理加速和99%的内存节省。
*   <strong>训练无关和架构无关：</strong> 扩散缓存作为一种纯推理时优化技术，无需额外的训练或微调，且与模型架构无关，使其具有高度的灵活性和普适性。
*   <strong>与现有技术的互补性：</strong> 扩散缓存可以与采样优化、模型蒸馏等其他加速技术结合，实现互补效益，共同构建统一高效的推理框架。
*   <strong>广泛的应用潜力：</strong> 扩散缓存已在图像和视频编辑、3D生成、音频生成、超分辨率、世界模型、离散扩散模型和AI for Science等多种多模态生成任务中展现出强大的适应性和通用性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>内存消耗挑战：</strong> 缓存中间激活需要大量GPU内存，尤其是在高分辨率图像生成、长序列视频合成以及多任务并发推理场景下，可能导致内存溢出（OOM）。
*   <strong>生成质量下降：</strong> 缓存引入的近似误差可能导致细粒度信息丢失、纹理模糊、边缘失真和微结构损失，在高加速比下尤为明显，限制了其在高精度任务中的应用。
*   <strong>理论基础不足：</strong> 现有缓存方法大多是工程驱动的探索，缺乏可解释和可验证的理论框架来表征缓存引入的分布偏差和误差传播。
*   <strong>与其他加速策略的集成挑战：</strong> 尽管缓存具有互补性，但多种加速机制联合应用时，其引入的误差可能叠加，导致模型与原始分布的偏差加剧。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>内存效率优化：</strong> 开发更高效的缓存策略，例如累积残差特征（CRF）缓存，以显著减少内存占用。
*   <strong>误差分析与控制：</strong> 建立更严格、统一的缓存诱导误差分析理论，量化其对扩散动态的影响，并探索缓存策略与不同采样方法（如DDIM、Flow Matching）的兼容性。
*   <strong>深度集成框架：</strong> 设计原则性的集成框架，以量化模型、缓存、量化和剪枝等多种加速策略之间的相互作用，并识别最佳集成条件，以平衡加速和生成质量。
*   <strong>多模态和交互式应用：</strong> 随着模型规模的增长和生成任务的多模态化、交互式化，扩散缓存有望成为下一代高效生成框架的关键组成部分，并推动实时、可持续的生成式AI发展。</p>
<hr />
<p>这篇论文为扩散模型加速领域提供了一个全面的视角，特别强调了缓存方法作为一种有前景的解决方案。它不仅系统地回顾了现有技术，还提出了一个统一的框架来理解和分类这些方法，并指出了未来的研究方向，对于推动生成式AI的实际部署和理论发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}.</li>
<li>We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19755v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19755v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19255v1'></a></p>
<h2 id="advances-in-4d-representation-geometry-motion-and-interaction"><a href="https://arxiv.org/abs/2510.19255v1">Advances in 4D Representation: Geometry, Motion, and Interaction</a></h2>
<p><strong>Authors:</strong> Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI). While our survey is not the first
of its kind, we build our coverage of the domain from a unique and distinctive
perspective of 4D representations\/}, to model 3D geometry evolving over time
while exhibiting motion and interaction. Specifically, instead of offering an
exhaustive enumeration of many works, we take a more selective approach by
focusing on representative works to highlight both the desirable properties and
ensuing challenges of each representation under different computation,
application, and data scenarios. The main take-away message we aim to convey to
the readers is on how to select and then customize the appropriate 4D
representations for their tasks. Organizationally, we separate the 4D
representations based on three key pillars: geometry, motion, and interaction.
Our discourse will not only encompass the most popular representations of
today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),
but also bring attention to relatively under-explored representations in the 4D
context, such as structured models and long-range motions. Throughout our
survey, we will reprise the role of large language models (LLMs) and video
foundational models (VFMs) in a variety of 4D applications, while steering our
discussion towards their current limitations and how they can be addressed. We
also provide a dedicated coverage on what 4D datasets are currently available,
as well as what is lacking, in driving the subfield forward. Project
page:https://mingrui-zhao.github.io/4DRep-GMI/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Mingrui Zhao等人撰写的论文“Advances in 4D Representation: Geometry, Motion, and Interaction”的摘要，内容基于您提供的PDF全文和摘要。</p>
<p><strong>论文摘要：4D表示的进展：几何、运动和交互</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在全面回顾和分析4D数据（即随时间演变并展现运动和交互的3D几何）的生成和重建领域。核心问题是如何选择和定制合适的4D表示方法，以应对不同计算、应用和数据场景下的挑战，并弥补现有综述在几何、运动和交互表示方面的不足。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>独特的4D表示视角：</strong> 论文从4D表示的独特视角构建其覆盖范围，将4D表示分为三个关键支柱：几何、运动和交互。
*   <strong>几何表示的分类与分析：</strong> 区分了非结构化表示（如神经辐射场NeRFs和3D高斯泼溅3DGS）和结构化表示（如模板、基于部件的模型和图），并详细分析了它们在4D环境中的优势、局限性和挑战。
*   <strong>运动建模的全面覆盖：</strong> 详细讨论了四种主要的运动类别：关节运动、基于变形的运动、基于跟踪的运动和混合运动，并分析了不同运动类型如何与表示选择相互作用。
*   <strong>交互建模的深入探讨：</strong> 探讨了如何表示交互实体、姿态、接触、动作和可供性，以及如何整合物理先验以确保物理合理性。
*   <strong>数据驱动先验和训练策略：</strong> 强调了大型语言模型（LLMs）和视频基础模型（VFMs）在4D应用中的作用，并分析了逐场景优化、端到端训练和混合优化等训练策略。
*   <strong>数据集和基准的专门覆盖：</strong> 提供了现有4D数据集的详细列表，并指出了当前数据集在推动该子领域发展方面的不足。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文没有呈现具体实验结果，而是一篇综述，其主要“结果”是：
*   <strong>系统性框架：</strong> 建立了一个以表示为中心的统一概念框架，用于理解和指导4D内容的捕获、理解、合成和交互。
*   <strong>揭示权衡：</strong> 明确了不同表示选择在效率、保真度和泛化能力之间固有的权衡，帮助读者根据任务需求选择最合适的表示。
*   <strong>推动未来研究：</strong> 通过识别当前局限性和新兴趋势，为4D表示领域的未来研究方向提供了清晰的路线图，例如开发统一、自适应和结构感知的表示，以及整合世界知识和物理先验。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>拓扑灵活性不足：</strong> 网格表示在处理拓扑变化（如分裂、合并）和体积现象（如云、烟雾）时存在局限性。
*   <strong>计算成本高昂：</strong> NeRF和3DGS等基于优化的方法训练时间长、计算资源需求大，难以扩展到大规模或商业应用。
*   <strong>稀疏输入场景的挑战：</strong> NeRFs和3DGS在稀疏输入条件下表现不佳，需要密集的多视角视频捕获。
*   <strong>非结构化表示的局限性：</strong> 尽管NeRFs和3DGS流行，但它们不适合需要紧凑和结构化表示的建模、编辑或交互等4D任务。
*   <strong>结构化表示的泛化能力：</strong> 模板和基于部件的模型在跨类别泛化方面仍有限制，需要更多工作来自动推断骨架结构和蒙皮权重。
*   <strong>数据集稀缺：</strong> 缺乏大规模、高质量、带有完整地面真实几何、外观、运动和交互标注的4D数据集，限制了模型的训练和泛化能力。
*   <strong>评估指标不足：</strong> 现有评估指标在功能正确性、结构平衡和设计复杂性方面仍有欠缺，且缺乏针对长期运动和多样化对象类别的综合基准。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>混合表示：</strong> 结合结构化模型的几何层次和可解释性与隐式神经表示的灵活性和表达能力。
*   <strong>整合世界知识：</strong> 利用LLMs和VFMs进行多模态推理和常识指导，以及通过可微分物理损失或重建-模拟管道整合物理先验。
*   <strong>功能感知指标：</strong> 开发能够评估功能正确性、结构平衡和设计复杂性的新评估指标。
*   <strong>大规模4D数据集：</strong> 迫切需要开发大规模、标准化、涵盖多样化对象类别和运动类型的4D基准。
*   <strong>自监督和因果学习：</strong> 探索超越重建或监督驱动训练的自监督、因果和物理信息学习方法。
*   <strong>解耦表示：</strong> 进一步研究几何、运动、外观和光照独立可操作的解耦表示，以提高可控性和可编辑性。
*   <strong>工业级精度：</strong> 解决当前方法在CAD领域缺乏工业级精度的问题，以满足装配、机械重建和机器人训练等应用的需求。</p>
<p>这篇论文为4D表示领域提供了一个全面的、以表示为中心的视角，不仅总结了现有技术，更重要的是，它为未来的研究指明了方向，强调了在几何、运动和交互表示方面仍存在的挑战和机遇。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19255v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19255v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19814v1'></a></p>
<h2 id="how-to-evaluate-monocular-depth-estimation"><a href="https://arxiv.org/abs/2510.19814v1">How to Evaluate Monocular Depth Estimation?</a></h2>
<p><strong>Authors:</strong> Siyang Wu, Jack Nugent, Willow Yang, Jia Deng</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Monocular depth estimation is an important task with rapid progress, but how
to evaluate it remains an open question, as evidenced by a lack of
standardization in existing literature and a large selection of evaluation
metrics whose trade-offs and behaviors are not well understood. This paper
contributes a novel, quantitative analysis of existing metrics in terms of
their sensitivity to various types of perturbations of ground truth,
emphasizing comparison to human judgment. Our analysis reveals that existing
metrics are severely under-sensitive to curvature perturbation such as making
flat surfaces wavy. To remedy this, we introduce a new metric based on relative
surface normals, along with new depth visualization tools and a principled
method to create composite metrics with better human alignment. Code and data
are available at: https://github.com/princeton-vl/evalmde.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：How to Evaluate Monocular Depth Estimation?</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于对单目深度估计的现有评估指标进行了首次定量分析，揭示了它们在曲率扰动方面的严重不足。为了解决这一问题，作者提出了一种基于相对表面法线的新指标，并引入了新的可视化工具和一种创建与人类判断更一致的复合指标的原则性方法。这项工作旨在标准化和改进单目深度估计的评估方式。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>定量分析现有指标：</strong> 论文的关键创新在于对现有评估指标进行了系统性的、定量的分析，特别关注它们对不同类型地面真值扰动（尤其是曲率扰动）的敏感性，并将其与人类判断进行比较。这是对现有文献中缺乏标准化和对指标行为理解不足的回应。</li>
<li><strong>发现现有指标的不足：</strong> 明确指出现有指标对“曲率扰动”（如将平面变为波浪形）严重不敏感，这是一个重要的发现，揭示了当前评估方法可能无法捕捉到深度估计中的关键误差类型。</li>
<li><strong>引入新指标：</strong> 提出了一种基于“相对表面法线”的新指标，旨在更好地捕捉和评估深度估计中的曲率和表面几何细节。</li>
<li><strong>新的可视化工具和复合指标方法：</strong> 除了新指标，还提供了新的深度可视化工具，以及一种“原则性方法”来创建与人类判断更一致的复合指标，这表明作者不仅关注单一指标，还关注如何综合利用多种信息进行更全面的评估。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>标准化评估：</strong> 这项研究有望推动单目深度估计领域评估方法的标准化，减少现有文献中指标选择的混乱。</li>
<li><strong>更准确的模型开发：</strong> 通过提供更敏感、更符合人类感知的评估指标，研究人员将能够更准确地识别模型在几何细节和表面曲率方面的弱点，从而开发出性能更优越、视觉质量更好的深度估计模型。</li>
<li><strong>促进公平比较：</strong> 统一的评估框架将使得不同研究团队的模型能够进行更公平、更有意义的比较，加速领域进步。</li>
<li><strong>提升应用可靠性：</strong> 对于依赖深度信息的下游应用（如机器人导航、自动驾驶、AR/VR），更准确的深度评估意味着更可靠的输入，从而提升这些应用的性能和安全性。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学和自动驾驶：</strong> 准确的深度估计对于避障、路径规划、三维重建和场景理解至关重要。改进的评估方法将帮助开发更可靠的深度传感器和算法。</li>
<li><strong>增强现实 (AR) 和虚拟现实 (VR)：</strong> 逼真的AR/VR体验高度依赖于精确的场景深度理解，以便进行遮挡、光照和物理交互。</li>
<li><strong>三维重建和建模：</strong> 从单目图像进行三维重建是许多应用的基础，更精细的深度评估有助于生成更高质量的三维模型。</li>
<li><strong>计算机图形学：</strong> 深度信息可用于渲染、光照模拟和特效制作。</li>
<li><strong>医学影像：</strong> 在某些医学影像任务中，深度信息可以帮助医生进行诊断和手术规划。</li>
<li><strong>工业检测和质量控制：</strong> 精确的物体形状和尺寸测量是工业应用的关键，深度估计的改进评估将有助于提升这些任务的精度。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>“人类判断”的定义和获取：</strong> 摘要强调与“人类判断”的比较，但并未详细说明如何量化和获取这些人类判断。人类对深度和曲率的感知可能存在主观性，如何将其转化为可量化的基准是一个挑战。</li>
<li><strong>计算成本：</strong> 新的基于相对表面法线的指标以及复合指标的计算成本可能高于现有的一些简单指标，这在实时应用中可能是一个考虑因素。</li>
<li><strong>泛化性：</strong> 论文分析的“各种类型的地面真值扰动”是否涵盖了所有实际场景中可能出现的深度估计误差类型？新指标在各种复杂场景（如纹理缺失、反射、透明物体等）下的表现如何，仍需进一步验证。</li>
<li><strong>仅限于单目深度估计：</strong> 顾名思义，这项研究专注于单目深度估计。对于立体视觉、多视图几何或深度传感器（如LiDAR、ToF）的评估，可能需要不同的考量。</li>
<li><strong>“原则性方法”的具体细节：</strong> 摘要中提到的创建复合指标的“原则性方法”的具体细节和其理论基础并未展开，这可能需要阅读全文才能理解其鲁棒性和普适性。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过对现有评估指标的批判性分析和提出创新的解决方案，有望对单目深度估计领域产生深远影响，引导研究人员开发出更精确、更符合人类视觉感知的深度估计模型。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper
contributes a novel, quantitative analysis of existing metrics in terms of
their sensitivity to various types of perturbations of ground truth,
emphasizing comparison to human judgment.</li>
<li>To remedy this, we introduce a new metric based on relative
surface normals, along with new depth visualization tools and a principled
method to create composite metrics with better human alignment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19814v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19814v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19654v1'></a></p>
<h2 id="from-forecasting-to-planning-policy-world-model-for-collaborative-state-action-prediction"><a href="https://arxiv.org/abs/2510.19654v1">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a></h2>
<p><strong>Authors:</strong> Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Despite remarkable progress in driving world models, their potential for
autonomous systems remains largely untapped: the world models are mostly
learned for world simulation and decoupled from trajectory planning. While
recent efforts aim to unify world modeling and planning in a single framework,
the synergistic facilitation mechanism of world modeling for planning still
requires further exploration. In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme. Through collaborative state-action prediction,
PWM can mimic the human-like anticipatory perception, yielding more reliable
planning performance. To facilitate the efficiency of video forecasting, we
further introduce a dynamically enhanced parallel token generation mechanism,
equipped with a context-guided tokenizer and an adaptive dynamic focal loss.
Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.
Code and model weights will be released at
https://github.com/6550Zhao/Policy-World-Model.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zhida Zhao等人的论文“From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction”的全面摘要。</p>
<hr />
<h3 id="-">论文摘要：从预测到规划：用于协同状态-动作预测的策略世界模型</h3>
<p><strong>1. 论文主要问题或研究问题：</strong>
尽管驾驶世界模型取得了显著进展，但其在自动驾驶系统中的潜力尚未完全发挥。现有世界模型主要用于世界模拟，并与轨迹规划解耦。虽然近期有工作尝试将世界建模和规划统一到单一框架中，但如何协同促进世界建模以更好地服务于规划仍需进一步探索。具体来说，论文旨在解决如何让世界模型通过学习到的世界知识，以更可靠的方式直接支持轨迹规划，而不是仅仅作为模拟器或独立任务。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了一个名为<strong>策略世界模型（Policy World Model, PWM）</strong>的新型驾驶范式，其核心创新点包括：</p>
<ul>
<li><strong>统一的世界建模与轨迹规划架构：</strong> PWM将世界建模和轨迹规划整合到一个统一的架构中。它通过“无动作未来状态预测”方案，利用学习到的世界知识来增强规划性能，模仿人类的“预期感知”能力。</li>
<li><strong>无动作视频生成预训练：</strong> PWM首先在无标签视频序列上进行无动作视频生成预训练，以获取世界建模能力。这消除了对动作标注数据的依赖，提高了训练的可扩展性，并允许更灵活的未来状态推演。</li>
<li><strong>协同状态-动作预测：</strong> 在微调和推理阶段，PWM结合当前和历史视频帧，首先生成文本描述以理解当前环境，然后通过视频生成推演出合理的未来状态。最终的动作预测是基于生成的描述和预测的未来状态作为多模态依据。</li>
<li><strong>高效的视频预测机制：</strong> 为了提高视频预测效率，PWM引入了动态增强的并行token生成机制，包括：<ul>
<li><strong>上下文引导的tokenizer：</strong> 实现紧凑的图像token表示（每张图像28个token），确保效率和视觉连贯性。</li>
<li><strong>自适应动态focal loss：</strong> 在训练中强调时间变化的图像区域，以确保视频生成质量，并更好地捕捉时空动态。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
PWM在流行的基准测试（nuScenes和NAVSIM）上进行了评估，并取得了显著成果：</p>
<ul>
<li><strong>卓越的规划性能：</strong> 在nuScenes数据集上，PWM实现了最低的平均碰撞率（0.07%和0.04%），超越了现有最先进的方法。</li>
<li><strong>仅使用前置摄像头输入：</strong> 尽管仅使用前置摄像头输入，PWM在NAVSIM基准测试上的PDMS性能与依赖多视角和多模态输入（如LiDAR）的最先进方法相当或更优。</li>
<li><strong>高效的视觉生成：</strong> 结合其创新机制，PWM能够以合理的计算开销生成高质量的未来视频帧，同时保持约40 FPS的帧率（不进行像素空间解码）。</li>
<li><strong>动态Focal Loss的有效性：</strong> 消融研究表明，动态Focal Loss显著提升了视频预测和规划性能，尤其是在预训练阶段应用时，能更有效地捕捉时空动态。</li>
<li><strong>未来帧预测对规划的积极影响：</strong> 引入未来帧预测显著降低了nuScenes上的平均碰撞率。在NAVSIM上，未来帧预测促使模型采取更保守的规划策略，牺牲部分进度（EP）以换取更高的安全裕度（NC和TTC）。</li>
</ul>
<p>这些结果凸显了从视频表示中学习以驱动更安全、更可扩展的自动驾驶系统的巨大潜力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>单视角输入的鲁棒性限制：</strong> 仅依赖单视角输入可能会在恶劣能见度条件下影响系统的鲁棒性。
*   <strong>规划时域限制：</strong> 短暂的规划时域限制了PWM在长时域场景中的适用性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>多视角输入的有效整合：</strong> 进一步探索如何高效整合多视角输入，以提高系统的鲁棒性。
*   <strong>增强长期预测能力：</strong> 提升模型的长期预测能力，以更好地应对长时域场景，从而提高泛化能力和实际应用准备度。</p>
<hr />
<p>总而言之，这篇论文通过提出Policy World Model，成功地将世界建模和轨迹规划统一起来，并通过无动作未来状态预测和动态增强的并行token生成机制，显著提升了自动驾驶系统的规划性能和效率。其在仅使用前置摄像头输入的情况下达到或超越了多视角/多模态方法的性能，为自动驾驶领域开辟了新的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme.</li>
<li>Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19654v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19654v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19618v1'></a></p>
<h2 id="pragmatic-heterogeneous-collaborative-perception-via-generative-communication-mechanism"><a href="https://arxiv.org/abs/2510.19618v1">Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</a></h2>
<p><strong>Authors:</strong> Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-agent collaboration enhances the perception capabilities of individual
agents through information sharing. However, in real-world applications,
differences in sensors and models across heterogeneous agents inevitably lead
to domain gaps during collaboration. Existing approaches based on adaptation
and reconstruction fail to support pragmatic heterogeneous collaboration due to
two key limitations: (1) Intrusive retraining of the encoder or core modules
disrupts the established semantic consistency among agents; and (2)
accommodating new agents incurs high computational costs, limiting scalability.
To address these challenges, we present a novel Generative Communication
mechanism (GenComm) that facilitates seamless perception across heterogeneous
multi-agent systems through feature generation, without altering the original
network, and employs lightweight numerical alignment of spatial information to
efficiently integrate new agents at minimal cost. Specifically, a tailored
Deformable Message Extractor is designed to extract spatial message for each
collaborator, which is then transmitted in place of intermediate features. The
Spatial-Aware Feature Generator, utilizing a conditional diffusion model,
generates features aligned with the ego agent's semantic space while preserving
the spatial information of the collaborators. These generated features are
further refined by a Channel Enhancer before fusion. Experiments conducted on
the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm
outperforms existing state-of-the-art methods, achieving an 81\% reduction in
both computational cost and parameter count when incorporating new agents. Our
code is available at https://github.com/jeffreychou777/GenComm.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Junfei Zhou等人撰写的论文“Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
多智能体协作感知通过信息共享增强了单个智能体的感知能力，但在现实世界应用中，异构智能体之间传感器和模型的差异不可避免地导致领域鸿沟。现有的基于适应（adaptation-based）和重建（reconstruction-based）的方法在支持实用异构协作感知方面存在两个主要限制：
1. <strong>侵入性再训练：</strong> 对编码器或核心模块进行再训练会破坏智能体之间已建立的语义一致性。
2. <strong>可扩展性差：</strong> 适应新智能体需要高计算成本，限制了系统的可扩展性。
该研究旨在解决如何在异构多智能体系统中实现无缝协作感知，同时避免侵入性修改，并以最小成本高效集成新智能体。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了一种新颖的<strong>生成式通信机制（GenComm）</strong>，通过特征生成促进异构多智能体系统间的无缝感知，且不改变原始网络。其核心创新包括：
*   <strong>非侵入式特征生成：</strong> GenComm通过生成与自我智能体语义空间对齐的特征，同时保留协作智能体的空间信息，避免了对原始网络进行侵入性再训练，从而维护了语义一致性。
*   <strong>轻量级空间信息数值对齐：</strong> 采用轻量级的数值对齐方法，以最小成本高效集成新智能体，解决了可扩展性问题。
*   <strong>可变形消息提取器（Deformable Message Extractor）：</strong> 专门设计用于为每个协作智能体提取空间消息，并替代中间特征进行传输。该提取器利用可变形卷积，动态参考周围像素，增强了区分前景和背景的能力。
*   <strong>空间感知特征生成器（Spatial-Aware Feature Generator）：</strong> 利用条件扩散模型，根据接收到的空间消息生成特征。这些特征与自我智能体的语义空间对齐，并保留了协作智能体的空间信息。
*   <strong>通道增强器（Channel Enhancer）：</strong> 在特征融合之前，进一步细化生成的特征，以确保通道维度上的语义一致性。
*   <strong>两阶段训练策略：</strong> 第一阶段在同构设置下训练模型的核心组件；第二阶段针对异构设置，仅对轻量级消息提取器进行微调，以对齐空间消息的数值分布，确保与接收方自身提取的空间信息一致。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> 在OPV2V-H、DAIR-V2X和V2X-Real数据集上进行的实验表明，GenComm优于现有的最先进方法。
*   <strong>显著的成本和参数减少：</strong> 在集成新智能体时，GenComm将计算成本和参数数量均降低了81%，这凸显了其出色的可扩展性。
*   <strong>通信效率提升：</strong> 通过传输压缩后的空间感知表示而非完整特征图，GenComm有效降低了通信开销。
*   <strong>鲁棒性：</strong> 在存在姿态误差和时间延迟的动态协作场景中，GenComm表现出优越的鲁棒性。
*   <strong>可视化验证：</strong> 可视化结果表明，生成的特征的语义空间与自我智能体一致，同时完整保留了协作智能体的空间信息，在特征质量和检测性能上均优于CodeFilling。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>需要供应商共识：</strong> 尽管论文假设了一个更现实的非完全连接通信图，但该方法仍然需要供应商之间的共识，这可能会受到商业竞争和恶意攻击潜在风险等因素的阻碍。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>缓解供应商共识问题：</strong> 探索无需严格供应商共识即可实现异构协作的方法，例如通过联邦学习或更去中心化的机制。
*   <strong>更复杂的攻击场景：</strong> 进一步研究在存在恶意攻击或更复杂通信干扰情况下的GenComm鲁棒性。
*   <strong>多模态融合的进一步优化：</strong> 尽管GenComm在异构设置下表现良好，但可以探索更先进的多模态融合技术，以进一步提升感知性能。
*   <strong>实时部署优化：</strong> 针对更严格的实时性要求，进一步优化GenComm的推理延迟和计算效率。</p>
<hr />
<p>总而言之，GenComm为异构多智能体协作感知提供了一个实用且高效的解决方案，通过其独特的生成式通信机制，在保持语义一致性、降低计算成本和提高可扩展性方面取得了显著进展，为自动驾驶等领域的实际部署奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Existing approaches based on adaptation
and reconstruction fail to support pragmatic heterogeneous collaboration due to
two key limitations: (1) Intrusive retraining of the encoder or core modules
disrupts the established semantic consistency among agents; and (2)
accommodating new agents incurs high computational costs, limiting scalability.</li>
<li>To address these challenges, we present a novel Generative Communication
mechanism (GenComm) that facilitates seamless perception across heterogeneous
multi-agent systems through feature generation, without altering the original
network, and employs lightweight numerical alignment of spatial information to
efficiently integrate new agents at minimal cost.</li>
<li>Experiments conducted on
the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm
outperforms existing state-of-the-art methods, achieving an 81\% reduction in
both computational cost and parameter count when incorporating new agents.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19618v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19618v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19592v1'></a></p>
<h2 id="decomposed-attention-fusion-in-mllms-for-training-free-video-reasoning-segmentation"><a href="https://arxiv.org/abs/2510.19592v1">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</a></h2>
<p><strong>Authors:</strong> Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal large language models (MLLMs) demonstrate strong video
understanding by attending to visual tokens relevant to textual queries. To
directly adapt this for localization in a training-free manner, we cast video
reasoning segmentation as a video QA task and extract attention maps via
rollout mechanism. However, raw attention maps are noisy and poorly aligned
with object regions. We propose Decomposed Attention Fusion (DecAF), which
refines these maps through two mechanisms: (1) contrastive object-background
fusion and (2) complementary video-frame fusion. This method suppresses
irrelevant activations and enhances object-focused cues, enabling direct
conversion of attention maps into coarse segmentation masks. In addition, we
introduce attention-guided SAM2 prompting for obtaining fine-grained masks.
Unlike existing methods that jointly train MLLMs with SAM, our method operates
entirely without retraining. DecAF outperforms training-free methods and
achieves performance comparable to training-based methods on both referring and
reasoning VOS benchmarks. The code will be available at
https://github.com/HYUNJS/DecAF.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文“Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation”的全面摘要。</p>
<hr />
<p><strong>论文题目：</strong> Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation
<strong>作者：</strong> Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为“Decomposed Attention Fusion (DecAF)”的新颖训练无关框架，旨在利用多模态大型语言模型（MLLMs）固有的视频理解能力，解决视频推理分割（Video Reasoning Segmentation）任务。该任务要求根据复杂的文本查询，在视频中定位并分割相关对象。</p>
<ol>
<li>
<p><strong>主要问题或研究问题：</strong></p>
<ul>
<li>MLLMs在视频理解方面表现出色，但其原始注意力图通常噪声大、与对象区域对齐不佳，难以直接用于训练无关的视频推理分割任务。</li>
<li>现有训练无关方法（如Loc-Head和TAM）在泛化性、鲁棒性以及处理多对象、小对象或需要复杂推理的场景时存在局限性。</li>
<li>如何从粗糙的注意力图生成精细的分割掩码，同时避免现有方法中MLLMs与分割模型（如SAM）联合训练的需求。</li>
</ul>
</li>
<li>
<p><strong>关键创新或方法贡献：</strong></p>
<ul>
<li><strong>将视频推理分割转化为视频问答任务：</strong> 论文将视频推理分割任务重新定义为视频问答（Video QA）任务，并通过注意力回溯（attention rollout）机制提取注意力图，以实现训练无关的定位。</li>
<li><strong>分解注意力融合（DecAF）框架：</strong> 提出DecAF来精炼原始注意力图，通过两种机制抑制无关激活并增强对象焦点：<ul>
<li><strong>对比对象-背景融合（Contrastive Object-Background Fusion）：</strong> 通过从对象聚焦的注意力图中减去背景聚焦的注意力图，有效抑制无关区域的噪声，突出目标对象信号。</li>
<li><strong>互补视频-帧融合（Complementary Video-Frame Fusion）：</strong> 结合视频级注意力图（捕捉时间上下文，但粒度粗糙）和帧级注意力图（提供对象中心、细粒度细节，但缺乏时间连贯性）的优势，生成更鲁棒的注意力图。</li>
</ul>
</li>
<li><strong>视觉感知归一化的注意力回溯（Vision-Aware Normalization for Attention Rollout）：</strong> 改进了标准的注意力回溯机制，引入了视觉感知归一化方案和头部加权聚合，以更好地捕捉语言条件下的定位信息，并减少噪声头部的负面影响。</li>
<li><strong>注意力引导的SAM2提示（Attention-guided SAM2 Prompting）：</strong> 引入了一种训练无关的SAM2提示流程，通过注意力图生成点查询，并结合注意力一致性分数（attention consistency score）来评估和过滤生成的掩码轨迹，从而从粗糙的注意力图获得精细的分割掩码。</li>
</ul>
</li>
<li>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>超越现有训练无关方法：</strong> DecAF在多个MLLM家族（LLaVA、InternVL、Qwen2VL/Qwen2.5VL）和五个数据集（Ref-DAVIS、Ref-YouTube-VOS、MeViS、ReasonVOS、ReVOS）上，无论是否结合SAM，都持续优于先前的训练无关方法（如Loc-Head和TAM），尤其在需要复杂推理的数据集上优势显著。</li>
<li><strong>与训练基于方法媲美的性能：</strong> DecAF在指代VOS和推理VOS基准测试上取得了与训练基于方法（如LISA、VISA、VideoLISA等）相当甚至更优的性能，尽管DecAF完全无需重新训练。</li>
<li><strong>泛化性和鲁棒性：</strong> 论文结果表明，DecAF框架在不同MLLMs和数据集上表现出更一致和可泛化的性能，解决了现有方法对数据集特定采样和模型特定启发式规则的依赖问题。</li>
<li><strong>有效生成精细掩码：</strong> 通过注意力引导的SAM2提示，DecAF能够将粗糙的注意力图转化为精确可靠的分割掩码，即使在多对象、小对象、时间推理和世界知识推理等挑战性场景下也能保持鲁棒的定位和分割质量。</li>
</ul>
</li>
<li>
<p><strong>论文中提及的局限性：</strong></p>
<ul>
<li><strong>注意力图分辨率：</strong> 尽管DecAF显著提升了性能，但注意力图的原始分辨率仍然较低，导致直接从注意力图生成的粗糙分割掩码在捕捉精细边界方面存在不足。</li>
<li><strong>注意力一致性分数失效案例：</strong> 在某些情况下，当目标对象占据较大区域但注意力权重仅覆盖其中一小部分时，注意力一致性分数可能会惩罚正确生成的掩码轨迹。类似地，当目标对象很小且在视频中短暂出现时，也可能因注意力权重占比小而导致错误过滤。</li>
<li><strong>结构捕捉限制：</strong> 当目标对象极其细长（如滑翔伞线）时，注意力图可能无法完全捕捉其结构，导致分割掩码质量不佳。</li>
</ul>
</li>
<li>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提升注意力图的细粒度：</strong> 进一步研究如何从MLLMs中提取更高分辨率或更细粒度的注意力信息，以直接生成更精确的分割掩码，减少对外部分割模型（如SAM2）的依赖。</li>
<li><strong>优化注意力一致性评分机制：</strong> 改进注意力一致性评分方法，使其在处理大面积对象或细长结构对象时更具鲁棒性，避免误判。</li>
<li><strong>探索更复杂的推理场景：</strong> 尽管DecAF在推理VOS上表现良好，但仍可进一步探索MLLMs在更复杂、多步骤推理场景下的定位能力。</li>
<li><strong>与其他MLLM架构的集成：</strong> 持续探索DecAF在未来新型MLLM架构上的适用性和优化潜力，特别是那些支持更高原生分辨率视频输入的模型。</li>
</ul>
</li>
</ol>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Decomposed Attention Fusion (DecAF), which
refines these maps through two mechanisms: (1) contrastive object-background
fusion and (2) complementary video-frame fusion.</li>
<li>Unlike existing methods that jointly train MLLMs with SAM, our method operates
entirely without retraining.</li>
<li>DecAF outperforms training-free methods and
achieves performance comparable to training-based methods on both referring and
reasoning VOS benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19592v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19592v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19559v1'></a></p>
<h2 id="a-matter-of-time-revealing-the-structure-of-time-in-vision-language-models"><a href="https://arxiv.org/abs/2510.19559v1">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.IR, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Large-scale vision-language models (VLMs) such as CLIP have gained popularity
for their generalizable and expressive multimodal representations. By
leveraging large-scale training data with diverse textual metadata, VLMs
acquire open-vocabulary capabilities, solving tasks beyond their training
scope. This paper investigates the temporal awareness of VLMs, assessing their
ability to position visual content in time. We introduce TIME10k, a benchmark
dataset of over 10,000 images with temporal ground truth, and evaluate the
time-awareness of 37 VLMs by a novel methodology. Our investigation reveals
that temporal information is structured along a low-dimensional, non-linear
manifold in the VLM embedding space. Based on this insight, we propose methods
to derive an explicit ``timeline'' representation from the embedding space.
These representations model time and its chronological progression and thereby
facilitate temporal reasoning tasks. Our timeline approaches achieve
competitive to superior accuracy compared to a prompt-based baseline while
being computationally efficient. All code and data are available at
https://tekayanidham.github.io/timeline-page/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer撰写的论文“A Matter of Time: Revealing the Structure of Time in Vision-Language Models”的全面摘要。</p>
<hr />
<h3 id="-_1">论文摘要：《时间问题：揭示视觉-语言模型中的时间结构》</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文主要探讨大型视觉-语言模型（VLMs）是否具有内在的时间感知能力，以及它们如何将视觉内容定位在时间轴上。具体来说，研究旨在回答以下问题：
* 开放词汇VLMs在多大程度上具有时间感知能力？
* 图像和文本的时间信息在嵌入空间中是如何组织的？
* 如果存在时间结构，如何有效地表示时间以预测图像的首次出现时间？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
* <strong>TIME10k数据集：</strong> 引入了一个包含超过10,000张带有时间真值的人造物体图像的基准数据集，涵盖1715年至2024年的时间范围和六个物体类别（飞机、汽车、乐器、手机、船舶、武器和弹药），用于系统评估VLMs的时间感知能力。
* <strong>时间探测（Time Probing）基线：</strong> 提出了一种基于提示的基线方法，通过计算图像嵌入与不同年份时间提示嵌入之间的点积相似度来预测图像的首次出现年份。
* <strong>嵌入空间分析：</strong> 首次系统地分析了VLMs嵌入空间中时间信息的结构。研究发现时间信息沿着低维、非线性流形组织，这与语言模型中时间信息通常呈线性结构不同。
* <strong>时间轴建模方法：</strong>
    * <strong>基于UMAP的时间轴表示：</strong> 利用UMAP（统一流形近似与投影）将高维时间嵌入投影到一维时间轴上，并通过优化UMAP参数以最大化投影顺序与真实时间顺序之间的Spearman秩相关性。
    * <strong>基于贝塞尔曲线的时间轴表示：</strong> 提出了一种在原始嵌入空间或其子空间中通过贝塞尔曲线近似时间嵌入序列的方法，显式地建模时间的年代进展，并强制执行年代一致性。</p>
<p><strong>3. 主要结果及其意义：</strong>
* <strong>VLMs的时间感知能力：</strong> 评估了37个最先进的VLMs，发现现代VLMs具有显著的时间感知能力，但性能因训练数据质量和骨干模型复杂性而异。OpenCLIP和EVA-CLIP模型表现最佳。
* <strong>时间信息的结构：</strong> 关键发现是，时间信息在VLMs的高维嵌入空间中沿着一个紧凑的、低维的非线性流形组织，并具有强大的年代结构。这表明VLMs以不同于纯语言模型的方式组织时间信息。
* <strong>时间轴建模的有效性：</strong> 提出的基于UMAP和贝塞尔曲线的时间轴方法能够有效地从嵌入空间中提取显式的时间表示。这些方法在年代对齐方面（Spearman秩相关性高达0.99）表现出色，并且在时间预测准确性方面（MAE和TAI）与基于提示的基线相比具有竞争性甚至更优的性能。
* <strong>计算效率：</strong> 基于贝塞尔曲线的方法在计算效率上显著优于UMAP方法和时间探测基线，能够实现更快的推理速度。</p>
<p><strong>4. 论文中提及的局限性：</strong>
* <strong>类别不平衡和年度范围的影响：</strong> 论文指出，类别不平衡和不同年度范围对结果的影响需要进一步研究。
* <strong>数据集扩展：</strong> TIME10k数据集未来应扩展到包含以人为中心方面（如服装）的类别。
* <strong>方法泛化性：</strong> 时间轴提取方法的评估应扩展到更多VLMs以及生成模型，以观察它们是否表现出类似的时间感知能力。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
* 深入研究类别不平衡和年度范围对时间感知性能的影响。
* 扩展TIME10k数据集以包含更多以人为中心的类别，如服装，以更全面地评估VLMs的时间感知能力。
* 将时间轴提取方法应用于更广泛的VLMs和生成模型，以探索其泛化能力。
* 探讨如果VLMs能够如此有效地隐式捕获和结构化时间信息，这种方法是否可以推广到其他序数问题。</p>
<hr />
<p>这篇论文为理解VLMs如何处理时间信息提供了开创性的见解，并提出了有效且高效的方法来从其嵌入空间中提取和利用时间结构，对于历史图像分析、文化研究和数字档案管理等领域具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce TIME10k, a benchmark
dataset of over 10,000 images with temporal ground truth, and evaluate the
time-awareness of 37 VLMs by a novel methodology.</li>
<li>Based on this insight, we propose methods
to derive an explicit ``timeline'' representation from the embedding space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19559v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19559v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19400v1'></a></p>
<h2 id="seeing-across-views-benchmarking-spatial-reasoning-of-vision-language-models-in-robotic-scenes"><a href="https://arxiv.org/abs/2510.19400v1">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a></h2>
<p><strong>Authors:</strong> Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-language models (VLMs) are essential to Embodied AI, enabling robots
to perceive, reason, and act in complex environments. They also serve as the
foundation for the recent Vision-Language-Action (VLA) models. Yet most
evaluations of VLMs focus on single-view settings, leaving their ability to
integrate multi-view information underexplored. At the same time, multi-camera
setups are increasingly standard in robotic platforms, as they provide
complementary perspectives to mitigate occlusion and depth ambiguity. Whether
VLMs can effectively leverage such multi-view inputs for robotic reasoning
therefore remains an open question. To bridge this gap, we introduce
MV-RoboBench, a benchmark specifically designed to evaluate the multi-view
spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench
consists of 1.7k manually curated QA items across eight subtasks, divided into
two primary categories: spatial understanding and robotic execution. We
evaluate a diverse set of existing VLMs, including both open-source and
closed-source models, along with enhanced versions incorporating CoT-inspired
techniques. The results show that state-of-the-art models remain far below
human performance, underscoring the substantial challenges VLMs face in
multi-view robotic perception. Additionally, our analysis uncovers two key
findings: (i) spatial intelligence and robotic task execution are positively
correlated in multi-view robotic scenarios; and (ii) strong performance on
existing general-purpose single-view spatial understanding benchmarks does not
reliably translate to success in the robotic spatial tasks assessed by our
benchmark. We release MV-RoboBench as an open resource to foster progress in
spatially grounded VLMs and VLAs, providing not only data but also a
standardized evaluation protocol for multi-view embodied reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于引入了MV-RoboBench，这是一个专门用于评估视觉-语言模型（VLMs）在机器人操作场景中多视角空间推理能力的基准。它通过1.7k个手动策划的问答项，涵盖空间理解和机器人执行两大类八个子任务，揭示了当前最先进的VLMs在多视角机器人感知方面与人类表现的巨大差距。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>关键创新在于<strong>MV-RoboBench基准的创建</strong>。不同于以往主要关注单视角设置的VLM评估，MV-RoboBench明确针对多摄像头机器人平台的需求，设计了多视角空间推理任务。其方法论包括：
*   <strong>多视角数据收集与标注：</strong> 手动策划1.7k个问答项，确保了数据质量和任务的复杂性。
*   <strong>任务分类：</strong> 将任务细分为空间理解和机器人执行两大类，更全面地评估VLMs的能力。
*   <strong>多模型评估：</strong> 评估了多种现有VLM（开源和闭源），并探索了CoT（Chain-of-Thought）启发式技术的增强版本，提供了广泛的基线。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动多视角VLM研究：</strong> MV-RoboBench将成为一个标准化的评估工具，激励研究人员开发更强大的、能够有效整合多视角信息的VLM和VLA模型。</li>
<li><strong>加速具身智能发展：</strong> 解决多视角空间推理是具身AI和机器人技术从实验室走向实际应用的关键瓶颈，该基准将直接促进这一领域的进步。</li>
<li><strong>揭示当前模型局限性：</strong> 论文明确指出当前SOTA模型与人类表现的巨大差距，以及单视角基准的成功不一定能转化为多视角机器人任务的成功，这为未来的研究指明了方向。</li>
<li><strong>促进跨模态和多模态学习：</strong> 强调了视觉和语言在复杂空间推理中的协同作用，并为多模态学习在具身智能领域的应用提供了新的挑战和机会。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>具身AI和机器人学：</strong> 这是最直接的受益领域，包括机器人抓取、导航、操作、人机协作等。</li>
<li><strong>自动驾驶：</strong> 自动驾驶车辆通常配备多个摄像头和传感器，需要整合多视角信息进行环境感知和决策。</li>
<li><strong>虚拟现实/增强现实 (VR/AR)：</strong> 在这些应用中，理解用户周围的3D空间和物体关系至关重要，多视角信息整合能力可以提升用户体验。</li>
<li><strong>智能监控：</strong> 多摄像头监控系统需要对场景进行全局理解和事件推理。</li>
<li><strong>通用视觉-语言模型开发：</strong> 任何旨在提升VLM空间推理能力的通用模型都将从这个基准中受益。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>数据规模：</strong> 1.7k个手动策划的QA项虽然质量高，但相对于大规模数据驱动的深度学习模型训练而言，可能不算庞大。这可能限制了基准在训练新模型时的直接应用，而更多地作为评估工具。</li>
<li><strong>任务复杂度：</strong> 摘要中提到“八个子任务，分为空间理解和机器人执行”，但具体任务的复杂性和多样性仍需进一步了解。例如，是否涵盖了动态场景、不确定性、精细操作等更具挑战性的方面。</li>
<li><strong>泛化能力：</strong> MV-RoboBench专注于“机器人操作”场景。虽然这很重要，但其结果和发现是否能完全泛化到其他多视角具身AI场景（如自动驾驶、复杂环境导航）仍需验证。</li>
<li><strong>评估指标：</strong> 摘要未详细说明具体的评估指标，例如，对于机器人执行任务，是评估成功率、效率还是其他更细致的指标。</li>
<li><strong>“CoT-inspired techniques”的具体实现：</strong> 摘要提到使用了CoT启发式技术，但未详细说明其具体形式和效果，这可能影响对模型性能提升的理解。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入MV-RoboBench，填补了VLM评估在多视角机器人场景中的空白，为具身AI领域带来了急需的标准化工具和新的研究方向。其发现不仅揭示了当前模型的局限性，也为未来开发更智能、更具空间推理能力的VLM和VLA模型提供了宝贵的见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce
MV-RoboBench, a benchmark specifically designed to evaluate the multi-view
spatial reasoning capabilities of VLMs in robotic manipulation.</li>
<li>The results show that state-of-the-art models remain far below
human performance, underscoring the substantial challenges VLMs face in
multi-view robotic perception.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19400v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19400v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19371v1'></a></p>
<h2 id="aegisrf-adversarial-perturbations-guided-with-sensitivity-for-protecting-intellectual-property-of-neural-radiance-fields"><a href="https://arxiv.org/abs/2510.19371v1">AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</a></h2>
<p><strong>Authors:</strong> Woo Jae Kim, Kyu Beom Han, Yoonki Cho, Youngju Na, Junsik Jung, Sooel Son, Sung-eui Yoon</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D
scene representation and novel view synthesis, protecting their intellectual
property (IP) from unauthorized use is becoming increasingly crucial. In this
work, we aim to protect the IP of NeRFs by injecting adversarial perturbations
that disrupt their unauthorized applications. However, perturbing the 3D
geometry of NeRFs can easily deform the underlying scene structure and thus
substantially degrade the rendering quality, which has led existing attempts to
avoid geometric perturbations or restrict them to explicit spaces like meshes.
To overcome this limitation, we introduce a learnable sensitivity to quantify
the spatially varying impact of geometric perturbations on rendering quality.
Building upon this, we propose AegisRF, a novel framework that consists of a
Perturbation Field, which injects adversarial perturbations into the
pre-rendering outputs (color and volume density) of NeRF models to fool an
unauthorized downstream target model, and a Sensitivity Field, which learns the
sensitivity to adaptively constrain geometric perturbations, preserving
rendering quality while disrupting unauthorized use. Our experimental
evaluations demonstrate the generalized applicability of AegisRF across diverse
downstream tasks and modalities, including multi-view image classification and
voxel-based 3D localization, while maintaining high visual fidelity. Codes are
available at https://github.com/wkim97/AegisRF.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文“AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields”的全面摘要。</p>
<hr />
<p><strong>论文摘要：AegisRF: 基于敏感度引导的对抗性扰动，用于保护神经辐射场（NeRF）的知识产权</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决神经辐射场（NeRF）的知识产权（IP）保护问题。随着NeRFs在3D场景表示和新颖视图合成方面的广泛应用，其IP面临未经授权使用的风险日益增加。现有方法在对NeRF的3D几何进行扰动时，往往会导致底层场景结构变形，从而严重降低渲染质量，因此通常避免几何扰动或将其限制在网格等显式空间中。核心研究问题是如何在不显著损害渲染质量的前提下，通过注入对抗性扰动来有效干扰未经授权的下游应用，从而保护NeRF的IP。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
AegisRF框架引入了两项关键创新：</p>
<ul>
<li><strong>可学习的敏感度（Learnable Sensitivity）：</strong> 论文引入了一种可学习的敏感度度量，用于量化3D空间中几何扰动对渲染质量影响的空间变化程度。这使得系统能够对渲染质量敏感的区域施加更严格的几何扰动约束，而在感知度较低的区域允许更大的扰动，从而在最大化对未经授权任务的干扰与最小化渲染质量下降之间取得平衡。</li>
<li><strong>AegisRF框架：</strong> 这是一个新颖的框架，包含两个核心组件：<ul>
<li><strong>扰动场（Perturbation Field）：</strong> 该组件负责将对抗性扰动注入NeRF模型的预渲染输出（颜色和体密度），旨在欺骗未经授权的下游目标模型。</li>
<li><strong>敏感度场（Sensitivity Field）：</strong> 该组件学习上述敏感度，以自适应地约束几何扰动的大小，从而在破坏未经授权使用的同时保持渲染质量。</li>
</ul>
</li>
</ul>
<p>通过在推理时直接扰动NeRF的预渲染输出，AegisRF能够为各种数据模态（如图像、体素）生成对抗性示例，从而实现对多种下游任务的通用保护。</p>
<p><strong>3. 主要结果及其意义：</strong>
实验评估表明：</p>
<ul>
<li><strong>卓越的保护性能与视觉保真度：</strong> AegisRF在多视图图像分类和基于体素的3D定位等多样化下游任务中表现出强大的IP保护能力，同时保持了高视觉保真度。与现有方法（如NeRFail和Adv-FT）相比，AegisRF在PSNR等自然度指标上表现更好，同时实现了更强的扰动效果（例如，在多视图分类中，准确率显著降低，在3D定位中，AP25显著降低）。</li>
<li><strong>敏感度引导的有效性：</strong> 对敏感度场的分析证实，通过敏感度引导来约束扰动对于维持高渲染质量至关重要。敏感度场能够学习在复杂纹理区域（如椅子）上具有较低敏感度，而在空旷空间中具有较高敏感度，这与人类视觉感知对几何扰动的敏感度变化是一致的。</li>
<li><strong>鲁棒性与可迁移性：</strong> AegisRF的扰动对常见的图像变换（如JPEG压缩、高斯模糊）具有鲁棒性。此外，它在跨模型（从代理模型到未知目标模型）的下游任务中也表现出可观的可迁移性，这增强了其在实际IP保护场景中的实用性。</li>
<li><strong>计算成本：</strong> AegisRF引入的计算成本相对于原始NeRF而言是边际性的，这表明其在实际应用中具有可行性。</li>
</ul>
<p>这些结果的意义在于，AegisRF提供了一种有效且通用的NeRF IP保护解决方案，克服了现有方法在处理几何扰动时的局限性，并在保护效果和视觉质量之间取得了更好的平衡。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到了以下局限性：</p>
<ul>
<li><strong>跨模型可迁移性：</strong> 尽管AegisRF在跨模型可迁移性方面表现出可观的性能，但其目前的水平仍然有限，特别是在应用于更复杂的任务（如3D定位）或具有显著不同架构的感知模型（如Swin-based和CNN-based骨干网络）时。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>增强跨模型可迁移性：</strong> 未来的研究应致力于开发先进技术，通过在训练过程中整合更多样化的代理模型或采用现有黑盒对抗性攻击技术，以提高AegisRF在更广泛的感知模型和下游任务中的扰动效率。这将进一步提升AegisRF的实用性。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D
scene representation and novel view synthesis, protecting their intellectual
property (IP) from unauthorized use is becoming increasingly crucial.</li>
<li>To overcome this limitation, we introduce a learnable sensitivity to quantify
the spatially varying impact of geometric perturbations on rendering quality.</li>
<li>Building upon this, we propose AegisRF, a novel framework that consists of a
Perturbation Field, which injects adversarial perturbations into the
pre-rendering outputs (color and volume density) of NeRF models to fool an
unauthorized downstream target model, and a Sensitivity Field, which learns the
sensitivity to adaptively constrain geometric perturbations, preserving
rendering quality while disrupting unauthorized use.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19371v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19371v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19336v1'></a></p>
<h2 id="damo-data-mixing-optimizer-in-fine-tuning-multimodal-llms-for-mobile-phone-agents"><a href="https://arxiv.org/abs/2510.19336v1">DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a></h2>
<p><strong>Authors:</strong> Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Mobile Phone Agents (MPAs) have emerged as a promising research direction due
to their broad applicability across diverse scenarios. While Multimodal Large
Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness
in handling multiple mobile phone tasks simultaneously remains limited.
Although multitask supervised fine-tuning (SFT) is widely adopted for multitask
learning, existing approaches struggle to determine optimal training data
compositions for peak performance. To address this challenge, we propose DaMo
(Data Mixture Optimizer) - a novel solution employing a trainable network that
predicts optimal data mixtures by forecasting downstream task performance for
any given dataset ratio. To support comprehensive evaluation, we introduce
PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on
multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse
real-world industrial mobile application scenarios. Demonstrating strong
predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo
efficiently extrapolates optimal data mixing configurations. Our results show
DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to
alternative methods. Furthermore, extensive experiments across established
benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench
reveal DaMo's superior generalization, outperforming other approaches by 2.57%
in terms of average score. When used solely for MLLM optimization on the
BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,
DaMo maintains robust scalability, preserving its effectiveness when applied to
other model architectures. The code and dataset are available at
https://github.com/OPPO-Mente-Lab/DaMo.git</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Kai Shi等人撰写的论文“DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents”的全面摘要。</p>
<hr />
<h3 id="damo-llm">论文摘要：DaMo: 移动智能体多模态LLM微调中的数据混合优化器</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决多模态大型语言模型（MLLMs）在移动智能体（MPAs）中同时处理多任务时效率有限的问题。具体而言，尽管多任务监督微调（SFT）被广泛采用，但现有方法难以确定最佳的训练数据组成，以实现模型在下游任务上的峰值性能。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
*   <strong>DaMo (Data Mixture Optimizer) 框架：</strong> 提出了一种新颖的解决方案，通过一个可训练的网络来预测最佳数据混合比例。该网络能够根据给定的数据集比例预测下游任务性能，从而高效地推断出最优数据混合配置。这避免了传统方法中昂贵的手动迭代和穷举搜索。
*   <strong>下游任务性能预测 (DaPP)：</strong> DaMo的核心是DaPP方法，它利用一个可训练的神经网络直接预测模型在下游任务上的性能，而无需实际模型训练。这解决了现有方法未能直接关联数据混合与模型在下游任务上性能的问题。
*   <strong>PhoneAgentBench 基准测试：</strong> 为了支持全面的评估，论文引入了PhoneAgentBench，这是首个专门用于评估MLLMs在多模态移动电话任务上的基准测试。它包含1235个QA对，涵盖了真实世界的工业移动应用场景，并评估了复杂任务规划、设备原生工具使用、多模态记忆和屏幕上下文理解四项基本能力。
*   <strong>可扩展性与泛化性：</strong> DaMo被设计为模型无关的预测器，通过少量校准样本的线性映射，可以有效地将其应用于其他模型架构，并保持稳定的预测准确性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>PhoneAgentBench 性能提升：</strong> DaMo在PhoneAgentBench上实现了3.38%的性能提升，显著优于其他替代方法。与未进行SFT的原始模型相比，DaMo在PhoneAgentBench上实现了超过23%的性能提升。
*   <strong>卓越的泛化能力：</strong> 在BFCL-v3、MME-Reasoning、MME-Perception和OCRBench等通用基准测试中，DaMo的平均得分比其他方法高出2.57%，显示出其优越的泛化能力。
*   <strong>特定任务优化效果：</strong> 当仅用于BFCL-v3任务的MLLM优化时，DaMo将指标提高了12.47%，表明专注于任务特定目标可以带来显著的改进。
*   <strong>鲁棒的预测能力与可扩展性：</strong> 在小规模试点实验中，DaMo展示了强大的预测能力（R²=0.81），能够高效地推断出最佳数据混合配置。此外，DaMo在应用于其他模型架构时仍能保持其有效性，Pearson相关系数普遍高于0.75，经过线性映射后甚至超过0.9。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>样本顺序和固定数据混合比例的假设：</strong> 该研究基于两个关键假设：1) 忽略单个数据集中样本的顺序；2) 在整个训练过程中保持固定的数据混合比例。
*   <strong>动态数据混合的探索性：</strong> 尽管初步尝试放宽这些假设（特别是通过动态数据混合调整）仍在探索中，但尚未建立系统的方法来推断最佳动态混合，也未量化其计算成本和性能收益。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>动态数据混合优化框架：</strong> 计划将蒙特卡洛树搜索（MCTS）与强化学习相结合，以迭代确定阶段特定的数据混合，旨在推断最佳混合轨迹，从而减轻任务冲突和灾难性遗忘。
*   <strong>整合样本质量指标：</strong> 建议将样本质量指标作为下游性能预测的输入变量，以实现难度感知采样。
*   <strong>PhoneAgentBench 增强：</strong> 进一步增强PhoneAgentBench，使其更好地适应设备端AI部署的快速发展需求，确保其对新兴移动中心AI范式的适应性。</p>
<hr />
<p>这篇论文通过引入DaMo框架和PhoneAgentBench基准测试，为移动智能体领域的多模态LLMs微调提供了一个创新且高效的解决方案。其核心贡献在于能够预测最佳数据混合比例，显著提升了模型在多任务和通用任务上的性能，并展示了良好的泛化性和可扩展性。尽管存在对样本顺序和固定混合比例的假设，但论文也明确指出了未来的研究方向，以解决这些局限性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, we propose DaMo
(Data Mixture Optimizer) - a novel solution employing a trainable network that
predicts optimal data mixtures by forecasting downstream task performance for
any given dataset ratio.</li>
<li>To support comprehensive evaluation, we introduce
PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on
multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse
real-world industrial mobile application scenarios.</li>
<li>Our results show
DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to
alternative methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19336v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19336v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-23 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
