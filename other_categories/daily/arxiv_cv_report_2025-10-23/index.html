<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-23 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-22/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-24/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-23">Arxiv Computer Vision Papers - 2025-10-23</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-22" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-22)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#a-survey-on-cache-methods-in-diffusion-models-toward-efficient-multi-modal-generation" class="nav-link">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#advances-in-4d-representation-geometry-motion-and-interaction" class="nav-link">Advances in 4D Representation: Geometry, Motion, and Interaction</a>
                </li>
                <li class="nav-item">
                    <a href="#how-to-evaluate-monocular-depth-estimation" class="nav-link">How to Evaluate Monocular Depth Estimation?</a>
                </li>
                <li class="nav-item">
                    <a href="#from-forecasting-to-planning-policy-world-model-for-collaborative-state-action-prediction" class="nav-link">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#pragmatic-heterogeneous-collaborative-perception-via-generative-communication-mechanism" class="nav-link">Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</a>
                </li>
                <li class="nav-item">
                    <a href="#decomposed-attention-fusion-in-mllms-for-training-free-video-reasoning-segmentation" class="nav-link">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#a-matter-of-time-revealing-the-structure-of-time-in-vision-language-models" class="nav-link">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-across-views-benchmarking-spatial-reasoning-of-vision-language-models-in-robotic-scenes" class="nav-link">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a>
                </li>
                <li class="nav-item">
                    <a href="#aegisrf-adversarial-perturbations-guided-with-sensitivity-for-protecting-intellectual-property-of-neural-radiance-fields" class="nav-link">AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</a>
                </li>
                <li class="nav-item">
                    <a href="#damo-data-mixing-optimizer-in-fine-tuning-multimodal-llms-for-mobile-phone-agents" class="nav-link">DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-23">Arxiv Computer Vision Papers - 2025-10-23</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-22">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-22)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»å¤©ç Arxiv è®¡ç®æºè§è§è®ºæä¸»è¦å´ç»<strong>å¤æ¨¡æå­¦ä¹ ã3D/4D è§è§çè§£ãé«ææ¨¡åè®¾è®¡ä»¥åæºå¨äººæç¥ä¸è§å</strong>å±å¼ãæ¾èè¶å¿åæ¬å°å¤§åè¯­è¨æ¨¡å (LLMs) çè½åæ©å±å°è§è§é¢åï¼ä»¥å®ç°æ´å¤æçæ¨çåäº¤äºï¼åæ¶å³æ³¨æ¨¡åæçåå®éåºç¨ã</p>
<p><strong>ä¸»è¦ä¸»é¢åè¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æå­¦ä¹ ä¸å¤§åè¯­è¨æ¨¡å (MLLMs) çèåï¼</strong> å¤ç¯è®ºææ¢ç´¢äº MLLMs å¨è§é¢çè§£ãç©ºé´æ¨çåæºå¨äººä»£çä¸­çåºç¨ï¼å¼ºè°äºè§è§-è¯­è¨æ¨¡åçå¼ºå¤§æ½åã</li>
<li><strong>3D/4D è§è§çè§£ä¸è¡¨ç¤ºï¼</strong> å¯¹ 4D å ä½ãè¿å¨åäº¤äºçè¡¨ç¤ºä»¥åç¥ç»è¾å°åº (NeRF) çä¿æ¤ååºç¨æ¯éè¦çç ç©¶æ¹åã</li>
<li><strong>æ¨¡åæçä¸ä¼åï¼</strong> æ©æ£æ¨¡åä¸­çç¼å­æ¹æ³ä»¥åå¤æ¨¡æ LLMs çæ°æ®æ··åä¼åè¡¨æäºå¯¹æ¨¡åæçåå¾®è°ç­ç¥çæç»­å³æ³¨ã</li>
<li><strong>è¯ä¼°ä¸åºåï¼</strong> å¦ä½ææè¯ä¼°åç®æ·±åº¦ä¼°è®¡åè§è§-è¯­è¨æ¨¡åçç©ºé´æ¨çè½åæ¯å³é®çææã</li>
<li><strong>æºå¨äººæç¥ä¸è§åï¼</strong> ä»é¢æµå°è§åçç­ç¥ä¸çæ¨¡åä»¥åå¼æåä½æç¥å±ç¤ºäºè®¡ç®æºè§è§å¨æºå¨äººé¢åçå®éåºç¨ã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation" by Jiacheng Liu et al. (1)</strong>ï¼è¿æ¯ä¸ç¯åæ¶ä¸éè¦çç»¼è¿°ï¼å®ç³»ç»å°æ¢³çäºæ©æ£æ¨¡åä¸­çç¼å­ææ¯ï¼å¯¹äºæåå¤æ¨¡æçææçå·ææå¯¼æä¹ãèèå°æ©æ£æ¨¡åå¨çæé¢åçå¹¿æ³åºç¨ï¼è¿ç¯ç»¼è¿°å°å¯¹ç ç©¶äººåäº§çæ·±è¿å½±åã</li>
<li><strong>"From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction" by Zhida Zhao et al. (4)</strong>ï¼è¿ç¯è®ºææåºäºä¸ä¸ªä»é¢æµè½¬åè§åçç­ç¥ä¸çæ¨¡åï¼å¯¹äºæºå¨äººåä½åå¤æä»»å¡æ§è¡å·æçªç ´æ§æä¹ãå®å°è§è§æç¥ä¸å³ç­å¶å®æ´ç´§å¯å°ç»åèµ·æ¥ï¼æ¯è¿åæ´æºè½æºå¨äººç³»ç»çéè¦ä¸æ­¥ã</li>
<li><strong>"A Matter of Time: Revealing the Structure of Time in Vision-Language Models" by Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer (7)</strong>ï¼è¿ç¯è®ºææ·±å¥æ¢è®¨äºè§è§-è¯­è¨æ¨¡åä¸­æ¶é´ç»æççè§£ï¼è¿æ¯ä¸ä¸ªè¢«ç¸å¯¹å¿½è§ä½è³å³éè¦çæ¹é¢ãå¯¹æ¶é´æ¨ççæ´å¥½çè§£å°æå¤§å°æåæ¨¡åå¨è§é¢çè§£åå¨æåºæ¯åæä¸­çè½åã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>4D å ä½ãè¿å¨åäº¤äºçç»ä¸è¡¨ç¤º (2)</strong>ï¼è¶è¶ä¼ ç»ç 3D å»ºæ¨¡ï¼å°æ¶é´ç»´åº¦çº³å¥è¡¨ç¤ºä¸­ï¼ä»¥æ´å¥½å°ææå¨æåºæ¯ã</li>
<li><strong>åºäºçæå¼éä¿¡æºå¶çå¼æåä½æç¥ (5)</strong>ï¼å©ç¨çææ¨¡åä¿è¿ä¸åä¼ æå¨ææºè½ä½ä¹é´çææä¿¡æ¯å±äº«ï¼ä»¥åæå¼ææ§ææã</li>
<li><strong>è®­ç»æ å³çè§é¢æ¨çåå² (6)</strong>ï¼éè¿åè§£æ³¨æåèåï¼å¨ä¸è¿è¡é¢å¤è®­ç»çæåµä¸å®ç°è§é¢æ¨çåå²ï¼è¿é¢ç¤ºçæ´çµæ´»åéç¨çæ¨¡ååºç¨ã</li>
<li><strong>ç¥ç»è¾å°åº (NeRF) çç¥è¯äº§æä¿æ¤ (9)</strong>ï¼éç NeRF çæ®åï¼å¯¹å¶è¿è¡å¯¹ææ§æ°å¨ä»¥ä¿æ¤ç¥è¯äº§ææ¯ä¸ä¸ªæ°é¢ä¸éè¦çç ç©¶æ¹åã</li>
<li><strong>å¤æ¨¡æ LLMs çæ°æ®æ··åä¼å (10)</strong>ï¼éå¯¹ç¹å®åºç¨ï¼å¦ç§»å¨ææºä»£çï¼å¯¹ MLLMs è¿è¡é«æå¾®è°ï¼å¼ºè°äºå®éé¨ç½²ä¸­çä¼åç­ç¥ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå¸ææ·±å¥äºè§£ç¹å®é¢åçå¿ç¢ç ç©¶äººåï¼æå»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºçææ¨¡ååæçæå´è¶£çç ç©¶äººåï¼</strong><ul>
<li><strong>1. "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation"</strong> (æä¾å¨é¢çèæ¯åæªæ¥æ¹å)</li>
</ul>
</li>
<li><strong>å¯¹äºæºå¨äººåå·èº«æºè½æå´è¶£çç ç©¶äººåï¼</strong><ul>
<li><strong>4. "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction"</strong> (ä»£è¡¨äºæºå¨äººè§åçææ°è¿å±)</li>
<li><strong>8. "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes"</strong> (æä¾äºè¯ä¼° MLLMs å¨æºå¨äººåºæ¯ä¸­ç©ºé´æ¨çè½åçå³é®åºå)</li>
</ul>
</li>
<li><strong>å¯¹äºå¤æ¨¡æå­¦ä¹ åè§é¢çè§£æå´è¶£çç ç©¶äººåï¼</strong><ul>
<li><strong>7. "A Matter of Time: Revealing the Structure of Time in Vision-Language Models"</strong> (æ·±å¥æ¢è®¨äºè§é¢çè§£ä¸­çæ ¸å¿ææ)</li>
<li><strong>6. "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation"</strong> (å±ç¤ºäº MLLMs å¨è§é¢ä»»å¡ä¸­çæ°é¢åºç¨)</li>
</ul>
</li>
<li><strong>å¯¹äº 3D è§è§åæ¨¡åå®å¨æå´è¶£çç ç©¶äººåï¼</strong><ul>
<li><strong>9. "AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields"</strong> (æåºäº NeRF å®å¨æ§çæ°é®é¢åè§£å³æ¹æ¡)</li>
</ul>
</li>
</ul>
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥ Arxiv è®¡ç®æºè§è§é¢åçå³é®è¿å±ï¼å¹¶æå¯¼æ¨è¿è¡æ´æ·±å¥çéè¯»ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.19755v1">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a></li>
<li><a href="#2510.19255v1">Advances in 4D Representation: Geometry, Motion, and Interaction</a></li>
<li><a href="#2510.19814v1">How to Evaluate Monocular Depth Estimation?</a></li>
<li><a href="#2510.19654v1">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a></li>
<li><a href="#2510.19618v1">Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</a></li>
<li><a href="#2510.19592v1">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</a></li>
<li><a href="#2510.19559v1">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a></li>
<li><a href="#2510.19400v1">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a></li>
<li><a href="#2510.19371v1">AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</a></li>
<li><a href="#2510.19336v1">DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.19755v1'></a></p>
<h2 id="a-survey-on-cache-methods-in-diffusion-models-toward-efficient-multi-modal-generation"><a href="https://arxiv.org/abs/2510.19755v1">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a></h2>
<p><strong>Authors:</strong> Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion Models have become a cornerstone of modern generative AI for their
exceptional generation quality and controllability. However, their inherent
\textit{multi-step iterations} and \textit{complex backbone networks} lead to
prohibitive computational overhead and generation latency, forming a major
bottleneck for real-time applications. Although existing acceleration
techniques have made progress, they still face challenges such as limited
applicability, high training costs, or quality degradation.
  Against this backdrop, \textbf{Diffusion Caching} offers a promising
training-free, architecture-agnostic, and efficient inference paradigm. Its
core mechanism identifies and reuses intrinsic computational redundancies in
the diffusion process. By enabling feature-level cross-step reuse and
inter-layer scheduling, it reduces computation without modifying model
parameters. This paper systematically reviews the theoretical foundations and
evolution of Diffusion Caching and proposes a unified framework for its
classification and analysis.
  Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}. This trend enhances caching flexibility across diverse tasks and
enables integration with other acceleration techniques such as sampling
optimization and model distillation, paving the way for a unified, efficient
inference framework for future multimodal and interactive applications. We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jiacheng Liuç­äººæ°åçè®ºæãA Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generationãçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼æ©æ£æ¨¡åä¸­çç¼å­æ¹æ³ï¼è¿åé«æå¤æ¨¡æçæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
æ©æ£æ¨¡åï¼DMsï¼å¨çæå¼AIé¢ååå¾äºçªç ´æ§è¿å±ï¼ä½å¶åºæçâå¤æ­¥è¿­ä»£âåâå¤æéª¨å¹²ç½ç»âå¯¼è´äºå·¨å¤§çè®¡ç®å¼éåçæå»¶è¿ãè¿ä¸¥éé»ç¢äºæ©æ£æ¨¡åå¨å®æ¶äº¤äºåºç¨ä¸­çé¨ç½²ãå°½ç®¡ç°æå éææ¯åå¾äºä¸äºè¿å±ï¼ä½å®ä»¬ä»é¢ä¸´éç¨æ§æéãè®­ç»ææ¬é«ææçæè´¨éä¸éç­ææãå æ­¤ï¼è®ºææ¨å¨è§£å³å¦ä½å¨ä¸çºç²çæè´¨éçåæä¸ï¼é«æéä½æ©æ£æ¨¡åçæ¨çå»¶è¿è¿ä¸å³é®é®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæçæ ¸å¿åæ°å¨äºæåºå¹¶ç³»ç»æ§å°ç»¼è¿°äº<strong>æ©æ£ç¼å­ï¼Diffusion Cachingï¼</strong>è¿ä¸è®­ç»æ å³ãæ¶ææ å³ä¸é«æçæ¨çèå¼ãå¶ä¸»è¦æ¹æ³è®ºè´¡ç®åæ¬ï¼
*   <strong>è¯å«åéç¨è®¡ç®åä½ï¼</strong> æ©æ£ç¼å­çæ ¸å¿æºå¶å¨äºè¯å«å¹¶éç¨æ©æ£æ¨çè¿ç¨ä¸­åºæçè®¡ç®åä½ãéè¿ç¹å¾å±é¢çè·¨æ­¥éç¨åå±é´è°åº¦ï¼å¨ä¸ä¿®æ¹æ¨¡ååæ°çæåµä¸ææéä½äºè®¡ç®è´è·ã
*   <strong>ç»ä¸åç±»ååææ¡æ¶ï¼</strong> è®ºææå»ºäºé¦ä¸ªæ©æ£ç¼å­çç³»ç»æ§åç±»ååææ¡æ¶ï¼ä»è§¦åæ¡ä»¶ãéç¨ç²åº¦åæ´æ°ç­ç¥ä¸ä¸ªç»´åº¦æ­ç¤ºäºä¸åæ¹æ³çåå¨é»è¾åææ¯æ¼è¿ã
*   <strong>æ¼è¿è½¨è¿¹åæï¼</strong> è®ºæéè¿å¯¹ä»£è¡¨æ§æ¹æ³çæ¯è¾åæï¼æ­ç¤ºäºæ©æ£ç¼å­ä»âéæéç¨âåâå¨æé¢æµâçæ¼è¿è½¨è¿¹ã
    *   <strong>éæç¼å­ï¼Static Cachingï¼ï¼</strong> éç¨åºå®éç¨ç­ç¥ï¼å¨é¢å®ä¹å±ææ¶é´æ­¥è¿è¡ç¼å­ï¼ä¸å¨æææ¨çè¿è¡ä¸­ä¿æä¸åãä¾å¦DeepCacheå©ç¨U-Netä¸éæ ·å±ç¹å¾ååå°çç¹ç¹ï¼FasterDiffusionéè¿ç¼ç å¨ä¼ æ­éç¨ç¼ç å¨ç¹å¾ã
    *   <strong>å¨æç¼å­ï¼Dynamic Cachingï¼ï¼</strong> å¼å¥éè¯¯æ£æ¥æºå¶ï¼æ ¹æ®é¢å®ä¹ææ ï¼å¦ç¹å¾ç¸ä¼¼åº¦ï¼å¨æå³å®ä½æ¶è¿è¡è®¡ç®ãæ´æ°ç¼å­æç´æ¥ä½¿ç¨ç¼å­ã
        *   <strong>æ¶é´æ­¥èªéåºç¼å­ï¼Timestep-Adaptive Cachingï¼ï¼</strong> æ ¹æ®ç¹å¾å¨æ©æ£é¶æ®µçä¸åç¨³å®æ§åå¨ææ§ï¼å¨æè°æ´ç¼å­æ¿æ´»åå·æ°æ¶æºãä¾å¦TeaCacheéè¿è®¡ç®ç¸é»è¾åºçç¸å¯¹L1å·®å¼æ¥é¢æµè¾åºååã
        *   <strong>å±èªéåºç¼å­ï¼Layer-Adaptive Cachingï¼ï¼</strong> æ ¹æ®ç½ç»ä¸­åå±ç¹å¾çå¨æç¹æ§ï¼å¦æ¢¯åº¦å¹åº¦ãç¹å¾å·®å¼ï¼ï¼èªéåºè°æ´åå±çç¼å­åæ´æ°é¢çãä¾å¦Block Cachingæ ¹æ®åè¾åºçç¸å¯¹ç»å¯¹ååæ¥å³å®ç¼å­ç­ç¥ã
        *   <strong>é¢æµç¼å­ï¼Predictive Cachingï¼ï¼</strong> å°ç¼å­è§ä¸ºæ°å¼é¢æµé®é¢ï¼å©ç¨åå²ç¼å­è¡¨ç¤ºæ¾å¼é¢æµæªæ¥å¤æ­¥ç¹å¾ãä¾å¦TaylorSeeréè¿æ³°åçº§æ°å±å¼æ¥é¢æµç¹å¾è½¨è¿¹ï¼HiCacheéç¨Hermiteå¤é¡¹å¼ä½ä¸ºé¢æµåºç¡ã
        *   <strong>æ··åç¼å­ï¼Hybrid Cachingï¼ï¼</strong> ç»åå¤ç§ç»´åº¦ï¼æ¶é´æ­¥ãç½ç»å±çº§ãç¹å¾å¨æï¼çç­ç¥ï¼å®ç°è·¨ç©ºé´åæ¶é´åçè®¡ç®/éç¨åè°ãä¾å¦ClusCaå¼å¥ç©ºé´tokenç¸ä¼¼æ§ï¼SpeCaéç¨é¢æµ-éªè¯é­ç¯æ¡æ¶ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçæ¨çå éï¼</strong> æ©æ£ç¼å­æ¹æ³å¨ä¸çºç²çæè´¨éçåæä¸ï¼æ¾èéä½äºæ©æ£æ¨¡åçè®¡ç®å¼éåæ¨çå»¶è¿ãä¾å¦ï¼TaylorSeerå®ç°äºè¿5åçå éï¼FreqCaå¨Qwen-Imageä¸å®ç°äºé«è¾¾7.14åçæ¨çå éå99%çåå­èçã
*   <strong>è®­ç»æ å³åæ¶ææ å³ï¼</strong> æ©æ£ç¼å­ä½ä¸ºä¸ç§çº¯æ¨çæ¶ä¼åææ¯ï¼æ éé¢å¤çè®­ç»æå¾®è°ï¼ä¸ä¸æ¨¡åæ¶ææ å³ï¼ä½¿å¶å·æé«åº¦ççµæ´»æ§åæ®éæ§ã
*   <strong>ä¸ç°æææ¯çäºè¡¥æ§ï¼</strong> æ©æ£ç¼å­å¯ä»¥ä¸éæ ·ä¼åãæ¨¡åè¸é¦ç­å¶ä»å éææ¯ç»åï¼å®ç°äºè¡¥æçï¼å±åæå»ºç»ä¸é«æçæ¨çæ¡æ¶ã
*   <strong>å¹¿æ³çåºç¨æ½åï¼</strong> æ©æ£ç¼å­å·²å¨å¾ååè§é¢ç¼è¾ã3Dçæãé³é¢çæãè¶åè¾¨çãä¸çæ¨¡åãç¦»æ£æ©æ£æ¨¡ååAI for Scienceç­å¤ç§å¤æ¨¡æçæä»»å¡ä¸­å±ç°åºå¼ºå¤§çéåºæ§åéç¨æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åå­æ¶èææï¼</strong> ç¼å­ä¸­é´æ¿æ´»éè¦å¤§éGPUåå­ï¼å°¤å¶æ¯å¨é«åè¾¨çå¾åçæãé¿åºåè§é¢åæä»¥åå¤ä»»å¡å¹¶åæ¨çåºæ¯ä¸ï¼å¯è½å¯¼è´åå­æº¢åºï¼OOMï¼ã
*   <strong>çæè´¨éä¸éï¼</strong> ç¼å­å¼å¥çè¿ä¼¼è¯¯å·®å¯è½å¯¼è´ç»ç²åº¦ä¿¡æ¯ä¸¢å¤±ãçº¹çæ¨¡ç³ãè¾¹ç¼å¤±çåå¾®ç»ææå¤±ï¼å¨é«å éæ¯ä¸å°¤ä¸ºææ¾ï¼éå¶äºå¶å¨é«ç²¾åº¦ä»»å¡ä¸­çåºç¨ã
*   <strong>çè®ºåºç¡ä¸è¶³ï¼</strong> ç°æç¼å­æ¹æ³å¤§å¤æ¯å·¥ç¨é©±å¨çæ¢ç´¢ï¼ç¼ºä¹å¯è§£éåå¯éªè¯ççè®ºæ¡æ¶æ¥è¡¨å¾ç¼å­å¼å¥çåå¸åå·®åè¯¯å·®ä¼ æ­ã
*   <strong>ä¸å¶ä»å éç­ç¥çéæææï¼</strong> å°½ç®¡ç¼å­å·æäºè¡¥æ§ï¼ä½å¤ç§å éæºå¶èååºç¨æ¶ï¼å¶å¼å¥çè¯¯å·®å¯è½å å ï¼å¯¼è´æ¨¡åä¸åå§åå¸çåå·®å å§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>åå­æçä¼åï¼</strong> å¼åæ´é«æçç¼å­ç­ç¥ï¼ä¾å¦ç´¯ç§¯æ®å·®ç¹å¾ï¼CRFï¼ç¼å­ï¼ä»¥æ¾èåå°åå­å ç¨ã
*   <strong>è¯¯å·®åæä¸æ§å¶ï¼</strong> å»ºç«æ´ä¸¥æ ¼ãç»ä¸çç¼å­è¯±å¯¼è¯¯å·®åæçè®ºï¼éåå¶å¯¹æ©æ£å¨æçå½±åï¼å¹¶æ¢ç´¢ç¼å­ç­ç¥ä¸ä¸åéæ ·æ¹æ³ï¼å¦DDIMãFlow Matchingï¼çå¼å®¹æ§ã
*   <strong>æ·±åº¦éææ¡æ¶ï¼</strong> è®¾è®¡ååæ§çéææ¡æ¶ï¼ä»¥éåæ¨¡åãç¼å­ãéåååªæç­å¤ç§å éç­ç¥ä¹é´çç¸äºä½ç¨ï¼å¹¶è¯å«æä½³éææ¡ä»¶ï¼ä»¥å¹³è¡¡å éåçæè´¨éã
*   <strong>å¤æ¨¡æåäº¤äºå¼åºç¨ï¼</strong> éçæ¨¡åè§æ¨¡çå¢é¿åçæä»»å¡çå¤æ¨¡æåãäº¤äºå¼åï¼æ©æ£ç¼å­æææä¸ºä¸ä¸ä»£é«æçææ¡æ¶çå³é®ç»æé¨åï¼å¹¶æ¨å¨å®æ¶ãå¯æç»­ççæå¼AIåå±ã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºæ©æ£æ¨¡åå éé¢åæä¾äºä¸ä¸ªå¨é¢çè§è§ï¼ç¹å«å¼ºè°äºç¼å­æ¹æ³ä½ä¸ºä¸ç§æåæ¯çè§£å³æ¹æ¡ãå®ä¸ä»ç³»ç»å°åé¡¾äºç°æææ¯ï¼è¿æåºäºä¸ä¸ªç»ä¸çæ¡æ¶æ¥çè§£ååç±»è¿äºæ¹æ³ï¼å¹¶æåºäºæªæ¥çç ç©¶æ¹åï¼å¯¹äºæ¨å¨çæå¼AIçå®éé¨ç½²åçè®ºåå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}.</li>
<li>We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19755v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19755v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19255v1'></a></p>
<h2 id="advances-in-4d-representation-geometry-motion-and-interaction"><a href="https://arxiv.org/abs/2510.19255v1">Advances in 4D Representation: Geometry, Motion, and Interaction</a></h2>
<p><strong>Authors:</strong> Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI). While our survey is not the first
of its kind, we build our coverage of the domain from a unique and distinctive
perspective of 4D representations\/}, to model 3D geometry evolving over time
while exhibiting motion and interaction. Specifically, instead of offering an
exhaustive enumeration of many works, we take a more selective approach by
focusing on representative works to highlight both the desirable properties and
ensuing challenges of each representation under different computation,
application, and data scenarios. The main take-away message we aim to convey to
the readers is on how to select and then customize the appropriate 4D
representations for their tasks. Organizationally, we separate the 4D
representations based on three key pillars: geometry, motion, and interaction.
Our discourse will not only encompass the most popular representations of
today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),
but also bring attention to relatively under-explored representations in the 4D
context, such as structured models and long-range motions. Throughout our
survey, we will reprise the role of large language models (LLMs) and video
foundational models (VFMs) in a variety of 4D applications, while steering our
discussion towards their current limitations and how they can be addressed. We
also provide a dedicated coverage on what 4D datasets are currently available,
as well as what is lacking, in driving the subfield forward. Project
page:https://mingrui-zhao.github.io/4DRep-GMI/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºMingrui Zhaoç­äººæ°åçè®ºæâAdvances in 4D Representation: Geometry, Motion, and Interactionâçæè¦ï¼åå®¹åºäºæ¨æä¾çPDFå¨æåæè¦ã</p>
<p><strong>è®ºææè¦ï¼4Dè¡¨ç¤ºçè¿å±ï¼å ä½ãè¿å¨åäº¤äº</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨å¨é¢åé¡¾ååæ4Dæ°æ®ï¼å³éæ¶é´æ¼åå¹¶å±ç°è¿å¨åäº¤äºç3Då ä½ï¼ççæåéå»ºé¢åãæ ¸å¿é®é¢æ¯å¦ä½éæ©åå®å¶åéç4Dè¡¨ç¤ºæ¹æ³ï¼ä»¥åºå¯¹ä¸åè®¡ç®ãåºç¨åæ°æ®åºæ¯ä¸çææï¼å¹¶å¼¥è¡¥ç°æç»¼è¿°å¨å ä½ãè¿å¨åäº¤äºè¡¨ç¤ºæ¹é¢çä¸è¶³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç¬ç¹ç4Dè¡¨ç¤ºè§è§ï¼</strong> è®ºæä»4Dè¡¨ç¤ºçç¬ç¹è§è§æå»ºå¶è¦çèå´ï¼å°4Dè¡¨ç¤ºåä¸ºä¸ä¸ªå³é®æ¯æ±ï¼å ä½ãè¿å¨åäº¤äºã
*   <strong>å ä½è¡¨ç¤ºçåç±»ä¸åæï¼</strong> åºåäºéç»æåè¡¨ç¤ºï¼å¦ç¥ç»è¾å°åºNeRFså3Dé«æ¯æ³¼æº3DGSï¼åç»æåè¡¨ç¤ºï¼å¦æ¨¡æ¿ãåºäºé¨ä»¶çæ¨¡ååå¾ï¼ï¼å¹¶è¯¦ç»åæäºå®ä»¬å¨4Dç¯å¢ä¸­çä¼å¿ãå±éæ§åææã
*   <strong>è¿å¨å»ºæ¨¡çå¨é¢è¦çï¼</strong> è¯¦ç»è®¨è®ºäºåç§ä¸»è¦çè¿å¨ç±»å«ï¼å³èè¿å¨ãåºäºåå½¢çè¿å¨ãåºäºè·è¸ªçè¿å¨åæ··åè¿å¨ï¼å¹¶åæäºä¸åè¿å¨ç±»åå¦ä½ä¸è¡¨ç¤ºéæ©ç¸äºä½ç¨ã
*   <strong>äº¤äºå»ºæ¨¡çæ·±å¥æ¢è®¨ï¼</strong> æ¢è®¨äºå¦ä½è¡¨ç¤ºäº¤äºå®ä½ãå§¿æãæ¥è§¦ãå¨ä½åå¯ä¾æ§ï¼ä»¥åå¦ä½æ´åç©çåéªä»¥ç¡®ä¿ç©çåçæ§ã
*   <strong>æ°æ®é©±å¨åéªåè®­ç»ç­ç¥ï¼</strong> å¼ºè°äºå¤§åè¯­è¨æ¨¡åï¼LLMsï¼åè§é¢åºç¡æ¨¡åï¼VFMsï¼å¨4Dåºç¨ä¸­çä½ç¨ï¼å¹¶åæäºéåºæ¯ä¼åãç«¯å°ç«¯è®­ç»åæ··åä¼åç­è®­ç»ç­ç¥ã
*   <strong>æ°æ®éååºåçä¸é¨è¦çï¼</strong> æä¾äºç°æ4Dæ°æ®éçè¯¦ç»åè¡¨ï¼å¹¶æåºäºå½åæ°æ®éå¨æ¨å¨è¯¥å­é¢ååå±æ¹é¢çä¸è¶³ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºææ²¡æåç°å·ä½å®éªç»æï¼èæ¯ä¸ç¯ç»¼è¿°ï¼å¶ä¸»è¦âç»æâæ¯ï¼
*   <strong>ç³»ç»æ§æ¡æ¶ï¼</strong> å»ºç«äºä¸ä¸ªä»¥è¡¨ç¤ºä¸ºä¸­å¿çç»ä¸æ¦å¿µæ¡æ¶ï¼ç¨äºçè§£åæå¯¼4Dåå®¹çæè·ãçè§£ãåæåäº¤äºã
*   <strong>æ­ç¤ºæè¡¡ï¼</strong> æç¡®äºä¸åè¡¨ç¤ºéæ©å¨æçãä¿çåº¦åæ³åè½åä¹é´åºæçæè¡¡ï¼å¸®å©è¯»èæ ¹æ®ä»»å¡éæ±éæ©æåéçè¡¨ç¤ºã
*   <strong>æ¨å¨æªæ¥ç ç©¶ï¼</strong> éè¿è¯å«å½åå±éæ§åæ°å´è¶å¿ï¼ä¸º4Dè¡¨ç¤ºé¢åçæªæ¥ç ç©¶æ¹åæä¾äºæ¸æ°çè·¯çº¿å¾ï¼ä¾å¦å¼åç»ä¸ãèªéåºåç»ææç¥çè¡¨ç¤ºï¼ä»¥åæ´åä¸çç¥è¯åç©çåéªã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>ææçµæ´»æ§ä¸è¶³ï¼</strong> ç½æ ¼è¡¨ç¤ºå¨å¤çææååï¼å¦åè£ãåå¹¶ï¼åä½ç§¯ç°è±¡ï¼å¦äºãçé¾ï¼æ¶å­å¨å±éæ§ã
*   <strong>è®¡ç®ææ¬é«æï¼</strong> NeRFå3DGSç­åºäºä¼åçæ¹æ³è®­ç»æ¶é´é¿ãè®¡ç®èµæºéæ±å¤§ï¼é¾ä»¥æ©å±å°å¤§è§æ¨¡æåä¸åºç¨ã
*   <strong>ç¨çè¾å¥åºæ¯çææï¼</strong> NeRFså3DGSå¨ç¨çè¾å¥æ¡ä»¶ä¸è¡¨ç°ä¸ä½³ï¼éè¦å¯éçå¤è§è§è§é¢æè·ã
*   <strong>éç»æåè¡¨ç¤ºçå±éæ§ï¼</strong> å°½ç®¡NeRFså3DGSæµè¡ï¼ä½å®ä»¬ä¸éåéè¦ç´§ååç»æåè¡¨ç¤ºçå»ºæ¨¡ãç¼è¾æäº¤äºç­4Dä»»å¡ã
*   <strong>ç»æåè¡¨ç¤ºçæ³åè½åï¼</strong> æ¨¡æ¿ååºäºé¨ä»¶çæ¨¡åå¨è·¨ç±»å«æ³åæ¹é¢ä»æéå¶ï¼éè¦æ´å¤å·¥ä½æ¥èªå¨æ¨æ­éª¨æ¶ç»æåèç®æéã
*   <strong>æ°æ®éç¨ç¼ºï¼</strong> ç¼ºä¹å¤§è§æ¨¡ãé«è´¨éãå¸¦æå®æ´å°é¢çå®å ä½ãå¤è§ãè¿å¨åäº¤äºæ æ³¨ç4Dæ°æ®éï¼éå¶äºæ¨¡åçè®­ç»åæ³åè½åã
*   <strong>è¯ä¼°ææ ä¸è¶³ï¼</strong> ç°æè¯ä¼°ææ å¨åè½æ­£ç¡®æ§ãç»æå¹³è¡¡åè®¾è®¡å¤ææ§æ¹é¢ä»ææ¬ ç¼ºï¼ä¸ç¼ºä¹éå¯¹é¿æè¿å¨åå¤æ ·åå¯¹è±¡ç±»å«çç»¼ååºåã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ··åè¡¨ç¤ºï¼</strong> ç»åç»æåæ¨¡åçå ä½å±æ¬¡åå¯è§£éæ§ä¸éå¼ç¥ç»è¡¨ç¤ºççµæ´»æ§åè¡¨è¾¾è½åã
*   <strong>æ´åä¸çç¥è¯ï¼</strong> å©ç¨LLMsåVFMsè¿è¡å¤æ¨¡ææ¨çåå¸¸è¯æå¯¼ï¼ä»¥åéè¿å¯å¾®åç©çæå¤±æéå»º-æ¨¡æç®¡éæ´åç©çåéªã
*   <strong>åè½æç¥ææ ï¼</strong> å¼åè½å¤è¯ä¼°åè½æ­£ç¡®æ§ãç»æå¹³è¡¡åè®¾è®¡å¤ææ§çæ°è¯ä¼°ææ ã
*   <strong>å¤§è§æ¨¡4Dæ°æ®éï¼</strong> è¿«åéè¦å¼åå¤§è§æ¨¡ãæ ååãæ¶µçå¤æ ·åå¯¹è±¡ç±»å«åè¿å¨ç±»åç4Dåºåã
*   <strong>èªçç£åå æå­¦ä¹ ï¼</strong> æ¢ç´¢è¶è¶éå»ºæçç£é©±å¨è®­ç»çèªçç£ãå æåç©çä¿¡æ¯å­¦ä¹ æ¹æ³ã
*   <strong>è§£è¦è¡¨ç¤ºï¼</strong> è¿ä¸æ­¥ç ç©¶å ä½ãè¿å¨ãå¤è§ååç§ç¬ç«å¯æä½çè§£è¦è¡¨ç¤ºï¼ä»¥æé«å¯æ§æ§åå¯ç¼è¾æ§ã
*   <strong>å·¥ä¸çº§ç²¾åº¦ï¼</strong> è§£å³å½åæ¹æ³å¨CADé¢åç¼ºä¹å·¥ä¸çº§ç²¾åº¦çé®é¢ï¼ä»¥æ»¡è¶³è£éãæºæ¢°éå»ºåæºå¨äººè®­ç»ç­åºç¨çéæ±ã</p>
<p>è¿ç¯è®ºæä¸º4Dè¡¨ç¤ºé¢åæä¾äºä¸ä¸ªå¨é¢çãä»¥è¡¨ç¤ºä¸ºä¸­å¿çè§è§ï¼ä¸ä»æ»ç»äºç°æææ¯ï¼æ´éè¦çæ¯ï¼å®ä¸ºæªæ¥çç ç©¶ææäºæ¹åï¼å¼ºè°äºå¨å ä½ãè¿å¨åäº¤äºè¡¨ç¤ºæ¹é¢ä»å­å¨çææåæºéã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19255v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19255v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19814v1'></a></p>
<h2 id="how-to-evaluate-monocular-depth-estimation"><a href="https://arxiv.org/abs/2510.19814v1">How to Evaluate Monocular Depth Estimation?</a></h2>
<p><strong>Authors:</strong> Siyang Wu, Jack Nugent, Willow Yang, Jia Deng</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Monocular depth estimation is an important task with rapid progress, but how
to evaluate it remains an open question, as evidenced by a lack of
standardization in existing literature and a large selection of evaluation
metrics whose trade-offs and behaviors are not well understood. This paper
contributes a novel, quantitative analysis of existing metrics in terms of
their sensitivity to various types of perturbations of ground truth,
emphasizing comparison to human judgment. Our analysis reveals that existing
metrics are severely under-sensitive to curvature perturbation such as making
flat surfaces wavy. To remedy this, we introduce a new metric based on relative
surface normals, along with new depth visualization tools and a principled
method to create composite metrics with better human alignment. Code and data
are available at: https://github.com/princeton-vl/evalmde.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼How to Evaluate Monocular Depth Estimation?</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºå¯¹åç®æ·±åº¦ä¼°è®¡çç°æè¯ä¼°ææ è¿è¡äºé¦æ¬¡å®éåæï¼æ­ç¤ºäºå®ä»¬å¨æ²çæ°å¨æ¹é¢çä¸¥éä¸è¶³ãä¸ºäºè§£å³è¿ä¸é®é¢ï¼ä½èæåºäºä¸ç§åºäºç¸å¯¹è¡¨é¢æ³çº¿çæ°ææ ï¼å¹¶å¼å¥äºæ°çå¯è§åå·¥å·åä¸ç§åå»ºä¸äººç±»å¤æ­æ´ä¸è´çå¤åææ çååæ§æ¹æ³ãè¿é¡¹å·¥ä½æ¨å¨æ åååæ¹è¿åç®æ·±åº¦ä¼°è®¡çè¯ä¼°æ¹å¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<ul>
<li><strong>å®éåæç°æææ ï¼</strong> è®ºæçå³é®åæ°å¨äºå¯¹ç°æè¯ä¼°ææ è¿è¡äºç³»ç»æ§çãå®éçåæï¼ç¹å«å³æ³¨å®ä»¬å¯¹ä¸åç±»åå°é¢çå¼æ°å¨ï¼å°¤å¶æ¯æ²çæ°å¨ï¼çæææ§ï¼å¹¶å°å¶ä¸äººç±»å¤æ­è¿è¡æ¯è¾ãè¿æ¯å¯¹ç°ææç®ä¸­ç¼ºä¹æ åååå¯¹ææ è¡ä¸ºçè§£ä¸è¶³çååºã</li>
<li><strong>åç°ç°æææ çä¸è¶³ï¼</strong> æç¡®æåºç°æææ å¯¹âæ²çæ°å¨âï¼å¦å°å¹³é¢åä¸ºæ³¢æµªå½¢ï¼ä¸¥éä¸ææï¼è¿æ¯ä¸ä¸ªéè¦çåç°ï¼æ­ç¤ºäºå½åè¯ä¼°æ¹æ³å¯è½æ æ³ææå°æ·±åº¦ä¼°è®¡ä¸­çå³é®è¯¯å·®ç±»åã</li>
<li><strong>å¼å¥æ°ææ ï¼</strong> æåºäºä¸ç§åºäºâç¸å¯¹è¡¨é¢æ³çº¿âçæ°ææ ï¼æ¨å¨æ´å¥½å°ææåè¯ä¼°æ·±åº¦ä¼°è®¡ä¸­çæ²çåè¡¨é¢å ä½ç»èã</li>
<li><strong>æ°çå¯è§åå·¥å·åå¤åææ æ¹æ³ï¼</strong> é¤äºæ°ææ ï¼è¿æä¾äºæ°çæ·±åº¦å¯è§åå·¥å·ï¼ä»¥åä¸ç§âååæ§æ¹æ³âæ¥åå»ºä¸äººç±»å¤æ­æ´ä¸è´çå¤åææ ï¼è¿è¡¨æä½èä¸ä»å³æ³¨åä¸ææ ï¼è¿å³æ³¨å¦ä½ç»¼åå©ç¨å¤ç§ä¿¡æ¯è¿è¡æ´å¨é¢çè¯ä¼°ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ ååè¯ä¼°ï¼</strong> è¿é¡¹ç ç©¶æææ¨å¨åç®æ·±åº¦ä¼°è®¡é¢åè¯ä¼°æ¹æ³çæ ååï¼åå°ç°ææç®ä¸­ææ éæ©çæ··ä¹±ã</li>
<li><strong>æ´åç¡®çæ¨¡åå¼åï¼</strong> éè¿æä¾æ´ææãæ´ç¬¦åäººç±»æç¥çè¯ä¼°ææ ï¼ç ç©¶äººåå°è½å¤æ´åç¡®å°è¯å«æ¨¡åå¨å ä½ç»èåè¡¨é¢æ²çæ¹é¢çå¼±ç¹ï¼ä»èå¼ååºæ§è½æ´ä¼è¶ãè§è§è´¨éæ´å¥½çæ·±åº¦ä¼°è®¡æ¨¡åã</li>
<li><strong>ä¿è¿å¬å¹³æ¯è¾ï¼</strong> ç»ä¸çè¯ä¼°æ¡æ¶å°ä½¿å¾ä¸åç ç©¶å¢éçæ¨¡åè½å¤è¿è¡æ´å¬å¹³ãæ´ææä¹çæ¯è¾ï¼å éé¢åè¿æ­¥ã</li>
<li><strong>æååºç¨å¯é æ§ï¼</strong> å¯¹äºä¾èµæ·±åº¦ä¿¡æ¯çä¸æ¸¸åºç¨ï¼å¦æºå¨äººå¯¼èªãèªå¨é©¾é©¶ãAR/VRï¼ï¼æ´åç¡®çæ·±åº¦è¯ä¼°æå³çæ´å¯é çè¾å¥ï¼ä»èæåè¿äºåºç¨çæ§è½åå®å¨æ§ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>æºå¨äººå­¦åèªå¨é©¾é©¶ï¼</strong> åç¡®çæ·±åº¦ä¼°è®¡å¯¹äºé¿éãè·¯å¾è§åãä¸ç»´éå»ºååºæ¯çè§£è³å³éè¦ãæ¹è¿çè¯ä¼°æ¹æ³å°å¸®å©å¼åæ´å¯é çæ·±åº¦ä¼ æå¨åç®æ³ã</li>
<li><strong>å¢å¼ºç°å® (AR) åèæç°å® (VR)ï¼</strong> é¼ççAR/VRä½éªé«åº¦ä¾èµäºç²¾ç¡®çåºæ¯æ·±åº¦çè§£ï¼ä»¥ä¾¿è¿è¡é®æ¡ãåç§åç©çäº¤äºã</li>
<li><strong>ä¸ç»´éå»ºåå»ºæ¨¡ï¼</strong> ä»åç®å¾åè¿è¡ä¸ç»´éå»ºæ¯è®¸å¤åºç¨çåºç¡ï¼æ´ç²¾ç»çæ·±åº¦è¯ä¼°æå©äºçææ´é«è´¨éçä¸ç»´æ¨¡åã</li>
<li><strong>è®¡ç®æºå¾å½¢å­¦ï¼</strong> æ·±åº¦ä¿¡æ¯å¯ç¨äºæ¸²æãåç§æ¨¡æåç¹æå¶ä½ã</li>
<li><strong>å»å­¦å½±åï¼</strong> å¨æäºå»å­¦å½±åä»»å¡ä¸­ï¼æ·±åº¦ä¿¡æ¯å¯ä»¥å¸®å©å»çè¿è¡è¯æ­åææ¯è§åã</li>
<li><strong>å·¥ä¸æ£æµåè´¨éæ§å¶ï¼</strong> ç²¾ç¡®çç©ä½å½¢ç¶åå°ºå¯¸æµéæ¯å·¥ä¸åºç¨çå³é®ï¼æ·±åº¦ä¼°è®¡çæ¹è¿è¯ä¼°å°æå©äºæåè¿äºä»»å¡çç²¾åº¦ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>âäººç±»å¤æ­âçå®ä¹åè·åï¼</strong> æè¦å¼ºè°ä¸âäººç±»å¤æ­âçæ¯è¾ï¼ä½å¹¶æªè¯¦ç»è¯´æå¦ä½éååè·åè¿äºäººç±»å¤æ­ãäººç±»å¯¹æ·±åº¦åæ²ççæç¥å¯è½å­å¨ä¸»è§æ§ï¼å¦ä½å°å¶è½¬åä¸ºå¯éåçåºåæ¯ä¸ä¸ªææã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> æ°çåºäºç¸å¯¹è¡¨é¢æ³çº¿çææ ä»¥åå¤åææ çè®¡ç®ææ¬å¯è½é«äºç°æçä¸äºç®åææ ï¼è¿å¨å®æ¶åºç¨ä¸­å¯è½æ¯ä¸ä¸ªèèå ç´ ã</li>
<li><strong>æ³åæ§ï¼</strong> è®ºæåæçâåç§ç±»åçå°é¢çå¼æ°å¨âæ¯å¦æ¶µçäºææå®éåºæ¯ä¸­å¯è½åºç°çæ·±åº¦ä¼°è®¡è¯¯å·®ç±»åï¼æ°ææ å¨åç§å¤æåºæ¯ï¼å¦çº¹çç¼ºå¤±ãåå°ãéæç©ä½ç­ï¼ä¸çè¡¨ç°å¦ä½ï¼ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>ä»éäºåç®æ·±åº¦ä¼°è®¡ï¼</strong> é¡¾åæä¹ï¼è¿é¡¹ç ç©¶ä¸æ³¨äºåç®æ·±åº¦ä¼°è®¡ãå¯¹äºç«ä½è§è§ãå¤è§å¾å ä½ææ·±åº¦ä¼ æå¨ï¼å¦LiDARãToFï¼çè¯ä¼°ï¼å¯è½éè¦ä¸åçèéã</li>
<li><strong>âååæ§æ¹æ³âçå·ä½ç»èï¼</strong> æè¦ä¸­æå°çåå»ºå¤åææ çâååæ§æ¹æ³âçå·ä½ç»èåå¶çè®ºåºç¡å¹¶æªå±å¼ï¼è¿å¯è½éè¦éè¯»å¨ææè½çè§£å¶é²æ£æ§åæ®éæ§ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¯¹ç°æè¯ä¼°ææ çæ¹å¤æ§åæåæåºåæ°çè§£å³æ¹æ¡ï¼ææå¯¹åç®æ·±åº¦ä¼°è®¡é¢åäº§çæ·±è¿å½±åï¼å¼å¯¼ç ç©¶äººåå¼ååºæ´ç²¾ç¡®ãæ´ç¬¦åäººç±»è§è§æç¥çæ·±åº¦ä¼°è®¡æ¨¡åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper
contributes a novel, quantitative analysis of existing metrics in terms of
their sensitivity to various types of perturbations of ground truth,
emphasizing comparison to human judgment.</li>
<li>To remedy this, we introduce a new metric based on relative
surface normals, along with new depth visualization tools and a principled
method to create composite metrics with better human alignment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19814v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19814v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19654v1'></a></p>
<h2 id="from-forecasting-to-planning-policy-world-model-for-collaborative-state-action-prediction"><a href="https://arxiv.org/abs/2510.19654v1">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a></h2>
<p><strong>Authors:</strong> Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Despite remarkable progress in driving world models, their potential for
autonomous systems remains largely untapped: the world models are mostly
learned for world simulation and decoupled from trajectory planning. While
recent efforts aim to unify world modeling and planning in a single framework,
the synergistic facilitation mechanism of world modeling for planning still
requires further exploration. In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme. Through collaborative state-action prediction,
PWM can mimic the human-like anticipatory perception, yielding more reliable
planning performance. To facilitate the efficiency of video forecasting, we
further introduce a dynamically enhanced parallel token generation mechanism,
equipped with a context-guided tokenizer and an adaptive dynamic focal loss.
Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.
Code and model weights will be released at
https://github.com/6550Zhao/Policy-World-Model.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhida Zhaoç­äººçè®ºæâFrom Forecasting to Planning: Policy World Model for Collaborative State-Action Predictionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="-">è®ºææè¦ï¼ä»é¢æµå°è§åï¼ç¨äºååç¶æ-å¨ä½é¢æµçç­ç¥ä¸çæ¨¡å</h3>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡é©¾é©¶ä¸çæ¨¡ååå¾äºæ¾èè¿å±ï¼ä½å¶å¨èªå¨é©¾é©¶ç³»ç»ä¸­çæ½åå°æªå®å¨åæ¥ãç°æä¸çæ¨¡åä¸»è¦ç¨äºä¸çæ¨¡æï¼å¹¶ä¸è½¨è¿¹è§åè§£è¦ãè½ç¶è¿ææå·¥ä½å°è¯å°ä¸çå»ºæ¨¡åè§åç»ä¸å°åä¸æ¡æ¶ä¸­ï¼ä½å¦ä½ååä¿è¿ä¸çå»ºæ¨¡ä»¥æ´å¥½å°æå¡äºè§åä»éè¿ä¸æ­¥æ¢ç´¢ãå·ä½æ¥è¯´ï¼è®ºææ¨å¨è§£å³å¦ä½è®©ä¸çæ¨¡åéè¿å­¦ä¹ å°çä¸çç¥è¯ï¼ä»¥æ´å¯é çæ¹å¼ç´æ¥æ¯æè½¨è¿¹è§åï¼èä¸æ¯ä»ä»ä½ä¸ºæ¨¡æå¨æç¬ç«ä»»å¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªåä¸º<strong>ç­ç¥ä¸çæ¨¡åï¼Policy World Model, PWMï¼</strong>çæ°åé©¾é©¶èå¼ï¼å¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>ç»ä¸çä¸çå»ºæ¨¡ä¸è½¨è¿¹è§åæ¶æï¼</strong> PWMå°ä¸çå»ºæ¨¡åè½¨è¿¹è§åæ´åå°ä¸ä¸ªç»ä¸çæ¶æä¸­ãå®éè¿âæ å¨ä½æªæ¥ç¶æé¢æµâæ¹æ¡ï¼å©ç¨å­¦ä¹ å°çä¸çç¥è¯æ¥å¢å¼ºè§åæ§è½ï¼æ¨¡ä»¿äººç±»çâé¢ææç¥âè½åã</li>
<li><strong>æ å¨ä½è§é¢çæé¢è®­ç»ï¼</strong> PWMé¦åå¨æ æ ç­¾è§é¢åºåä¸è¿è¡æ å¨ä½è§é¢çæé¢è®­ç»ï¼ä»¥è·åä¸çå»ºæ¨¡è½åãè¿æ¶é¤äºå¯¹å¨ä½æ æ³¨æ°æ®çä¾èµï¼æé«äºè®­ç»çå¯æ©å±æ§ï¼å¹¶åè®¸æ´çµæ´»çæªæ¥ç¶ææ¨æ¼ã</li>
<li><strong>ååç¶æ-å¨ä½é¢æµï¼</strong> å¨å¾®è°åæ¨çé¶æ®µï¼PWMç»åå½åååå²è§é¢å¸§ï¼é¦åçæææ¬æè¿°ä»¥çè§£å½åç¯å¢ï¼ç¶åéè¿è§é¢çææ¨æ¼åºåççæªæ¥ç¶æãæç»çå¨ä½é¢æµæ¯åºäºçæçæè¿°åé¢æµçæªæ¥ç¶æä½ä¸ºå¤æ¨¡æä¾æ®ã</li>
<li><strong>é«æçè§é¢é¢æµæºå¶ï¼</strong> ä¸ºäºæé«è§é¢é¢æµæçï¼PWMå¼å¥äºå¨æå¢å¼ºçå¹¶è¡tokençææºå¶ï¼åæ¬ï¼<ul>
<li><strong>ä¸ä¸æå¼å¯¼çtokenizerï¼</strong> å®ç°ç´§åçå¾åtokenè¡¨ç¤ºï¼æ¯å¼ å¾å28ä¸ªtokenï¼ï¼ç¡®ä¿æçåè§è§è¿è´¯æ§ã</li>
<li><strong>èªéåºå¨æfocal lossï¼</strong> å¨è®­ç»ä¸­å¼ºè°æ¶é´ååçå¾ååºåï¼ä»¥ç¡®ä¿è§é¢çæè´¨éï¼å¹¶æ´å¥½å°æææ¶ç©ºå¨æã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
PWMå¨æµè¡çåºåæµè¯ï¼nuScenesåNAVSIMï¼ä¸è¿è¡äºè¯ä¼°ï¼å¹¶åå¾äºæ¾èææï¼</p>
<ul>
<li><strong>åè¶çè§åæ§è½ï¼</strong> å¨nuScenesæ°æ®éä¸ï¼PWMå®ç°äºæä½çå¹³åç¢°æçï¼0.07%å0.04%ï¼ï¼è¶è¶äºç°ææåè¿çæ¹æ³ã</li>
<li><strong>ä»ä½¿ç¨åç½®æåå¤´è¾å¥ï¼</strong> å°½ç®¡ä»ä½¿ç¨åç½®æåå¤´è¾å¥ï¼PWMå¨NAVSIMåºåæµè¯ä¸çPDMSæ§è½ä¸ä¾èµå¤è§è§åå¤æ¨¡æè¾å¥ï¼å¦LiDARï¼çæåè¿æ¹æ³ç¸å½ææ´ä¼ã</li>
<li><strong>é«æçè§è§çæï¼</strong> ç»åå¶åæ°æºå¶ï¼PWMè½å¤ä»¥åççè®¡ç®å¼éçæé«è´¨éçæªæ¥è§é¢å¸§ï¼åæ¶ä¿æçº¦40 FPSçå¸§çï¼ä¸è¿è¡åç´ ç©ºé´è§£ç ï¼ã</li>
<li><strong>å¨æFocal Lossçæææ§ï¼</strong> æ¶èç ç©¶è¡¨æï¼å¨æFocal Lossæ¾èæåäºè§é¢é¢æµåè§åæ§è½ï¼å°¤å¶æ¯å¨é¢è®­ç»é¶æ®µåºç¨æ¶ï¼è½æ´ææå°æææ¶ç©ºå¨æã</li>
<li><strong>æªæ¥å¸§é¢æµå¯¹è§åçç§¯æå½±åï¼</strong> å¼å¥æªæ¥å¸§é¢æµæ¾èéä½äºnuScenesä¸çå¹³åç¢°æçãå¨NAVSIMä¸ï¼æªæ¥å¸§é¢æµä¿ä½¿æ¨¡åéåæ´ä¿å®çè§åç­ç¥ï¼çºç²é¨åè¿åº¦ï¼EPï¼ä»¥æ¢åæ´é«çå®å¨è£åº¦ï¼NCåTTCï¼ã</li>
</ul>
<p>è¿äºç»æå¸æ¾äºä»è§é¢è¡¨ç¤ºä¸­å­¦ä¹ ä»¥é©±å¨æ´å®å¨ãæ´å¯æ©å±çèªå¨é©¾é©¶ç³»ç»çå·¨å¤§æ½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åè§è§è¾å¥çé²æ£æ§éå¶ï¼</strong> ä»ä¾èµåè§è§è¾å¥å¯è½ä¼å¨æ¶å£è½è§åº¦æ¡ä»¶ä¸å½±åç³»ç»çé²æ£æ§ã
*   <strong>è§åæ¶åéå¶ï¼</strong> ç­æçè§åæ¶åéå¶äºPWMå¨é¿æ¶ååºæ¯ä¸­çéç¨æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤è§è§è¾å¥çæææ´åï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¦ä½é«ææ´åå¤è§è§è¾å¥ï¼ä»¥æé«ç³»ç»çé²æ£æ§ã
*   <strong>å¢å¼ºé¿æé¢æµè½åï¼</strong> æåæ¨¡åçé¿æé¢æµè½åï¼ä»¥æ´å¥½å°åºå¯¹é¿æ¶ååºæ¯ï¼ä»èæé«æ³åè½ååå®éåºç¨åå¤åº¦ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿æåºPolicy World Modelï¼æåå°å°ä¸çå»ºæ¨¡åè½¨è¿¹è§åç»ä¸èµ·æ¥ï¼å¹¶éè¿æ å¨ä½æªæ¥ç¶æé¢æµåå¨æå¢å¼ºçå¹¶è¡tokençææºå¶ï¼æ¾èæåäºèªå¨é©¾é©¶ç³»ç»çè§åæ§è½åæçãå¶å¨ä»ä½¿ç¨åç½®æåå¤´è¾å¥çæåµä¸è¾¾å°æè¶è¶äºå¤è§è§/å¤æ¨¡ææ¹æ³çæ§è½ï¼ä¸ºèªå¨é©¾é©¶é¢åå¼è¾äºæ°çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme.</li>
<li>Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19654v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19654v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19618v1'></a></p>
<h2 id="pragmatic-heterogeneous-collaborative-perception-via-generative-communication-mechanism"><a href="https://arxiv.org/abs/2510.19618v1">Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</a></h2>
<p><strong>Authors:</strong> Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-agent collaboration enhances the perception capabilities of individual
agents through information sharing. However, in real-world applications,
differences in sensors and models across heterogeneous agents inevitably lead
to domain gaps during collaboration. Existing approaches based on adaptation
and reconstruction fail to support pragmatic heterogeneous collaboration due to
two key limitations: (1) Intrusive retraining of the encoder or core modules
disrupts the established semantic consistency among agents; and (2)
accommodating new agents incurs high computational costs, limiting scalability.
To address these challenges, we present a novel Generative Communication
mechanism (GenComm) that facilitates seamless perception across heterogeneous
multi-agent systems through feature generation, without altering the original
network, and employs lightweight numerical alignment of spatial information to
efficiently integrate new agents at minimal cost. Specifically, a tailored
Deformable Message Extractor is designed to extract spatial message for each
collaborator, which is then transmitted in place of intermediate features. The
Spatial-Aware Feature Generator, utilizing a conditional diffusion model,
generates features aligned with the ego agent's semantic space while preserving
the spatial information of the collaborators. These generated features are
further refined by a Channel Enhancer before fusion. Experiments conducted on
the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm
outperforms existing state-of-the-art methods, achieving an 81\% reduction in
both computational cost and parameter count when incorporating new agents. Our
code is available at https://github.com/jeffreychou777/GenComm.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Junfei Zhouç­äººæ°åçè®ºæâPragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanismâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¤æºè½ä½åä½æç¥éè¿ä¿¡æ¯å±äº«å¢å¼ºäºåä¸ªæºè½ä½çæç¥è½åï¼ä½å¨ç°å®ä¸çåºç¨ä¸­ï¼å¼ææºè½ä½ä¹é´ä¼ æå¨åæ¨¡åçå·®å¼ä¸å¯é¿åå°å¯¼è´é¢åé¸¿æ²ãç°æçåºäºéåºï¼adaptation-basedï¼åéå»ºï¼reconstruction-basedï¼çæ¹æ³å¨æ¯æå®ç¨å¼æåä½æç¥æ¹é¢å­å¨ä¸¤ä¸ªä¸»è¦éå¶ï¼
1. <strong>ä¾µå¥æ§åè®­ç»ï¼</strong> å¯¹ç¼ç å¨ææ ¸å¿æ¨¡åè¿è¡åè®­ç»ä¼ç ´åæºè½ä½ä¹é´å·²å»ºç«çè¯­ä¹ä¸è´æ§ã
2. <strong>å¯æ©å±æ§å·®ï¼</strong> éåºæ°æºè½ä½éè¦é«è®¡ç®ææ¬ï¼éå¶äºç³»ç»çå¯æ©å±æ§ã
è¯¥ç ç©¶æ¨å¨è§£å³å¦ä½å¨å¼æå¤æºè½ä½ç³»ç»ä¸­å®ç°æ ç¼åä½æç¥ï¼åæ¶é¿åä¾µå¥æ§ä¿®æ¹ï¼å¹¶ä»¥æå°ææ¬é«æéææ°æºè½ä½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäºä¸ç§æ°é¢ç<strong>çæå¼éä¿¡æºå¶ï¼GenCommï¼</strong>ï¼éè¿ç¹å¾çæä¿è¿å¼æå¤æºè½ä½ç³»ç»é´çæ ç¼æç¥ï¼ä¸ä¸æ¹ååå§ç½ç»ãå¶æ ¸å¿åæ°åæ¬ï¼
*   <strong>éä¾µå¥å¼ç¹å¾çæï¼</strong> GenComméè¿çæä¸èªææºè½ä½è¯­ä¹ç©ºé´å¯¹é½çç¹å¾ï¼åæ¶ä¿çåä½æºè½ä½çç©ºé´ä¿¡æ¯ï¼é¿åäºå¯¹åå§ç½ç»è¿è¡ä¾µå¥æ§åè®­ç»ï¼ä»èç»´æ¤äºè¯­ä¹ä¸è´æ§ã
*   <strong>è½»éçº§ç©ºé´ä¿¡æ¯æ°å¼å¯¹é½ï¼</strong> éç¨è½»éçº§çæ°å¼å¯¹é½æ¹æ³ï¼ä»¥æå°ææ¬é«æéææ°æºè½ä½ï¼è§£å³äºå¯æ©å±æ§é®é¢ã
*   <strong>å¯åå½¢æ¶æ¯æåå¨ï¼Deformable Message Extractorï¼ï¼</strong> ä¸é¨è®¾è®¡ç¨äºä¸ºæ¯ä¸ªåä½æºè½ä½æåç©ºé´æ¶æ¯ï¼å¹¶æ¿ä»£ä¸­é´ç¹å¾è¿è¡ä¼ è¾ãè¯¥æåå¨å©ç¨å¯åå½¢å·ç§¯ï¼å¨æåèå¨å´åç´ ï¼å¢å¼ºäºåºååæ¯åèæ¯çè½åã
*   <strong>ç©ºé´æç¥ç¹å¾çæå¨ï¼Spatial-Aware Feature Generatorï¼ï¼</strong> å©ç¨æ¡ä»¶æ©æ£æ¨¡åï¼æ ¹æ®æ¥æ¶å°çç©ºé´æ¶æ¯çæç¹å¾ãè¿äºç¹å¾ä¸èªææºè½ä½çè¯­ä¹ç©ºé´å¯¹é½ï¼å¹¶ä¿çäºåä½æºè½ä½çç©ºé´ä¿¡æ¯ã
*   <strong>ééå¢å¼ºå¨ï¼Channel Enhancerï¼ï¼</strong> å¨ç¹å¾èåä¹åï¼è¿ä¸æ­¥ç»åçæçç¹å¾ï¼ä»¥ç¡®ä¿ééç»´åº¦ä¸çè¯­ä¹ä¸è´æ§ã
*   <strong>ä¸¤é¶æ®µè®­ç»ç­ç¥ï¼</strong> ç¬¬ä¸é¶æ®µå¨åæè®¾ç½®ä¸è®­ç»æ¨¡åçæ ¸å¿ç»ä»¶ï¼ç¬¬äºé¶æ®µéå¯¹å¼æè®¾ç½®ï¼ä»å¯¹è½»éçº§æ¶æ¯æåå¨è¿è¡å¾®è°ï¼ä»¥å¯¹é½ç©ºé´æ¶æ¯çæ°å¼åå¸ï¼ç¡®ä¿ä¸æ¥æ¶æ¹èªèº«æåçç©ºé´ä¿¡æ¯ä¸è´ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> å¨OPV2V-HãDAIR-V2XåV2X-Realæ°æ®éä¸è¿è¡çå®éªè¡¨æï¼GenCommä¼äºç°æçæåè¿æ¹æ³ã
*   <strong>æ¾èçææ¬ååæ°åå°ï¼</strong> å¨éææ°æºè½ä½æ¶ï¼GenCommå°è®¡ç®ææ¬ååæ°æ°éåéä½äº81%ï¼è¿å¸æ¾äºå¶åºè²çå¯æ©å±æ§ã
*   <strong>éä¿¡æçæåï¼</strong> éè¿ä¼ è¾åç¼©åçç©ºé´æç¥è¡¨ç¤ºèéå®æ´ç¹å¾å¾ï¼GenCommææéä½äºéä¿¡å¼éã
*   <strong>é²æ£æ§ï¼</strong> å¨å­å¨å§¿æè¯¯å·®åæ¶é´å»¶è¿çå¨æåä½åºæ¯ä¸­ï¼GenCommè¡¨ç°åºä¼è¶çé²æ£æ§ã
*   <strong>å¯è§åéªè¯ï¼</strong> å¯è§åç»æè¡¨æï¼çæçç¹å¾çè¯­ä¹ç©ºé´ä¸èªææºè½ä½ä¸è´ï¼åæ¶å®æ´ä¿çäºåä½æºè½ä½çç©ºé´ä¿¡æ¯ï¼å¨ç¹å¾è´¨éåæ£æµæ§è½ä¸åä¼äºCodeFillingã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>éè¦ä¾åºåå±è¯ï¼</strong> å°½ç®¡è®ºæåè®¾äºä¸ä¸ªæ´ç°å®çéå®å¨è¿æ¥éä¿¡å¾ï¼ä½è¯¥æ¹æ³ä»ç¶éè¦ä¾åºåä¹é´çå±è¯ï¼è¿å¯è½ä¼åå°åä¸ç«äºåæ¶ææ»å»æ½å¨é£é©ç­å ç´ çé»ç¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ç¼è§£ä¾åºåå±è¯é®é¢ï¼</strong> æ¢ç´¢æ éä¸¥æ ¼ä¾åºåå±è¯å³å¯å®ç°å¼æåä½çæ¹æ³ï¼ä¾å¦éè¿èé¦å­¦ä¹ ææ´å»ä¸­å¿åçæºå¶ã
*   <strong>æ´å¤æçæ»å»åºæ¯ï¼</strong> è¿ä¸æ­¥ç ç©¶å¨å­å¨æ¶ææ»å»ææ´å¤æéä¿¡å¹²æ°æåµä¸çGenCommé²æ£æ§ã
*   <strong>å¤æ¨¡æèåçè¿ä¸æ­¥ä¼åï¼</strong> å°½ç®¡GenCommå¨å¼æè®¾ç½®ä¸è¡¨ç°è¯å¥½ï¼ä½å¯ä»¥æ¢ç´¢æ´åè¿çå¤æ¨¡æèåææ¯ï¼ä»¥è¿ä¸æ­¥æåæç¥æ§è½ã
*   <strong>å®æ¶é¨ç½²ä¼åï¼</strong> éå¯¹æ´ä¸¥æ ¼çå®æ¶æ§è¦æ±ï¼è¿ä¸æ­¥ä¼åGenCommçæ¨çå»¶è¿åè®¡ç®æçã</p>
<hr />
<p>æ»èè¨ä¹ï¼GenCommä¸ºå¼æå¤æºè½ä½åä½æç¥æä¾äºä¸ä¸ªå®ç¨ä¸é«æçè§£å³æ¹æ¡ï¼éè¿å¶ç¬ç¹ççæå¼éä¿¡æºå¶ï¼å¨ä¿æè¯­ä¹ä¸è´æ§ãéä½è®¡ç®ææ¬åæé«å¯æ©å±æ§æ¹é¢åå¾äºæ¾èè¿å±ï¼ä¸ºèªå¨é©¾é©¶ç­é¢åçå®éé¨ç½²å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Existing approaches based on adaptation
and reconstruction fail to support pragmatic heterogeneous collaboration due to
two key limitations: (1) Intrusive retraining of the encoder or core modules
disrupts the established semantic consistency among agents; and (2)
accommodating new agents incurs high computational costs, limiting scalability.</li>
<li>To address these challenges, we present a novel Generative Communication
mechanism (GenComm) that facilitates seamless perception across heterogeneous
multi-agent systems through feature generation, without altering the original
network, and employs lightweight numerical alignment of spatial information to
efficiently integrate new agents at minimal cost.</li>
<li>Experiments conducted on
the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm
outperforms existing state-of-the-art methods, achieving an 81\% reduction in
both computational cost and parameter count when incorporating new agents.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19618v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19618v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19592v1'></a></p>
<h2 id="decomposed-attention-fusion-in-mllms-for-training-free-video-reasoning-segmentation"><a href="https://arxiv.org/abs/2510.19592v1">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</a></h2>
<p><strong>Authors:</strong> Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal large language models (MLLMs) demonstrate strong video
understanding by attending to visual tokens relevant to textual queries. To
directly adapt this for localization in a training-free manner, we cast video
reasoning segmentation as a video QA task and extract attention maps via
rollout mechanism. However, raw attention maps are noisy and poorly aligned
with object regions. We propose Decomposed Attention Fusion (DecAF), which
refines these maps through two mechanisms: (1) contrastive object-background
fusion and (2) complementary video-frame fusion. This method suppresses
irrelevant activations and enhances object-focused cues, enabling direct
conversion of attention maps into coarse segmentation masks. In addition, we
introduce attention-guided SAM2 prompting for obtaining fine-grained masks.
Unlike existing methods that jointly train MLLMs with SAM, our method operates
entirely without retraining. DecAF outperforms training-free methods and
achieves performance comparable to training-based methods on both referring and
reasoning VOS benchmarks. The code will be available at
https://github.com/HYUNJS/DecAF.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæâDecomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºæé¢ç®ï¼</strong> Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation
<strong>ä½èï¼</strong> Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim</p>
<p><strong>æè¦ï¼</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸ºâDecomposed Attention Fusion (DecAF)âçæ°é¢è®­ç»æ å³æ¡æ¶ï¼æ¨å¨å©ç¨å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼åºæçè§é¢çè§£è½åï¼è§£å³è§é¢æ¨çåå²ï¼Video Reasoning Segmentationï¼ä»»å¡ãè¯¥ä»»å¡è¦æ±æ ¹æ®å¤æçææ¬æ¥è¯¢ï¼å¨è§é¢ä¸­å®ä½å¹¶åå²ç¸å³å¯¹è±¡ã</p>
<ol>
<li>
<p><strong>ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong></p>
<ul>
<li>MLLMså¨è§é¢çè§£æ¹é¢è¡¨ç°åºè²ï¼ä½å¶åå§æ³¨æåå¾éå¸¸åªå£°å¤§ãä¸å¯¹è±¡åºåå¯¹é½ä¸ä½³ï¼é¾ä»¥ç´æ¥ç¨äºè®­ç»æ å³çè§é¢æ¨çåå²ä»»å¡ã</li>
<li>ç°æè®­ç»æ å³æ¹æ³ï¼å¦Loc-HeadåTAMï¼å¨æ³åæ§ãé²æ£æ§ä»¥åå¤çå¤å¯¹è±¡ãå°å¯¹è±¡æéè¦å¤ææ¨ççåºæ¯æ¶å­å¨å±éæ§ã</li>
<li>å¦ä½ä»ç²ç³çæ³¨æåå¾çæç²¾ç»çåå²æ©ç ï¼åæ¶é¿åç°ææ¹æ³ä¸­MLLMsä¸åå²æ¨¡åï¼å¦SAMï¼èåè®­ç»çéæ±ã</li>
</ul>
</li>
<li>
<p><strong>å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong></p>
<ul>
<li><strong>å°è§é¢æ¨çåå²è½¬åä¸ºè§é¢é®ç­ä»»å¡ï¼</strong> è®ºæå°è§é¢æ¨çåå²ä»»å¡éæ°å®ä¹ä¸ºè§é¢é®ç­ï¼Video QAï¼ä»»å¡ï¼å¹¶éè¿æ³¨æååæº¯ï¼attention rolloutï¼æºå¶æåæ³¨æåå¾ï¼ä»¥å®ç°è®­ç»æ å³çå®ä½ã</li>
<li><strong>åè§£æ³¨æåèåï¼DecAFï¼æ¡æ¶ï¼</strong> æåºDecAFæ¥ç²¾ç¼åå§æ³¨æåå¾ï¼éè¿ä¸¤ç§æºå¶æå¶æ å³æ¿æ´»å¹¶å¢å¼ºå¯¹è±¡ç¦ç¹ï¼<ul>
<li><strong>å¯¹æ¯å¯¹è±¡-èæ¯èåï¼Contrastive Object-Background Fusionï¼ï¼</strong> éè¿ä»å¯¹è±¡èç¦çæ³¨æåå¾ä¸­åå»èæ¯èç¦çæ³¨æåå¾ï¼æææå¶æ å³åºåçåªå£°ï¼çªåºç®æ å¯¹è±¡ä¿¡å·ã</li>
<li><strong>äºè¡¥è§é¢-å¸§èåï¼Complementary Video-Frame Fusionï¼ï¼</strong> ç»åè§é¢çº§æ³¨æåå¾ï¼æææ¶é´ä¸ä¸æï¼ä½ç²åº¦ç²ç³ï¼åå¸§çº§æ³¨æåå¾ï¼æä¾å¯¹è±¡ä¸­å¿ãç»ç²åº¦ç»èï¼ä½ç¼ºä¹æ¶é´è¿è´¯æ§ï¼çä¼å¿ï¼çææ´é²æ£çæ³¨æåå¾ã</li>
</ul>
</li>
<li><strong>è§è§æç¥å½ä¸åçæ³¨æååæº¯ï¼Vision-Aware Normalization for Attention Rolloutï¼ï¼</strong> æ¹è¿äºæ åçæ³¨æååæº¯æºå¶ï¼å¼å¥äºè§è§æç¥å½ä¸åæ¹æ¡åå¤´é¨å æèåï¼ä»¥æ´å¥½å°ææè¯­è¨æ¡ä»¶ä¸çå®ä½ä¿¡æ¯ï¼å¹¶åå°åªå£°å¤´é¨çè´é¢å½±åã</li>
<li><strong>æ³¨æåå¼å¯¼çSAM2æç¤ºï¼Attention-guided SAM2 Promptingï¼ï¼</strong> å¼å¥äºä¸ç§è®­ç»æ å³çSAM2æç¤ºæµç¨ï¼éè¿æ³¨æåå¾çæç¹æ¥è¯¢ï¼å¹¶ç»åæ³¨æåä¸è´æ§åæ°ï¼attention consistency scoreï¼æ¥è¯ä¼°åè¿æ»¤çæçæ©ç è½¨è¿¹ï¼ä»èä»ç²ç³çæ³¨æåå¾è·å¾ç²¾ç»çåå²æ©ç ã</li>
</ul>
</li>
<li>
<p><strong>ä¸»è¦ç»æåå¶æä¹ï¼</strong></p>
<ul>
<li><strong>è¶è¶ç°æè®­ç»æ å³æ¹æ³ï¼</strong> DecAFå¨å¤ä¸ªMLLMå®¶æï¼LLaVAãInternVLãQwen2VL/Qwen2.5VLï¼åäºä¸ªæ°æ®éï¼Ref-DAVISãRef-YouTube-VOSãMeViSãReasonVOSãReVOSï¼ä¸ï¼æ è®ºæ¯å¦ç»åSAMï¼é½æç»­ä¼äºååçè®­ç»æ å³æ¹æ³ï¼å¦Loc-HeadåTAMï¼ï¼å°¤å¶å¨éè¦å¤ææ¨ççæ°æ®éä¸ä¼å¿æ¾èã</li>
<li><strong>ä¸è®­ç»åºäºæ¹æ³åª²ç¾çæ§è½ï¼</strong> DecAFå¨æä»£VOSåæ¨çVOSåºåæµè¯ä¸åå¾äºä¸è®­ç»åºäºæ¹æ³ï¼å¦LISAãVISAãVideoLISAç­ï¼ç¸å½çè³æ´ä¼çæ§è½ï¼å°½ç®¡DecAFå®å¨æ ééæ°è®­ç»ã</li>
<li><strong>æ³åæ§åé²æ£æ§ï¼</strong> è®ºæç»æè¡¨æï¼DecAFæ¡æ¶å¨ä¸åMLLMsåæ°æ®éä¸è¡¨ç°åºæ´ä¸è´åå¯æ³åçæ§è½ï¼è§£å³äºç°ææ¹æ³å¯¹æ°æ®éç¹å®éæ ·åæ¨¡åç¹å®å¯åå¼è§åçä¾èµé®é¢ã</li>
<li><strong>ææçæç²¾ç»æ©ç ï¼</strong> éè¿æ³¨æåå¼å¯¼çSAM2æç¤ºï¼DecAFè½å¤å°ç²ç³çæ³¨æåå¾è½¬åä¸ºç²¾ç¡®å¯é çåå²æ©ç ï¼å³ä½¿å¨å¤å¯¹è±¡ãå°å¯¹è±¡ãæ¶é´æ¨çåä¸çç¥è¯æ¨çç­æææ§åºæ¯ä¸ä¹è½ä¿æé²æ£çå®ä½ååå²è´¨éã</li>
</ul>
</li>
<li>
<p><strong>è®ºæä¸­æåçå±éæ§ï¼</strong></p>
<ul>
<li><strong>æ³¨æåå¾åè¾¨çï¼</strong> å°½ç®¡DecAFæ¾èæåäºæ§è½ï¼ä½æ³¨æåå¾çåå§åè¾¨çä»ç¶è¾ä½ï¼å¯¼è´ç´æ¥ä»æ³¨æåå¾çæçç²ç³åå²æ©ç å¨ææç²¾ç»è¾¹çæ¹é¢å­å¨ä¸è¶³ã</li>
<li><strong>æ³¨æåä¸è´æ§åæ°å¤±ææ¡ä¾ï¼</strong> å¨æäºæåµä¸ï¼å½ç®æ å¯¹è±¡å æ®è¾å¤§åºåä½æ³¨æåæéä»è¦çå¶ä¸­ä¸å°é¨åæ¶ï¼æ³¨æåä¸è´æ§åæ°å¯è½ä¼æ©ç½æ­£ç¡®çæçæ©ç è½¨è¿¹ãç±»ä¼¼å°ï¼å½ç®æ å¯¹è±¡å¾å°ä¸å¨è§é¢ä¸­ç­æåºç°æ¶ï¼ä¹å¯è½å æ³¨æåæéå æ¯å°èå¯¼è´éè¯¯è¿æ»¤ã</li>
<li><strong>ç»æææéå¶ï¼</strong> å½ç®æ å¯¹è±¡æå¶ç»é¿ï¼å¦æ»ç¿ä¼çº¿ï¼æ¶ï¼æ³¨æåå¾å¯è½æ æ³å®å¨ææå¶ç»æï¼å¯¼è´åå²æ©ç è´¨éä¸ä½³ã</li>
</ul>
</li>
<li>
<p><strong>æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong></p>
<ul>
<li><strong>æåæ³¨æåå¾çç»ç²åº¦ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½ä»MLLMsä¸­æåæ´é«åè¾¨çææ´ç»ç²åº¦çæ³¨æåä¿¡æ¯ï¼ä»¥ç´æ¥çææ´ç²¾ç¡®çåå²æ©ç ï¼åå°å¯¹å¤é¨åå²æ¨¡åï¼å¦SAM2ï¼çä¾èµã</li>
<li><strong>ä¼åæ³¨æåä¸è´æ§è¯åæºå¶ï¼</strong> æ¹è¿æ³¨æåä¸è´æ§è¯åæ¹æ³ï¼ä½¿å¶å¨å¤çå¤§é¢ç§¯å¯¹è±¡æç»é¿ç»æå¯¹è±¡æ¶æ´å·é²æ£æ§ï¼é¿åè¯¯å¤ã</li>
<li><strong>æ¢ç´¢æ´å¤æçæ¨çåºæ¯ï¼</strong> å°½ç®¡DecAFå¨æ¨çVOSä¸è¡¨ç°è¯å¥½ï¼ä½ä»å¯è¿ä¸æ­¥æ¢ç´¢MLLMså¨æ´å¤æãå¤æ­¥éª¤æ¨çåºæ¯ä¸çå®ä½è½åã</li>
<li><strong>ä¸å¶ä»MLLMæ¶æçéæï¼</strong> æç»­æ¢ç´¢DecAFå¨æªæ¥æ°åMLLMæ¶æä¸çéç¨æ§åä¼åæ½åï¼ç¹å«æ¯é£äºæ¯ææ´é«åçåè¾¨çè§é¢è¾å¥çæ¨¡åã</li>
</ul>
</li>
</ol>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Decomposed Attention Fusion (DecAF), which
refines these maps through two mechanisms: (1) contrastive object-background
fusion and (2) complementary video-frame fusion.</li>
<li>Unlike existing methods that jointly train MLLMs with SAM, our method operates
entirely without retraining.</li>
<li>DecAF outperforms training-free methods and
achieves performance comparable to training-based methods on both referring and
reasoning VOS benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19592v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19592v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19559v1'></a></p>
<h2 id="a-matter-of-time-revealing-the-structure-of-time-in-vision-language-models"><a href="https://arxiv.org/abs/2510.19559v1">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.IR, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Large-scale vision-language models (VLMs) such as CLIP have gained popularity
for their generalizable and expressive multimodal representations. By
leveraging large-scale training data with diverse textual metadata, VLMs
acquire open-vocabulary capabilities, solving tasks beyond their training
scope. This paper investigates the temporal awareness of VLMs, assessing their
ability to position visual content in time. We introduce TIME10k, a benchmark
dataset of over 10,000 images with temporal ground truth, and evaluate the
time-awareness of 37 VLMs by a novel methodology. Our investigation reveals
that temporal information is structured along a low-dimensional, non-linear
manifold in the VLM embedding space. Based on this insight, we propose methods
to derive an explicit ``timeline'' representation from the embedding space.
These representations model time and its chronological progression and thereby
facilitate temporal reasoning tasks. Our timeline approaches achieve
competitive to superior accuracy compared to a prompt-based baseline while
being computationally efficient. All code and data are available at
https://tekayanidham.github.io/timeline-page/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Nidham Tekaya, Manuela Waldner, Matthias Zeppelzaueræ°åçè®ºæâA Matter of Time: Revealing the Structure of Time in Vision-Language Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="-_1">è®ºææè¦ï¼ãæ¶é´é®é¢ï¼æ­ç¤ºè§è§-è¯­è¨æ¨¡åä¸­çæ¶é´ç»æã</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºæä¸»è¦æ¢è®¨å¤§åè§è§-è¯­è¨æ¨¡åï¼VLMsï¼æ¯å¦å·æåå¨çæ¶é´æç¥è½åï¼ä»¥åå®ä»¬å¦ä½å°è§è§åå®¹å®ä½å¨æ¶é´è½´ä¸ãå·ä½æ¥è¯´ï¼ç ç©¶æ¨å¨åç­ä»¥ä¸é®é¢ï¼
* å¼æ¾è¯æ±VLMså¨å¤å¤§ç¨åº¦ä¸å·ææ¶é´æç¥è½åï¼
* å¾ååææ¬çæ¶é´ä¿¡æ¯å¨åµå¥ç©ºé´ä¸­æ¯å¦ä½ç»ç»çï¼
* å¦æå­å¨æ¶é´ç»æï¼å¦ä½ææå°è¡¨ç¤ºæ¶é´ä»¥é¢æµå¾åçé¦æ¬¡åºç°æ¶é´ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
* <strong>TIME10kæ°æ®éï¼</strong> å¼å¥äºä¸ä¸ªåå«è¶è¿10,000å¼ å¸¦ææ¶é´çå¼çäººé ç©ä½å¾åçåºåæ°æ®éï¼æ¶µç1715å¹´è³2024å¹´çæ¶é´èå´åå­ä¸ªç©ä½ç±»å«ï¼é£æºãæ±½è½¦ãä¹å¨ãææºãè¹è¶ãæ­¦å¨åå¼¹è¯ï¼ï¼ç¨äºç³»ç»è¯ä¼°VLMsçæ¶é´æç¥è½åã
* <strong>æ¶é´æ¢æµï¼Time Probingï¼åºçº¿ï¼</strong> æåºäºä¸ç§åºäºæç¤ºçåºçº¿æ¹æ³ï¼éè¿è®¡ç®å¾ååµå¥ä¸ä¸åå¹´ä»½æ¶é´æç¤ºåµå¥ä¹é´çç¹ç§¯ç¸ä¼¼åº¦æ¥é¢æµå¾åçé¦æ¬¡åºç°å¹´ä»½ã
* <strong>åµå¥ç©ºé´åæï¼</strong> é¦æ¬¡ç³»ç»å°åæäºVLMsåµå¥ç©ºé´ä¸­æ¶é´ä¿¡æ¯çç»æãç ç©¶åç°æ¶é´ä¿¡æ¯æ²¿çä½ç»´ãéçº¿æ§æµå½¢ç»ç»ï¼è¿ä¸è¯­è¨æ¨¡åä¸­æ¶é´ä¿¡æ¯éå¸¸åçº¿æ§ç»æä¸åã
* <strong>æ¶é´è½´å»ºæ¨¡æ¹æ³ï¼</strong>
    * <strong>åºäºUMAPçæ¶é´è½´è¡¨ç¤ºï¼</strong> å©ç¨UMAPï¼ç»ä¸æµå½¢è¿ä¼¼ä¸æå½±ï¼å°é«ç»´æ¶é´åµå¥æå½±å°ä¸ç»´æ¶é´è½´ä¸ï¼å¹¶éè¿ä¼åUMAPåæ°ä»¥æå¤§åæå½±é¡ºåºä¸çå®æ¶é´é¡ºåºä¹é´çSpearmanç§©ç¸å³æ§ã
    * <strong>åºäºè´å¡å°æ²çº¿çæ¶é´è½´è¡¨ç¤ºï¼</strong> æåºäºä¸ç§å¨åå§åµå¥ç©ºé´æå¶å­ç©ºé´ä¸­éè¿è´å¡å°æ²çº¿è¿ä¼¼æ¶é´åµå¥åºåçæ¹æ³ï¼æ¾å¼å°å»ºæ¨¡æ¶é´çå¹´ä»£è¿å±ï¼å¹¶å¼ºå¶æ§è¡å¹´ä»£ä¸è´æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
* <strong>VLMsçæ¶é´æç¥è½åï¼</strong> è¯ä¼°äº37ä¸ªæåè¿çVLMsï¼åç°ç°ä»£VLMså·ææ¾èçæ¶é´æç¥è½åï¼ä½æ§è½å è®­ç»æ°æ®è´¨éåéª¨å¹²æ¨¡åå¤ææ§èå¼ãOpenCLIPåEVA-CLIPæ¨¡åè¡¨ç°æä½³ã
* <strong>æ¶é´ä¿¡æ¯çç»æï¼</strong> å³é®åç°æ¯ï¼æ¶é´ä¿¡æ¯å¨VLMsçé«ç»´åµå¥ç©ºé´ä¸­æ²¿çä¸ä¸ªç´§åçãä½ç»´çéçº¿æ§æµå½¢ç»ç»ï¼å¹¶å·æå¼ºå¤§çå¹´ä»£ç»æãè¿è¡¨æVLMsä»¥ä¸åäºçº¯è¯­è¨æ¨¡åçæ¹å¼ç»ç»æ¶é´ä¿¡æ¯ã
* <strong>æ¶é´è½´å»ºæ¨¡çæææ§ï¼</strong> æåºçåºäºUMAPåè´å¡å°æ²çº¿çæ¶é´è½´æ¹æ³è½å¤ææå°ä»åµå¥ç©ºé´ä¸­æåæ¾å¼çæ¶é´è¡¨ç¤ºãè¿äºæ¹æ³å¨å¹´ä»£å¯¹é½æ¹é¢ï¼Spearmanç§©ç¸å³æ§é«è¾¾0.99ï¼è¡¨ç°åºè²ï¼å¹¶ä¸å¨æ¶é´é¢æµåç¡®æ§æ¹é¢ï¼MAEåTAIï¼ä¸åºäºæç¤ºçåºçº¿ç¸æ¯å·æç«äºæ§çè³æ´ä¼çæ§è½ã
* <strong>è®¡ç®æçï¼</strong> åºäºè´å¡å°æ²çº¿çæ¹æ³å¨è®¡ç®æçä¸æ¾èä¼äºUMAPæ¹æ³åæ¶é´æ¢æµåºçº¿ï¼è½å¤å®ç°æ´å¿«çæ¨çéåº¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
* <strong>ç±»å«ä¸å¹³è¡¡åå¹´åº¦èå´çå½±åï¼</strong> è®ºææåºï¼ç±»å«ä¸å¹³è¡¡åä¸åå¹´åº¦èå´å¯¹ç»æçå½±åéè¦è¿ä¸æ­¥ç ç©¶ã
* <strong>æ°æ®éæ©å±ï¼</strong> TIME10kæ°æ®éæªæ¥åºæ©å±å°åå«ä»¥äººä¸ºä¸­å¿æ¹é¢ï¼å¦æè£ï¼çç±»å«ã
* <strong>æ¹æ³æ³åæ§ï¼</strong> æ¶é´è½´æåæ¹æ³çè¯ä¼°åºæ©å±å°æ´å¤VLMsä»¥åçææ¨¡åï¼ä»¥è§å¯å®ä»¬æ¯å¦è¡¨ç°åºç±»ä¼¼çæ¶é´æç¥è½åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
* æ·±å¥ç ç©¶ç±»å«ä¸å¹³è¡¡åå¹´åº¦èå´å¯¹æ¶é´æç¥æ§è½çå½±åã
* æ©å±TIME10kæ°æ®éä»¥åå«æ´å¤ä»¥äººä¸ºä¸­å¿çç±»å«ï¼å¦æè£ï¼ä»¥æ´å¨é¢å°è¯ä¼°VLMsçæ¶é´æç¥è½åã
* å°æ¶é´è½´æåæ¹æ³åºç¨äºæ´å¹¿æ³çVLMsåçææ¨¡åï¼ä»¥æ¢ç´¢å¶æ³åè½åã
* æ¢è®¨å¦æVLMsè½å¤å¦æ­¤ææå°éå¼æè·åç»æåæ¶é´ä¿¡æ¯ï¼è¿ç§æ¹æ³æ¯å¦å¯ä»¥æ¨å¹¿å°å¶ä»åºæ°é®é¢ã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºçè§£VLMså¦ä½å¤çæ¶é´ä¿¡æ¯æä¾äºå¼åæ§çè§è§£ï¼å¹¶æåºäºææä¸é«æçæ¹æ³æ¥ä»å¶åµå¥ç©ºé´ä¸­æååå©ç¨æ¶é´ç»æï¼å¯¹äºåå²å¾ååæãæåç ç©¶åæ°å­æ¡£æ¡ç®¡çç­é¢åå·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce TIME10k, a benchmark
dataset of over 10,000 images with temporal ground truth, and evaluate the
time-awareness of 37 VLMs by a novel methodology.</li>
<li>Based on this insight, we propose methods
to derive an explicit ``timeline'' representation from the embedding space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19559v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19559v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19400v1'></a></p>
<h2 id="seeing-across-views-benchmarking-spatial-reasoning-of-vision-language-models-in-robotic-scenes"><a href="https://arxiv.org/abs/2510.19400v1">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</a></h2>
<p><strong>Authors:</strong> Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-language models (VLMs) are essential to Embodied AI, enabling robots
to perceive, reason, and act in complex environments. They also serve as the
foundation for the recent Vision-Language-Action (VLA) models. Yet most
evaluations of VLMs focus on single-view settings, leaving their ability to
integrate multi-view information underexplored. At the same time, multi-camera
setups are increasingly standard in robotic platforms, as they provide
complementary perspectives to mitigate occlusion and depth ambiguity. Whether
VLMs can effectively leverage such multi-view inputs for robotic reasoning
therefore remains an open question. To bridge this gap, we introduce
MV-RoboBench, a benchmark specifically designed to evaluate the multi-view
spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench
consists of 1.7k manually curated QA items across eight subtasks, divided into
two primary categories: spatial understanding and robotic execution. We
evaluate a diverse set of existing VLMs, including both open-source and
closed-source models, along with enhanced versions incorporating CoT-inspired
techniques. The results show that state-of-the-art models remain far below
human performance, underscoring the substantial challenges VLMs face in
multi-view robotic perception. Additionally, our analysis uncovers two key
findings: (i) spatial intelligence and robotic task execution are positively
correlated in multi-view robotic scenarios; and (ii) strong performance on
existing general-purpose single-view spatial understanding benchmarks does not
reliably translate to success in the robotic spatial tasks assessed by our
benchmark. We release MV-RoboBench as an open resource to foster progress in
spatially grounded VLMs and VLAs, providing not only data but also a
standardized evaluation protocol for multi-view embodied reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºå¼å¥äºMV-RoboBenchï¼è¿æ¯ä¸ä¸ªä¸é¨ç¨äºè¯ä¼°è§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨æºå¨äººæä½åºæ¯ä¸­å¤è§è§ç©ºé´æ¨çè½åçåºåãå®éè¿1.7kä¸ªæå¨ç­åçé®ç­é¡¹ï¼æ¶µçç©ºé´çè§£åæºå¨äººæ§è¡ä¸¤å¤§ç±»å«ä¸ªå­ä»»å¡ï¼æ­ç¤ºäºå½åæåè¿çVLMså¨å¤è§è§æºå¨äººæç¥æ¹é¢ä¸äººç±»è¡¨ç°çå·¨å¤§å·®è·ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>å³é®åæ°å¨äº<strong>MV-RoboBenchåºåçåå»º</strong>ãä¸åäºä»¥å¾ä¸»è¦å³æ³¨åè§è§è®¾ç½®çVLMè¯ä¼°ï¼MV-RoboBenchæç¡®éå¯¹å¤æåå¤´æºå¨äººå¹³å°çéæ±ï¼è®¾è®¡äºå¤è§è§ç©ºé´æ¨çä»»å¡ãå¶æ¹æ³è®ºåæ¬ï¼
*   <strong>å¤è§è§æ°æ®æ¶éä¸æ æ³¨ï¼</strong> æå¨ç­å1.7kä¸ªé®ç­é¡¹ï¼ç¡®ä¿äºæ°æ®è´¨éåä»»å¡çå¤ææ§ã
*   <strong>ä»»å¡åç±»ï¼</strong> å°ä»»å¡ç»åä¸ºç©ºé´çè§£åæºå¨äººæ§è¡ä¸¤å¤§ç±»ï¼æ´å¨é¢å°è¯ä¼°VLMsçè½åã
*   <strong>å¤æ¨¡åè¯ä¼°ï¼</strong> è¯ä¼°äºå¤ç§ç°æVLMï¼å¼æºåé­æºï¼ï¼å¹¶æ¢ç´¢äºCoTï¼Chain-of-Thoughtï¼å¯åå¼ææ¯çå¢å¼ºçæ¬ï¼æä¾äºå¹¿æ³çåºçº¿ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨å¤è§è§VLMç ç©¶ï¼</strong> MV-RoboBenchå°æä¸ºä¸ä¸ªæ ååçè¯ä¼°å·¥å·ï¼æ¿å±ç ç©¶äººåå¼åæ´å¼ºå¤§çãè½å¤æææ´åå¤è§è§ä¿¡æ¯çVLMåVLAæ¨¡åã</li>
<li><strong>å éå·èº«æºè½åå±ï¼</strong> è§£å³å¤è§è§ç©ºé´æ¨çæ¯å·èº«AIåæºå¨äººææ¯ä»å®éªå®¤èµ°åå®éåºç¨çå³é®ç¶é¢ï¼è¯¥åºåå°ç´æ¥ä¿è¿è¿ä¸é¢åçè¿æ­¥ã</li>
<li><strong>æ­ç¤ºå½åæ¨¡åå±éæ§ï¼</strong> è®ºææç¡®æåºå½åSOTAæ¨¡åä¸äººç±»è¡¨ç°çå·¨å¤§å·®è·ï¼ä»¥ååè§è§åºåçæåä¸ä¸å®è½è½¬åä¸ºå¤è§è§æºå¨äººä»»å¡çæåï¼è¿ä¸ºæªæ¥çç ç©¶ææäºæ¹åã</li>
<li><strong>ä¿è¿è·¨æ¨¡æåå¤æ¨¡æå­¦ä¹ ï¼</strong> å¼ºè°äºè§è§åè¯­è¨å¨å¤æç©ºé´æ¨çä¸­çååä½ç¨ï¼å¹¶ä¸ºå¤æ¨¡æå­¦ä¹ å¨å·èº«æºè½é¢åçåºç¨æä¾äºæ°çææåæºä¼ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å·èº«AIåæºå¨äººå­¦ï¼</strong> è¿æ¯æç´æ¥çåçé¢åï¼åæ¬æºå¨äººæåãå¯¼èªãæä½ãäººæºåä½ç­ã</li>
<li><strong>èªå¨é©¾é©¶ï¼</strong> èªå¨é©¾é©¶è½¦è¾éå¸¸éå¤å¤ä¸ªæåå¤´åä¼ æå¨ï¼éè¦æ´åå¤è§è§ä¿¡æ¯è¿è¡ç¯å¢æç¥åå³ç­ã</li>
<li><strong>èæç°å®/å¢å¼ºç°å® (VR/AR)ï¼</strong> å¨è¿äºåºç¨ä¸­ï¼çè§£ç¨æ·å¨å´ç3Dç©ºé´åç©ä½å³ç³»è³å³éè¦ï¼å¤è§è§ä¿¡æ¯æ´åè½åå¯ä»¥æåç¨æ·ä½éªã</li>
<li><strong>æºè½çæ§ï¼</strong> å¤æåå¤´çæ§ç³»ç»éè¦å¯¹åºæ¯è¿è¡å¨å±çè§£åäºä»¶æ¨çã</li>
<li><strong>éç¨è§è§-è¯­è¨æ¨¡åå¼åï¼</strong> ä»»ä½æ¨å¨æåVLMç©ºé´æ¨çè½åçéç¨æ¨¡åé½å°ä»è¿ä¸ªåºåä¸­åçã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>æ°æ®è§æ¨¡ï¼</strong> 1.7kä¸ªæå¨ç­åçQAé¡¹è½ç¶è´¨éé«ï¼ä½ç¸å¯¹äºå¤§è§æ¨¡æ°æ®é©±å¨çæ·±åº¦å­¦ä¹ æ¨¡åè®­ç»èè¨ï¼å¯è½ä¸ç®åºå¤§ãè¿å¯è½éå¶äºåºåå¨è®­ç»æ°æ¨¡åæ¶çç´æ¥åºç¨ï¼èæ´å¤å°ä½ä¸ºè¯ä¼°å·¥å·ã</li>
<li><strong>ä»»å¡å¤æåº¦ï¼</strong> æè¦ä¸­æå°âå«ä¸ªå­ä»»å¡ï¼åä¸ºç©ºé´çè§£åæºå¨äººæ§è¡âï¼ä½å·ä½ä»»å¡çå¤ææ§åå¤æ ·æ§ä»éè¿ä¸æ­¥äºè§£ãä¾å¦ï¼æ¯å¦æ¶µçäºå¨æåºæ¯ãä¸ç¡®å®æ§ãç²¾ç»æä½ç­æ´å·æææ§çæ¹é¢ã</li>
<li><strong>æ³åè½åï¼</strong> MV-RoboBenchä¸æ³¨äºâæºå¨äººæä½âåºæ¯ãè½ç¶è¿å¾éè¦ï¼ä½å¶ç»æååç°æ¯å¦è½å®å¨æ³åå°å¶ä»å¤è§è§å·èº«AIåºæ¯ï¼å¦èªå¨é©¾é©¶ãå¤æç¯å¢å¯¼èªï¼ä»ééªè¯ã</li>
<li><strong>è¯ä¼°ææ ï¼</strong> æè¦æªè¯¦ç»è¯´æå·ä½çè¯ä¼°ææ ï¼ä¾å¦ï¼å¯¹äºæºå¨äººæ§è¡ä»»å¡ï¼æ¯è¯ä¼°æåçãæçè¿æ¯å¶ä»æ´ç»è´çææ ã</li>
<li><strong>âCoT-inspired techniquesâçå·ä½å®ç°ï¼</strong> æè¦æå°ä½¿ç¨äºCoTå¯åå¼ææ¯ï¼ä½æªè¯¦ç»è¯´æå¶å·ä½å½¢å¼åææï¼è¿å¯è½å½±åå¯¹æ¨¡åæ§è½æåççè§£ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥MV-RoboBenchï¼å¡«è¡¥äºVLMè¯ä¼°å¨å¤è§è§æºå¨äººåºæ¯ä¸­çç©ºç½ï¼ä¸ºå·èº«AIé¢åå¸¦æ¥äºæ¥éçæ ååå·¥å·åæ°çç ç©¶æ¹åãå¶åç°ä¸ä»æ­ç¤ºäºå½åæ¨¡åçå±éæ§ï¼ä¹ä¸ºæªæ¥å¼åæ´æºè½ãæ´å·ç©ºé´æ¨çè½åçVLMåVLAæ¨¡åæä¾äºå®è´µçè§è§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce
MV-RoboBench, a benchmark specifically designed to evaluate the multi-view
spatial reasoning capabilities of VLMs in robotic manipulation.</li>
<li>The results show that state-of-the-art models remain far below
human performance, underscoring the substantial challenges VLMs face in
multi-view robotic perception.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19400v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19400v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19371v1'></a></p>
<h2 id="aegisrf-adversarial-perturbations-guided-with-sensitivity-for-protecting-intellectual-property-of-neural-radiance-fields"><a href="https://arxiv.org/abs/2510.19371v1">AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</a></h2>
<p><strong>Authors:</strong> Woo Jae Kim, Kyu Beom Han, Yoonki Cho, Youngju Na, Junsik Jung, Sooel Son, Sung-eui Yoon</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D
scene representation and novel view synthesis, protecting their intellectual
property (IP) from unauthorized use is becoming increasingly crucial. In this
work, we aim to protect the IP of NeRFs by injecting adversarial perturbations
that disrupt their unauthorized applications. However, perturbing the 3D
geometry of NeRFs can easily deform the underlying scene structure and thus
substantially degrade the rendering quality, which has led existing attempts to
avoid geometric perturbations or restrict them to explicit spaces like meshes.
To overcome this limitation, we introduce a learnable sensitivity to quantify
the spatially varying impact of geometric perturbations on rendering quality.
Building upon this, we propose AegisRF, a novel framework that consists of a
Perturbation Field, which injects adversarial perturbations into the
pre-rendering outputs (color and volume density) of NeRF models to fool an
unauthorized downstream target model, and a Sensitivity Field, which learns the
sensitivity to adaptively constrain geometric perturbations, preserving
rendering quality while disrupting unauthorized use. Our experimental
evaluations demonstrate the generalized applicability of AegisRF across diverse
downstream tasks and modalities, including multi-view image classification and
voxel-based 3D localization, while maintaining high visual fidelity. Codes are
available at https://github.com/wkim97/AegisRF.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæâAegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fieldsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼AegisRF: åºäºææåº¦å¼å¯¼çå¯¹ææ§æ°å¨ï¼ç¨äºä¿æ¤ç¥ç»è¾å°åºï¼NeRFï¼çç¥è¯äº§æ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç¥ç»è¾å°åºï¼NeRFï¼çç¥è¯äº§æï¼IPï¼ä¿æ¤é®é¢ãéçNeRFså¨3Dåºæ¯è¡¨ç¤ºåæ°é¢è§å¾åææ¹é¢çå¹¿æ³åºç¨ï¼å¶IPé¢ä¸´æªç»ææä½¿ç¨çé£é©æ¥çå¢å ãç°ææ¹æ³å¨å¯¹NeRFç3Då ä½è¿è¡æ°å¨æ¶ï¼å¾å¾ä¼å¯¼è´åºå±åºæ¯ç»æåå½¢ï¼ä»èä¸¥ééä½æ¸²æè´¨éï¼å æ­¤éå¸¸é¿åå ä½æ°å¨æå°å¶éå¶å¨ç½æ ¼ç­æ¾å¼ç©ºé´ä¸­ãæ ¸å¿ç ç©¶é®é¢æ¯å¦ä½å¨ä¸æ¾èæå®³æ¸²æè´¨éçåæä¸ï¼éè¿æ³¨å¥å¯¹ææ§æ°å¨æ¥ææå¹²æ°æªç»ææçä¸æ¸¸åºç¨ï¼ä»èä¿æ¤NeRFçIPã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
AegisRFæ¡æ¶å¼å¥äºä¸¤é¡¹å³é®åæ°ï¼</p>
<ul>
<li><strong>å¯å­¦ä¹ çææåº¦ï¼Learnable Sensitivityï¼ï¼</strong> è®ºæå¼å¥äºä¸ç§å¯å­¦ä¹ çææåº¦åº¦éï¼ç¨äºéå3Dç©ºé´ä¸­å ä½æ°å¨å¯¹æ¸²æè´¨éå½±åçç©ºé´ååç¨åº¦ãè¿ä½¿å¾ç³»ç»è½å¤å¯¹æ¸²æè´¨éææçåºåæ½å æ´ä¸¥æ ¼çå ä½æ°å¨çº¦æï¼èå¨æç¥åº¦è¾ä½çåºååè®¸æ´å¤§çæ°å¨ï¼ä»èå¨æå¤§åå¯¹æªç»ææä»»å¡çå¹²æ°ä¸æå°åæ¸²æè´¨éä¸éä¹é´åå¾å¹³è¡¡ã</li>
<li><strong>AegisRFæ¡æ¶ï¼</strong> è¿æ¯ä¸ä¸ªæ°é¢çæ¡æ¶ï¼åå«ä¸¤ä¸ªæ ¸å¿ç»ä»¶ï¼<ul>
<li><strong>æ°å¨åºï¼Perturbation Fieldï¼ï¼</strong> è¯¥ç»ä»¶è´è´£å°å¯¹ææ§æ°å¨æ³¨å¥NeRFæ¨¡åçé¢æ¸²æè¾åºï¼é¢è²åä½å¯åº¦ï¼ï¼æ¨å¨æ¬ºéªæªç»ææçä¸æ¸¸ç®æ æ¨¡åã</li>
<li><strong>ææåº¦åºï¼Sensitivity Fieldï¼ï¼</strong> è¯¥ç»ä»¶å­¦ä¹ ä¸è¿°ææåº¦ï¼ä»¥èªéåºå°çº¦æå ä½æ°å¨çå¤§å°ï¼ä»èå¨ç ´åæªç»ææä½¿ç¨çåæ¶ä¿ææ¸²æè´¨éã</li>
</ul>
</li>
</ul>
<p>éè¿å¨æ¨çæ¶ç´æ¥æ°å¨NeRFçé¢æ¸²æè¾åºï¼AegisRFè½å¤ä¸ºåç§æ°æ®æ¨¡æï¼å¦å¾åãä½ç´ ï¼çæå¯¹ææ§ç¤ºä¾ï¼ä»èå®ç°å¯¹å¤ç§ä¸æ¸¸ä»»å¡çéç¨ä¿æ¤ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªè¯ä¼°è¡¨æï¼</p>
<ul>
<li><strong>åè¶çä¿æ¤æ§è½ä¸è§è§ä¿çåº¦ï¼</strong> AegisRFå¨å¤è§å¾å¾ååç±»ååºäºä½ç´ ç3Då®ä½ç­å¤æ ·åä¸æ¸¸ä»»å¡ä¸­è¡¨ç°åºå¼ºå¤§çIPä¿æ¤è½åï¼åæ¶ä¿æäºé«è§è§ä¿çåº¦ãä¸ç°ææ¹æ³ï¼å¦NeRFailåAdv-FTï¼ç¸æ¯ï¼AegisRFå¨PSNRç­èªç¶åº¦ææ ä¸è¡¨ç°æ´å¥½ï¼åæ¶å®ç°äºæ´å¼ºçæ°å¨ææï¼ä¾å¦ï¼å¨å¤è§å¾åç±»ä¸­ï¼åç¡®çæ¾èéä½ï¼å¨3Då®ä½ä¸­ï¼AP25æ¾èéä½ï¼ã</li>
<li><strong>ææåº¦å¼å¯¼çæææ§ï¼</strong> å¯¹ææåº¦åºçåæè¯å®ï¼éè¿ææåº¦å¼å¯¼æ¥çº¦ææ°å¨å¯¹äºç»´æé«æ¸²æè´¨éè³å³éè¦ãææåº¦åºè½å¤å­¦ä¹ å¨å¤æçº¹çåºåï¼å¦æ¤å­ï¼ä¸å·æè¾ä½ææåº¦ï¼èå¨ç©ºæ·ç©ºé´ä¸­å·æè¾é«ææåº¦ï¼è¿ä¸äººç±»è§è§æç¥å¯¹å ä½æ°å¨çææåº¦ååæ¯ä¸è´çã</li>
<li><strong>é²æ£æ§ä¸å¯è¿ç§»æ§ï¼</strong> AegisRFçæ°å¨å¯¹å¸¸è§çå¾ååæ¢ï¼å¦JPEGåç¼©ãé«æ¯æ¨¡ç³ï¼å·æé²æ£æ§ãæ­¤å¤ï¼å®å¨è·¨æ¨¡åï¼ä»ä»£çæ¨¡åå°æªç¥ç®æ æ¨¡åï¼çä¸æ¸¸ä»»å¡ä¸­ä¹è¡¨ç°åºå¯è§çå¯è¿ç§»æ§ï¼è¿å¢å¼ºäºå¶å¨å®éIPä¿æ¤åºæ¯ä¸­çå®ç¨æ§ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> AegisRFå¼å¥çè®¡ç®ææ¬ç¸å¯¹äºåå§NeRFèè¨æ¯è¾¹éæ§çï¼è¿è¡¨æå¶å¨å®éåºç¨ä¸­å·æå¯è¡æ§ã</li>
</ul>
<p>è¿äºç»æçæä¹å¨äºï¼AegisRFæä¾äºä¸ç§ææä¸éç¨çNeRF IPä¿æ¤è§£å³æ¹æ¡ï¼åæäºç°ææ¹æ³å¨å¤çå ä½æ°å¨æ¶çå±éæ§ï¼å¹¶å¨ä¿æ¤ææåè§è§è´¨éä¹é´åå¾äºæ´å¥½çå¹³è¡¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æå°äºä»¥ä¸å±éæ§ï¼</p>
<ul>
<li><strong>è·¨æ¨¡åå¯è¿ç§»æ§ï¼</strong> å°½ç®¡AegisRFå¨è·¨æ¨¡åå¯è¿ç§»æ§æ¹é¢è¡¨ç°åºå¯è§çæ§è½ï¼ä½å¶ç®åçæ°´å¹³ä»ç¶æéï¼ç¹å«æ¯å¨åºç¨äºæ´å¤æçä»»å¡ï¼å¦3Då®ä½ï¼æå·ææ¾èä¸åæ¶æçæç¥æ¨¡åï¼å¦Swin-basedåCNN-basedéª¨å¹²ç½ç»ï¼æ¶ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>å¢å¼ºè·¨æ¨¡åå¯è¿ç§»æ§ï¼</strong> æªæ¥çç ç©¶åºè´åäºå¼ååè¿ææ¯ï¼éè¿å¨è®­ç»è¿ç¨ä¸­æ´åæ´å¤æ ·åçä»£çæ¨¡åæéç¨ç°æé»çå¯¹ææ§æ»å»ææ¯ï¼ä»¥æé«AegisRFå¨æ´å¹¿æ³çæç¥æ¨¡ååä¸æ¸¸ä»»å¡ä¸­çæ°å¨æçãè¿å°è¿ä¸æ­¥æåAegisRFçå®ç¨æ§ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D
scene representation and novel view synthesis, protecting their intellectual
property (IP) from unauthorized use is becoming increasingly crucial.</li>
<li>To overcome this limitation, we introduce a learnable sensitivity to quantify
the spatially varying impact of geometric perturbations on rendering quality.</li>
<li>Building upon this, we propose AegisRF, a novel framework that consists of a
Perturbation Field, which injects adversarial perturbations into the
pre-rendering outputs (color and volume density) of NeRF models to fool an
unauthorized downstream target model, and a Sensitivity Field, which learns the
sensitivity to adaptively constrain geometric perturbations, preserving
rendering quality while disrupting unauthorized use.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19371v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19371v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19336v1'></a></p>
<h2 id="damo-data-mixing-optimizer-in-fine-tuning-multimodal-llms-for-mobile-phone-agents"><a href="https://arxiv.org/abs/2510.19336v1">DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents</a></h2>
<p><strong>Authors:</strong> Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Mobile Phone Agents (MPAs) have emerged as a promising research direction due
to their broad applicability across diverse scenarios. While Multimodal Large
Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness
in handling multiple mobile phone tasks simultaneously remains limited.
Although multitask supervised fine-tuning (SFT) is widely adopted for multitask
learning, existing approaches struggle to determine optimal training data
compositions for peak performance. To address this challenge, we propose DaMo
(Data Mixture Optimizer) - a novel solution employing a trainable network that
predicts optimal data mixtures by forecasting downstream task performance for
any given dataset ratio. To support comprehensive evaluation, we introduce
PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on
multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse
real-world industrial mobile application scenarios. Demonstrating strong
predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo
efficiently extrapolates optimal data mixing configurations. Our results show
DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to
alternative methods. Furthermore, extensive experiments across established
benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench
reveal DaMo's superior generalization, outperforming other approaches by 2.57%
in terms of average score. When used solely for MLLM optimization on the
BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,
DaMo maintains robust scalability, preserving its effectiveness when applied to
other model architectures. The code and dataset are available at
https://github.com/OPPO-Mente-Lab/DaMo.git</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Kai Shiç­äººæ°åçè®ºæâDaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agentsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="damo-llm">è®ºææè¦ï¼DaMo: ç§»å¨æºè½ä½å¤æ¨¡æLLMå¾®è°ä¸­çæ°æ®æ··åä¼åå¨</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨ç§»å¨æºè½ä½ï¼MPAsï¼ä¸­åæ¶å¤çå¤ä»»å¡æ¶æçæéçé®é¢ãå·ä½èè¨ï¼å°½ç®¡å¤ä»»å¡çç£å¾®è°ï¼SFTï¼è¢«å¹¿æ³éç¨ï¼ä½ç°ææ¹æ³é¾ä»¥ç¡®å®æä½³çè®­ç»æ°æ®ç»æï¼ä»¥å®ç°æ¨¡åå¨ä¸æ¸¸ä»»å¡ä¸çå³°å¼æ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
*   <strong>DaMo (Data Mixture Optimizer) æ¡æ¶ï¼</strong> æåºäºä¸ç§æ°é¢çè§£å³æ¹æ¡ï¼éè¿ä¸ä¸ªå¯è®­ç»çç½ç»æ¥é¢æµæä½³æ°æ®æ··åæ¯ä¾ãè¯¥ç½ç»è½å¤æ ¹æ®ç»å®çæ°æ®éæ¯ä¾é¢æµä¸æ¸¸ä»»å¡æ§è½ï¼ä»èé«æå°æ¨æ­åºæä¼æ°æ®æ··åéç½®ãè¿é¿åäºä¼ ç»æ¹æ³ä¸­æè´µçæå¨è¿­ä»£åç©·ä¸¾æç´¢ã
*   <strong>ä¸æ¸¸ä»»å¡æ§è½é¢æµ (DaPP)ï¼</strong> DaMoçæ ¸å¿æ¯DaPPæ¹æ³ï¼å®å©ç¨ä¸ä¸ªå¯è®­ç»çç¥ç»ç½ç»ç´æ¥é¢æµæ¨¡åå¨ä¸æ¸¸ä»»å¡ä¸çæ§è½ï¼èæ éå®éæ¨¡åè®­ç»ãè¿è§£å³äºç°ææ¹æ³æªè½ç´æ¥å³èæ°æ®æ··åä¸æ¨¡åå¨ä¸æ¸¸ä»»å¡ä¸æ§è½çé®é¢ã
*   <strong>PhoneAgentBench åºåæµè¯ï¼</strong> ä¸ºäºæ¯æå¨é¢çè¯ä¼°ï¼è®ºæå¼å¥äºPhoneAgentBenchï¼è¿æ¯é¦ä¸ªä¸é¨ç¨äºè¯ä¼°MLLMså¨å¤æ¨¡æç§»å¨çµè¯ä»»å¡ä¸çåºåæµè¯ãå®åå«1235ä¸ªQAå¯¹ï¼æ¶µçäºçå®ä¸ççå·¥ä¸ç§»å¨åºç¨åºæ¯ï¼å¹¶è¯ä¼°äºå¤æä»»å¡è§åãè®¾å¤åçå·¥å·ä½¿ç¨ãå¤æ¨¡æè®°å¿åå±å¹ä¸ä¸æçè§£åé¡¹åºæ¬è½åã
*   <strong>å¯æ©å±æ§ä¸æ³åæ§ï¼</strong> DaMoè¢«è®¾è®¡ä¸ºæ¨¡åæ å³çé¢æµå¨ï¼éè¿å°éæ ¡åæ ·æ¬ççº¿æ§æ å°ï¼å¯ä»¥ææå°å°å¶åºç¨äºå¶ä»æ¨¡åæ¶æï¼å¹¶ä¿æç¨³å®çé¢æµåç¡®æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>PhoneAgentBench æ§è½æåï¼</strong> DaMoå¨PhoneAgentBenchä¸å®ç°äº3.38%çæ§è½æåï¼æ¾èä¼äºå¶ä»æ¿ä»£æ¹æ³ãä¸æªè¿è¡SFTçåå§æ¨¡åç¸æ¯ï¼DaMoå¨PhoneAgentBenchä¸å®ç°äºè¶è¿23%çæ§è½æåã
*   <strong>åè¶çæ³åè½åï¼</strong> å¨BFCL-v3ãMME-ReasoningãMME-PerceptionåOCRBenchç­éç¨åºåæµè¯ä¸­ï¼DaMoçå¹³åå¾åæ¯å¶ä»æ¹æ³é«åº2.57%ï¼æ¾ç¤ºåºå¶ä¼è¶çæ³åè½åã
*   <strong>ç¹å®ä»»å¡ä¼åææï¼</strong> å½ä»ç¨äºBFCL-v3ä»»å¡çMLLMä¼åæ¶ï¼DaMoå°ææ æé«äº12.47%ï¼è¡¨æä¸æ³¨äºä»»å¡ç¹å®ç®æ å¯ä»¥å¸¦æ¥æ¾èçæ¹è¿ã
*   <strong>é²æ£çé¢æµè½åä¸å¯æ©å±æ§ï¼</strong> å¨å°è§æ¨¡è¯ç¹å®éªä¸­ï¼DaMoå±ç¤ºäºå¼ºå¤§çé¢æµè½åï¼RÂ²=0.81ï¼ï¼è½å¤é«æå°æ¨æ­åºæä½³æ°æ®æ··åéç½®ãæ­¤å¤ï¼DaMoå¨åºç¨äºå¶ä»æ¨¡åæ¶ææ¶ä»è½ä¿æå¶æææ§ï¼Pearsonç¸å³ç³»æ°æ®éé«äº0.75ï¼ç»è¿çº¿æ§æ å°åçè³è¶è¿0.9ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ ·æ¬é¡ºåºååºå®æ°æ®æ··åæ¯ä¾çåè®¾ï¼</strong> è¯¥ç ç©¶åºäºä¸¤ä¸ªå³é®åè®¾ï¼1) å¿½ç¥åä¸ªæ°æ®éä¸­æ ·æ¬çé¡ºåºï¼2) å¨æ´ä¸ªè®­ç»è¿ç¨ä¸­ä¿æåºå®çæ°æ®æ··åæ¯ä¾ã
*   <strong>å¨ææ°æ®æ··åçæ¢ç´¢æ§ï¼</strong> å°½ç®¡åæ­¥å°è¯æ¾å®½è¿äºåè®¾ï¼ç¹å«æ¯éè¿å¨ææ°æ®æ··åè°æ´ï¼ä»å¨æ¢ç´¢ä¸­ï¼ä½å°æªå»ºç«ç³»ç»çæ¹æ³æ¥æ¨æ­æä½³å¨ææ··åï¼ä¹æªéåå¶è®¡ç®ææ¬åæ§è½æ¶çã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨ææ°æ®æ··åä¼åæ¡æ¶ï¼</strong> è®¡åå°èç¹å¡æ´æ æç´¢ï¼MCTSï¼ä¸å¼ºåå­¦ä¹ ç¸ç»åï¼ä»¥è¿­ä»£ç¡®å®é¶æ®µç¹å®çæ°æ®æ··åï¼æ¨å¨æ¨æ­æä½³æ··åè½¨è¿¹ï¼ä»èåè½»ä»»å¡å²çªåç¾é¾æ§éå¿ã
*   <strong>æ´åæ ·æ¬è´¨éææ ï¼</strong> å»ºè®®å°æ ·æ¬è´¨éææ ä½ä¸ºä¸æ¸¸æ§è½é¢æµçè¾å¥åéï¼ä»¥å®ç°é¾åº¦æç¥éæ ·ã
*   <strong>PhoneAgentBench å¢å¼ºï¼</strong> è¿ä¸æ­¥å¢å¼ºPhoneAgentBenchï¼ä½¿å¶æ´å¥½å°éåºè®¾å¤ç«¯AIé¨ç½²çå¿«éåå±éæ±ï¼ç¡®ä¿å¶å¯¹æ°å´ç§»å¨ä¸­å¿AIèå¼çéåºæ§ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥DaMoæ¡æ¶åPhoneAgentBenchåºåæµè¯ï¼ä¸ºç§»å¨æºè½ä½é¢åçå¤æ¨¡æLLMså¾®è°æä¾äºä¸ä¸ªåæ°ä¸é«æçè§£å³æ¹æ¡ãå¶æ ¸å¿è´¡ç®å¨äºè½å¤é¢æµæä½³æ°æ®æ··åæ¯ä¾ï¼æ¾èæåäºæ¨¡åå¨å¤ä»»å¡åéç¨ä»»å¡ä¸çæ§è½ï¼å¹¶å±ç¤ºäºè¯å¥½çæ³åæ§åå¯æ©å±æ§ãå°½ç®¡å­å¨å¯¹æ ·æ¬é¡ºåºååºå®æ··åæ¯ä¾çåè®¾ï¼ä½è®ºæä¹æç¡®æåºäºæªæ¥çç ç©¶æ¹åï¼ä»¥è§£å³è¿äºå±éæ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, we propose DaMo
(Data Mixture Optimizer) - a novel solution employing a trainable network that
predicts optimal data mixtures by forecasting downstream task performance for
any given dataset ratio.</li>
<li>To support comprehensive evaluation, we introduce
PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on
multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse
real-world industrial mobile application scenarios.</li>
<li>Our results show
DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to
alternative methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19336v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19336v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-23 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
