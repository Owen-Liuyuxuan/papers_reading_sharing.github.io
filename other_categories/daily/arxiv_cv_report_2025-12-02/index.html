<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-02 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-01/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-03/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-02">Arxiv Computer Vision Papers - 2025-12-02</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-sync-multi-camera-synchronization-via-cross-view-object-motion" class="nav-link">Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion</a>
                </li>
                <li class="nav-item">
                    <a href="#objects-in-generated-videos-are-slower-than-they-appear-models-suffer-sub-earth-gravity-and-dont-know-galileos-principlefor-now" class="nav-link">Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now</a>
                </li>
                <li class="nav-item">
                    <a href="#generative-video-motion-editing-with-3d-point-tracks" class="nav-link">Generative Video Motion Editing with 3D Point Tracks</a>
                </li>
                <li class="nav-item">
                    <a href="#tuna-taming-unified-visual-representations-for-native-unified-multimodal-models" class="nav-link">TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</a>
                </li>
                <li class="nav-item">
                    <a href="#manualvla-a-unified-vla-model-for-chain-of-thought-manual-generation-and-robotic-manipulation" class="nav-link">ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#improved-mean-flows-on-the-challenges-of-fastforward-generative-models" class="nav-link">Improved Mean Flows: On the Challenges of Fastforward Generative Models</a>
                </li>
                <li class="nav-item">
                    <a href="#airsim360-a-panoramic-simulation-platform-within-drone-view" class="nav-link">AirSim360: A Panoramic Simulation Platform within Drone View</a>
                </li>
                <li class="nav-item">
                    <a href="#mv-tap-tracking-any-point-in-multi-view-videos" class="nav-link">MV-TAP: Tracking Any Point in Multi-View Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-visual-affordance-from-audio" class="nav-link">Learning Visual Affordance from Audio</a>
                </li>
                <li class="nav-item">
                    <a href="#road-rollouts-as-demonstrations-for-closed-loop-supervised-fine-tuning-of-autonomous-driving-policies" class="nav-link">RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-02">Arxiv Computer Vision Papers - 2025-12-02</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月1日Arxiv计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月1日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>日期：</strong> 2025年12月1日</p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期Arxiv论文集聚焦于<strong>多模态理解、生成模型改进、以及机器人与自动驾驶的实际应用</strong>。我们观察到对<strong>视频理解和生成</strong>的持续深入研究，特别是在<strong>运动同步、生成内容真实性</strong>以及<strong>视频编辑</strong>方面。同时，<strong>统一视觉表示</strong>和<strong>跨模态学习</strong>（如视觉与音频、视觉与文本）是构建更强大、更通用AI模型的关键方向。此外，<strong>仿真平台</strong>的进步和<strong>闭环学习</strong>在自动驾驶领域的应用也值得关注。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>“Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion”</strong> 提出了一种新颖的多摄像头同步方法，利用跨视图物体运动进行校准，这对于多视角3D重建和场景理解至关重要。</li>
<li><strong>“Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now”</strong> 揭示了当前生成视频模型在物理规律（如重力）上的不足，并提出了改进方向，这对提升生成视频的真实感和可信度具有重要意义。</li>
<li><strong>“TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models”</strong> 探索了如何为原生统一的多模态模型构建更有效的统一视觉表示，是迈向更通用AI的关键一步。</li>
<li><strong>“ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation”</strong> 展示了一个统一的视觉语言模型，能够同时处理思维链式推理和机器人操作，预示着AI在复杂任务执行上的潜力。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>生成模型中的物理一致性：</strong> 关注如何让生成模型（尤其是视频）遵循现实世界的物理规律，例如重力、运动学等。</li>
<li><strong>跨模态的深度融合：</strong> 探索视觉信息与音频、文本、甚至触觉等其他模态的更深层次融合，以实现更全面的理解和交互。</li>
<li><strong>统一的多模态表示学习：</strong> 研究如何构建能够有效处理多种模态数据的通用表示，为构建更强大的多模态模型奠定基础。</li>
<li><strong>仿真平台与闭环学习：</strong> 利用先进的仿真平台（如AirSim360）进行大规模数据生成和模型训练，并结合闭环学习方法（如RoaD）提升自动驾驶等领域的性能。</li>
<li><strong>细粒度的视频运动理解与编辑：</strong> 进一步研究视频中物体的精确跟踪、同步以及基于3D点轨迹的生成式编辑。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的广泛影响和技术创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>“Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion”</strong>：对于需要精确多视角几何理解的研究领域（如3D重建、SLAM、增强现实）具有直接应用价值。</li>
<li><strong>“Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now”</strong>：对于所有从事视频生成研究的研究人员都至关重要，它指出了当前模型的一个普遍性缺陷，并可能启发新的评估指标和训练策略。</li>
<li><strong>“TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models”</strong>：对于希望构建更通用、更强大的多模态AI系统的研究者来说，理解其表示学习方法将非常有益。</li>
<li><strong>“ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation”</strong>：对于机器人学、人机交互以及需要复杂推理和操作的AI应用领域的研究者，这篇论文提供了重要的思路和方法。</li>
</ol>
<hr />
<p>希望这份执行摘要能帮助您快速了解近期Arxiv计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.02017v1">Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion</a></li>
<li><a href="#2512.02016v1">Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now</a></li>
<li><a href="#2512.02015v1">Generative Video Motion Editing with 3D Point Tracks</a></li>
<li><a href="#2512.02014v1">TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</a></li>
<li><a href="#2512.02013v1">ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</a></li>
<li><a href="#2512.02012v1">Improved Mean Flows: On the Challenges of Fastforward Generative Models</a></li>
<li><a href="#2512.02009v1">AirSim360: A Panoramic Simulation Platform within Drone View</a></li>
<li><a href="#2512.02006v1">MV-TAP: Tracking Any Point in Multi-View Videos</a></li>
<li><a href="#2512.02005v1">Learning Visual Affordance from Audio</a></li>
<li><a href="#2512.01993v1">RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.02017v1'></a></p>
<h2 id="visual-sync-multi-camera-synchronization-via-cross-view-object-motion"><a href="https://arxiv.org/abs/2512.02017v1">Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion</a></h2>
<p><strong>Authors:</strong> Shaowei Liu, David Yifan Yao, Saurabh Gupta, Shenlong Wang</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“VisualSync: Multi-Camera Synchronization via Cross-View Object Motion”论文的中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> VisualSync: Multi-Camera Synchronization via Cross-View Object Motion</p>
<p><strong>作者：</strong> Shaowei Liu, David Yifan Yao, Saurabh Gupta, Shenlong Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
本文旨在解决一个普遍存在的挑战：如何精确地同步来自多个不同视角的、未预先校准且不同步的视频流。现有的同步方法通常依赖于受控环境、特定目标、手动干预或昂贵的硬件，这限制了它们在日常生活中“随处可见”的视频场景中的应用。</p>
<p><strong>2. 核心创新与方法论贡献：</strong>
VisualSync 提出了一种基于多视角动态的优化框架，能够以毫秒级精度对未预先校准、不同步的视频进行对齐。其核心洞察在于：当一个三维运动点在两个摄像机中同时可见时，一旦视频被正确同步，该点将遵循对极约束。为了利用这一原理，VisualSync 整合了现有的先进技术，包括：
*   <strong>三维重建与相机姿态估计：</strong> 利用 VGGT 等工具估计相机参数（内参和外参）。
*   <strong>密集跟踪与跨视角匹配：</strong> 利用 CoTracker 等工具提取密集的三维点轨迹（tracklets），并利用 Mast3R 等工具建立跨视角的对应关系。
*   <strong>基于对极几何的优化：</strong> 将同步问题转化为一个优化问题，通过最小化跨视角对应点对之间的对极误差来估计每个摄像机的精确时间偏移量。该方法采用三阶段策略：首先估计相机参数和跨视角对应关系（Stage 0），然后通过穷举搜索估计每对摄像机之间的最优时间偏移量（Stage 1），最后全局优化所有摄像机的偏移量以实现整体同步（Stage 2）。</p>
<p><strong>3. 主要结果与意义：</strong>
在四个多样化且具有挑战性的数据集上进行的实验表明，VisualSync 在同步精度上显著优于现有基线方法，实现了低于 50 毫秒的中位数同步误差。该方法在处理具有大尺度运动、视角变化、运动模糊和变焦的真实世界视频时表现出强大的鲁棒性和通用性。其意义在于，它为处理日常生活中普遍存在的、未同步的多视角视频数据提供了一个实用且高效的解决方案，为后续的 4D 场景理解、新视角合成等下游应用奠定了基础。</p>
<p><strong>4. 提及的局限性：</strong>
论文中指出了 VisualSync 的三个主要局限性：
*   <strong>相机姿态的依赖性：</strong> 方法需要一个可靠的相机姿态子集（尽管不要求整个序列都精确）。
*   <strong>运动速度变化的处理：</strong> 对于包含非均匀运动速度的视频片段（例如，慢动作和快动作交替的场景）可能难以处理。
*   <strong>计算复杂度：</strong> 成对估计步骤的计算复杂度为 O(N^2)，其中 N 是视频的数量，这在大规模设置下可能会影响效率。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
虽然论文没有明确列出未来研究方向，但基于其方法和局限性，可以推测以下潜在方向：
*   <strong>完全无监督的相机姿态估计：</strong> 进一步减少对预先估计的相机姿态的依赖。
*   <strong>处理更复杂的运动模式：</strong> 探索能够处理非均匀运动速度和更极端运动场景的方法。
*   <strong>提高大规模场景的效率：</strong> 研究更高效的成对估计策略或全局优化方法，以支持更多数量的摄像机。
*   <strong>端到端学习：</strong> 探索将同步过程与下游任务（如新视角合成）进行端到端联合优化的可能性。
*   <strong>实时同步：</strong> 进一步优化算法以实现实时或近实时的多视角视频同步。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy.</li>
<li>Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02017v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02017v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02016v1'></a></p>
<h2 id="objects-in-generated-videos-are-slower-than-they-appear-models-suffer-sub-earth-gravity-and-dont-know-galileos-principlefor-now"><a href="https://arxiv.org/abs/2512.02016v1">Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now</a></h2>
<p><strong>Authors:</strong> Varun Varma Thozhiyoor, Shivam Tripathi, Venkatesh Babu Radhakrishnan, Anand Bhattad</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio <script type="math/tex">t_1^2/t_2^2 = h_1/h_2</script>, a relationship independent of <script type="math/tex">g</script>, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises <script type="math/tex">g_{\mathrm{eff}}</script> from <script type="math/tex">1.81\,\mathrm{m/s^2}</script> to <script type="math/tex">6.43\,\mathrm{m/s^2}</script> (reaching <script type="math/tex">65\%</script> of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now
<strong>Authors:</strong> Varun Varma Thozhiyoor, Shivam Tripathi, Venkatesh Babu Radhakrishnan, Anand Bhattad
<strong>Categories:</strong> cs.CV
<strong>Published Date:</strong> 2025-12-1</p>
<p><strong>Abstract:</strong>
Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio <script type="math/tex">t_1^2/t_2^2 = h_1/h_2</script>, a relationship independent of <script type="math/tex">g</script>, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises <script type="math/tex">g_{\mathrm{eff}}</script> from <script type="math/tex">1.81\,\mathrm{m/s^2}</script> to <script type="math/tex">6.43\,\mathrm{m/s^2}</script> (reaching <script type="math/tex">65\%</script> of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.</p>
<hr />
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）：</strong></p>
<p>本研究揭示了当前视频生成模型在模拟重力方面存在显著的物理不准确性，它们生成的物体下落速度比实际慢，表现出“亚地表重力”。研究者提出了一种创新的、无单位的相对测试方法，以克服尺度模糊性，并证明了模型违反了伽利略的等效原理。更重要的是，他们展示了通过少量特定数据的微调，可以显著改善模型对重力的物理理解，并能泛化到更复杂的物理场景。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>单位无关的相对测试协议：</strong> 这是本研究的核心创新。为了解决传统物理测试中常见的尺度模糊性（如相机焦距、物体真实尺寸、帧率不确定性等），作者设计了一个“单位无关”的测试方法。该方法基于伽利略自由落体定律的一个推论：对于两个同时开始下落、高度分别为 <script type="math/tex">h_1</script> 和 <script type="math/tex">h_2</script> 的物体，其下落时间 <script type="math/tex">t_1</script> 和 <script type="math/tex">t_2</script> 满足关系 <script type="math/tex">t_1^2/t_2^2 = h_1/h_2</script>。这个关系式不依赖于重力加速度 <script type="math/tex">g</script>、相机参数或物体尺寸，因此能够更纯粹地评估模型对物理规律的内在理解。</li>
<li><strong>识别“亚地表重力”和伽利略等效原理的违反：</strong> 通过上述单位无关的测试，研究者能够精确地量化模型在模拟重力加速度上的偏差，并发现模型未能遵循伽利略的等效原理（即不同质量的物体在同一重力场下自由落体时具有相同的加速度）。</li>
<li><strong>轻量级适配器进行物理规律的“特化”：</strong> 研究者提出了一种通过“低秩适配器”（Low-Rank Adaptor, LoRA）进行微调的方法。这种方法仅使用少量（100个）单球下落的视频片段，就能显著提升模型对重力加速度的模拟精度，并且这种改进还能零样本（zero-shot）地泛化到更复杂的场景，如双球下落和斜面运动。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升视频生成模型的“世界模型”能力：</strong> 随着视频生成模型被视为潜在的“世界模型”，理解和模拟物理规律是其核心能力之一。这项研究直接指出了当前模型在物理模拟上的一个关键短板，并提供了一种评估和改进的方法，将推动视频生成模型向更逼真、更具物理一致性的方向发展。</li>
<li><strong>为物理模拟和验证提供新工具：</strong> 该研究提出的单位无关测试协议，为评估AI模型对物理规律的理解提供了一个更鲁棒、更通用的工具，可以应用于其他需要物理模拟的AI任务。</li>
<li><strong>加速AI在物理相关领域的应用：</strong> 如果模型能够更准确地模拟物理现象，将极大地促进AI在机器人学、自动驾驶、游戏开发、科学模拟等领域的应用。</li>
<li><strong>数据效率的提升：</strong> 展示了通过少量数据进行针对性微调即可显著改善特定物理规律的模拟能力，这对于在数据稀缺的物理场景下训练模型具有重要意义。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>视频生成和内容创作：</strong> 提高生成视频的真实感和物理一致性，避免出现不自然的运动轨迹。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 创造更具沉浸感和物理真实感的虚拟环境。</li>
<li><strong>机器人学和仿真：</strong> 训练机器人模型时，需要准确的物理仿真来预测和规划动作。</li>
<li><strong>自动驾驶：</strong> 模拟车辆在不同物理条件下的运动，用于训练和测试自动驾驶算法。</li>
<li><strong>游戏开发：</strong> 创造更逼真的游戏物理引擎。</li>
<li><strong>科学研究和教育：</strong> 用于可视化和模拟物理实验，辅助科学理解和教学。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>“亚地表重力”的根本原因未完全揭示：</strong> 摘要指出了模型存在“亚地表重力”的问题，但并未深入探讨模型内部机制导致这一现象的根本原因（例如，是训练数据中的偏差，还是模型架构本身的限制）。</li>
<li><strong>物理规律的修复仍不完美：</strong> 尽管通过微调显著提升了重力模拟的精度（达到65%），但摘要也明确指出是“部分缓解”和“达到65%”，这意味着模型尚未完全掌握真实的地球重力，仍有改进空间。</li>
<li><strong>仅关注重力这一单一物理定律：</strong> 研究聚焦于重力，虽然是基础物理定律，但世界模型需要理解的物理规律远不止于此。这项研究的成果是否能直接推广到其他物理定律（如惯性、摩擦力、弹性碰撞等）仍需进一步验证。</li>
<li><strong>“零样本泛化”的范围有限：</strong> 虽然展示了对双球下落和斜面运动的零样本泛化能力，但这些场景仍然相对简单，且是基于重力定律的直接延伸。对于更复杂的物理交互和多体动力学，其泛化能力可能受到限制。</li>
<li><strong>微调数据的来源和多样性：</strong> 摘要提到使用“100个单球剪辑”，但这些剪辑的具体内容、多样性（例如，不同材质、不同初始速度、不同背景等）并未详细说明，这可能影响微调效果的普适性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文在计算机视觉领域具有重要的理论和实践意义。它不仅揭示了当前视频生成模型在物理模拟方面的一个普遍且关键的缺陷，更重要的是，它提供了一种创新的、鲁棒的评估方法，并展示了一种高效的数据驱动的修复策略。这项研究为构建更具物理智能的AI模型铺平了道路，尤其是在需要精确物理交互的应用场景中，其潜在价值巨大。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio <script type="math/tex">t_1^2/t_2^2 = h_1/h_2</script>, a relationship independent of <script type="math/tex">g</script>, focal length, and scale.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02016v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02016v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02015v1'></a></p>
<h2 id="generative-video-motion-editing-with-3d-point-tracks"><a href="https://arxiv.org/abs/2512.02015v1">Generative Video Motion Editing with 3D Point Tracks</a></h2>
<p><strong>Authors:</strong> Yao-Chih Lee, Zhoutong Zhang, Jiahui Huang, Jui-Hsien Wang, Joon-Young Lee, Jia-Bin Huang, Eli Shechtman, Zhengqi Li</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Generative Video Motion Editing with 3D Point Tracks”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Generative Video Motion Editing with 3D Point Tracks
<strong>作者：</strong> Yao-Chih Lee, Zhoutong Zhang, Jiahui Huang, Jui-Hsien Wang, Joon-Young Lee, Jia-Bin Huang, Eli Shechtman, Zhengqi Li</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
视频的叙事性很大程度上依赖于相机运动和物体运动的协同。然而，精确地编辑这些运动，尤其是在物体运动复杂且存在遮挡的情况下，仍然是一个重大挑战。现有的图像到视频（I2V）方法往往缺乏对整个场景的上下文感知，导致编辑不一致；而视频到视频（V2V）方法虽然能实现视角变化或基本物体平移，但对精细的物体运动控制能力有限。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
本文提出了一种名为 <strong>Edit-by-Track</strong> 的新颖 <strong>基于3D点轨迹的视频到视频（V2V）框架</strong>，实现了相机和物体运动的联合编辑。其核心创新包括：</p>
<ul>
<li><strong>3D点轨迹作为统一的运动表示：</strong> 利用3D点轨迹来捕捉相机和物体运动的丰富上下文信息，并建立稀疏对应关系，从而将源视频的丰富上下文转移到新的运动中，同时保持时空连贯性。与2D轨迹相比，3D轨迹提供了明确的深度线索，有助于解决深度顺序和遮挡问题，实现更精确的运动编辑。</li>
<li><strong>3D轨迹条件化V2V模型：</strong> 框架的核心是一个基于预训练的文本到视频（T2V）扩散模型（Wan-2.1）进行微调的V2V模型。通过一个新颖的 <strong>3D轨迹条件化模块</strong>，该模块能够自适应地从源视频中采样视觉上下文，并将其投射到目标帧空间，实现3D感知的运动控制。该模块采用交叉注意力机制，能够处理可变数量的3D轨迹，并对噪声轨迹具有鲁棒性。</li>
<li><strong>两阶段训练策略：</strong> 为了解决高质量、带标注的视频对稀缺的问题，论文采用了两阶段训练策略：<ul>
<li><strong>第一阶段（合成数据引导）：</strong> 在合成的视频对上进行训练，以学习核心的运动控制能力。</li>
<li><strong>第二阶段（真实数据微调）：</strong> 在从单目视频中采样的大量真实视频对上进行微调，以显著提高模型的泛化能力，并弥合合成与真实数据之间的领域差距。</li>
</ul>
</li>
<li><strong>支持多种编辑任务：</strong> 该框架能够实现多种多样的编辑任务，包括：<ul>
<li>联合相机和物体运动编辑。</li>
<li>运动迁移（例如，多舞者同步）。</li>
<li>非刚性形变（例如，物体形状变形）。</li>
<li>物体移除和复制。</li>
<li>处理部分轨迹输入，减轻用户负担。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
论文通过在DyCheck数据集和“in-the-wild”视频上的大量实验证明了Edit-by-Track的有效性。
*   <strong>定量结果：</strong> 在PSNR、SSIM、LPIPS等指标上，该方法在联合相机和物体运动编辑任务上显著优于现有方法，尤其是在保持场景上下文和处理复杂运动方面。
*   <strong>定性结果：</strong> 论文展示了Edit-by-Track能够实现精细的相机视角和物体运动控制，生成逼真且连贯的视频，甚至在不切实际的编辑场景下也能取得良好效果。
*   <strong>意义：</strong> 该方法为视频编辑领域带来了新的可能性，使得用户能够以前所未有的精度和灵活性来控制视频的动态内容，解锁了新的创意潜力。</p>
<p><strong>4. 论文提及的局限性：</strong>
*   <strong>密集轨迹的挑战：</strong> 当点轨迹密集聚集，尤其是在小物体上且伴随大运动时，模型可能难以准确提取视觉上下文和应用运动条件，导致失真。
*   <strong>复杂物理现象的合成：</strong> 模型可能难以正确合成由编辑运动引起的复杂物理现象（例如，液体动力学），尽管它能处理一些二次效应（如水花、阴影）。
*   <strong>对生成先验的依赖：</strong> 模型在合成复杂物理现象方面的局限性，反映了当前生成模型在物理约束方面的不足。</p>
<p><strong>5. 未来研究方向：</strong>
论文指出，上述局限性可以通过以下方式缓解：
*   <strong>物理约束的生成模型：</strong> 发展更强的物理约束生成模型。
*   <strong>数据规模化：</strong> 利用更大规模的数据进行训练。
*   <strong>用户界面改进：</strong> 开发更易于使用的3D GUI编辑器，使相机和3D物体运动编辑更加便捷。</p>
<p>总而言之，Edit-by-Track通过引入3D点轨迹作为统一的运动表示和创新的3D轨迹条件化V2V框架，显著提升了视频运动编辑的精度和灵活性，为视频创作和后期制作开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a track-conditioned V2V framework that enables joint editing of camera and object motion.</li>
<li>We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions.</li>
<li>These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence.</li>
<li>Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02015v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02015v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02014v1'></a></p>
<h2 id="tuna-taming-unified-visual-representations-for-native-unified-multimodal-models"><a href="https://arxiv.org/abs/2512.02014v1">TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</a></h2>
<p><strong>Authors:</strong> Zhiheng Liu, Weiming Ren, Haozhe Liu, Zijian Zhou, Shoufa Chen, Haonan Qiu, Xiaoke Huang, Zhaochong An, Fanny Yang, Aditya Patel, Viktar Atliha, Tony Ng, Xiao Han, Chuyan Zhu, Chenyang Zhang, Ding Liu, Juan-Manuel Perez-Rua, Sen He, Jürgen Schmidhuber, Wenhu Chen, Ping Luo, Wei Liu, Tao Xiang, Jonas Schult, Yuren Cong</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models (TUNA：驯服统一视觉表示以实现原生统一多模态模型)</p>
<p><strong>作者：</strong> Zhiheng Liu, Weiming Ren, Haozhe Liu, Zijian Zhou, Shoufa Chen, Haonan Qiu, Xiaoke Huang, Zhaochong An, Fanny Yang, Aditya Patel, Viktar Atliha, Tony Ng, Xiao Han, Chuyan Zhu, Chenyang Zhang, Ding Liu, Juan-Manuel Perez-Rua, Sen He, Jürgen Schmidhuber, Wenhu Chen, Ping Luo, Wei Liu, Tao Xiang, Jonas Schult, Yuren Cong</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究目标：</strong></p>
<p>该论文旨在解决当前统一多模态模型（UMMs）在处理图像和视频的理解与生成任务时面临的挑战。现有模型通常采用解耦的视觉表示，这会导致不同编码器之间的格式不匹配，增加模型复杂性并可能影响性能。研究目标是开发一种“原生”的统一多模态模型，能够在一个统一的视觉表示空间内，高效且协同地完成多种多模态任务，包括图像和视频的理解、生成以及图像编辑。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>统一视觉表示：</strong> TUNA 的核心创新在于其“统一连续视觉表示”的设计。它通过级联一个变分自编码器（VAE）编码器和一个表示编码器来构建这一表示。这种统一的空间避免了不同编码器产生的表示格式不匹配问题，使得模型能够端到端地处理图像和视频，用于理解和生成任务。</li>
<li><strong>原生统一多模态模型：</strong> TUNA 被设计为一个“原生”模型，意味着它从头开始联合训练理解和生成目标，而不是将预训练好的独立模型进行组合。</li>
<li><strong>三阶段训练策略：</strong> 论文提出了一种三阶段的训练策略，逐步适应模型组件：<ul>
<li><strong>阶段 1：</strong> 预训练统一表示和流匹配头，冻结 LLM 解码器。</li>
<li><strong>阶段 2：</strong> 全模型继续预训练，引入更多模态和任务。</li>
<li><strong>阶段 3：</strong> 监督微调（SFT），进一步优化模型性能。</li>
</ul>
</li>
<li><strong>结合自回归和流匹配：</strong> TUNA 结合了自回归文本生成和流匹配（flow matching）的视觉生成方法，以实现高效且高质量的图像和视频生成。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>状态艺术（State-of-the-Art）性能：</strong> TUNA 在多个多模态理解和生成基准测试中取得了最先进的性能，包括图像和视频理解、图像和视频生成以及图像编辑。</li>
<li><strong>优于解耦表示：</strong> 实验表明，TUNA 的统一视觉表示在理解和生成任务上均优于采用解耦表示的现有模型，证明了其设计的有效性。</li>
<li><strong>表示编码器重要性：</strong> 研究发现，更强大的预训练表示编码器能够显著提升模型在所有多模态任务上的性能，强调了表示编码器在 TUNA 框架中的关键作用。</li>
<li><strong>理解与生成协同增益：</strong> 在统一的设置下，联合训练理解和生成数据能够使两个任务相互促进，而非相互干扰，这得益于统一视觉表示带来的跨任务依赖性。</li>
<li><strong>效率和可扩展性：</strong> TUNA 的统一设计简化了训练和推理过程，并且在不同模型规模下都展现出良好的性能，证明了其有效性和可扩展性。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>计算成本：</strong> 论文中提到，视频训练的计算成本很高，因此 7B 变体在训练时没有包含视频数据。这暗示了处理大规模视频数据仍然是一个挑战。</li>
<li><strong>模型规模的影响：</strong> 虽然 TUNA 在不同规模下表现良好，但更强大的预训练表示编码器能带来更好的性能，这表明模型性能可能仍然受限于基础模型的表示能力。</li>
<li><strong>与 Show-o2 的比较：</strong> 尽管 TUNA 优于 Show-o2，但 Show-o2 的方法也展示了统一视觉表示的潜力，TUNA 的优势在于其更早期的特征融合和端到端的训练方式。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>进一步提升视频处理能力：</strong> 论文中提到视频训练的成本问题，未来可以探索更高效的视频理解和生成方法。</li>
<li><strong>探索更强大的表示编码器：</strong> 持续研究和开发更强大的预训练表示编码器，以进一步提升 TUNA 的整体性能。</li>
<li><strong>扩展到更多模态：</strong> 将 TUNA 的统一表示框架扩展到音频、3D 数据等更多模态，构建更全面的多模态模型。</li>
<li><strong>更精细的控制和可解释性：</strong> 进一步研究如何对 TUNA 的生成过程进行更精细的控制，并提高模型的可解释性。</li>
<li><strong>更复杂的指令遵循：</strong> 探索 TUNA 在处理更复杂、更具挑战性的指令遵循任务上的能力。</li>
</ul>
<p><strong>总结：</strong></p>
<p>TUNA 论文提出了一种创新的“原生统一多模态模型”，通过级联 VAE 编码器和表示编码器，构建了一个统一的连续视觉表示空间。这一设计有效解决了现有 UMMs 中表示格式不匹配的问题，实现了图像和视频理解、生成以及图像编辑等多种任务的端到端处理。实验结果表明，TUNA 在多项基准测试中取得了最先进的性能，并且优于采用解耦表示的模型。论文强调了强大的预训练表示编码器和联合训练理解与生成任务的重要性，展示了 TUNA 在多模态 AI 领域的重要进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder.</li>
<li>Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02014v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02014v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02013v1'></a></p>
<h2 id="manualvla-a-unified-vla-model-for-chain-of-thought-manual-generation-and-robotic-manipulation"><a href="https://arxiv.org/abs/2512.02013v1">ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Chenyang Gu, Jiaming Liu, Hao Chen, Runzhong Huang, Qingpo Wuwu, Zhuoyang Liu, Xiaoqi Li, Ying Li, Renrui Zhang, Peng Jia, Pheng-Ann Heng, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the "how" process from the "what" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</p>
<p><strong>作者：</strong> Chenyang Gu, Jiaming Liu, Hao Chen, Runzhong Huang, Qingpo Wuwu, Zhuoyang Liu, Xiaoqi Li, Ying Li, Renrui Zhang, Peng Jia, Pheng-Ann Heng, Shanghang Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在解决当前视觉-语言-动作（VLA）模型在处理需要<strong>长时序规划和精确操作</strong>的任务时面临的挑战，特别是当任务目标明确但中间步骤未知时（例如，乐高积木组装或物体重排）。现有VLA模型难以将高层规划与精确的机器人操作协调起来，即难以从“做什么”（What）推断出“怎么做”（How）。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
*   <strong>ManualVLA框架：</strong> 提出了一种名为ManualVLA的统一VLA框架，基于<strong>混合专家（Mixture-of-Transformers, MoT）架构</strong>，实现了多模态手动指令生成和动作执行的协同工作。
*   <strong>规划专家与动作专家：</strong> ManualVLA包含一个<strong>规划专家</strong>，用于生成包含图像、位置提示和文本指令的多模态中间手册（manuals）。然后，通过<strong>Manual Chain-of-Thought (ManualCoT)推理过程</strong>，将这些手册输入到<strong>动作专家</strong>中，其中每一步手册都提供明确的控制条件，同时其潜在表示提供隐式引导，以实现精确操作。
*   <strong>显式与隐式CoT推理：</strong> 引入了<strong>显式CoT</strong>（通过将位置提示作为视觉提示嵌入到动作专家的观察中）和<strong>隐式CoT</strong>（在潜在空间中利用手册作为动作建模的条件信号）相结合的推理机制。
*   <strong>高保真数字孪生工具包：</strong> 为了缓解数据收集的负担，开发了一个基于<strong>3D高斯溅射（3D Gaussian Splatting）</strong>的高保真数字孪生工具包，能够自动生成用于规划专家训练的手册数据。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的性能提升：</strong> 在乐高积木组装和物体重排等长时序、目标导向的任务上，ManualVLA取得了显著的成功率提升，平均成功率比之前的分层SOTA基线高出<strong>32%</strong>。
*   <strong>准确的手册生成：</strong> 规划专家能够生成高质量的中间手册，包括准确的图像、位置和文本描述，这对于后续的精确操作至关重要。
*   <strong>鲁棒的动作执行：</strong> ManualCoT推理过程使得动作专家能够有效地利用手册信息，即使手册存在轻微不准确，也能保持稳定的操作性能。
*   <strong>泛化能力：</strong> 实验证明ManualVLA在目标状态、物体形状、背景和光照条件变化方面表现出良好的泛化能力。
*   <strong>高效的数据利用：</strong> 尽管ManualVLA需要大量数据进行规划专家训练，但通过数字孪生工具包有效解决了数据收集难题。同时，在下游任务微调时，ManualVLA仅需约100个轨迹即可实现可泛化的操作。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>乐高放置错误：</strong> 在乐高组装任务中，ManualVLA偶尔会出现错误的积木放置，尽管系统通常能够从错误中恢复。
*   <strong>大角度旋转下的放置误差：</strong> 在物体重排任务中，当需要进行大角度旋转以实现精确放置时，ManualVLA可能会失败。作者推测这可能是由于训练数据中此类极端旋转案例数量有限所致。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>处理更复杂的旋转场景：</strong> 增加训练数据中包含极端旋转的案例，以提高在物体重排等任务中的鲁棒性。
*   <strong>进一步提升手册的准确性和精细度：</strong> 探索更先进的手册生成技术，以减少中间步骤的误差累积。
*   <strong>扩展到更广泛的任务和环境：</strong> 将ManualVLA的框架和方法应用于更多样化的机器人操作任务和更复杂的真实世界环境。
*   <strong>探索更高效的训练策略：</strong> 研究如何进一步减少对大量预训练数据的依赖，或者开发更高效的迁移学习方法。</p>
<p><strong>论文的重要性：</strong>
该论文的重要贡献在于提出了一种新颖的<strong>ManualVLA框架</strong>，成功地将<strong>长时序规划</strong>与<strong>精确的机器人操作</strong>相结合，解决了现有VLA模型在复杂任务中的瓶颈。通过引入<strong>多模态手册生成</strong>和<strong>ManualCoT推理</strong>机制，ManualVLA能够从“What”推断出“How”，为实现更通用的机器人智能迈出了重要一步。其<strong>数字孪生工具包</strong>的开发也为未来机器人学习研究提供了有价值的数据生成方法。该研究在实际机器人操作任务中取得了显著的性能提升，证明了其在<strong>长时序机器人任务规划和执行</strong>方面的潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution.</li>
<li>To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02013v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02013v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02012v1'></a></p>
<h2 id="improved-mean-flows-on-the-challenges-of-fastforward-generative-models"><a href="https://arxiv.org/abs/2512.02012v1">Improved Mean Flows: On the Challenges of Fastforward Generative Models</a></h2>
<p><strong>Authors:</strong> Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman, J. Zico Kolter, Kaiming He</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity <script type="math/tex">v</script>, re-parameterized by a network that predicts the average velocity <script type="math/tex">u</script>. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our <script type="math/tex">\textbf{improved MeanFlow}</script> (<script type="math/tex">\textbf{iMF}</script>) method, trained entirely from scratch, achieves <script type="math/tex">\textbf{1.72}</script> FID with a single function evaluation (1-NFE) on ImageNet 256<script type="math/tex">\times</script>256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Improved Mean Flows: On the Challenges of Fastforward Generative Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Improved Mean Flows: On the Challenges of Fastforward Generative Models
<strong>作者：</strong> Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman, J. Zico Kolter, Kaiming He</p>
<p><strong>摘要：</strong></p>
<p>这篇论文深入探讨了近期提出的“均值流”（MeanFlow, MF）框架在单步生成模型方面存在的两个关键挑战，并提出了改进方法，即“改进均值流”（Improved MeanFlow, iMF）。</p>
<p><strong>1. 研究问题/研究目标：</strong></p>
<ul>
<li><strong>问题一：训练目标不标准且依赖网络。</strong> 原始MF的训练目标不仅依赖于真实的底层场，还依赖于网络自身的预测，这使得它不是一个标准的回归问题，并可能导致训练不稳定。</li>
<li><strong>问题二：固定引导尺度牺牲灵活性。</strong> 原始MF在训练时固定了分类器无关引导（Classifier-Free Guidance, CFG）的尺度，这限制了在推理时调整引导强度的灵活性，而最优的引导尺度会随模型能力和设置而变化。</li>
</ul>
<p><strong>2. 主要创新和方法贡献：</strong></p>
<ul>
<li><strong>将MF重构为基于瞬时速度 <script type="math/tex">v</script> 的损失函数。</strong> 作者将原始MF的训练目标从依赖于网络预测的平均速度 <script type="math/tex">u</script> 的损失（<script type="math/tex">u</script>-loss）重构为基于瞬时速度 <script type="math/tex">v</script> 的损失（<script type="math/tex">v</script>-loss），但该损失仍然通过网络预测的平均速度 <script type="math/tex">u</script> 来参数化。这种重构使得训练目标不再依赖于网络本身，形成了一个更标准的回归问题，从而提高了训练稳定性。</li>
<li><strong>引入灵活的CFG条件化。</strong> 作者将CFG的引导尺度 <script type="math/tex">w</script> 以及引导区间的参数（<script type="math/tex">t_{min}, t_{max}</script>）视为可学习的条件变量，并使用“上下文内条件化”（in-context conditioning）机制来处理这些多样化的条件。这使得模型在训练和推理时都能灵活地调整CFG尺度，从而获得更好的性能和更强的泛化能力。</li>
<li><strong>改进的上下文内条件化。</strong> 论文提出了一种改进的上下文内条件化方法，通过将不同类型的条件（时间步、类别、CFG参数）转换为多个可学习的token，并与图像的latent token一起输入Transformer，从而更有效地处理异构条件。这种方法还显著减少了模型参数量，因为可以移除参数量大的adaLN-zero层。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>在ImageNet 256x256上的卓越性能。</strong> iMF模型在ImageNet 256x256数据集上，仅用一次函数评估（1-NFE）就达到了 <strong>1.72 FID</strong> 的优异成绩。</li>
<li><strong>大幅超越现有单步生成模型。</strong> iMF在1-NFE生成方面，相比于原始MF和其他同类方法，取得了显著的性能提升，甚至缩小了与多步生成模型之间的差距，而无需使用蒸馏（distillation）。</li>
<li><strong>证明了单步生成模型的潜力。</strong> 论文表明，经过改进的单步生成模型（fastforward generative models）可以作为一种独立的、高性能的生成范式。</li>
<li><strong>灵活CFG的优势。</strong> 通过灵活的CFG条件化，模型能够适应不同的引导尺度，并在推理时实现“无CFG”的效果，显著提高了FID。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对tokenizer的依赖。</strong> 论文中使用的模型仍然依赖于预训练的VAE tokenizer将图像编码为latent space，这在一定程度上增加了推理成本。</li>
<li><strong>CFG尺度的选择。</strong> 虽然模型支持灵活的CFG尺度，但找到最优的引导尺度仍然需要一定的探索。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>去除tokenizer。</strong> 论文提到，随着1-NFE生成技术的进步，tokenizer的成本变得越来越显著，未来的研究可以探索更高效的tokenizer或直接进行像素空间生成。</li>
<li><strong>进一步探索单步生成模型。</strong> 论文的成功为单步生成模型作为一种独立的范式奠定了基础，未来的工作可以继续探索其在其他任务和数据集上的应用。</li>
</ul>
<p>总而言之，这篇论文通过解决原始MeanFlow在训练目标和引导机制上的核心问题，提出了iMF模型，在单步生成领域取得了突破性的进展，为未来高效、高质量的生成模型研究开辟了新方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02012v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02012v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02009v1'></a></p>
<h2 id="airsim360-a-panoramic-simulation-platform-within-drone-view"><a href="https://arxiv.org/abs/2512.02009v1">AirSim360: A Panoramic Simulation Platform within Drone View</a></h2>
<p><strong>Authors:</strong> Xian Ge, Yuling Pan, Yuhang Zhang, Xiang Li, Weijun Zhang, Dizhe Zhang, Zhaoliang Wan, Xin Lin, Xiangkai Zhang, Juntao Liang, Jason Li, Wenjie Jiang, Bo Du, Ming-Hsuan Yang, Lu Qi</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：AirSim360: A Panoramic Simulation Platform within Drone View</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>该论文提出了AirSim360，一个创新的全景式无人机视角仿真平台，旨在解决当前360度全景理解领域大规模、多样化数据稀缺的瓶颈。该平台通过渲染对齐的数据和标注范式、交互式行人感知系统以及自动化轨迹生成，为像素级几何、语义和实体理解以及导航任务提供了强大的支持。通过生成超过60K的全景样本并进行广泛实验，AirSim360首次系统性地在全景设置下对4D真实世界进行了建模。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>AirSim360的核心创新在于其<strong>“渲染对齐的数据和标注范式”</strong>。这表明该平台不仅仅是生成全景图像，而是能够生成与渲染过程紧密结合的、像素级别的精确几何、语义和实体标注。这意味着从仿真中提取的数据具有高度的可用性和准确性，可以直接用于训练和评估各种计算机视觉模型。</p>
<p>此外，以下几点也是重要的创新点：</p>
<ul>
<li><strong>交互式行人感知系统：</strong> 这意味着仿真能够模拟人类的行为，这对于需要理解和预测行人动态的应用（如自动驾驶、城市规划）至关重要。这超越了静态场景的模拟，增加了动态和交互性。</li>
<li><strong>自动化轨迹生成：</strong> 这为无人机导航任务提供了直接的支持，使得研究人员可以方便地生成不同场景下的飞行路径，并评估导航算法的性能。</li>
<li><strong>首次系统性建模4D真实世界下的全景设置：</strong> 这是对现有仿真技术的一个重要突破。以往的仿真可能侧重于2D图像或特定视角的3D，而AirSim360则将全景视角与4D（空间+时间）的真实世界动态结合起来，这在处理时空信息方面具有重要意义。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>AirSim360的出现有望对360度全景理解领域产生深远影响：</p>
<ul>
<li><strong>数据驱动研究的加速：</strong> 解决了数据稀缺问题，将极大地推动全景图像理解、场景重建、目标检测、语义分割、实例分割等任务的研究进展。研究人员无需再花费大量精力收集和标注昂贵的全景数据集。</li>
<li><strong>更鲁棒和泛化的模型：</strong> 通过大规模、多样化的仿真数据，可以训练出更鲁棒、泛化能力更强的模型，能够更好地应对真实世界中各种复杂和未知的场景。</li>
<li><strong>推动新的应用落地：</strong> 仿真平台为开发和测试新的全景应用提供了基础，例如更智能的VR/AR体验、更精确的机器人导航、更全面的城市监控等。</li>
<li><strong>促进跨模态和多任务学习：</strong> 平台提供的多维度标注（几何、语义、实体）为研究跨模态学习和多任务学习提供了丰富的资源。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>自动驾驶：</strong> 全景视角对于理解车辆周围环境至关重要，尤其是在十字路口、环岛等复杂场景。仿真数据可用于训练感知模型，预测行人行为，以及测试导航策略。</li>
<li><strong>机器人导航与感知：</strong> 无人机和地面机器人需要360度的环境感知来安全有效地进行导航和任务执行。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 逼真的全景环境仿真可以用于创建更沉浸式的VR/AR体验，以及开发新的交互方式。</li>
<li><strong>城市规划与监控：</strong> 通过无人机视角的全景数据，可以对城市进行全面的建模和分析，用于交通流量监测、基础设施评估、安全监控等。</li>
<li><strong>3D重建与场景理解：</strong> 平台提供的几何信息有助于开发更精确的3D重建算法，并深入理解场景的结构和内容。</li>
<li><strong>内容创作：</strong> 为全景视频、360度图像的后期制作和特效提供仿真基础。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要中强调了平台的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>仿真与真实世界的差距 (Sim-to-Real Gap)：</strong> 任何仿真平台都存在与真实世界之间的差距。尽管AirSim360力求逼真，但其生成的图像和标注可能无法完全捕捉真实世界中所有细微的物理现象、光照变化、传感器噪声以及复杂的材质表现。这仍然是使用仿真数据训练模型时需要考虑的关键问题。</li>
<li><strong>行人行为建模的复杂度：</strong> 尽管引入了“交互式行人感知系统”，但人类行为的复杂性和不可预测性是极难完全建模的。仿真中的行人行为可能仍然是简化的，无法完全覆盖所有真实情况。</li>
<li><strong>计算资源需求：</strong> 生成大规模、高分辨率的全景图像和详细的标注通常需要大量的计算资源和时间。这可能会限制研究人员在本地进行大规模实验的能力。</li>
<li><strong>特定场景的覆盖范围：</strong> 尽管提到了“wide-ranging scene sampling”，但摘要并未详细说明平台能够覆盖的场景类型和多样性。可能存在某些特定类型的场景（如极端天气、特殊工业环境）在仿真中难以充分体现。</li>
<li><strong>对现有仿真器的依赖或改进：</strong> 摘要提到“Unlike existing simulators”，暗示AirSim360可能基于现有的仿真引擎（如AirSim本身）进行扩展或改进。其性能和功能可能在一定程度上受限于底层引擎的特性。</li>
</ul>
<p>总而言之，AirSim360是一个非常有前景的研究工作，它通过提供一个强大的全景仿真平台，有望解决当前360度全景理解领域面临的关键挑战，并推动相关研究和应用的发展。其“渲染对齐”的数据生成范式和对4D真实世界的系统性建模是其核心亮点。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02009v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02009v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02006v1'></a></p>
<h2 id="mv-tap-tracking-any-point-in-multi-view-videos"><a href="https://arxiv.org/abs/2512.02006v1">MV-TAP: Tracking Any Point in Multi-View Videos</a></h2>
<p><strong>Authors:</strong> Jahyeok Koo, Inès Hyeonsu Kim, Mungyeom Kim, Junghyun Park, Seohyun Park, Jaeyeong Kim, Jung Yi, Seokju Cho, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“MV-TAP: Tracking Any Point in Multi-View Videos”论文的全面中文摘要，重点突出了其研究问题、方法创新、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> MV-TAP: Tracking Any Point in Multi-View Videos</p>
<p><strong>作者：</strong> Jahyeok Koo, Inès Hyeonsu Kim, Mungyeom Kim, Junghyun Park, Seohyeon Park, Jaeyeong Kim, Jung Yi, Seokju Cho, Seungryong Kim</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决<strong>多视角视频中的任意点跟踪（Multi-view Point Tracking）</strong>这一新兴且重要的任务。在复杂的真实世界场景中，多视角摄像系统提供了丰富的观测信息，理解动态物体在多视角下的运动至关重要，这在动作捕捉、机器人操作和自动驾驶等领域有着广泛应用。然而，现有的点跟踪方法主要集中在单视角视频，它们在处理遮挡、深度不确定性以及运动模糊等问题时存在固有的局限性。直接将单视角方法独立应用于每个视角，无法充分利用多视角信息来解决这些模糊性，也无法构建可靠的多视角点轨迹。因此，论文的核心研究问题是如何<strong>有效地利用多视角信息来提升点跟踪的鲁棒性和准确性</strong>，尤其是在像素空间中进行跟踪。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
MV-TAP 的核心创新在于其提出的<strong>多视角点跟踪框架</strong>，该框架能够有效整合来自多个视角的信息，以实现更完整和可靠的轨迹估计。其关键贡献包括：</p>
<ul>
<li><strong>定义了多视角点跟踪任务：</strong> 首次明确提出了在像素空间中进行多视角点跟踪的任务，旨在建立鲁棒的时空对应关系。</li>
<li><strong>MV-TAP 模型架构：</strong><ul>
<li><strong>相机几何与跨视角注意力机制：</strong> 模型利用相机几何信息（通过 Plücker 坐标编码）和创新的<strong>跨视角注意力模块</strong>来聚合来自不同视角和时间步的信息。这使得模型能够理解点在不同视角下的相对几何关系。</li>
<li><strong>多视角时空 Transformer：</strong> 采用 Transformer 架构，集成了<strong>时间注意力、空间注意力和视角注意力</strong>，以协同处理时空信息，并有效利用跨视角线索。</li>
<li><strong>相机编码模块：</strong> 将相机参数编码为嵌入向量，注入模型，使模型能够显式地感知多视角相机几何信息，从而捕捉跨视角的空间对应关系。</li>
<li><strong>局部 4D 相关性：</strong> 借鉴了单视角跟踪方法，利用局部 4D 相关性来捕捉时域上的外观线索。</li>
</ul>
</li>
<li><strong>大规模合成数据集与评估基准：</strong> 为了支持该任务的研究，论文构建了一个<strong>大规模的合成训练数据集</strong>，包含同步的多视角视频、点轨迹和相机参数。同时，还提出了一个<strong>真实世界评估集</strong>，专门用于多视角跟踪任务的评估。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在多个具有挑战性的多视角基准数据集（如 DexYCB, Panoptic Studio, Kubric, Harmony4D）上的广泛实验，MV-TAP 取得了显著的成果：</p>
<ul>
<li><strong>性能超越现有方法：</strong> MV-TAP 在各项评估指标（如 &lt; δavg, OA, AJ）上均显著优于现有的单视角和多视角点跟踪方法，证明了其有效利用多视角信息的能力。</li>
<li><strong>鲁棒性提升：</strong> 模型在处理遮挡、大运动和非刚性运动等复杂场景时表现出更强的鲁棒性，能够保持一致的轨迹。</li>
<li><strong>有效性验证：</strong> 消融实验表明，视角注意力和相机编码模块对提升模型性能至关重要，它们共同作用，显著增强了模型的多视角感知能力。</li>
<li><strong>奠定研究基础：</strong> MV-TAP 的成功不仅展示了其在多视角点跟踪任务上的优越性，更重要的是，它为该新兴领域提供了一个<strong>强大的基线模型和一套完整的解决方案</strong>（包括数据集和评估方法），极大地推动了该领域的研究进展。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong>
论文中也指出了 MV-TAP 的一些局限性：</p>
<ul>
<li><strong>查询点假设：</strong> MV-TAP 假设查询点在所有视角下都提供。然而，在实际应用中，这种假设往往不切实际，因为在某些视角下可能无法获得查询点。</li>
<li><strong>单视角查询初始化：</strong> 论文的查询初始化实验表明，仅从单视角查询点出发，在其他视角找到可靠的对应关系仍然具有挑战性，这限制了模型的广泛适用性。</li>
</ul>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>多视角查询初始化：</strong> 开发更先进的策略来处理单视角查询点，实现更可靠的多视角点跟踪。</li>
<li><strong>更广泛的适用性：</strong> 探索 MV-TAP 在更广泛场景下的应用，例如处理更具挑战性的动态场景和更复杂的物体交互。</li>
<li><strong>提升效率：</strong> 尽管 MV-TAP 已经取得了很好的性能，但进一步优化模型的计算效率，使其能够应用于实时场景，也是一个重要的研究方向。</li>
</ul>
<p>总而言之，这篇论文成功地定义并解决了多视角点跟踪这一重要且具有挑战性的问题，通过引入创新的模型架构和数据集，显著提升了点跟踪的性能和鲁棒性，并为该领域未来的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information.</li>
<li>Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02006v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02006v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.02005v1'></a></p>
<h2 id="learning-visual-affordance-from-audio"><a href="https://arxiv.org/abs/2512.02005v1">Learning Visual Affordance from Audio</a></h2>
<p><strong>Authors:</strong> Lidong Lu, Guo Chen, Zhu Wei, Yicheng Liu, Tong Lu</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Learning Visual Affordance from Audio”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Learning Visual Affordance from Audio</p>
<p><strong>作者：</strong> Lidong Lu, Guo Chen, Zhu Wei, Yicheng Liu, Tong Lu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题与动机：</strong></p>
<p>该论文旨在解决<strong>视觉感知中的交互区域识别（Affordance Grounding）</strong>问题，特别是如何利用<strong>声音信息</strong>来更直观、更鲁棒地识别物体可交互的区域。现有方法主要依赖文本指令或演示视频，但这些方法常受限于歧义、遮挡或需要精确的指令，难以在复杂或动态场景下有效工作。论文提出的核心问题是：<strong>声音能否揭示物体交互的区域？</strong></p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li><strong>提出新的任务：Audio-Visual Affordance Grounding (AV-AG)</strong>。该任务专注于从<strong>动作声音</strong>中分割出物体可交互的区域，强调声音作为一种实时、语义丰富且视觉独立的线索。</li>
<li><strong>构建首个 AV-AG 数据集 (AVAGD)</strong>。该数据集包含大量动作声音、物体图像以及像素级的交互区域（功能区域和依赖区域）标注。其特点是数据规模大、类别丰富（97个物体类别，55个交互类别），覆盖多种领域，并且包含一个用于评估零样本泛化的“未见”子集。</li>
<li><strong>提出 AVAGFormer 模型</strong>。该模型是针对 AV-AG 任务设计的端到端模型，其核心创新包括：<ul>
<li><strong>语义条件化跨模态混合器 (Semantic-Conditioned Cross-Modal Mixer)</strong>：能够将文本语义（如“功能”或“依赖”）注入到跨模态融合过程中，引导模型更准确地对齐音频和视觉信息，生成功能和依赖区域的特定表示。</li>
<li><strong>双头交互式解码器 (Dual-Head Affordance Decoder)</strong>：采用两个并行的解码头，一个用于预测功能区域掩码，另一个利用功能区域的预测结果来指导依赖区域的预测，实现隐式跨任务协同，提升整体性能和泛化能力。</li>
</ul>
</li>
<li><strong>详细的消融研究和分析</strong>：论文深入分析了模型各组件（如跨模态混合器、双头解码器、辅助损失等）的贡献，以及 AV-AG 任务与现有音频视觉分割 (AVS) 任务的区别。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>AVAGFormer 在 AV-AG 任务上取得了最先进 (State-of-the-Art) 的性能</strong>，显著优于从相关任务（如 AVS）迁移过来的基线模型。</li>
<li>在零样本泛化评估中，AVAGFormer 也表现出强大的泛化能力，表明其学习到的跨模态表示能够有效应对未见过的数据。</li>
<li>研究结果证明了声音信息在视觉交互区域识别中的重要价值，尤其是在视觉信息不完整或存在遮挡的情况下。</li>
<li>AVAGD 数据集的发布为未来在音频视觉理解、多模态对齐和具身智能等领域的研究提供了重要的基准和资源。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li>论文提到，现有 AVS 模型在 AV-AG 任务上表现不佳，表明从对象级 AVS 到部分级 AV-AG 的任务转变带来了显著的挑战，需要更强的语义理解能力。</li>
<li>对于依赖区域的预测，由于并非所有类别都有明确的依赖区域，这给模型训练带来了一定的不平衡性。</li>
<li>虽然 AVAGFormer 取得了优异的性能，但论文也指出，在复杂场景下，模型仍可能存在误识别，尤其是在处理多个声音源时。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>扩展 AVAGD 数据集以支持动态和多步交互</strong>：未来的工作可以进一步丰富数据集，包含更复杂的、需要一系列动作才能完成的交互场景，以更好地模拟真实世界的具身智能任务。</li>
<li><strong>探索更精细的跨模态融合机制</strong>：进一步研究如何更有效地融合音频和视觉信息，以应对更具挑战性的场景和更细粒度的交互理解。</li>
<li><strong>提升模型在复杂环境下的鲁棒性</strong>：例如，在存在背景噪声或多个声音源干扰的情况下，如何保持准确的交互区域识别。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文成功地提出了一个全新的<strong>音频视觉交互区域识别 (AV-AG)</strong> 任务，并为此构建了一个大规模、高质量的数据集 (AVAGD)。其核心贡献在于提出了<strong>AVAGFormer 模型</strong>，该模型通过创新的<strong>语义条件化跨模态混合器</strong>和<strong>双头交互式解码器</strong>，有效融合了音频和视觉信息，实现了在 AV-AG 任务上的最先进性能。这项工作不仅为多模态理解和具身智能领域开辟了新的研究方向，也为未来更智能的机器人交互提供了重要的技术基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds.</li>
<li>Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction.</li>
<li>Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks.</li>
<li>Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02005v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02005v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.01993v1'></a></p>
<h2 id="road-rollouts-as-demonstrations-for-closed-loop-supervised-fine-tuning-of-autonomous-driving-policies"><a href="https://arxiv.org/abs/2512.01993v1">RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies</a></h2>
<p><strong>Authors:</strong> Guillermo Garcia-Cobo, Maximilian Igl, Peter Karkus, Zhejun Zhang, Michael Watson, Yuxiao Chen, Boris Ivanovic, Marco Pavone</p>
<p><strong>Published:</strong> 2025-12-01</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\% and reduces collisions by 54\%.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies”的全面中文摘要：</p>
<p><strong>论文题目：</strong> RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies (RoaD：用于闭环监督精调自动驾驶策略的轨迹作为演示)</p>
<p><strong>作者：</strong> Guillermo Garcia-Cobo, Maximilian Igl, Peter Karkus, Zhejun Zhang, Michael Watson, Yuxiao Chen, Boris Ivanovic, Marco Pavone</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>自动驾驶策略通常采用<strong>开环行为克隆 (Behavior Cloning, BC)</strong> 的方式，基于人类演示数据进行训练。然而，当这些策略在<strong>闭环</strong>（即策略的输出会影响其未来的输入）环境中部署时，会遇到<strong>协变量偏移 (covariate shift)</strong> 的问题。这种偏移导致策略的预测与实际环境不匹配，从而引发<strong>累积误差</strong>，降低其鲁棒性，尤其是在长尾场景和交互式场景中。现有的闭环监督精调 (Closed-Loop Supervised Fine-Tuning, CL-SFT) 方法虽然有所改进，但通常依赖于<strong>离散动作空间</strong>、<strong>确定性动力学</strong>以及<strong>对专家演示的严格假设</strong>，限制了其在现代端到端 (End-to-End, E2E) 驾驶策略上的应用。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>本文提出了一种名为 <strong>RoaD (Rollouts as Demonstrations)</strong> 的新颖、简单且高效的闭环监督精调方法，旨在解决上述问题。RoaD的核心贡献在于：</p>
<ul>
<li><strong>利用策略自身的闭环轨迹作为额外训练数据：</strong> RoaD的核心思想是生成策略在闭环环境下的轨迹（称为“rollouts”），并将这些轨迹作为额外的监督信号来精调策略。这直接解决了开环训练与闭环部署之间的协变量偏移问题。</li>
<li><strong>引入专家指导的轨迹生成：</strong> 在生成闭环轨迹时，RoaD会引入<strong>专家指导</strong>，使生成的轨迹更偏向于高质量的行为。这确保了生成的演示数据既具有信息量，又贴近策略实际可能遇到的情况。</li>
<li><strong>移除对离散动作和确定性动力学的假设：</strong> 与先前工作（如CAT-K）不同，RoaD不要求动作空间是离散的，也不依赖于精确的逆动力学模型。它通过<strong>采样K个动作候选</strong>（Sample-K）并选择最接近专家轨迹的动作来指导轨迹生成，使其能够适用于更广泛的现代E2E驾驶策略（如基于Transformer或扩散模型的策略）。</li>
<li><strong>轻量级恢复模式 (Recovery Mode)：</strong> 为了应对策略在某些情况下可能产生与专家轨迹相距甚远的动作，RoaD引入了一个可选的<strong>恢复模式</strong>。当检测到策略输出的动作偏离专家轨迹过远时，该模式会轻微地将策略的输出引导回专家轨迹，以确保轨迹的质量。</li>
<li><strong>数据效率和可复用性：</strong> RoaD通过重用收集到的闭环轨迹数据集进行多次优化，显著提高了数据效率，这对于高成本的模拟器（如需要渲染传感器输入的E2E模拟器）尤为重要。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>交通仿真 (WOSAC)：</strong> RoaD在WOSAC基准测试中表现出色，与现有的SOTA CL-SFT方法CAT-K相当，甚至在某些指标上有所超越。更重要的是，RoaD的成功表明其方法论可以应用于更广泛的场景，而不仅仅是交通仿真。</li>
<li><strong>端到端驾驶 (AlpaSim)：</strong> 在高保真度的端到端驾驶模拟器AlpaSim中，RoaD取得了显著的性能提升。与基线模型相比，RoaD精调后的策略<strong>驾驶得分提高了41%</strong>，<strong>碰撞率降低了54%</strong>。这证明了RoaD在处理复杂、高保真度的E2E驾驶任务中的有效性。</li>
<li><strong>数据效率：</strong> RoaD即使在仅收集一次闭环数据的情况下也能带来显著的性能提升，这对于高成本的模拟环境来说是一个重要的优势。</li>
<li><strong>通用性：</strong> RoaD的创新使其能够应用于现代E2E驾驶策略，克服了先前CL-SFT方法在动作空间和动力学假设上的限制。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对预训练策略的依赖：</strong> RoaD的有效性在一定程度上依赖于预训练策略本身具有较高的性能。</li>
<li><strong>专家轨迹的假设：</strong> 方法假设专家轨迹代表了良好的行为，并且在策略的轻微偏差下仍然是可行的。</li>
<li><strong>距离度量：</strong> 专家指导的轨迹生成依赖于一个合适的距离度量来衡量策略输出与专家轨迹的接近程度。</li>
<li><strong>模拟器依赖：</strong> 该方法在AlpaSim等高保真度模拟器上进行了验证，其在真实世界中的表现仍需进一步验证。</li>
<li><strong>Sim2Real差距：</strong> 论文提到，模拟环境与真实世界之间可能存在的差距（Sim2Real gap）是所有基于模拟的训练方法面临的挑战。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>显式解决Sim2Real差距：</strong> 未来工作可以更明确地解决模拟到现实的迁移问题，例如通过联合训练模拟和真实数据，或者引入特征相似性瓶颈。</li>
<li><strong>多智能体交互：</strong> 虽然在交通仿真中考虑了多智能体，但进一步探索RoaD在更复杂的、动态交互式多智能体场景中的应用。</li>
<li><strong>更广泛的策略类型：</strong> 探索RoaD在更多新兴的E2E策略类型上的适用性。</li>
<li><strong>自动化超参数调优：</strong> 尽管RoaD对超参数不敏感，但进一步研究自动化超参数调优策略可以简化其实际应用。</li>
</ul>
<p><strong>总结：</strong></p>
<p>RoaD方法通过巧妙地利用策略自身的闭环轨迹并结合专家指导，成功地解决了自动驾驶策略在闭环部署中面临的协变量偏移问题。它克服了先前CL-SFT方法的局限性，显著提升了策略在交通仿真和端到端驾驶任务中的性能，同时保持了数据效率和通用性。这项工作为开发更鲁棒、更高效的自动驾驶策略提供了一种有前景的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data.</li>
<li>We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\% and reduces collisions by 54\%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.01993v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.01993v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-02 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
