<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-25 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-13
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-24/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-26/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-25">Arxiv Computer Vision Papers - 2025-11-25</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#vdc-agent-when-video-detailed-captioners-evolve-themselves-via-agentic-self-reflection" class="nav-link">VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection</a>
                </li>
                <li class="nav-item">
                    <a href="#are-image-to-video-models-good-zero-shot-image-editors" class="nav-link">Are Image-to-Video Models Good Zero-Shot Image Editors?</a>
                </li>
                <li class="nav-item">
                    <a href="#breaking-the-likelihood-quality-trade-off-in-diffusion-models-by-merging-pretrained-experts" class="nav-link">Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts</a>
                </li>
                <li class="nav-item">
                    <a href="#cloud4d" class="nav-link">Cloud4D</a>
                </li>
                <li class="nav-item">
                    <a href="#cook-and-clean-together-teaching-embodied-agents-for-parallel-task-execution" class="nav-link">Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</a>
                </li>
                <li class="nav-item">
                    <a href="#flow-map-distillation-without-data" class="nav-link">Flow Map Distillation Without Data</a>
                </li>
                <li class="nav-item">
                    <a href="#ref-sam3d-bridging-sam3d-with-text-for-reference-3d-reconstruction" class="nav-link">Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#sam3-adapter-efficient-adaptation-of-segment-anything-3-for-camouflage-object-segmentation-shadow-detection-and-medical-image-segmentation" class="nav-link">SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#chain-of-visual-thought-teaching-vlms-to-see-and-think-better-with-continuous-visual-tokens" class="nav-link">Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</a>
                </li>
                <li class="nav-item">
                    <a href="#unigame-turning-a-unified-multimodal-model-into-its-own-adversary" class="nav-link">UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-25">Arxiv Computer Vision Papers - 2025-11-25</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域论文的每日报告执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告执行摘要 (2025-11-24)</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的快速进展：</p>
<ul>
<li><strong>多模态理解与生成：</strong> 论文普遍关注如何更好地融合视觉信息与其他模态（如文本），以及如何利用多模态模型进行更精细的图像编辑和视频生成。</li>
<li><strong>具身智能与交互：</strong> 具身智能代理在复杂任务执行、协作以及与环境的交互能力方面取得了显著进步。</li>
<li><strong>模型效率与泛化能力：</strong> 研究人员致力于提升现有模型的效率，例如通过知识蒸馏、专家合并等技术，同时探索模型在零样本（zero-shot）和少样本（few-shot）场景下的泛化能力。</li>
<li><strong>3D 视觉与重建：</strong> 3D 重建和理解仍然是热门领域，特别是如何将现有的 2D 模型能力迁移到 3D 空间。</li>
<li><strong>模型自适应与鲁棒性：</strong> 提高模型在特定任务或对抗性场景下的适应性和鲁棒性是重要的研究方向。</li>
</ul>
<p><strong>特别显著或创新的论文：</strong></p>
<ul>
<li><strong>"VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection"</strong>: 这篇论文引入了“代理式自我反思”的概念，让视频详细描述生成器能够自我改进，这在提升生成质量和自主性方面具有重要意义。</li>
<li><strong>"Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts"</strong>: 通过合并预训练专家模型来打破扩散模型中常见的“似然-质量”权衡，这可能为生成模型带来更高的质量和更强的可控性。</li>
<li><strong>"Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens"</strong>: 提出了一种新的视觉思考链方法，通过连续视觉令牌来增强视觉语言模型（VLMs）的“看”和“思考”能力，是提升 VLM 性能的潜在突破。</li>
<li><strong>"UniGame: Turning a Unified Multimodal Model Into Its Own Adversary"</strong>: 通过将统一的多模态模型转化为自身的“对手”，这是一种新颖的对抗性训练方法，旨在提高模型的鲁棒性和泛化能力。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>代理式自我学习与反思：</strong> 类似于 VDC-Agent 的方法，让模型具备自我评估和改进的能力，是未来自主 AI 的重要方向。</li>
<li><strong>连续视觉令牌：</strong> Chain-of-Visual-Thought 提出的概念，将视觉信息以更连续、更具结构化的方式输入 VLM，可能改变 VLM 的内部表征方式。</li>
<li><strong>模型“自对抗”训练：</strong> UniGame 的思路，利用模型自身来生成对抗样本，是一种更高效、更具针对性的鲁棒性提升方法。</li>
<li><strong>3D 视觉与文本的深度融合：</strong> Ref-SAM3D 展示了将强大的 2D 分割模型（如 SAM）能力扩展到 3D 重建，并结合文本指令，是迈向更智能 3D 应用的关键一步。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在影响和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>"VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection"</strong>: 对于关注视频理解、生成以及模型自主学习的研究者。</li>
<li><strong>"Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts"</strong>: 对于研究扩散模型、生成模型质量提升以及模型融合技术的开发者。</li>
<li><strong>"Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens"</strong>: 对于研究视觉语言模型、多模态理解以及提升模型推理能力的研究者。</li>
<li><strong>"Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction"</strong>: 对于从事 3D 视觉、三维重建以及将 2D 模型能力迁移到 3D 领域的研究者。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速了解本期 Arxiv 论文的亮点和趋势，以便您能更有效地分配阅读时间。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.19436v1">VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection</a></li>
<li><a href="#2511.19435v1">Are Image-to-Video Models Good Zero-Shot Image Editors?</a></li>
<li><a href="#2511.19434v1">Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts</a></li>
<li><a href="#2511.19431v1">Cloud4D</a></li>
<li><a href="#2511.19430v1">Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</a></li>
<li><a href="#2511.19428v1">Flow Map Distillation Without Data</a></li>
<li><a href="#2511.19426v1">Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction</a></li>
<li><a href="#2511.19425v1">SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation</a></li>
<li><a href="#2511.19418v1">Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</a></li>
<li><a href="#2511.19413v1">UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.19436v1'></a></p>
<h2 id="vdc-agent-when-video-detailed-captioners-evolve-themselves-via-agentic-self-reflection"><a href="https://arxiv.org/abs/2511.19436v1">VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection</a></h2>
<p><strong>Authors:</strong> Qiang Wang, Xinyuan Gao, SongLin Dong, Jizhou Han, Jiangyang Li, Yuhang He, Yihong Gong</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection (VDC-Agent：视频详细描述生成器如何通过代理式自我反思实现自我进化)</p>
<p><strong>作者：</strong> Qiang Wang, Xinyuan Gao, SongLin Dong, Jizhou Han, Jiangyang Li, Yuhang He, Yihong Gong</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>视频详细描述生成（Video Detailed Captioning, VDC）任务旨在生成精细且全面的视频描述，以捕捉视频中的对象、动作、交互和场景转换。现有方法通常依赖于大量人工标注数据或更强大的教师模型（如GPT-4V或大型闭源模型）来进行微调，这带来了高昂的成本、访问限制和计算资源需求。因此，如何使视频描述生成模型在无需人工标注或更强教师模型的情况下，实现自主的自我改进和迭代优化，是本文要解决的核心问题。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>本文提出了 <strong>VDC-Agent</strong>，一个<strong>自进化的视频详细描述生成框架</strong>。其核心创新在于：</p>
<ul>
<li><strong>代理式自我反思（Agentic Self-Reflection）：</strong> VDC-Agent 将 MLLM（多模态大语言模型）本身视为一个自主代理，通过一个<strong>闭环系统</strong>来实现自我进化。该系统包含三个主要阶段：<ul>
<li><strong>生成（Generation）：</strong> 使用初始提示生成候选描述。</li>
<li><strong>评估（Scoring）：</strong> 基于预设的原则（如覆盖对象、动作、时间动态等），由 MLLM 为生成的描述打分，并提供改进提示。</li>
<li><strong>提示优化（Prompt Refinement）：</strong> 根据评估结果，迭代地优化输入给 MLLM 的提示，以生成更高质量的描述。</li>
</ul>
</li>
<li><strong>自我反思机制：</strong> 当新生成的描述质量比前一次更差时，VDC-Agent 会触发一个<strong>自我反思路径</strong>。该路径会回顾之前的“思维链”（chain-of-thought），诊断上一次提示优化失败的原因，并提出更可靠的更新方案，从而避免重复错误。</li>
<li><strong>无需人工标注和教师模型：</strong> VDC-Agent 的整个框架在<strong>无标签视频</strong>上运行，并且不依赖于更强大的外部教师模型，而是利用 MLLM 自身的评估和反思能力。</li>
<li><strong>VDC-Agent-19K 数据集构建：</strong> 通过在大量无标签视频上运行 VDC-Agent，生成了大量的（描述，分数）轨迹。这些轨迹被转化为偏好对（preference tuples），并经过过滤（去除 JSON 解析错误等），最终构建了一个包含 <strong>18,886 个自动生成偏好对</strong>的数据集，命名为 VDC-Agent-19K。</li>
<li><strong>课程式直接偏好优化（Curriculum DPO）：</strong> 为了更有效地利用 VDC-Agent-19K 数据集，论文引入了一种<strong>课程式 DPO</strong> 方法。该方法利用描述对之间的<strong>分数差距（∆Score）</strong>作为难度信号，优先学习分数差距大的样本（易于学习），然后逐渐引入分数差距小的样本（精细调整），实现“由易到难”的偏好对齐。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>SOTA 性能：</strong> 基于 Qwen2.5-VL-7B-Instruct 模型微调的 VDC-Agent-7B 在 VDC 基准测试中取得了<strong>新的 SOTA 性能</strong>，平均准确率达到 <strong>49.08%</strong>，平均分数达到 <strong>2.50</strong>。</li>
<li><strong>超越现有方法：</strong> VDC-Agent-7B 的性能超越了许多专门的视频描述生成模型，并且相比其基线模型 Qwen2.5-VL-7B-Instruct，在<strong>相似的推理成本下</strong>，准确率提升了 <strong>+5.13%</strong>，分数提升了 <strong>+0.27</strong>。</li>
<li><strong>维度分析：</strong> 在 VDC 的五个维度（相机、短描述、背景、主要对象、详细描述）上，VDC-Agent-7B 均取得了显著提升，尤其在相机、背景、主要对象和详细描述等维度上表现突出，表明其对视频的空间布局、实体和时间事件的理解能力更强。</li>
<li><strong>鲁棒性：</strong> 消融实验表明，VDC-Agent 的性能对输入的<strong>原则集（principle sets）</strong>具有很强的鲁棒性，改进主要来源于代理式自我反思机制本身，而非特定的原则措辞。</li>
<li><strong>效率与质量的权衡：</strong> 论文还探讨了最大迭代次数 T 对性能和生成时间的影响，表明存在一个计算-质量的权衡，T=4 是一个合理的默认值。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>推理时间：</strong> 虽然 VDC-Agent 在训练时实现了自我反思，但其在生成数据集阶段的迭代过程仍然需要一定的推理时间。虽然论文声称在训练后，推理成本与基线模型相似，但生成训练数据的过程本身是耗时的。</li>
<li><strong>原则的必要性：</strong> 虽然原则集对性能有积极影响，但其设计仍然需要一定的领域知识来定义“好描述”的标准。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>扩展到更大的骨干模型：</strong> 将 VDC-Agent 框架应用于更大的 MLLM 模型（如 Qwen-32B），以进一步探索性能上限。</li>
<li><strong>更广泛的视频理解任务：</strong> 将该方法扩展到其他视频理解任务，如视频问答（Video Question Answering, VQA），以验证其通用性。</li>
<li><strong>探索性能天花板和可扩展性：</strong> 进一步研究该代理式框架在不同规模和复杂度的视频数据上的性能表现和可扩展性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>VDC-Agent 提出了一种新颖的、无需人工标注和教师模型的视频详细描述生成框架。通过让 MLLM 进行代理式自我反思，该框架能够自主地生成、评估和优化视频描述，并构建高质量的偏好数据集。结合课程式 DPO 训练，VDC-Agent-7B 显著提升了视频描述的质量，并在 VDC 基准测试中取得了 SOTA 性能，为视频理解领域提供了一种高效且可扩展的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models.</li>
<li>Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19436v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19436v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19435v1'></a></p>
<h2 id="are-image-to-video-models-good-zero-shot-image-editors"><a href="https://arxiv.org/abs/2511.19435v1">Are Image-to-Video Models Good Zero-Shot Image Editors?</a></h2>
<p><strong>Authors:</strong> Zechuan Zhang, Zhenyuan Chen, Zongxin Yang, Yi Yang</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Are Image-to-Video Models Good Zero-Shot Image Editors?”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Are Image-to-Video Models Good Zero-Shot Image Editors? (图像到视频模型能否成为优秀的零样本图像编辑器？)</p>
<p><strong>作者：</strong> Zechuan Zhang, Zhenyuan Chen, Zongxin Yang, Yi Yang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题与背景：</strong>
大型视频扩散模型在模拟世界和进行时间推理方面展现出强大的能力，但它们作为零样本图像编辑器的潜力尚未得到充分探索。现有的图像编辑方法通常依赖于单帧模型，在处理需要时间连贯性、物理一致性或因果推理的复杂编辑任务时存在局限性。本文旨在探究如何有效地利用预训练的图像到视频扩散模型，在无需额外微调的情况下，实现指令驱动的零样本图像编辑。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
作者提出了 <strong>IF-Edit</strong> 框架，这是一个无需微调的解决方案，旨在解决利用图像到视频扩散模型进行零样本图像编辑时遇到的三个核心挑战：
*   <strong>提示不匹配 (Prompt Misalignment)：</strong> 引入 <strong>Chain-of-Thought Prompt Enhancement (CoT)</strong> 模块。该模块利用视觉语言模型（VLM）来联合理解输入图像和文本指令，将其转化为更具时间连贯性和推理能力的提示，从而更好地匹配视频模型的内在世界模拟先验。
*   <strong>冗余的时间潜在表示 (Redundant Temporal Latents)：</strong> 提出 <strong>Temporal Latent Dropout (TLD)</strong> 策略。该策略在早期去噪阶段（专家切换点之后）压缩帧潜在表示，通过稀疏化时间潜在表示来加速去噪过程，同时保留关键帧以维持全局语义和时间连贯性，从而显著减少计算量。
*   <strong>后期帧模糊 (Blurry Late-Stage Frames)：</strong> 设计 <strong>Self-Consistent Post-Refinement (SCPR)</strong> 步骤。该步骤通过计算拉普拉斯分数选择最清晰的后期帧，并利用该帧进行短暂的“静止视频”轨迹精炼，以增强细节和清晰度，实现自我一致的后处理增强。</p>
<p><strong>3. 主要结果与意义：</strong>
IF-Edit 在四个公开基准测试（涵盖非刚性变形、物理和时间推理以及通用指令编辑）上进行了评估。实验结果表明：
*   IF-Edit 在<strong>非刚性变形</strong>和<strong>推理类任务</strong>（如时间、因果推理）上表现出色，显著优于许多现有方法，这得益于视频模型固有的时间连贯性和物理模拟能力。
*   在<strong>通用指令编辑</strong>任务上，IF-Edit 表现出与强大的开源系统相当的竞争力。
*   该研究系统地评估了现成的图像到视频模型作为图像编辑器的能力，揭示了它们在时间连贯性和物理现实主义方面的独特优势，并提出了一种简单有效的统一视频-图像生成推理方法。</p>
<p><strong>4. 论文提及的局限性：</strong>
*   <strong>有限的通用指令编辑能力：</strong> 在没有任务特定微调的情况下，IF-Edit 在基于区域的或高度抽象的编辑任务上表现不如专门的图像编辑器。视频扩散模型倾向于模拟物理上可行的、时间上平滑的变换，对于不符合这些特性的插入或替换操作（如图9所示）可能存在困难。
*   <strong>高 GPU 内存需求：</strong> 尽管 TLD 进行了优化，但视频骨干模型由于需要处理多帧，仍然需要较高的 GPU 内存（&gt; 40 GB）。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   通过<strong>微调</strong>或<strong>引导式区域控制</strong>来改进其在区域特定编辑任务上的表现。
*   探索<strong>模型压缩、量化或剪枝</strong>技术以降低 GPU 内存需求，尽管这可能会牺牲一些速度。
*   利用视频模型强大的世界模拟先验，为<strong>蒸馏视频先验到紧凑的图像推理模型</strong>或训练<strong>多模态大型语言模型 (MLLMs)</strong> 和<strong>世界模型</strong>提供新的思路。
*   进一步探索视频模型在<strong>统一图像编辑和视觉推理</strong>方面的潜力。</p>
<p>总而言之，IF-Edit 框架通过巧妙地 repurpose 图像到视频扩散模型，有效地解决了零样本图像编辑中的关键挑战，展示了视频模型在处理需要时间连贯性和物理一致性的复杂编辑任务上的巨大潜力，并为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19435v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19435v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19434v1'></a></p>
<h2 id="breaking-the-likelihood-quality-trade-off-in-diffusion-models-by-merging-pretrained-experts"><a href="https://arxiv.org/abs/2511.19434v1">Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts</a></h2>
<p><strong>Authors:</strong> Yasin Esfandiari, Stefan Bauer, Sebastian U. Stich, Andrea Dittadi</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV, cs.LG, stat.ML</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts
<strong>作者：</strong> Yasin Esfandiari, Stefan Bauer, Sebastian U. Stich, Andrea Dittadi</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
本文旨在解决图像生成扩散模型中普遍存在的“似然-质量权衡”（likelihood-quality trade-off）问题。具体来说，以最大化数据似然为目标的训练方法往往在低噪声阶段过度优化，导致图像细节不佳，视觉保真度受损；而以提升感知样本质量为目标的训练方法，则倾向于在较高噪声阶段进行优化，从而牺牲了模型的整体数据似然性。这种权衡使得模型难以同时在生成图像的真实感和统计准确性上都达到最优。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
作者提出了一种简单且即插即用的采样方法，通过<strong>合并两个预训练的扩散模型专家</strong>来打破这一权衡。核心思想是利用<strong>在去噪轨迹中切换不同专家</strong>。具体而言：
*   <strong>高噪声阶段使用图像质量专家：</strong> 利用一个在较高噪声水平下表现出色的专家（如EDM模型）来塑造图像的全局结构和整体外观，以提升感知质量。
*   <strong>低噪声阶段使用似然专家：</strong> 在去噪过程的后期，切换到一个在低噪声水平下表现出色的专家（如VDM模型）来优化像素统计信息，从而提高数据似然性。
该方法<strong>无需重新训练或微调</strong>，仅需选择一个中间的切换点（时间步）。作者通过将扩散模型的目标函数重构为与信噪比（SNR）相关的形式，为不同噪声调度下的专家模型提供了统一的对齐框架，并提出了一个<strong>时间重映射（time remapping）</strong>的机制来适应不同专家的噪声调度。</p>
<p><strong>3. 主要结果与意义：</strong>
在CIFAR-10和ImageNet32数据集上的实验结果表明，该合并模型在<strong>保持或超越其基础组件性能</strong>的同时，<strong>显著改善了数据似然性和感知样本质量</strong>。通过调整切换点（η），研究人员能够灵活地在似然性和FID（Fréchet Inception Distance）之间进行权衡，并找到一个最优的折衷点，该点性能优于单独的EDM或VDM模型。这有力地证明了<strong>跨噪声水平的专家切换是一种有效打破扩散模型似然-质量权衡的方法</strong>。该方法为生成高质量且统计准确的图像提供了新的途径。</p>
<p><strong>4. 论文提及的局限性：</strong>
*   <strong>性能依赖于预训练模型：</strong> 合并模型的性能很大程度上取决于所选预训练专家的特性。
*   <strong>最优切换阈值需经验确定：</strong> 最佳的切换点（η）需要通过实验来确定，缺乏自动化或学习机制。
*   <strong>局限于像素空间模型：</strong> 目前的研究仅限于像素空间的扩散模型，未来可能需要适应其他架构和训练范式。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>自动化或学习式切换机制：</strong> 开发能够自动学习最优切换点或动态调整切换策略的方法。
*   <strong>集成先进采样器：</strong> 将该方法与更高级的采样技术相结合，以进一步提升性能。
*   <strong>扩展到其他模型：</strong> 将此方法应用于潜在空间扩散模型或一致性模型（consistency models）。
*   <strong>探索其他架构：</strong> 研究该方法在不同扩散模型架构（如Transformer）上的适用性。</p>
<p>总而言之，这篇论文提出了一种新颖且实用的方法，通过巧妙地融合两个在不同方面（感知质量和数据似然）具有优势的预训练扩散模型，成功地解决了图像生成领域一个长期存在的挑战，为构建更强大的生成模型开辟了新的思路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory.</li>
<li>On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19434v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19434v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19431v1'></a></p>
<h2 id="cloud4d"><a href="https://arxiv.org/abs/2511.19431v1">Cloud4D</a></h2>
<p><strong>Authors:</strong> Jacob Lin, Edward Gryspeerdt, Ronald Clark</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV, physics.ao-ph</p>
<p><strong>Abstract:</strong></p>
<p>There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error (<script type="math/tex"><10\%</script>) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Cloud4D: Estimating Cloud Properties at a High Spatial and Temporal Resolution”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Cloud4D: 估算高空间和时间分辨率的云属性</p>
<p><strong>作者：</strong> Jacob Lin, Edward Gryspeerdt, Ronald Clark</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前数值天气预报和气候模型在处理诸如极端降水、阵风、湍流和地表辐照度等精细大气现象时面临挑战，主要原因是这些模型通常以公里级分辨率运行，难以捕捉个体云的复杂性和动态。为了改进这些模型并准确模拟大气演变，需要更高分辨率的真实世界观测数据，而现有仪器难以满足这一需求。特别是，浅层积云因其尺度小、生命周期短，难以被传统雷达和卫星测量系统精确捕捉，这导致了模型中参数化近似的误差，进而影响了天气预报和气候预测的准确性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了 <strong>Cloud4D</strong>，一个首个基于学习的框架，能够仅利用同步的地面相机数据，重建物理上一致的四维（4D）云状态。其核心创新包括：</p>
<ul>
<li><strong>高分辨率2.5D云层模型：</strong> 利用相机几何和<strong>同调性（homography）引导的2D-to-3D Transformer架构</strong>，将图像映射到云层，并预测关键的2.5D云属性，如液态水路径（LWP）、云底高（CBH）和云几何厚度（Δh）。这种方法利用了云层的空间结构，将问题转化为更易处理的2D-to-2D任务。</li>
<li><strong>3D精炼（Refinement）阶段：</strong> 引入一个<strong>稀疏Transformer</strong>来精炼初始的3D云液态水含量（LWC）场，从而获得更精确的3D云分布。该方法通过处理稀疏体素来降低计算复杂度，同时学习3D云的先验知识。</li>
<li><strong>高精度风场估计：</strong> 通过追踪重建的3D云液态水含量随时间的变化，<strong>估算高度和时间变化的水平风场</strong>。这利用了CoTracker3等先进的点追踪技术，实现了对云运动的精确跟踪。</li>
<li><strong>端到端框架：</strong> Cloud4D整合了云层建模、3D精炼和风场估计，形成一个完整的系统，能够从原始图像数据生成高分辨率的3D云属性和风场信息。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
Cloud4D在为期两个月的实际部署中，使用六个朝上的相机，取得了显著的成果：</p>
<ul>
<li><strong>空间-时间分辨率的巨大提升：</strong> 相较于最先进的卫星测量，Cloud4D在时空分辨率上实现了<strong>数量级（order-of-magnitude）的提升</strong>，能够以25米的空间分辨率和5秒的时间分辨率进行云属性估计。</li>
<li><strong>高精度验证：</strong> 与地面雷达测量相比，Cloud4D的云属性估算保持了<strong>个位数百分比的相对误差（&lt;10%）</strong>。</li>
<li><strong>填补观测空白：</strong> Cloud4D提供了一种<strong>低成本、可扩展</strong>的方式来获取高分辨率的云观测数据，填补了当前观测能力在精细尺度云物理过程上的长期空白，为改进物理模型和训练数据驱动模型提供了宝贵的数据支持。</li>
<li><strong>风场估算能力：</strong> 估算出的水平风场与雷达风廓线仪的测量结果在<strong>量级和方向上相似</strong>，证明了其在风场估计方面的有效性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>云类型限制：</strong> 目前的模型主要在<strong>积云（cumulus clouds）</strong>上进行了训练和评估。虽然积云对大气有重要影响，但其他云类型也是重要的气候驱动因素，将模型扩展到其他云类型是未来的一个方向。
*   <strong>单层云假设：</strong> 研究主要集中在<strong>单层云</strong>的检索，因为地面相机通常会被上层云遮挡。对于多层云或光学厚云的检索能力有待提高。
*   <strong>环境因素影响：</strong> 地面相机容易受到<strong>雨、雾、雪</strong>等环境条件的影响，可能导致观测的局限性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展到多层云和光学厚云：</strong> 开发能够处理多层云和光学厚云的框架，以提高模型的鲁棒性和应用范围。
*   <strong>整合辐射传输约束：</strong> 将辐射传输模型纳入检索过程，以进一步提高物理准确性。
*   <strong>与可微分模拟器耦合：</strong> 将Cloud4D的检索结果与可微分模拟器相结合，以增强物理保真度。
*   <strong>探索其他云类型：</strong> 将Cloud4D扩展到其他类型的云，以更全面地研究大气现象。
*   <strong>利用立体视觉技术：</strong> 虽然本文未直接使用立体视觉技术，但探索其在相机阵列中的应用可能进一步提升3D重建的精度。</p>
<p><strong>论文对计算机视觉领域的新颖性/重要性：</strong>
Cloud4D在计算机视觉领域的重要贡献在于其<strong>开创性地将地面相机阵列应用于高分辨率的3D云属性重建</strong>。它巧妙地结合了<strong>同调性引导的2D-to-3D Transformer</strong>，解决了从多视角图像中恢复3D几何和物理信息这一核心挑战。这种方法不仅在<strong>时空分辨率上实现了质的飞跃</strong>，而且在<strong>精度上达到了与专业气象仪器相媲美的水平</strong>，同时成本更低、部署更灵活。这为计算机视觉在地球科学、气象学和气候建模等跨学科领域的应用开辟了新的途径，展示了利用现有视觉技术解决复杂科学问题的巨大潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras.</li>
<li>Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error (<script type="math/tex"><10\%</script>) against collocated radar measurements.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19431v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19431v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19430v1'></a></p>
<h2 id="cook-and-clean-together-teaching-embodied-agents-for-parallel-task-execution"><a href="https://arxiv.org/abs/2511.19430v1">Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</a></h2>
<p><strong>Authors:</strong> Dingkang Liang, Cheng Zhang, Xiaopeng Xu, Jianzhong Ju, Zhenbo Luo, Xiang Bai</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</p>
<p><strong>作者：</strong> Dingkang Liang, Cheng Zhang, Xiaopeng Xu, Jianzhong Ju, Zhenbo Luo, Xiang Bai</p>
<p><strong>摘要：</strong></p>
<p>这篇论文解决了当前具身智能体（embodied AI）在执行复杂任务时面临的关键挑战：如何高效地进行任务调度，尤其是在需要并行处理子任务的情况下。现有研究在任务规划方面存在不足，往往忽略了运筹学（Operations Research, OR）知识和三维空间定位（3D spatial grounding），导致生成的任务计划不够优化，无法充分利用并行子任务来缩短总完成时间。</p>
<p><strong>1. 研究问题：</strong></p>
<p>论文的核心研究问题是如何让具身智能体在理解自然语言指令的同时，结合运筹学知识和三维空间信息，生成高效的任务执行计划，特别是能够识别并利用并行子任务来最小化总完成时间。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li><strong>提出 ORS3D 任务：</strong> 作者首次提出了“运筹学知识驱动的三维空间定位任务调度”（Operations Research knowledge-based 3D Grounded Task Scheduling, ORS3D）这一新任务。该任务要求智能体不仅理解指令，还要进行三维空间定位，并利用 OR 知识进行高效调度。</li>
<li><strong>构建 ORS3D-60K 数据集：</strong> 为了支持 ORS3D 任务的研究，作者构建了一个大规模数据集 ORS3D-60K，包含 60,825 个复合任务，覆盖 4,376 个真实室内场景。该数据集的特点是引入了 OR 知识，并且任务文本长度长，对模型的理解和推理能力提出了更高要求。</li>
<li><strong>提出 GRANT 模型：</strong> 作者提出了一个名为 GRANT 的具身多模态大语言模型（embodied Multi-modal Large Language Model, MLLM）。GRANT 的核心创新在于其“调度令牌机制”（Scheduling Token Mechanism, STM），该机制能够将 LLM 的输出与外部优化求解器连接起来，生成最优的任务调度。GRANT 还包含一个“3D 空间定位头”（3D Grounding Head），用于生成目标对象的精确三维定位掩码。</li>
<li><strong>并行子任务识别与调度：</strong> ORS3D 任务的关键在于识别“并行子任务”（parallelizable subtasks），即那些在执行过程中不需要持续关注的子任务（如使用微波炉加热食物），以便智能体可以在等待期间执行其他“非并行子任务”（non-parallelizable subtasks）。GRANT 模型通过 STM 和优化求解器来解决这一调度问题。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>GRANT 模型性能优越：</strong> 在 ORS3D-60K 数据集上的实验表明，GRANT 模型在语言理解、三维空间定位和调度效率方面均取得了显著的性能提升。与基线方法相比，GRANT 在任务完成时间效率上提升了 30.53%，在空间定位准确率上提升了 1.38%，整体性能提升了 10.46%。</li>
<li><strong>有效性验证：</strong> 消融研究表明，调度令牌机制（STM）对于提升调度效率至关重要，而准确识别并行子任务是实现高效调度的前提。</li>
<li><strong>数据集的价值：</strong> ORS3D-60K 数据集为具身智能体在复杂、动态环境中的任务规划和执行研究提供了一个新的、更具挑战性的基准。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>动态环境的鲁棒性：</strong> 尽管 GRANT 在模拟环境中表现出色，但其在动态真实机器人环境中的鲁棒性仍需进一步验证。</li>
<li><strong>端到端可微分推理：</strong> 目前，GRANT 的调度部分依赖于外部优化求解器，作者希望未来能将求解器集成到语言模型中，实现端到端的、可微分的推理。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>部署到物理机器人：</strong> 将 GRANT 模型部署到真实的物理机器人上，以验证其在动态和不可预测环境中的性能。</li>
<li><strong>端到端可微分调度：</strong> 探索将外部调度求解器与 LLM 融合，实现端到端的、可微分的调度推理，以进一步优化模型。</li>
<li><strong>更复杂的任务和环境：</strong> 扩展到更复杂的多智能体协作任务或更具挑战性的三维环境。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文在具身智能体领域做出了重要贡献，通过引入 ORS3D 任务和 ORS3D-60K 数据集，推动了对高效任务调度和三维空间定位的研究。提出的 GRANT 模型及其调度令牌机制（STM）有效地解决了现有方法的不足，展示了结合运筹学知识和多模态理解在具身智能体任务执行中的巨大潜力。该工作为未来具身智能体在真实世界中的复杂任务执行奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization.</li>
<li>Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19430v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19430v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19428v1'></a></p>
<h2 id="flow-map-distillation-without-data"><a href="https://arxiv.org/abs/2511.19428v1">Flow Map Distillation Without Data</a></h2>
<p><strong>Authors:</strong> Shangyuan Tong, Nanye Ma, Saining Xie, Tommi Jaakkola</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并提供以下中文解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文的核心贡献在于提出了一种<strong>无数据流图蒸馏（Flow Map Distillation Without Data）</strong>的新范式。它成功地克服了传统流图蒸馏依赖外部数据集所带来的“教师-数据不匹配”风险，通过仅从先验分布采样来学习教师模型的生成能力，从而实现了更鲁棒、更高质量的蒸馏。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>该论文的关键创新在于其<strong>数据无关的蒸馏策略</strong>。具体方法论体现在：</p>
<ul>
<li><strong>从先验分布采样：</strong> 传统的流图蒸馏需要从外部数据集采样，而该方法创新性地提出仅从教师模型自身遵循的先验分布进行采样。这保证了采样数据与教师模型的生成能力是完全一致的，消除了数据不匹配的风险。</li>
<li><strong>学习预测教师的采样路径并主动纠错：</strong> 为了在无数据的情况下准确模仿教师模型的生成过程，该方法设计了一个框架，能够学习预测教师模型在采样过程中的路径，并且能够主动纠正自身累积的误差，以确保生成结果的高保真度。这暗示了一种更精细的、动态的学习机制。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<p>这项研究对生成模型加速领域具有重大的潜在影响：</p>
<ul>
<li><strong>打破数据依赖瓶颈：</strong> 传统蒸馏方法对数据集的依赖是其推广和应用的一大障碍。无数据蒸馏的提出，极大地降低了蒸馏的门槛，使得更多研究者和开发者能够更便捷地利用强大的预训练模型。</li>
<li><strong>提升蒸馏的鲁棒性和通用性：</strong> 消除了“教师-数据不匹配”的风险，意味着蒸馏过程不再受限于特定数据集的质量和代表性，从而提高了蒸馏的鲁棒性和通用性。</li>
<li><strong>推动生成模型的高效部署：</strong> 通过显著减少采样步数（例如，在ImageNet上实现1步采样），该方法为生成模型的实际部署和应用提供了极大的便利，尤其是在对实时性要求较高的场景。</li>
<li><strong>建立新的研究范式：</strong> 该论文提出的“无数据蒸馏”理念，有望成为加速生成模型领域的一个新的研究方向和主流范式，激励更多关于数据无关学习的研究。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<p>这项研究的成果可以广泛应用于以下领域：</p>
<ul>
<li><strong>图像生成：</strong> 如StyleGAN、Diffusion Models等高质量图像生成模型的加速。</li>
<li><strong>视频生成：</strong> 加速视频生成模型的采样过程，实现更流畅、更高质量的视频内容创作。</li>
<li><strong>3D内容生成：</strong> 加速3D模型、场景的生成。</li>
<li><strong>科学模拟：</strong> 在物理、化学等领域，如果存在基于流模型的模拟，该方法也可用于加速模拟过程。</li>
<li><strong>模型压缩与部署：</strong> 对于资源受限的设备，可以通过蒸馏获得更轻量、更快速的模型。</li>
<li><strong>对抗性攻击与防御：</strong> 生成模型在对抗性领域也有应用，加速生成过程可能有助于更快的对抗样本生成或防御策略研究。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<p>尽管摘要展示了令人印象深刻的成果，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算复杂度：</strong> 虽然最终采样步数减少，但“学习预测教师的采样路径并主动纠错”的过程本身可能需要较高的计算资源和训练时间。摘要中并未详细说明训练过程的效率。</li>
<li><strong>对教师模型的依赖：</strong> 该方法仍然高度依赖于一个高质量的、预训练好的教师模型。如果教师模型本身存在缺陷，蒸馏效果也会受到影响。</li>
<li><strong>理论保证的深度：</strong> 摘要强调了“ principled framework”，但具体理论上的保证（例如，误差累积的界限、收敛性证明等）可能需要深入阅读论文全文才能了解。</li>
<li><strong>泛化性到其他类型的流模型：</strong> 摘要提到了“flow maps”，但其方法是否能无缝泛化到所有类型的流模型（例如，连续流模型）仍需验证。</li>
<li><strong>对先验分布的假设：</strong> 方法的有效性在一定程度上依赖于先验分布的性质以及教师模型对该分布的遵循程度。如果先验分布的定义或教师模型的训练方式不符合预期，可能会影响效果。</li>
</ul>
<p>总而言之，这篇论文提出的“无数据流图蒸馏”是一个极具创新性和前瞻性的工作，它不仅解决了现有技术中的关键痛点，而且在理论和实践上都取得了显著的突破，有望为生成模型加速领域带来革命性的变化。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling.</li>
<li>To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity.</li>
<li>Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin.</li>
<li>Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19428v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19428v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19426v1'></a></p>
<h2 id="ref-sam3d-bridging-sam3d-with-text-for-reference-3d-reconstruction"><a href="https://arxiv.org/abs/2511.19426v1">Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction</a></h2>
<p><strong>Authors:</strong> Yun Zhou, Yaoting Wang, Guangquan Jie, Jinyu Liu, Henghui Ding</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面摘要。</p>
<p><strong>论文题目：</strong> Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction</p>
<p><strong>作者：</strong> Yun Zhou, Yaoting Wang, Guangquan Jie, Jinyu Liu, Henghui Ding</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决现有3D对象重建模型（特别是SAM3D）的一个关键局限性：<strong>无法直接根据文本描述来重建特定的3D对象</strong>。尽管SAM3D在3D重建方面表现出色，但它依赖于用户提供的空间提示（如掩码、边界框），这在许多实际应用中（如3D编辑、游戏开发）并不方便，甚至不可行。当用户希望通过高层语义属性而非精确空间定位来指定目标时，现有方法就显得力不从心。因此，研究的核心问题是如何实现<strong>基于自然语言描述的、从单张RGB图像进行的3D对象重建</strong>。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>论文的核心创新在于提出了<strong>Ref-SAM3D</strong>，一个简单而有效的SAM3D扩展。其主要贡献包括：</p>
<ul>
<li><strong>引入文本作为高层先验：</strong> Ref-SAM3D将自然语言描述作为一种新的、高层级的参考信号，弥补了SAM3D仅依赖空间提示的不足。</li>
<li><strong>文本到掩码的集成：</strong> 通过集成一个具备视觉语言理解能力的掩码生成器（例如，利用SAM3），Ref-SAM3D能够将文本描述转化为精确的对象掩码。</li>
<li><strong>无缝衔接现有模型：</strong> Ref-SAM3D对基础掩码生成器和SAM3D重建器几乎没有进行架构上的修改，也无需重新训练，使其成为一个即插即用的模块，能够轻松集成到现有工作流程中。</li>
<li><strong>实现文本引导的3D重建：</strong> 整个流程仅需一张RGB图像和一个文本描述，即可自动生成对应对象的3D重建结果。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>高质量的零样本3D重建：</strong> 通过大量的定性实验，论文证明了Ref-SAM3D仅凭自然语言和单张2D图像，就能实现具有竞争力的、高保真度的零样本3D重建性能。</li>
<li><strong>提升交互灵活性和可访问性：</strong> Ref-SAM3D极大地增强了3D重建的交互灵活性和可访问性，使得用户能够以更自然、更直观的方式（通过语言）来指定和重建3D对象。</li>
<li><strong>弥合2D视觉与3D几何理解的鸿沟：</strong> 该方法有效地连接了2D图像的视觉线索和3D几何理解能力，为参考引导式3D重建提供了一种更灵活的范式。</li>
<li><strong>推动多模态3D理解研究：</strong> Ref-SAM3D为更直观、更具语言驱动力的3D内容创作奠定了基础，并有望激发未来在多模态3D理解领域的研究。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中明确提到的局限性主要体现在其<strong>依赖于现有的SAM3D模型</strong>。虽然Ref-SAM3D本身无需额外训练，但其重建质量和能力很大程度上取决于SAM3D的性能。此外，虽然论文展示了在不同复杂场景下的优秀表现，但对于极度模糊或高度相似的实例区分，仍可能存在挑战（尽管论文在实验部分设计了专门的“多实例”场景来测试这一点）。</p>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的语言理解与控制：</strong> 进一步提升模型对复杂、细微的语言描述的理解能力，实现更精细的3D对象编辑和控制。</li>
<li><strong>多模态融合的深化：</strong> 探索将更多模态（如音频、深度信息）与文本和图像结合，以实现更鲁棒和全面的3D重建。</li>
<li><strong>实时交互式3D内容创作：</strong> 将Ref-SAM3D的能力应用于实时3D内容创作工具，实现更流畅、更具沉浸感的创作体验。</li>
<li><strong>大规模数据集的构建与利用：</strong> 进一步构建更大规模、更多样化的文本-3D对应数据集，以训练更强大的语言引导式3D重建模型。</li>
<li><strong>模型效率与泛化能力的提升：</strong> 探索更高效的模型架构和训练策略，以在保持高性能的同时，降低计算成本，并提高模型在各种未知场景下的泛化能力。</li>
</ul>
<hr />
<p>总而言之，Ref-SAM3D是一项重要的工作，它成功地将自然语言理解能力引入到3D对象重建领域，显著提升了现有方法的可用性和灵活性，为未来的多模态3D内容创作和理解开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image.</li>
<li>Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19426v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19426v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19425v1'></a></p>
<h2 id="sam3-adapter-efficient-adaptation-of-segment-anything-3-for-camouflage-object-segmentation-shadow-detection-and-medical-image-segmentation"><a href="https://arxiv.org/abs/2511.19425v1">SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation</a></h2>
<p><strong>Authors:</strong> Tianrun Chen, Runlong Cao, Xinda Yu, Lanyun Zhu, Chaotao Ding, Deyi Ji, Cheng Chen, Qi Zhu, Chunyan Xu, Papa Mao, Ying Zang</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation”的全面摘要：</p>
<p><strong>论文题目：</strong> SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation</p>
<p><strong>作者：</strong> Tianrun Chen, Runlong Cao, Xinda Yu, Lanyun Zhu, Chaotao Ding, Deyi Ji, Cheng Chen, Qi Zhu, Chunyan Xu, Papa Mao, Ying Zang</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>尽管大型基础模型（如 Segment Anything, SAM）在图像分割领域展现了前所未有的通用性，但其早期版本（SAM 和 SAM2）在处理精细、低级别的分割任务时仍存在挑战，例如伪装物体检测、医学图像分割、细胞图像分割和阴影检测。这些任务需要模型能够理解和区分与背景高度融合的物体，或捕捉细微的图像特征。本文的研究问题在于如何充分释放新一代基础模型 SAM3 的强大潜力，使其在这些具有挑战性的下游任务上达到最先进（SOTA）的性能。</p>
<p><strong>2. 关键创新或方法贡献：</strong></p>
<ul>
<li><strong>SAM3-Adapter 框架：</strong> 论文提出了 SAM3-Adapter，这是第一个专门为 SAM3 模型设计的轻量级适配器框架。该框架旨在高效地将 SAM3 的通用分割能力适配到各种专业下游任务中。</li>
<li><strong>模块化和可组合设计：</strong> 沿袭了 SAM-Adapter 的设计理念，SAM3-Adapter 具有模块化和可组合的特性，能够灵活适应不同的任务需求，并保持参数效率。</li>
<li><strong>分层适配：</strong> SAM3-Adapter 为 SAM3 的多阶段分层架构设计了相应的适配器，确保了每个阶段的编码器特征都能得到有效利用，从而实现更精细的引导。</li>
<li><strong>灵活的任务特定输入：</strong> 适配器接受的任务特定信息（F_t）具有高度灵活性，可以根据下游应用的需求，从数据集统计信息、手工规则或多种引导信号的组合中派生，实现更细粒度的控制。</li>
<li><strong>冻结主干网络：</strong> SAM3 的强大视觉编码器在训练过程中被冻结，保留了其在海量数据上学到的丰富视觉表示，同时避免了重新训练整个模型的巨大计算成本。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>SOTA 性能：</strong> SAM3-Adapter 在伪装物体检测、医学图像分割（包括多项任务）和阴影检测等多个具有挑战性的下游任务上取得了显著的性能提升，并建立了新的 SOTA 记录。</li>
<li><strong>超越前代模型：</strong> 实验结果表明，SAM3-Adapter 显著优于单独使用 SAM3、SAM2，以及之前的 SAM 和 SAM2 适配器方法。</li>
<li><strong>计算效率：</strong> 尽管性能大幅提升，SAM3-Adapter 保持了参数效率，降低了计算开销，使其在实际应用中更具可行性。</li>
<li><strong>通用性和适应性：</strong> SAM3-Adapter 展现了更强的泛化能力和更丰富的任务适应性，能够处理各种精细分割的挑战。</li>
<li><strong>验证了基础模型缩放的潜力：</strong> 研究有力地证明，基础模型的规模化（如 SAM3）与智能的适配器技术相结合，能够直接转化为专业领域的突破性性能。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<p>论文中并未明确提及 SAM3-Adapter 本身的局限性。然而，从其研究动机来看，基础模型在处理“精细、低级别”任务时的固有挑战是其改进的出发点。虽然 SAM3-Adapter 显著提升了性能，但对于某些极端复杂或高度抽象的分割任务，可能仍有进一步优化的空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的下游任务：</strong> 将 SAM3-Adapter 应用于更多不同类型的分割任务，以进一步探索其通用性。</li>
<li><strong>更精细的适配器设计：</strong> 研究更复杂的适配器结构或训练策略，以应对更具挑战性的分割场景。</li>
<li><strong>实时应用：</strong> 进一步优化 SAM3-Adapter 的效率，以满足实时分割的需求。</li>
<li><strong>多模态融合：</strong> 探索将 SAM3-Adapter 与其他模态（如文本、点云）信息结合，以实现更强大的分割能力。</li>
<li><strong>可解释性研究：</strong> 深入分析 SAM3-Adapter 如何引导 SAM3 模型在特定任务上取得更好的表现，提升模型的可解释性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文成功地解决了大型基础模型在精细分割任务上的不足，通过提出 SAM3-Adapter 这一高效轻量级的适配器框架，显著提升了 SAM3 模型在伪装物体检测、医学图像分割和阴影检测等领域的性能，并达到了 SOTA 水平。该研究不仅为 SAM3 的应用开辟了新的道路，也为未来基础模型在专业领域的适配和应用提供了重要的参考和基础。论文的开源代码和预训练模型将有助于社区进一步的研究和开发。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability.</li>
<li>SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19425v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19425v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19418v1'></a></p>
<h2 id="chain-of-visual-thought-teaching-vlms-to-see-and-think-better-with-continuous-visual-tokens"><a href="https://arxiv.org/abs/2511.19418v1">Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</a></h2>
<p><strong>Authors:</strong> Yiming Qin, Bomin Wei, Jiaxin Ge, Konstantinos Kallidromitis, Stephanie Fu, Trevor Darrell, Xudong Wang</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</p>
<p><strong>作者：</strong> Yiming Qin, Bomin Wei, Jiaxin Ge, Konstantinos Kallidromitis, Stephanie Fu, Trevor Darrell, Xudong Wang</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>Chain-of-Visual-Thought (COVT)</strong> 的新框架，旨在解决当前视觉语言模型 (VLMs) 在处理需要精细视觉感知（如空间推理和几何感知）的任务时存在的局限性。</p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>当前 VLMs 在处理需要密集视觉信息（如边界、布局、深度、几何形状）的任务时表现不佳，因为它们主要依赖于离散的文本空间进行推理，这会丢失重要的视觉细节。将视觉信息强行映射到文本空间会导致信息损失，甚至可能误导模型的推理过程。因此，研究的核心问题是如何让 VLMs 能够像人类一样进行视觉思考，即在连续的视觉空间中进行推理，而不是仅仅依赖于文本。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>引入连续视觉令牌 (Continuous Visual Tokens)：</strong> COVT 的核心创新在于引入了紧凑的、连续的视觉令牌。这些令牌是从轻量级的“视觉专家”（如分割、深度、边缘检测和自监督表示学习模型）中提取的，能够编码丰富的感知线索，如 2D 外观、3D 几何、空间布局和边缘结构。</li>
<li><strong>视觉思维链 (Chain-of-Visual-Thought)：</strong> COVT 框架使 VLMs 能够生成一系列视觉令牌，形成一个“视觉思维链”。这个链条将语义推理与感知基础相结合，使得模型能够进行更精细、更具空间和几何意识的推理。</li>
<li><strong>自包含且可解释的框架：</strong> COVT 是一个自包含的、可微分的框架，无需外部工具。在推理时，模型直接在连续的视觉令牌空间中进行操作，保持了效率。同时，这些视觉令牌可以被解码成人类可读的密集预测（如分割掩码、深度图），从而提供了模型推理过程的可视化和可解释性。</li>
<li><strong>定制化的对齐策略和训练流程：</strong> 论文提出了针对不同类型视觉令牌（任务导向型和表示型）的定制化对齐策略，并设计了一个包含四个阶段（理解、生成、推理、高效推理）的训练流程，以逐步教会模型有效地利用视觉令牌进行推理。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>显著的性能提升：</strong> 在超过十个不同的感知基准测试（包括 CV-Bench、MMVP、RealWorldQA、MMStar、WorldMedQA 和 HRBench）上，将 COVT 集成到 Qwen2.5-VL 和 LLaVA 等强大的 VLMs 中，性能一致性地提高了 3% 到 16%。</li>
<li><strong>提升视觉感知能力：</strong> COVT 在 CV-Bench 的深度子任务上取得了 14.0% 的显著提升，在 HRBench 上也获得了 4.5% 的整体提升，证明了其在精细视觉推理任务上的有效性。</li>
<li><strong>泛化能力：</strong> COVT 不仅在视觉中心任务上表现出色，在非视觉中心任务上也保持了竞争力，甚至略有提升，表明其泛化能力。</li>
<li><strong>可解释性：</strong> 通过解码视觉令牌，COVT 能够提供模型推理过程的可视化，增强了多模态智能的可解释性。</li>
<li><strong>效率：</strong> 尽管引入了额外的视觉信息，COVT 在推理时保持了效率，因为模型直接在紧凑的连续令牌空间中操作，并且可以选择性地解码以获得解释。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>未穷尽的视觉模型和令牌组合：</strong> 论文承认其当前框架主要关注了分割、深度、边缘和 DINO 等代表性的感知轴，但并未穷尽所有可能的视觉模型或令牌组合。探索更多或混合的视觉专家可能带来更丰富的令牌表示。</li>
<li><strong>非完全交织的多模态推理：</strong> 当前的 COVT 框架生成连续的视觉思维，但尚未实现与自由形式文本推理的完全交织。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>探索更广泛的视觉模型和令牌组合：</strong> 进一步研究和集成更多类型的视觉专家，以生成更具表现力或互补性的视觉令牌。</li>
<li><strong>实现交织的多模态推理：</strong> 开发能够无缝融合文本和视觉思维的统一链式推理机制，实现更深层次的交织式多模态推理。</li>
</ul>
<p><strong>总结：</strong></p>
<p>COVT 框架通过引入连续视觉令牌和构建视觉思维链，成功地弥合了 VLMs 在语言推理和精细视觉感知之间的鸿沟。它不仅显著提升了 VLMs 在各种视觉任务上的性能，还提供了更好的可解释性，并保持了推理效率。这项工作为构建更强大、更具理解力和更可信赖的多模态人工智能系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19418v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19418v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.19413v1'></a></p>
<h2 id="unigame-turning-a-unified-multimodal-model-into-its-own-adversary"><a href="https://arxiv.org/abs/2511.19413v1">UniGame: Turning a Unified Multimodal Model Into Its Own Adversary</a></h2>
<p><strong>Authors:</strong> Zhaolong Su, Wang Lu, Hao Chen, Sharon Li, Jindong Wang</p>
<p><strong>Published:</strong> 2025-11-24</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“UniGame: Turning a Unified Multimodal Model Into Its Own Adversary”的全面中文摘要：</p>
<p><strong>论文题目：</strong> UniGame: 将统一的多模态模型转化为自身的“对手”</p>
<p><strong>作者：</strong> Zhaolong Su, Wang Lu, Hao Chen, Sharon Li, Jindong Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>统一的多模态模型（Unified Multimodal Models, UMMs）在理解和生成任务上都取得了显著进展，但其内在存在一个根本性的不一致性：<strong>理解任务偏好紧凑的嵌入表示，而生成任务则需要更丰富的重构表示。</strong> 这种结构上的权衡导致了决策边界的错位、跨模态连贯性下降，以及在分布外（out-of-distribution）和对抗性扰动下的脆弱性增加。本研究旨在解决这一核心不一致性问题，提升 UMMs 的整体性能和鲁棒性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>作者提出了 <strong>UniGame</strong>，一个<strong>自对抗的后训练框架</strong>，专门针对 UMMs 的不一致性问题。其核心创新在于：</p>
<ul>
<li><strong>自对抗训练范式：</strong> UniGame 将模型的生成分支视为一个“对手”，使其主动寻找并生成能够挑战理解分支的“脆弱”样本。通过这种“你中有我，我中有你”的对抗性自我博弈，模型能够发现并纠正自身的内在缺陷。</li>
<li><strong>轻量级扰动模块（Perturber C）：</strong> 在共享的 token 接口处引入一个轻量级的 MLP 模块，用于生成结构化的、有界限的对抗性扰动。</li>
<li><strong>解码器约束的对抗样本生成：</strong> 与传统的像素级对抗训练不同，UniGame 确保生成的对抗样本是<strong>解码器约束的</strong>，即它们在生成后仍然是视觉上合理且语义上一致的。</li>
<li><strong>硬样本挖掘与重放（Hard-sample buffer B）：</strong> 通过一个过滤机制，收集那些能够有效挑战理解分支的“硬样本”，并将其用于后续训练，进一步强化模型的鲁棒性。</li>
<li><strong>架构无关性：</strong> UniGame 是一个即插即用的框架，不依赖于特定的 UMM 架构，并且可以与现有的后训练方法互补。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<p>实验结果表明，UniGame 在多个方面取得了显著的提升：</p>
<ul>
<li><strong>一致性提升：</strong> UniGame 显著提高了模型的一致性得分（+4.6%）。</li>
<li><strong>理解与生成性能提升：</strong> 在理解任务上，性能提升了 +3.6%；在生成任务上，性能提升了 +0.02%。</li>
<li><strong>鲁棒性增强：</strong> 在分布外（NaturalBench）和对抗性（AdVQA）鲁棒性测试中，分别取得了 +4.8% 和 +6.2% 的显著提升。</li>
<li><strong>效率与可扩展性：</strong> UniGame 引入的额外参数量不到 1%，并且可以轻松集成到现有模型和后训练流程中，显示了其高效性和良好的可扩展性。</li>
</ul>
<p>这些结果表明，<strong>对抗性自我博弈</strong>是一种通用且有效的方法，可以增强未来多模态基础模型的<strong>连贯性、稳定性和统一能力</strong>。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模型覆盖范围：</strong> 目前主要在 Janus-Pro-7B 模型上进行了评估，更广泛的模型覆盖范围可能揭示更多见解。</li>
<li><strong>数据集多样性：</strong> 主要在有限的数据集上进行了测试，未来需要更广泛、更具挑战性的基准来验证其通用性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的模型和数据集验证：</strong> 将 UniGame 应用于更多不同架构的 UMMs，并在更多样化和更具挑战性的数据集上进行评估。</li>
<li><strong>探索更复杂的对抗策略：</strong> 研究更高级的对抗生成和挖掘策略，以进一步提升模型的性能和鲁棒性。</li>
<li><strong>理论分析的深化：</strong> 进一步深入研究其理论基础，例如在更复杂的非凸非凹博弈场景下的收敛性和稳定性。</li>
<li><strong>与其他后训练方法的融合：</strong> 探索 UniGame 与其他先进的后训练方法（如强化学习、提示调优等）的更深层次融合。</li>
</ul>
<p>总而言之，UniGame 提出了一种新颖的自对抗后训练框架，通过让模型的生成分支成为理解分支的“对手”，有效地解决了统一多模态模型中存在的理解与生成不一致性问题，显著提升了模型的一致性、性能和鲁棒性，为未来多模态基础模型的发展提供了新的思路和有效工具。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.19413v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.19413v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-25 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
