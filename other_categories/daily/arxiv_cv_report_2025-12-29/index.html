<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-29 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-26/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-30/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-29">Arxiv Computer Vision Papers - 2025-12-29</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#mai-ui-technical-report-real-world-centric-foundation-gui-agents" class="nav-link">MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</a>
                </li>
                <li class="nav-item">
                    <a href="#see-less-see-right-bi-directional-perceptual-shaping-for-multimodal-reasoning" class="nav-link">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#proedit-inversion-based-editing-from-prompts-done-right" class="nav-link">ProEdit: Inversion-based Editing From Prompts Done Right</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-association-via-track-detection-matching-for-multi-object-tracking" class="nav-link">Learning Association via Track-Detection Matching for Multi-Object Tracking</a>
                </li>
                <li class="nav-item">
                    <a href="#yume-15-a-text-controlled-interactive-world-generation-model" class="nav-link">Yume-1.5: A Text-Controlled Interactive World Generation Model</a>
                </li>
                <li class="nav-item">
                    <a href="#streamavatar-streaming-diffusion-models-for-real-time-interactive-human-avatars" class="nav-link">StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</a>
                </li>
                <li class="nav-item">
                    <a href="#backdoor-attacks-on-prompt-driven-video-segmentation-foundation-models" class="nav-link">Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#longfly-long-horizon-uav-vision-and-language-navigation-with-spatiotemporal-context-integration" class="nav-link">LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</a>
                </li>
                <li class="nav-item">
                    <a href="#ishift-lightweight-slow-fast-gui-agent-with-adaptive-perception" class="nav-link">iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#look-closer-an-adversarial-parametric-editing-framework-for-hallucination-mitigation-in-vlms" class="nav-link">Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-29">Arxiv Computer Vision Papers - 2025-12-29</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月26日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月26日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期论文集聚焦于<strong>多模态理解与生成</strong>、<strong>基础模型（Foundation Models）的实用化与安全性</strong>，以及<strong>高效的交互式视觉智能体</strong>。特别值得注意的是，研究人员正积极探索如何使大型视觉模型在真实世界场景中更具鲁棒性、可控性和交互性，同时关注其潜在的安全漏洞。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>真实世界中心的基础GUI智能体：</strong> MAI-UI 和 iSHIFT 两篇论文（1, 9）共同展示了在构建能够理解和操作真实世界图形用户界面（GUI）的智能体方面的进展。MAI-UI 强调“真实世界中心”的设计理念，而 iSHIFT 则提出了轻量级、自适应感知的方法，预示着更实用、更高效的GUI自动化智能体即将到来。</li>
<li><strong>多模态交互与生成能力的提升：</strong> Yume-1.5 (5) 和 StreamAvatar (6) 在文本控制的交互式内容生成方面取得了显著进展。Yume-1.5 实现了文本控制的交互式世界生成，而 StreamAvatar 则专注于实时交互式人类化身的流式扩散模型，这对于虚拟现实、元宇宙等应用具有重要意义。</li>
<li><strong>视觉语言模型（VLMs）的鲁棒性与可编辑性：</strong> ProEdit (3) 和 Look Closer! (10) 均致力于提升VLMs的可控性和准确性。ProEdit 提出了一种基于反演的提示编辑方法，而 Look Closer! 则通过对抗性参数化编辑来缓解幻觉问题，这对于提高VLM的可靠性和用户体验至关重要。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>双向感知塑造（Bi-directional Perceptual Shaping）：</strong> See Less, See Right (2) 提出的双向感知塑造方法，通过在多模态推理中显式地引导感知过程，为提升模型在复杂推理任务中的表现提供了新思路。</li>
<li><strong>长时域UAV导航的融合：</strong> LongFly (8) 在长时域无人机（UAV）视觉与语言导航方面，通过时空上下文集成，解决了长距离、复杂环境下的导航挑战。</li>
<li><strong>基础模型的后门攻击研究：</strong> Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models (7) 揭示了基础模型在提示驱动下的潜在安全风险，强调了模型安全性和鲁棒性研究的重要性。</li>
<li><strong>跟踪与检测的关联学习：</strong> Learning Association via Track-Detection Matching (4) 提出了一种新的多目标跟踪方法，通过匹配跟踪与检测来学习关联，有望提升多目标跟踪的精度和效率。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其对当前研究热点和未来方向的潜在影响，以下论文值得深入阅读：</p>
<ol>
<li><strong>MAI-UI Technical Report: Real-World Centric Foundation GUI Agents (1)</strong>：对于希望构建实际应用型智能体的研究者，理解其“真实世界中心”的设计理念至关重要。</li>
<li><strong>See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning (2)</strong>：该方法在多模态推理领域具有创新性，可能为提升模型理解能力提供新的技术路径。</li>
<li><strong>ProEdit: Inversion-based Editing From Prompts Done Right (3)</strong> 和 <strong>Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs (10)</strong>：这两篇论文直接解决了当前VLM应用中的关键痛点——可控性和准确性，对于任何从事VLM研究或应用的人员都极具价值。</li>
<li><strong>StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars (6)</strong>：在元宇宙和虚拟交互领域具有前瞻性，对实时生成高质量虚拟形象的技术细节感兴趣的研究者不容错过。</li>
</ol>
<hr />
<p>希望这份摘要能帮助您快速了解近期 Arxiv 计算机视觉领域的最新进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.22047v1">MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</a></li>
<li><a href="#2512.22120v1">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</a></li>
<li><a href="#2512.22118v1">ProEdit: Inversion-based Editing From Prompts Done Right</a></li>
<li><a href="#2512.22105v1">Learning Association via Track-Detection Matching for Multi-Object Tracking</a></li>
<li><a href="#2512.22096v1">Yume-1.5: A Text-Controlled Interactive World Generation Model</a></li>
<li><a href="#2512.22065v1">StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</a></li>
<li><a href="#2512.22046v1">Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</a></li>
<li><a href="#2512.22010v1">LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</a></li>
<li><a href="#2512.22009v1">iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception</a></li>
<li><a href="#2512.21999v1">Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.22047v1'></a></p>
<h2 id="mai-ui-technical-report-real-world-centric-foundation-gui-agents"><a href="https://arxiv.org/abs/2512.22047v1">MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</a></h2>
<p><strong>Authors:</strong> Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提炼出以下关键信息：</p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了 MAI-UI，一个涵盖多种规模（2B 至 235B）的<strong>基础 GUI 智能体家族</strong>，旨在解决现实世界中 GUI 智能体部署的四大挑战。MAI-UI 通过创新的自进化数据流水线、设备-云协同系统和在线强化学习框架，在 GUI 基础（grounding）和移动导航任务上均取得了<strong>新的 SOTA 性能</strong>，显著超越了现有领先模型。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>MAI-UI 的核心创新在于其<strong>统一的方法论</strong>，旨在克服现实世界部署的障碍：</p>
<ul>
<li><strong>自进化数据流水线 (Self-evolving Data Pipeline):</strong> 这是最关键的创新之一。它不仅仅是收集 UI 信息，而是<strong>扩展了导航数据，主动融入了用户交互（user interaction）和多模态指令（MCP tool calls）</strong>。这意味着智能体能够从真实的用户行为和更丰富的指令中学习，从而更贴近实际使用场景。</li>
<li><strong>设备-云协同系统 (Native Device-Cloud Collaboration System):</strong> 这是一个<strong>务实的部署架构</strong>。它能够根据任务状态（task state）智能地路由执行，将计算任务分配到设备端或云端。这不仅提升了<strong>设备端性能（+33%）</strong>，<strong>减少了云端调用（-40%）</strong>，还<strong>保护了用户隐私</strong>。</li>
<li><strong>在线强化学习框架 (Online RL Framework):</strong> 采用<strong>先进的优化技术</strong>来<strong>扩展并行环境和上下文长度</strong>。这使得智能体能够更有效地在动态环境中学习和适应，并处理更复杂的交互序列。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>推动下一代人机交互 (HCI) 的发展:</strong> MAI-UI 的成功部署将使 GUI 智能体真正成为下一代人机交互的革命性力量，使计算机能够更智能、更自然地理解和操作用户界面。</li>
<li><strong>为通用 GUI 智能体奠定基础:</strong> MAI-UI 的“基础智能体”概念，以及其多尺寸模型家族，预示着未来可能出现能够处理各种 GUI 任务的通用型智能体。</li>
<li><strong>加速智能体在真实世界的应用:</strong> 该研究直接解决了现实世界部署的挑战，如动态环境适应性和效率问题，为将 GUI 智能体推向实际应用铺平了道路。</li>
<li><strong>提升移动端智能体能力:</strong> 在移动 GUI 导航上的 SOTA 表现，将极大地提升移动设备上的自动化和智能化体验。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>自动化测试和质量保证:</strong> 智能体可以模拟用户行为，进行更全面、更高效的 UI 测试。</li>
<li><strong>无障碍技术:</strong> 帮助残障人士更轻松地与数字设备交互。</li>
<li><strong>个性化用户体验:</strong> 根据用户习惯和偏好，智能体可以主动调整界面和操作流程。</li>
<li><strong>智能助手和虚拟代理:</strong> 更强大的 GUI 操作能力将使智能助手能够执行更复杂的任务，如预订机票、填写表格等。</li>
<li><strong>教育和培训:</strong> 智能体可以作为交互式教程，引导用户学习软件操作。</li>
<li><strong>机器人控制:</strong> 将 GUI 操作能力与机器人本体控制相结合，实现更复杂的任务。</li>
<li><strong>软件开发和原型设计:</strong> 辅助开发者进行 UI 设计和功能验证。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>“基础智能体”的通用性仍需验证:</strong> 虽然模型涵盖多种尺寸，但其在<strong>跨领域、跨平台 GUI 的泛化能力</strong>仍需在更广泛的场景下进行验证。摘要中提到的 SOTA 表现主要集中在特定的基准测试上。</li>
<li><strong>“自进化”的定义和机制:</strong> 摘要提到了“自进化数据流水线”，但具体的进化机制、如何保证进化过程的稳定性和有效性，以及潜在的“漂移”问题，并未详细说明。</li>
<li><strong>“用户隐私”的实现细节:</strong> 设备-云协同系统声称能保护用户隐私，但具体的隐私保护技术和策略（例如数据匿名化、差分隐私等）并未在摘要中披露。</li>
<li><strong>“动态环境”的定义和挑战:</strong> 摘要提到了“动态环境”的挑战，但具体是哪些类型的动态变化（例如 UI 布局变化、网络延迟、用户输入中断等）以及 MAI-UI 如何应对这些变化，需要更深入的了解。</li>
<li><strong>计算资源和部署成本:</strong> 尽管有设备-云协同，但 235B 的模型规模仍然巨大，其在实际部署中的<strong>计算资源需求和成本</strong>可能是一个挑战。</li>
<li><strong>“MCP tool calls”的含义和范围:</strong> 摘要中提到了“MCP tool calls”，这可能指的是多模态指令（Multimodal Command Processing）或者某种特定的工具调用接口。其具体含义和支持的工具范围会影响智能体的能力。</li>
</ul>
<p><strong>总结来说，</strong> MAI-UI 的研究在 GUI 智能体领域具有里程碑式的意义。它不仅在多个关键任务上取得了显著的性能提升，更重要的是，它提出了一套<strong>务实且创新的方法论</strong>，直接解决了 GUI 智能体在现实世界部署的关键瓶颈。其设备-云协同和自进化数据流水线的设计，预示着更智能、更高效、更安全的下一代人机交互体验的到来。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants.</li>
<li>MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation.</li>
<li>On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22047v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22047v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22120v1'></a></p>
<h2 id="see-less-see-right-bi-directional-perceptual-shaping-for-multimodal-reasoning"><a href="https://arxiv.org/abs/2512.22120v1">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</a></h2>
<p><strong>Authors:</strong> Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning (少看多看：双向感知塑造用于多模态推理)</p>
<p><strong>作者：</strong> Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决大型视觉语言模型（VLMs）在多模态推理任务中存在的感知瓶颈。现有方法通常依赖于中间视觉线索（如通过外部工具或生成潜在视觉标记），但这些方法存在以下不足：
*   <strong>忽略精细视觉证据：</strong> 难以捕捉图表中的细线（polylines）等精细视觉细节。
*   <strong>泛化能力差：</strong> 在不同领域和数据集上泛化能力不足。
*   <strong>推理成本高：</strong> 在推理时需要额外的计算步骤，增加了延迟。
*   <strong>易产生文本捷径：</strong> 模型可能过度依赖文本信息，而忽略了视觉证据。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>作者提出了<strong>双向感知塑造（BiPS）</strong>框架，一种创新的训练时方法，旨在将推理时的中间视觉线索转化为训练信号，从而塑造模型的内部感知策略。BiPS的核心在于利用两种互补的KL（Kullback-Leibler）散度约束：</p>
<ul>
<li><strong>一致性约束（Consistency Constraint）：</strong> 通过将原始图像与一个<strong>证据保留视图（Evidence-Preserving View）</strong>进行比较，该视图仅保留与问题相关的区域。此约束鼓励模型粗粒度但完整地覆盖支持性像素，确保模型关注到关键信息。</li>
<li><strong>分离约束（Separation Constraint）：</strong> 通过将原始图像与一个<strong>证据消融视图（Evidence-Ablated View）</strong>进行比较，该视图掩盖了关键像素，使得图像无法支持原始答案。此约束旨在阻止模型依赖文本捷径，强制模型依赖精细的视觉信息进行推理。</li>
</ul>
<p>BiPS采用<strong>粗粒度到细粒度（Coarse-to-Fine）</strong>的两阶段训练课程：
*   <strong>阶段一（一致性阶段）：</strong> 最小化Lcons，主要关注证据定位。
*   <strong>阶段二（分离阶段）：</strong> 最大化Lsep，引入证据消融视图，确保模型推理的视觉基础。</p>
<p>为了生成高质量的训练数据，论文还构建了一个<strong>程序化数据构建流水线</strong>，利用图表渲染代码（如ECD [47]）来精确生成证据保留和证据消融视图，避免了昂贵的人工标注。</p>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著性能提升：</strong> 在八个基准测试中，BiPS将Qwen2.5-VL-7B模型平均提升了8.2%。</li>
<li><strong>强大的跨领域泛化能力：</strong> BiPS在未见过的数据集和图像类型上表现出强大的泛化能力，即使仅使用13K图表样本训练，也能在通用VQA任务上取得显著提升。</li>
<li><strong>数据效率高：</strong> 相比于其他需要大量图表样本的专用模型，BiPS通过增强模型核心视觉感知能力，实现了更高效的数据利用。</li>
<li><strong>克服文本捷径：</strong> 分离约束有效阻止了模型依赖文本信息，确保了推理过程的视觉基础。</li>
<li><strong>推理效率高：</strong> BiPS在训练时引入了感知塑造，而无需在推理时生成额外的视觉线索，因此不会增加推理成本。</li>
<li><strong>案例研究证明：</strong> 在图表理解和视觉计数等任务的案例研究中，BiPS能够生成更具视觉依据的答案，而基线模型则容易依赖统计线索或产生幻觉。</li>
</ul>
<p><strong>4. 局限性：</strong></p>
<ul>
<li><strong>对数据生成流水线依赖：</strong> BiPS的有效性在一定程度上依赖于程序化数据生成流水线的质量和准确性。</li>
<li><strong>训练课程的敏感性：</strong> 虽然论文提出了粗粒度到细粒度的训练课程，但实验表明，同时优化Lcons和Lsep（联合训练）或反转训练顺序可能会导致性能下降，表明训练课程的设计对最终性能有影响。</li>
<li><strong>超参数敏感性：</strong> 实验表明，过大的KL约束系数（α和β）可能会导致性能下降，说明需要适度的约束来平衡模型性能。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>更通用的数据生成：</strong> 探索更通用的程序化数据生成方法，以覆盖更广泛的视觉推理任务和数据类型。</li>
<li><strong>自适应感知塑造：</strong> 研究如何使感知塑造过程更加自适应，根据任务和数据的特性动态调整。</li>
<li><strong>与其他推理机制的结合：</strong> 探索将BiPS与其他先进的多模态推理机制（如更复杂的注意力机制、知识图谱等）相结合，以进一步提升模型性能。</li>
<li><strong>可解释性增强：</strong> 虽然BiPS提高了视觉依赖性，但进一步研究其内部机制的可解释性，理解模型如何利用塑造后的感知信号进行推理，将是有价值的方向。</li>
</ul>
<p><strong>总结：</strong></p>
<p>该论文提出了一种名为BiPS的双向感知塑造框架，通过在训练阶段引入证据保留和证据消融视图，并利用KL散度约束来塑造模型的视觉感知能力。BiPS有效地解决了现有VLMs在精细视觉证据捕捉、领域泛化和推理成本方面的问题，显著提升了模型在图表理解和多模态推理任务上的性能。其程序化数据生成方法和粗粒度到细粒度的训练课程是关键创新，为提高VLMs的视觉基础推理能力提供了新的范式。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22120v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22120v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22118v1'></a></p>
<h2 id="proedit-inversion-based-editing-from-prompts-done-right"><a href="https://arxiv.org/abs/2512.22118v1">ProEdit: Inversion-based Editing From Prompts Done Right</a></h2>
<p><strong>Authors:</strong> Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ProEdit: Inversion-based Editing From Prompts Done Right”的全面中文摘要，重点突出了其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文题目：</strong> ProEdit: Inversion-based Editing From Prompts Done Right (ProEdit：基于提示的正确反演式编辑)</p>
<p><strong>作者：</strong> Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决了现有基于反演的视觉编辑方法在进行图像和视频编辑时遇到的一个关键问题：<strong>过度注入源图像信息导致编辑失败</strong>。现有方法为了保持编辑的一致性，会在采样过程中引入大量源图像的信息。然而，这种策略过度依赖源信息，严重影响了目标图像中主体属性（如姿态、数量、颜色）的准确修改，导致编辑结果不符合用户指令。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>为了解决上述问题，作者提出了名为 <strong>ProEdit</strong> 的新颖、无需训练的编辑方法，从<strong>注意力（attention）</strong>和<strong>潜在空间（latent）</strong>两个方面进行改进：</p>
<ul>
<li>
<p><strong>注意力方面 (Attention Aspect) - KV-mix：</strong></p>
<ul>
<li>引入了 <strong>KV-mix</strong> 机制，通过混合源图像和目标图像在<strong>编辑区域</strong>的 KV 特征，来减轻源图像对编辑区域的影响，同时保持背景的一致性。</li>
<li>对于<strong>非编辑区域</strong>，则完全注入源 KV 特征，以确保背景的稳定性。</li>
<li>该机制可以应用于所有注意力操作，无需手动调整头、层或块。</li>
</ul>
</li>
<li>
<p><strong>潜在空间方面 (Latent Aspect) - Latents-Shift：</strong></p>
<ul>
<li>提出了 <strong>Latents-Shift</strong> 模块，借鉴了风格迁移中的 AdaIN（Adaptive Instance Normalization）思想。</li>
<li>通过在<strong>编辑区域</strong>的源潜在表示中注入随机噪声，来扰动其分布，从而消除反演得到的潜在表示对采样过程的影响。</li>
<li>这有助于减少源图像属性的干扰，同时保持结构和背景的一致性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 大量实验表明，ProEdit 在多个图像和视频编辑基准测试中取得了<strong>最先进 (SOTA) 的性能</strong>。</li>
<li><strong>属性编辑能力增强：</strong> 特别是在属性编辑方面，ProEdit 展现出前所未有的性能，有效解决了现有方法在此类任务上的不足。</li>
<li><strong>即插即用性 (Plug-and-play)：</strong> ProEdit 的设计是即插即用的，可以<strong>无缝集成</strong>到现有的反演和编辑方法中，如 RF-Solver、FireFlow 和 UniEdit，极大地增强了现有方法的适用性。</li>
<li><strong>一致性与准确性兼顾：</strong> ProEdit 能够同时实现<strong>高背景一致性</strong>和<strong>准确的属性编辑</strong>，解决了源图像信息注入与编辑目标之间的矛盾。</li>
<li><strong>通用性：</strong> 该方法不仅适用于图像编辑，也成功应用于视频编辑任务，证明了其<strong>通用性</strong>。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中并未明确列出局限性，但从方法描述和实验结果来看，其核心在于解决源信息注入问题。潜在的局限性可能在于：</p>
<ul>
<li><strong>计算成本：</strong> 虽然是训练-free 的，但引入的 KV-mix 和 Latents-Shift 模块可能会增加一定的计算开销。</li>
<li><strong>掩码提取的精度：</strong> 论文提到使用注意力图提取掩码，虽然经过了扩散处理，但对于非常精细的编辑区域，掩码的精度可能仍是影响最终效果的因素。</li>
<li><strong>对特定模型的依赖（潜在）：</strong> 虽然是即插即用的，但其效果可能在不同基础反演模型上表现略有差异。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的掩码控制：</strong> 探索更鲁棒和精细的编辑区域掩码提取方法，以应对更复杂的编辑场景。</li>
<li><strong>更高效的注意力融合：</strong> 进一步优化 KV-mix 机制，探索更高效的注意力特征融合策略，以在保证性能的同时降低计算复杂度。</li>
<li><strong>更广泛的模型集成：</strong> 将 ProEdit 集成到更多不同类型的生成模型（如 GANs）的反演编辑流程中，探索其通用性。</li>
<li><strong>交互式编辑的增强：</strong> 结合大型语言模型（如论文中提到的 Qwen3-8B）进行指令引导编辑，可以进一步探索更自然、更用户友好的交互式编辑方式。</li>
<li><strong>视频编辑的鲁棒性提升：</strong> 尽管在视频编辑方面表现出色，但对于更复杂的视频内容（如快速运动、遮挡等），仍有进一步提升时空一致性和编辑质量的空间。</li>
</ul>
<p><strong>总结：</strong></p>
<p>ProEdit 是一项重要的研究成果，它成功地解决了基于反演的视觉编辑方法中长期存在的源图像信息过度注入问题。通过创新的 KV-mix 和 Latents-Shift 模块，ProEdit 在保持背景一致性的同时，显著提升了编辑的准确性和灵活性，尤其是在属性编辑方面。其即插即用的特性使其能够轻松地增强现有编辑方法，为未来更强大、更易用的视觉编辑工具奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose ProEdit to address this issue both in the attention and the latent aspects.</li>
<li>In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency.</li>
<li>In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling.</li>
<li>Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22118v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22118v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22105v1'></a></p>
<h2 id="learning-association-via-track-detection-matching-for-multi-object-tracking"><a href="https://arxiv.org/abs/2512.22105v1">Learning Association via Track-Detection Matching for Multi-Object Tracking</a></h2>
<p><strong>Authors:</strong> Momir Adžemović</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-object tracking aims to maintain object identities over time by associating detections across video frames. Two dominant paradigms exist in literature: tracking-by-detection methods, which are computationally efficient but rely on handcrafted association heuristics, and end-to-end approaches, which learn association from data at the cost of higher computational complexity. We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame. TDLP is architecturally designed primarily for geometric features such as bounding boxes, while optionally incorporating additional cues, including pose and appearance. Unlike heuristic-based methods, TDLP learns association directly from data without handcrafted rules, while remaining modular and computationally efficient compared to end-to-end trackers. Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods. Finally, we provide a detailed analysis comparing link prediction with metric learning-based association and show that link prediction is more effective, particularly when handling heterogeneous features such as detection bounding boxes. Our code is available at \href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对Momir Adžemović撰写的论文“Learning Association via Track-Detection Matching for Multi-Object Tracking”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Learning Association via Track-Detection Matching for Multi-Object Tracking</p>
<p><strong>作者：</strong> Momir Adžemović</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
本文旨在解决多目标跟踪（Multi-Object Tracking, MOT）中的核心挑战：如何在视频帧之间准确地维护和关联不同对象的身份。现有的方法主要分为两类：
*   <strong>跟踪-检测（Tracking-by-Detection）方法：</strong> 计算效率高，但依赖于手工设计的关联启发式规则，泛化能力受限。
*   <strong>端到端（End-to-End）方法：</strong> 能从数据中学习关联，但计算复杂度高，训练和推理速度慢。
这两种方法之间存在一个“差距”，即需要一种既能学习关联又能保持效率和模块化特性的方法。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
作者提出了<strong>Track-Detection Link Prediction (TDLP)</strong>，一种新的跟踪-检测方法，它将数据关联问题转化为<strong>逐帧的链接预测（Link Prediction）</strong>问题。其核心创新在于：
*   <strong>逐帧链接预测：</strong> TDLP不依赖手工规则，而是通过预测每个检测框是否是现有轨迹的正确延续来学习关联。
*   <strong>架构设计：</strong> TDLP主要针对几何特征（如边界框），并可选择性地整合姿态和外观等其他线索。它采用Transformer编码器来处理时空信息和多模态特征。
*   <strong>特征融合与交互：</strong> 模型通过静态编码器、运动编码器、时间编码器和对象交互编码器来提取和融合多模态特征，并建模对象间的交互。
*   <strong>计算效率与模块化：</strong> 相较于端到端方法，TDLP在训练和推理时计算成本更低，同时保留了跟踪-检测方法的模块化优势。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能卓越：</strong> TDLP在多个具有挑战性的基准数据集（如DanceTrack, SportsMOT, BEE24）上取得了最先进的性能，超越了现有的跟踪-检测和端到端方法。
*   <strong>轻量级变体优势：</strong> 即使是仅使用边界框特征的轻量级TDLP-bbox变体，也优于所有依赖手工规则的跟踪器，甚至优于一些使用外观特征的方法。
*   <strong>对度量学习的分析：</strong> 文章深入分析了链接预测与度量学习（Metric Learning）在关联任务上的差异，并证明了在处理异构特征（尤其是边界框）时，链接预测更为有效。
*   <strong>鲁棒性：</strong> TDLP在处理非线性运动和遮挡等复杂场景时表现出更强的鲁棒性，尤其是在阈值测试中，能有效抑制假阳性检测。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>计算成本：</strong> TDLP的主要局限性在于其计算成本，这源于二次方的跟踪-检测评分和Transformer编码器。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>优化计算成本：</strong> 未来工作将探索更长的时序窗口和架构优化，以进一步降低计算成本。
*   <strong>更深入的分析：</strong> 论文中提到对MOT17数据集的局限性（训练数据不足、无验证集）需要更深入的研究。</p>
<p><strong>总结：</strong>
这篇论文提出了TDLP，一种创新的跟踪-检测方法，通过将数据关联视为逐帧的链接预测问题，成功地弥合了现有方法在效率和性能之间的差距。TDLP通过学习数据关联，避免了手工规则的限制，并能有效融合多模态特征，在多个基准测试中取得了显著的性能提升。其对链接预测与度量学习的深入分析，为理解和改进多目标跟踪中的关联机制提供了重要见解。尽管存在计算成本方面的挑战，TDLP为未来更高效、更鲁棒的多目标跟踪系统奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame.</li>
<li>Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22105v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22105v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22096v1'></a></p>
<h2 id="yume-15-a-text-controlled-interactive-world-generation-model"><a href="https://arxiv.org/abs/2512.22096v1">Yume-1.5: A Text-Controlled Interactive World Generation Model</a></h2>
<p><strong>Authors:</strong> Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Yume1.5: A Text-Controlled Interactive World Generation Model”的全面中文摘要：</p>
<p><strong>Yume1.5: 一种文本控制的交互式世界生成模型</strong></p>
<p><strong>1. 研究问题/背景</strong></p>
<p>尽管基于扩散模型的交互式和可探索世界生成方法取得了显著进展，但现有方法普遍面临严峻挑战：参数量过大、推理过程耗时过长、历史上下文急剧增长导致实时性能受限，并且缺乏精细的文本控制生成能力。这些问题严重阻碍了生成逼真、交互性强且可实时探索的虚拟世界。</p>
<p><strong>2. 主要创新点/方法论贡献</strong></p>
<p>为了解决上述问题，论文提出了 <strong>Yume1.5</strong>，一个新颖的框架，能够从单个图像或文本提示生成逼真、交互式且连续的世界。Yume1.5 的核心在于其精心设计的框架，支持通过键盘进行交互式探索，并包含三个主要组成部分：</p>
<ul>
<li><strong>长视频生成框架：</strong> 集成了统一的上下文压缩与线性注意力机制，以高效处理长视频序列，克服了传统方法中上下文长度增加带来的计算瓶颈。</li>
<li><strong>实时流加速策略：</strong> 采用双向注意力蒸馏和增强的文本嵌入方案，显著提升了生成速度和实时性，使得模型能够进行实时流式交互。</li>
<li><strong>文本控制的世界事件生成：</strong> 引入了一种文本控制方法，能够生成世界中的动态事件，极大地增强了生成内容的丰富性和可控性。</li>
</ul>
<p>具体技术贡献包括：
*   <strong>联合时空通道建模 (TSCM)：</strong> 提出了一种新的建模方法，用于高效的长视频生成，即使上下文长度增加，也能保持稳定的采样速度。
*   <strong>加速方法：</strong> 结合了 Self-Forcing 和 TSCM，加速了 Yume1.5 的推理过程，并有效减少了误差累积。
*   <strong>数据处理与模型架构设计：</strong> 通过精心设计的数据集（包括真实世界、合成和事件数据集）以及模型架构，Yume1.5 在世界生成和编辑方面取得了优越的性能。</p>
<p><strong>3. 主要结果及其意义</strong></p>
<p>Yume1.5 在多个方面取得了显著成果：</p>
<ul>
<li><strong>卓越的可控性：</strong> 在指令跟随（相机运动跟踪）方面得分达到 0.836，显著优于现有模型。</li>
<li><strong>高效的生成速度：</strong> 在单块 A100 GPU 上，以 540p 分辨率实现了平均 12 fps 的生成速度。</li>
<li><strong>高质量的长视频生成：</strong> 在长视频生成任务中，Yume1.5 能够保持更稳定的美学分数和图像质量，尤其是在处理更长的视频序列时。</li>
<li><strong>交互式探索能力：</strong> 通过键盘控制，用户可以直观地探索生成的虚拟世界，实现了真正的交互性。</li>
<li><strong>文本控制事件生成：</strong> 能够根据文本描述生成动态事件，增加了生成内容的趣味性和真实感。</li>
</ul>
<p>这些结果表明，Yume1.5 在生成逼真、交互式且可控的虚拟世界方面取得了重大突破，为沉浸式体验和虚拟环境模拟开辟了新的可能性。</p>
<p><strong>4. 提及的局限性</strong></p>
<p>论文也指出了 Yume1.5 的一些局限性：</p>
<ul>
<li><strong>生成伪影：</strong> 在某些场景下，模型仍会产生一些生成伪影，例如车辆倒行或角色反向行走。</li>
<li><strong>高密度场景性能下降：</strong> 在极高人群密度的场景下，生成性能会有所下降。</li>
<li><strong>模型规模限制：</strong> 尽管通过 TSCM 缓解了部分问题，但 5B 参数模型的容量仍然有限，进一步提升分辨率会增加生成延迟。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>论文展望了未来的研究方向：</p>
<ul>
<li><strong>扩展到更复杂的交互：</strong> 支持更复杂的虚拟世界交互和更广泛的应用场景，例如虚拟环境和模拟系统。</li>
<li><strong>探索更大模型架构：</strong> 考虑使用 Mixture-of-Experts (MoE) 等架构来增加模型参数量，同时降低推理延迟，以应对更复杂的生成任务。</li>
<li><strong>提升生成质量：</strong> 进一步研究以消除生成伪影，并提高在高密度场景下的性能。</li>
</ul>
<p>总而言之，Yume1.5 是一个在文本控制的交互式世界生成领域具有重要意义的模型。它通过创新的长视频生成框架、实时加速策略和文本控制事件生成能力，有效解决了现有方法的关键挑战，并为未来更逼真、更具交互性的虚拟世界生成奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22096v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22096v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22065v1'></a></p>
<h2 id="streamavatar-streaming-diffusion-models-for-real-time-interactive-human-avatars"><a href="https://arxiv.org/abs/2512.22065v1">StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</a></h2>
<p><strong>Authors:</strong> Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.HC</p>
<p><strong>Abstract:</strong></p>
<p>Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: https://streamavatar.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars”的全面中文摘要：</p>
<p><strong>论文题目：</strong> StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars</p>
<p><strong>作者：</strong> Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决数字人类研究中的一个关键且具有挑战性的目标：实现<strong>实时、流式交互式人类虚拟形象</strong>。现有的基于扩散模型的虚拟形象生成方法虽然在生成质量上表现出色，但其<strong>非因果（non-causal）架构和高计算成本</strong>使其不适用于流式生成。此外，现有的交互式方法通常<strong>局限于头部和肩部区域</strong>，限制了其生成手势和身体动作的能力。因此，研究的核心问题是如何在保持高质量生成的同时，实现低延迟、流式、全身交互式虚拟形象的生成。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了应对上述挑战，作者提出了一个名为 <strong>StreamAvatar</strong> 的框架，其核心创新在于一个<strong>两阶段的自回归适应与加速框架</strong>：</p>
<ul>
<li>
<p><strong>阶段一：自回归蒸馏（Autoregressive Distillation）</strong></p>
<ul>
<li><strong>模型重构：</strong> 将原有的双向扩散模型（Diffusion Transformer, DiT）重构为<strong>块因果（block-causal）的DiT</strong>，以支持自回归生成。</li>
<li><strong>蒸馏过程：</strong> 利用<strong>分数身份蒸馏（Score Identity Distillation, SiD）</strong>技术，将一个强大的、但速度较慢的双向教师模型（teacher model）的生成能力蒸馏到一个快速的、单向的（因果）学生模型（student model）中。这显著减少了推理步骤，将DiT的去噪过程加速了40倍。</li>
<li><strong>长期稳定性与一致性组件：</strong><ul>
<li><strong>参考槽（Reference Sink）：</strong> 强制模型持续关注参考帧，以防止在长视频生成中出现身份漂移。</li>
<li><strong>参考锚定位置编码（Reference-Anchored Positional Re-encoding, RAPR）：</strong> 解决训练-测试不匹配和注意力衰减问题，通过限制位置编码的最大距离来模拟长视频位置偏移，从而提高长序列生成的一致性。</li>
<li><strong>一致性感知判别器（Consistency-Aware Discriminator）：</strong> 在第二阶段用于提升生成质量和稳定性。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>阶段二：对抗性精炼（Adversarial Refinement）</strong></p>
<ul>
<li>为了解决蒸馏过程中可能出现的质量下降（如模糊、失真）和时间不一致问题，引入了<strong>一致性感知判别器</strong>。该判别器包含局部真实性分支和全局一致性分支，以评估生成帧的真实性和整体一致性。</li>
</ul>
</li>
<li>
<p><strong>交互式能力增强：</strong></p>
<ul>
<li><strong>音频掩码（Audio Mask）：</strong> 采用TalkNet生成的音频掩码来区分说话和倾听阶段，而非直接分离音频。这避免了音频特征的失真，并提供了精确的时间控制。</li>
<li><strong>音频注意力模块：</strong> 在Transformer块中引入了两个音频相关注意力模块：<strong>音频注意力（Audio Attention）</strong>用于驱动说话阶段的表情和动作，<strong>交互音频注意力（Interact Audio Attention）</strong>用于生成倾听阶段的自然反应。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>实时性与效率：</strong> StreamAvatar实现了<strong>实时流式生成</strong>，RTF值远低于1，整体延迟仅为1.20秒，比现有方法快几个数量级。
*   <strong>高质量生成：</strong> 在FID、FVD、ASE、IQA等指标上，StreamAvatar在短视频和长视频生成上均取得了<strong>最先进（state-of-the-art）或具有竞争力</strong>的性能，尤其在生成质量、运动幅度方面表现优异。
*   <strong>交互自然性：</strong> 模型能够生成<strong>自然、连贯的说话和倾听行为</strong>，包括丰富的表情、手势和身体动作，并且在说话和倾听状态之间实现<strong>平滑过渡</strong>。
*   <strong>长期一致性：</strong> 通过引入Reference Sink和RAPR，模型在长视频生成中表现出<strong>更好的身份保持和时间一致性</strong>。
*   <strong>全身生成：</strong> 与许多仅限于头部和肩部的方法不同，StreamAvatar能够生成<strong>全身</strong>的虚拟形象。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>有限的时间上下文：</strong> 在某些区域，如果这些区域在长时间内被遮挡，模型可能<strong>难以生成一致的内容</strong>。
*   <strong>VAE解码的计算瓶颈：</strong> VAE解码占用了模型总处理时间的一半以上，未来可以探索<strong>更高效的VAE解码</strong>方法来进一步降低流式延迟。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>长时记忆机制：</strong> 引入长时记忆机制来解决因有限时间上下文导致的区域不一致问题。
*   <strong>高效VAE解码：</strong> 探索更优化的VAE解码策略，以进一步提升流式生成的速度。
*   <strong>伦理考量与安全：</strong> 作者强调了该技术可能被滥用的风险（如制造虚假身份、欺诈等），并承诺通过水印、明确披露合成内容等方式来缓解这些风险。未来的工作将致力于与社区合作，开发更先进的深度伪造检测工具，并建立媒体溯源标准。</p>
<p><strong>总结：</strong>
StreamAvatar通过创新的两阶段蒸馏和精炼框架，以及用于提升长期稳定性和交互性的关键组件，成功地解决了现有扩散模型在实时流式、全身交互式虚拟形象生成方面的瓶颈。该方法在生成质量、效率和交互自然性方面均取得了显著的进步，为数字人类和虚拟交互领域带来了重要的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming.</li>
<li>To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator.</li>
<li>Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures.</li>
<li>Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22065v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22065v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22046v1'></a></p>
<h2 id="backdoor-attacks-on-prompt-driven-video-segmentation-foundation-models"><a href="https://arxiv.org/abs/2512.22046v1">Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</a></h2>
<p><strong>Authors:</strong> Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV, cs.CR</p>
<p><strong>Abstract:</strong></p>
<p>Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models</p>
<p><strong>作者：</strong> Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang</p>
<p><strong>摘要：</strong></p>
<p>这篇论文<strong>首次提出了针对提示驱动视频分割基础模型（VSFMs）的后门攻击框架 BadVSFM</strong>，揭示了这些模型在安全方面存在的严重漏洞。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>随着像SAM2这样的提示驱动VSFM在自动驾驶和数字病理等领域的广泛应用，其鲁棒性问题日益受到关注，特别是后门攻击的威胁。然而，研究人员发现，将经典的后门攻击方法（如BadNet）直接应用于VSFM时，效果非常差，攻击成功率（ASR）低于5%。论文深入分析了这一失败现象，发现传统后门攻击训练下，图像编码器的干净样本和触发样本的梯度高度对齐，模型注意力仍然集中在真实目标上，未能学习到与触发器相关的独特表示。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<p>为了克服上述挑战，论文提出了<strong>BadVSFM</strong>，一个专门为提示驱动VSFM设计的两阶段后门攻击框架：</p>
<ul>
<li><strong>阶段一：图像编码器对齐。</strong> 该阶段仅微调图像编码器，使得触发帧的嵌入映射到一个预设的目标嵌入，同时保持干净帧的嵌入与一个干净参考模型对齐，以保留模型效用。</li>
<li><strong>阶段二：掩码解码器训练。</strong> 该阶段仅更新掩码解码器，使得跨不同提示类型（点、框、掩码）的触发帧-提示对能够生成一个共享的目标掩码（例如，全零掩码），同时保持干净帧的分割行为与参考解码器一致。</li>
</ul>
<p>该框架通过<strong>显式地分离触发样本和干净样本的表示空间，并条件化解码器以生成目标掩码</strong>，实现了有效的后门注入。</p>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>强大的后门攻击效果：</strong> 在DAVIS和LVOS两个数据集上，使用多种VSFM模型（包括SAM2、MedSAM2、SAM2-Long、BioSAM2和EdgeTAM）进行的广泛实验表明，BadVSFM能够实现<strong>强大且可控的后门效果</strong>，显著高于现有基线后门攻击方法，ASR提升高达90%以上。</li>
<li><strong>保持模型效用：</strong> 尽管实现了高ASR，BadVSFM在保持模型在干净数据上的分割性能（mIoU和J&amp;F指标）方面表现出色，仅有微小的下降，甚至在某些情况下有所提升。</li>
<li><strong>鲁棒性与必要性：</strong> 通过对损失函数、训练阶段、攻击目标、触发器配置和中毒率的系统性消融研究，证明了BadVSFM对合理的超参数变化具有鲁棒性，并且<strong>两阶段设计对于同时保证后门效果和干净性能至关重要</strong>。</li>
<li><strong>解释性分析：</strong> 梯度冲突分析和注意力图可视化表明，BadVSFM能够<strong>将触发样本和干净样本的表示推向不同的方向，并将注意力转移到触发区域</strong>，这解释了为何传统后门攻击在VSFM上失效。</li>
<li><strong>现有防御措施的无效性：</strong> 四种代表性的防御方法（微调、剪枝、Spectral Signatures和STRIP）在对抗BadVSFM时<strong>效果不佳</strong>，凸显了当前VSFM面临的实际且未被充分探索的安全漏洞。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>模型覆盖范围有限：</strong> 实验主要集中在有限的VSFM模型上，未来研究需要扩展到更多模型架构，以验证攻击原理的普适性。</li>
<li><strong>数据集多样性有限：</strong> 评估主要基于DAVIS和LVOS两个数据集，未来需要更多样化的数据集来评估模型的泛化能力。</li>
<li><strong>防御策略探索有限：</strong> 仅评估了四种通用防御技术，揭示了现有通用防御对VSFM的不足，需要开发更具针对性的防御策略。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的模型和数据集评估：</strong> 探索BadVSFM在更多不同类型和复杂场景下的VSFM模型上的表现。</li>
<li><strong>开发针对性的防御机制：</strong> 设计专门用于VSFM的后门防御和检测方法，例如基于时空异常检测或触发器反演的方法。</li>
<li><strong>深入理解VSFM的脆弱性：</strong> 进一步研究VSFM在复杂时空条件下的脆弱性，以及如何增强其对后门威胁的抵抗力。</li>
<li><strong>探索更隐蔽的攻击方式：</strong> 研究如何设计更难被检测到的触发器和攻击策略。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文<strong>首次系统地提出了针对提示驱动视频分割基础模型（VSFM）的后门攻击方法BadVSFM</strong>，并深入分析了传统后门攻击失效的原因。研究结果表明，BadVSFM能够有效地在VSFM中植入后门，同时保持模型在干净数据上的性能，并且现有的通用防御方法难以有效抵御。这揭示了VSFM领域一个<strong>被低估的安全风险</strong>，并强调了开发<strong>专门针对VSFM的后门攻击和防御策略</strong>的紧迫性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22046v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22046v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22010v1'></a></p>
<h2 id="longfly-long-horizon-uav-vision-and-language-navigation-with-spatiotemporal-context-integration"><a href="https://arxiv.org/abs/2512.22010v1">LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</a></h2>
<p><strong>Authors:</strong> Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Unmanned aerial vehicles (UAVs) are crucial tools for post-disaster search and rescue, facing challenges such as high information density, rapid changes in viewpoint, and dynamic structures, especially in long-horizon navigation. However, current UAV vision-and-language navigation(VLN) methods struggle to model long-horizon spatiotemporal context in complex environments, resulting in inaccurate semantic alignment and unstable path planning. To this end, we propose LongFly, a spatiotemporal context modeling framework for long-horizon UAV VLN. LongFly proposes a history-aware spatiotemporal modeling strategy that transforms fragmented and redundant historical data into structured, compact, and expressive representations. First, we propose the slot-based historical image compression module, which dynamically distills multi-view historical observations into fixed-length contextual representations. Then, the spatiotemporal trajectory encoding module is introduced to capture the temporal dynamics and spatial structure of UAV trajectories. Finally, to integrate existing spatiotemporal context with current observations, we design the prompt-guided multimodal integration module to support time-based reasoning and robust waypoint prediction. Experimental results demonstrate that LongFly outperforms state-of-the-art UAV VLN baselines by 7.89\% in success rate and 6.33\% in success weighted by path length, consistently across both seen and unseen environments.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration</p>
<p><strong>作者：</strong> Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, and Xiangyang Ji</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决无人机（UAV）在执行长距离、复杂环境下的视觉与语言导航（VLN）任务时遇到的核心挑战。现有UAV VLN方法在处理长距离导航时，难以有效建模时空上下文信息，导致语义对齐不准确、路径规划不稳定，尤其是在信息密度高、视角变化快、结构动态复杂的场景下表现尤为明显。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
作者提出了<strong>LongFly</strong>，一个专门为长距离UAV VLN设计的时空上下文建模框架。其核心创新在于引入了<strong>历史感知时空建模策略</strong>，将零散且冗余的历史数据转化为结构化、紧凑且富有表现力的表示。具体而言，LongFly包含三个关键模块：</p>
<ul>
<li><strong>基于槽的历史图像压缩（SHIC）模块：</strong> 该模块动态地从多视角历史观测中提取关键信息，将其压缩成固定长度的上下文表示。这解决了直接存储高维历史特征带来的计算成本问题，并能捕捉持久的地标和空间布局。</li>
<li><strong>时空轨迹编码（STE）模块：</strong> 该模块用于捕捉UAV轨迹的时间动态和空间结构。它将历史航点信息转化为轨迹令牌，以显式的运动先验来捕捉长距离路径的演变。</li>
<li><strong>提示引导的多模态融合（PGM）模块：</strong> 该模块用于整合现有的时空上下文（历史视觉记忆和运动历史）与当前观测。它将多模态上下文组织成一个结构化的提示，并利用大型多模态语言模型（MLLM）进行时间推理和鲁棒的航点预测，从而实现指令对齐和长距离导航的一致性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
LongFly在OpenUAV数据集上进行了广泛的实验评估，并在<strong>测试集（Unseen）</strong>上取得了显著的性能提升。与最先进的UAV VLN基线方法相比，LongFly在<strong>成功率（SR）上提升了7.89%</strong>，在<strong>路径长度加权的成功率（SPL）上提升了6.33%</strong>。这些改进在<strong>所有难度级别（Full、Easy、Hard）</strong>以及<strong>所有未见场景（Overall、Object、Map）</strong>中都得到了一致的体现。</p>
<ul>
<li><strong>意义重大：</strong> 这些结果表明，LongFly提出的时空上下文建模策略对于提升UAV在复杂、长距离导航任务中的鲁棒性、准确性和效率至关重要。它有效地解决了现有方法在处理长距离依赖性时的不足，为实现更自主、更可靠的UAV导航提供了新的解决方案。尤其是在处理复杂布局、长距离依赖和语义模糊的场景下，LongFly展现出强大的优势。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到，尽管LongFly取得了显著进展，但与人类的表现相比仍存在差距。此外，在<strong>未见环境（Unseen Environments）</strong>上的泛化能力仍有待提高，作者指出环境分布的偏移比物体类别的偏移更具挑战性，暗示了增加训练环境多样性的重要性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于论文的发现和局限性，潜在的未来研究方向包括：</p>
<ul>
<li><strong>增强对未见环境的泛化能力：</strong> 通过引入更多样化的训练环境和数据增强技术来提高模型在全新场景下的适应性。</li>
<li><strong>进一步提升与人类表现的差距：</strong> 探索更先进的建模技术或更精细的指令理解机制，以缩小与人类导航员的性能差距。</li>
<li><strong>探索更高效的上下文压缩与融合机制：</strong> 尽管SHIC模块已有效压缩历史信息，但仍可研究更轻量级或更具信息保留能力的压缩方法。同时，探索更高效的多模态融合策略，以进一步降低计算复杂度。</li>
<li><strong>更广泛的应用场景探索：</strong> 将LongFly框架扩展到其他需要长距离、复杂环境导航的机器人应用中，例如地面机器人或水下机器人。</li>
<li><strong>实时性与效率的进一步优化：</strong> 对于实际的UAV应用，实时性至关重要。未来研究可以关注如何进一步优化模型的推理速度，使其能够满足实时导航的需求。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose LongFly, a spatiotemporal context modeling framework for long-horizon UAV VLN.</li>
<li>First, we propose the slot-based historical image compression module, which dynamically distills multi-view historical observations into fixed-length contextual representations.</li>
<li>Experimental results demonstrate that LongFly outperforms state-of-the-art UAV VLN baselines by 7.89\% in success rate and 6.33\% in success weighted by path length, consistently across both seen and unseen environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22010v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22010v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.22009v1'></a></p>
<h2 id="ishift-lightweight-slow-fast-gui-agent-with-adaptive-perception"><a href="https://arxiv.org/abs/2512.22009v1">iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception</a></h2>
<p><strong>Authors:</strong> Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) show strong potential for interpreting and interacting with complex, pixel-rich Graphical User Interface (GUI) environments. However, building agents that are both efficient for high-level tasks and precise for fine-grained interactions remains challenging. GUI agents must perform routine actions efficiently while also handling tasks that demand exact visual grounding, yet existing approaches struggle when accuracy depends on identifying specific interface elements. These MLLMs also remain large and cannot adapt their reasoning depth to the task at hand. In this work, we introduce iSHIFT: Implicit Slow-fast Hybrid Inference with Flexible Tokens, a lightweight agent that integrates latent thinking (implicit chain-of-thought) with a perception control module. iSHIFT enables an MLLM to switch between a slow mode, which leverages detailed visual grounding for high precision and a fast mode that uses global cues for efficiency. Special perception tokens guide attention to relevant screen regions, allowing the model to decide both how to reason and where to focus. Despite its compact 2.5B size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文标题：</strong> iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception
<strong>作者：</strong> Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian
<strong>分类：</strong> cs.CV
<strong>发表日期：</strong> 2025-12-26</p>
<hr />
<h3 id="_1">论文分析</h3>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种名为 iSHIFT 的轻量级 GUI 智能体，它通过结合隐式链式思考（latent thinking）和自适应感知控制模块，实现了在不同任务复杂度下推理速度和精度的权衡。iSHIFT 能够根据任务需求在“慢速模式”（高精度视觉细节）和“快速模式”（全局线索效率）之间切换，并通过特殊的感知 token 来引导模型关注屏幕的关键区域，从而在保持模型紧凑（2.5B 参数）的同时，达到了与现有最先进方法相当的性能。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>iSHIFT 的核心创新在于其 <strong>“隐式慢-快混合推理与灵活 token”（Implicit Slow-fast Hybrid Inference with Flexible Tokens）</strong> 的设计理念，具体体现在以下几个方面：</p>
<ul>
<li>
<p><strong>慢-快推理模式切换 (Slow-Fast Inference Modes):</strong> 这是最核心的创新点。</p>
<ul>
<li><strong>慢速模式 (Slow Mode):</strong> 强调 <strong>详细的视觉接地 (detailed visual grounding)</strong>。这意味着模型会深入分析界面元素的细节，例如文本内容、图标形状、布局关系等，以实现高精度操作，尤其适用于需要精确识别特定界面元素（如点击某个按钮、填写特定文本框）的任务。</li>
<li><strong>快速模式 (Fast Mode):</strong> 侧重于利用 <strong>全局线索 (global cues)</strong> 来提高效率。模型会从整体上理解界面布局和上下文信息，快速做出判断，适用于对精度要求不高但需要快速响应的任务，例如滚动页面、识别整体页面类型等。</li>
<li><strong>自适应切换机制:</strong> 模型能够根据任务的性质和当前状态，<strong>动态地在两种模式之间切换</strong>，从而在效率和精度之间找到最佳平衡点。</li>
</ul>
</li>
<li>
<p><strong>感知控制模块与特殊感知 Token (Perception Control Module &amp; Special Perception Tokens):</strong></p>
<ul>
<li><strong>引导注意力 (Guiding Attention):</strong> iSHIFT 引入了 <strong>特殊的感知 token</strong>，这些 token 并非传统的视觉特征或语言 token，而是被设计用来 <strong>显式地指导模型将注意力集中到屏幕上的相关区域</strong>。这使得模型能够主动地“决定”应该关注界面的哪些部分，而不是被动地处理所有像素信息。</li>
<li><strong>决策推理与视觉焦点结合:</strong> 这些感知 token 不仅帮助模型进行视觉聚焦，还与模型的推理过程相结合，<strong>共同决定“如何推理”和“在哪里聚焦”</strong>。这是一种更主动、更具策略性的感知方式。</li>
</ul>
</li>
<li>
<p><strong>隐式链式思考 (Implicit Chain-of-Thought / Latent Thinking):</strong> 论文提到“latent thinking”，这暗示模型在内部进行一种类似链式思考的推理过程，但这种思考是<strong>隐式的</strong>，不需要显式地生成中间的推理步骤。这有助于模型在不增加模型复杂度的前提下，提升其逻辑推理能力，尤其是在处理多步操作或复杂决策时。</p>
</li>
<li>
<p><strong>轻量级设计 (Lightweight Design):</strong> 尽管 iSHIFT 能够达到 SOTA 性能，但其参数量仅为 <strong>2.5B</strong>。这表明其设计在模型效率方面做了大量优化，使其更易于部署和运行，尤其是在资源受限的环境中。</p>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>iSHIFT 的研究对计算机视觉和多模态大模型领域具有重要的潜在影响：</p>
<ul>
<li><strong>推动 GUI 智能体的发展:</strong> GUI 智能体是实现人机交互自动化和增强用户体验的关键。iSHIFT 的方法为构建更高效、更智能的 GUI 智能体提供了新的思路，尤其是在处理复杂、动态的 GUI 环境方面。</li>
<li><strong>提升 MLLMs 的效率和适应性:</strong> 现有的 MLLMs 通常参数量巨大，且推理速度较慢。iSHIFT 的轻量级设计和自适应推理模式，证明了在不牺牲性能的前提下，可以显著提升 MLLMs 的效率和对不同任务的适应性。</li>
<li><strong>开创主动感知的新范式:</strong> 通过引入特殊的感知 token 来引导注意力，iSHIFT 提出了一种更主动、更具策略性的视觉感知方式，这可能启发未来在其他视觉任务中设计更智能的注意力机制。</li>
<li><strong>降低部署门槛:</strong> 2.5B 的参数量使得 iSHIFT 更有可能被部署到边缘设备或对计算资源有严格要求的场景，从而加速 MLLMs 的实际应用。</li>
<li><strong>为“决策式感知”提供理论支持:</strong> iSHIFT 的方法将“决策”与“感知”紧密结合，模型不仅感知，还根据任务目标主动决定感知什么，这为“决策式感知”或“目标驱动感知”的研究提供了实践案例。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>自动化软件测试 (Automated Software Testing):</strong> 能够更高效、更精确地模拟用户操作，发现软件中的 bug。</li>
<li><strong>机器人导航与交互 (Robotics Navigation and Interaction):</strong> 机器人可以通过理解和操作 GUI 来与软件系统交互，完成更复杂的任务。</li>
<li><strong>无障碍技术 (Accessibility Technologies):</strong> 帮助视障人士或其他有特殊需求的用户更便捷地与数字界面交互。</li>
<li><strong>远程协助与支持 (Remote Assistance and Support):</strong> 远程操作员可以更准确地指导用户完成软件操作。</li>
<li><strong>游戏 AI (Game AI):</strong> 游戏中的 NPC 可以更智能地理解和操作游戏界面，提升游戏体验。</li>
<li><strong>智能助手与自动化工作流 (Intelligent Assistants and Automated Workflows):</strong> 构建更强大的自动化工具，处理日常的电脑操作任务。</li>
<li><strong>教育与培训 (Education and Training):</strong> 创建交互式教程，指导用户学习软件操作。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了 iSHIFT 的诸多优点，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>“隐式”推理的解释性问题:</strong> “Latent thinking”或隐式链式思考虽然提高了效率，但可能降低了模型决策过程的可解释性。当出现错误时，可能难以追溯具体原因。</li>
<li><strong>感知 Token 的设计与泛化性:</strong> 特殊感知 token 的设计可能需要针对特定类型的 GUI 或任务进行优化。其泛化能力（能否轻松应用于完全不同类型的界面）有待验证。</li>
<li><strong>模式切换的鲁棒性:</strong> 慢-快模式的切换机制虽然是优点，但其切换的准确性和鲁棒性（在模糊或不确定的情况下能否正确切换）是关键。如果切换不当，可能导致性能下降。</li>
<li><strong>对“高层任务”和“细粒度交互”的定义:</strong> 摘要提到“high-level tasks”和“fine-grained interactions”，但具体界限和模型在不同复杂度任务上的表现差异，需要更详细的实验数据来支撑。</li>
<li><strong>“紧凑”的定义:</strong> 2.5B 参数量在 MLLMs 中确实算紧凑，但与传统的 CV 模型相比仍可能较大。其在极度资源受限环境下的表现仍需考量。</li>
<li><strong>基准数据集的局限性:</strong> 摘要提到“multiple benchmark datasets”，但这些数据集是否能完全代表真实世界 GUI 交互的复杂性和多样性，以及模型在未见过的新型 GUI 上的泛化能力，是需要进一步研究的。</li>
<li><strong>“状态-of-the-art”的定义:</strong> 摘要声称“matches state-of-the-art performance”，这通常意味着在特定指标上达到或超越现有最佳水平。但“state-of-the-art”的定义本身可能随时间推移而变化，并且可能只在某些特定任务或数据集上成立。</li>
</ul>
<p>总而言之，iSHIFT 是一项非常有前景的研究，它通过创新的慢-快推理模式和主动感知机制，有效地解决了 GUI 智能体在效率和精度之间的权衡问题，并实现了轻量化设计。这为未来更智能、更易于部署的 MLLMs 在实际应用中开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce iSHIFT: Implicit Slow-fast Hybrid Inference with Flexible Tokens, a lightweight agent that integrates latent thinking (implicit chain-of-thought) with a perception control module.</li>
<li>Despite its compact 2.5B size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.22009v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.22009v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21999v1'></a></p>
<h2 id="look-closer-an-adversarial-parametric-editing-framework-for-hallucination-mitigation-in-vlms"><a href="https://arxiv.org/abs/2512.21999v1">Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</a></h2>
<p><strong>Authors:</strong> Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He</p>
<p><strong>Published:</strong> 2025-12-26</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inherently limits their optimization potential. To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \textbf{A}ctivate-\textbf{L}ocate-\textbf{E}dit \textbf{A}dversarially paradigm. Specifically, we first construct an activation dataset that comprises grounded responses (positive samples attentively anchored in visual features) and hallucinatory responses (negative samples reflecting LLM prior bias and internal knowledge artifacts). Next, we identify critical hallucination-prone parameter clusters by analyzing differential hidden states of response pairs. Then, these clusters are fine-tuned using prompts injected with adversarial tuned prefixes that are optimized to maximize visual neglect, thereby forcing the model to prioritize visual evidence over inherent parametric biases. Evaluations on both generative and discriminative VLM tasks demonstrate the significant effectiveness of ALEAHallu in alleviating hallucinations. Our code is available at https://github.com/hujiayu1223/ALEAHallu.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于视觉语言模型（VLM）幻觉缓解的论文的全面中文摘要，重点关注其研究问题、方法、结果、局限性和未来方向。</p>
<p><strong>论文题目：</strong> Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs</p>
<p><strong>作者：</strong> Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/研究动机：</strong></p>
<p>视觉语言模型（VLM）在多模态应用中展现出巨大潜力，但普遍存在“幻觉”问题，即生成的文本内容与输入的视觉信息不符。现有研究认为幻觉主要源于VLM过度依赖语言先验知识而忽视视觉特征整合。现有的缓解策略多为启发式解码校准，其不可训练的特性限制了优化潜力。因此，本文旨在提出一种<strong>可优化且高效</strong>的方法来解决VLM的幻觉问题。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>本文提出了一种名为 <strong>ALEAHallu</strong>（Activate-Locate-Edit Adversarially）的对抗性参数编辑框架，用于缓解VLM的幻觉问题。其核心创新在于：</p>
<ul>
<li><strong>激活-定位-编辑（Activate-Locate-Edit）的对抗性范式：</strong><ul>
<li><strong>激活数据集构建：</strong> 创造一个包含“接地响应”（视觉特征锚定）和“幻觉响应”（语言先验偏见）的配对数据集。</li>
<li><strong>编辑区域定位：</strong> 通过分析正负响应对在模型隐藏状态下的差异，识别出与幻觉最相关的关键参数簇。</li>
<li><strong>对抗性参数编辑：</strong> 利用对抗性调优的前缀（Adversarial Prefix Tuning）来微调这些关键参数簇。该前缀被优化以最大化模型对视觉信息的“忽视”，从而迫使模型在标准提示下更优先考虑视觉证据，而非固有的参数偏见。</li>
</ul>
</li>
<li><strong>对抗性前缀调优（Adversarial Prefix Tuning）：</strong> 提出了一种自动优化对抗性前缀的方法，避免了手动设计的前缀的局限性，使其成为一个可学习的组件。</li>
<li><strong>参数高效性：</strong> 该方法仅编辑少量关键参数簇，而非全局微调，从而在不增加额外推理开销的情况下实现幻觉缓解。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著的幻觉缓解效果：</strong> 在图像描述生成和视觉问答（VQA）等生成性和判别性任务上，ALEAHallu均显著降低了幻觉率。</li>
<li><strong>提升视觉注意力：</strong> 实验表明，ALEAHallu能够有效提升VLM对图像内容的关注度，使模型更倾向于依赖视觉信息。</li>
<li><strong>跨数据集泛化能力：</strong> 在不同数据集上的实验证明了ALEAHallu的泛化能力，表明其编辑效果可以跨越不同数据集。</li>
<li><strong>高效性：</strong> 由于仅编辑少量参数且不增加推理开销，ALEAHallu在实际应用中具有很高的效率。</li>
<li><strong>意义：</strong> 本研究为解决VLM幻觉问题提供了一种新颖且有效的解决方案，有助于提升VLM在医疗、教育、媒体等高风险领域的可靠性和安全性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中并未明确提及ALEAHallu的局限性，但从其方法论和实验结果来看，可以推测：</p>
<ul>
<li><strong>数据集依赖性：</strong> 激活数据集的质量和规模可能影响编辑效果。</li>
<li><strong>过度缓解的风险：</strong> 虽然论文强调了平衡，但过度缓解幻觉可能在某些创意领域限制模型的想象力。</li>
<li><strong>对特定模型架构的适应性：</strong> 该方法主要针对Transformer架构的VLM，其在其他架构上的适用性有待验证。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>课程学习（Curriculum Learning）：</strong> 论文作者计划集成课程学习，以进一步探索知识编辑在幻觉抑制方面的性能上限。</li>
<li><strong>更广泛的应用领域：</strong> 将该方法应用于更多下游任务和更复杂的VLM架构。</li>
<li><strong>平衡事实性与创造性：</strong> 在缓解幻觉的同时，探索如何更好地平衡事实准确性与模型的创造性表达。</li>
<li><strong>可解释性增强：</strong> 进一步研究ALEAHallu如何影响模型的内部机制，以提供更深入的可解释性。</li>
</ul>
<hr />
<p>总而言之，这篇论文提出了一种创新的对抗性参数编辑框架ALEAHallu，通过激活-定位-编辑的范式，有效地解决了VLM的幻觉问题，并在多个任务上取得了显著成果，为提升VLM的可靠性提供了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \textbf{A}ctivate-\textbf{L}ocate-\textbf{E}dit \textbf{A}dversarially paradigm.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21999v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21999v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-29 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
