<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-21 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-20/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-22/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-21">Arxiv Computer Vision Papers - 2025-10-21</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#a-comprehensive-survey-on-world-models-for-embodied-ai" class="nav-link">A Comprehensive Survey on World Models for Embodied AI</a>
                </li>
                <li class="nav-item">
                    <a href="#glyph-scaling-context-windows-via-visual-text-compression" class="nav-link">Glyph: Scaling Context Windows via Visual-Text Compression</a>
                </li>
                <li class="nav-item">
                    <a href="#ultracua-a-foundation-model-for-computer-use-agents-with-hybrid-action" class="nav-link">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a>
                </li>
                <li class="nav-item">
                    <a href="#botany-bot-digital-twin-monitoring-of-occluded-and-underleaf-plant-structures-with-gaussian-splats" class="nav-link">Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</a>
                </li>
                <li class="nav-item">
                    <a href="#elastic-vits-from-pretrained-models-without-retraining" class="nav-link">Elastic ViTs from Pretrained Models without Retraining</a>
                </li>
                <li class="nav-item">
                    <a href="#gas-improving-discretization-of-diffusion-odes-via-generalized-adversarial-solver" class="nav-link">GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</a>
                </li>
                <li class="nav-item">
                    <a href="#multilingual-text-to-image-person-retrieval-via-bidirectional-relation-reasoning-and-aligning" class="nav-link">Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning</a>
                </li>
                <li class="nav-item">
                    <a href="#picabench-how-far-are-we-from-physically-realistic-image-editing" class="nav-link">PICABench: How Far Are We from Physically Realistic Image Editing?</a>
                </li>
                <li class="nav-item">
                    <a href="#camit-a-time-aware-car-model-dataset-for-classification-and-generation" class="nav-link">CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#wp-cracknet-a-collaborative-adversarial-learning-framework-for-end-to-end-weakly-supervised-road-crack-detection" class="nav-link">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-21">Arxiv Computer Vision Papers - 2025-10-21</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月19日Arxiv计算机视觉领域论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解最新进展。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域每日报告执行摘要 (2025年10月19日)</strong></p>
<p><strong>概述：</strong>
今日发布的10篇论文展现了计算机视觉和机器学习领域多个前沿方向的活跃研究。主要趋势包括：<strong>具身智能与世界模型</strong>的深入探索、<strong>多模态与长上下文窗口处理</strong>的创新、<strong>基础模型在复杂任务中的应用</strong>、<strong>3D视觉与数字孪生</strong>的精细化建模，以及<strong>模型效率与泛化能力</strong>的提升。此外，<strong>图像生成与编辑的物理真实性</strong>、<strong>特定领域数据集构建</strong>和<strong>弱监督学习</strong>也持续受到关注。</p>
<p><strong>主要主题与趋势：</strong></p>
<ol>
<li><strong>具身智能与世界模型 (Embodied AI &amp; World Models):</strong> 多篇论文聚焦于如何让AI更好地理解和操作物理世界。特别是“世界模型”作为具身智能的核心组件，其综述和应用是重要方向。</li>
<li><strong>多模态与长上下文处理 (Multimodal &amp; Long Context):</strong> 随着大模型的发展，如何有效处理和压缩视觉-文本等多模态信息，以支持更长的上下文窗口，成为提升模型能力的关键。</li>
<li><strong>基础模型与代理 (Foundation Models &amp; Agents):</strong> 基础模型正被应用于更复杂的“代理”任务，例如计算机使用代理，这预示着AI在自动化复杂操作方面的潜力。</li>
<li><strong>3D视觉与数字孪生 (3D Vision &amp; Digital Twins):</strong> 利用高斯泼溅等新技术进行精细的3D重建和监测，尤其是在遮挡和复杂环境下的应用，是3D视觉领域的新亮点。</li>
<li><strong>模型效率与泛化 (Model Efficiency &amp; Generalization):</strong> 如何在不重新训练的情况下，使现有模型更具弹性或提高其泛化能力，是优化资源和提升实用性的重要研究。</li>
</ol>
<p><strong>特别重要或创新论文：</strong></p>
<ul>
<li><strong>"Glyph: Scaling Context Windows via Visual-Text Compression" by Jiale Cheng et al.:</strong> 这篇论文在多模态大模型背景下极具创新性。它提出了一种通过视觉-文本压缩来扩展上下文窗口的方法，有望显著提升多模态模型的处理能力和效率，对于处理长篇文档、视频理解等任务具有重要意义。</li>
<li><strong>"UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action" by Yuhao Yang et al.:</strong> 这篇论文代表了基础模型在具身智能和自动化领域的一个重要进展。构建一个能够执行混合动作的计算机使用代理，是实现通用AI助手和自动化复杂数字任务的关键一步。</li>
<li><strong>"Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats" by Simeon Adebola et al.:</strong> 这项工作将前沿的3D重建技术（高斯泼溅）应用于一个具体的、具有挑战性的实际问题（植物结构监测），展示了3D视觉在农业、生物科学等领域的巨大潜力，其精细化建模能力令人印象深刻。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>视觉-文本压缩 (Visual-Text Compression):</strong> 作为解决多模态模型长上下文瓶颈的关键技术，未来可能会有更多研究关注其效率和保真度。</li>
<li><strong>混合动作代理 (Hybrid Action Agents):</strong> 结合不同粒度或模态的动作（例如鼠标点击、键盘输入、自然语言指令）来控制计算机，是具身智能在数字世界中应用的重要方向。</li>
<li><strong>高斯泼溅 (Gaussian Splats) 在复杂环境3D重建中的应用:</strong> 这种新兴的3D表示方法在精细化、实时性方面展现出巨大优势，未来有望在更多领域取代传统方法。</li>
<li><strong>无重训练的弹性模型 (Elastic Models without Retraining):</strong> 探索如何在不进行昂贵重训练的情况下，使模型适应不同计算资源或任务需求，是模型部署和可持续性的重要方向。</li>
<li><strong>广义对抗求解器 (Generalized Adversarial Solver) 改进扩散模型离散化:</strong> 结合对抗学习来优化扩散模型的采样过程，有望提升生成质量和效率。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<ol>
<li><strong>"Glyph: Scaling Context Windows via Visual-Text Compression" by Jiale Cheng et al.:</strong> 对于关注多模态大模型、长上下文处理和效率优化的研究人员，这篇论文是必读的。</li>
<li><strong>"UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action" by Yuhao Yang et al.:</strong> 如果您对具身智能、AI代理、自动化或基础模型在复杂任务中的应用感兴趣，这篇论文提供了前瞻性的视角。</li>
<li><strong>"Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats" by Simeon Adebola et al.:</strong> 对于3D视觉、数字孪生、高斯泼溅技术及其在实际应用中的潜力感兴趣的研究人员，这篇论文提供了高质量的案例研究。</li>
<li><strong>"A Comprehensive Survey on World Models for Embodied AI" by Xinqing Li et al.:</strong> 对于希望全面了解具身智能中“世界模型”发展现状、挑战和未来方向的研究人员，这篇综述是极佳的起点。</li>
</ol>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究方向最相关的论文。建议根据您的具体兴趣，进一步深入阅读推荐的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.16732v1">A Comprehensive Survey on World Models for Embodied AI</a></li>
<li><a href="#2510.17800v1">Glyph: Scaling Context Windows via Visual-Text Compression</a></li>
<li><a href="#2510.17790v1">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a></li>
<li><a href="#2510.17783v1">Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</a></li>
<li><a href="#2510.17700v1">Elastic ViTs from Pretrained Models without Retraining</a></li>
<li><a href="#2510.17699v1">GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</a></li>
<li><a href="#2510.17685v1">Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning</a></li>
<li><a href="#2510.17681v1">PICABench: How Far Are We from Physically Realistic Image Editing?</a></li>
<li><a href="#2510.17626v1">CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</a></li>
<li><a href="#2510.17566v1">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.16732v1'></a></p>
<h2 id="a-comprehensive-survey-on-world-models-for-embodied-ai"><a href="https://arxiv.org/abs/2510.16732v1">A Comprehensive Survey on World Models for Embodied AI</a></h2>
<p><strong>Authors:</strong> Xinqing Li, Xin He, Le Zhang, Yun Liu</p>
<p><strong>Published:</strong> 2025-10-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Xinqing Li, Xin He, Le Zhang, Yun Liu撰写的论文“A Comprehensive Survey on World Models for Embodied AI”的全面摘要：</p>
<p><strong>论文摘要：A Comprehensive Survey on World Models for Embodied AI</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
这篇综述论文旨在解决具身AI（Embodied AI）领域中世界模型（World Models）的碎片化研究现状。具身AI要求智能体能够感知复杂的多模态环境、在其中行动，并预测其行动如何改变未来的世界状态。世界模型作为内部模拟器，捕捉环境动态，支持感知、预测和决策制定中的前向与反事实推演。然而，当前世界模型研究缺乏统一的框架、术语和分类，导致不同子社区之间存在不一致性，这阻碍了该领域的系统性进展和跨领域泛化。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的核心贡献在于提出了一个统一的三轴分类框架，用于组织具身AI中的世界模型研究：
*   <strong>功能性（Functionality）：</strong> 将世界模型分为“决策耦合型”（Decision-Coupled）和“通用型”（General-Purpose）。决策耦合型模型是任务特定的，为特定决策任务优化学习动态；通用型模型是任务无关的模拟器，专注于广泛预测，以实现跨各种下游应用的泛化。
*   <strong>时间建模（Temporal Modeling）：</strong> 区分了“顺序模拟与推理”（Sequential Simulation and Inference）和“全局差异预测”（Global Difference Prediction）。前者以自回归方式逐步展开未来状态；后者并行估计整个未来状态。
*   <strong>空间表示（Spatial Representation）：</strong> 涵盖了四种主要策略：“全局潜在向量”（Global Latent Vector）、“令牌特征序列”（Token Feature Sequence）、“空间潜在网格”（Spatial Latent Grid）和“分解渲染表示”（Decomposed Rendering Representation），这些策略在效率、表达能力和物理保真度之间进行权衡。</p>
<p>此外，论文还系统化了机器人、自动驾驶和通用视频设置中的数据资源和评估指标，涵盖了像素预测质量、状态级理解和任务性能，并对最先进的模型进行了定量比较。</p>
<p><strong>3. 主要结果及其意义：</strong>
该综述通过其提出的三轴分类框架，清晰地组织和回顾了现有世界模型研究，揭示了不同方法在功能、时间建模和空间表示上的设计选择和权衡。通过对现有数据资源和评估指标的系统化，论文为未来的研究提供了标准化的比较基础。定量比较部分展示了当前最先进模型在像素生成、场景理解和控制任务上的表现，例如在nuScenes数据集上的驾驶视频生成中，DrivePhysica在视觉保真度上表现最佳，而MiLA在时间连贯性上最强。在Occ3D-nuScenes上的4D占用预测中，COME（结合GT ego）取得了最佳的平均mIoU和每视距mIoU。这些结果突出了不同模型在特定任务和评估维度上的优势，并强调了在效率、保真度和泛化能力之间进行权衡的必要性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文明确指出了当前世界模型研究的几个主要局限性：
*   <strong>数据稀缺与异构性：</strong> 具身AI领域缺乏统一的大规模数据集，导致模型泛化能力受限。
*   <strong>评估指标不足：</strong> 现有评估指标（如FID和FVD）侧重于像素保真度，但往往忽略了物理一致性、动态性和因果关系。缺乏跨领域标准化的评估框架。
*   <strong>计算效率与性能权衡：</strong> 尽管Transformer和Diffusion模型表现出色，但其高昂的推理成本与机器人系统实时控制的需求相冲突。传统RNN和全局潜在向量虽然效率高，但在捕捉长期依赖方面存在局限。
*   <strong>长期时间一致性与误差累积：</strong> 自回归设计虽然紧凑且样本高效，但会随着时间累积误差；全局预测虽然提高了多步连贯性，但计算成本高且闭环交互性较弱。
*   <strong>动态场景下的空间表示：</strong> 潜在向量、令牌序列和空间网格在效率和表达能力之间存在权衡；分解渲染方法（如NeRF和3DGS）虽然保真度高，但在动态场景中扩展性差。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：
*   <strong>统一数据集与评估框架：</strong> 优先构建统一的多模态、跨领域数据集，以实现可迁移的预训练。开发超越感知真实感的评估框架，以评估物理一致性、因果推理和长期动态。
*   <strong>计算效率优化：</strong> 关注模型架构优化，如量化、剪枝和稀疏计算，以降低推理延迟。探索新的时间建模方法，如状态空间模型（SSMs，例如Mamba），以在保持实时效率的同时增强长期推理能力。
*   <strong>混合建模策略：</strong> 整合自回归和全局预测方法的优势，以平衡效率、保真度和交互性。
*   <strong>增强长期时间一致性：</strong> 引入显式记忆或分层规划来提高长期预测的稳定性。利用CoT（Chain-of-Thought）启发式任务分解，通过中间目标设定来改善时间一致性。
*   <strong>统一架构：</strong> 开发能够有效平衡效率、保真度和交互性的统一架构，无缝整合时间与空间建模。
*   <strong>物理基础的基准与架构：</strong> 创建统一的、物理基础的基准，并探索高效的架构，以实现具身AI的下一代世界模型。</p>
<p>总而言之，这篇综述为具身AI中的世界模型研究提供了一个全面且结构化的视角，不仅总结了现有进展，更清晰地指出了未来的研究方向和挑战，对于推动该领域的发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.16732v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.16732v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17800v1'></a></p>
<h2 id="glyph-scaling-context-windows-via-visual-text-compression"><a href="https://arxiv.org/abs/2510.17800v1">Glyph: Scaling Context Windows via Visual-Text Compression</a></h2>
<p><strong>Authors:</strong> Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Jiale Cheng等人撰写的论文“Glyph: Scaling Context Windows via Visual-Text Compression”的全面摘要，使用中文书写：</p>
<p><strong>论文摘要：Glyph: 通过视觉-文本压缩扩展上下文窗口</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
大型语言模型（LLMs）在处理长上下文任务（如文档理解、代码分析和多步推理）方面日益重要。然而，将上下文窗口扩展到百万级别会带来巨大的计算和内存成本，严重限制了长上下文LLMs的实用性。本文旨在解决这一挑战，即如何在不牺牲性能的前提下，有效扩展LLMs处理长文本的能力，同时避免传统方法带来的高昂成本。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了一种名为 <strong>Glyph</strong> 的新颖框架，通过“视觉上下文扩展”来解决上述问题。其核心创新点包括：
*   <strong>视觉-文本压缩范式：</strong> Glyph 不直接扩展基于token的序列，而是将长文本渲染成紧凑的图像，并使用视觉-语言模型（VLMs）进行处理。这种方法显著压缩了文本输入，同时保留了语义信息，将每个视觉token视为多个文本token的紧凑载体，从而提高了信息密度。
*   <strong>LLM驱动的遗传搜索：</strong> 为了平衡准确性和压缩率，Glyph 设计了一个LLM驱动的遗传搜索算法，自动识别最佳的视觉渲染配置（例如字体大小、布局、分辨率）。
*   <strong>三阶段训练流程：</strong>
    *   <strong>持续预训练：</strong> 使VLM能够理解和推理具有不同视觉风格的渲染长文本。
    *   <strong>LLM驱动的渲染搜索：</strong> 自动发现下游任务的最佳渲染配置。
    *   <strong>后训练：</strong> 在发现的最佳配置下进行监督微调（SFT）和强化学习（RL），并辅以辅助OCR对齐任务，以进一步提高模型在视觉压缩输入上的长上下文能力和文本识别能力。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>显著的文本压缩：</strong> Glyph 在各种长上下文基准测试上实现了3-4倍的token压缩，同时保持了与Qwen3-8B等领先LLMs相当的准确性。在极端压缩下，一个128K上下文的VLM可以处理1M token级别的文本任务。
*   <strong>推理和训练效率提升：</strong> 这种压缩带来了显著的效率提升，预填充和解码速度提高了约4倍，SFT训练速度提高了约2倍。
*   <strong>跨模态泛化能力：</strong> 渲染的文本数据有助于处理真实世界的多模态任务，如文档理解。
*   <strong>上下文扩展潜力：</strong> 实验表明，Glyph 在长上下文任务中表现出更稳定的性能退化，随着输入长度的增加，其优势愈发明显，展现了将有效上下文扩展到远超当前限制的潜力（例如，处理4M甚至8M上下文token）。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>渲染参数的敏感性：</strong> 模型的性能会受到渲染配置（如分辨率、字体、间距）的显著影响。尽管遗传搜索能找到好的配置，但如何使模型对各种渲染设置更具鲁棒性仍是一个开放问题。
*   <strong>OCR相关挑战：</strong> 在Ruler基准测试中，UUID识别对当前VLM来说仍然极具挑战性，即使是最强的模型也难以正确复现。这种罕见的字母数字序列可能由于训练数据中的分布稀疏性或视觉编码器的架构限制导致字符错序或错误分类。
*   <strong>任务多样性：</strong> 本文的基准测试主要集中在长上下文理解，未能完全涵盖真实世界应用的多样性，如代理或推理密集型任务。与纯文本模型相比，视觉-文本模型在任务间的泛化能力较弱。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>自适应渲染模型：</strong> 训练能够根据任务类型或用户查询调整渲染策略的模型，以平衡压缩和性能。
*   <strong>增强视觉编码器：</strong> 提高视觉编码器对细粒度文本识别和与语言表示对齐的能力，以提升跨任务的鲁棒性和可迁移性。
*   <strong>视觉-文本与纯文本模型的对齐：</strong> 通过知识蒸馏或跨模态监督等方法，缩小视觉-文本模型与纯文本模型在泛化能力上的差距。
*   <strong>扩展到更广泛的应用：</strong> 将Glyph应用于代理记忆系统、管理长期对话或利用结构化视觉布局进行推理和检索的任务。
*   <strong>上下文工程优化：</strong> 进一步探索如何优化上下文信息的表示和管理，以实现从1M到10M输入token的上下文扩展。</p>
<p>总而言之，Glyph 提出了一种新颖且高效的长上下文建模范式，通过视觉-文本压缩克服了传统LLMs的计算和内存瓶颈，为未来LLMs的上下文扩展提供了新的方向和巨大的潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs).</li>
<li>Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17800v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17800v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17790v1'></a></p>
<h2 id="ultracua-a-foundation-model-for-computer-use-agents-with-hybrid-action"><a href="https://arxiv.org/abs/2510.17790v1">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a></h2>
<p><strong>Authors:</strong> Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yuhao Yang等人撰写的论文“UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action”的全面摘要。</p>
<hr />
<h3 id="ultracua">论文摘要：UltraCUA: 计算机使用智能体的混合动作基础模型</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前计算机使用智能体（CUAs）主要依赖于原始的图形用户界面（GUI）动作（如点击、输入、滚动），这导致了冗长的执行链、级联错误和性能瓶颈。与利用丰富的程序化接口（API、MCP服务器、工具）的其他智能体不同，CUAs在这些能力方面是孤立的。因此，核心问题是如何弥合GUI原始操作与高级程序化工具调用之间的差距，以实现更鲁棒、高效和统一的计算机使用自动化。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
UltraCUA通过引入<strong>混合动作</strong>机制来解决上述问题，无缝集成了GUI原始操作和高级程序化工具调用。其方法论包含四个关键组成部分：</p>
<ul>
<li><strong>程序化工具的自动化收集管道：</strong> 论文开发了一个可扩展的管道，从软件文档、开源仓库和代码生成中自动提取和集成程序化工具，甚至能按需生成新工具，从而构建了一个包含数百种工具的丰富工具集。</li>
<li><strong>可验证计算机使用任务的双管道合成数据引擎：</strong> 为了解决大规模CUA训练数据生成和验证的挑战，论文设计了一个双管道引擎，生成了超过17,000个可验证的真实世界计算机使用任务。这包括“评估器优先”生成（确保可验证性）和“指令优先”生成（提供多样性和上下文相关性）。</li>
<li><strong>大规模高质量混合动作轨迹收集：</strong> 论文收集了超过20,000条成功的混合动作轨迹，这些轨迹结合了低级GUI动作和高级程序化工具调用。通过结合强大的规划器模型（OpenAI 03）和先进的视觉定位模型（GTA1-7B），智能体能够根据任务上下文在不同动作模式间进行策略性切换。</li>
<li><strong>两阶段训练管道：</strong> 采用监督微调（SFT）和在线强化学习（RL）相结合的两阶段训练方法。SFT阶段在高质量轨迹上进行，建立混合动作能力；RL阶段通过自博弈优化动作选择，实现低级和高级动作之间的策略性交替。</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
实验结果表明UltraCUA模型在性能上显著优于现有最先进的智能体：</p>
<ul>
<li><strong>OSWorld基准测试：</strong> UltraCUA的7B和32B模型在OSWorld上实现了平均22%的相对性能提升，并且在步骤数上快了11%。例如，UltraCUA-7B在15步内成功率达到28.9%，超越了所有可比较的7B模型（如UI-TARS-1.5-7B的23.4%）。</li>
<li><strong>跨平台泛化能力：</strong> 在WindowsAgentArena上的域外评估显示，UltraCUA-7B模型在未经Windows特定训练的情况下，成功率达到21.7%，优于在Windows数据上训练的基线模型。这验证了混合动作策略的跨操作系统可迁移性。</li>
<li><strong>混合动作机制的关键性：</strong> 混合动作机制被证明是至关重要的，它在保持执行效率的同时减少了错误传播。消融研究进一步证实，混合动作空间、工作记忆机制和强化学习阶段都对智能体性能有积极影响。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确列出“局限性”部分，但从描述中可以推断出一些潜在的挑战或未完全解决的问题：</p>
<ul>
<li><strong>工具语法复杂性：</strong> 在RL训练早期，模型在处理复杂的工具语法时会遇到困难，这可能导致格式惩罚主导学习信号。论文通过专注于结果和工具使用奖励来缓解，但仍暗示了这一挑战。</li>
<li><strong>OOD工具泛化中的适应挑战：</strong> 尽管模型能够适应未见过的工具，但步骤数的增加表明适应过程仍存在挑战，模型可能会在选择合适的工具之前探索不熟悉的工具。</li>
<li><strong>推理速度：</strong> 对于OpenCUA系列模型，由于推理速度和基础设施的次优，整体平均运行次数少于4次，这可能暗示了在实际部署中仍需优化推理效率。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文为未来的研究奠定了基础，可以从以下几个方面进行探索：</p>
<ul>
<li><strong>更高效的工具学习：</strong> 进一步优化强化学习策略，以更有效地学习和掌握复杂的工具语法，减少早期训练中的格式错误。</li>
<li><strong>增强OOD工具泛化能力：</strong> 探索更鲁棒的机制，使智能体能够更高效地适应和利用在训练中未见过的程序化工具，减少适应过程中的额外步骤。</li>
<li><strong>推理效率优化：</strong> 针对模型在推理速度和基础设施方面的挑战，可以研究更轻量级的模型架构、更高效的推理算法或硬件加速，以提高实时应用中的性能。</li>
<li><strong>更广泛的应用场景：</strong> 将UltraCUA的混合动作范式扩展到更多样化的应用领域和更复杂的真实世界任务中，进一步验证其通用性和鲁棒性。</li>
<li><strong>多模态感知与理解的深度融合：</strong> 进一步探索视觉接地与程序化智能的深度融合，使智能体能够更精细地理解用户界面元素和任务上下文，从而做出更智能的动作决策。</li>
</ul>
<hr />
<p>总而言之，UltraCUA通过其创新的混合动作空间和全面的方法论，成功地弥合了GUI原始操作与高级程序化工具调用之间的鸿沟，为构建更强大、高效和通用的计算机使用智能体开辟了新途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions.</li>
<li>Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17790v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17790v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17783v1'></a></p>
<h2 id="botany-bot-digital-twin-monitoring-of-occluded-and-underleaf-plant-structures-with-gaussian-splats"><a href="https://arxiv.org/abs/2510.17783v1">Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</a></h2>
<p><strong>Authors:</strong> Simeon Adebola, Chung Min Kim, Justin Kerr, Shuangyu Xie, Prithvi Akella, Jose Luis Susa Rincon, Eugen Solowjow, Ken Goldberg</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文介绍了一个名为 Botany-Bot 的创新系统，旨在通过结合机器人操作和 3D 高斯泼溅模型，创建植物的详细“带注释的数字孪生”。其核心贡献在于解决了传统固定相机植物表型系统因叶片遮挡而无法获取植物细节的问题，特别是通过机器人算法主动操纵叶片以拍摄被遮挡的结构。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>该论文的关键创新在于其集成式方法，将以下几个要素结合起来：</p>
<ul>
<li><strong>机器人辅助叶片操纵：</strong> 这是最显著的创新点。通过工业机器人手臂主动抬起或推动叶片，系统能够获取传统方法无法触及的遮挡区域（如茎芽、叶片背面/正面）的高分辨率图像。这克服了被动成像的根本限制。</li>
<li><strong>3D 分割高斯泼溅模型 (Gaussian Splats)：</strong> 利用高斯泼溅技术构建植物的 3D 数字孪生，这是一种新兴的神经渲染技术，以其高质量和高效的渲染能力而闻名。结合 3D 分割，可以创建精细的、可索引的植物结构模型。</li>
<li><strong>集成硬件平台：</strong> 结合了立体相机、数字转盘和灯箱，为数据采集提供了受控且全面的环境。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>Botany-Bot 对计算机视觉和植物科学领域具有多方面的潜在影响：</p>
<ul>
<li><strong>高精度植物表型：</strong> 显著提升了植物表型数据的精细度和完整性，能够获取以前难以量化的微观结构信息，这对于植物育种、作物改良、病虫害检测和植物生理学研究至关重要。</li>
<li><strong>数字孪生技术在生物领域的应用：</strong> 推动了高保真数字孪生技术在复杂生物体（如植物）上的应用，为未来的精准农业、智能温室管理和植物生长模拟提供了新的范式。</li>
<li><strong>机器人与视觉的深度融合：</strong> 展示了机器人主动感知和操纵在克服视觉挑战方面的强大潜力，为未来在其他复杂、遮挡环境下的目标检测和数据采集提供了灵感。</li>
<li><strong>新型数据集的创建：</strong> 能够生成包含遮挡细节的独特数据集，这将促进新的计算机视觉算法（如遮挡处理、多视图重建）的开发和评估。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>精准农业和智能温室：</strong> 实时监测植物健康、生长状态、病虫害早期预警，实现精细化管理。</li>
<li><strong>植物育种和遗传学：</strong> 更准确地评估不同基因型在微观结构上的差异，加速新品种的选育。</li>
<li><strong>植物病理学和昆虫学：</strong> 早期发现叶片背面或茎部的病变、虫害，提高防治效率。</li>
<li><strong>植物生理学研究：</strong> 深入研究植物对环境变化的响应，例如气孔分布、叶片结构变化等。</li>
<li><strong>机器人操作和感知：</strong> 为开发更智能、更灵活的机器人系统提供案例和挑战，特别是在处理柔软、易损物体方面。</li>
<li><strong>3D 重建和神经渲染：</strong> 推动高斯泼溅等技术在复杂、动态场景下的应用和优化。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>系统复杂性和成本：</strong> 摘要中描述的硬件平台（工业机器人、立体相机、转盘、灯箱）表明这是一个相对复杂且成本较高的系统，可能不适用于大规模、低成本的部署。</li>
<li><strong>处理速度和可扩展性：</strong> 机器人操纵叶片并拍摄图像的过程可能相对耗时，对于需要快速处理大量植物的场景，其效率可能是一个限制。摘要中未提及处理单株植物所需的时间。</li>
<li><strong>机器人操作的鲁棒性：</strong> 尽管给出了操作成功率（抬起/推动叶片77.9%），但仍有约22%的失败率。对于脆弱的植物，机器人操作的力度、精度和对不同植物形态的适应性是挑战。</li>
<li><strong>泛化能力：</strong> 摘要未明确说明该系统对不同植物种类、大小和形态的泛化能力。某些植物的叶片结构可能更难操纵或重建。</li>
<li><strong>数据处理和存储：</strong> 生成的“详细带注释的数字孪生”和高分辨率图像将产生大量数据，对数据处理、存储和分析能力提出要求。</li>
<li><strong>精度指标的上下文：</strong> 摘要中给出的精度（如叶片分割90.8%，检测86.2%）是针对特定数据集和实验条件而言的，其在更广泛场景下的表现仍需进一步验证。</li>
</ul>
<hr />
<p>总而言之，Botany-Bot 代表了计算机视觉、机器人学和植物科学交叉领域的一个重要进展，通过主动感知和先进的 3D 重建技术，为植物表型和数字孪生构建开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17783v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17783v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17700v1'></a></p>
<h2 id="elastic-vits-from-pretrained-models-without-retraining"><a href="https://arxiv.org/abs/2510.17700v1">Elastic ViTs from Pretrained Models without Retraining</a></h2>
<p><strong>Authors:</strong> Walter Simoncini, Michael Dorkenwald, Tijmen Blankevoort, Cees G. M. Snoek, Yuki M. Asano</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Walter Simoncini等人撰写的论文“Elastic ViTs from Pretrained Models without Retraining”的全面摘要。</p>
<hr />
<h3 id="elastic-vits-from-pretrained-models-without-retraining_1">论文摘要：Elastic ViTs from Pretrained Models without Retraining</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前视觉基础模型（Vision Foundation Models）虽然性能卓越，但通常只提供有限的预设尺寸，这导致在实际部署中难以根据具体的计算预算和任务需求进行灵活调整，往往需要选择过大的模型，造成资源浪费。论文旨在解决如何从单一预训练模型中高效地提取一系列具有不同计算预算的子网络，以实现弹性推理，而无需重新训练或依赖标注数据。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了一种名为 <strong>SnapViT</strong> 的新型结构化剪枝方法，用于预训练的视觉Transformer（ViTs），其核心创新点包括：</p>
<ul>
<li><strong>单次剪枝与弹性推理：</strong> SnapViT 能够在单次运行中，快速（在单个A100 GPU上少于五分钟）生成适用于各种稀疏度级别的弹性模型，从而实现跨计算预算的连续推理。</li>
<li><strong>结合局部梯度与全局Hessian相关性：</strong> 论文引入了一个“可剪枝性分数”，该分数结合了两个关键项：<ul>
<li><strong>局部Hessian近似：</strong> 使用自监督梯度（基于DINO目标）高效地估计参数的局部敏感性，无需分类头，适用于监督和自监督模型。</li>
<li><strong>全局Hessian估计（通过xNES）：</strong> 针对Hessian矩阵的非对角线元素（捕捉跨网络结构相关性）计算的复杂性，论文提出了一种新颖的进化算法（xNES，指数自然进化策略）来高效近似这些全局相关性，避免了显式计算完整的Hessian。</li>
</ul>
</li>
<li><strong>自监督重要性评分机制：</strong> 该方法不依赖标注数据，通过自监督损失（例如DINO目标）来指导剪枝过程，并使用基于PCA的嵌入余弦相似度作为适应度函数，确保在不重新训练或使用标签的情况下保持强大的性能。</li>
<li><strong>结构化剪枝策略：</strong> 能够对Transformer组件（如前馈块内的行-列组合）和更大的结构（如整个注意力头）进行选择性剪枝。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的剪枝性能：</strong> 在DINO、SigLIPv2、DeIT和AugReg等模型上的实验表明，SnapViT在各种稀疏度下均优于或媲美最先进的剪枝方法，尤其是在高稀疏度比率下。例如，DINOv1 ViT-B/16模型在剪枝40%稀疏度后，推理速度提升1.58倍，而准确率下降小于5%。
*   <strong>无需重新训练和标签：</strong> 该方法在不进行重新训练或使用任何标签的情况下，实现了强大的性能，这对于处理非公开预训练数据集或在资源受限环境下部署模型具有重要意义。
*   <strong>对大型模型的适用性：</strong> 即使对于DINOv3 ViT-H+/16和SigLIPv2 ViT-G/16等大型模型（数十亿参数），SnapViT也能有效剪枝，尽管在极高稀疏度下性能下降更快，但结合简单的权重校正技术仍能恢复性能。
*   <strong>对语义分割任务的泛化能力：</strong> 在Pascal VOC 2012语义分割任务上，SnapViT也表现出与最先进方法相当或更优的性能，尤其是在高稀疏度下。
*   <strong>效率和可扩展性：</strong> 在单个A100 GPU上，生成弹性模型所需时间少于五分钟，证明了其出色的可扩展性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>大型预训练模型的剪枝挑战：</strong> 论文观察到，在大型数据集上训练的视觉基础模型（如DINOv3和SigLIPv2）更难剪枝，因为其表征知识可能更均匀地分布在参数中，使得识别“不重要”的单元变得不那么明显。
*   <strong>极端稀疏度下的性能下降：</strong> 尽管SnapViT在较高稀疏度下表现良好，但在极端稀疏度（例如超过30%）下，某些大型模型的准确性会迅速下降。
*   <strong>小模型在极端稀疏度下的容量限制：</strong> 对于像ViT-S/16这样的小模型，在极端剪枝（例如50%稀疏度）后，剩余的表征容量有限，可能导致权重校正效果不佳。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步优化大型模型剪枝：</strong> 探索更先进的策略，以应对大型预训练模型中知识均匀分布带来的剪枝挑战，可能包括更复杂的权重校正或多阶段剪枝方法。
*   <strong>数据中心剪枝的探索：</strong> 论文初步表明，数据与剪枝任务的对齐对性能有显著影响，未来可以深入研究如何通过数据中心的方法进一步优化剪枝，以生成泛化能力更强的稀疏模型。
*   <strong>与其他弹性推理方法的结合：</strong> 探索SnapViT与现有弹性推理方法（如Matryoshka表示）的结合，以实现更广泛的灵活性和性能提升。
*   <strong>更高效的Hessian近似：</strong> 尽管xNES已经很高效，但仍可以探索其他更快的Hessian非对角线结构近似方法，以进一步缩短剪枝时间。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets.</li>
<li>Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free.</li>
<li>Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget.</li>
<li>Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17700v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17700v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17699v1'></a></p>
<h2 id="gas-improving-discretization-of-diffusion-odes-via-generalized-adversarial-solver"><a href="https://arxiv.org/abs/2510.17699v1">GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</a></h2>
<p><strong>Authors:</strong> Aleksandr Oganov, Ilya Bykov, Eva Neudachina, Mishan Aliev, Alexander Tolmachev, Alexander Sidorov, Aleksandr Zuev, Andrey Okhotin, Denis Rakitin, Aibek Alanov</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为“广义对抗求解器”（Generalized Adversarial Solver, GAS）的新方法，旨在显著减少扩散模型采样所需的计算量，同时保持甚至提升生成质量。GAS通过引入一个简化的ODE采样器参数化（广义求解器），避免了现有蒸馏方法中复杂的训练技巧，并结合对抗训练来更好地保留图像细节和减少伪影。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>广义求解器 (Generalized Solver) 参数化：</strong> 核心创新在于提出了一种“简单”的ODE采样器参数化。这暗示它可能比现有方法更直接、更易于实现，并且不需要复杂的训练策略（如多阶段训练、特定的调度器等）。这种简化可能降低了训练难度，并提高了方法的鲁棒性。</li>
<li><strong>结合对抗训练 (Adversarial Training)：</strong> 为了解决现有方法在细节保留和伪影抑制方面的不足，GAS将原始的蒸馏损失与对抗训练相结合。对抗训练通常能促使生成器产生更真实、细节更丰富的输出，因为它迫使生成器欺骗判别器，从而学习到更精细的数据分布特征。</li>
<li><strong>明确关注细节保真度：</strong> 论文明确指出其方法“不明确关注保留细粒度细节”是现有方法的缺点，而GAS通过对抗训练来“增强细节保真度”，这表明它在设计上就考虑了生成质量的关键方面。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>加速扩散模型采样：</strong> 这是最直接和最重要的影响。如果GAS能够以更少的函数评估（FEVs）实现与现有SOTA相当或更好的质量，将极大地提高扩散模型的实用性，使其在实时应用、资源受限环境（如移动设备）中更可行。</li>
<li><strong>简化模型蒸馏过程：</strong> 避免“复杂的训练技巧”意味着研究人员和开发者可以更容易地实现和应用扩散模型的加速，降低了进入门槛和开发成本。</li>
<li><strong>提升生成质量，特别是细节方面：</strong> 强调细节保真度意味着GAS可能在生成高分辨率图像、纹理、面部特征等对细节敏感的任务上表现出色，从而推动扩散模型在艺术创作、图像编辑、虚拟现实等领域的应用。</li>
<li><strong>推动扩散模型研究方向：</strong> 可能会激发更多关于如何有效结合ODE求解器优化和对抗训练的研究，以及探索更简单、更鲁棒的扩散模型加速方法。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>实时图像生成：</strong> 例如，交互式图像编辑、视频生成、游戏资产创建。</li>
<li><strong>资源受限设备上的生成任务：</strong> 如移动端的AI艺术应用、边缘计算设备上的图像增强。</li>
<li><strong>高分辨率图像合成：</strong> 建筑渲染、医学图像生成、卫星图像合成等需要精细细节的领域。</li>
<li><strong>条件生成任务：</strong> 如文本到图像（text-to-image）、图像到图像（image-to-image）翻译，其中快速反馈和高质量细节至关重要。</li>
<li><strong>3D内容生成：</strong> 扩散模型已开始应用于3D生成，加速采样将对3D资产的快速迭代和预览产生巨大影响。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>“类似资源约束下”的性能比较：</strong> 摘要中提到“在类似资源约束下”展示了优越性能。这可能意味着在某些极端资源受限或资源非常充裕的情况下，其优势可能不那么明显，或者需要进一步的实验来验证。</li>
<li><strong>对抗训练的潜在挑战：</strong> 尽管对抗训练有助于细节，但它也可能带来训练不稳定、模式崩溃（mode collapse）等问题，尽管摘要声称“不需要额外的训练技巧”，但对抗训练本身就可能需要仔细的超参数调整。</li>
<li><strong>“简单参数化”的具体含义：</strong> 摘要没有详细说明“广义求解器”的具体参数化形式。其简单性是否会限制其在某些复杂数据分布上的表达能力，仍有待验证。</li>
<li><strong>泛化能力：</strong> 论文主要关注图像生成，其方法在其他扩散模型应用（如音频生成、时间序列预测）上的泛化能力如何，尚不清楚。</li>
<li><strong>与最新SOTA的比较：</strong> 摘要声称“改进了现有方法”，但具体是哪些方法，以及在哪些指标上取得了多大的提升，需要查阅论文正文才能了解。尤其是在2025年10月发表，届时可能已经有更多新的SOTA方法出现。</li>
</ul>
<hr />
<p>总而言之，这篇论文提出了一种有前景的方法来解决扩散模型采样效率和细节保真度之间的权衡问题。其“简单参数化”和“对抗训练”的结合，有望为扩散模型的实际应用带来显著的性能提升和更广泛的普及。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling.</li>
<li>In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17699v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17699v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17685v1'></a></p>
<h2 id="multilingual-text-to-image-person-retrieval-via-bidirectional-relation-reasoning-and-aligning"><a href="https://arxiv.org/abs/2510.17685v1">Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning</a></h2>
<p><strong>Authors:</strong> Min Cao, Xinyu Zhou, Ding Jiang, Bo Du, Mang Ye, Min Zhang</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Min Cao, Xinyu Zhou, Ding Jiang, Bo Du, Mang Ye, Min Zhang撰写的论文“Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning”的全面摘要，用中文写成：</p>
<hr />
<p><strong>论文摘要：多语言文本到图像人物检索中的双向隐式关系推理与对齐</strong></p>
<p>这篇论文解决了文本到图像人物检索（TIPR）领域中的两个关键挑战：模态异构性和语言多样性。传统的TIPR方法主要集中于英文文本查询，并且在跨模态对齐方面，要么忽略细粒度差异（全局对齐），要么需要预先的局部对齐信息（局部对齐），这限制了其在多语言环境中的应用和性能。</p>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决现有TIPR方法在处理多语言文本查询时的局限性，并克服模态异构性问题，以实现跨语言和跨模态的鲁棒人物检索。具体来说，它提出了一个新颖的“多语言TIPR”任务，并致力于构建相应的数据集和开发有效的框架。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>多语言TIPR任务的开创性工作：</strong> 论文首次提出了多语言TIPR任务，并构建了相应的基准数据集。
*   <strong>LMs驱动的领域自适应翻译（LDAT）流水线：</strong> 为了解决多语言TIPR数据稀缺的问题，论文提出了一种LDAT流水线，利用大型语言模型（LLMs）进行初始翻译，并通过过滤和重写阶段整合领域特定知识，以生成高质量的多语言TIPR数据集。这有效缓解了LLMs翻译中缺乏领域知识导致的噪声问题。
*   <strong>Bi-IRRA框架：</strong> 论文提出了一个名为Bi-IRRA（Bidirectional Implicit Relation Reasoning and Aligning）的跨模态框架，旨在学习跨语言和跨模态的对齐。
    *   <strong>双向隐式关系推理（Bi-IRR）模块：</strong> 该模块通过双语掩码语言建模（MLM）和跨语言蒸馏掩码图像建模（D-MIM）预训练任务，实现对掩码图像和文本的双向预测。这隐式地增强了跨语言和跨模态的局部关系建模，从而在细粒度层面建立对齐。
    *   <strong>多维全局对齐（Md-GA）模块：</strong> 该模块集成了双语图像-文本对比学习（ITC）和双语非对称图像-文本匹配（A-ITM）预训练任务，以弥合模态异构性，实现粗粒度层面的全局对齐。特别地，A-ITM中的非对称掩码操作有助于在存在噪声目标文本的情况下进行鲁棒学习。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> Bi-IRRA在所有多语言TIPR数据集上均取得了新的最先进结果，显著优于现有方法。
*   <strong>多语言环境下的鲁棒性：</strong> 实验证明，Bi-IRRA在中文、法文和德文等非英文环境中表现出卓越的检索性能，验证了其在多语言场景下的强大鲁棒性和泛化能力。
*   <strong>LDAT的有效性：</strong> 消融研究表明，LDAT流水线通过整合领域特定知识，能够有效提升翻译质量，为多语言TIPR任务提供高质量的数据。
*   <strong>Bi-IRRA模块的有效性：</strong> Bi-IRR模块（包括双语MLM和跨语言D-MIM）以及Md-GA模块（包括双语ITC和A-ITM）的各个组件都对整体性能有显著贡献，证明了其在细粒度和粗粒度对齐方面的有效性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据生成中的噪声：</strong> 尽管LDAT流水线通过过滤和重写阶段努力去噪，但在LMs驱动的翻译过程中，仍不可避免地会引入一些噪声，这可能影响模型对齐的准确性。
*   <strong>传统TIPR方法在多语言数据上的表现：</strong> 传统TIPR方法由于其模型架构并非为多语言学习而设计，即使在多语言数据集上进行训练，其性能也往往不尽如人意。
*   <strong>Bi-lingual ITC中掩码的局限性：</strong> 在Bi-lingual ITC中应用输入掩码并未带来性能提升，这归因于其架构差异，即ITC直接从编码的单模态特征计算对比损失，而没有经过跨模态交互模块，限制了掩码输入的正则化效果。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步提升多语言翻译质量：</strong> 尽管LDAT已有效缓解噪声，但仍可探索更先进的LMs或翻译技术，以进一步提升多语言TIPR数据的翻译质量。
*   <strong>更复杂的跨模态交互：</strong> 探索更深层次、更复杂的跨模态交互机制，以更好地捕捉图像和文本之间的细粒度语义关系。
*   <strong>自适应掩码策略：</strong> 研究更智能、自适应的掩码策略，以在不同语言和模态下最大化模型学习效率。
*   <strong>扩展到更多语言和领域：</strong> 将多语言TIPR任务扩展到更多语言和更广泛的领域，以验证Bi-IRRA框架的通用性和可扩展性。
*   <strong>实时应用优化：</strong> 针对实际公共安全领域的需求，进一步优化模型的推理速度和效率，以支持实时人物检索应用。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities.</li>
<li>The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17685v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17685v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17681v1'></a></p>
<h2 id="picabench-how-far-are-we-from-physically-realistic-image-editing"><a href="https://arxiv.org/abs/2510.17681v1">PICABench: How Far Are We from Physically Realistic Image Editing?</a></h2>
<p><strong>Authors:</strong> Yuandong Pu, Le Zhuo, Songhao Han, Jinbo Xing, Kaiwen Zhu, Shuo Cao, Bin Fu, Si Liu, Hongsheng Li, Yu Qiao, Wenlong Zhang, Xi Chen, Yihao Liu</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：PICABench: How Far Are We from Physically Realistic Image Editing?</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于提出了 <strong>PICABench</strong>，这是一个用于系统性评估图像编辑物理真实感的基准。它超越了现有模型和基准仅关注指令完成的局限，首次将物理效果（如阴影、反射、物体交互等）纳入评估范围，并提出了 <strong>PICAEval</strong> 这一可靠的评估协议。此外，论文还探索了从视频中学习物理知识的解决方案，并构建了训练数据集 <strong>PICA-100K</strong>。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>物理真实感评估的引入和系统化：</strong> 这是最核心的创新。论文明确指出现有图像编辑模型和基准忽视了物理效果，并首次将物理真实感提升到与指令完成同等重要的地位。它将物理真实感细分为八个子维度（涵盖光学、力学和状态转换），并应用于常见的编辑操作，实现了对这一复杂问题的系统性解构和评估。</li>
<li><strong>VLM-as-a-judge 评估协议 (PICAEval)：</strong> 提出了一种新颖且可靠的评估协议，利用视觉语言模型（VLM）作为“评判者”，结合逐案例、区域级别的人工标注和问题，克服了传统指标难以捕捉物理真实感的挑战。这代表了评估方法学上的一大进步。</li>
<li><strong>从视频中学习物理知识的探索和数据集构建 (PICA-100K)：</strong> 论文不仅提出了问题和评估方法，还积极探索了解决方案，即通过从视频中学习来提升物理真实感，并为此构建了一个大规模训练数据集 PICA-100K。这为未来的研究提供了宝贵的资源和方向。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>重新定义图像编辑的“好”：</strong> PICABench 将促使研究人员和开发者重新思考图像编辑的质量标准，从仅仅关注“内容正确”转向“内容正确且物理真实”。这将推动图像编辑技术向更高层次的真实感和可信度发展。</li>
<li><strong>推动物理感知型生成模型的发展：</strong> 这一基准将成为开发和评估物理感知型（physics-aware）图像生成和编辑模型的关键工具。未来的模型将需要显式地学习和模拟物理规律，而不仅仅是像素级的映射。</li>
<li><strong>促进多模态评估方法学创新：</strong> PICAEval 中 VLM-as-a-judge 的方法，结合人工标注，为复杂、主观的生成内容评估提供了新的范式，可能启发其他生成任务的评估方法。</li>
<li><strong>激发新的研究方向：</strong> 论文明确指出物理真实感仍是一个“具有巨大探索空间”的挑战性问题，这将吸引更多研究者投入到物理模拟、隐式物理学习、因果推理等相关领域的研究。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>电影和游戏产业：</strong> 对物理真实感的高度需求使得这些领域将直接受益。更真实的图像编辑能力可以显著提升视觉特效、虚拟场景和角色交互的沉浸感。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 在这些应用中，虚拟物体与真实环境的无缝融合至关重要。物理真实感的提升将使得虚拟内容在现实世界中看起来更加自然和可信。</li>
<li><strong>电子商务和产品可视化：</strong> 能够生成具有正确阴影、反射和材质交互的产品图像，将大大提升在线购物体验和产品展示的吸引力。</li>
<li><strong>数字内容创作和设计：</strong> 艺术家和设计师将能够更轻松地创建出具有物理一致性的图像，减少后期修饰的工作量。</li>
<li><strong>机器人和具身智能：</strong> 机器人需要理解物理世界才能进行有效的交互。虽然不是直接应用，但对物理规律的建模和生成能力，可能间接促进机器人对环境的理解和模拟。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>评估的复杂性与主观性：</strong> 尽管 PICAEval 结合了 VLM 和人工标注，但物理真实感在某些方面仍可能具有一定的主观性。如何确保 VLM 判定的鲁棒性和泛化性，以及人工标注的一致性，是一个持续的挑战。</li>
<li><strong>物理规律的完备性：</strong> 摘要提到了八个子维度（光学、力学、状态转换），但物理世界极其复杂。这些维度是否足以覆盖所有重要的物理效应，以及如何处理更复杂的物理现象（如流体动力学、热力学等），可能是一个潜在的局限。</li>
<li><strong>解决方案的初步性：</strong> 论文提到探索了从视频中学习物理知识并构建了 PICA-100K 数据集，但摘要并未深入阐述这些解决方案的具体效果和局限。它明确指出“物理真实感仍然是一个具有巨大探索空间的挑战性问题”，暗示了现有解决方案可能仍处于早期阶段，距离完全解决问题尚远。</li>
<li><strong>计算资源需求：</strong> 训练能够理解和模拟复杂物理规律的模型，以及运行 VLM-as-a-judge 的评估协议，可能需要大量的计算资源。</li>
<li><strong>数据集的覆盖范围：</strong> PICA-100K 数据集虽然规模大，但其涵盖的物理场景和编辑操作是否足够多样化，以应对所有现实世界的复杂情况，仍需进一步验证。</li>
</ul>
<hr />
<p>总而言之，PICABench 是一项具有前瞻性和重要性的工作，它将图像编辑领域推向了物理真实感这一更深层次的挑战，并为未来的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore.</li>
<li>We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17681v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17681v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17626v1'></a></p>
<h2 id="camit-a-time-aware-car-model-dataset-for-classification-and-generation"><a href="https://arxiv.org/abs/2510.17626v1">CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</a></h2>
<p><strong>Authors:</strong> Frédéric LIN, Biruk Abere Ambaw, Adrian Popescu, Hejer Ammar, Romaric Audigier, Hervé Le Borgne</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Frédéric LIN等人撰写的论文“CaMiT: A Time-Aware Car Model Dataset for Classification and Generation”的全面摘要。</p>
<hr />
<h3 id="camit">论文摘要：CaMiT: 一个用于分类和生成的时间感知汽车模型数据集</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前AI视觉系统在处理随时间演变的视觉数据时面临挑战，尤其是在物体外观会发生变化的领域（如技术产品）。现有的时间感知视觉模型主要关注通用类别或特定传感器数据，缺乏对细粒度技术产品（如汽车模型）长期外观演变进行建模的能力。因此，本文旨在解决如何有效地在视觉模型中对细粒度技术产品随时间变化的描绘进行建模的问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>CaMiT数据集的引入：</strong> 论文核心贡献是推出了CaMiT（Car Models in Time）数据集，这是一个时间感知的细粒度汽车模型数据集。它包含78.7万个190种汽车模型的标注样本（2007-2023年）和510万个未标注样本（2005-2023年），支持监督和自监督学习。该数据集通过半自动化标注流程构建，结合了VLM和监督模型以提高标注准确性。
*   <strong>时间增量分类设置：</strong> 论文提出了一个现实的持续学习场景，即时间增量分类设置，以应对类别出现、演变和消失的问题。
*   <strong>时间增量预训练（TIP）和分类器学习（TICL）：</strong> 针对时间漂移问题，论文评估了两种缓解策略：
    *   <strong>时间增量预训练（TIP）：</strong> 更新骨干模型以适应新数据。
    *   <strong>时间增量分类器学习（TICL）：</strong> 仅更新最终分类层，同时冻结骨干模型。
*   <strong>时间感知图像生成（TAIG）：</strong> 引入了时间感知图像生成任务，通过在训练过程中一致地使用时间元数据（如发布年份）来生成图像，以提高生成内容的真实感。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>静态预训练的有效性：</strong> 在领域内数据上进行静态预训练的模型，即使与大规模通用模型相比，也能达到有竞争力的性能，且资源效率更高。这表明针对特定领域的模型在某些情况下优于通用模型。
*   <strong>时间漂移对准确性的影响：</strong> 静态预训练的模型在跨年份测试时准确性会下降，尤其是在训练和测试年份差距较大时，这证实了时间漂移的存在。
*   <strong>时间增量策略的积极效果：</strong> TIP和TICL策略均能有效缓解时间漂移问题，提高了模型的时间鲁棒性。其中，TICL在跨时间（尤其是回溯测试）上表现出最佳准确性，并且在效率和效果之间取得了良好平衡。这强调了在细粒度分类中，更新分类器以适应概念动态的重要性。
*   <strong>时间感知图像生成的改进：</strong> 通过在训练时利用时间元数据，时间感知图像生成（TAIG）产生的图像比传统生成方法更具真实感和多样性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>选择偏差：</strong> CaMiT数据集受到多种选择偏差的影响，例如数据来源于Flickr（可能影响泛化能力），以及汽车模型和品牌在地理分布和时间覆盖上的不平衡。
*   <strong>标注限制：</strong> 采用半自动化标注流程可能引入偏差，尽管通过多重过滤和验证确保了高准确性。
*   <strong>时间元数据的模糊性：</strong> 数据集中的时间标注可能混淆了真正的设计演变和物理老化（旧车照片），这可能影响时间建模结果的精确性。
*   <strong>数据访问：</strong> 为了遵守版权法规，数据集仅分发图像链接、嵌入和元数据，而非图像本身，这可能导致链接随时间失效，影响可比性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>时间元数据解耦：</strong> 未来工作可以尝试解耦设计演变和物理老化这两个因素，通过将时间戳与官方模型发布日期或注册元数据对齐，更精确地研究设计变化。
*   <strong>更精细的时间感知建模：</strong> 鼓励进一步探索更精细的时间感知建模方法，以应对视觉概念的动态变化。
*   <strong>时间感知生成管道的改进：</strong> 进一步研究更复杂的、包含时间意识的内容生成管道。
*   <strong>扩展到其他领域：</strong> 将CaMiT的经验和方法应用于其他细粒度技术产品或视觉概念，以研究其时间演变。</p>
<hr />
<p>这篇论文通过引入CaMiT数据集，为计算机视觉领域研究细粒度视觉概念的时间演变提供了一个宝贵的资源。它不仅揭示了时间漂移对模型性能的影响，还提出了有效的缓解策略，并为时间感知的图像生成开辟了新的方向，对持续学习和生成模型的发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts.</li>
<li>To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17626v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17626v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17566v1'></a></p>
<h2 id="wp-cracknet-a-collaborative-adversarial-learning-framework-for-end-to-end-weakly-supervised-road-crack-detection"><a href="https://arxiv.org/abs/2510.17566v1">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a></h2>
<p><strong>Authors:</strong> Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, Lihua Xie</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection”论文的全面摘要，用中文写成：</p>
<p><strong>论文摘要：WP-CrackNet：一种用于端到端弱监督道路裂缝检测的协同对抗学习框架</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决智能城市基础设施维护中道路裂缝检测的关键问题。传统的像素级裂缝标注成本高昂且耗时，严重限制了大规模道路检测系统的可扩展性。因此，研究问题是如何开发一种端到端、弱监督的像素级道路裂缝检测方法，仅使用图像级标签就能实现与监督方法相当的性能。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
WP-CrackNet引入了以下关键创新：
*   <strong>端到端弱监督框架：</strong> 提出了一种新颖的端到端弱监督方法，仅使用图像级标签进行像素级裂缝检测，避免了昂贵的像素级标注。
*   <strong>协同对抗学习策略：</strong> 框架集成了三个协同组件——分类器、重构器和检测器，并通过对抗性学习交替训练。分类器生成类激活图（CAMs），重构器衡量特征可推断性，而检测器则从后处理的裂缝CAMs生成的伪标签中学习。这种相互反馈机制提高了学习稳定性和检测精度。
*   <strong>路径感知注意力模块（PAAM）：</strong> 为了进一步提升检测性能，设计了PAAM。它通过建模空间和通道依赖性，融合了分类器的高级语义信息和重构器的低级结构线索。
*   <strong>中心增强CAM一致性模块（CECCM）：</strong> 提出了CECCM，通过中心高斯加权和一致性约束来细化裂缝CAMs，从而生成更好的伪标签。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能可比性：</strong> 在三个自建的图像级数据集上进行的广泛实验表明，WP-CrackNet在像素级裂缝检测方面取得了与监督方法相当的性能。
*   <strong>超越现有弱监督方法：</strong> 该方法显著优于现有的弱监督方法，尤其是在IoU指标上表现出显著提升（例如，在Crack500数据集上IoU提升12.012%-41.633%）。
*   <strong>可扩展性提升：</strong> 通过减少对像素级标注的依赖，WP-CrackNet极大地推动了可扩展的道路检测，使其更适用于大规模基础设施维护。
*   <strong>CAMs质量提升：</strong> 对抗性训练策略使裂缝CAMs能够更完整地覆盖裂缝区域，而CECCM则增强了裂缝CAMs的生成和空间聚合。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>Crack500数据集上的性能下降：</strong> 在Crack500数据集上，WP-CrackNet的IoU略有下降（0.313%至9.085%），这主要归因于：
    *   <strong>场景可变性和裂缝形态多样性：</strong> Crack500包含四种裂缝类型，宽度、长度和分支模式各异，且在不同路面材料和光照条件下采集，背景纹理复杂，增加了裂缝边界定位的难度。
    *   <strong>像素级标签数量：</strong> Crack500数据集的像素级标注数量多于其他数据集，使得监督网络能够学习更精确的几何先验并有效处理细小或部分遮挡的裂缝。WP-CrackNet依赖图像级线索进行隐式定位，这限制了其在这些情况下的边界精度。
*   <strong>模型参数量：</strong> 相较于一些监督方法，WP-CrackNet的模型参数量略大，这主要是由于其多模块协同训练策略。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>模型压缩与加速：</strong> 未来的工作将侧重于通过模型剪枝和知识蒸馏来压缩和加速WP-CrackNet，使其更适合在边缘设备（如无人机）上进行实时部署。
*   <strong>联合训练与领域适应：</strong> 计划结合少量精细的像素级标注进行联合训练，以进一步提高检测性能和增强领域适应能力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17566v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17566v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-21 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
