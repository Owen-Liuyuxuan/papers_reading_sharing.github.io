<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-21 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-20/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-22/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-21">Arxiv Computer Vision Papers - 2025-10-21</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#a-comprehensive-survey-on-world-models-for-embodied-ai" class="nav-link">A Comprehensive Survey on World Models for Embodied AI</a>
                </li>
                <li class="nav-item">
                    <a href="#glyph-scaling-context-windows-via-visual-text-compression" class="nav-link">Glyph: Scaling Context Windows via Visual-Text Compression</a>
                </li>
                <li class="nav-item">
                    <a href="#ultracua-a-foundation-model-for-computer-use-agents-with-hybrid-action" class="nav-link">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a>
                </li>
                <li class="nav-item">
                    <a href="#botany-bot-digital-twin-monitoring-of-occluded-and-underleaf-plant-structures-with-gaussian-splats" class="nav-link">Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</a>
                </li>
                <li class="nav-item">
                    <a href="#elastic-vits-from-pretrained-models-without-retraining" class="nav-link">Elastic ViTs from Pretrained Models without Retraining</a>
                </li>
                <li class="nav-item">
                    <a href="#gas-improving-discretization-of-diffusion-odes-via-generalized-adversarial-solver" class="nav-link">GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</a>
                </li>
                <li class="nav-item">
                    <a href="#multilingual-text-to-image-person-retrieval-via-bidirectional-relation-reasoning-and-aligning" class="nav-link">Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning</a>
                </li>
                <li class="nav-item">
                    <a href="#picabench-how-far-are-we-from-physically-realistic-image-editing" class="nav-link">PICABench: How Far Are We from Physically Realistic Image Editing?</a>
                </li>
                <li class="nav-item">
                    <a href="#camit-a-time-aware-car-model-dataset-for-classification-and-generation" class="nav-link">CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#wp-cracknet-a-collaborative-adversarial-learning-framework-for-end-to-end-weakly-supervised-road-crack-detection" class="nav-link">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-21">Arxiv Computer Vision Papers - 2025-10-21</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ19æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§é¢åæ¯æ¥æ¥åæ§è¡æè¦ (2025å¹´10æ19æ¥)</strong></p>
<p><strong>æ¦è¿°ï¼</strong>
ä»æ¥åå¸ç10ç¯è®ºæå±ç°äºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå¤ä¸ªåæ²¿æ¹åçæ´»è·ç ç©¶ãä¸»è¦è¶å¿åæ¬ï¼<strong>å·èº«æºè½ä¸ä¸çæ¨¡å</strong>çæ·±å¥æ¢ç´¢ã<strong>å¤æ¨¡æä¸é¿ä¸ä¸æçªå£å¤ç</strong>çåæ°ã<strong>åºç¡æ¨¡åå¨å¤æä»»å¡ä¸­çåºç¨</strong>ã<strong>3Dè§è§ä¸æ°å­å­ªç</strong>çç²¾ç»åå»ºæ¨¡ï¼ä»¥å<strong>æ¨¡åæçä¸æ³åè½å</strong>çæåãæ­¤å¤ï¼<strong>å¾åçæä¸ç¼è¾çç©ççå®æ§</strong>ã<strong>ç¹å®é¢åæ°æ®éæå»º</strong>å<strong>å¼±çç£å­¦ä¹ </strong>ä¹æç»­åå°å³æ³¨ã</p>
<p><strong>ä¸»è¦ä¸»é¢ä¸è¶å¿ï¼</strong></p>
<ol>
<li><strong>å·èº«æºè½ä¸ä¸çæ¨¡å (Embodied AI &amp; World Models):</strong> å¤ç¯è®ºæèç¦äºå¦ä½è®©AIæ´å¥½å°çè§£åæä½ç©çä¸çãç¹å«æ¯âä¸çæ¨¡åâä½ä¸ºå·èº«æºè½çæ ¸å¿ç»ä»¶ï¼å¶ç»¼è¿°ååºç¨æ¯éè¦æ¹åã</li>
<li><strong>å¤æ¨¡æä¸é¿ä¸ä¸æå¤ç (Multimodal &amp; Long Context):</strong> éçå¤§æ¨¡åçåå±ï¼å¦ä½ææå¤çååç¼©è§è§-ææ¬ç­å¤æ¨¡æä¿¡æ¯ï¼ä»¥æ¯ææ´é¿çä¸ä¸æçªå£ï¼æä¸ºæåæ¨¡åè½åçå³é®ã</li>
<li><strong>åºç¡æ¨¡åä¸ä»£ç (Foundation Models &amp; Agents):</strong> åºç¡æ¨¡åæ­£è¢«åºç¨äºæ´å¤æçâä»£çâä»»å¡ï¼ä¾å¦è®¡ç®æºä½¿ç¨ä»£çï¼è¿é¢ç¤ºçAIå¨èªå¨åå¤ææä½æ¹é¢çæ½åã</li>
<li><strong>3Dè§è§ä¸æ°å­å­ªç (3D Vision &amp; Digital Twins):</strong> å©ç¨é«æ¯æ³¼æºç­æ°ææ¯è¿è¡ç²¾ç»ç3Déå»ºåçæµï¼å°¤å¶æ¯å¨é®æ¡åå¤æç¯å¢ä¸çåºç¨ï¼æ¯3Dè§è§é¢åçæ°äº®ç¹ã</li>
<li><strong>æ¨¡åæçä¸æ³å (Model Efficiency &amp; Generalization):</strong> å¦ä½å¨ä¸éæ°è®­ç»çæåµä¸ï¼ä½¿ç°ææ¨¡åæ´å·å¼¹æ§ææé«å¶æ³åè½åï¼æ¯ä¼åèµæºåæåå®ç¨æ§çéè¦ç ç©¶ã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°è®ºæï¼</strong></p>
<ul>
<li><strong>"Glyph: Scaling Context Windows via Visual-Text Compression" by Jiale Cheng et al.:</strong> è¿ç¯è®ºæå¨å¤æ¨¡æå¤§æ¨¡åèæ¯ä¸æå·åæ°æ§ãå®æåºäºä¸ç§éè¿è§è§-ææ¬åç¼©æ¥æ©å±ä¸ä¸æçªå£çæ¹æ³ï¼æææ¾èæåå¤æ¨¡ææ¨¡åçå¤çè½ååæçï¼å¯¹äºå¤çé¿ç¯ææ¡£ãè§é¢çè§£ç­ä»»å¡å·æéè¦æä¹ã</li>
<li><strong>"UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action" by Yuhao Yang et al.:</strong> è¿ç¯è®ºæä»£è¡¨äºåºç¡æ¨¡åå¨å·èº«æºè½åèªå¨åé¢åçä¸ä¸ªéè¦è¿å±ãæå»ºä¸ä¸ªè½å¤æ§è¡æ··åå¨ä½çè®¡ç®æºä½¿ç¨ä»£çï¼æ¯å®ç°éç¨AIå©æåèªå¨åå¤ææ°å­ä»»å¡çå³é®ä¸æ­¥ã</li>
<li><strong>"Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats" by Simeon Adebola et al.:</strong> è¿é¡¹å·¥ä½å°åæ²¿ç3Déå»ºææ¯ï¼é«æ¯æ³¼æºï¼åºç¨äºä¸ä¸ªå·ä½çãå·ææææ§çå®éé®é¢ï¼æ¤ç©ç»æçæµï¼ï¼å±ç¤ºäº3Dè§è§å¨åä¸ãçç©ç§å­¦ç­é¢åçå·¨å¤§æ½åï¼å¶ç²¾ç»åå»ºæ¨¡è½åä»¤äººå°è±¡æ·±å»ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>è§è§-ææ¬åç¼© (Visual-Text Compression):</strong> ä½ä¸ºè§£å³å¤æ¨¡ææ¨¡åé¿ä¸ä¸æç¶é¢çå³é®ææ¯ï¼æªæ¥å¯è½ä¼ææ´å¤ç ç©¶å³æ³¨å¶æçåä¿çåº¦ã</li>
<li><strong>æ··åå¨ä½ä»£ç (Hybrid Action Agents):</strong> ç»åä¸åç²åº¦ææ¨¡æçå¨ä½ï¼ä¾å¦é¼ æ ç¹å»ãé®çè¾å¥ãèªç¶è¯­è¨æä»¤ï¼æ¥æ§å¶è®¡ç®æºï¼æ¯å·èº«æºè½å¨æ°å­ä¸çä¸­åºç¨çéè¦æ¹åã</li>
<li><strong>é«æ¯æ³¼æº (Gaussian Splats) å¨å¤æç¯å¢3Déå»ºä¸­çåºç¨:</strong> è¿ç§æ°å´ç3Dè¡¨ç¤ºæ¹æ³å¨ç²¾ç»åãå®æ¶æ§æ¹é¢å±ç°åºå·¨å¤§ä¼å¿ï¼æªæ¥ææå¨æ´å¤é¢ååä»£ä¼ ç»æ¹æ³ã</li>
<li><strong>æ éè®­ç»çå¼¹æ§æ¨¡å (Elastic Models without Retraining):</strong> æ¢ç´¢å¦ä½å¨ä¸è¿è¡æè´µéè®­ç»çæåµä¸ï¼ä½¿æ¨¡åéåºä¸åè®¡ç®èµæºæä»»å¡éæ±ï¼æ¯æ¨¡åé¨ç½²åå¯æç»­æ§çéè¦æ¹åã</li>
<li><strong>å¹¿ä¹å¯¹ææ±è§£å¨ (Generalized Adversarial Solver) æ¹è¿æ©æ£æ¨¡åç¦»æ£å:</strong> ç»åå¯¹æå­¦ä¹ æ¥ä¼åæ©æ£æ¨¡åçéæ ·è¿ç¨ï¼æææåçæè´¨éåæçã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<ol>
<li><strong>"Glyph: Scaling Context Windows via Visual-Text Compression" by Jiale Cheng et al.:</strong> å¯¹äºå³æ³¨å¤æ¨¡æå¤§æ¨¡åãé¿ä¸ä¸æå¤çåæçä¼åçç ç©¶äººåï¼è¿ç¯è®ºææ¯å¿è¯»çã</li>
<li><strong>"UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action" by Yuhao Yang et al.:</strong> å¦ææ¨å¯¹å·èº«æºè½ãAIä»£çãèªå¨åæåºç¡æ¨¡åå¨å¤æä»»å¡ä¸­çåºç¨æå´è¶£ï¼è¿ç¯è®ºææä¾äºåç»æ§çè§è§ã</li>
<li><strong>"Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats" by Simeon Adebola et al.:</strong> å¯¹äº3Dè§è§ãæ°å­å­ªçãé«æ¯æ³¼æºææ¯åå¶å¨å®éåºç¨ä¸­çæ½åæå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºææä¾äºé«è´¨éçæ¡ä¾ç ç©¶ã</li>
<li><strong>"A Comprehensive Survey on World Models for Embodied AI" by Xinqing Li et al.:</strong> å¯¹äºå¸æå¨é¢äºè§£å·èº«æºè½ä¸­âä¸çæ¨¡åâåå±ç°ç¶ãææåæªæ¥æ¹åçç ç©¶äººåï¼è¿ç¯ç»¼è¿°æ¯æä½³çèµ·ç¹ã</li>
</ol>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæãå»ºè®®æ ¹æ®æ¨çå·ä½å´è¶£ï¼è¿ä¸æ­¥æ·±å¥éè¯»æ¨èçè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.16732v1">A Comprehensive Survey on World Models for Embodied AI</a></li>
<li><a href="#2510.17800v1">Glyph: Scaling Context Windows via Visual-Text Compression</a></li>
<li><a href="#2510.17790v1">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a></li>
<li><a href="#2510.17783v1">Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</a></li>
<li><a href="#2510.17700v1">Elastic ViTs from Pretrained Models without Retraining</a></li>
<li><a href="#2510.17699v1">GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</a></li>
<li><a href="#2510.17685v1">Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning</a></li>
<li><a href="#2510.17681v1">PICABench: How Far Are We from Physically Realistic Image Editing?</a></li>
<li><a href="#2510.17626v1">CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</a></li>
<li><a href="#2510.17566v1">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.16732v1'></a></p>
<h2 id="a-comprehensive-survey-on-world-models-for-embodied-ai"><a href="https://arxiv.org/abs/2510.16732v1">A Comprehensive Survey on World Models for Embodied AI</a></h2>
<p><strong>Authors:</strong> Xinqing Li, Xin He, Le Zhang, Yun Liu</p>
<p><strong>Published:</strong> 2025-10-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºXinqing Li, Xin He, Le Zhang, Yun Liuæ°åçè®ºæâA Comprehensive Survey on World Models for Embodied AIâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼A Comprehensive Survey on World Models for Embodied AI</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¿ç¯ç»¼è¿°è®ºææ¨å¨è§£å³å·èº«AIï¼Embodied AIï¼é¢åä¸­ä¸çæ¨¡åï¼World Modelsï¼çç¢çåç ç©¶ç°ç¶ãå·èº«AIè¦æ±æºè½ä½è½å¤æç¥å¤æçå¤æ¨¡æç¯å¢ãå¨å¶ä¸­è¡å¨ï¼å¹¶é¢æµå¶è¡å¨å¦ä½æ¹åæªæ¥çä¸çç¶æãä¸çæ¨¡åä½ä¸ºåé¨æ¨¡æå¨ï¼ææç¯å¢å¨æï¼æ¯ææç¥ãé¢æµåå³ç­å¶å®ä¸­çååä¸åäºå®æ¨æ¼ãç¶èï¼å½åä¸çæ¨¡åç ç©¶ç¼ºä¹ç»ä¸çæ¡æ¶ãæ¯è¯­ååç±»ï¼å¯¼è´ä¸åå­ç¤¾åºä¹é´å­å¨ä¸ä¸è´æ§ï¼è¿é»ç¢äºè¯¥é¢åçç³»ç»æ§è¿å±åè·¨é¢åæ³åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿è´¡ç®å¨äºæåºäºä¸ä¸ªç»ä¸çä¸è½´åç±»æ¡æ¶ï¼ç¨äºç»ç»å·èº«AIä¸­çä¸çæ¨¡åç ç©¶ï¼
*   <strong>åè½æ§ï¼Functionalityï¼ï¼</strong> å°ä¸çæ¨¡ååä¸ºâå³ç­è¦ååâï¼Decision-Coupledï¼åâéç¨åâï¼General-Purposeï¼ãå³ç­è¦ååæ¨¡åæ¯ä»»å¡ç¹å®çï¼ä¸ºç¹å®å³ç­ä»»å¡ä¼åå­¦ä¹ å¨æï¼éç¨åæ¨¡åæ¯ä»»å¡æ å³çæ¨¡æå¨ï¼ä¸æ³¨äºå¹¿æ³é¢æµï¼ä»¥å®ç°è·¨åç§ä¸æ¸¸åºç¨çæ³åã
*   <strong>æ¶é´å»ºæ¨¡ï¼Temporal Modelingï¼ï¼</strong> åºåäºâé¡ºåºæ¨¡æä¸æ¨çâï¼Sequential Simulation and Inferenceï¼åâå¨å±å·®å¼é¢æµâï¼Global Difference Predictionï¼ãåèä»¥èªåå½æ¹å¼éæ­¥å±å¼æªæ¥ç¶æï¼åèå¹¶è¡ä¼°è®¡æ´ä¸ªæªæ¥ç¶æã
*   <strong>ç©ºé´è¡¨ç¤ºï¼Spatial Representationï¼ï¼</strong> æ¶µçäºåç§ä¸»è¦ç­ç¥ï¼âå¨å±æ½å¨åéâï¼Global Latent Vectorï¼ãâä»¤çç¹å¾åºåâï¼Token Feature Sequenceï¼ãâç©ºé´æ½å¨ç½æ ¼âï¼Spatial Latent Gridï¼åâåè§£æ¸²æè¡¨ç¤ºâï¼Decomposed Rendering Representationï¼ï¼è¿äºç­ç¥å¨æçãè¡¨è¾¾è½ååç©çä¿çåº¦ä¹é´è¿è¡æè¡¡ã</p>
<p>æ­¤å¤ï¼è®ºæè¿ç³»ç»åäºæºå¨äººãèªå¨é©¾é©¶åéç¨è§é¢è®¾ç½®ä¸­çæ°æ®èµæºåè¯ä¼°ææ ï¼æ¶µçäºåç´ é¢æµè´¨éãç¶æçº§çè§£åä»»å¡æ§è½ï¼å¹¶å¯¹æåè¿çæ¨¡åè¿è¡äºå®éæ¯è¾ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥ç»¼è¿°éè¿å¶æåºçä¸è½´åç±»æ¡æ¶ï¼æ¸æ°å°ç»ç»ååé¡¾äºç°æä¸çæ¨¡åç ç©¶ï¼æ­ç¤ºäºä¸åæ¹æ³å¨åè½ãæ¶é´å»ºæ¨¡åç©ºé´è¡¨ç¤ºä¸çè®¾è®¡éæ©åæè¡¡ãéè¿å¯¹ç°ææ°æ®èµæºåè¯ä¼°ææ çç³»ç»åï¼è®ºæä¸ºæªæ¥çç ç©¶æä¾äºæ ååçæ¯è¾åºç¡ãå®éæ¯è¾é¨åå±ç¤ºäºå½åæåè¿æ¨¡åå¨åç´ çæãåºæ¯çè§£åæ§å¶ä»»å¡ä¸çè¡¨ç°ï¼ä¾å¦å¨nuScenesæ°æ®éä¸çé©¾é©¶è§é¢çæä¸­ï¼DrivePhysicaå¨è§è§ä¿çåº¦ä¸è¡¨ç°æä½³ï¼èMiLAå¨æ¶é´è¿è´¯æ§ä¸æå¼ºãå¨Occ3D-nuScenesä¸ç4Då ç¨é¢æµä¸­ï¼COMEï¼ç»åGT egoï¼åå¾äºæä½³çå¹³åmIoUåæ¯è§è·mIoUãè¿äºç»æçªåºäºä¸åæ¨¡åå¨ç¹å®ä»»å¡åè¯ä¼°ç»´åº¦ä¸çä¼å¿ï¼å¹¶å¼ºè°äºå¨æçãä¿çåº¦åæ³åè½åä¹é´è¿è¡æè¡¡çå¿è¦æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææç¡®æåºäºå½åä¸çæ¨¡åç ç©¶çå ä¸ªä¸»è¦å±éæ§ï¼
*   <strong>æ°æ®ç¨ç¼ºä¸å¼ææ§ï¼</strong> å·èº«AIé¢åç¼ºä¹ç»ä¸çå¤§è§æ¨¡æ°æ®éï¼å¯¼è´æ¨¡åæ³åè½ååéã
*   <strong>è¯ä¼°ææ ä¸è¶³ï¼</strong> ç°æè¯ä¼°ææ ï¼å¦FIDåFVDï¼ä¾§éäºåç´ ä¿çåº¦ï¼ä½å¾å¾å¿½ç¥äºç©çä¸è´æ§ãå¨ææ§åå æå³ç³»ãç¼ºä¹è·¨é¢åæ ååçè¯ä¼°æ¡æ¶ã
*   <strong>è®¡ç®æçä¸æ§è½æè¡¡ï¼</strong> å°½ç®¡TransformeråDiffusionæ¨¡åè¡¨ç°åºè²ï¼ä½å¶é«æçæ¨çææ¬ä¸æºå¨äººç³»ç»å®æ¶æ§å¶çéæ±ç¸å²çªãä¼ ç»RNNåå¨å±æ½å¨åéè½ç¶æçé«ï¼ä½å¨ææé¿æä¾èµæ¹é¢å­å¨å±éã
*   <strong>é¿ææ¶é´ä¸è´æ§ä¸è¯¯å·®ç´¯ç§¯ï¼</strong> èªåå½è®¾è®¡è½ç¶ç´§åä¸æ ·æ¬é«æï¼ä½ä¼éçæ¶é´ç´¯ç§¯è¯¯å·®ï¼å¨å±é¢æµè½ç¶æé«äºå¤æ­¥è¿è´¯æ§ï¼ä½è®¡ç®ææ¬é«ä¸é­ç¯äº¤äºæ§è¾å¼±ã
*   <strong>å¨æåºæ¯ä¸çç©ºé´è¡¨ç¤ºï¼</strong> æ½å¨åéãä»¤çåºååç©ºé´ç½æ ¼å¨æçåè¡¨è¾¾è½åä¹é´å­å¨æè¡¡ï¼åè§£æ¸²ææ¹æ³ï¼å¦NeRFå3DGSï¼è½ç¶ä¿çåº¦é«ï¼ä½å¨å¨æåºæ¯ä¸­æ©å±æ§å·®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>ç»ä¸æ°æ®éä¸è¯ä¼°æ¡æ¶ï¼</strong> ä¼åæå»ºç»ä¸çå¤æ¨¡æãè·¨é¢åæ°æ®éï¼ä»¥å®ç°å¯è¿ç§»çé¢è®­ç»ãå¼åè¶è¶æç¥çå®æçè¯ä¼°æ¡æ¶ï¼ä»¥è¯ä¼°ç©çä¸è´æ§ãå ææ¨çåé¿æå¨æã
*   <strong>è®¡ç®æçä¼åï¼</strong> å³æ³¨æ¨¡åæ¶æä¼åï¼å¦éåãåªæåç¨çè®¡ç®ï¼ä»¥éä½æ¨çå»¶è¿ãæ¢ç´¢æ°çæ¶é´å»ºæ¨¡æ¹æ³ï¼å¦ç¶æç©ºé´æ¨¡åï¼SSMsï¼ä¾å¦Mambaï¼ï¼ä»¥å¨ä¿æå®æ¶æççåæ¶å¢å¼ºé¿ææ¨çè½åã
*   <strong>æ··åå»ºæ¨¡ç­ç¥ï¼</strong> æ´åèªåå½åå¨å±é¢æµæ¹æ³çä¼å¿ï¼ä»¥å¹³è¡¡æçãä¿çåº¦åäº¤äºæ§ã
*   <strong>å¢å¼ºé¿ææ¶é´ä¸è´æ§ï¼</strong> å¼å¥æ¾å¼è®°å¿æåå±è§åæ¥æé«é¿æé¢æµçç¨³å®æ§ãå©ç¨CoTï¼Chain-of-Thoughtï¼å¯åå¼ä»»å¡åè§£ï¼éè¿ä¸­é´ç®æ è®¾å®æ¥æ¹åæ¶é´ä¸è´æ§ã
*   <strong>ç»ä¸æ¶æï¼</strong> å¼åè½å¤ææå¹³è¡¡æçãä¿çåº¦åäº¤äºæ§çç»ä¸æ¶æï¼æ ç¼æ´åæ¶é´ä¸ç©ºé´å»ºæ¨¡ã
*   <strong>ç©çåºç¡çåºåä¸æ¶æï¼</strong> åå»ºç»ä¸çãç©çåºç¡çåºåï¼å¹¶æ¢ç´¢é«æçæ¶æï¼ä»¥å®ç°å·èº«AIçä¸ä¸ä»£ä¸çæ¨¡åã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºå·èº«AIä¸­çä¸çæ¨¡åç ç©¶æä¾äºä¸ä¸ªå¨é¢ä¸ç»æåçè§è§ï¼ä¸ä»æ»ç»äºç°æè¿å±ï¼æ´æ¸æ°å°æåºäºæªæ¥çç ç©¶æ¹ååææï¼å¯¹äºæ¨å¨è¯¥é¢åçåå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.16732v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.16732v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17800v1'></a></p>
<h2 id="glyph-scaling-context-windows-via-visual-text-compression"><a href="https://arxiv.org/abs/2510.17800v1">Glyph: Scaling Context Windows via Visual-Text Compression</a></h2>
<p><strong>Authors:</strong> Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºJiale Chengç­äººæ°åçè®ºæâGlyph: Scaling Context Windows via Visual-Text Compressionâçå¨é¢æè¦ï¼ä½¿ç¨ä¸­æä¹¦åï¼</p>
<p><strong>è®ºææè¦ï¼Glyph: éè¿è§è§-ææ¬åç¼©æ©å±ä¸ä¸æçªå£</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨å¤çé¿ä¸ä¸æä»»å¡ï¼å¦ææ¡£çè§£ãä»£ç åæåå¤æ­¥æ¨çï¼æ¹é¢æ¥çéè¦ãç¶èï¼å°ä¸ä¸æçªå£æ©å±å°ç¾ä¸çº§å«ä¼å¸¦æ¥å·¨å¤§çè®¡ç®ååå­ææ¬ï¼ä¸¥ééå¶äºé¿ä¸ä¸æLLMsçå®ç¨æ§ãæ¬ææ¨å¨è§£å³è¿ä¸ææï¼å³å¦ä½å¨ä¸çºç²æ§è½çåæä¸ï¼æææ©å±LLMså¤çé¿ææ¬çè½åï¼åæ¶é¿åä¼ ç»æ¹æ³å¸¦æ¥çé«æææ¬ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäºä¸ç§åä¸º <strong>Glyph</strong> çæ°é¢æ¡æ¶ï¼éè¿âè§è§ä¸ä¸ææ©å±âæ¥è§£å³ä¸è¿°é®é¢ãå¶æ ¸å¿åæ°ç¹åæ¬ï¼
*   <strong>è§è§-ææ¬åç¼©èå¼ï¼</strong> Glyph ä¸ç´æ¥æ©å±åºäºtokençåºåï¼èæ¯å°é¿ææ¬æ¸²ææç´§åçå¾åï¼å¹¶ä½¿ç¨è§è§-è¯­è¨æ¨¡åï¼VLMsï¼è¿è¡å¤çãè¿ç§æ¹æ³æ¾èåç¼©äºææ¬è¾å¥ï¼åæ¶ä¿çäºè¯­ä¹ä¿¡æ¯ï¼å°æ¯ä¸ªè§è§tokenè§ä¸ºå¤ä¸ªææ¬tokençç´§åè½½ä½ï¼ä»èæé«äºä¿¡æ¯å¯åº¦ã
*   <strong>LLMé©±å¨çéä¼ æç´¢ï¼</strong> ä¸ºäºå¹³è¡¡åç¡®æ§ååç¼©çï¼Glyph è®¾è®¡äºä¸ä¸ªLLMé©±å¨çéä¼ æç´¢ç®æ³ï¼èªå¨è¯å«æä½³çè§è§æ¸²æéç½®ï¼ä¾å¦å­ä½å¤§å°ãå¸å±ãåè¾¨çï¼ã
*   <strong>ä¸é¶æ®µè®­ç»æµç¨ï¼</strong>
    *   <strong>æç»­é¢è®­ç»ï¼</strong> ä½¿VLMè½å¤çè§£åæ¨çå·æä¸åè§è§é£æ ¼çæ¸²æé¿ææ¬ã
    *   <strong>LLMé©±å¨çæ¸²ææç´¢ï¼</strong> èªå¨åç°ä¸æ¸¸ä»»å¡çæä½³æ¸²æéç½®ã
    *   <strong>åè®­ç»ï¼</strong> å¨åç°çæä½³éç½®ä¸è¿è¡çç£å¾®è°ï¼SFTï¼åå¼ºåå­¦ä¹ ï¼RLï¼ï¼å¹¶è¾ä»¥è¾å©OCRå¯¹é½ä»»å¡ï¼ä»¥è¿ä¸æ­¥æé«æ¨¡åå¨è§è§åç¼©è¾å¥ä¸çé¿ä¸ä¸æè½ååææ¬è¯å«è½åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>æ¾èçææ¬åç¼©ï¼</strong> Glyph å¨åç§é¿ä¸ä¸æåºåæµè¯ä¸å®ç°äº3-4åçtokenåç¼©ï¼åæ¶ä¿æäºä¸Qwen3-8Bç­é¢åLLMsç¸å½çåç¡®æ§ãå¨æç«¯åç¼©ä¸ï¼ä¸ä¸ª128Kä¸ä¸æçVLMå¯ä»¥å¤ç1M tokençº§å«çææ¬ä»»å¡ã
*   <strong>æ¨çåè®­ç»æçæåï¼</strong> è¿ç§åç¼©å¸¦æ¥äºæ¾èçæçæåï¼é¢å¡«ååè§£ç éåº¦æé«äºçº¦4åï¼SFTè®­ç»éåº¦æé«äºçº¦2åã
*   <strong>è·¨æ¨¡ææ³åè½åï¼</strong> æ¸²æçææ¬æ°æ®æå©äºå¤ççå®ä¸ççå¤æ¨¡æä»»å¡ï¼å¦ææ¡£çè§£ã
*   <strong>ä¸ä¸ææ©å±æ½åï¼</strong> å®éªè¡¨æï¼Glyph å¨é¿ä¸ä¸æä»»å¡ä¸­è¡¨ç°åºæ´ç¨³å®çæ§è½éåï¼éçè¾å¥é¿åº¦çå¢å ï¼å¶ä¼å¿æåææ¾ï¼å±ç°äºå°ææä¸ä¸ææ©å±å°è¿è¶å½åéå¶çæ½åï¼ä¾å¦ï¼å¤ç4Mçè³8Mä¸ä¸ætokenï¼ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ¸²æåæ°çæææ§ï¼</strong> æ¨¡åçæ§è½ä¼åå°æ¸²æéç½®ï¼å¦åè¾¨çãå­ä½ãé´è·ï¼çæ¾èå½±åãå°½ç®¡éä¼ æç´¢è½æ¾å°å¥½çéç½®ï¼ä½å¦ä½ä½¿æ¨¡åå¯¹åç§æ¸²æè®¾ç½®æ´å·é²æ£æ§ä»æ¯ä¸ä¸ªå¼æ¾é®é¢ã
*   <strong>OCRç¸å³ææï¼</strong> å¨Ruleråºåæµè¯ä¸­ï¼UUIDè¯å«å¯¹å½åVLMæ¥è¯´ä»ç¶æå·æææ§ï¼å³ä½¿æ¯æå¼ºçæ¨¡åä¹é¾ä»¥æ­£ç¡®å¤ç°ãè¿ç§ç½è§çå­æ¯æ°å­åºåå¯è½ç±äºè®­ç»æ°æ®ä¸­çåå¸ç¨çæ§æè§è§ç¼ç å¨çæ¶æéå¶å¯¼è´å­ç¬¦éåºæéè¯¯åç±»ã
*   <strong>ä»»å¡å¤æ ·æ§ï¼</strong> æ¬æçåºåæµè¯ä¸»è¦éä¸­å¨é¿ä¸ä¸æçè§£ï¼æªè½å®å¨æ¶µççå®ä¸çåºç¨çå¤æ ·æ§ï¼å¦ä»£çææ¨çå¯éåä»»å¡ãä¸çº¯ææ¬æ¨¡åç¸æ¯ï¼è§è§-ææ¬æ¨¡åå¨ä»»å¡é´çæ³åè½åè¾å¼±ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>èªéåºæ¸²ææ¨¡åï¼</strong> è®­ç»è½å¤æ ¹æ®ä»»å¡ç±»åæç¨æ·æ¥è¯¢è°æ´æ¸²æç­ç¥çæ¨¡åï¼ä»¥å¹³è¡¡åç¼©åæ§è½ã
*   <strong>å¢å¼ºè§è§ç¼ç å¨ï¼</strong> æé«è§è§ç¼ç å¨å¯¹ç»ç²åº¦ææ¬è¯å«åä¸è¯­è¨è¡¨ç¤ºå¯¹é½çè½åï¼ä»¥æåè·¨ä»»å¡çé²æ£æ§åå¯è¿ç§»æ§ã
*   <strong>è§è§-ææ¬ä¸çº¯ææ¬æ¨¡åçå¯¹é½ï¼</strong> éè¿ç¥è¯è¸é¦æè·¨æ¨¡æçç£ç­æ¹æ³ï¼ç¼©å°è§è§-ææ¬æ¨¡åä¸çº¯ææ¬æ¨¡åå¨æ³åè½åä¸çå·®è·ã
*   <strong>æ©å±å°æ´å¹¿æ³çåºç¨ï¼</strong> å°Glyphåºç¨äºä»£çè®°å¿ç³»ç»ãç®¡çé¿æå¯¹è¯æå©ç¨ç»æåè§è§å¸å±è¿è¡æ¨çåæ£ç´¢çä»»å¡ã
*   <strong>ä¸ä¸æå·¥ç¨ä¼åï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¦ä½ä¼åä¸ä¸æä¿¡æ¯çè¡¨ç¤ºåç®¡çï¼ä»¥å®ç°ä»1Må°10Mè¾å¥tokençä¸ä¸ææ©å±ã</p>
<p>æ»èè¨ä¹ï¼Glyph æåºäºä¸ç§æ°é¢ä¸é«æçé¿ä¸ä¸æå»ºæ¨¡èå¼ï¼éè¿è§è§-ææ¬åç¼©åæäºä¼ ç»LLMsçè®¡ç®ååå­ç¶é¢ï¼ä¸ºæªæ¥LLMsçä¸ä¸ææ©å±æä¾äºæ°çæ¹ååå·¨å¤§çæ½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs).</li>
<li>Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17800v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17800v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17790v1'></a></p>
<h2 id="ultracua-a-foundation-model-for-computer-use-agents-with-hybrid-action"><a href="https://arxiv.org/abs/2510.17790v1">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a></h2>
<p><strong>Authors:</strong> Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yuhao Yangç­äººæ°åçè®ºæâUltraCUA: A Foundation Model for Computer Use Agents with Hybrid Actionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="ultracua">è®ºææè¦ï¼UltraCUA: è®¡ç®æºä½¿ç¨æºè½ä½çæ··åå¨ä½åºç¡æ¨¡å</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åè®¡ç®æºä½¿ç¨æºè½ä½ï¼CUAsï¼ä¸»è¦ä¾èµäºåå§çå¾å½¢ç¨æ·çé¢ï¼GUIï¼å¨ä½ï¼å¦ç¹å»ãè¾å¥ãæ»å¨ï¼ï¼è¿å¯¼è´äºåé¿çæ§è¡é¾ãçº§èéè¯¯åæ§è½ç¶é¢ãä¸å©ç¨ä¸°å¯çç¨åºåæ¥å£ï¼APIãMCPæå¡å¨ãå·¥å·ï¼çå¶ä»æºè½ä½ä¸åï¼CUAså¨è¿äºè½åæ¹é¢æ¯å­¤ç«çãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¼¥åGUIåå§æä½ä¸é«çº§ç¨åºåå·¥å·è°ç¨ä¹é´çå·®è·ï¼ä»¥å®ç°æ´é²æ£ãé«æåç»ä¸çè®¡ç®æºä½¿ç¨èªå¨åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
UltraCUAéè¿å¼å¥<strong>æ··åå¨ä½</strong>æºå¶æ¥è§£å³ä¸è¿°é®é¢ï¼æ ç¼éæäºGUIåå§æä½åé«çº§ç¨åºåå·¥å·è°ç¨ãå¶æ¹æ³è®ºåå«åä¸ªå³é®ç»æé¨åï¼</p>
<ul>
<li><strong>ç¨åºåå·¥å·çèªå¨åæ¶éç®¡éï¼</strong> è®ºæå¼åäºä¸ä¸ªå¯æ©å±çç®¡éï¼ä»è½¯ä»¶ææ¡£ãå¼æºä»åºåä»£ç çæä¸­èªå¨æååéæç¨åºåå·¥å·ï¼çè³è½æéçææ°å·¥å·ï¼ä»èæå»ºäºä¸ä¸ªåå«æ°ç¾ç§å·¥å·çä¸°å¯å·¥å·éã</li>
<li><strong>å¯éªè¯è®¡ç®æºä½¿ç¨ä»»å¡çåç®¡éåææ°æ®å¼æï¼</strong> ä¸ºäºè§£å³å¤§è§æ¨¡CUAè®­ç»æ°æ®çæåéªè¯çææï¼è®ºæè®¾è®¡äºä¸ä¸ªåç®¡éå¼æï¼çæäºè¶è¿17,000ä¸ªå¯éªè¯ççå®ä¸çè®¡ç®æºä½¿ç¨ä»»å¡ãè¿åæ¬âè¯ä¼°å¨ä¼åâçæï¼ç¡®ä¿å¯éªè¯æ§ï¼åâæä»¤ä¼åâçæï¼æä¾å¤æ ·æ§åä¸ä¸æç¸å³æ§ï¼ã</li>
<li><strong>å¤§è§æ¨¡é«è´¨éæ··åå¨ä½è½¨è¿¹æ¶éï¼</strong> è®ºææ¶éäºè¶è¿20,000æ¡æåçæ··åå¨ä½è½¨è¿¹ï¼è¿äºè½¨è¿¹ç»åäºä½çº§GUIå¨ä½åé«çº§ç¨åºåå·¥å·è°ç¨ãéè¿ç»åå¼ºå¤§çè§åå¨æ¨¡åï¼OpenAI 03ï¼ååè¿çè§è§å®ä½æ¨¡åï¼GTA1-7Bï¼ï¼æºè½ä½è½å¤æ ¹æ®ä»»å¡ä¸ä¸æå¨ä¸åå¨ä½æ¨¡å¼é´è¿è¡ç­ç¥æ§åæ¢ã</li>
<li><strong>ä¸¤é¶æ®µè®­ç»ç®¡éï¼</strong> éç¨çç£å¾®è°ï¼SFTï¼åå¨çº¿å¼ºåå­¦ä¹ ï¼RLï¼ç¸ç»åçä¸¤é¶æ®µè®­ç»æ¹æ³ãSFTé¶æ®µå¨é«è´¨éè½¨è¿¹ä¸è¿è¡ï¼å»ºç«æ··åå¨ä½è½åï¼RLé¶æ®µéè¿èªåå¼ä¼åå¨ä½éæ©ï¼å®ç°ä½çº§åé«çº§å¨ä½ä¹é´çç­ç¥æ§äº¤æ¿ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
å®éªç»æè¡¨æUltraCUAæ¨¡åå¨æ§è½ä¸æ¾èä¼äºç°ææåè¿çæºè½ä½ï¼</p>
<ul>
<li><strong>OSWorldåºåæµè¯ï¼</strong> UltraCUAç7Bå32Bæ¨¡åå¨OSWorldä¸å®ç°äºå¹³å22%çç¸å¯¹æ§è½æåï¼å¹¶ä¸å¨æ­¥éª¤æ°ä¸å¿«äº11%ãä¾å¦ï¼UltraCUA-7Bå¨15æ­¥åæåçè¾¾å°28.9%ï¼è¶è¶äºææå¯æ¯è¾ç7Bæ¨¡åï¼å¦UI-TARS-1.5-7Bç23.4%ï¼ã</li>
<li><strong>è·¨å¹³å°æ³åè½åï¼</strong> å¨WindowsAgentArenaä¸çåå¤è¯ä¼°æ¾ç¤ºï¼UltraCUA-7Bæ¨¡åå¨æªç»Windowsç¹å®è®­ç»çæåµä¸ï¼æåçè¾¾å°21.7%ï¼ä¼äºå¨Windowsæ°æ®ä¸è®­ç»çåºçº¿æ¨¡åãè¿éªè¯äºæ··åå¨ä½ç­ç¥çè·¨æä½ç³»ç»å¯è¿ç§»æ§ã</li>
<li><strong>æ··åå¨ä½æºå¶çå³é®æ§ï¼</strong> æ··åå¨ä½æºå¶è¢«è¯ææ¯è³å³éè¦çï¼å®å¨ä¿ææ§è¡æççåæ¶åå°äºéè¯¯ä¼ æ­ãæ¶èç ç©¶è¿ä¸æ­¥è¯å®ï¼æ··åå¨ä½ç©ºé´ãå·¥ä½è®°å¿æºå¶åå¼ºåå­¦ä¹ é¶æ®µé½å¯¹æºè½ä½æ§è½æç§¯æå½±åã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®ååºâå±éæ§âé¨åï¼ä½ä»æè¿°ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çææææªå®å¨è§£å³çé®é¢ï¼</p>
<ul>
<li><strong>å·¥å·è¯­æ³å¤ææ§ï¼</strong> å¨RLè®­ç»æ©æï¼æ¨¡åå¨å¤çå¤æçå·¥å·è¯­æ³æ¶ä¼éå°å°é¾ï¼è¿å¯è½å¯¼è´æ ¼å¼æ©ç½ä¸»å¯¼å­¦ä¹ ä¿¡å·ãè®ºæéè¿ä¸æ³¨äºç»æåå·¥å·ä½¿ç¨å¥å±æ¥ç¼è§£ï¼ä½ä»æç¤ºäºè¿ä¸ææã</li>
<li><strong>OODå·¥å·æ³åä¸­çéåºææï¼</strong> å°½ç®¡æ¨¡åè½å¤éåºæªè§è¿çå·¥å·ï¼ä½æ­¥éª¤æ°çå¢å è¡¨æéåºè¿ç¨ä»å­å¨ææï¼æ¨¡åå¯è½ä¼å¨éæ©åéçå·¥å·ä¹åæ¢ç´¢ä¸çæçå·¥å·ã</li>
<li><strong>æ¨çéåº¦ï¼</strong> å¯¹äºOpenCUAç³»åæ¨¡åï¼ç±äºæ¨çéåº¦ååºç¡è®¾æ½çæ¬¡ä¼ï¼æ´ä½å¹³åè¿è¡æ¬¡æ°å°äº4æ¬¡ï¼è¿å¯è½æç¤ºäºå¨å®éé¨ç½²ä¸­ä»éä¼åæ¨çæçã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸ºæªæ¥çç ç©¶å¥ å®äºåºç¡ï¼å¯ä»¥ä»ä»¥ä¸å ä¸ªæ¹é¢è¿è¡æ¢ç´¢ï¼</p>
<ul>
<li><strong>æ´é«æçå·¥å·å­¦ä¹ ï¼</strong> è¿ä¸æ­¥ä¼åå¼ºåå­¦ä¹ ç­ç¥ï¼ä»¥æ´ææå°å­¦ä¹ åææ¡å¤æçå·¥å·è¯­æ³ï¼åå°æ©æè®­ç»ä¸­çæ ¼å¼éè¯¯ã</li>
<li><strong>å¢å¼ºOODå·¥å·æ³åè½åï¼</strong> æ¢ç´¢æ´é²æ£çæºå¶ï¼ä½¿æºè½ä½è½å¤æ´é«æå°éåºåå©ç¨å¨è®­ç»ä¸­æªè§è¿çç¨åºåå·¥å·ï¼åå°éåºè¿ç¨ä¸­çé¢å¤æ­¥éª¤ã</li>
<li><strong>æ¨çæçä¼åï¼</strong> éå¯¹æ¨¡åå¨æ¨çéåº¦ååºç¡è®¾æ½æ¹é¢çææï¼å¯ä»¥ç ç©¶æ´è½»éçº§çæ¨¡åæ¶æãæ´é«æçæ¨çç®æ³æç¡¬ä»¶å éï¼ä»¥æé«å®æ¶åºç¨ä¸­çæ§è½ã</li>
<li><strong>æ´å¹¿æ³çåºç¨åºæ¯ï¼</strong> å°UltraCUAçæ··åå¨ä½èå¼æ©å±å°æ´å¤æ ·åçåºç¨é¢ååæ´å¤æççå®ä¸çä»»å¡ä¸­ï¼è¿ä¸æ­¥éªè¯å¶éç¨æ§åé²æ£æ§ã</li>
<li><strong>å¤æ¨¡ææç¥ä¸çè§£çæ·±åº¦èåï¼</strong> è¿ä¸æ­¥æ¢ç´¢è§è§æ¥å°ä¸ç¨åºåæºè½çæ·±åº¦èåï¼ä½¿æºè½ä½è½å¤æ´ç²¾ç»å°çè§£ç¨æ·çé¢åç´ åä»»å¡ä¸ä¸æï¼ä»èååºæ´æºè½çå¨ä½å³ç­ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼UltraCUAéè¿å¶åæ°çæ··åå¨ä½ç©ºé´åå¨é¢çæ¹æ³è®ºï¼æåå°å¼¥åäºGUIåå§æä½ä¸é«çº§ç¨åºåå·¥å·è°ç¨ä¹é´çé¸¿æ²ï¼ä¸ºæå»ºæ´å¼ºå¤§ãé«æåéç¨çè®¡ç®æºä½¿ç¨æºè½ä½å¼è¾äºæ°éå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions.</li>
<li>Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17790v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17790v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17783v1'></a></p>
<h2 id="botany-bot-digital-twin-monitoring-of-occluded-and-underleaf-plant-structures-with-gaussian-splats"><a href="https://arxiv.org/abs/2510.17783v1">Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</a></h2>
<p><strong>Authors:</strong> Simeon Adebola, Chung Min Kim, Justin Kerr, Shuangyu Xie, Prithvi Akella, Jose Luis Susa Rincon, Eugen Solowjow, Ken Goldberg</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæä»ç»äºä¸ä¸ªåä¸º Botany-Bot çåæ°ç³»ç»ï¼æ¨å¨éè¿ç»åæºå¨äººæä½å 3D é«æ¯æ³¼æºæ¨¡åï¼åå»ºæ¤ç©çè¯¦ç»âå¸¦æ³¨éçæ°å­å­ªçâãå¶æ ¸å¿è´¡ç®å¨äºè§£å³äºä¼ ç»åºå®ç¸æºæ¤ç©è¡¨åç³»ç»å å¶çé®æ¡èæ æ³è·åæ¤ç©ç»èçé®é¢ï¼ç¹å«æ¯éè¿æºå¨äººç®æ³ä¸»å¨æçºµå¶çä»¥ææè¢«é®æ¡çç»æã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶éæå¼æ¹æ³ï¼å°ä»¥ä¸å ä¸ªè¦ç´ ç»åèµ·æ¥ï¼</p>
<ul>
<li><strong>æºå¨äººè¾å©å¶çæçºµï¼</strong> è¿æ¯ææ¾èçåæ°ç¹ãéè¿å·¥ä¸æºå¨äººæèä¸»å¨æ¬èµ·ææ¨å¨å¶çï¼ç³»ç»è½å¤è·åä¼ ç»æ¹æ³æ æ³è§¦åçé®æ¡åºåï¼å¦èè½ãå¶çèé¢/æ­£é¢ï¼çé«åè¾¨çå¾åãè¿åæäºè¢«å¨æåçæ ¹æ¬éå¶ã</li>
<li><strong>3D åå²é«æ¯æ³¼æºæ¨¡å (Gaussian Splats)ï¼</strong> å©ç¨é«æ¯æ³¼æºææ¯æå»ºæ¤ç©ç 3D æ°å­å­ªçï¼è¿æ¯ä¸ç§æ°å´çç¥ç»æ¸²æææ¯ï¼ä»¥å¶é«è´¨éåé«æçæ¸²æè½åèé»åãç»å 3D åå²ï¼å¯ä»¥åå»ºç²¾ç»çãå¯ç´¢å¼çæ¤ç©ç»ææ¨¡åã</li>
<li><strong>éæç¡¬ä»¶å¹³å°ï¼</strong> ç»åäºç«ä½ç¸æºãæ°å­è½¬çåç¯ç®±ï¼ä¸ºæ°æ®ééæä¾äºåæ§ä¸å¨é¢çç¯å¢ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>Botany-Bot å¯¹è®¡ç®æºè§è§åæ¤ç©ç§å­¦é¢åå·æå¤æ¹é¢çæ½å¨å½±åï¼</p>
<ul>
<li><strong>é«ç²¾åº¦æ¤ç©è¡¨åï¼</strong> æ¾èæåäºæ¤ç©è¡¨åæ°æ®çç²¾ç»åº¦åå®æ´æ§ï¼è½å¤è·åä»¥åé¾ä»¥éåçå¾®è§ç»æä¿¡æ¯ï¼è¿å¯¹äºæ¤ç©è²ç§ãä½ç©æ¹è¯ãçè«å®³æ£æµåæ¤ç©ççå­¦ç ç©¶è³å³éè¦ã</li>
<li><strong>æ°å­å­ªçææ¯å¨çç©é¢åçåºç¨ï¼</strong> æ¨å¨äºé«ä¿çæ°å­å­ªçææ¯å¨å¤æçç©ä½ï¼å¦æ¤ç©ï¼ä¸çåºç¨ï¼ä¸ºæªæ¥çç²¾ååä¸ãæºè½æ¸©å®¤ç®¡çåæ¤ç©çé¿æ¨¡ææä¾äºæ°çèå¼ã</li>
<li><strong>æºå¨äººä¸è§è§çæ·±åº¦èåï¼</strong> å±ç¤ºäºæºå¨äººä¸»å¨æç¥åæçºµå¨åæè§è§æææ¹é¢çå¼ºå¤§æ½åï¼ä¸ºæªæ¥å¨å¶ä»å¤æãé®æ¡ç¯å¢ä¸çç®æ æ£æµåæ°æ®ééæä¾äºçµæã</li>
<li><strong>æ°åæ°æ®éçåå»ºï¼</strong> è½å¤çæåå«é®æ¡ç»èçç¬ç¹æ°æ®éï¼è¿å°ä¿è¿æ°çè®¡ç®æºè§è§ç®æ³ï¼å¦é®æ¡å¤çãå¤è§å¾éå»ºï¼çå¼ååè¯ä¼°ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>ç²¾ååä¸åæºè½æ¸©å®¤ï¼</strong> å®æ¶çæµæ¤ç©å¥åº·ãçé¿ç¶æãçè«å®³æ©æé¢è­¦ï¼å®ç°ç²¾ç»åç®¡çã</li>
<li><strong>æ¤ç©è²ç§åéä¼ å­¦ï¼</strong> æ´åç¡®å°è¯ä¼°ä¸ååºå åå¨å¾®è§ç»æä¸çå·®å¼ï¼å éæ°åç§çéè²ã</li>
<li><strong>æ¤ç©ççå­¦åæè«å­¦ï¼</strong> æ©æåç°å¶çèé¢æèé¨ççåãè«å®³ï¼æé«é²æ²»æçã</li>
<li><strong>æ¤ç©ççå­¦ç ç©¶ï¼</strong> æ·±å¥ç ç©¶æ¤ç©å¯¹ç¯å¢ååçååºï¼ä¾å¦æ°å­åå¸ãå¶çç»æååç­ã</li>
<li><strong>æºå¨äººæä½åæç¥ï¼</strong> ä¸ºå¼åæ´æºè½ãæ´çµæ´»çæºå¨äººç³»ç»æä¾æ¡ä¾åææï¼ç¹å«æ¯å¨å¤çæè½¯ãææç©ä½æ¹é¢ã</li>
<li><strong>3D éå»ºåç¥ç»æ¸²æï¼</strong> æ¨å¨é«æ¯æ³¼æºç­ææ¯å¨å¤æãå¨æåºæ¯ä¸çåºç¨åä¼åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>ç³»ç»å¤ææ§åææ¬ï¼</strong> æè¦ä¸­æè¿°çç¡¬ä»¶å¹³å°ï¼å·¥ä¸æºå¨äººãç«ä½ç¸æºãè½¬çãç¯ç®±ï¼è¡¨æè¿æ¯ä¸ä¸ªç¸å¯¹å¤æä¸ææ¬è¾é«çç³»ç»ï¼å¯è½ä¸éç¨äºå¤§è§æ¨¡ãä½ææ¬çé¨ç½²ã</li>
<li><strong>å¤çéåº¦åå¯æ©å±æ§ï¼</strong> æºå¨äººæçºµå¶çå¹¶ææå¾åçè¿ç¨å¯è½ç¸å¯¹èæ¶ï¼å¯¹äºéè¦å¿«éå¤çå¤§éæ¤ç©çåºæ¯ï¼å¶æçå¯è½æ¯ä¸ä¸ªéå¶ãæè¦ä¸­æªæåå¤çåæ ªæ¤ç©æéçæ¶é´ã</li>
<li><strong>æºå¨äººæä½çé²æ£æ§ï¼</strong> å°½ç®¡ç»åºäºæä½æåçï¼æ¬èµ·/æ¨å¨å¶ç77.9%ï¼ï¼ä½ä»æçº¦22%çå¤±è´¥çãå¯¹äºèå¼±çæ¤ç©ï¼æºå¨äººæä½çååº¦ãç²¾åº¦åå¯¹ä¸åæ¤ç©å½¢æçéåºæ§æ¯ææã</li>
<li><strong>æ³åè½åï¼</strong> æè¦æªæç¡®è¯´æè¯¥ç³»ç»å¯¹ä¸åæ¤ç©ç§ç±»ãå¤§å°åå½¢æçæ³åè½åãæäºæ¤ç©çå¶çç»æå¯è½æ´é¾æçºµæéå»ºã</li>
<li><strong>æ°æ®å¤çåå­å¨ï¼</strong> çæçâè¯¦ç»å¸¦æ³¨éçæ°å­å­ªçâåé«åè¾¨çå¾åå°äº§çå¤§éæ°æ®ï¼å¯¹æ°æ®å¤çãå­å¨ååæè½åæåºè¦æ±ã</li>
<li><strong>ç²¾åº¦ææ çä¸ä¸æï¼</strong> æè¦ä¸­ç»åºçç²¾åº¦ï¼å¦å¶çåå²90.8%ï¼æ£æµ86.2%ï¼æ¯éå¯¹ç¹å®æ°æ®éåå®éªæ¡ä»¶èè¨çï¼å¶å¨æ´å¹¿æ³åºæ¯ä¸çè¡¨ç°ä»éè¿ä¸æ­¥éªè¯ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼Botany-Bot ä»£è¡¨äºè®¡ç®æºè§è§ãæºå¨äººå­¦åæ¤ç©ç§å­¦äº¤åé¢åçä¸ä¸ªéè¦è¿å±ï¼éè¿ä¸»å¨æç¥ååè¿ç 3D éå»ºææ¯ï¼ä¸ºæ¤ç©è¡¨ååæ°å­å­ªçæå»ºå¼è¾äºæ°çå¯è½æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17783v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17783v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17700v1'></a></p>
<h2 id="elastic-vits-from-pretrained-models-without-retraining"><a href="https://arxiv.org/abs/2510.17700v1">Elastic ViTs from Pretrained Models without Retraining</a></h2>
<p><strong>Authors:</strong> Walter Simoncini, Michael Dorkenwald, Tijmen Blankevoort, Cees G. M. Snoek, Yuki M. Asano</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Walter Simonciniç­äººæ°åçè®ºæâElastic ViTs from Pretrained Models without Retrainingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="elastic-vits-from-pretrained-models-without-retraining_1">è®ºææè¦ï¼Elastic ViTs from Pretrained Models without Retraining</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åè§è§åºç¡æ¨¡åï¼Vision Foundation Modelsï¼è½ç¶æ§è½åè¶ï¼ä½éå¸¸åªæä¾æéçé¢è®¾å°ºå¯¸ï¼è¿å¯¼è´å¨å®éé¨ç½²ä¸­é¾ä»¥æ ¹æ®å·ä½çè®¡ç®é¢ç®åä»»å¡éæ±è¿è¡çµæ´»è°æ´ï¼å¾å¾éè¦éæ©è¿å¤§çæ¨¡åï¼é æèµæºæµªè´¹ãè®ºææ¨å¨è§£å³å¦ä½ä»åä¸é¢è®­ç»æ¨¡åä¸­é«æå°æåä¸ç³»åå·æä¸åè®¡ç®é¢ç®çå­ç½ç»ï¼ä»¥å®ç°å¼¹æ§æ¨çï¼èæ ééæ°è®­ç»æä¾èµæ æ³¨æ°æ®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ç§åä¸º <strong>SnapViT</strong> çæ°åç»æååªææ¹æ³ï¼ç¨äºé¢è®­ç»çè§è§Transformerï¼ViTsï¼ï¼å¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>åæ¬¡åªæä¸å¼¹æ§æ¨çï¼</strong> SnapViT è½å¤å¨åæ¬¡è¿è¡ä¸­ï¼å¿«éï¼å¨åä¸ªA100 GPUä¸å°äºäºåéï¼çæéç¨äºåç§ç¨çåº¦çº§å«çå¼¹æ§æ¨¡åï¼ä»èå®ç°è·¨è®¡ç®é¢ç®çè¿ç»­æ¨çã</li>
<li><strong>ç»åå±é¨æ¢¯åº¦ä¸å¨å±Hessianç¸å³æ§ï¼</strong> è®ºæå¼å¥äºä¸ä¸ªâå¯åªææ§åæ°âï¼è¯¥åæ°ç»åäºä¸¤ä¸ªå³é®é¡¹ï¼<ul>
<li><strong>å±é¨Hessianè¿ä¼¼ï¼</strong> ä½¿ç¨èªçç£æ¢¯åº¦ï¼åºäºDINOç®æ ï¼é«æå°ä¼°è®¡åæ°çå±é¨æææ§ï¼æ éåç±»å¤´ï¼éç¨äºçç£åèªçç£æ¨¡åã</li>
<li><strong>å¨å±Hessianä¼°è®¡ï¼éè¿xNESï¼ï¼</strong> éå¯¹Hessianç©éµçéå¯¹è§çº¿åç´ ï¼ææè·¨ç½ç»ç»æç¸å³æ§ï¼è®¡ç®çå¤ææ§ï¼è®ºææåºäºä¸ç§æ°é¢çè¿åç®æ³ï¼xNESï¼ææ°èªç¶è¿åç­ç¥ï¼æ¥é«æè¿ä¼¼è¿äºå¨å±ç¸å³æ§ï¼é¿åäºæ¾å¼è®¡ç®å®æ´çHessianã</li>
</ul>
</li>
<li><strong>èªçç£éè¦æ§è¯åæºå¶ï¼</strong> è¯¥æ¹æ³ä¸ä¾èµæ æ³¨æ°æ®ï¼éè¿èªçç£æå¤±ï¼ä¾å¦DINOç®æ ï¼æ¥æå¯¼åªæè¿ç¨ï¼å¹¶ä½¿ç¨åºäºPCAçåµå¥ä½å¼¦ç¸ä¼¼åº¦ä½ä¸ºéåºåº¦å½æ°ï¼ç¡®ä¿å¨ä¸éæ°è®­ç»æä½¿ç¨æ ç­¾çæåµä¸ä¿æå¼ºå¤§çæ§è½ã</li>
<li><strong>ç»æååªæç­ç¥ï¼</strong> è½å¤å¯¹Transformerç»ä»¶ï¼å¦åé¦ååçè¡-åç»åï¼åæ´å¤§çç»æï¼å¦æ´ä¸ªæ³¨æåå¤´ï¼è¿è¡éæ©æ§åªæã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çåªææ§è½ï¼</strong> å¨DINOãSigLIPv2ãDeITåAugRegç­æ¨¡åä¸çå®éªè¡¨æï¼SnapViTå¨åç§ç¨çåº¦ä¸åä¼äºæåª²ç¾æåè¿çåªææ¹æ³ï¼å°¤å¶æ¯å¨é«ç¨çåº¦æ¯çä¸ãä¾å¦ï¼DINOv1 ViT-B/16æ¨¡åå¨åªæ40%ç¨çåº¦åï¼æ¨çéåº¦æå1.58åï¼èåç¡®çä¸éå°äº5%ã
*   <strong>æ ééæ°è®­ç»åæ ç­¾ï¼</strong> è¯¥æ¹æ³å¨ä¸è¿è¡éæ°è®­ç»æä½¿ç¨ä»»ä½æ ç­¾çæåµä¸ï¼å®ç°äºå¼ºå¤§çæ§è½ï¼è¿å¯¹äºå¤çéå¬å¼é¢è®­ç»æ°æ®éæå¨èµæºåéç¯å¢ä¸é¨ç½²æ¨¡åå·æéè¦æä¹ã
*   <strong>å¯¹å¤§åæ¨¡åçéç¨æ§ï¼</strong> å³ä½¿å¯¹äºDINOv3 ViT-H+/16åSigLIPv2 ViT-G/16ç­å¤§åæ¨¡åï¼æ°åäº¿åæ°ï¼ï¼SnapViTä¹è½ææåªæï¼å°½ç®¡å¨æé«ç¨çåº¦ä¸æ§è½ä¸éæ´å¿«ï¼ä½ç»åç®åçæéæ ¡æ­£ææ¯ä»è½æ¢å¤æ§è½ã
*   <strong>å¯¹è¯­ä¹åå²ä»»å¡çæ³åè½åï¼</strong> å¨Pascal VOC 2012è¯­ä¹åå²ä»»å¡ä¸ï¼SnapViTä¹è¡¨ç°åºä¸æåè¿æ¹æ³ç¸å½ææ´ä¼çæ§è½ï¼å°¤å¶æ¯å¨é«ç¨çåº¦ä¸ã
*   <strong>æçåå¯æ©å±æ§ï¼</strong> å¨åä¸ªA100 GPUä¸ï¼çæå¼¹æ§æ¨¡åæéæ¶é´å°äºäºåéï¼è¯æäºå¶åºè²çå¯æ©å±æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¤§åé¢è®­ç»æ¨¡åçåªæææï¼</strong> è®ºæè§å¯å°ï¼å¨å¤§åæ°æ®éä¸è®­ç»çè§è§åºç¡æ¨¡åï¼å¦DINOv3åSigLIPv2ï¼æ´é¾åªæï¼å ä¸ºå¶è¡¨å¾ç¥è¯å¯è½æ´ååå°åå¸å¨åæ°ä¸­ï¼ä½¿å¾è¯å«âä¸éè¦âçåååå¾ä¸é£ä¹ææ¾ã
*   <strong>æç«¯ç¨çåº¦ä¸çæ§è½ä¸éï¼</strong> å°½ç®¡SnapViTå¨è¾é«ç¨çåº¦ä¸è¡¨ç°è¯å¥½ï¼ä½å¨æç«¯ç¨çåº¦ï¼ä¾å¦è¶è¿30%ï¼ä¸ï¼æäºå¤§åæ¨¡åçåç¡®æ§ä¼è¿éä¸éã
*   <strong>å°æ¨¡åå¨æç«¯ç¨çåº¦ä¸çå®¹ééå¶ï¼</strong> å¯¹äºåViT-S/16è¿æ ·çå°æ¨¡åï¼å¨æç«¯åªæï¼ä¾å¦50%ç¨çåº¦ï¼åï¼å©ä½çè¡¨å¾å®¹éæéï¼å¯è½å¯¼è´æéæ ¡æ­£ææä¸ä½³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è¿ä¸æ­¥ä¼åå¤§åæ¨¡ååªæï¼</strong> æ¢ç´¢æ´åè¿çç­ç¥ï¼ä»¥åºå¯¹å¤§åé¢è®­ç»æ¨¡åä¸­ç¥è¯åååå¸å¸¦æ¥çåªæææï¼å¯è½åæ¬æ´å¤æçæéæ ¡æ­£æå¤é¶æ®µåªææ¹æ³ã
*   <strong>æ°æ®ä¸­å¿åªæçæ¢ç´¢ï¼</strong> è®ºæåæ­¥è¡¨æï¼æ°æ®ä¸åªæä»»å¡çå¯¹é½å¯¹æ§è½ææ¾èå½±åï¼æªæ¥å¯ä»¥æ·±å¥ç ç©¶å¦ä½éè¿æ°æ®ä¸­å¿çæ¹æ³è¿ä¸æ­¥ä¼ååªæï¼ä»¥çææ³åè½åæ´å¼ºçç¨çæ¨¡åã
*   <strong>ä¸å¶ä»å¼¹æ§æ¨çæ¹æ³çç»åï¼</strong> æ¢ç´¢SnapViTä¸ç°æå¼¹æ§æ¨çæ¹æ³ï¼å¦Matryoshkaè¡¨ç¤ºï¼çç»åï¼ä»¥å®ç°æ´å¹¿æ³ççµæ´»æ§åæ§è½æåã
*   <strong>æ´é«æçHessianè¿ä¼¼ï¼</strong> å°½ç®¡xNESå·²ç»å¾é«æï¼ä½ä»å¯ä»¥æ¢ç´¢å¶ä»æ´å¿«çHessianéå¯¹è§çº¿ç»æè¿ä¼¼æ¹æ³ï¼ä»¥è¿ä¸æ­¥ç¼©ç­åªææ¶é´ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets.</li>
<li>Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free.</li>
<li>Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget.</li>
<li>Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17700v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17700v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17699v1'></a></p>
<h2 id="gas-improving-discretization-of-diffusion-odes-via-generalized-adversarial-solver"><a href="https://arxiv.org/abs/2510.17699v1">GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</a></h2>
<p><strong>Authors:</strong> Aleksandr Oganov, Ilya Bykov, Eva Neudachina, Mishan Aliev, Alexander Tolmachev, Alexander Sidorov, Aleksandr Zuev, Andrey Okhotin, Denis Rakitin, Aibek Alanov</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸ºâå¹¿ä¹å¯¹ææ±è§£å¨âï¼Generalized Adversarial Solver, GASï¼çæ°æ¹æ³ï¼æ¨å¨æ¾èåå°æ©æ£æ¨¡åéæ ·æéçè®¡ç®éï¼åæ¶ä¿æçè³æåçæè´¨éãGASéè¿å¼å¥ä¸ä¸ªç®åçODEéæ ·å¨åæ°åï¼å¹¿ä¹æ±è§£å¨ï¼ï¼é¿åäºç°æè¸é¦æ¹æ³ä¸­å¤æçè®­ç»æå·§ï¼å¹¶ç»åå¯¹æè®­ç»æ¥æ´å¥½å°ä¿çå¾åç»èååå°ä¼ªå½±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<ul>
<li><strong>å¹¿ä¹æ±è§£å¨ (Generalized Solver) åæ°åï¼</strong> æ ¸å¿åæ°å¨äºæåºäºä¸ç§âç®åâçODEéæ ·å¨åæ°åãè¿æç¤ºå®å¯è½æ¯ç°ææ¹æ³æ´ç´æ¥ãæ´æäºå®ç°ï¼å¹¶ä¸ä¸éè¦å¤æçè®­ç»ç­ç¥ï¼å¦å¤é¶æ®µè®­ç»ãç¹å®çè°åº¦å¨ç­ï¼ãè¿ç§ç®åå¯è½éä½äºè®­ç»é¾åº¦ï¼å¹¶æé«äºæ¹æ³çé²æ£æ§ã</li>
<li><strong>ç»åå¯¹æè®­ç» (Adversarial Training)ï¼</strong> ä¸ºäºè§£å³ç°ææ¹æ³å¨ç»èä¿çåä¼ªå½±æå¶æ¹é¢çä¸è¶³ï¼GASå°åå§çè¸é¦æå¤±ä¸å¯¹æè®­ç»ç¸ç»åãå¯¹æè®­ç»éå¸¸è½ä¿ä½¿çæå¨äº§çæ´çå®ãç»èæ´ä¸°å¯çè¾åºï¼å ä¸ºå®è¿«ä½¿çæå¨æ¬ºéªå¤å«å¨ï¼ä»èå­¦ä¹ å°æ´ç²¾ç»çæ°æ®åå¸ç¹å¾ã</li>
<li><strong>æç¡®å³æ³¨ç»èä¿çåº¦ï¼</strong> è®ºææç¡®æåºå¶æ¹æ³âä¸æç¡®å³æ³¨ä¿çç»ç²åº¦ç»èâæ¯ç°ææ¹æ³çç¼ºç¹ï¼èGASéè¿å¯¹æè®­ç»æ¥âå¢å¼ºç»èä¿çåº¦âï¼è¿è¡¨æå®å¨è®¾è®¡ä¸å°±èèäºçæè´¨éçå³é®æ¹é¢ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>å éæ©æ£æ¨¡åéæ ·ï¼</strong> è¿æ¯æç´æ¥åæéè¦çå½±åãå¦æGASè½å¤ä»¥æ´å°çå½æ°è¯ä¼°ï¼FEVsï¼å®ç°ä¸ç°æSOTAç¸å½ææ´å¥½çè´¨éï¼å°æå¤§å°æé«æ©æ£æ¨¡åçå®ç¨æ§ï¼ä½¿å¶å¨å®æ¶åºç¨ãèµæºåéç¯å¢ï¼å¦ç§»å¨è®¾å¤ï¼ä¸­æ´å¯è¡ã</li>
<li><strong>ç®åæ¨¡åè¸é¦è¿ç¨ï¼</strong> é¿åâå¤æçè®­ç»æå·§âæå³çç ç©¶äººååå¼åèå¯ä»¥æ´å®¹æå°å®ç°ååºç¨æ©æ£æ¨¡åçå éï¼éä½äºè¿å¥é¨æ§åå¼åææ¬ã</li>
<li><strong>æåçæè´¨éï¼ç¹å«æ¯ç»èæ¹é¢ï¼</strong> å¼ºè°ç»èä¿çåº¦æå³çGASå¯è½å¨çæé«åè¾¨çå¾åãçº¹çãé¢é¨ç¹å¾ç­å¯¹ç»èææçä»»å¡ä¸è¡¨ç°åºè²ï¼ä»èæ¨å¨æ©æ£æ¨¡åå¨èºæ¯åä½ãå¾åç¼è¾ãèæç°å®ç­é¢åçåºç¨ã</li>
<li><strong>æ¨å¨æ©æ£æ¨¡åç ç©¶æ¹åï¼</strong> å¯è½ä¼æ¿åæ´å¤å³äºå¦ä½ææç»åODEæ±è§£å¨ä¼ååå¯¹æè®­ç»çç ç©¶ï¼ä»¥åæ¢ç´¢æ´ç®åãæ´é²æ£çæ©æ£æ¨¡åå éæ¹æ³ã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å®æ¶å¾åçæï¼</strong> ä¾å¦ï¼äº¤äºå¼å¾åç¼è¾ãè§é¢çæãæ¸¸æèµäº§åå»ºã</li>
<li><strong>èµæºåéè®¾å¤ä¸ççæä»»å¡ï¼</strong> å¦ç§»å¨ç«¯çAIèºæ¯åºç¨ãè¾¹ç¼è®¡ç®è®¾å¤ä¸çå¾åå¢å¼ºã</li>
<li><strong>é«åè¾¨çå¾ååæï¼</strong> å»ºç­æ¸²æãå»å­¦å¾åçæãå«æå¾ååæç­éè¦ç²¾ç»ç»èçé¢åã</li>
<li><strong>æ¡ä»¶çæä»»å¡ï¼</strong> å¦ææ¬å°å¾åï¼text-to-imageï¼ãå¾åå°å¾åï¼image-to-imageï¼ç¿»è¯ï¼å¶ä¸­å¿«éåé¦åé«è´¨éç»èè³å³éè¦ã</li>
<li><strong>3Dåå®¹çæï¼</strong> æ©æ£æ¨¡åå·²å¼å§åºç¨äº3Dçæï¼å ééæ ·å°å¯¹3Dèµäº§çå¿«éè¿­ä»£åé¢è§äº§çå·¨å¤§å½±åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>âç±»ä¼¼èµæºçº¦æä¸âçæ§è½æ¯è¾ï¼</strong> æè¦ä¸­æå°âå¨ç±»ä¼¼èµæºçº¦æä¸âå±ç¤ºäºä¼è¶æ§è½ãè¿å¯è½æå³çå¨æäºæç«¯èµæºåéæèµæºéå¸¸åè£çæåµä¸ï¼å¶ä¼å¿å¯è½ä¸é£ä¹ææ¾ï¼æèéè¦è¿ä¸æ­¥çå®éªæ¥éªè¯ã</li>
<li><strong>å¯¹æè®­ç»çæ½å¨ææï¼</strong> å°½ç®¡å¯¹æè®­ç»æå©äºç»èï¼ä½å®ä¹å¯è½å¸¦æ¥è®­ç»ä¸ç¨³å®ãæ¨¡å¼å´©æºï¼mode collapseï¼ç­é®é¢ï¼å°½ç®¡æè¦å£°ç§°âä¸éè¦é¢å¤çè®­ç»æå·§âï¼ä½å¯¹æè®­ç»æ¬èº«å°±å¯è½éè¦ä»ç»çè¶åæ°è°æ´ã</li>
<li><strong>âç®ååæ°åâçå·ä½å«ä¹ï¼</strong> æè¦æ²¡æè¯¦ç»è¯´æâå¹¿ä¹æ±è§£å¨âçå·ä½åæ°åå½¢å¼ãå¶ç®åæ§æ¯å¦ä¼éå¶å¶å¨æäºå¤ææ°æ®åå¸ä¸çè¡¨è¾¾è½åï¼ä»æå¾éªè¯ã</li>
<li><strong>æ³åè½åï¼</strong> è®ºæä¸»è¦å³æ³¨å¾åçæï¼å¶æ¹æ³å¨å¶ä»æ©æ£æ¨¡ååºç¨ï¼å¦é³é¢çæãæ¶é´åºåé¢æµï¼ä¸çæ³åè½åå¦ä½ï¼å°ä¸æ¸æ¥ã</li>
<li><strong>ä¸ææ°SOTAçæ¯è¾ï¼</strong> æè¦å£°ç§°âæ¹è¿äºç°ææ¹æ³âï¼ä½å·ä½æ¯åªäºæ¹æ³ï¼ä»¥åå¨åªäºææ ä¸åå¾äºå¤å¤§çæåï¼éè¦æ¥éè®ºææ­£ææè½äºè§£ãå°¤å¶æ¯å¨2025å¹´10æåè¡¨ï¼å±æ¶å¯è½å·²ç»ææ´å¤æ°çSOTAæ¹æ³åºç°ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºææåºäºä¸ç§æåæ¯çæ¹æ³æ¥è§£å³æ©æ£æ¨¡åéæ ·æçåç»èä¿çåº¦ä¹é´çæè¡¡é®é¢ãå¶âç®ååæ°åâåâå¯¹æè®­ç»âçç»åï¼ææä¸ºæ©æ£æ¨¡åçå®éåºç¨å¸¦æ¥æ¾èçæ§è½æååæ´å¹¿æ³çæ®åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling.</li>
<li>In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17699v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17699v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17685v1'></a></p>
<h2 id="multilingual-text-to-image-person-retrieval-via-bidirectional-relation-reasoning-and-aligning"><a href="https://arxiv.org/abs/2510.17685v1">Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning</a></h2>
<p><strong>Authors:</strong> Min Cao, Xinyu Zhou, Ding Jiang, Bo Du, Mang Ye, Min Zhang</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºMin Cao, Xinyu Zhou, Ding Jiang, Bo Du, Mang Ye, Min Zhangæ°åçè®ºæâMultilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligningâçå¨é¢æè¦ï¼ç¨ä¸­æåæï¼</p>
<hr />
<p><strong>è®ºææè¦ï¼å¤è¯­è¨ææ¬å°å¾åäººç©æ£ç´¢ä¸­çååéå¼å³ç³»æ¨çä¸å¯¹é½</strong></p>
<p>è¿ç¯è®ºæè§£å³äºææ¬å°å¾åäººç©æ£ç´¢ï¼TIPRï¼é¢åä¸­çä¸¤ä¸ªå³é®ææï¼æ¨¡æå¼ææ§åè¯­è¨å¤æ ·æ§ãä¼ ç»çTIPRæ¹æ³ä¸»è¦éä¸­äºè±æææ¬æ¥è¯¢ï¼å¹¶ä¸å¨è·¨æ¨¡æå¯¹é½æ¹é¢ï¼è¦ä¹å¿½ç¥ç»ç²åº¦å·®å¼ï¼å¨å±å¯¹é½ï¼ï¼è¦ä¹éè¦é¢åçå±é¨å¯¹é½ä¿¡æ¯ï¼å±é¨å¯¹é½ï¼ï¼è¿éå¶äºå¶å¨å¤è¯­è¨ç¯å¢ä¸­çåºç¨åæ§è½ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³ç°æTIPRæ¹æ³å¨å¤çå¤è¯­è¨ææ¬æ¥è¯¢æ¶çå±éæ§ï¼å¹¶åææ¨¡æå¼ææ§é®é¢ï¼ä»¥å®ç°è·¨è¯­è¨åè·¨æ¨¡æçé²æ£äººç©æ£ç´¢ãå·ä½æ¥è¯´ï¼å®æåºäºä¸ä¸ªæ°é¢çâå¤è¯­è¨TIPRâä»»å¡ï¼å¹¶è´åäºæå»ºç¸åºçæ°æ®éåå¼åææçæ¡æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¤è¯­è¨TIPRä»»å¡çå¼åæ§å·¥ä½ï¼</strong> è®ºæé¦æ¬¡æåºäºå¤è¯­è¨TIPRä»»å¡ï¼å¹¶æå»ºäºç¸åºçåºåæ°æ®éã
*   <strong>LMsé©±å¨çé¢åèªéåºç¿»è¯ï¼LDATï¼æµæ°´çº¿ï¼</strong> ä¸ºäºè§£å³å¤è¯­è¨TIPRæ°æ®ç¨ç¼ºçé®é¢ï¼è®ºææåºäºä¸ç§LDATæµæ°´çº¿ï¼å©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMsï¼è¿è¡åå§ç¿»è¯ï¼å¹¶éè¿è¿æ»¤åéåé¶æ®µæ´åé¢åç¹å®ç¥è¯ï¼ä»¥çæé«è´¨éçå¤è¯­è¨TIPRæ°æ®éãè¿ææç¼è§£äºLLMsç¿»è¯ä¸­ç¼ºä¹é¢åç¥è¯å¯¼è´çåªå£°é®é¢ã
*   <strong>Bi-IRRAæ¡æ¶ï¼</strong> è®ºææåºäºä¸ä¸ªåä¸ºBi-IRRAï¼Bidirectional Implicit Relation Reasoning and Aligningï¼çè·¨æ¨¡ææ¡æ¶ï¼æ¨å¨å­¦ä¹ è·¨è¯­è¨åè·¨æ¨¡æçå¯¹é½ã
    *   <strong>ååéå¼å³ç³»æ¨çï¼Bi-IRRï¼æ¨¡åï¼</strong> è¯¥æ¨¡åéè¿åè¯­æ©ç è¯­è¨å»ºæ¨¡ï¼MLMï¼åè·¨è¯­è¨è¸é¦æ©ç å¾åå»ºæ¨¡ï¼D-MIMï¼é¢è®­ç»ä»»å¡ï¼å®ç°å¯¹æ©ç å¾ååææ¬çååé¢æµãè¿éå¼å°å¢å¼ºäºè·¨è¯­è¨åè·¨æ¨¡æçå±é¨å³ç³»å»ºæ¨¡ï¼ä»èå¨ç»ç²åº¦å±é¢å»ºç«å¯¹é½ã
    *   <strong>å¤ç»´å¨å±å¯¹é½ï¼Md-GAï¼æ¨¡åï¼</strong> è¯¥æ¨¡åéæäºåè¯­å¾å-ææ¬å¯¹æ¯å­¦ä¹ ï¼ITCï¼ååè¯­éå¯¹ç§°å¾å-ææ¬å¹éï¼A-ITMï¼é¢è®­ç»ä»»å¡ï¼ä»¥å¼¥åæ¨¡æå¼ææ§ï¼å®ç°ç²ç²åº¦å±é¢çå¨å±å¯¹é½ãç¹å«å°ï¼A-ITMä¸­çéå¯¹ç§°æ©ç æä½æå©äºå¨å­å¨åªå£°ç®æ ææ¬çæåµä¸è¿è¡é²æ£å­¦ä¹ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> Bi-IRRAå¨ææå¤è¯­è¨TIPRæ°æ®éä¸ååå¾äºæ°çæåè¿ç»æï¼æ¾èä¼äºç°ææ¹æ³ã
*   <strong>å¤è¯­è¨ç¯å¢ä¸çé²æ£æ§ï¼</strong> å®éªè¯æï¼Bi-IRRAå¨ä¸­æãæ³æåå¾·æç­éè±æç¯å¢ä¸­è¡¨ç°åºåè¶çæ£ç´¢æ§è½ï¼éªè¯äºå¶å¨å¤è¯­è¨åºæ¯ä¸çå¼ºå¤§é²æ£æ§åæ³åè½åã
*   <strong>LDATçæææ§ï¼</strong> æ¶èç ç©¶è¡¨æï¼LDATæµæ°´çº¿éè¿æ´åé¢åç¹å®ç¥è¯ï¼è½å¤æææåç¿»è¯è´¨éï¼ä¸ºå¤è¯­è¨TIPRä»»å¡æä¾é«è´¨éçæ°æ®ã
*   <strong>Bi-IRRAæ¨¡åçæææ§ï¼</strong> Bi-IRRæ¨¡åï¼åæ¬åè¯­MLMåè·¨è¯­è¨D-MIMï¼ä»¥åMd-GAæ¨¡åï¼åæ¬åè¯­ITCåA-ITMï¼çåä¸ªç»ä»¶é½å¯¹æ´ä½æ§è½ææ¾èè´¡ç®ï¼è¯æäºå¶å¨ç»ç²åº¦åç²ç²åº¦å¯¹é½æ¹é¢çæææ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®çæä¸­çåªå£°ï¼</strong> å°½ç®¡LDATæµæ°´çº¿éè¿è¿æ»¤åéåé¶æ®µåªåå»åªï¼ä½å¨LMsé©±å¨çç¿»è¯è¿ç¨ä¸­ï¼ä»ä¸å¯é¿åå°ä¼å¼å¥ä¸äºåªå£°ï¼è¿å¯è½å½±åæ¨¡åå¯¹é½çåç¡®æ§ã
*   <strong>ä¼ ç»TIPRæ¹æ³å¨å¤è¯­è¨æ°æ®ä¸çè¡¨ç°ï¼</strong> ä¼ ç»TIPRæ¹æ³ç±äºå¶æ¨¡åæ¶æå¹¶éä¸ºå¤è¯­è¨å­¦ä¹ èè®¾è®¡ï¼å³ä½¿å¨å¤è¯­è¨æ°æ®éä¸è¿è¡è®­ç»ï¼å¶æ§è½ä¹å¾å¾ä¸å°½å¦äººæã
*   <strong>Bi-lingual ITCä¸­æ©ç çå±éæ§ï¼</strong> å¨Bi-lingual ITCä¸­åºç¨è¾å¥æ©ç å¹¶æªå¸¦æ¥æ§è½æåï¼è¿å½å äºå¶æ¶æå·®å¼ï¼å³ITCç´æ¥ä»ç¼ç çåæ¨¡æç¹å¾è®¡ç®å¯¹æ¯æå¤±ï¼èæ²¡æç»è¿è·¨æ¨¡æäº¤äºæ¨¡åï¼éå¶äºæ©ç è¾å¥çæ­£ååææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è¿ä¸æ­¥æåå¤è¯­è¨ç¿»è¯è´¨éï¼</strong> å°½ç®¡LDATå·²ææç¼è§£åªå£°ï¼ä½ä»å¯æ¢ç´¢æ´åè¿çLMsæç¿»è¯ææ¯ï¼ä»¥è¿ä¸æ­¥æåå¤è¯­è¨TIPRæ°æ®çç¿»è¯è´¨éã
*   <strong>æ´å¤æçè·¨æ¨¡æäº¤äºï¼</strong> æ¢ç´¢æ´æ·±å±æ¬¡ãæ´å¤æçè·¨æ¨¡æäº¤äºæºå¶ï¼ä»¥æ´å¥½å°ææå¾ååææ¬ä¹é´çç»ç²åº¦è¯­ä¹å³ç³»ã
*   <strong>èªéåºæ©ç ç­ç¥ï¼</strong> ç ç©¶æ´æºè½ãèªéåºçæ©ç ç­ç¥ï¼ä»¥å¨ä¸åè¯­è¨åæ¨¡æä¸æå¤§åæ¨¡åå­¦ä¹ æçã
*   <strong>æ©å±å°æ´å¤è¯­è¨åé¢åï¼</strong> å°å¤è¯­è¨TIPRä»»å¡æ©å±å°æ´å¤è¯­è¨åæ´å¹¿æ³çé¢åï¼ä»¥éªè¯Bi-IRRAæ¡æ¶çéç¨æ§åå¯æ©å±æ§ã
*   <strong>å®æ¶åºç¨ä¼åï¼</strong> éå¯¹å®éå¬å±å®å¨é¢åçéæ±ï¼è¿ä¸æ­¥ä¼åæ¨¡åçæ¨çéåº¦åæçï¼ä»¥æ¯æå®æ¶äººç©æ£ç´¢åºç¨ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities.</li>
<li>The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17685v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17685v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17681v1'></a></p>
<h2 id="picabench-how-far-are-we-from-physically-realistic-image-editing"><a href="https://arxiv.org/abs/2510.17681v1">PICABench: How Far Are We from Physically Realistic Image Editing?</a></h2>
<p><strong>Authors:</strong> Yuandong Pu, Le Zhuo, Songhao Han, Jinbo Xing, Kaiwen Zhu, Shuo Cao, Bin Fu, Si Liu, Hongsheng Li, Yu Qiao, Wenlong Zhang, Xi Chen, Yihao Liu</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼PICABench: How Far Are We from Physically Realistic Image Editing?</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºæåºäº <strong>PICABench</strong>ï¼è¿æ¯ä¸ä¸ªç¨äºç³»ç»æ§è¯ä¼°å¾åç¼è¾ç©ççå®æçåºåãå®è¶è¶äºç°ææ¨¡åååºåä»å³æ³¨æä»¤å®æçå±éï¼é¦æ¬¡å°ç©çææï¼å¦é´å½±ãåå°ãç©ä½äº¤äºç­ï¼çº³å¥è¯ä¼°èå´ï¼å¹¶æåºäº <strong>PICAEval</strong> è¿ä¸å¯é çè¯ä¼°åè®®ãæ­¤å¤ï¼è®ºæè¿æ¢ç´¢äºä»è§é¢ä¸­å­¦ä¹ ç©çç¥è¯çè§£å³æ¹æ¡ï¼å¹¶æå»ºäºè®­ç»æ°æ®é <strong>PICA-100K</strong>ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<ul>
<li><strong>ç©ççå®æè¯ä¼°çå¼å¥åç³»ç»åï¼</strong> è¿æ¯ææ ¸å¿çåæ°ãè®ºææç¡®æåºç°æå¾åç¼è¾æ¨¡åååºåå¿½è§äºç©çææï¼å¹¶é¦æ¬¡å°ç©ççå®ææåå°ä¸æä»¤å®æåç­éè¦çå°ä½ãå®å°ç©ççå®æç»åä¸ºå«ä¸ªå­ç»´åº¦ï¼æ¶µçåå­¦ãåå­¦åç¶æè½¬æ¢ï¼ï¼å¹¶åºç¨äºå¸¸è§çç¼è¾æä½ï¼å®ç°äºå¯¹è¿ä¸å¤æé®é¢çç³»ç»æ§è§£æåè¯ä¼°ã</li>
<li><strong>VLM-as-a-judge è¯ä¼°åè®® (PICAEval)ï¼</strong> æåºäºä¸ç§æ°é¢ä¸å¯é çè¯ä¼°åè®®ï¼å©ç¨è§è§è¯­è¨æ¨¡åï¼VLMï¼ä½ä¸ºâè¯å¤èâï¼ç»åéæ¡ä¾ãåºåçº§å«çäººå·¥æ æ³¨åé®é¢ï¼åæäºä¼ ç»ææ é¾ä»¥ææç©ççå®æçææãè¿ä»£è¡¨äºè¯ä¼°æ¹æ³å­¦ä¸çä¸å¤§è¿æ­¥ã</li>
<li><strong>ä»è§é¢ä¸­å­¦ä¹ ç©çç¥è¯çæ¢ç´¢åæ°æ®éæå»º (PICA-100K)ï¼</strong> è®ºæä¸ä»æåºäºé®é¢åè¯ä¼°æ¹æ³ï¼è¿ç§¯ææ¢ç´¢äºè§£å³æ¹æ¡ï¼å³éè¿ä»è§é¢ä¸­å­¦ä¹ æ¥æåç©ççå®æï¼å¹¶ä¸ºæ­¤æå»ºäºä¸ä¸ªå¤§è§æ¨¡è®­ç»æ°æ®é PICA-100Kãè¿ä¸ºæªæ¥çç ç©¶æä¾äºå®è´µçèµæºåæ¹åã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>éæ°å®ä¹å¾åç¼è¾çâå¥½âï¼</strong> PICABench å°ä¿ä½¿ç ç©¶äººååå¼åèéæ°æèå¾åç¼è¾çè´¨éæ åï¼ä»ä»ä»å³æ³¨âåå®¹æ­£ç¡®âè½¬åâåå®¹æ­£ç¡®ä¸ç©ççå®âãè¿å°æ¨å¨å¾åç¼è¾ææ¯åæ´é«å±æ¬¡ççå®æåå¯ä¿¡åº¦åå±ã</li>
<li><strong>æ¨å¨ç©çæç¥åçææ¨¡åçåå±ï¼</strong> è¿ä¸åºåå°æä¸ºå¼ååè¯ä¼°ç©çæç¥åï¼physics-awareï¼å¾åçæåç¼è¾æ¨¡åçå³é®å·¥å·ãæªæ¥çæ¨¡åå°éè¦æ¾å¼å°å­¦ä¹ åæ¨¡æç©çè§å¾ï¼èä¸ä»ä»æ¯åç´ çº§çæ å°ã</li>
<li><strong>ä¿è¿å¤æ¨¡æè¯ä¼°æ¹æ³å­¦åæ°ï¼</strong> PICAEval ä¸­ VLM-as-a-judge çæ¹æ³ï¼ç»åäººå·¥æ æ³¨ï¼ä¸ºå¤æãä¸»è§ççæåå®¹è¯ä¼°æä¾äºæ°çèå¼ï¼å¯è½å¯åå¶ä»çæä»»å¡çè¯ä¼°æ¹æ³ã</li>
<li><strong>æ¿åæ°çç ç©¶æ¹åï¼</strong> è®ºææç¡®æåºç©ççå®æä»æ¯ä¸ä¸ªâå·æå·¨å¤§æ¢ç´¢ç©ºé´âçæææ§é®é¢ï¼è¿å°å¸å¼æ´å¤ç ç©¶èæå¥å°ç©çæ¨¡æãéå¼ç©çå­¦ä¹ ãå ææ¨çç­ç¸å³é¢åçç ç©¶ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>çµå½±åæ¸¸æäº§ä¸ï¼</strong> å¯¹ç©ççå®æçé«åº¦éæ±ä½¿å¾è¿äºé¢åå°ç´æ¥åçãæ´çå®çå¾åç¼è¾è½åå¯ä»¥æ¾èæåè§è§ç¹æãèæåºæ¯åè§è²äº¤äºçæ²æµ¸æã</li>
<li><strong>èæç°å® (VR) åå¢å¼ºç°å® (AR)ï¼</strong> å¨è¿äºåºç¨ä¸­ï¼èæç©ä½ä¸çå®ç¯å¢çæ ç¼èåè³å³éè¦ãç©ççå®æçæåå°ä½¿å¾èæåå®¹å¨ç°å®ä¸çä¸­çèµ·æ¥æ´å èªç¶åå¯ä¿¡ã</li>
<li><strong>çµå­åå¡åäº§åå¯è§åï¼</strong> è½å¤çæå·ææ­£ç¡®é´å½±ãåå°åæè´¨äº¤äºçäº§åå¾åï¼å°å¤§å¤§æåå¨çº¿è´­ç©ä½éªåäº§åå±ç¤ºçå¸å¼åã</li>
<li><strong>æ°å­åå®¹åä½åè®¾è®¡ï¼</strong> èºæ¯å®¶åè®¾è®¡å¸å°è½å¤æ´è½»æ¾å°åå»ºåºå·æç©çä¸è´æ§çå¾åï¼åå°åæä¿®é¥°çå·¥ä½éã</li>
<li><strong>æºå¨äººåå·èº«æºè½ï¼</strong> æºå¨äººéè¦çè§£ç©çä¸çæè½è¿è¡ææçäº¤äºãè½ç¶ä¸æ¯ç´æ¥åºç¨ï¼ä½å¯¹ç©çè§å¾çå»ºæ¨¡åçæè½åï¼å¯è½é´æ¥ä¿è¿æºå¨äººå¯¹ç¯å¢ççè§£åæ¨¡æã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>è¯ä¼°çå¤ææ§ä¸ä¸»è§æ§ï¼</strong> å°½ç®¡ PICAEval ç»åäº VLM åäººå·¥æ æ³¨ï¼ä½ç©ççå®æå¨æäºæ¹é¢ä»å¯è½å·æä¸å®çä¸»è§æ§ãå¦ä½ç¡®ä¿ VLM å¤å®çé²æ£æ§åæ³åæ§ï¼ä»¥åäººå·¥æ æ³¨çä¸è´æ§ï¼æ¯ä¸ä¸ªæç»­çææã</li>
<li><strong>ç©çè§å¾çå®å¤æ§ï¼</strong> æè¦æå°äºå«ä¸ªå­ç»´åº¦ï¼åå­¦ãåå­¦ãç¶æè½¬æ¢ï¼ï¼ä½ç©çä¸çæå¶å¤æãè¿äºç»´åº¦æ¯å¦è¶³ä»¥è¦çææéè¦çç©çæåºï¼ä»¥åå¦ä½å¤çæ´å¤æçç©çç°è±¡ï¼å¦æµä½å¨åå­¦ãç­åå­¦ç­ï¼ï¼å¯è½æ¯ä¸ä¸ªæ½å¨çå±éã</li>
<li><strong>è§£å³æ¹æ¡çåæ­¥æ§ï¼</strong> è®ºææå°æ¢ç´¢äºä»è§é¢ä¸­å­¦ä¹ ç©çç¥è¯å¹¶æå»ºäº PICA-100K æ°æ®éï¼ä½æè¦å¹¶æªæ·±å¥éè¿°è¿äºè§£å³æ¹æ¡çå·ä½ææåå±éãå®æç¡®æåºâç©ççå®æä»ç¶æ¯ä¸ä¸ªå·æå·¨å¤§æ¢ç´¢ç©ºé´çæææ§é®é¢âï¼æç¤ºäºç°æè§£å³æ¹æ¡å¯è½ä»å¤äºæ©æé¶æ®µï¼è·ç¦»å®å¨è§£å³é®é¢å°è¿ã</li>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> è®­ç»è½å¤çè§£åæ¨¡æå¤æç©çè§å¾çæ¨¡åï¼ä»¥åè¿è¡ VLM-as-a-judge çè¯ä¼°åè®®ï¼å¯è½éè¦å¤§éçè®¡ç®èµæºã</li>
<li><strong>æ°æ®éçè¦çèå´ï¼</strong> PICA-100K æ°æ®éè½ç¶è§æ¨¡å¤§ï¼ä½å¶æ¶µççç©çåºæ¯åç¼è¾æä½æ¯å¦è¶³å¤å¤æ ·åï¼ä»¥åºå¯¹ææç°å®ä¸ççå¤ææåµï¼ä»éè¿ä¸æ­¥éªè¯ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼PICABench æ¯ä¸é¡¹å·æåç»æ§åéè¦æ§çå·¥ä½ï¼å®å°å¾åç¼è¾é¢åæ¨åäºç©ççå®æè¿ä¸æ´æ·±å±æ¬¡çææï¼å¹¶ä¸ºæªæ¥çç ç©¶å¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore.</li>
<li>We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17681v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17681v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17626v1'></a></p>
<h2 id="camit-a-time-aware-car-model-dataset-for-classification-and-generation"><a href="https://arxiv.org/abs/2510.17626v1">CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</a></h2>
<p><strong>Authors:</strong> FrÃ©dÃ©ric LIN, Biruk Abere Ambaw, Adrian Popescu, Hejer Ammar, Romaric Audigier, HervÃ© Le Borgne</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾FrÃ©dÃ©ric LINç­äººæ°åçè®ºæâCaMiT: A Time-Aware Car Model Dataset for Classification and Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="camit">è®ºææè¦ï¼CaMiT: ä¸ä¸ªç¨äºåç±»åçæçæ¶é´æç¥æ±½è½¦æ¨¡åæ°æ®é</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åAIè§è§ç³»ç»å¨å¤çéæ¶é´æ¼åçè§è§æ°æ®æ¶é¢ä¸´ææï¼å°¤å¶æ¯å¨ç©ä½å¤è§ä¼åçååçé¢åï¼å¦ææ¯äº§åï¼ãç°æçæ¶é´æç¥è§è§æ¨¡åä¸»è¦å³æ³¨éç¨ç±»å«æç¹å®ä¼ æå¨æ°æ®ï¼ç¼ºä¹å¯¹ç»ç²åº¦ææ¯äº§åï¼å¦æ±½è½¦æ¨¡åï¼é¿æå¤è§æ¼åè¿è¡å»ºæ¨¡çè½åãå æ­¤ï¼æ¬ææ¨å¨è§£å³å¦ä½ææå°å¨è§è§æ¨¡åä¸­å¯¹ç»ç²åº¦ææ¯äº§åéæ¶é´ååçæç»è¿è¡å»ºæ¨¡çé®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>CaMiTæ°æ®éçå¼å¥ï¼</strong> è®ºææ ¸å¿è´¡ç®æ¯æ¨åºäºCaMiTï¼Car Models in Timeï¼æ°æ®éï¼è¿æ¯ä¸ä¸ªæ¶é´æç¥çç»ç²åº¦æ±½è½¦æ¨¡åæ°æ®éãå®åå«78.7ä¸ä¸ª190ç§æ±½è½¦æ¨¡åçæ æ³¨æ ·æ¬ï¼2007-2023å¹´ï¼å510ä¸ä¸ªæªæ æ³¨æ ·æ¬ï¼2005-2023å¹´ï¼ï¼æ¯æçç£åèªçç£å­¦ä¹ ãè¯¥æ°æ®ééè¿åèªå¨åæ æ³¨æµç¨æå»ºï¼ç»åäºVLMåçç£æ¨¡åä»¥æé«æ æ³¨åç¡®æ§ã
*   <strong>æ¶é´å¢éåç±»è®¾ç½®ï¼</strong> è®ºææåºäºä¸ä¸ªç°å®çæç»­å­¦ä¹ åºæ¯ï¼å³æ¶é´å¢éåç±»è®¾ç½®ï¼ä»¥åºå¯¹ç±»å«åºç°ãæ¼ååæ¶å¤±çé®é¢ã
*   <strong>æ¶é´å¢éé¢è®­ç»ï¼TIPï¼ååç±»å¨å­¦ä¹ ï¼TICLï¼ï¼</strong> éå¯¹æ¶é´æ¼ç§»é®é¢ï¼è®ºæè¯ä¼°äºä¸¤ç§ç¼è§£ç­ç¥ï¼
    *   <strong>æ¶é´å¢éé¢è®­ç»ï¼TIPï¼ï¼</strong> æ´æ°éª¨å¹²æ¨¡åä»¥éåºæ°æ°æ®ã
    *   <strong>æ¶é´å¢éåç±»å¨å­¦ä¹ ï¼TICLï¼ï¼</strong> ä»æ´æ°æç»åç±»å±ï¼åæ¶å»ç»éª¨å¹²æ¨¡åã
*   <strong>æ¶é´æç¥å¾åçæï¼TAIGï¼ï¼</strong> å¼å¥äºæ¶é´æç¥å¾åçæä»»å¡ï¼éè¿å¨è®­ç»è¿ç¨ä¸­ä¸è´å°ä½¿ç¨æ¶é´åæ°æ®ï¼å¦åå¸å¹´ä»½ï¼æ¥çæå¾åï¼ä»¥æé«çæåå®¹ççå®æã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>éæé¢è®­ç»çæææ§ï¼</strong> å¨é¢ååæ°æ®ä¸è¿è¡éæé¢è®­ç»çæ¨¡åï¼å³ä½¿ä¸å¤§è§æ¨¡éç¨æ¨¡åç¸æ¯ï¼ä¹è½è¾¾å°æç«äºåçæ§è½ï¼ä¸èµæºæçæ´é«ãè¿è¡¨æéå¯¹ç¹å®é¢åçæ¨¡åå¨æäºæåµä¸ä¼äºéç¨æ¨¡åã
*   <strong>æ¶é´æ¼ç§»å¯¹åç¡®æ§çå½±åï¼</strong> éæé¢è®­ç»çæ¨¡åå¨è·¨å¹´ä»½æµè¯æ¶åç¡®æ§ä¼ä¸éï¼å°¤å¶æ¯å¨è®­ç»åæµè¯å¹´ä»½å·®è·è¾å¤§æ¶ï¼è¿è¯å®äºæ¶é´æ¼ç§»çå­å¨ã
*   <strong>æ¶é´å¢éç­ç¥çç§¯æææï¼</strong> TIPåTICLç­ç¥åè½ææç¼è§£æ¶é´æ¼ç§»é®é¢ï¼æé«äºæ¨¡åçæ¶é´é²æ£æ§ãå¶ä¸­ï¼TICLå¨è·¨æ¶é´ï¼å°¤å¶æ¯åæº¯æµè¯ï¼ä¸è¡¨ç°åºæä½³åç¡®æ§ï¼å¹¶ä¸å¨æçåææä¹é´åå¾äºè¯å¥½å¹³è¡¡ãè¿å¼ºè°äºå¨ç»ç²åº¦åç±»ä¸­ï¼æ´æ°åç±»å¨ä»¥éåºæ¦å¿µå¨æçéè¦æ§ã
*   <strong>æ¶é´æç¥å¾åçæçæ¹è¿ï¼</strong> éè¿å¨è®­ç»æ¶å©ç¨æ¶é´åæ°æ®ï¼æ¶é´æç¥å¾åçæï¼TAIGï¼äº§ççå¾åæ¯ä¼ ç»çææ¹æ³æ´å·çå®æåå¤æ ·æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>éæ©åå·®ï¼</strong> CaMiTæ°æ®éåå°å¤ç§éæ©åå·®çå½±åï¼ä¾å¦æ°æ®æ¥æºäºFlickrï¼å¯è½å½±åæ³åè½åï¼ï¼ä»¥åæ±½è½¦æ¨¡åååçå¨å°çåå¸åæ¶é´è¦çä¸çä¸å¹³è¡¡ã
*   <strong>æ æ³¨éå¶ï¼</strong> éç¨åèªå¨åæ æ³¨æµç¨å¯è½å¼å¥åå·®ï¼å°½ç®¡éè¿å¤éè¿æ»¤åéªè¯ç¡®ä¿äºé«åç¡®æ§ã
*   <strong>æ¶é´åæ°æ®çæ¨¡ç³æ§ï¼</strong> æ°æ®éä¸­çæ¶é´æ æ³¨å¯è½æ··æ·äºçæ­£çè®¾è®¡æ¼ååç©çèåï¼æ§è½¦ç§çï¼ï¼è¿å¯è½å½±åæ¶é´å»ºæ¨¡ç»æçç²¾ç¡®æ§ã
*   <strong>æ°æ®è®¿é®ï¼</strong> ä¸ºäºéµå®çææ³è§ï¼æ°æ®éä»ååå¾åé¾æ¥ãåµå¥ååæ°æ®ï¼èéå¾åæ¬èº«ï¼è¿å¯è½å¯¼è´é¾æ¥éæ¶é´å¤±æï¼å½±åå¯æ¯æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¶é´åæ°æ®è§£è¦ï¼</strong> æªæ¥å·¥ä½å¯ä»¥å°è¯è§£è¦è®¾è®¡æ¼ååç©çèåè¿ä¸¤ä¸ªå ç´ ï¼éè¿å°æ¶é´æ³ä¸å®æ¹æ¨¡ååå¸æ¥æææ³¨ååæ°æ®å¯¹é½ï¼æ´ç²¾ç¡®å°ç ç©¶è®¾è®¡ååã
*   <strong>æ´ç²¾ç»çæ¶é´æç¥å»ºæ¨¡ï¼</strong> é¼å±è¿ä¸æ­¥æ¢ç´¢æ´ç²¾ç»çæ¶é´æç¥å»ºæ¨¡æ¹æ³ï¼ä»¥åºå¯¹è§è§æ¦å¿µçå¨æååã
*   <strong>æ¶é´æç¥çæç®¡éçæ¹è¿ï¼</strong> è¿ä¸æ­¥ç ç©¶æ´å¤æçãåå«æ¶é´æè¯çåå®¹çæç®¡éã
*   <strong>æ©å±å°å¶ä»é¢åï¼</strong> å°CaMiTçç»éªåæ¹æ³åºç¨äºå¶ä»ç»ç²åº¦ææ¯äº§åæè§è§æ¦å¿µï¼ä»¥ç ç©¶å¶æ¶é´æ¼åã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥CaMiTæ°æ®éï¼ä¸ºè®¡ç®æºè§è§é¢åç ç©¶ç»ç²åº¦è§è§æ¦å¿µçæ¶é´æ¼åæä¾äºä¸ä¸ªå®è´µçèµæºãå®ä¸ä»æ­ç¤ºäºæ¶é´æ¼ç§»å¯¹æ¨¡åæ§è½çå½±åï¼è¿æåºäºææçç¼è§£ç­ç¥ï¼å¹¶ä¸ºæ¶é´æç¥çå¾åçæå¼è¾äºæ°çæ¹åï¼å¯¹æç»­å­¦ä¹ åçææ¨¡åçåå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts.</li>
<li>To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17626v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17626v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.17566v1'></a></p>
<h2 id="wp-cracknet-a-collaborative-adversarial-learning-framework-for-end-to-end-weakly-supervised-road-crack-detection"><a href="https://arxiv.org/abs/2510.17566v1">WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</a></h2>
<p><strong>Authors:</strong> Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, Lihua Xie</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâWP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detectionâè®ºæçå¨é¢æè¦ï¼ç¨ä¸­æåæï¼</p>
<p><strong>è®ºææè¦ï¼WP-CrackNetï¼ä¸ç§ç¨äºç«¯å°ç«¯å¼±çç£éè·¯è£ç¼æ£æµçååå¯¹æå­¦ä¹ æ¡æ¶</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºè½åå¸åºç¡è®¾æ½ç»´æ¤ä¸­éè·¯è£ç¼æ£æµçå³é®é®é¢ãä¼ ç»çåç´ çº§è£ç¼æ æ³¨ææ¬é«æä¸èæ¶ï¼ä¸¥ééå¶äºå¤§è§æ¨¡éè·¯æ£æµç³»ç»çå¯æ©å±æ§ãå æ­¤ï¼ç ç©¶é®é¢æ¯å¦ä½å¼åä¸ç§ç«¯å°ç«¯ãå¼±çç£çåç´ çº§éè·¯è£ç¼æ£æµæ¹æ³ï¼ä»ä½¿ç¨å¾åçº§æ ç­¾å°±è½å®ç°ä¸çç£æ¹æ³ç¸å½çæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
WP-CrackNetå¼å¥äºä»¥ä¸å³é®åæ°ï¼
*   <strong>ç«¯å°ç«¯å¼±çç£æ¡æ¶ï¼</strong> æåºäºä¸ç§æ°é¢çç«¯å°ç«¯å¼±çç£æ¹æ³ï¼ä»ä½¿ç¨å¾åçº§æ ç­¾è¿è¡åç´ çº§è£ç¼æ£æµï¼é¿åäºæè´µçåç´ çº§æ æ³¨ã
*   <strong>ååå¯¹æå­¦ä¹ ç­ç¥ï¼</strong> æ¡æ¶éæäºä¸ä¸ªååç»ä»¶ââåç±»å¨ãéæå¨åæ£æµå¨ï¼å¹¶éè¿å¯¹ææ§å­¦ä¹ äº¤æ¿è®­ç»ãåç±»å¨çæç±»æ¿æ´»å¾ï¼CAMsï¼ï¼éæå¨è¡¡éç¹å¾å¯æ¨æ­æ§ï¼èæ£æµå¨åä»åå¤ççè£ç¼CAMsçæçä¼ªæ ç­¾ä¸­å­¦ä¹ ãè¿ç§ç¸äºåé¦æºå¶æé«äºå­¦ä¹ ç¨³å®æ§åæ£æµç²¾åº¦ã
*   <strong>è·¯å¾æç¥æ³¨æåæ¨¡åï¼PAAMï¼ï¼</strong> ä¸ºäºè¿ä¸æ­¥æåæ£æµæ§è½ï¼è®¾è®¡äºPAAMãå®éè¿å»ºæ¨¡ç©ºé´åééä¾èµæ§ï¼èåäºåç±»å¨çé«çº§è¯­ä¹ä¿¡æ¯åéæå¨çä½çº§ç»æçº¿ç´¢ã
*   <strong>ä¸­å¿å¢å¼ºCAMä¸è´æ§æ¨¡åï¼CECCMï¼ï¼</strong> æåºäºCECCMï¼éè¿ä¸­å¿é«æ¯å æåä¸è´æ§çº¦ææ¥ç»åè£ç¼CAMsï¼ä»èçææ´å¥½çä¼ªæ ç­¾ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½å¯æ¯æ§ï¼</strong> å¨ä¸ä¸ªèªå»ºçå¾åçº§æ°æ®éä¸è¿è¡çå¹¿æ³å®éªè¡¨æï¼WP-CrackNetå¨åç´ çº§è£ç¼æ£æµæ¹é¢åå¾äºä¸çç£æ¹æ³ç¸å½çæ§è½ã
*   <strong>è¶è¶ç°æå¼±çç£æ¹æ³ï¼</strong> è¯¥æ¹æ³æ¾èä¼äºç°æçå¼±çç£æ¹æ³ï¼å°¤å¶æ¯å¨IoUææ ä¸è¡¨ç°åºæ¾èæåï¼ä¾å¦ï¼å¨Crack500æ°æ®éä¸IoUæå12.012%-41.633%ï¼ã
*   <strong>å¯æ©å±æ§æåï¼</strong> éè¿åå°å¯¹åç´ çº§æ æ³¨çä¾èµï¼WP-CrackNetæå¤§å°æ¨å¨äºå¯æ©å±çéè·¯æ£æµï¼ä½¿å¶æ´éç¨äºå¤§è§æ¨¡åºç¡è®¾æ½ç»´æ¤ã
*   <strong>CAMsè´¨éæåï¼</strong> å¯¹ææ§è®­ç»ç­ç¥ä½¿è£ç¼CAMsè½å¤æ´å®æ´å°è¦çè£ç¼åºåï¼èCECCMåå¢å¼ºäºè£ç¼CAMsççæåç©ºé´èåã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>Crack500æ°æ®éä¸çæ§è½ä¸éï¼</strong> å¨Crack500æ°æ®éä¸ï¼WP-CrackNetçIoUç¥æä¸éï¼0.313%è³9.085%ï¼ï¼è¿ä¸»è¦å½å äºï¼
    *   <strong>åºæ¯å¯åæ§åè£ç¼å½¢æå¤æ ·æ§ï¼</strong> Crack500åå«åç§è£ç¼ç±»åï¼å®½åº¦ãé¿åº¦ååæ¯æ¨¡å¼åå¼ï¼ä¸å¨ä¸åè·¯é¢ææååç§æ¡ä»¶ä¸ééï¼èæ¯çº¹çå¤æï¼å¢å äºè£ç¼è¾¹çå®ä½çé¾åº¦ã
    *   <strong>åç´ çº§æ ç­¾æ°éï¼</strong> Crack500æ°æ®éçåç´ çº§æ æ³¨æ°éå¤äºå¶ä»æ°æ®éï¼ä½¿å¾çç£ç½ç»è½å¤å­¦ä¹ æ´ç²¾ç¡®çå ä½åéªå¹¶ææå¤çç»å°æé¨åé®æ¡çè£ç¼ãWP-CrackNetä¾èµå¾åçº§çº¿ç´¢è¿è¡éå¼å®ä½ï¼è¿éå¶äºå¶å¨è¿äºæåµä¸çè¾¹çç²¾åº¦ã
*   <strong>æ¨¡ååæ°éï¼</strong> ç¸è¾äºä¸äºçç£æ¹æ³ï¼WP-CrackNetçæ¨¡ååæ°éç¥å¤§ï¼è¿ä¸»è¦æ¯ç±äºå¶å¤æ¨¡åååè®­ç»ç­ç¥ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¨¡ååç¼©ä¸å éï¼</strong> æªæ¥çå·¥ä½å°ä¾§éäºéè¿æ¨¡ååªæåç¥è¯è¸é¦æ¥åç¼©åå éWP-CrackNetï¼ä½¿å¶æ´éåå¨è¾¹ç¼è®¾å¤ï¼å¦æ äººæºï¼ä¸è¿è¡å®æ¶é¨ç½²ã
*   <strong>èåè®­ç»ä¸é¢åéåºï¼</strong> è®¡åç»åå°éç²¾ç»çåç´ çº§æ æ³¨è¿è¡èåè®­ç»ï¼ä»¥è¿ä¸æ­¥æé«æ£æµæ§è½åå¢å¼ºé¢åéåºè½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.17566v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.17566v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-21 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
