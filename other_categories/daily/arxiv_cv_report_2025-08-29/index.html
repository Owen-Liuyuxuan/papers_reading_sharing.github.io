<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-08-29 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-08-28/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-01/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-08-29">Arxiv Computer Vision Papers - 2025-08-29</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#enhancing-pseudo-boxes-via-data-level-lidar-camera-fusion-for-unsupervised-3d-object-detection" class="nav-link">Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#gs-generative-segmentation-via-label-diffusion" class="nav-link">GS: Generative Segmentation via Label Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-prototype-alignment-for-semi-supervised-pathology-image-segmentation" class="nav-link">Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#scalable-object-detection-in-the-car-interior-with-vision-foundation-models" class="nav-link">Scalable Object Detection in the Car Interior With Vision Foundation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#generalizing-monocular-3d-object-detection" class="nav-link">Generalizing Monocular 3D Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#craftgraffiti-exploring-human-identity-with-custom-graffiti-art-via-facial-preserving-diffusion-models" class="nav-link">CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#dino-u-net-exploiting-high-fidelity-dense-features-from-foundation-models-for-medical-image-segmentation" class="nav-link">Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#plug-in-feedback-self-adaptive-attention-in-clip-for-training-free-open-vocabulary-segmentation" class="nav-link">Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#bridging-domain-gaps-for-fine-grained-moth-classification-through-expert-informed-adaptation-and-foundation-model-priors" class="nav-link">Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</a>
                </li>
                <li class="nav-item">
                    <a href="#openm3d-open-vocabulary-multi-view-indoor-3d-object-detection-without-human-annotations" class="nav-link">OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-08-29">Arxiv Computer Vision Papers - 2025-08-29</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´8æ28æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçç®ææ§è¡æè¦ï¼</p>
<hr />
<p><strong>æ§è¡æè¦ï¼Arxivè®¡ç®æºè§è§è®ºæéè§ (2025å¹´8æ28æ¥)</strong></p>
<p>æ¬æ§è¡æè¦æ¨å¨ä¸ºå¿ç¢çç ç©¶äººåæä¾2025å¹´8æ28æ¥Arxivä¸ææ°åå¸ç10ç¯è®¡ç®æºè§è§è®ºæçå¿«éæ¦è§ï¼éç¹å³æ³¨å¶ä¸»è¦è¶å¿ãåæ°ç¹åæªæ¥æ¹åã</p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿ï¼</strong></p>
<ul>
<li><strong>åºç¡æ¨¡å (Foundation Models) çå¹¿æ³åºç¨ï¼</strong> å¤ä¸ªç ç©¶å©ç¨å¤§åé¢è®­ç»æ¨¡åï¼å¦Vision Foundation Models, CLIP, DINOï¼ä½ä¸ºå¼ºå¤§çç¹å¾æåå¨æåéªç¥è¯ï¼ä»¥è§£å³åç§ä¸æ¸¸ä»»å¡ï¼åæ¬ç®æ æ£æµãå¾ååå²åç»ç²åº¦åç±»ãè¿è¡¨æåºç¡æ¨¡åå·²æä¸ºæ¨å¨CVé¢åè¿æ­¥çæ ¸å¿é©±å¨åã</li>
<li><strong>ä½èµæº/æ çç£/é¶æ ·æ¬å­¦ä¹ ï¼</strong> æ¾èçè¶å¿æ¯åå°å¯¹å¤§éäººå·¥æ æ³¨æ°æ®çä¾èµãè®ºææ¢ç´¢äºæ çç£3Dç®æ æ£æµãæ æ æ³¨çå¤è§è§3Dæ£æµãåçç£åå²ä»¥åè®­ç»æ å³çå¼æ¾è¯æ±åå²ç­æ¹æ³ï¼æ¨å¨æé«æ¨¡åçæ³åè½ååé¨ç½²æçã</li>
<li><strong>3D è§è§çæç»­è¿æ­¥ï¼</strong> 3Dç®æ æ£æµï¼åæ¬åç®ãå¤è§è§åå¤æ¨¡æèåï¼LiDAR-Cameraï¼æ¹æ³ï¼ä»å¨ç§¯æåå±ï¼å¹¶çéäºæ³åè½ååæ çç£å­¦ä¹ ã</li>
<li><strong>å¾ååå²çå¤æ ·ååæ°ï¼</strong> åå²ä»»å¡éè¿çææ¨¡åï¼æ©æ£æ¨¡åï¼ãå¤æ¨¡æååå¯¹é½ãä»¥åå©ç¨åºç¡æ¨¡åçé«ä¿çç¹å¾æèªéåºæ³¨æåæºå¶ï¼å®ç°äºå¨éç¨ãå»å­¦åå¼æ¾è¯æ±åºæ¯ä¸çæ°çªç ´ã</li>
<li><strong>é¢åéåºä¸æ³åï¼</strong> éå¯¹ç¹å®é¢åï¼å¦è½¦è½½åé¥°ãççå¾åãæè«åç±»ï¼çææï¼ç ç©¶äººåè´åäºæåæ¨¡åå¨ä¸åæ°æ®åå¸ä¸çé²æ£æ§åæ³åè½åã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>[10] OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotationsï¼</strong> è¿ç¯è®ºæå¨3Dç®æ æ£æµé¢åå®ç°äºå¤é¡¹çªç ´ï¼ç»åäºå¼æ¾è¯æ±ãå¤è§è§åæ äººå·¥æ æ³¨çèå¼ï¼æå¤§å°éä½äº3Dåºæ¯çè§£çæ æ³¨ææ¬ï¼å·æå·¨å¤§çå®éåºç¨æ½åã</li>
<li><strong>[2] GS: Generative Segmentation via Label Diffusionï¼</strong> å°æ©æ£æ¨¡åå¼å¥å¾ååå²ä»»å¡ï¼éè¿æ ç­¾æ©æ£å®ç°çæå¼åå²ï¼ä¸ºåå²ä»»å¡æä¾äºä¸ä¸ªå¨æ°çè§è§åæ¹æ³ï¼ææå¨å¤æåºæ¯åæ°æ®ç¨ç¼ºæ¶å±ç°ä¼å¿ã</li>
<li><strong>[8] Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentationï¼</strong> è¯¥å·¥ä½éè¿å¨CLIPä¸­å¼å¥å³æå³ç¨çåé¦èªéåºæ³¨æåæºå¶ï¼å®ç°äºè®­ç»æ å³çå¼æ¾è¯æ±åå²ï¼æå¤§å°æé«äºå¼æ¾è¯æ±åå²çæçåçµæ´»æ§ã</li>
<li><strong>[7] Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentationï¼</strong> ææå°å°å¼ºå¤§çVision Transformerï¼å¦DINOï¼çå¯éç¹å¾ä¸U-Netæ¶æç»åï¼æ¾èæåäºå»å­¦å¾ååå²çæ§è½ï¼å¯¹å»çAIé¢åå·æéè¦æä¹ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>æ©æ£æ¨¡åå¨æç¥ä»»å¡ä¸­çåºç¨ï¼</strong> ä¸åå±éäºå¾åçæï¼æ©æ£æ¨¡åæ­£è¢«æ¢ç´¢ç¨äºæ´å¹¿æ³çæç¥ä»»å¡ï¼å¦å¾ååå²ã</li>
<li><strong>åºç¡æ¨¡åä½ä¸ºâå³æå³ç¨âç»ä»¶ï¼</strong> åºç¡æ¨¡åä¸ä»ç¨äºå¾®è°ï¼å¶åé¨æºå¶ï¼å¦å¯éç¹å¾ãæ³¨æåï¼æ­£è¢«å·§å¦å°æååå©ç¨ï¼ä»¥å®ç°è®­ç»æ å³æé«æçä¸æ¸¸ä»»å¡ã</li>
<li><strong>å¤æ¨¡æä¸å¤è§è§æ çç£/é¶æ ·æ¬å­¦ä¹ ï¼</strong> ç»åä¸åæ¨¡æï¼LiDAR-Cameraï¼åè§è§ä¿¡æ¯ï¼å¨æ çç£æé¶æ ·æ¬è®¾ç½®ä¸è§£å³3Dæç¥é®é¢ï¼æ¯æªæ¥éä½ææ¬åæé«æ³åè½åçå³é®ã</li>
<li><strong>ä¸å®¶ç¥è¯ä¸åºç¡æ¨¡åç»åï¼</strong> å°é¢åä¸å®¶ç¥è¯èå¥åºç¡æ¨¡åçéåºè¿ç¨ï¼ä»¥è§£å³ç»ç²åº¦åç±»ç­ç¹å®é¢åçææã</li>
</ul>
<p><strong>4. æå¼å¾æ·±å¥éè¯»çè®ºæï¼</strong></p>
<p>ä¸ºäºå¨é¢äºè§£å½åé¢åçåæ²¿è¿å±ï¼å»ºè®®éç¹éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>[10] OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotationsï¼</strong> ä»£è¡¨äº3Dè§è§åæ çç£/å¼æ¾è¯æ±å­¦ä¹ çææ°èåï¼å·æé«å½±ååã</li>
<li><strong>[2] GS: Generative Segmentation via Label Diffusionï¼</strong> æ¢ç´¢äºæ©æ£æ¨¡åå¨åå²ä»»å¡ä¸­çæ°èå¼ï¼ä¸ºæªæ¥ç ç©¶æä¾äºæ°æè·¯ã</li>
<li><strong>[8] Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentationï¼</strong> å±ç¤ºäºå¦ä½é«æå©ç¨ç°æåºç¡æ¨¡åå®ç°è®­ç»æ å³çå¼æ¾è¯æ±è½åï¼å·æå¾é«çå®ç¨ä»·å¼ã</li>
<li><strong>[7] Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentationï¼</strong> å¯¹äºå³æ³¨å»å­¦å¾ååæååºç¡æ¨¡ååºç¨çç ç©¶äººåï¼æä¾äºå°åè¿æ¨¡ååºç¨äºå³é®é¢åçææç­ç¥ã</li>
</ul>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2508.20530v1">Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</a></li>
<li><a href="#2508.20020v1">GS: Generative Segmentation via Label Diffusion</a></li>
<li><a href="#2508.19574v1">Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</a></li>
<li><a href="#2508.19651v1">Scalable Object Detection in the Car Interior With Vision Foundation Models</a></li>
<li><a href="#2508.19593v1">Generalizing Monocular 3D Object Detection</a></li>
<li><a href="#2508.20640v1">CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models</a></li>
<li><a href="#2508.20909v1">Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</a></li>
<li><a href="#2508.20265v1">Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation</a></li>
<li><a href="#2508.20089v1">Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</a></li>
<li><a href="#2508.20063v1">OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2508.20530v1'></a></p>
<h2 id="enhancing-pseudo-boxes-via-data-level-lidar-camera-fusion-for-unsupervised-3d-object-detection"><a href="https://arxiv.org/abs/2508.20530v1">Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</a></h2>
<p><strong>Authors:</strong> Mingqian Ji, Jian Yang, Shanshan Zhang</p>
<p><strong>Published:</strong> 2025-08-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4<script type="math/tex">\%</script> mAP on the nuScenes validation
benchmark.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§æ°é¢çæ°æ®çº§LiDAR-ç¸æºèåæ¡æ¶ï¼ç¨äºæ çç£3Dç®æ æ£æµï¼æ¨å¨è§£å³ç°ææ¹æ³ä¸­ä¼ªæ ç­¾è´¨éåéçé®é¢ãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-concise-summary">1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (Concise Summary)</h3>
<p>è¯¥è®ºææåºäºä¸ç§æ°é¢çæ°æ®çº§LiDAR-ç¸æºèåæ¡æ¶ï¼ç¨äºæ çç£3Dç®æ æ£æµï¼ä»¥åæç°ææ ç­¾çº§èåç­ç¥çå±éæ§ãéè¿å¨æ©æé¶æ®µæ´åRGBå¾ååLiDARæ°æ®ï¼å¹¶å©ç¨è§è§åºç¡æ¨¡åè¿è¡ååèåãåªå£°è¿æ»¤åå¨æèªæ¼åç­ç¥ï¼æ¾èæåäºä¼ªæ ç­¾çè´¨éå3Då®ä½ç²¾åº¦ãå®éªç»æè¡¨æï¼å¶è®­ç»åºçæ£æµå¨å¨nuScenesæ°æ®éä¸åå¾äº28.4% mAPçæ¾èæ§è½æåï¼è¶è¶äºç°ææåè¿çæ¹æ³ã</p>
<hr />
<h3 id="2-key-innovation-or-methodological-approach">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</h3>
<p>è¯¥è®ºæçæ ¸å¿åæ°å¨äºæåºäº<strong>æ°æ®çº§ï¼data-levelï¼LiDAR-ç¸æºèåæ¡æ¶</strong>ï¼èéä¼ ç»çæ ç­¾çº§èåï¼ä»¥ååå©ç¨LiDARåRGBå¾åä¹é´çæ°æ®äºè¡¥æ§ãå·ä½æ¹æ³åæ¬ï¼</p>
<ul>
<li><strong>æ©ææ°æ®èå</strong>ï¼ä¸åäºå°LiDARåRGBåç¬çæçä¼ªæ ç­¾è¿è¡ç®åæ´åï¼è¯¥æ¹æ³å¨åå§æ°æ®å±é¢ï¼æ©æé¶æ®µï¼å°±å°ä¸¤ç§æ¨¡æçæ°æ®è¿è¡èåã</li>
<li><strong>ååèåæºå¶</strong>ï¼<ul>
<li>å©ç¨é¢è®­ç»ç<strong>è§è§åºç¡æ¨¡å</strong>ï¼Vision Foundation Modelsï¼å¯¹RGBå¾åè¿è¡å®ä¾åå²åæ·±åº¦ä¼°è®¡ã</li>
<li><strong>3Dç¹è·å2Dç±»å«æ ç­¾</strong>ï¼å°2Då¾åä¸­çç±»å«ä¿¡æ¯ï¼æ¥èªå®ä¾åå²ï¼æ å°å°3Dç¹äºä¸ã</li>
<li><strong>2Dåç´ å¢å¼º3Dç¹å¯åº¦</strong>ï¼å°2Dåç´ æå½±å°3Dç©ºé´ï¼ä»¥å¢å¼ºLiDARç¹äºçå¯åº¦ï¼å°¤å¶æ¯å¨ç¨çåºåã</li>
</ul>
</li>
<li><strong>é²æ£æ§åªå£°è¿æ»¤</strong>ï¼ä¸ºäºç¼è§£æ·±åº¦ä¼°è®¡ååå²ç»æä¸­çåªå£°ï¼æåºäºï¼<ul>
<li><strong>å±é¨åå¾è¿æ»¤ï¼Local Radius Filteringï¼</strong>ï¼ç¨äºæå¶æ·±åº¦ä¼°è®¡è¯¯å·®ã</li>
<li><strong>å¨å±ç»è®¡è¿æ»¤ï¼Global Statistical Filteringï¼</strong>ï¼ç¨äºç§»é¤ç±åå²éè¯¯å¼èµ·çç¦»ç¾¤ç¹ã</li>
</ul>
</li>
<li><strong>æ°æ®çº§èåçå¨æèªæ¼åç­ç¥ï¼Dynamic Self-Evolution Strategyï¼</strong>ï¼åºäºæ°æ®çº§èåçä¼ªæ ç­¾ï¼éè¿è¿­ä»£ç»åè¿ç¨ï¼å¨å¯éè¡¨ç¤ºä¸æ¾èæåä¼ªæ ç­¾çå®ä½ç²¾åº¦ã</li>
</ul>
<hr />
<h3 id="3-potential-impact-on-the-field">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<ul>
<li><strong>æ¨å¨æ çç£3Dç®æ æ£æµåå±</strong>ï¼æ¾èæåäºæ çç£æ¹æ³å¨å¤æåºæ¯ä¸çæ§è½ï¼ä½¿å¶æ´æ¥è¿çè³å¯è½è¶è¶é¨åæçç£æ¹æ³ï¼éä½äºå¯¹æè´µ3Dæ æ³¨æ°æ®çä¾èµï¼ä»èå éäºç¸å³ç ç©¶ååºç¨ã</li>
<li><strong>å¼ºè°æ°æ®çº§èåçæ½å</strong>ï¼è¯æäºå¨æ©æé¶æ®µèåå¤æ¨¡ææ°æ®ï¼LiDARåRGBï¼çä¼è¶æ§ï¼ä¸ºæªæ¥å¤æ¨¡ææç¥ç³»ç»è®¾è®¡æä¾äºæ°èå¼ï¼å¯è½å¯åæ´å¤ç ç©¶æ¢ç´¢ä¸åæ¨¡æå¨æ°æ®å±é¢çæ·±åº¦èåã</li>
<li><strong>ä¿è¿è§è§åºç¡æ¨¡åçåºç¨</strong>ï¼å±ç¤ºäºå©ç¨é¢è®­ç»çè§è§åºç¡æ¨¡åï¼å¦å®ä¾åå²åæ·±åº¦ä¼°è®¡ï¼ä½ä¸ºå¤æ¨¡æèåçå¼ºå¤§åç«¯çæææ§ï¼ä¸ºè¿äºæ¨¡åå¨æ´å¤æä»»å¡ä¸­çåºç¨æä¾äºæ°çæè·¯ã</li>
<li><strong>å®ç¨æ§æå</strong>ï¼ä¸ºèªå¨é©¾é©¶ãæºå¨äººç­éè¦é«ç²¾åº¦3Dæç¥çåºç¨æä¾äºæ´ç»æµãé«æçè§£å³æ¹æ¡ï¼éä½äºé¨ç½²ææ¬åæ¶é´ã</li>
</ul>
<hr />
<h3 id="4-related-areas-or-applications">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>èªå¨é©¾é©¶ (Autonomous Driving)</strong>ï¼ç´æ¥åçï¼éä½äºè®­ç»3Dæç¥æ¨¡åçæ°æ®ææ¬ï¼å éäºèªå¨é©¾é©¶ææ¯çç ååé¨ç½²ã</li>
<li><strong>æºå¨äººå­¦ (Robotics)</strong>ï¼ç¨äºæºå¨äººå¯¼èªãç¯å¢æç¥ãç©ä½æååäººæºäº¤äºï¼å°¤å¶æ¯å¨æªç¥æå¨æç¯å¢ä¸­ã</li>
<li><strong>æºæ§åå¸ (Smart Cities)</strong>ï¼äº¤éçæ§ãåºç¡è®¾æ½ç®¡çãè¡äººæµåæç­åºæ¯ä¸­ç3Dç®æ è¯å«åè·è¸ªã</li>
<li><strong>å¢å¼ºç°å®/èæç°å® (AR/VR)</strong>ï¼3Dåºæ¯çè§£ãç¯å¢éå»ºåèæç©ä½ä¸çå®ä¸ççç²¾ç¡®èåã</li>
<li><strong>å·¥ä¸èªå¨å (Industrial Automation)</strong>ï¼ä¾å¦ä»åºä¸­çç©ä½è¯å«ãå®ä½ååæ£ï¼æé«èªå¨åæ°´å¹³ã</li>
</ul>
<hr />
<h3 id="5-limitations-inferred-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferred from the Abstract)</h3>
<ul>
<li><strong>å¯¹2Dè§è§åºç¡æ¨¡åçä¾èµ</strong>ï¼æ¹æ³çæ§è½å¨ä¸å®ç¨åº¦ä¸åéäºæä½¿ç¨ç2Då®ä¾åå²åæ·±åº¦ä¼°è®¡æ¨¡åçåç¡®æ§ãè¿äºæ¨¡åçè¯¯å·®ä¼ç´æ¥å½±å3Dä¼ªæ ç­¾çè´¨éï¼å°¤å¶æ¯å¨å¤ææä¸çæ³çå¾åæ¡ä»¶ä¸ã</li>
<li><strong>åªå£°ç¼è§£çææ</strong>ï¼å°½ç®¡æåºäºè¿æ»¤æ¹æ³ï¼ä½æ·±åº¦ä¼°è®¡å2Dåå²åºæçåªå£°åä¸ç¡®å®æ§ä»ç¶æ¯éè¦æç»­å³æ³¨çé®é¢ï¼å°¤å¶æ¯å¨æ¶å£ç¯å¢ï¼å¦é¨é¾ãä½åç§ãå¼ºååï¼ä¸ï¼è¿äºåªå£°å¯è½é¾ä»¥å®å¨æ¶é¤ã</li>
<li><strong>è®¡ç®èµæºæ¶è</strong>ï¼æ©ææ°æ®çº§èåãè§è§åºç¡æ¨¡åçåºç¨ä»¥åè¿­ä»£èªæ¼åç­ç¥ï¼å¯è½éè¦è¾é«çè®¡ç®èµæºåæ¶é´ææ¬ï¼è¿å¨èµæºåéçè¾¹ç¼è®¾å¤æå®æ¶åºç¨ä¸­å¯è½æ¯ä¸ä¸ªèéã</li>
<li><strong>æ³åè½å</strong>ï¼å®éªä¸»è¦å¨nuScenesæ°æ®éä¸è¿è¡ãå¶å¨ä¸åä¼ æå¨éç½®ãä¸ååºæ¯ç±»åï¼å¦è¶éãå®¤åï¼æä¸åå°çç¯å¢ä¸çæ³åè½åå°å¾è¿ä¸æ­¥éªè¯ã</li>
<li><strong>ç¹å®æ¨¡æéå¶</strong>ï¼ç®åä»éäºLiDARåRGBå¾åçèåãå¯¹äºå¶ä»æ¨¡æï¼å¦é·è¾¾ãç­æåï¼çæ´åæ½åæªæåï¼è¿äºæ¨¡æå¨æäºç¹å®åºæ¯ä¸å¯è½æä¾é¢å¤çé²æ£æ§ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage.</li>
<li>To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers.</li>
<li>Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.</li>
<li>Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4<script type="math/tex">\%</script> mAP on the nuScenes validation
benchmark.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.20530v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.20530v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.20020v1'></a></p>
<h2 id="gs-generative-segmentation-via-label-diffusion"><a href="https://arxiv.org/abs/2508.20020v1">GS: Generative Segmentation via Label Diffusion</a></h2>
<p><strong>Authors:</strong> Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang</p>
<p><strong>Published:</strong> 2025-08-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ä¸ªå¨è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åï¼ç¹å«æ¯è§è§-è¯­è¨çè§£ï¼Vision-Language Understandingï¼æ¹åä¸å·ææ½å¨éè¦æ§çæ°æ¹æ³ã</p>
<hr />
<h3 id="1-2-3">1. è®ºææ ¸å¿è´¡ç®çç®æ´æ»ç» (2-3å¥è¯)</h3>
<p>æ¬ææåºäºGSï¼Generative Segmentationï¼æ¡æ¶ï¼å°è¯­è¨é©±å¨çå¾ååå²ä»»å¡éæ°å®ä¹ä¸ºéè¿æ ç­¾æ©æ£ï¼label diffusionï¼å®ç°ççæå¼ä»»å¡ãä¸ç°ææ¹æ³å°åå²è§ä¸ºè¾å©è¿ç¨ä¸åï¼GSç´æ¥ä»åªå£°ä¸­çæåå²æ©ç ï¼å¹¶ä»¥è¾å¥å¾ååè¯­è¨æè¿°ä¸ºæ¡ä»¶ï¼ä½¿æ ç­¾çææä¸ºæ ¸å¿å»ºæ¨¡ç®æ ãå®éªç»æè¡¨æï¼GSæ¾èè¶è¶äºç°æå¤å«å¼ååºäºæ©æ£çæ¹æ³ï¼å¨è¯­è¨é©±å¨åå²ä»»å¡ä¸è¾¾å°äºæ°çSOTAã</p>
<h3 id="2">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</h3>
<p>GSçæ ¸å¿åæ°å¨äº<strong>èå¼è½¬å</strong>ï¼å®å°å¾ååå²ä»»å¡æ¬èº«ä»ä¼ ç»çå¤å«å¼é®é¢ï¼å°åç´ åç±»ä¸ºåæ¯æèæ¯ï¼æå°åå²ä½ä¸ºå¾åæ©æ£æ¨¡åçè¾å©è¾åºï¼éæ°å®ä¹ä¸º<strong>çæå¼ä»»å¡</strong>ã</p>
<p>å·ä½çæ¹æ³å­¦åæ°æ¯å¼å¥äº<strong>âæ ç­¾æ©æ£âï¼label diffusionï¼</strong>æºå¶ãä¸ç°ææ©æ£æ¨¡åéå¸¸çæå¾åï¼ä»¥æ ç­¾æææ¬ä¸ºæ¡ä»¶ï¼ä¸åï¼GSé¢ è¦äºè¿ä¸è¿ç¨ï¼å®ç´æ¥ä»éæºåªå£°ä¸­éæ­¥çæé«è´¨éçåå²æ©ç ï¼å¹¶ä»¥è¾å¥å¾ååå¯¹åºçèªç¶è¯­è¨æè¿°ä½ä¸ºçæè¿ç¨çæ¡ä»¶ãè¿ç§âæ ç­¾ä¼åâççæèå¼ä½¿å¾æ ç­¾çææä¸ºç«¯å°ç«¯è®­ç»çä¸»è¦ç®æ ï¼ä»èè½å¤å¯¹çææ©ç çç©ºé´ç²¾åº¦åè¯­ä¹ä¿çåº¦è¿è¡æ¾å¼ä¸ç²¾ç»çæ§å¶ã</p>
<h3 id="3">3. å¯¹é¢åæ½å¨å½±å</h3>
<ol>
<li><strong>å¼è¾æ°èå¼ï¼</strong> å°åå²ä»»å¡ä»å¤å«å¼æè¾å©ä»»å¡è½¬åä¸ºæ ¸å¿çæå¼ä»»å¡ï¼ä¸ºè§è§-è¯­è¨çè§£é¢åçåå²é®é¢å¼è¾äºæ°çç ç©¶æ¹åï¼å¯è½å¯åå¶ä»ç»æåé¢æµä»»å¡ï¼å¦å³é®ç¹æ£æµãå§¿æä¼°è®¡ï¼ä¸­åºç¨æ©æ£æ¨¡åçæ°æè·¯ã</li>
<li><strong>æ§è½çªç ´ï¼</strong> å¨Panoptic Narrative Grounding (PNG) ç­å¤æä¸å·ææææ§çå¤æ¨¡æåå²åºåæµè¯ä¸åå¾SOTAï¼è¯æäºçæå¼åå²å¨å¤çå¤æåè¿°æ§æè¿°åå¨æ¯çº§æ¨çæ¹é¢çä¼è¶æ§ã</li>
<li><strong>æåå¯æ§æ§ä¸ç²¾åº¦ï¼</strong> å¼ºè°å¯¹ç©ºé´åè¯­ä¹ä¿çåº¦çæ¾å¼æ§å¶ï¼é¢ç¤ºçæªæ¥æ¨¡åå¨çæé«è´¨éãé«ç²¾åº¦åå²ç»ææ¹é¢çæ½åï¼å°¤å¶æ¯å¨éè¦ç²¾ç»äº¤äºåç¼è¾çåºæ¯ã</li>
<li><strong>ç»ä¸è§è§-è¯­è¨ä»»å¡ï¼</strong> éè¿å°åå²èå¥çææ¡æ¶ï¼å¯è½ä¸ºæ´æ·±å±æ¬¡çè§è§-è¯­è¨èåå»ºæ¨¡æä¾æ°çè§è§ï¼ä¿è¿æ¨¡åæ´å¥½å°çè§£å¾ååå®¹ä¸è¯­è¨æè¿°ä¹é´çå¤æå¯¹åºå³ç³»ã</li>
</ol>
<h3 id="4">4. å¯è½åççç¸å³é¢åæåºç¨</h3>
<ol>
<li><strong>äººæºäº¤äº (HCI)ï¼</strong> åè®¸ç¨æ·éè¿èªç¶è¯­è¨æä»¤ç²¾ç¡®å°éæ©ãç¼è¾ææä½å¾åä¸­çç¹å®åºåï¼ä¾å¦æºè½å¾åç¼è¾è½¯ä»¶ãèæå©æã</li>
<li><strong>æºå¨äººå­¦ (Robotics)ï¼</strong> æºå¨äººå¯ä»¥éè¿è¯­è¨æä»¤çè§£ç¯å¢å¹¶æ§è¡ç²¾ç»çæä½ï¼ä¾å¦âæä½æ¡å­ä¸çé£ä¸ªèè²æ¯å­âéè¦ç²¾ç¡®å°åå²åºæ¯å­ã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å»çæç ç©¶äººåå¯ä»¥éè¿æè¿°æ¥è¾å©çç¶åºåçç²¾ç¡®åå²ååæï¼æé«è¯æ­æçååç¡®æ§ã</li>
<li><strong>åå®¹åä½ä¸ç¼è¾ï¼</strong> å¨çµå½±ãå¹¿åå¶ä½ä¸­ï¼åºäºææ¬æè¿°è¿è¡ç²¾ç¡®çå¯¹è±¡éæ©ãæ å¾åä¿®æ¹ï¼å¤§å¤§æé«å·¥ä½æçã</li>
<li><strong>èªå¨é©¾é©¶ï¼</strong> è½¦è¾éè¦çè§£å¤æçåºæ¯æè¿°å¹¶è¯å«ç¹å®ç®æ ï¼ä¾å¦âåæ¹å·¦ä¾§çè¡äººâï¼ï¼ä»¥è¿è¡å®å¨å³ç­ã</li>
<li><strong>è¾å©ææ¯ï¼</strong> å¸®å©è§éäººå£«éè¿è¯­è¨æè¿°æ´æ·±å¥å°çè§£å¾ååå®¹ï¼æåå¯è®¿é®æ§ã</li>
<li><strong>éç¨è§è§-è¯­è¨çè§£ï¼</strong> ä»»ä½éè¦æ·±åº¦ç»åè§è§åè¯­è¨ä¿¡æ¯è¿è¡ç²¾ç»ç©ºé´æ¨çåè¾åºçä»»å¡ã</li>
</ol>
<h3 id="5">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</h3>
<ol>
<li><strong>è®¡ç®ææ¬ï¼</strong> æ©æ£æ¨¡åéå¸¸è®¡ç®éå¤§ï¼çæè¿ç¨æ¶åå¤æ­¥è¿­ä»£ï¼å¯è½å¯¼è´æ¨çéåº¦è¾æ¢ï¼é¾ä»¥æ»¡è¶³å®æ¶åºç¨éæ±ã</li>
<li><strong>æ°æ®ä¾èµï¼</strong> è®­ç»è¿ç§å¤æççæå¼æ¨¡åï¼ç¹å«æ¯éè¦å¾åãè¯­è¨åç²¾ç¡®æ ç­¾ä¸èå¯¹é½çï¼éå¸¸éè¦å¤§è§æ¨¡ãé«è´¨éä¸æ æ³¨ç²¾ç»çæ°æ®éï¼è¿å¯è½éå¶å¶å¨æ°æ®ç¨ç¼ºé¢åçåºç¨ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡æ¯è¯­è¨é©±å¨ï¼ä½æ¨¡åå¯¹è®­ç»æ°æ®ä¸­æªåºç°è¿çå¨æ°æ¦å¿µãé«åº¦æ½è±¡ææ­§ä¹æ§å¼ºçè¯­è¨æè¿°çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>å¯æ§æ§ç²åº¦ï¼</strong> æè¦ä¸­æå°âæ¾å¼æ§å¶âï¼ä½å®éæä½ä¸­ï¼å¦ä½éè¿è¯­è¨æè¿°æ¥ç²¾ç»å°å¾®è°çææ©ç çæ¯ä¸ä¸ªç»èï¼ä»¥åå¤çè¯­è¨æè¿°ä¸­çæ¨¡ç³æ§æå²çªï¼å¯è½ä»æ¯ä¸ä¸ªææã</li>
<li><strong>æ¨¡åå¤ææ§ï¼</strong> çæå¼æ¨¡åéå¸¸æ¯å¤å«å¼æ¨¡åæ´å¤æï¼è®­ç»åè°è¯å¯è½æ´å·æææ§ã</li>
</ol>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion.</li>
<li>To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions.</li>
<li>Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.20020v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.20020v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.19574v1'></a></p>
<h2 id="multimodal-prototype-alignment-for-semi-supervised-pathology-image-segmentation"><a href="https://arxiv.org/abs/2508.19574v1">Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</a></h2>
<p><strong>Authors:</strong> Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu</p>
<p><strong>Published:</strong> 2025-08-27</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.</p>
<p><strong>Analysis:</strong></p>
<p>è¿æ¯ä¸ç¯å³äºåçç£ççå¾ååå²çè®ºææè¦åæï¼ä»¥ä¸æ¯æçä¸ä¸è§£è¯»ï¼</p>
<hr />
<h3 id="multimodal-prototype-alignment-for-semi-supervised-pathology-image-segmentation_1">è®ºææè¦åæï¼Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®æ»ç» (2-3å¥è¯)</strong></p>
<p>è¯¥è®ºææåºäºä¸ç§åä¸ºMPAMatchçæ°ååçç£åå²æ¡æ¶ï¼æ¨å¨è§£å³ççå¾ååå²ä¸­æ¨¡ç³è¾¹çåé«æ æ³¨ææ¬çææãå¶æ ¸å¿è´¡ç®å¨äºå¼å¥äºå¤æ¨¡æååå¼å¯¼ççç£èå¼ï¼éè¿å¾åååä¸åç´ æ ç­¾ãä»¥åææ¬ååä¸åç´ æ ç­¾ä¹é´çåéå¯¹æ¯å­¦ä¹ ï¼é¦æ¬¡å°ææ¬ååçç£å¼å¥åå²ä»»å¡ï¼ä»èå¨ç»æåè¯­ä¹å±é¢æä¾æ´ä¸°å¯ççç£ä¿¡æ¯ãæ­¤å¤ï¼å®è¿éè¿æ´åççé¢è®­ç»åºç¡æ¨¡åï¼Uniï¼æ¹è¿äºTransUNetæ¶æï¼ä»¥æ´ææå°æåççç¸å³ç¹å¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>MPAMatchçå³é®åæ°å¨äºå¶<strong>å¤æ¨¡æååå¼å¯¼ççç£èå¼</strong>å<strong>åéå¯¹æ¯å­¦ä¹ æ¹æ¡</strong>ï¼
*   <strong>åéå¯¹æ¯å­¦ä¹ ï¼</strong>
    *   <strong>å¾åååä¸åç´ æ ç­¾çå¯¹æ¯å­¦ä¹ ï¼</strong> æä¾ç»æå±é¢ççç£ï¼å¢å¼ºå¯¹å¾åå±é¨ç¹å¾çå¤å«è½åã
    *   <strong>ææ¬ååä¸åç´ æ ç­¾çå¯¹æ¯å­¦ä¹ ï¼</strong> è¿æ¯è¯¥æ¹æ³çç¬ç¹ä¹å¤ï¼é¦æ¬¡å°é«å±è¯­ä¹ä¿¡æ¯ï¼éè¿ææ¬æè¿°æè·ï¼å¼å¥åç´ çº§åå²ä»»å¡ï¼æ¾èæ¹åäºè¯­ä¹è¾¹çå»ºæ¨¡ï¼å®ç°äºä»ç²å°ç»ççç£ç­ç¥ã
*   <strong>æ¶ææ¹è¿ï¼</strong> å°ç»å¸çTransUNetæ¶æä¸­çViTéª¨å¹²ç½ç»æ¿æ¢ä¸ºç»è¿ççå­¦é¢è®­ç»çåºç¡æ¨¡åï¼Uniï¼ï¼è¿ä½¿å¾æ¨¡åè½å¤æåæ´å·é¢åç¹å¼æ§åé²æ£æ§çççç¸å³ç¹å¾ã
*   <strong>åçç£å­¦ä¹ èå¼ï¼</strong> å¨æ æ ç­¾æ ·æ¬ä¸éè¿è¿ç§å¤æ¨¡æååå¯¹é½è¿è¡åç´ çº§å¯¹æ¯å­¦ä¹ ï¼ææå©ç¨äºæªæ æ³¨æ°æ®ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>éä½æ æ³¨ææ¬ï¼</strong> éè¿é«æçåçç£å­¦ä¹ ï¼MPAMatchææå¤§å¹åå°ççå¾ååå²æéçåç´ çº§æ æ³¨å·¥ä½éï¼å éAIå¨æ°å­ççé¢åçåºç¨åé¨ç½²ã</li>
<li><strong>æååå²ç²¾åº¦åé²æ£æ§ï¼</strong> å¼å¥ææ¬è¯­ä¹ä¿¡æ¯ï¼ç¹å«æ¯å¯¹æ¨¡ç³è¾¹ççå»ºæ¨¡è½åï¼å°æ¾èæé«ççå¾ååå²çåç¡®æ§åå¯¹å¤æç»æççè§£ã</li>
<li><strong>å¼è¾æ°ç ç©¶æ¹åï¼</strong> é¦æ¬¡å°ææ¬ååçç£å¼å¥åå²ä»»å¡ï¼ä¸ºå¤æ¨¡æå­¦ä¹ å¨åç´ çº§ä»»å¡ä¸­çåºç¨å¼è¾äºæ°çéå¾ï¼é¼å±ç ç©¶èæ¢ç´¢æ´å¤æ¨¡æï¼å¦åºå ç»æ°æ®ãä¸´åºæ¥åï¼ä¸å¾ååå²çç»åã</li>
<li><strong>æ¨å¨åºç¡æ¨¡åå¨å»çé¢åçåºç¨ï¼</strong> ç»åççé¢è®­ç»çåºç¡æ¨¡åï¼éªè¯äºå¶å¨ç¹å®é¢åä»»å¡ä¸­çå¼ºå¤§ç¹å¾æåè½åï¼ä¸ºæªæ¥å»çAIæ¨¡åçå¼åæä¾äºèä¾ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>æ°å­ççå­¦åè®¡ç®ççå­¦ï¼</strong> è¿ç¤åå²ãèºä½åå²ãç»ç»ç±»åè¯å«ãç¾çåçº§åé¢åè¯ä¼°ç­ã</li>
<li><strong>å»å­¦å¾ååæï¼</strong> å¶ä»éè¦ç²¾ç»åç´ çº§åå²ä¸æ æ³¨ææ¬é«æçå»å­¦å½±åä»»å¡ï¼å¦æ¾å°å­¦å¾åï¼CT/MRIï¼ä¸­çå¨å®æçç¶åå²ã</li>
<li><strong>åçç£å­¦ä¹ ï¼</strong> ä¸ºéç¨åçç£å­¦ä¹ æ¹æ³æä¾äºæ°çå¤æ¨¡æåååå¼å¯¼ç­ç¥ã</li>
<li><strong>å¤æ¨¡æå­¦ä¹ ï¼</strong> æ¨å¨äºå¾åä¸ææ¬ç­å¤æ¨¡æä¿¡æ¯èåå¨åç´ çº§ä»»å¡ä¸­çç ç©¶ã</li>
<li><strong>åºç¡æ¨¡ååºç¨ï¼</strong> ä¸ºå¦ä½å°å¤§åé¢è®­ç»æ¨¡åï¼å¦è§è§Transformerï¼éåºå°ç¹å®é¢åï¼å¦å»çï¼å¹¶ç»åä¸æ¸¸ä»»å¡æä¾äºå®è·µç»éªã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>ææ¬ååçæä¸è´¨éï¼</strong> æè¦ä¸­æªè¯¦ç»è¯´æææ¬ååæ¯å¦ä½çææè·åçãé«è´¨éãå·æä»£è¡¨æ§çææ¬ååå¯¹äºè¯­ä¹çç£è³å³éè¦ï¼å¶çæè¿ç¨å¯è½å¤æä¸éè¦é¢åä¸å®¶ç¥è¯ï¼çè³å¯è½å¼å¥ä¸»è§æ§æåå·®ã</li>
<li><strong>å¯¹åºç¡æ¨¡åçä¾èµï¼</strong> æ§è½å¯è½é«åº¦ä¾èµäºâççé¢è®­ç»åºç¡æ¨¡åï¼Uniï¼âçè´¨éãå¯ç¨æ§åå¶é¢è®­ç»æ°æ®çè§æ¨¡åå¤æ ·æ§ãå¦æè¯¥æ¨¡åä¸å¬å¼æé¾ä»¥è·åï¼åæ¹æ³çå¤ç°åæ¨å¹¿å¯è½åéã</li>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> åç´ çº§å¯¹æ¯å­¦ä¹ ï¼å°¤å¶æ¯å¨é«åè¾¨çççå¾åä¸ï¼éå¸¸éè¦å¤§éçè®¡ç®èµæºï¼GPUåå­åè®¡ç®æ¶é´ï¼ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡å¨å¤ä¸ªæ°æ®éä¸è¿è¡äºéªè¯ï¼ä½ççå¾åçå¤ææ§åå¤æ ·æ§æé«ï¼ä¸åæè²ãä¸åç¾çç±»åãä¸åæ«æä»ªï¼ï¼ææ¬åååå¾åååå¨é¢å¯¹æªè§è¿çæ°ççç±»åæç½è§çåæ¶çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>å¯è§£éæ§ï¼</strong> å¤æ¨¡æååå¯¹é½åå¯¹æ¯å­¦ä¹ çåé¨æºå¶å¯è½ç¸å¯¹å¤æï¼å¶å³ç­è¿ç¨çå¯è§£éæ§å¯è½ä¸å¦ä¸äºæ´ç®åçæ¨¡åã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm.</li>
<li>Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.19574v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.19574v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.19651v1'></a></p>
<h2 id="scalable-object-detection-in-the-car-interior-with-vision-foundation-models"><a href="https://arxiv.org/abs/2508.19651v1">Scalable Object Detection in the Car Interior With Vision Foundation Models</a></h2>
<p><strong>Authors:</strong> BÃ¡lint MÃ©szÃ¡ros, Ahmet Firintepe, Sebastian Schmidt, Stephan GÃ¼nnemann</p>
<p><strong>Published:</strong> 2025-08-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL<script type="math/tex">_{score}</script> of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL<script type="math/tex">_{SNR}</script> three times higher than GPT-4o.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææè¦æä¾äºä¸ä¸ªå³äºå¨è½¦è½½ç¯å¢ä¸­åºç¨è§è§åºç¡æ¨¡åè¿è¡ç©ä½æ£æµåå®ä½çæè¶£è§è§ãä»¥ä¸æ¯æçåæï¼</p>
<h3 id="1-2-3_1">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (2-3 å¥è¯)</h3>
<p>è¯¥è®ºææåºäºä¸ä¸ªåä¸ºODALï¼Object Detection and Localizationï¼çæ¡æ¶ï¼æ¨å¨è§£å³è½¦è½½ç³»ç»èµæºåéä¸ï¼å¨æ±½è½¦åé¨ç½²è§è§åºç¡æ¨¡åè¿è¡ç©ä½æ£æµåå®ä½çææãéè¿éç¨åå¸å¼æ¶æï¼å°è®¡ç®ä»»å¡å¨è½¦è½½ç«¯åäºç«¯ä¹é´åæï¼ODALæ¡æ¶æååæäºç´æ¥å¨è½¦åè¿è¡å¤§ååºç¡æ¨¡åçéå¶ãç ç©¶è¡¨æï¼ç»è¿å¾®è°çè½»éçº§ODAL-LLaVAæ¨¡åä¸ä»å¨æ§è½ä¸è¶è¶äºGPT-4oï¼è¿æ¾èéä½äºå¹»è§ç°è±¡ï¼ä¸ºè½¦è½½AIä»»å¡æ ç«äºæ°æ åã</p>
<h3 id="2_1">2. å³é®åæ°ææ¹æ³è®º</h3>
<ul>
<li><strong>åå¸å¼æ¶æå©ç¨è§è§åºç¡æ¨¡åï¼</strong> æ ¸å¿åæ°å¨äºå¶<strong>åå¸å¼ODALæ¡æ¶</strong>ï¼å®å·§å¦å°å°è§è§åºç¡æ¨¡åçè®¡ç®ä»»å¡åè§£ï¼ä¸é¨åå¨èµæºåéçè½¦è½½ç«¯æ§è¡ï¼å¦ä¸é¨ååå¸è½½å°äºç«¯ãè¿ä½¿å¾å¨è½¦åç¯å¢ä¸­ä½¿ç¨éå¸¸éè¦å¤§éè®¡ç®èµæºçè§è§åºç¡æ¨¡åæä¸ºå¯è½ã</li>
<li><strong>è½»éçº§æ¨¡åå¾®è°è¶è¶SOTAï¼</strong> è®ºæå±ç¤ºäºéè¿å¯¹LLaVA 1.5 7Bè¿æ ·çè½»éçº§æ¨¡åè¿è¡<strong>å¾®è°</strong>ï¼ä¸ä»è½å¤§å¹æåå¶å¨ç¹å®ä»»å¡ä¸çæ§è½ï¼ODAL<script type="math/tex">_{score}</script>æå71%ï¼ï¼çè³è½è¶è¶å½åæåè¿çéç¨æ¨¡åï¼GPT-4oï¼ï¼å¹¶å¨åå°å¹»è§æ¹é¢è¡¨ç°æ´ä¼ï¼ODAL<script type="math/tex">_{SNR}</script>æ¯GPT-4oçä¸åï¼ã</li>
<li><strong>æ°åè¯ä¼°ææ ODALbenchï¼</strong> å¼å¥äº<strong>ODALbench</strong>è¿ä¸æ°çåº¦éæ åï¼ç¨äºå¨é¢è¯ä¼°æ£æµåå®ä½æ§è½ï¼è¿æå©äºå¨è¯¥ç¹å®é¢åå»ºç«ç»ä¸çæ§è½è¯ä¼°åºåã</li>
</ul>
<h3 id="3_1">3. å¯¹é¢åæ½å¨å½±å</h3>
<ul>
<li><strong>æ¨å¨è¾¹ç¼AIåè½¦è½½AIåå±ï¼</strong> è¯¥ç ç©¶ä¸ºå¨èµæºåéçè¾¹ç¼è®¾å¤ï¼å¦æ±½è½¦ï¼ä¸é¨ç½²å¤æAIæ¨¡åæä¾äºä¸ä¸ªå¯è¡çèå¼ãå®å¯è½å éè½¦è½½AIï¼å¦æºè½åº§è±å©æãé©¾é©¶åçæ§ãä¹å®¢å®å¨ï¼çæ®ååè½åæåã</li>
<li><strong>éæ°å®ä¹åºç¡æ¨¡åé¨ç½²ç­ç¥ï¼</strong> è®ºæè¯æäºåå¸å¼æ¶æå¨å©ç¨å¤§ååºç¡æ¨¡åæ¹é¢çæ½åï¼è¿å¯è½ä¿ä½¿æªæ¥æ´å¤AIåºç¨éç¨æ··åï¼æ¬å°+äºç«¯ï¼é¨ç½²ç­ç¥ï¼ä»¥å¹³è¡¡æ§è½ãèµæºåææ¬ã</li>
<li><strong>å¼ºè°è½»éçº§æ¨¡åå¾®è°çéè¦æ§ï¼</strong> ç»æè¡¨æï¼éå¯¹ç¹å®é¢åè¿è¡æ·±åº¦å¾®è°çè½»éçº§æ¨¡åï¼å¶æ§è½å¯ä»¥è¶è¶éç¨åå¤§åæ¨¡åï¼è¿å¯è½ä¼é¼å±ç ç©¶äººååå¼åèæ´å¤å°å³æ³¨æ¨¡åæçåé¢åéåºæ§ï¼èéä¸å³è¿½æ±æ¨¡åè§æ¨¡ã</li>
<li><strong>å»ºç«æ°çè¯ä¼°æ åï¼</strong> ODALbenchçå¼å¥æææä¸ºè½¦è½½åé¨ç©ä½æ£æµåå®ä½ä»»å¡çè¡ä¸æ åï¼ä¿è¿æ´å¬å¹³ãæ´å¨é¢çæ¨¡åæ¯è¾ã</li>
</ul>
<h3 id="4_1">4. ç¸å³é¢åæåºç¨</h3>
<ul>
<li><strong>æºè½åº§è±åè½¦è½½å©æï¼</strong> ç´æ¥åçï¼å¯å®ç°æ´ç²¾åçç©ä½è¯å«åå®ä½ï¼æåè¯­é³å©æååºè´¨éåç¨æ·ä½éªã</li>
<li><strong>é©¾é©¶ååä¹å®¢çæ§ï¼</strong> ç¨äºæ£æµè½¦åå¼å¸¸ç©åãå¿ç«¥éçãé©¾é©¶åç²å³æåå¿ç­ï¼æåè¡è½¦å®å¨ã</li>
<li><strong>è¾¹ç¼è®¡ç®åç©èç½ï¼IoTï¼ï¼</strong> ä»»ä½éè¦å¼ºå¤§AIè½åä½æ¬å°è®¡ç®èµæºæéçè¾¹ç¼è®¾å¤ï¼å¦æºè½å®¶å±è®¾å¤ãå·¥ä¸æºå¨äººãæºè½æåå¤´ï¼é½å¯ä»¥åé´è¿ç§åå¸å¼æ¶æã</li>
<li><strong>æºå¨äººå­¦ï¼</strong> æºå¨äººéå¸¸å·ææéçæ¿è½½è®¡ç®è½åï¼ä½å¯è½éè¦å¤æçè§è§çè§£è½åï¼è¯¥æ¹æ³å¯ç¨äºå®ç°æ´é«çº§çæç¥åè½ã</li>
<li><strong>éç§è®¡ç®ï¼</strong> è½ç¶æè¦æªæåï¼ä½åå¸å¼æ¶æå¨æªæ¥å¯è½ä¸èé¦å­¦ä¹ æéç§ä¿æ¤ææ¯ç»åï¼ä»¥å¨äºç«¯å¤çæ°æ®æ¶æ´å¥½å°ä¿æ¤ç¨æ·éç§ã</li>
</ul>
<h3 id="5_1">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</h3>
<ul>
<li><strong>ç½ç»è¿æ¥ä¾èµæ§ï¼</strong> åå¸å¼æ¶æé«åº¦ä¾èµç¨³å®ãä½å»¶è¿çäºç«¯ç½ç»è¿æ¥ãå¨ç½ç»ä¿¡å·å·®ææ ä¿¡å·çåºåï¼ç³»ç»çæ§è½å¯è½ä¼ä¸¥éä¸éçè³æ æ³å·¥ä½ã</li>
<li><strong>éç§åå®å¨é®é¢ï¼</strong> å°è½¦ååºæ¯æ°æ®ä¼ è¾å°äºç«¯è¿è¡å¤çï¼å¯è½ä¼å¼åç¨æ·éç§åæ°æ®å®å¨æ¹é¢çæå¿§ï¼éè¦å¼ºå¤§çå å¯åéç§ä¿æ¤æºå¶ã</li>
<li><strong>å®æ¶æ§ææï¼</strong> å°½ç®¡è§£å³äºèµæºéå¶ï¼ä½äºç«¯éä¿¡çå¾è¿å»¶è¿ï¼latencyï¼å¯¹äºæäºå¯¹å®æ¶æ§è¦æ±æé«çè½¦è½½ä»»å¡ï¼å¦ç´§æ¥å®å¨ååºï¼å¯è½ä»ç¶æ¯ä¸ä¸ªææãæè¦ä¸­æªæä¾å·ä½çå»¶è¿ææ ã</li>
<li><strong>è¿è¥ææ¬ï¼</strong> æç»­å©ç¨äºç«¯è®¡ç®èµæºä¼äº§çç¸åºçè¿è¥ææ¬ï¼è¿å¯¹äºå¤§è§æ¨¡é¨ç½²ååä¸åå¯è½æ¯ä¸ä¸ªéè¦èèçå ç´ ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡ODAL-LLaVAå¨âæ±½è½¦åé¨âä»»å¡ä¸è¡¨ç°åºè²ï¼ä½å¶å¨å¶ä»éç¨ç©ä½æ£æµæå®ä½ä»»å¡ä¸çæ³åè½åå¦ä½ï¼æè¦ä¸­å¹¶æªæåãå¶é«æ§è½å¯è½é«åº¦ä¾èµäºéå¯¹ç¹å®é¢åè¿è¡çå¾®è°ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding.</li>
<li>Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud.</li>
<li>To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain.</li>
<li>We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.19651v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.19651v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.19593v1'></a></p>
<h2 id="generalizing-monocular-3d-object-detection"><a href="https://arxiv.org/abs/2508.19593v1">Generalizing Monocular 3D Object Detection</a></h2>
<p><strong>Authors:</strong> Abhinav Kumar</p>
<p><strong>Published:</strong> 2025-08-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæçæè¦æä¾äºä¸ä¸ªå³äºåç®3Dç®æ æ£æµï¼Mono3Dï¼é¢åéè¦è¿å±çæ¦è§ï¼ç¹å«å³æ³¨å¶æ³åè½åãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-concise-summary_1">1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (Concise Summary)</h3>
<p>è¿ç¯è®ºæç³»ç»æ§å°è§£å³äºåç®3Dç®æ æ£æµæ¨¡åå¨å¤æåå¤æ ·ååºæ¯ä¸çæ³åè½åææãä½èéè¿æåºä¸ç³»ååæ°æ¹æ³ï¼åå«æåäºæ¨¡åå¨å¤çé®æ¡ãéåºæ°æ°æ®éãæ£æµå¤§åç©ä½ä»¥ååºå¯¹ä¸åç¸æºåæ°ï¼ç¹å«æ¯ç¸æºé«åº¦ï¼æ¶çé²æ£æ§ååç¡®æ§ãå¶æ ¸å¿è´¡ç®å¨äºç³»ç»æ§å°å¢å¼ºMono3Dæ¨¡åçå®ç¨æ§åå¯é æ§ã</p>
<h3 id="2-key-innovation-or-methodological-approach_1">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</h3>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶<strong>å¤ç®¡é½ä¸ãç³»ç»æ§å°è§£å³Mono3Dæ³åé®é¢çç­ç¥</strong>ãå®å¹¶éä¾èµåä¸ççªç ´æ§ç®æ³ï¼èæ¯éå¯¹æ³åè½åçå¤ä¸ªå·ä½ç»´åº¦ï¼é®æ¡ãæ°æ®éãç©ä½å¤§å°ãç¸æºåæ°ï¼æåºäºå®å¶åçè§£å³æ¹æ¡ï¼</p>
<ul>
<li><strong>GrooMeD-NMS (Occlusion Robustness):</strong> æåºäºä¸ç§<strong>æ°å­¦å¯å¾®åçéæå¤§å¼æå¶ï¼NMSï¼</strong>æ¹æ³ï¼æ¨å¨æé«æ¨¡åå¨å­å¨é®æ¡æåµä¸çæ£æµæ§è½ãå¯å¾®åNMSåè®¸ç«¯å°ç«¯è®­ç»ï¼æ´å¥½å°ä¼åé®æ¡åºæ¯ä¸çæ£æµç»æã</li>
<li><strong>DEVIANT Backbones (Dataset Generalization):</strong> æ¢ç´¢äº<strong>æ·±åº¦ç­åï¼depth equivariantï¼éª¨å¹²ç½ç»</strong>ï¼ä»¥å¢å¼ºæ¨¡åå¯¹æ°æ°æ®éçéåºæ§ãæ·±åº¦ç­åæ§æå³çæ¨¡åå¯¹è¾å¥æ·±åº¦å¾çåæ¢å·æä¸è´çååºï¼è¿å¯¹äºè·¨æ°æ®éçæ³åè³å³éè¦ã</li>
<li><strong>SeaBird (Large Object Detection):</strong> éå¯¹å¤§åç©ä½æ£æµä¸­çåªå£°æææ§é®é¢ï¼æåºäºä¸ç§<strong>åºäºé¸ç°å¾ï¼BEVï¼çåå²æ¹æ³ï¼å¹¶ç»åDiceæå¤±</strong>ãè¿è¡¨æä½èå°å¤§åç©ä½æ£æµéæ°å®ä¹ä¸ºä¸ä¸ªåå²ä»»å¡ï¼å¹¶å©ç¨BEVçä¼å¿æ¥éä½åªå£°å½±åã</li>
<li><strong>Mathematical Analysis for Camera Parameters:</strong> å¯¹Mono3Dæ¨¡åå¨<strong>æªè§è¿çç¸æºé«åº¦</strong>ç­åå¸å¤ï¼OODï¼è®¾ç½®ä¸çå¤æ¨è½åè¿è¡äº<strong>æ°å­¦åæ</strong>ï¼å¹¶æ®æ­¤æ¹è¿äºæ³åæ§è½ãè¿ä¸ºçè§£åæåæ¨¡åå¨ä¸åç¸æºéç½®ä¸çè¡¨ç°æä¾äºçè®ºåºç¡ã</li>
</ul>
<h3 id="3-potential-impact-on-the-field_1">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>è¿é¡¹ç ç©¶å¦ææåï¼å°å¯¹è®¡ç®æºè§è§é¢åäº§çæ¾èå½±åï¼å°¤å¶æ¯å¨ä¾èµ3Dæç¥çå®éåºç¨ä¸­ï¼</p>
<ul>
<li><strong>æåMono3Dçå®ç¨æ§ï¼</strong> éè¿è§£å³æ³åæ§è¿ä¸æ ¸å¿ææï¼è¯¥ç ç©¶å°ä½¿åç®3Dç®æ æ£æµæ¨¡åå¨çå®ä¸çå¤æå¤åçç¯å¢ä¸­æ´å å¯é åå®ç¨ï¼ä»èå éå¶å¨èªå¨é©¾é©¶ãå¢å¼ºç°å®åæºå¨äººç­é¢åçé¨ç½²ã</li>
<li><strong>æ¨å¨æ³åæ§ç ç©¶ï¼</strong> è®ºæéå¯¹ä¸åæ³åæææåºçå·ä½è§£å³æ¹æ¡ï¼å¦å¯å¾®åNMSãæ·±åº¦ç­åç½ç»ãBEVåå²æ¹æ³ï¼ä¸ºæªæ¥ç ç©¶æä¾äºæ°çæè·¯åå·¥å·ï¼é¼å±ç ç©¶äººåæ´ç»è´å°åæåè§£å³æ¨¡åæ³åé®é¢ã</li>
<li><strong>çè®ºä¸å®è·µç»åï¼</strong> å¯¹ç¸æºåæ°çæ°å­¦åæï¼ç»åå·ä½çç®æ³æ¹è¿ï¼ä½ç°äºçè®ºæå¯¼å®è·µçä»·å¼ï¼æå©äºæå»ºæ´å·é²æ£æ§çæ¨¡åã</li>
</ul>
<h3 id="4-related-areas-or-applications-that-might-benefit">4. ç¸å³é¢åæåºç¨åç (Related Areas or Applications that might benefit)</h3>
<p>é¤äºæè¦ä¸­æç¡®æå°ç<strong>èªå¨é©¾é©¶ãå¢å¼ºç°å®åæºå¨äººææ¯</strong>ï¼ä»¥ä¸é¢åååºç¨ä¹å¯è½ä»è¿é¡¹ç ç©¶ä¸­åçï¼</p>
<ul>
<li><strong>3Dåºæ¯çè§£ï¼</strong> ä»»ä½éè¦ä»åç®å¾åè¿è¡ç²¾ç¡®3Dåºæ¯éå»ºåçè§£çä»»å¡ï¼ä¾å¦æºè½åå¸çæ§ãæ äººæºå·¡æ£ã</li>
<li><strong>äººæºäº¤äºï¼</strong> éè¦å®æ¶æç¥ç¨æ·3Då§¿æææå¿çåºç¨ï¼å¦èæç°å®ï¼VRï¼ä¸­çäº¤äºãæºè½å®¶å±æ§å¶ã</li>
<li><strong>å·¥ä¸æ£æµä¸æµéï¼</strong> å¨çäº§çº¿ä¸è¿è¡ç©ä½å°ºå¯¸ãä½ç½®åç¼ºé·ç3Dæ£æµï¼å°¤å¶æ¯å¨ææ¬æææç©ºé´åéçåºæ¯ä¸ã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å¦æè½å°2Då»å­¦å¾åè½¬åä¸º3Dç»æï¼å°æå©äºç¾çè¯æ­åæ²»çè§åã</li>
<li><strong>è®¡ç®æºå¾å½¢å­¦ï¼</strong> è¾å©3Dæ¨¡åçæãåºæ¯éå»ºåèæç¯å¢çåå»ºã</li>
</ul>
<h3 id="5-limitations-that-can-be-inferred-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations that can be inferred from the abstract)</h3>
<p>å°½ç®¡æè¦å±ç¤ºäºå¼ºå¤§çè´¡ç®ï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨çå±éæ§ï¼</p>
<ul>
<li><strong>æ¨¡åå¤ææ§å¢å ï¼</strong> å¼å¥å¤ç§ä¸é¨çæ¨¡åï¼GrooMeD-NMSãDEVIANTéª¨å¹²ãSeaBirdï¼å¯è½ä¼æ¾èå¢å æ¨¡åçæ´ä½å¤ææ§ãè®¡ç®ææ¬ååå­éæ±ï¼è¿å¯è½å¯¹å®æ¶åºç¨æèµæºåéçè®¾å¤ææææã</li>
<li><strong>æ¨¡åé´ååä¸éæï¼</strong> æè¦å°è¿äºè§£å³æ¹æ¡è§ä¸ºç¬ç«çé®é¢è§£å³èãå¦ä½ææå°å°è¿äºä¸åçç»ä»¶éæå°ä¸ä¸ªç»ä¸çæ¡æ¶ä¸­ï¼å¹¶ç¡®ä¿å®ä»¬ä¹é´è¯å¥½çååä½ç¨èéç¸äºå¹²æ°ï¼æ¯ä¸ä¸ªæ½å¨çææã</li>
<li><strong>æ³åèå´çå®æ´æ§ï¼</strong> å°½ç®¡è§£å³äºé®æ¡ãæ°æ®éãç©ä½å¤§å°åç¸æºåæ°ç­å³é®æ³åç»´åº¦ï¼ä½âæ³åâæ¯ä¸ä¸ªéå¸¸å¹¿æ³çæ¦å¿µãæè¦å¹¶æªæåå¯¹å¶ä»éè¦å ç´ ï¼å¦æç«¯åç§ååãæ¶å£å¤©æ°æ¡ä»¶ãç©ä½æè´¨å¤æ ·æ§ãè¿å¨æ¨¡ç³ç­ï¼çæ³åè½åã</li>
<li><strong>åç®è§è§çåºæå±éï¼</strong> æ è®ºæ¨¡åå¦ä½ä¼åï¼åç®3Dæ£æµå§ç»é¢ä¸´ä»2Då¾åæ¨æ­3Dæ·±åº¦ä¿¡æ¯çåºææ¨¡ç³æ§ãè¯¥å·¥ä½æ¨å¨ç¼è§£è¿äºé®é¢ï¼ä½æ æ³ä»æ ¹æ¬ä¸æ¶é¤åç®è¾å¥çå±éæ§ï¼å¶ç²¾åº¦åé²æ£æ§å¯è½ä»é¾ä»¥ä¸å¤ç®ææ·±åº¦ä¼ æå¨æ¹æ¡åª²ç¾ã</li>
<li><strong>âæ¢ç´¢âçå«ä¹ï¼</strong> å¯¹äºDEVIANTéª¨å¹²ç½ç»ï¼æè¦ä½¿ç¨äºâwe exploreâï¼æä»¬æ¢ç´¢ï¼ï¼è¿å¯è½æç¤ºè¯¥æ¹åä»å¨ç ç©¶ä¸­ï¼å¶æç»çæ³åææåç¨³å®æ§å¯è½å°æªå®å¨éªè¯æè¾¾å°æä½³ç¶æã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS).</li>
<li>To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones.</li>
<li>To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.19593v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.19593v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.20640v1'></a></p>
<h2 id="craftgraffiti-exploring-human-identity-with-custom-graffiti-art-via-facial-preserving-diffusion-models"><a href="https://arxiv.org/abs/2508.20640v1">CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models</a></h2>
<p><strong>Authors:</strong> Ayan Banerjee, Fernando VilariÃ±o, Josep LladÃ³s</p>
<p><strong>Published:</strong> 2025-08-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæãCraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Modelsãå¨è®¡ç®æºè§è§åçæå¼AIé¢åæåºäºä¸ä¸ªå¼äººæ³¨ç®çè§£å³æ¹æ¡ï¼å°¤å¶æ¯å¨å¤çæç«¯é£æ ¼åè½¬æ¢æ¶çäººè¸èº«ä»½ä¿æé®é¢ã</p>
<hr />
<h3 id="1">1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç»</h3>
<p>CraftGraffiti æåºäºä¸ç§ç«¯å°ç«¯ãææ¬å¼å¯¼çæ¶é¸¦çææ¡æ¶ï¼æ¨å¨è§£å³å¨æç«¯é£æ ¼åï¼å¦æ¶é¸¦ï¼ä¸äººè¸èº«ä»½é¾ä»¥ä¿æçææãè¯¥æ¡æ¶éè¿ç»å LoRA å¾®è°çæ©æ£æ¨¡åè¿è¡é£æ ¼è¿ç§»ï¼å¹¶å¼å¥ä¸ç§å¸¦ææ¾å¼èº«ä»½åµå¥çäººè¸ä¸è´èªæ³¨æåæºå¶æ¥å¼ºå¶ä¿æèº«ä»½ï¼åæ¶å©ç¨ CLIP å¼å¯¼çæç¤ºæ©å±å®ç°æ å³é®ç¹çå§¿æå®å¶ãè®ºæçæ ¸å¿è´¡ç®å¨äºéªè¯äºâåé£æ ¼ï¼åèº«ä»½âçå¤çèå¼è½ææåå°å±æ§æ¼ç§»ï¼å¹¶å¨èº«ä»½ä¸è´æ§ãç¾å­¦åç¨æ·åå¥½æ¹é¢åå¾äºæåè¿çç»æã</p>
<h3 id="2_2">2. å³é®åæ°ææ¹æ³è®º</h3>
<p>è¯¥è®ºæçå³é®åæ°åæ¹æ³è®ºä½ç°å¨ä»¥ä¸å ä¸ªæ¹é¢ï¼</p>
<ul>
<li><strong>âåé£æ ¼ï¼åèº«ä»½âèå¼ï¼</strong> è®ºæå½¢å¼åå°è®ºè¯å¹¶ç»éªæ§å°éªè¯äºâåé£æ ¼è¿ç§»ï¼åèº«ä»½ä¿æâçå¤çé¡ºåºä¼äºååé¡ºåºï¼è½ææåå°çæè¿ç¨ä¸­å±æ§çæ¼ç§»ï¼è¿ä¸ºå¤ç®æ çæä»»å¡æä¾äºä¸ä¸ªéè¦çè®¾è®¡ååã</li>
<li><strong>äººè¸ä¸è´èªæ³¨æåæºå¶ä¸æ¾å¼èº«ä»½åµå¥ï¼</strong> ä¸ºäºå¨é£æ ¼ååå¼ºå¶ä¿æèº«ä»½ï¼CraftGraffiti å¨æ©æ£æ¨¡åçæ³¨æåå±ä¸­å¼å¥äºä¸ç§æ°é¢çäººè¸ä¸è´èªæ³¨æåæºå¶ï¼å¹¶è¾ä»¥æ¾å¼èº«ä»½åµå¥ãè¿ç§æºå¶è½å¤å¼å¯¼æ¨¡åå¨çæè¿ç¨ä¸­æ´å³æ³¨å¹¶ä¿çå³é®çé¢é¨ç¹å¾ã</li>
<li><strong>æ å³é®ç¹çå§¿æå®å¶ï¼</strong> ä¼ ç»çå§¿ææ§å¶éå¸¸ä¾èµäºå³é®ç¹æ£æµãè¯¥æ¹æ³éè¿ä½¿ç¨ CLIP å¼å¯¼çæç¤ºæ©å±æ¥å®ç°å¨æçå§¿æéæ°å®ä½ï¼åæ¶ä¿æé¢é¨è¿è´¯æ§ï¼é¿åäºå¯¹æ¾å¼å³é®ç¹æ æ³¨çä¾èµï¼æé«äºçµæ´»æ§ã</li>
<li><strong>LoRA å¾®è°çæ©æ£æ¨¡åï¼</strong> å©ç¨ LoRA (Low-Rank Adaptation) å¯¹é¢è®­ç»çæ©æ£ Transformer è¿è¡å¾®è°ï¼ä»¥é«æå°å®ç°æ¶é¸¦é£æ ¼è¿ç§»ï¼è¿æ¯ä¸ç§å¨ä¿ææ¨¡åå¤§é¨ååæ°ä¸åçæåµä¸ï¼ææéåºç¹å®é£æ ¼çç­ç¥ã</li>
</ul>
<h3 id="3_2">3. å¯¹é¢åçæ½å¨å½±å</h3>
<ul>
<li><strong>æ¨å¨èº«ä»½ä¿æçæææ¯ï¼</strong> è¯¥ç ç©¶ä¸ºå¨æç«¯é£æ ¼ååºæ¯ä¸ä¿æäººè¸èº«ä»½æä¾äºä¸ä¸ªååæ§çæ¹æ³ï¼è¿å¯¹äºçæå¼AIé¢åæ¯ä¸ä¸ªé¿æå­å¨çææãå®ä¸ºæªæ¥å¼åæ´é²æ£ãæ´å·èº«ä»½æè¯ççææ¨¡åå¥ å®äºåºç¡ã</li>
<li><strong>èµè½âèº«ä»½å°éåAIèºæ¯âï¼</strong> è®ºææç¡®æåºäºâèº«ä»½å°éåAIè¾å©èºæ¯âçç®æ ï¼è¿å¨AIèºæ¯åä½ä¸­å·æéè¦çä¼¦çåå®éæä¹ãå®ä½¿å¾ç¨æ·è½å¤å¨äº«åèºæ¯èªç±çåæ¶ï¼ç¡®ä¿å¶ä¸ªäººèº«ä»½ï¼å°¤å¶æ¯é¢é¨ç¹å¾ï¼å¾å°åç¡®çä¿çï¼ä»èå¢å¼ºç¨æ·ä½éªåä¿¡ä»»ã</li>
<li><strong>å¯åå¤ç®æ çæä»»å¡è®¾è®¡ï¼</strong> âåé£æ ¼ï¼åèº«ä»½âçèå¼éªè¯ï¼ä¸ºå¶ä»éè¦å¹³è¡¡å¤ä¸ªç¸äºå²çªççæç®æ ï¼å¦é£æ ¼ãåå®¹ãèº«ä»½ãå§¿æç­ï¼çä»»å¡æä¾äºæä»·å¼çæå¯¼ã</li>
<li><strong>æ©å±æ©æ£æ¨¡åçåºç¨ï¼</strong> éè¿ç»å LoRAãèªå®ä¹æ³¨æåæºå¶å CLIP å¼å¯¼ï¼å±ç¤ºäºæ©æ£æ¨¡åå¨å¤æãé«ä¿ççæä»»å¡ä¸­çå¼ºå¤§æ½ååçµæ´»æ§ã</li>
</ul>
<h3 id="4_2">4. ç¸å³é¢åæåºç¨</h3>
<p>è¿é¡¹ç ç©¶çææå¯ä»¥å¹¿æ³åºç¨äºä»¥ä¸é¢åï¼</p>
<ul>
<li><strong>ä¸ªæ§ååå®¹åä½ï¼</strong> ç¨æ·å¯ä»¥çæé«åº¦é£æ ¼åä½ä»è½è¯å«åºèªå·±é¢é¨çå¤´åãç¤¾äº¤åªä½å¾çãæ°å­èºæ¯ä½åç­ã</li>
<li><strong>èæè¯ç©¿ä¸æ°å­æ¶å°ï¼</strong> å¨èæç¯å¢ä¸­å±ç¤ºæè£æéé¥°æ¶ï¼ç¡®ä¿æ¨¡ç¹çè¸é¨ç¹å¾ä¿æä¸è´ï¼å¢å¼ºçå®æåä¸ªæ§åä½éªã</li>
<li><strong>æ¸¸æä¸å¨ç»è§è²è®¾è®¡ï¼</strong> ä»çå®äººç©ç§ççæé£æ ¼åçæ¸¸ææå¨ç»è§è²ï¼åæ¶ä¿çå¶æ ¸å¿é¢é¨ç¹å¾ã</li>
<li><strong>èºæ¯æ»¤éä¸ç¹æï¼</strong> ä¸ºç§çåè§é¢åºç¨åç§èºæ¯é£æ ¼æ»¤éï¼åæ¶ç¡®ä¿äººè¸çè¯å«åº¦ï¼æåç¨æ·ä½éªã</li>
<li><strong>æ°å­èº«ä»½ä¸åå®å®ï¼</strong> å¨åå®å®ä¸­åå»ºé«åº¦ä¸ªæ§åä¸å·æè¾¨è¯åº¦çæ°å­åèº«æèæå½¢è±¡ã</li>
<li><strong>é¢é¨ä¿®å¤ä¸å¢å¼ºï¼</strong> å¨å¯¹èæ§ç§çè¿è¡é£æ ¼åä¿®å¤æå¢å¼ºæ¶ï¼ç¡®ä¿é¢é¨èº«ä»½çåç¡®æ§ã</li>
</ul>
<h3 id="5_2">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</h3>
<p>å°½ç®¡è®ºæå±ç¤ºäºæ¾èçè¿æ­¥ï¼ä½ä»æè¦ä¸­ä»å¯æ¨æ­åºä¸äºæ½å¨çå±éæ§ï¼</p>
<ul>
<li><strong>é£æ ¼æ³åæ§ï¼</strong> è®ºææç¡®èç¦äºâæ¶é¸¦âè¿ç§âé«å¯¹æ¯åº¦ãæ½è±¡âçåªä»ãå¶æåºçæ¹æ³å¨å¶ä»æç«¯æéæç«¯é£æ ¼ï¼å¦æ²¹ç»ãæ°´å½©ãå¡éç­ï¼ä¸çè¡¨ç°å¦ä½ï¼ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>èº«ä»½ä¿æçèå´ï¼</strong> æè¦å¼ºè°çæ¯âé¢é¨èº«ä»½âãâç¼çãé¼»å­æå´å·´âçä¿çãè¿è¡¨æè¯¥æ¡æ¶å¯è½ä¸»è¦å³æ³¨é¢é¨ç¹å¾ï¼å¯¹äºå¨èº«èº«ä»½ãèº«ä½å§¿æçç»å¾®ç¹å¾æéé¢é¨å±æ§çä¿æè½åå¯è½æéã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> æ©æ£æ¨¡åï¼ç¹å«æ¯ç»è¿ LoRA å¾®è°å¹¶å¢å äºèªå®ä¹æ³¨æåæºå¶çæ¨¡åï¼éå¸¸è®¡ç®ææ¬è¾é«ï¼å¯è½å¯¹å®æ¶åºç¨æå¤§è§æ¨¡é¨ç½²ææææã</li>
<li><strong>âå±æ§æ¼ç§»âçç¨åº¦ï¼</strong> å°½ç®¡âåé£æ ¼ï¼åèº«ä»½âèå¼âåå°äºå±æ§æ¼ç§»âï¼ä½è¿å¹¶ä¸æå³çå®å¨æ¶é¤äºæ¼ç§»ãå¨æäºæç«¯æåµä¸ï¼é¢é¨ç¹å¾çç»å¾®ä¹å¤ä»å¯è½åå°å½±åï¼å¯¼è´è¯å«åº¦ä¸éã</li>
<li><strong>å¯¹é¢è®­ç»æ¨¡åå CLIP çä¾èµï¼</strong> æ¨¡åçæ§è½å¨ä¸å®ç¨åº¦ä¸ä¾èµäºæä½¿ç¨çé¢è®­ç»æ©æ£æ¨¡åå CLIP æ¨¡åçè´¨éåæ½å¨åå·®ãè¿äºåºç¡æ¨¡åçå±éæ§å¯è½ä¼ä¼ éå° CraftGraffiti çè¾åºä¸­ã</li>
<li><strong>âç«äºæ§âç»æçå«ä¹ï¼</strong> æè¦ä¸­æå°âå®éç»æå±ç¤ºäºç«äºæ§çäººè¸ç¹å¾ä¸è´æ§âï¼è¿éå¸¸æå³çè¡¨ç°è¯å¥½ï¼ä½å¯è½å¹¶éå®ç¾æ çãå¨æäºç¹å®æå¤æçäººè¸è¡¨æãåç§æé®æ¡æ¡ä»¶ä¸ï¼å¶é²æ£æ§ä»æå¾æ·±å¥æ¢è®¨ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.20640v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.20640v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.20909v1'></a></p>
<h2 id="dino-u-net-exploiting-high-fidelity-dense-features-from-foundation-models-for-medical-image-segmentation"><a href="https://arxiv.org/abs/2508.20909v1">Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</a></h2>
<p><strong>Authors:</strong> Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao</p>
<p><strong>Published:</strong> 2025-08-28</p>
<p><strong>Categories:</strong> cs.CV, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="dino-u-net-exploiting-high-fidelity-dense-features-from-foundation-models-for-medical-image-segmentation_1">Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (2-3 å¥è¯)</strong></p>
<p>æ¬ææåºäºDino U-Netï¼ä¸ä¸ªæ°é¢çç¼ç å¨-è§£ç å¨æ¶æï¼æ¨å¨å©ç¨DINOv3è§è§åºç¡æ¨¡åçé«ä¿çå¯éç¹å¾è¿è¡å»å­¦å¾ååå²ãå®éè¿ä¸ä¸ªåºäºå»ç»DINOv3éª¨å¹²çç¼ç å¨ï¼ç»åä¸é¨çééå¨åæ°é¢çä¿çåº¦æç¥æå½±æ¨¡åï¼FAPMï¼ï¼ææå°å°è¯­ä¹ç¹å¾ä¸ç©ºé´ç»èèåï¼å¹¶ä¿æç¹å¾è´¨éãå®éªè¯æï¼Dino U-Netå¨å¤ä¸ªå»å­¦æ°æ®éä¸å®ç°äºæåè¿çæ§è½ï¼å¹¶å±ç°åºè¯å¥½çå¯æ©å±æ§ï¼ä¸ºå»å­¦å¾ååå²æä¾äºä¸ç§åæ°é«æä¸é«æ§è½çè§£å³æ¹æ¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>Dino U-Netçæ ¸å¿åæ°å¨äºå¶ç¬ç¹çç¼ç å¨-è§£ç å¨æ¶æï¼ç¹å«æ¯å¦ä½é«æä¸é«è´¨éå°å©ç¨DINOv3åºç¡æ¨¡åçå¯éç¹å¾è¿è¡åç´ çº§é¢æµãå·ä½åæ¬ï¼</p>
<ul>
<li><strong>åºäºå»ç»DINOv3éª¨å¹²çç¼ç å¨ä¸ä¸é¨ééå¨ï¼</strong> è®ºæå©ç¨ä¸ä¸ªå»ç»çDINOv3éª¨å¹²ä½ä¸ºç¼ç å¨ï¼è¿æå¤§å°æé«äºåæ°æçãå¨æ­¤åºç¡ä¸ï¼å¼å¥äºä¸ä¸ªâä¸é¨ééå¨âæ¥èåDINOv3æåçä¸°å¯è¯­ä¹ç¹å¾ä¸ä½å±ç©ºé´ç»èãè¿å¯¹äºå»å­¦å¾ååå²è³å³éè¦ï¼å ä¸ºåå²ä»»å¡æ¢éè¦é«å±è¯­ä¹çè§£ï¼ä¹éè¦ç²¾ç¡®çè¾¹çåå±é¨ç»èã</li>
<li><strong>ä¿çåº¦æç¥æå½±æ¨¡åï¼FAPMï¼ï¼</strong> ä¸ºäºå¨å°é«ç»´ç¹å¾æå½±å°è§£ç å¨æéç»´åº¦æ¶ï¼æå¤§éåº¦å°ä¿çç¹å¾çè´¨éåä¿¡æ¯ï¼è®ºæè®¾è®¡äºä¸ä¸ªæ°çâä¿çåº¦æç¥æå½±æ¨¡åï¼FAPMï¼âãè¿ä¸ªæ¨¡åè½å¤ææå°ç²¾ç¼åæå½±ç¹å¾ï¼é¿åå¨éç»´è¿ç¨ä¸­ä¸¢å¤±å³é®ä¿¡æ¯ï¼ä»èç¡®ä¿è§£ç å¨è½å¤æ¥æ¶å°é«è´¨éçè¡¨ç¤ºã</li>
<li><strong>å©ç¨éç¨åºç¡æ¨¡åçå¯éé¢è®­ç»ç¹å¾ï¼</strong> è®ºæå¼ºè°äºå©ç¨DINOv3è¿ç§å¨æµ·éèªç¶å¾åä¸è¿è¡âå¯éé¢è®­ç»âçåºç¡æ¨¡åæå¸¦æ¥çä¼å¿ãDINOv3éè¿èªçç£å­¦ä¹ ï¼è½å¤å­¦ä¹ å°éå¸¸ä¸°å¯çãå¯¹å¾åå±é¨ç»æåå¨å±è¯­ä¹é½ææçç¹å¾ï¼è¿äºç¹å¾è¢«è¯æå¯ä»¥ææå°è¿ç§»å°å»å­¦é¢åã</li>
</ul>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<p>æ¬ç ç©¶çæ½å¨å½±åæ¯å¤æ¹é¢çï¼</p>
<ul>
<li><strong>æåå»å­¦å¾ååå²ç²¾åº¦ï¼</strong> Dino U-NetçSOTAæ§è½å°ä¸ºå»å­¦å¾ååå²æ ç«æ°çæ æï¼æææ¨å¨è¯¥é¢åå¨ä¸´åºè¯æ­åæ²»çè§åä¸­çåºç¨ã</li>
<li><strong>åæ°é«æçèå¼ï¼</strong> éè¿å»ç»å¤§ååºç¡æ¨¡åéª¨å¹²å¹¶å¼å¥è½»éçº§ééå¨ï¼è¯¥æ¹æ³æä¾äºä¸ç§åæ°é«æçè§£å³æ¹æ¡ãè¿å¯¹äºå»å­¦é¢åå°¤ä¸ºéè¦ï¼å ä¸ºå»å­¦æ°æ®ééå¸¸è§æ¨¡æéï¼ä¸è®­ç»æ°åäº¿åæ°çæ¨¡åææ¬é«æãè¿ç§èå¼ä½¿å¾å¨èµæºåéçç¯å¢ä¸ä¹è½ååå©ç¨å¤§åæ¨¡åçå¼ºå¤§è½åã</li>
<li><strong>éªè¯åºç¡æ¨¡åå¨ä¸ä¸é¢åçæ³åè½åï¼</strong> æ¬æè¿ä¸æ­¥éªè¯äºå¨éç¨èªç¶å¾åä¸é¢è®­ç»çåºç¡æ¨¡åï¼å¦DINOv3ï¼å¨é«åº¦ä¸ä¸åé¢åï¼å¦å»å­¦å¾ååæï¼çå¼ºå¤§è¿ç§»åæ³åè½åï¼å°¤å¶æ¯å¨éè¦ç²¾ç»åç´ çº§çè§£çå¯éé¢æµä»»å¡ä¸ã</li>
<li><strong>å éå»å­¦AIç åï¼</strong> æä¾äºä¸ä¸ªé«æ§è½ä¸æäºä½¿ç¨çæ¡æ¶ï¼ä»£ç å·²å¼æºï¼ï¼ææå éç ç©¶äººåå¨å»å­¦å¾ååå²é¢åçæ¢ç´¢ååæ°ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å¶ä»å»å­¦å¾ååæä»»å¡ï¼</strong> å¦çç¶æ£æµãå¾åéåãç¾çåç±»ï¼ç¹å«æ¯éè¦ç²¾ç¡®å®ä½çç¶çï¼ã3Déå»ºç­ï¼è¿äºä»»å¡åæ ·éè¦å¼ºå¤§çç¹å¾æåååç´ çº§çè§£è½åã</li>
<li><strong>å¶ä»éè¦ç²¾ç»åç´ çº§çè§£çä¸ä¸é¢åï¼</strong> å¦å·¥ä¸ç¼ºé·æ£æµãé¥æå¾ååæãçç©æ¾å¾®å¾åå¤çç­ï¼è¿äºé¢ååæ ·é¢ä¸´æ°æ®ç¨ç¼ºåå¯¹ç»èææçææï¼å¯ä»¥åé´Dino U-Netå©ç¨åºç¡æ¨¡åç¹å¾çç­ç¥ã</li>
<li><strong>å°æ ·æ¬ï¼few-shotï¼æé¶æ ·æ¬ï¼zero-shotï¼å­¦ä¹ ï¼</strong> DINOv3çå¼ºå¤§ç¹å¾æåè½åå¯è½ä¸ºè¿äºåºæ¯æä¾æ´å¼ºçæ³ååºç¡ï¼å°¤å¶æ¯å¨å»å­¦é¢åï¼æ°ç¾çæç½è§ç¾ççæ°æ®å¾å¾éå¸¸ç¨ç¼ºã</li>
<li><strong>æ¨¡ååç¼©ä¸é¨ç½²ï¼</strong> å»ç»éª¨å¹²çç­ç¥æå©äºå¨ä¿æé«æ§è½çåæ¶ï¼éä½æ¨¡åè®­ç»åå¾®è°çè®¡ç®ææ¬ï¼ä¸ºæªæ¥å¨è¾¹ç¼è®¾å¤æè®¡ç®èµæºæéçä¸´åºç¯å¢ä¸­é¨ç½²é«æ§è½æ¨¡åæä¾æè·¯ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>å¯¹DINOv3éª¨å¹²çä¾èµæ§ï¼</strong> å°½ç®¡å»ç»éª¨å¹²æé«äºåæ°æçï¼ä½DINOv3æ¬èº«æ¯å¨èªç¶å¾åä¸é¢è®­ç»çãè½ç¶ééå¨æ¨å¨å¼¥è¡¥ï¼ä½å¶ç¹å¾è¡¨ç¤ºå¯è½æ æ³å®å¨ææå»å­¦å¾åç¹æçãä¸èªç¶å¾åå·®å¼æå¤§çç»å¾®ççç¹å¾ï¼å°¤å¶æ¯å¨æäºä¸èªç¶å¾åè§è§ç¹æ§å·®å¼å·¨å¤§çå»å­¦æ¨¡æä¸ã</li>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> å³ä½¿éª¨å¹²å»ç»ï¼æè¦ä¸­æåçâ70äº¿åæ°âæ¨¡åå¨æ¨çé¶æ®µä»å¯è½éè¦æ¾èçè®¡ç®èµæºï¼è¿å¯¹äºå®æ¶æ§è¦æ±é«æèµæºåéçä¸´åºé¨ç½²ç¯å¢å¯è½æ¯ä¸ä¸ªææã</li>
<li><strong>å¯è§£éæ§ï¼</strong> åºç¡æ¨¡åéå¸¸è¢«è§ä¸ºé»ç®±ï¼å¼å¥é¢å¤çééå¨åæå½±æ¨¡åå¯è½ä¼è¿ä¸æ­¥å¢å æ¨¡åå³ç­è¿ç¨çå¤ææ§ï¼éä½å¶å¨ä¸´åºåºç¨ä¸­è³å³éè¦çå¯è§£éæ§ã</li>
<li><strong>ééå¨åFAPMçè®­ç»æ°æ®éæ±ï¼</strong> å°½ç®¡éª¨å¹²æ¯å»ç»çï¼ä½ééå¨åFAPMä»éè¦éå¯¹ç¹å®ä»»å¡è¿è¡è®­ç»ãæè¦ä¸­æªæåè¿äºæ¨¡åå¯¹è®­ç»æ°æ®éçæææ§ï¼è¿å¯è½å¨æåº¦ç¨ç¼ºçå»å­¦æ°æ®éä¸ææææã</li>
<li><strong>æ³åå°æªè§è¿çå»å­¦æ¨¡ææç½è§çåï¼</strong> å°½ç®¡å¨âä¸ä¸ªå¤æ ·åçå¬å±å»å­¦å¾ååå²æ°æ®éâä¸è¿è¡äºå®éªï¼ä½è¿äºæ°æ®éå¯è½æ æ³å®å¨è¦çææå»å­¦æ¨¡ææç½è§çåçå¤ææ§ãæ¨¡åå¨é¢å¯¹ä¸è®­ç»æ°æ®åå¸å·®å¼è¾å¤§çæ°æ¨¡ææçåæ¶çé²æ£æ§ä»éè¿ä¸æ­¥éªè¯ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model.</li>
<li>To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder.</li>
<li>Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.20909v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.20909v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.20265v1'></a></p>
<h2 id="plug-in-feedback-self-adaptive-attention-in-clip-for-training-free-open-vocabulary-segmentation"><a href="https://arxiv.org/abs/2508.20265v1">Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation</a></h2>
<p><strong>Authors:</strong> Zhixiang Chi, Yanan Wu, Li Gu, Huan Liu, Ziqiang Wang, Yang Zhang, Yang Wang, Konstantinos N. Plataniotis</p>
<p><strong>Published:</strong> 2025-08-27</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææè¦æåºäºä¸ç§éå¯¹CLIPæ¨¡åå¨å¼æ¾è¯æ±åå²ï¼Open-Vocabulary Segmentationï¼ä»»å¡ä¸­å®ä½è½åä¸è¶³çåæ°è§£å³æ¹æ¡ãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-concise-summary_2">1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (Concise Summary)</h3>
<p>è¿ç¯è®ºææåºäºä¸ç§æ éè®­ç»ï¼training-freeï¼çåé¦é©±å¨èªéåºæ¡æ¶ï¼æ¨å¨è§£å³CLIPæ¨¡åå¨å¼æ¾è¯æ±åå²ä¸­å®ä½è½åå·®çé®é¢ãå®éè¿å°æ¨¡åæç»è¾åºå±é¢çè¡¥ä¸çº§ï¼patch-levelï¼è¯­ä¹ä¸è´æ§åé¦åä¸­é´æ³¨æåæºå¶ï¼ä»èå¢å¼ºåé¨è¡¨ç¤ºä¸æç»é¢æµä¹é´çè¯­ä¹ä¸è´æ§ãä½ä¸ºä¸ä¸ªå³æå³ç¨ï¼plug-inï¼æ¨¡åï¼è¯¥æ¹æ³è½æ¾èæåç°æSOTAæ¨¡åå¨å¤ä¸ªåºåæµè¯ä¸çæ§è½ã</p>
<h3 id="2-key-innovation-or-methodological-approach_2">2. å³é®åæ°ææ¹æ³è®º (Key Innovation or Methodological Approach)</h3>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶<strong>âè¾åºå°ä¸­é´æ³¨æåâçåé¦é©±å¨èªéåºæºå¶</strong>ã
1.  <strong>åé¦åè·¯è®¾è®¡ï¼</strong> ä¼ ç»æ¹æ³éå¸¸ç´æ¥ä¿®æ¹ä¸­é´æ³¨æåï¼ä½å­å¨è¯­ä¹ä¼ æ­ä¸ä¸è´åç¼ºä¹ä¸ææ¬è¡¨ç¤ºç´æ¥äº¤äºçé®é¢ãæ¬æååæ°æ§å°å©ç¨æ¨¡åç<strong>æç»è¾åºé¢æµ</strong>ä½ä¸ºâæ´å¼ºçç©ºé´ä¸è´æ§åéªâï¼stronger spatial coherence priorï¼ãè¿äºè¾åºè¢«è®¤ä¸ºæ¯æ¨¡åå¤ççâé«æ½®âï¼å°è£äºæå¨é¢çè§è§åææ¬è¯­ä¹ã
2.  <strong>èªéåºè°æ´ï¼</strong> å°è¿äºè¾åºå±é¢çè¡¥ä¸çº§å¯¹åºå³ç³»ï¼patch-level correspondencesï¼åé¦å¹¶èªéåºå°è°æ´ä¸­é´æ³¨æåï¼ä»èå¨åé¨è¡¨ç¤ºåæç»é¢æµä¹é´å»ºç«æ´å¼ºçè¯­ä¹ä¸è´æ§ã
3.  <strong>è®­ç»æ å³æ§ä¸å³æå³ç¨ï¼</strong> æ´ä¸ªæ¡æ¶æ¯âè®­ç»æ å³âï¼training-freeï¼çï¼è¿æå³çæ ééæ°è®­ç»CLIPæ¨¡åï¼å¯ä»¥ç´æ¥ä½ä¸ºâå³æå³ç¨âæ¨¡åéæå°ç°ææ¹æ³ä¸­ï¼å¤§å¤§éä½äºåºç¨ææ¬åå¤ææ§ã
4.  <strong>æ ¸å¿æ¨¡åï¼</strong> ä¸ºå®ç°è¿ä¸åé¦æºå¶ï¼è®ºæè®¾è®¡äºå³é®æ¨¡åï¼åæ¬<strong>æ³¨æåéç¦»ï¼attention isolationï¼</strong>ãç¨äºç¨çèªéåºç<strong>åºäºç½®ä¿¡åº¦çåªæï¼confidence-based pruning for sparse adaptationï¼</strong>ä»¥å<strong>èªéåºéæï¼adaptation ensembleï¼</strong>ã</p>
<h3 id="3-potential-impact-on-the-field_2">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<ol>
<li><strong>æåCLIPå¨ä¸æ¸¸ä»»å¡ä¸­çå®ç¨æ§ï¼</strong> æ¾èå¢å¼ºäºCLIPå¨å¼æ¾è¯æ±åå²è¿ä¸å³é®ä¸æ¸¸ä»»å¡ä¸­çå®ä½è½åï¼ä½¿å¶å¨éè¦ç²¾ç»è§è§çè§£çåºç¨ä¸­æ´å·ç«äºåã</li>
<li><strong>å¼åæ°çæ¨¡åæ¹è¿èå¼ï¼</strong> âè¾åºå°åé¨è¡¨ç¤ºâçåé¦æºå¶å¯è½ä¸ºæ¹è¿å¶ä»å¤§åé¢è®­ç»æ¨¡åï¼ä¸ä»éäºCLIPï¼å¨ç¹å®ä»»å¡ä¸çè¡¨ç°æä¾æ°çæè·¯ï¼å°¤å¶æ¯å¨æ ééæ°è®­ç»çåºæ¯ä¸ã</li>
<li><strong>éä½ç ç©¶ååºç¨ææ¬ï¼</strong> âè®­ç»æ å³âåâå³æå³ç¨âçç¹æ§æå³çç ç©¶äººååå¼åèå¯ä»¥ä»¥è¾ä½çè®¡ç®ææ¬åæ¶é´æå¥ï¼å¿«éæåç°ææ¨¡åçæ§è½ï¼å éäºææ¯ä»ç ç©¶å°åºç¨çè½¬åã</li>
<li><strong>æ¨å¨å¼æ¾è¯æ±å­¦ä¹ åå±ï¼</strong> æé«äºæ¨¡åå¤çæªç¥ç±»å«å¯¹è±¡çè½åï¼å¯¹äºé¶æ ·æ¬ï¼zero-shotï¼åå°æ ·æ¬ï¼few-shotï¼å­¦ä¹ ç­é¢åå·æéè¦æä¹ã</li>
</ol>
<h3 id="4-related-areas-or-applications_1">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ol>
<li><strong>å¼æ¾è¯æ±åå²ä¸æ£æµï¼</strong> ç´æ¥åçé¢åï¼è½å¤æ´åç¡®å°è¯å«ååå²å¾åä¸­ä»»æææ¬æè¿°çå¯¹è±¡ã</li>
<li><strong>é¶æ ·æ¬/å°æ ·æ¬å­¦ä¹ ï¼</strong> å¢å¼ºäºæ¨¡åå¨é¢å¯¹æªè§è¿ç±»å«æ¶çæ³åè½ååå®ä½ç²¾åº¦ã</li>
<li><strong>å¾åç¼è¾ä¸åå®¹çæï¼</strong> åè®¸ç¨æ·éè¿ææ¬æä»¤æ´ç²¾ç¡®å°éæ©åä¿®æ¹å¾åä¸­çç¹å®åºåã</li>
<li><strong>æºå¨äººä¸èªå¨é©¾é©¶ï¼</strong> æåäºç³»ç»å¯¹ç¯å¢ä¸­æªç¥ç©ä½æåºæ¯ççè§£åäº¤äºè½åï¼ä¾å¦è¯å«åé¿å¼æ°åºç°çéç¢ç©ã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> è¾å©å»çæ ¹æ®ææ¬æè¿°ï¼æ´åç¡®å°å®ä½ååå²çååºåæç¹å®ç»ç»ç»æã</li>
<li><strong>è§è§é®ç­ï¼VQAï¼ä¸å¾åæ£ç´¢ï¼</strong> æ´ç²¾ç»çè§è§-ææ¬å¯¹é½æå©äºæåå¯¹å¤æé®é¢ççè§£åæ´åç¡®çå¾ååå®¹æ£ç´¢ã</li>
</ol>
<h3 id="5-limitations-inferred-from-the-abstract_1">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferred from the Abstract)</h3>
<ol>
<li><strong>æ¨çæ¶é´å¼éï¼</strong> å°½ç®¡æ¯âè®­ç»æ å³âä¸ä½¿ç¨äºâç¨çèªéåºâï¼ä½å¼å¥åé¦å¾ªç¯åé¢å¤çæ¨¡åï¼æ³¨æåéç¦»ãåªæãéæï¼å¯è½ä¼å¢å æ¨¡åçæ¨çæ¶é´ï¼inference latencyï¼ï¼è¿å¯¹äºå®æ¶åºç¨å¯è½æ¯ä¸ä¸ªèéã</li>
<li><strong>å¯¹åå§è¾åºè´¨éçä¾èµï¼</strong> è¯¥æ¹æ³çæææ§å¨ä¸å®ç¨åº¦ä¸ä¾èµäºæ¨¡ååå§è¾åºé¢æµçè´¨éãå¦æåå§è¾åºéå¸¸å·®ï¼åé¦æºå¶è½å¦ææå¼å¯¼æ¹è¿å¯è½å­å¨çé®ãæè¦ä¸­æå°è¾åºâå°è£äºæå¨é¢çè¯­ä¹âï¼æç¤ºå¶è´¨éè¶³å¤é«ï¼ä½æç«¯æåµä»ééªè¯ã</li>
<li><strong>æ¶æç¹å¼æ§ï¼</strong> è¯¥æ¹æ³æ¯ä¸ºCLIPçTransformeræ¶æåå¶æ³¨æåæºå¶è®¾è®¡çãå¶å¨å¶ä»éTransformeræä¸åæ³¨æåæºå¶çè§è§æ¨¡åä¸çæ®éæ§ææææ§æªå¨æè¦ä¸­æåã</li>
<li><strong>âæ ç¼éæâçå®éå¤ææ§ï¼</strong> å°½ç®¡å£°ç§°âæ ç¼éæâï¼ä½å¨å®éé¨ç½²ä¸­ï¼ä»»ä½å³æå³ç¨æ¨¡åé½å¯è½éè¦ä¸å®çå·¥ç¨ééåè°è¯ï¼å°¤å¶æ¯å¨å¤çå¤ç§SOTAæ¹æ³åä¸åéª¨å¹²ç½ç»æ¶ã</li>
</ol>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention.</li>
<li>Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior.</li>
<li>Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H).</li>
<li>Our approach consistently
improves their performance across eight benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.20265v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.20265v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.20089v1'></a></p>
<h2 id="bridging-domain-gaps-for-fine-grained-moth-classification-through-expert-informed-adaptation-and-foundation-model-priors"><a href="https://arxiv.org/abs/2508.20089v1">Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</a></h2>
<p><strong>Authors:</strong> Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas HÃ¸ye</p>
<p><strong>Published:</strong> 2025-08-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæçæè¦å±ç¤ºäºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå¨è§£å³å®éçç©å¤æ ·æ§çæµæææ¹é¢çææ°è¿å±ãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-2-3_2">1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</h3>
<p>è¯¥è®ºææåºäºä¸ç§è½»éçº§åç±»æ¹æ³ï¼éè¿å°é«æ§è½çBioCLIP2åºç¡æ¨¡åç¥è¯è¸é¦å°ConvNeXt-tinyæ¶æä¸­ï¼å¹¶ç»åæéçä¸å®¶æ æ³¨éå¤æ°æ®ï¼ä»¥è§£å³èªå¨ç¸æºç³»ç»æè·çé£è¾å¾åå¨ç²¾ç»ç²åº¦åç±»ä¸­å­å¨çé¢åæ¼ç§»é®é¢ãå®éªè¯æï¼è¯¥æ¹æ³å¨æ¾èéä½è®¡ç®ææ¬çåæ¶ï¼å®ç°äºä¸BioCLIP2ç¸å½çåç±»ç²¾åº¦ï¼ä¸ºé«æçæè«çæµç³»ç»åè·¨é¢åç²¾ç»ç²åº¦åç±»æä¾äºå®ç¨æå¯¼ã</p>
<h3 id="2_3">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</h3>
<p>è¯¥ç ç©¶çå³é®åæ°å¨äºå¶<strong>ååç»å</strong>çæ¹æ³è®ºï¼ææå°èåäºä»¥ä¸å ä¸ªæ ¸å¿ææ³ï¼</p>
<ul>
<li><strong>å©ç¨å¼ºå¤§çåºç¡æ¨¡ååéªç¥è¯ (Foundation Model Priors):</strong> å¼å¥äºé«æ§è½çBioCLIP2åºç¡æ¨¡åä½ä¸ºâæå¸âæ¨¡åï¼è¯¥æ¨¡åå¯è½å¨å¤§è§æ¨¡çç©å¾ååææ¬æ°æ®ä¸è¿è¡äºé¢è®­ç»ï¼ä»èå·å¤äºå¼ºå¤§çç¹å¾æååæ³åè½åã</li>
<li><strong>ç¥è¯è¸é¦ (Knowledge Distillation) å®ç°æ¨¡åè½»éå:</strong> å°BioCLIP2çä¸°å¯ç¥è¯è¸é¦å°ä¸ä¸ªè®¡ç®ææ¬æ¾èæ´ä½çConvNeXt-tinyâå­¦çâæ¨¡åä¸­ãè¿ä½¿å¾æ¨¡åè½å¤å¨ä¿æé«ç²¾åº¦çåæ¶ï¼å¤§å¹åå°è®¡ç®èµæºéæ±ï¼éç¨äºèµæºåéçé¨ç½²ç¯å¢ã</li>
<li><strong>ä¸å®¶ä¿¡æ¯å¼å¯¼çé¢åéåº (Expert-Informed Adaptation):</strong> ç»åâæéçä¸å®¶æ æ³¨éå¤æ°æ®âæ¥å¾®è°ææå¯¼è¸é¦è¿ç¨ï¼ç´æ¥è§£å³ç­å±å¾åä¸åæéå¤å¾åä¹é´çé¢åæ¼ç§»é®é¢ãè¿ç§æéå¯¹æ§çæ°æ®å©ç¨æ¯å¼¥åé¢åå·®è·çå³é®ã</li>
<li><strong>éå¯¹ç²¾ç»ç²åº¦åç±»çå®ç¨è§£å³æ¹æ¡:</strong> éå¯¹é£è¾è¿ç§å·æé«åº¦ç¸ä¼¼ç©ç§çç²¾ç»ç²åº¦åç±»ä»»å¡ï¼æä¾äºä¸ä¸ªå¨çå®ä¸çãåææ°æ®åºæ¯ä¸æ¢åç¡®åé«æçè§£å³æ¹æ¡ã</li>
</ul>
<h3 id="3_3">3. å¯¹é¢åæ½å¨å½±å</h3>
<ul>
<li><strong>æ¨å¨é«æãå¯é¨ç½²çCVç³»ç»åå±:</strong> è¯æäºéè¿åºç¡æ¨¡åè¸é¦å¯ä»¥ææå°å°å¤§åæ¨¡åçæ§è½ä¼å¿è½¬ç§»å°å°åãé«æçæ¨¡åä¸ï¼è¿å¯¹äºè¾¹ç¼è®¡ç®ãå®æ¶çæµåèµæºåéçåºç¨åºæ¯å·æéè¦æä¹ã</li>
<li><strong>ä¸ºé¢åéåºæä¾æ°èå¼:</strong> å¼ºè°äºç»ååºç¡æ¨¡ååéªç¥è¯åå°éç®æ é¢åä¸å®¶æ°æ®è¿è¡ç¥è¯è¸é¦ï¼æ¯è§£å³é¢åæ¼ç§»é®é¢çä¸ç§å¼ºå¤§ä¸å®ç¨çç­ç¥ï¼å°¤å¶éç¨äºæ°æ®æ æ³¨ææ¬é«æçä¸ä¸é¢åã</li>
<li><strong>å éçç©å¤æ ·æ§çæµåçæç ç©¶:</strong> ä¸ºèªå¨åæè«ï¼åå¶ä»çç©ï¼è¯å«ç³»ç»æä¾äºææ¯æ¯æï¼æå©äºæ´åç¡®ãæ´é«æå°çæµç©ç§æ°éååå¸ï¼ä»èæ´å¥½å°çè§£ååºå¯¹çç©å¤æ ·æ§ä¸éé®é¢ã</li>
<li><strong>éªè¯åºç¡æ¨¡åå¨ç¹å®é¢åçä»·å¼:</strong> è¿ä¸æ­¥å·©åºäºåCLIPè¿ç±»å¤æ¨¡æåºç¡æ¨¡åå¨ç¹å®ãä¸ä¸é¢åï¼å¦çç©å¾ååæï¼çå¼ºå¤§è¿ç§»å­¦ä¹ è½ååä½ä¸ºå¼ºå¤§ç¹å¾æåå¨çæ½åã</li>
</ul>
<h3 id="4_3">4. å¯è½åçäºæ­¤ç ç©¶çç¸å³é¢åæåºç¨</h3>
<ul>
<li><strong>å¶ä»ç²¾ç»ç²åº¦åç±»ä»»å¡:</strong> ä¾å¦ï¼é¸ç±»ç©ç§è¯å«ãæ¤ç©çå®³è¯æ­ãå»å­¦å½±åä¸­å¾®å°çç¶åç±»ãå·¥ä¸äº§åç¼ºé·æ£æµç­ï¼è¿äºä»»å¡éå¸¸é¢ä¸´æ°æ®ç¨ç¼ºãé¢åæ¼ç§»åå¯¹é«ç²¾åº¦è¦æ±çé®é¢ã</li>
<li><strong>çæå­¦åçç©å¤æ ·æ§çæµ:</strong> é¤äºé£è¾ï¼è¯¥æ¹æ³å¯æ¨å¹¿å°å¶ä»æè«ãé±¼ç±»ãä¸¤æ å¨ç©ãæ¤ç©ç­ç©ç§çèªå¨åè¯å«åè®¡æ°ï¼å°¤å¶æ¯å¨éå¤å¤æç¯å¢ä¸ã</li>
<li><strong>åä¸ç§æ:</strong> åä½ç©çè«å®³çæ©æé¢è­¦åè¯å«ï¼éè¿é¨ç½²è½»éçº§æ¨¡åå¨ç°é´è®¾å¤ä¸è¿è¡å®æ¶çæµã</li>
<li><strong>é¥æåç¯å¢çæµ:</strong> ä»å«æææ äººæºå¾åä¸­è¯å«ç¹å®å°ç©ãæ¤è¢«ç±»åæç¯å¢ååï¼å°¤å¶æ¯å¨éè¦å¿«éå¤çåé¨ç½²çåºæ¯ã</li>
<li><strong>å»çå½±ååæ:</strong> å°å¨å¤§åå¬å¼æ°æ®éä¸é¢è®­ç»çæ¨¡åéåºå°ç¹å®å»é¢æè®¾å¤äº§ççä¸´åºæ°æ®ä¸ï¼ä»¥è¾å©è¯æ­ï¼å°¤å¶æ¯å¨ç½è§ç¾çæéç§æææ°æ®åéçæåµä¸ã</li>
<li><strong>å·¥ä¸èªå¨ååè´¨éæ§å¶:</strong> å¨çäº§çº¿ä¸é¨ç½²è½»éçº§æ¨¡åè¿è¡å®æ¶äº§åè´¨éæ£æµï¼è¯å«å¾®å°ç¼ºé·ã</li>
</ul>
<h3 id="5_3">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</h3>
<ul>
<li><strong>éç¨æ§/æ³åè½å:</strong> å®éªä»å¨â101ç§ä¸¹éº¦é£è¾âåâAMIç¸æºç³»ç»âä¸è¿è¡ãè¯¥æ¹æ³å¨æ´å¹¿æ³çé£è¾ç©ç§ãä¸åå°çåºåãä¸åæ°åæ¡ä»¶æä¸åç±»åç¸æºç³»ç»ï¼åè¾¨çãåç§ãåªå£°ç¹æ§ï¼ä¸çè¡¨ç°ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>âæéä¸å®¶æ æ³¨éå¤æ°æ®âçæ°éæªæç¡®:</strong> æè¦æªå·ä½è¯´æâæéâæ°æ®çéçº§ãè¯¥æ¹æ³å¯¹æ°æ®éçæææ§å¦ä½ï¼å¨æ°æ®æåº¦ç¨ç¼ºï¼ä¾å¦ï¼åªæå åå¼ å¾åï¼çæåµä¸æ¯å¦ä¾ç¶ææï¼</li>
<li><strong>å¯¹ç¹å®åºç¡æ¨¡åçä¾èµ:</strong> æ¨¡åçæ§è½å¨å¾å¤§ç¨åº¦ä¸ä¾èµäºBioCLIP2çå¼ºå¤§è½åãå¦æBioCLIP2æ¬èº«å­å¨å±éæ§ï¼æèæªæ¥åºç°æ´ä¼çåºç¡æ¨¡åï¼åéè¦éæ°è¯ä¼°ææ´æ°ã</li>
<li><strong>ç¥è¯è¸é¦çåºæå±é:</strong> ç¥è¯è¸é¦éå¸¸æå³çå­¦çæ¨¡åçæ§è½ä¸éç±æå¸æ¨¡åå³å®ï¼å­¦çæ¨¡åå¾é¾è¶è¶æå¸æ¨¡åãè¿æå³çBioCLIP2çä»»ä½åºæåå·®æéè¯¯é½å¯è½è¢«ä¼ éç»ConvNeXt-tinyã</li>
<li><strong>ç¯å¢é²æ£æ§æªè¯¦è¿°:</strong> æè¦æªæåæ¨¡åå¨æç«¯éå¤ç¯å¢æ¡ä»¶ï¼å¦æ¶å£å¤©æ°ãåç§å§çååãé®æ¡ãé¨åå¯è§ç­ï¼ä¸çé²æ£æ§è¡¨ç°ã</li>
<li><strong>è®¡ç®ææ¬éä½çå·ä½éå:</strong> è½ç¶æå°äºâæ¾èéä½è®¡ç®ææ¬âï¼ä½æ²¡æç»åºå·ä½çéåææ ï¼å¦FLOPsãåæ°éãæ¨çæ¶é´ç­ï¼ï¼è¿ä½¿å¾é¾ä»¥ç´æ¥æ¯è¾å¶æçæåçç¨åº¦ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture.</li>
<li>Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.20089v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.20089v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.20063v1'></a></p>
<h2 id="openm3d-open-vocabulary-multi-view-indoor-3d-object-detection-without-human-annotations"><a href="https://arxiv.org/abs/2508.20063v1">OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</a></h2>
<p><strong>Authors:</strong> Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</p>
<p><strong>Published:</strong> 2025-08-27</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæãOpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotationsãå¨è®¡ç®æºè§è§é¢åï¼ç¹å«æ¯3Dç®æ æ£æµæ¹é¢ï¼æåºäºä¸ä¸ªå¼äººæ³¨ç®çæ°æ¹æ³ãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-2-3_3">1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3å¥è¯)</h3>
<p>OpenM3Dæåºäºä¸ç§æ°é¢çåé¶æ®µãå¼æ¾è¯æ±ãå¤è§è§å®¤å3Dç®æ æ£æµå¨ï¼å¶æ ¸å¿åæ°å¨äº<strong>æ éäººå·¥æ æ³¨</strong>å³å¯è¿è¡è®­ç»ãå®éè¿ç»åé«è´¨éç3Dä¼ªæ¡çææ¹æ³ï¼åºäºå¾åµå¥ï¼åå©ç¨å¤æ ·åCLIPç¹å¾çä½ç´ è¯­ä¹å¯¹é½æå¤±ï¼å®ç°äºå¨å®¤ååºåæµè¯ä¸­è¶è¶ç°ææ¹æ³çåç¡®æ§åéåº¦ãè¿é¡¹å·¥ä½æ¾èéä½äº3Dæ£æµçæ æ³¨ææ¬ï¼å¹¶æ¨å¨äºå¾ååº3Då¼æ¾è¯æ±æ£æµçåå±ã</p>
<h3 id="2_4">2. å³é®åæ°ææ¹æ³è®º</h3>
<p>OpenM3Dçå³é®åæ°å¨äºå¶<strong>æ äººå·¥æ æ³¨çè®­ç»èå¼</strong>ä»¥åå®ç°è¿ä¸èå¼æéç¨çç¬ç¹æ¹æ³ï¼</p>
<ol>
<li><strong>æ äººå·¥æ æ³¨çè®­ç»ï¼</strong> è¿æ¯ææ ¸å¿çåæ°ç¹ãè®ºææç¡®æåºï¼å¨è®­ç»è¿ç¨ä¸­ï¼ä¸ä½¿ç¨ä»»ä½äººå·¥æ æ³¨ç3Dè¾¹çæ¡æç±»å«ä¿¡æ¯ãè¿æå¤§å°éä½äº3Dæ£æµçæ æ³¨ææ¬ï¼æ¯è¯¥é¢åçä¸ä¸ªéè¦çªç ´ã</li>
<li><strong>é«è´¨é3Dä¼ªæ¡çæï¼</strong> ä¸ºäºå¼¥è¡¥ç¼ºä¹äººå·¥æ æ³¨ç3Dè¾¹çæ¡ï¼OpenM3Dæåºäºä¸ç§æ°é¢ç3Dä¼ªæ¡çææ¹æ³ãå®å©ç¨<strong>å¾åµå¥ææ¯</strong>å°2Då¾åä¸­çåå²ç»æç»åæè¿è´¯ç3Dç»æï¼ä»èçæé«è´¨éç3Dä¼ªæ¡ãè¿äºä¼ªæ¡ç¨äºè®­ç»ç±»ä¸å¯ç¥ç3Då®ä½æå¤±ã</li>
<li><strong>ä½ç´ è¯­ä¹å¯¹é½æå¤±ä¸å¤æ ·åCLIPç¹å¾ï¼</strong> ä¸ºäºå®ç°å¼æ¾è¯æ±è½åï¼OpenM3Då¼å¥äºä½ç´ è¯­ä¹å¯¹é½æå¤±ãå®ä»ä¸æ¯ä¸ªè¿è´¯3Dç»æç¸å³ç2Dåå²ä¸­éæ ·<strong>å¤æ ·åçé¢è®­ç»CLIPç¹å¾</strong>ï¼å¹¶å°å¶ä¸å¯¹åºç3Dä½ç´ ç¹å¾è¿è¡å¯¹é½ãè¿ä½¿å¾æ¨¡åè½å¤çè§£åè¯å«è®­ç»ä¸­æªè§çç©ä½ç±»å«ã</li>
<li><strong>åé¶æ®µé«ææ£æµå¨ï¼</strong> OpenM3Dæ¯ä¸ä¸ªåé¶æ®µæ£æµå¨ï¼å®å©ç¨ImGeoNetæ¨¡åå¯¼åºç2Dè¯±å¯¼ä½ç´ ç¹å¾ãç»åä¸è¿°ä¸¤ç§æå¤±ï¼å®å¨æ¨çæ¶ä»éå¤è§è§å¾åè¾å¥ï¼ä¾¿è½å®ç°é«æçï¼0.3ç§/åºæ¯ï¼åé«ç²¾åº¦ã</li>
</ol>
<h3 id="3_4">3. å¯¹è¯¥é¢åçæ½å¨å½±å</h3>
<ol>
<li><strong>éä½æ æ³¨ææ¬ï¼å éç ç©¶ä¸åºç¨ï¼</strong> 3Dç®æ æ£æµï¼ç¹å«æ¯å®¤ååºæ¯ï¼å¶æ°æ®æ æ³¨ææ¬æé«ãOpenM3Dçæ äººå·¥æ æ³¨è®­ç»èå¼å°æå¤§å°éä½è¿ä¸é¨æ§ï¼ä½¿å¾ç ç©¶äººååå¼åèè½å¤æ´å¿«å°æ¢ç´¢åé¨ç½²3Dè§è§ç³»ç»ï¼èæ éæå¥å¤§éèµæºè¿è¡æ°æ®æ æ³¨ã</li>
<li><strong>æ¨å¨å¼æ¾è¯æ±3Dæ£æµåå±ï¼</strong> ç®åå¼æ¾è¯æ±3Dæ£æµä»å¤äºæ©æé¶æ®µï¼ä¸å¾ååºæ¹æ³ç¸å¯¹æ»åãOpenM3Dçæåè¯æäºå¾ååºæ¹æ³å¨æ éäººå·¥æ æ³¨çæåµä¸å®ç°å¼æ¾è¯æ±3Dæ£æµçæ½åï¼å°æ¿å±æ´å¤ç ç©¶èæå¥å°è¿ä¸æ¹åã</li>
<li><strong>æåå¾ååº3Dæ£æµçç«äºåï¼</strong> ä¼ ç»ä¸ï¼åºäºç¹äºç3Dæ£æµæ¹æ³å¨ç²¾åº¦ä¸å¾å¾ä¼äºå¾ååºæ¹æ³ãOpenM3Då±ç¤ºäºå¾ååºæ¹æ³å¨æ çç£è®¾ç½®ä¸ä¹è½è¾¾å°åè¶çæ§è½ï¼çè³è¶è¶äºæäºå¼ºå²çåºçº¿åä¸¤é¶æ®µæ¹æ³ï¼è¿æå©äºæåå¾ååº3Dæ£æµå¨å®éåºç¨ä¸­çç«äºåã</li>
<li><strong>ä¿è¿èªçç£/å¼±çç£3Då­¦ä¹ ï¼</strong> è¯¥è®ºæä¸º3Dè§è§é¢åçèªçç£åå¼±çç£å­¦ä¹ æä¾äºæ°çæè·¯åèä¾ï¼ç¹å«æ¯å¦ä½ææå°ä»2Dä¿¡æ¯ï¼å¦2Dåå²ãCLIPç¹å¾ï¼ä¸­æå3Dè¯­ä¹åå ä½ä¿¡æ¯ã</li>
</ol>
<h3 id="4_4">4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</h3>
<ol>
<li><strong>æºå¨äººå­¦åèªä¸»ç³»ç»ï¼</strong> æºå¨äººéè¦å¨æªç¥æä¸æ­ååçç¯å¢ä¸­è¯å«åæä½åç§ç©ä½ãOpenM3Dçå¼æ¾è¯æ±åæ æ æ³¨ç¹æ§ä½¿å¶éå¸¸éåæºå¨äººå¯¼èªãæåååºæ¯çè§£ï¼å°¤å¶æ¯å¨å®¶åº­ãåå¬å®¤ç­å®¤åç¯å¢ä¸­ã</li>
<li><strong>å¢å¼ºç°å® (AR) / èæç°å® (VR)ï¼</strong> AR/VRåºç¨éè¦å®æ¶ãåç¡®å°è¯å«åå®ä½çå®ä¸çä¸­çç©ä½ï¼ä»¥ä¾¿è¿è¡èæåå®¹çå å åäº¤äºãOpenM3Då¯ä»¥å¸®å©AR/VRç³»ç»æ´å¥½å°çè§£ç¨æ·å¨å´çç¯å¢ï¼å®ç°æ´æ²æµ¸å¼çä½éªã</li>
<li><strong>æ°å­å­ªçåå®¤åæµç»ï¼</strong> èªå¨åå°æå»ºå·æè¯­ä¹ä¿¡æ¯çå®¤å3Dæ¨¡åï¼æ°å­å­ªçï¼æ¯è®¸å¤è¡ä¸çéæ±ãOpenM3Då¯ä»¥ç¨äºèªå¨è¯å«ååç±»å®¤åç©ä½ï¼å é3Dæ¨¡åçè¯­ä¹æ æ³¨è¿ç¨ã</li>
<li><strong>æºè½å®¶å±åç©èç½ (IoT)ï¼</strong> æºè½è®¾å¤éè¦çè§£å¶æå¤çç¯å¢åå¶ä¸­çç©ä½ãOpenM3Då¯ä»¥èµè½æºè½å®¶å±ç³»ç»ï¼ä½¿å¶è½å¤è¯å«æ´å¤ç§ç±»çç©ä½ï¼ä»èæä¾æ´æºè½ãæ´ä¸ªæ§åçæå¡ã</li>
<li><strong>å®å¨çæ§ï¼</strong> å¨çæ§åºæ¯ä¸­ï¼è¯å«ç¹å®ç©ä½æå¼å¸¸äºä»¶è³å³éè¦ãå¼æ¾è¯æ±è½åå¯ä»¥å¸®å©ç³»ç»æ£æµé¢å®ä¹ç±»å«ä¹å¤çæ½å¨å¨èææå´è¶£çç©ä½ã</li>
</ol>
<h3 id="5_4">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</h3>
<ol>
<li><strong>é¢åéå¶ï¼</strong> è®ºææç¡®æåºæ¯éå¯¹â<strong>å®¤å</strong>â3Dç®æ æ£æµãè¿æå³çè¯¥æ¹æ³å¯è½ä¸ç´æ¥éç¨äºå®¤å¤åºæ¯ï¼å ä¸ºå®¤å¤ç¯å¢çç©ä½ç±»åãå°ºåº¦ãåç§æ¡ä»¶åé®æ¡æ¨¡å¼ä¸å®¤åææ¾èå·®å¼ã</li>
<li><strong>è¾å¥æ°æ®è¦æ±ï¼</strong> å°½ç®¡æ éäººå·¥æ æ³¨ï¼ä½æ¨¡åä»éè¦â<strong>å¤è§è§RGB-Då¾å</strong>âä½ä¸ºè¾å¥ï¼å¹¶ä¸è¿äºå¾åæ¯â<strong>å·²å§¿æå (posed)</strong>âçãè¿æå³çå®ä¾èµäºæ·±åº¦ä¼ æå¨ååç¡®çç¸æºå§¿æä¼°è®¡ï¼è¿å¨æäºåºç¨åºæ¯ä¸­å¯è½æ¯ä¸ä¸ªéå¶ï¼ä¾å¦çº¯RGBè¾å¥ææ²¡æç²¾ç¡®å§¿æä¿¡æ¯çåºæ¯ã</li>
<li><strong>å¯¹é¢è®­ç»æ¨¡åçä¾èµï¼</strong> æ¨¡åçæ§è½å¨å¾å¤§ç¨åº¦ä¸ä¾èµäºå¶æä½¿ç¨çé¢è®­ç»æ¨¡åï¼å¦ImGeoNetç¨äº2Dè¯±å¯¼ä½ç´ ç¹å¾ï¼CLIPç¨äºè¯­ä¹ç¹å¾ï¼çè´¨éåæ³åè½åãå¦æè¿äºåºç¡æ¨¡åå­å¨åå·®æå¨ç¹å®é¢åè¡¨ç°ä¸ä½³ï¼å¯è½ä¼å½±åOpenM3Dçæç»æ§è½ã</li>
<li><strong>ä¼ªæ ç­¾è´¨éçæææ§ï¼</strong> æè¦ä¸­å¼ºè°äºâé«è´¨é3Dä¼ªæ¡âå¯¹äºè®­ç»åç¡®çåé¶æ®µæ£æµå¨çéè¦æ§ãè¿æå³çä¼ªæ¡çææ¹æ³çé²æ£æ§è³å³éè¦ãå¨æç«¯å¤æãé«åº¦æä¹±æåå«å¤§ééæ/åå°ç©ä½çå®¤ååºæ¯ä¸­ï¼ä¼ªæ¡ççæè´¨éå¯è½ä¼ä¸éï¼ä»èå½±åæ´ä½æ§è½ã</li>
<li><strong>âå¼æ¾è¯æ±âççæ­£æ³åè½åï¼</strong> å°½ç®¡ä½¿ç¨äºCLIPç¹å¾ï¼ä½CLIPæ¬èº«æ¯å¨å¤§é2Då¾åä¸è®­ç»çãå®å¯¹3Dç©ä½å±æ§ççè§£ï¼ä»¥åå¨éå¸¸è§è§è§æé«åº¦é®æ¡æåµä¸çè¯å«è½åï¼å¯è½ä»æå±éæ§ãæ¨¡åå¯¹å®å¨æ°é¢ãä¸2Dè®­ç»æ°æ®å·®å¼å¾å¤§ç3Dç©ä½çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
</ol>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations.</li>
<li>We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.20063v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.20063v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-08-29 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
