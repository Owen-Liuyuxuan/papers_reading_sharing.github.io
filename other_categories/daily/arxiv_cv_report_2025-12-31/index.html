<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-31 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-30/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-01/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-31">Arxiv Computer Vision Papers - 2025-12-31</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-12-29" class="nav-link">Arxiv 计算机视觉领域论文日报 (2025-12-29) 执行摘要</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-knows-transparency-repurposing-video-diffusion-for-transparent-object-depth-and-normal-estimation" class="nav-link">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#idt-a-physically-grounded-transformer-for-feed-forward-multi-view-intrinsic-decomposition" class="nav-link">IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</a>
                </li>
                <li class="nav-item">
                    <a href="#robomirror-understand-before-you-imitate-for-video-to-humanoid-locomotion" class="nav-link">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</a>
                </li>
                <li class="nav-item">
                    <a href="#omniagent-audio-guided-active-perception-agent-for-omnimodal-audio-video-understanding" class="nav-link">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#rethinking-the-spatio-temporal-alignment-of-end-to-end-3d-perception" class="nav-link">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#memorization-in-3d-shape-generation-an-empirical-study" class="nav-link">Memorization in 3D Shape Generation: An Empirical Study</a>
                </li>
                <li class="nav-item">
                    <a href="#same-or-not-enhancing-visual-perception-in-vision-language-models" class="nav-link">Same or Not? Enhancing Visual Perception in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#livetalk-real-time-multimodal-interactive-video-diffusion-via-improved-on-policy-distillation" class="nav-link">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</a>
                </li>
                <li class="nav-item">
                    <a href="#thinkgen-generalized-thinking-for-visual-generation" class="nav-link">ThinkGen: Generalized Thinking for Visual Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#rxnbench-a-multimodal-benchmark-for-evaluating-large-language-models-on-chemical-reaction-understanding-from-scientific-literature" class="nav-link">RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-31">Arxiv Computer Vision Papers - 2025-12-31</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-12-29">Arxiv 计算机视觉领域论文日报 (2025-12-29) 执行摘要</h2>
<p><strong>主要趋势与主题：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>多模态理解、生成模型（尤其是扩散模型）的拓展应用、以及对三维视觉任务的深入探索</strong>等方面的显著进展。特别值得注意的是，研究人员正积极将先进的生成模型（如扩散模型）应用于更复杂的感知任务，并探索其在物理世界交互和理解中的潜力。同时，对现有模型（如 Transformer）的改进和新架构的设计也持续进行，以提升在特定任务上的性能。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>扩散模型在感知任务中的创新应用：</strong> "Diffusion Knows Transparency" 和 "LiveTalk" 两篇论文展示了扩散模型在<strong>透明物体深度与法线估计</strong>以及<strong>实时交互式视频生成</strong>方面的突破性应用，预示着生成模型在解决传统感知难题上的巨大潜力。</li>
<li><strong>物理学原理与Transformer的结合：</strong> "IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition" 提出了一种<strong>基于物理原理的 Transformer 模型</strong>，用于多视图内在分解，这标志着将物理约束融入深度学习模型以提升准确性和鲁棒性的重要方向。</li>
<li><strong>具身智能与模仿学习的进步：</strong> "RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion" 在<strong>视频到人形机器人运动模仿</strong>方面取得了进展，强调了在模仿前进行理解的重要性，为具身智能的发展提供了新的思路。</li>
<li><strong>多模态理解的泛化能力：</strong> "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding" 和 "RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature" 分别在<strong>音频引导的跨模态感知</strong>和<strong>科学文献中的化学反应理解</strong>方面进行了探索，展示了模型在处理和理解不同模态信息以及特定领域知识的能力。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>扩散模型的泛化与应用拓展：</strong> 扩散模型正从图像生成扩展到更复杂的感知任务，如三维重建、物理属性估计等。</li>
<li><strong>物理学原理与深度学习的融合：</strong> 将物理定律和约束融入模型设计，以提高模型的准确性、可解释性和泛化能力。</li>
<li><strong>具身智能与交互式AI：</strong> 关注机器人如何理解和模仿人类行为，以及如何通过多模态信息进行主动感知和交互。</li>
<li><strong>大规模多模态理解与特定领域知识整合：</strong> 构建能够处理和理解多种模态信息，并能应用于特定科学领域（如化学）的模型。</li>
<li><strong>三维视觉的对齐与生成：</strong> 对三维感知中的时空对齐问题进行深入研究，以及探索三维形状生成中的记忆机制。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>考虑到其创新性和对未来研究方向的指导意义，以下论文值得深入阅读：</p>
<ol>
<li><strong>"Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"</strong>: 扩散模型在解决具有挑战性的透明物体感知问题上的创新应用，具有重要的理论和实践意义。</li>
<li><strong>"IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition"</strong>: 将物理学原理与 Transformer 结合，为提升三维视觉任务的性能提供了一种新颖且有前景的方法。</li>
<li><strong>"OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"</strong>: 在多模态理解领域，特别是音频引导下的主动感知方面，展示了新的研究思路和技术潜力。</li>
<li><strong>"RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion"</strong>: 对于具身智能和机器人模仿学习领域的研究者来说，这篇论文提供了关于如何提升模仿学习效果的重要见解。</li>
</ol>
<p>这份摘要旨在为忙碌的研究人员提供一个快速了解最新 Arxiv 论文动态的窗口，并引导他们关注最可能产生深远影响的研究方向。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.23705v1">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</a></li>
<li><a href="#2512.23667v1">IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</a></li>
<li><a href="#2512.23649v1">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</a></li>
<li><a href="#2512.23646v1">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</a></li>
<li><a href="#2512.23635v1">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</a></li>
<li><a href="#2512.23628v1">Memorization in 3D Shape Generation: An Empirical Study</a></li>
<li><a href="#2512.23592v1">Same or Not? Enhancing Visual Perception in Vision-Language Models</a></li>
<li><a href="#2512.23576v1">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</a></li>
<li><a href="#2512.23568v1">ThinkGen: Generalized Thinking for Visual Generation</a></li>
<li><a href="#2512.23565v1">RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.23705v1'></a></p>
<h2 id="diffusion-knows-transparency-repurposing-video-diffusion-for-transparent-object-depth-and-normal-estimation"><a href="https://arxiv.org/abs/2512.23705v1">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</a></h2>
<p><strong>Authors:</strong> Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation”的全面中文摘要：</p>
<p><strong>论文摘要：Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决计算机视觉领域中一个长期存在的难题：<strong>透明和反光物体的精确深度和法线估计</strong>。传统方法（如双目立体视觉、ToF传感器和单目深度估计模型）在处理这些物体时面临严峻挑战，因为它们的折射、反射和透射特性打破了这些方法的固有假设，导致深度图出现空洞、估计不稳定，并严重影响下游任务（如机器人抓取）的性能。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<p>论文的核心洞察在于，现代视频扩散模型（VDM）在生成逼真视频时，已经隐式地学习了透明物体的光学规律。基于此，作者提出了以下关键贡献：</p>
<ul>
<li><strong>TransPhy3D 数据集：</strong> 构建了首个大规模的透明/反光物体合成视频数据集，包含 11,000 个序列（132 万帧）。该数据集通过 Blender/Cycles 渲染，结合了丰富的静态和程序化生成的 3D 模型，并使用了物理渲染和 OptiX 去噪技术，生成了高质量的 RGB、深度和法线视频。</li>
<li><strong>DKT 模型（Diffusion Knows Transparency）：</strong> 提出了一种新颖的视频深度（和法线）估计框架。DKT 将视频深度估计重塑为一个<strong>视频到视频的翻译问题</strong>，而不是传统的判别式估计任务。</li>
<li><strong>基于 LoRA 的 VDM 迁移学习：</strong> 利用轻量级的 LoRA (Low-Rank Adaptation) 技术，对预训练的大型视频扩散模型进行微调，使其能够高效地适应透明物体深度估计任务，同时保留其强大的生成先验，避免灾难性遗忘。</li>
<li><strong>联合训练策略：</strong> 引入了一种协同训练策略，将 TransPhy3D 数据集与现有的逐帧合成数据集相结合，以充分利用现有资源并提升模型的泛化能力。</li>
<li><strong>视频到视频的翻译：</strong> DKT 模型通过将 RGB 和（带噪声的）深度潜在表示拼接起来输入到 DiT (Diffusion Transformer) 主干网络中，实现对任意长度输入视频的<strong>时间一致性</strong>深度估计。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>零样本 SOTA 性能：</strong> DKT 在多个透明物体相关的真实和合成视频基准测试（ClearPose, DREDS (CatKnown/CatNovel), TransPhy3D-Test）上实现了<strong>零样本（zero-shot）的 SOTA（State-of-the-Art）性能</strong>。</li>
<li><strong>显著的准确性和时间一致性提升：</strong> DKT 在准确性和时间一致性方面均显著优于现有的图像和视频基线方法（如 Depth-Anything-v2, Depth Crafter）。其法线估计变体 DKT-Normal 在 ClearPose 数据集上取得了最佳的视频法线估计结果。</li>
<li><strong>高效的推理速度：</strong> 一个紧凑的 1.3B 参数版本可以在 0.17 秒/帧（832x480 分辨率）的速度下运行，使其适用于实时应用。</li>
<li><strong>实际应用价值：</strong> 将 DKT 集成到机器人抓取系统中后，其输出的深度信息显著提高了在半透明、反光和漫反射表面上的抓取成功率，优于之前的估计器。</li>
<li><strong>理论意义：</strong> 研究结果有力地支持了“<strong>扩散模型理解透明性</strong>”的观点，表明生成模型强大的先验知识可以被高效、无标签地转化为鲁棒、时间一致的感知能力，为机器人操作等挑战性任务提供支持。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对渲染数据的依赖：</strong> 虽然 DKT 在零样本设置下表现出色，但其训练完全依赖于合成数据。虽然作者通过构建大规模、多样化的数据集来缓解这个问题，但真实世界数据的复杂性和不可预测性仍然可能带来挑战。</li>
<li><strong>计算成本：</strong> 虽然 LoRA 技术显著降低了微调成本，但训练大型视频扩散模型本身仍然需要大量的计算资源。</li>
<li><strong>推理步数的影响：</strong> 论文提到，增加推理步数对性能提升的边际效应不大，而过少的步数会导致预测不准确。为了平衡性能和效率，作者选择了 5 个推理步数。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>真实世界数据融合：</strong> 探索如何更有效地将少量真实世界标注数据或无监督的真实世界数据融入训练过程，以进一步提升模型在真实场景下的鲁棒性。</li>
<li><strong>更广泛的材料和场景：</strong> 将 DKT 的能力扩展到更广泛的材料类型（如高度漫反射、次表面散射等）和更复杂的场景（如动态环境、极端光照条件）。</li>
<li><strong>端到端机器人应用：</strong> 进一步探索 DKT 在更复杂的机器人任务中的应用，例如需要精细操作的装配、抓取不规则形状的物体等。</li>
<li><strong>模型压缩与加速：</strong> 进一步研究模型压缩和加速技术，以满足更严格的实时性要求，例如在嵌入式机器人平台上的部署。</li>
<li><strong>可解释性研究：</strong> 深入研究扩散模型如何“理解”透明性，以及其内部机制如何捕捉物理光学原理。</li>
</ul>
<p>总而言之，这篇论文通过巧妙地利用视频扩散模型的生成能力，成功地解决了透明物体深度和法线估计的长期难题，为机器人感知和三维重建领域带来了重要的进展。其提出的 TransPhy3D 数据集和 DKT 模型为未来的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23705v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23705v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23667v1'></a></p>
<h2 id="idt-a-physically-grounded-transformer-for-feed-forward-multi-view-intrinsic-decomposition"><a href="https://arxiv.org/abs/2512.23667v1">IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</a></h2>
<p><strong>Authors:</strong> Kang Du, Yirui Guan, Zeyu Wang</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面摘要。</p>
<p><strong>论文题目：</strong> IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</p>
<p><strong>作者：</strong> Kang Du, Yirui Guan, Zeyu Wang</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决了<strong>多视图内禀图像分解（Multi-View Intrinsic Image Decomposition）</strong>的挑战。内禀图像分解旨在将RGB图像分解为其底层的材质属性（如漫反射率/反照率）和光照信息（如漫反射阴影），这对于视觉理解至关重要。尽管单视图内禀图像分解（特别是基于扩散模型的方法）取得了显著进展，但将其扩展到多视图设置时，<strong>视图一致性（View Consistency）</strong>成为一个严峻的挑战，常常导致不同视图下的内禀因子不一致。现有的方法难以在多视图场景下有效地强制执行这种一致性。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>作者提出了<strong>内禀分解Transformer（Intrinsic Decomposition Transformer, IDT）</strong>，一个<strong>前馈（Feed-Forward）</strong>的多视图内禀图像分解框架。其核心创新点包括：</p>
<ul>
<li><strong>多视图Transformer聚合：</strong> 借鉴了多视图几何推理的Transformer架构，IDT利用Transformer的注意力机制来<strong>联合推理（Jointly Reason）</strong>多个输入图像，在一个<strong>单次前馈过程（Single Forward Pass）</strong>中聚合跨视图信息，从而实现视图一致性。这避免了迭代生成采样。</li>
<li><strong>物理学基础的图像形成模型：</strong> IDT采用了一个<strong>物理学基础的图像形成模型</strong>，将观察到的图像显式地分解为<strong>漫反射率（Diffuse Reflectance, Albedo）</strong>、<strong>漫反射阴影（Diffuse Shading）</strong>和<strong>镜面反射阴影（Specular Shading）</strong>。这种结构化分解将朗伯体（Lambertian）和非朗伯体光传输分离开来，使得材质和光照的分解更具<strong>可解释性（Interpretable）</strong>和<strong>可控性（Controllable）</strong>。特别是，它将视图不变的反照率与视图依赖的阴影分离开，有效防止了视图依赖效应（如镜面反射）渗入反照率。</li>
<li><strong>外观适配器（Appearance Adapters）：</strong> 为了实现因子（反照率、漫反射阴影、镜面反射阴影）的<strong>选择性推理（Selective Reasoning）</strong>，IDT引入了外观适配器。这些适配器将Transformer聚合的共享潜在Token路由到各自的预测头，使得模型能够根据不同内禀因子的特性进行专门化预测。</li>
<li><strong>场景条件下的交叉注意力（Scene-Conditioned Cross-Attention）：</strong> 适配器采用轻量级的交叉注意力机制，利用场景级Token（如相机信息）作为查询，从图像Patch Token中提取信息，从而生成紧凑的场景条件上下文，用于每个内禀因子的预测。</li>
<li><strong>球形高斯混合模型（SGM）用于光照表示：</strong> 论文引入了一个紧凑的场景级光照表示（SGM），用于条件化阴影预测，从而实现光照感知预测，而无需显式的物理渲染方程。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著提升多视图一致性：</strong> IDT在合成和真实世界数据集上都取得了<strong>显著提高的多视图一致性</strong>，优于现有的单视图和多视图基线方法。这表明联合多视图推理对于强制执行物理上合理的视图一致性至关重要。</li>
<li><strong>更清晰的内禀因子：</strong> IDT能够生成<strong>更清晰的漫反射率</strong>，<strong>更连贯的漫反射阴影</strong>，以及<strong>更分离的镜面反射分量</strong>。这使得分解后的内禀因子更具可解释性，并能有效减少材质和光照之间的混淆。</li>
<li><strong>优越的重构质量：</strong> 尽管专注于内禀分解，IDT在图像重构质量上也表现出色，表明其预测的内禀因子能够准确地重建原始图像。</li>
<li><strong>意义：</strong> IDT为<strong>可扩展的多视图内禀理解</strong>提供了一个简单而有效的基础。它克服了现有方法在多视图一致性上的局限，为下游的3D场景理解任务（如重光照、视图合成等）奠定了更坚实的基础。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中没有明确列出局限性，但从其方法和实验设置中可以推断出一些潜在的考虑：</p>
<ul>
<li><strong>对室内场景的侧重：</strong> 论文主要在室内数据集（Hypersim, InteriorVerse）上进行评估，其在更广泛的室外或复杂光照环境下的泛化能力可能需要进一步验证。</li>
<li><strong>对Transformer架构的依赖：</strong> IDT依赖于Transformer的计算能力，在非常大规模的场景或极高分辨率图像上，计算成本可能是一个考虑因素。</li>
<li><strong>物理模型简化：</strong> 虽然采用了物理学基础模型，但仍是对真实世界光照和材质交互的简化近似。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到更广泛的场景：</strong> 将IDT应用于室外场景，以及具有更复杂光照条件（如全局光照、多光源）的场景。</li>
<li><strong>动态场景的内禀分解：</strong> 探索IDT在处理具有运动物体的动态场景时的适用性。</li>
<li><strong>与其他3D视觉任务的融合：</strong> 将IDT作为基础模块，与3D重建、场景理解、虚拟现实等任务更紧密地结合。</li>
<li><strong>更精细的材质和光照建模：</strong> 进一步探索更复杂的BRDF模型和光照表示，以捕捉更丰富的材质和光照细节。</li>
<li><strong>端到端优化与更高效的架构：</strong> 探索更高效的Transformer变体或与其他网络架构的结合，以进一步提高效率和性能。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入一个基于Transformer的<strong>前馈多视图内禀分解框架（IDT）</strong>，并结合<strong>物理学基础的图像形成模型</strong>，成功地解决了多视图内禀图像分解中的<strong>视图一致性</strong>问题。其关键贡献在于利用Transformer的跨视图推理能力，以及将图像分解为可解释的物理因子，从而在多个数据集上取得了优于现有方法的性能，为多视图视觉理解任务提供了重要的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23667v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23667v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23649v1'></a></p>
<h2 id="robomirror-understand-before-you-imitate-for-video-to-humanoid-locomotion"><a href="https://arxiv.org/abs/2512.23649v1">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</a></h2>
<p><strong>Authors:</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion”的全面中文摘要：</p>
<p><strong>论文题目：</strong> RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion (RoboMirror：理解后模仿，实现视频到人形机器人运动的控制)</p>
<p><strong>作者：</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题与背景</strong></p>
<p>当前人形机器人运动控制面临一个关键挑战：如何让机器人真正理解视觉输入（如视频），并基于这种理解来生成逼真且符合语义的运动。现有方法主要依赖于预先采集的动作捕捉轨迹或稀疏的文本指令。动作捕捉绕过了视觉理解，而文本指令由于其语义的稀疏性，难以捕捉视频中丰富的动态信息。现有的基于视频的方法，如“姿态估计-重定向-跟踪”流水线，往往只能进行机械的姿态模仿，缺乏对动作意图的真正理解，并且容易受到姿态估计和重定向过程中的误差累积和延迟影响。这导致了视觉理解与机器人控制之间的巨大鸿沟。</p>
<p><strong>2. 核心创新与方法贡献</strong></p>
<p>为了解决上述问题，本文提出了 <strong>RoboMirror</strong>，一个创新的、<strong>无重定向 (retargeting-free)</strong> 的视频到人形机器人运动控制框架。其核心理念是“<strong>先理解，后模仿</strong>”。</p>
<p>RoboMirror 的主要创新点和方法贡献包括：</p>
<ul>
<li><strong>“理解后模仿”范式：</strong> RoboMirror 强调机器人应首先理解视频内容，然后基于这种理解来生成运动，而不是简单地模仿表面姿态。</li>
<li><strong>利用视觉语言模型 (VLM) 进行意图提取：</strong> 框架利用强大的 VLM（如 Qwen3-VL）来处理原始的<strong>自视角 (egocentric)</strong> 或<strong>第三人称 (third-person)</strong> 视频，从中提取出富含语义的<strong>视觉运动意图 (visual motion intents)</strong>。这些意图直接作为条件，用于指导后续的运动生成。</li>
<li><strong>运动潜变量重构：</strong> RoboMirror 引入了一个关键的“<strong>重构优于对齐 (reconstruction outperforms alignment)</strong>”的洞察。它不直接使用 VLM 的输出作为控制信号，而是将其作为条件，通过一个基于<strong>扩散模型 (diffusion model)</strong> 的网络来重构出<strong>运动潜变量 (motion latent)</strong>。这个潜变量不仅包含了 VLM 提取的语义信息，还被注入了运动的运动学信息，从而确保了运动的物理合理性和语义一致性。</li>
<li><strong>无姿态估计与重定向：</strong> 整个框架绕过了传统方法中耗时且易出错的姿态估计和重定向步骤，直接从视频意图生成可执行的机器人动作。</li>
<li><strong>基于扩散模型的策略：</strong> 重构出的运动潜变量被用作条件，直接指导一个<strong>扩散模型策略 (diffusion-based policy)</strong> 来生成平滑、物理上可行的机器人运动。</li>
<li><strong>两阶段框架：</strong> RoboMirror 采用一个两阶段的框架：<ul>
<li><strong>第一阶段：</strong> VLM 驱动的运动潜变量重构。</li>
<li><strong>第二阶段：</strong> 策略学习（包括一个教师策略和一个学生策略），其中学生策略学习在 VLM 提供的运动潜变量指导下生成动作。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<p>RoboMirror 在多个方面展现了其有效性和优越性：</p>
<ul>
<li><strong>显著降低延迟：</strong> 与基于姿态估计的基线方法相比，RoboMirror 将第三人称视频到控制的延迟<strong>降低了 80%</strong>（从 9.22 秒降至 1.84 秒），极大地提高了响应速度。</li>
<li><strong>提高任务成功率：</strong> 在实验中，RoboMirror 实现了比基线方法<strong>高 3.7% 的任务成功率</strong>，表明其生成的运动更加鲁棒和可靠。</li>
<li><strong>实现“身临其境”的遥操作：</strong> 利用自视角视频，RoboMirror 能够实现<strong>远程呈现 (telepresence)</strong>，让机器人能够“身临其境”地模仿人类的动作。</li>
<li><strong>无需显式姿态监督：</strong> 对于自视角视频，RoboMirror 能够在<strong>不依赖显式人类姿态监督</strong>的情况下合成鲁棒且语义丰富的运动，这是传统姿态估计流水线难以实现的。</li>
<li><strong>跨领域泛化能力：</strong> 实验证明了 RoboMirror 在不同模拟器（IsaacGym 和 MuJoCo）之间具有良好的<strong>跨领域泛化能力</strong>。</li>
<li><strong>推动了理解驱动的控制：</strong> RoboMirror 成功地将 VLM 的强大视觉理解能力与人形机器人的运动控制相结合，为<strong>理解驱动的控制 (understanding-driven control)</strong> 奠定了基础。</li>
</ul>
<p><strong>4. 提及的局限性</strong></p>
<p>论文中未明确列出局限性，但从其方法和实验设置中可以推断出一些潜在的方面：</p>
<ul>
<li><strong>对 VLM 的依赖：</strong> 框架的性能在很大程度上依赖于 VLM 的理解能力。如果 VLM 无法准确捕捉视频中的细微动作意图，可能会影响最终的运动生成。</li>
<li><strong>训练数据的需求：</strong> 虽然 RoboMirror 旨在减少对特定数据格式的依赖，但其训练过程（尤其是教师策略的训练）仍然需要大量的运动数据。</li>
<li><strong>计算资源：</strong> 扩散模型和 VLM 通常需要大量的计算资源进行训练和推理，尽管论文通过紧凑的网络架构和 DDIM 采样等技术优化了推理效率。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<p>基于 RoboMirror 的成功，未来的研究方向可以包括：</p>
<ul>
<li><strong>更精细的动作理解与控制：</strong> 进一步提升 VLM 对复杂动作意图的理解能力，并将其转化为更精细的运动控制指令。</li>
<li><strong>更广泛的动作类型：</strong> 将 RoboMirror 扩展到更广泛的动作类型，例如精细的手部操作、与环境的交互等。</li>
<li><strong>实时性优化：</strong> 进一步优化模型的推理速度，以满足更严格的实时性要求，尤其是在复杂的机器人应用场景中。</li>
<li><strong>鲁棒性提升：</strong> 探索更强大的方法来应对视频中的噪声、遮挡、光照变化等挑战，进一步提升在真实世界中的鲁棒性。</li>
<li><strong>人机协作：</strong> 将 RoboMirror 应用于人机协作场景，使机器人能够更好地理解人类的意图并与之协同工作。</li>
<li><strong>多模态融合：</strong> 探索将其他模态的信息（如触觉、听觉）与视频信息融合，以实现更全面的理解和控制。</li>
</ul>
<p><strong>总结</strong></p>
<p>RoboMirror 是一项重要的研究成果，它成功地弥合了视觉理解与人形机器人运动控制之间的鸿沟。通过创新的“理解后模仿”范式，利用 VLM 提取视频意图，并结合扩散模型进行运动潜变量重构和策略生成，RoboMirror 在降低延迟、提高成功率和实现无重定向的视频到运动控制方面取得了显著进展。这项工作为开发更智能、更具适应性的人形机器人控制系统开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control.</li>
<li>We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate".</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23649v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23649v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23646v1'></a></p>
<h2 id="omniagent-audio-guided-active-perception-agent-for-omnimodal-audio-video-understanding"><a href="https://arxiv.org/abs/2512.23646v1">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</a></h2>
<p><strong>Authors:</strong> Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding”的全面中文摘要：</p>
<p><strong>论文题目：</strong> OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding
<strong>作者：</strong> Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
尽管全模态大语言模型（OmniLLMs）在统一音频和视觉模态方面取得了显著进展，但它们在细粒度的跨模态理解和模态对齐方面仍存在不足。现有的方法通常依赖于僵化的、静态的工作流程，或者采用密集的帧-字幕生成方式，这导致计算开销大且难以实现真正精细化的音视频理解。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了一种名为 <strong>OmniAgent</strong> 的新型<strong>音频引导的主动感知智能体</strong>，旨在解决上述挑战。其核心创新点包括：</p>
<ul>
<li><strong>主动感知范式：</strong> OmniAgent 将音视频理解从被动响应转变为主动的多模态探究。它通过一个迭代的“思考-行动-观察-反思”（Think-Act-Observe-Reflect）循环，动态地编排和调用专门的感知工具（视频、音频、事件工具），以自主地规划推理过程。</li>
<li><strong>音频引导的粗粒度到细粒度感知：</strong> 论文引入了一种新颖的<strong>粗粒度到细粒度（coarse-to-fine）的音频引导感知范式</strong>。该范式利用音频线索来精确定位时间事件，并以此指导后续的推理，从而实现高效且精细化的跨模态理解。</li>
<li><strong>模态感知专家工具集：</strong> OmniAgent 构建了一个包含视频、音频和事件三大类别的模态感知专家工具集。这些工具在不同粒度和信息密度上提供服务，允许智能体根据任务需求动态选择和调用。</li>
<li><strong>自反思与跨模态一致性检查：</strong> 智能体通过反思模块来评估证据的有效性，并进行跨模态一致性检查，以识别视觉和听觉信号之间的潜在差异，从而动态调整其推理和感知策略。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在三个广泛使用的音视频理解基准（Daily-Omni, OmniVideoBench, WorldSense）上的广泛实验评估，OmniAgent 取得了<strong>最先进（State-of-the-Art, SoTA）的性能</strong>。</p>
<ul>
<li><strong>显著的性能提升：</strong> OmniAgent 在准确率上<strong>大幅超越</strong>了领先的开源和闭源模型，包括 Qwen3-Omni 和 Gemini2.5-Flash，平均提升幅度达到 <strong>10%-20%</strong>。</li>
<li><strong>解决跨模态对齐难题：</strong> 通过主动感知和音频引导的策略，OmniAgent 有效地解决了传统 OmniLLMs 在跨模态对齐和细粒度理解方面的固有难题。</li>
<li><strong>效率提升：</strong> 相较于一些基线方法（如 DVD），OmniAgent 在减少视觉令牌冗余和降低推理延迟方面表现出优势。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到，当前方法对外部模型和扩展上下文的依赖虽然提升了性能，但也<strong>限制了推理效率</strong>。</p>
<p><strong>5. 未来研究方向：</strong>
作者设想未来的研究方向包括：</p>
<ul>
<li><strong>训练一个全模态的智能体模型：</strong> 目标是训练一个能够自主调用工具、并根据模态输入动态决定如何关注特定音频或视频的智能体。</li>
<li><strong>提升推理效率：</strong> 通过在 KV 缓存中显式保留记忆，解决推理延迟的瓶颈问题。</li>
<li><strong>扩展性：</strong> OmniAgent 的框架具有良好的可扩展性，可以集成更多模态的工具，如图像 OCR 或传感器接口，以实现更全面的多模态感知。</li>
</ul>
<p><strong>总结：</strong>
OmniAgent 论文提出了一种创新的音频引导主动感知智能体框架，通过动态编排工具调用和利用音频线索进行粗细粒度感知，显著提升了音视频理解的精细度和准确性。该工作为解决跨模态对齐和细粒度理解的挑战提供了新的视角，并在多个基准测试中取得了优异的性能，标志着音视频理解领域向更主动、更智能的范式迈进。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning.</li>
<li>Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning.</li>
<li>Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23646v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23646v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23635v1'></a></p>
<h2 id="rethinking-the-spatio-temporal-alignment-of-end-to-end-3d-perception"><a href="https://arxiv.org/abs/2512.23635v1">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</a></h2>
<p><strong>Authors:</strong> Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于自动驾驶领域中时空对齐（Spatio-Temporal Alignment, STA）的论文。以下是根据您提供的PDF内容，对该论文的全面中文摘要：</p>
<p><strong>论文题目：</strong> Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception (重新思考端到端3D感知的时空对齐)</p>
<p><strong>研究问题/核心挑战：</strong>
论文的核心问题在于，当前端到端（E2E）自动驾驶感知系统中，时空对齐（STA）对于时间建模至关重要，它提供了宝贵的结构和纹理先验信息。然而，现有方法主要依赖于注意力机制，并采用单一的显式物理模型（如匀速模型）来简化运动模型，这忽略了物体运动状态和特征在不同类别和时间帧上的多样性，导致对齐效果不理想，尤其是在运动状态变化剧烈或语义信息受损的情况下。</p>
<p><strong>关键创新/方法贡献：</strong>
作者提出了<strong>HAT（Multiple Hypotheses Spatio-Temporal Alignment）</strong>模块，一个即插即用的时空对齐模块，旨在解决上述问题。HAT的主要创新点包括：</p>
<ol>
<li><strong>多假设生成器（Multiple Hypotheses Generator）：</strong> HAT不依赖单一运动模型，而是集成了多种显式运动模型（如匀速CV、静止STATIC、恒定加速度CA、恒定转弯速率和速度CTRV、恒定转弯速率和加速度CTRA），为历史实例生成多个空间锚点和运动感知的特征提案。这大大增加了对齐假设的多样性，能够更好地捕捉物体多样的运动状态。</li>
<li><strong>自适应解码器（Adaptive Decoder）：</strong> HAT引入了一个自适应解码器，能够根据查询（queries）中嵌入的语义和运动线索，动态地为这些多假设生成最优的对齐提案。该解码器通过加权融合的方式，从多个假设中选择最合适的锚点和特征，实现无监督的自适应对齐。</li>
<li><strong>显式-隐式混合对齐（Explicit-Implicit Mixing Alignment）：</strong> HAT结合了显式运动模型生成假设，并通过隐式的查询解码来选择最优假设，实现了显式和隐式方法的混合，增强了对齐的鲁棒性和适应性，克服了传统方法中手动调整超参数的限制。</li>
<li><strong>即插即用性（Plug-and-Play）：</strong> HAT被设计成一个模块化组件，可以无缝集成到现有的基于查询的3D时空检测器、跟踪器以及端到端自动驾驶方法中，无需额外的预训练或直接监督。</li>
</ol>
<p><strong>主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 在nuScenes数据集上，HAT显著提升了多种3D时空检测器和跟踪器的性能。<ul>
<li>在检测任务中，平均提升了0.7% NDS和0.6% mAP。</li>
<li>在跟踪任务中，平均提升了1.3% MOTA和1.0% AMOTA。</li>
<li>与DETR3D检测器结合时，在测试集上达到了<strong>46.0%的AMOTA</strong>，创下了当时最先进的跟踪结果。</li>
</ul>
</li>
<li><strong>端到端系统改进：</strong> 在一个以物体为中心的E2E AD方法（SparseDrive）中，HAT显著提高了3D感知性能（+1.3% mAP, +3.1% AMOTA），并<strong>将碰撞率降低了32%</strong>。</li>
<li><strong>鲁棒性验证：</strong> 在nuScenes-C（语义信息被故意破坏）的挑战性基准上，HAT通过增强运动建模，使得E2E感知和规划在语义受损的情况下更加鲁棒。</li>
<li><strong>运动建模的重要性：</strong> 实验结果有力地证明了，在E2E 3D感知中，运动建模与语义线索同等重要，甚至在某些情况下（如语义受损时）更为关键。</li>
</ul>
<p><strong>提及的局限性：</strong></p>
<ul>
<li><strong>对仅依赖边界框表示方法的有效性：</strong> 论文提到，HAT的多假设解码机制依赖于查询中固有的时间进步性运动线索。对于那些仅将解码的边界框作为唯一实例表示的方法，HAT的有效性可能会有所降低（如表7所示，在3DMOTFormer上的提升相对较小）。</li>
</ul>
<p><strong>潜在的未来研究方向：</strong>
论文中并未明确列出未来研究方向，但基于其工作，可以推测以下潜在方向：</p>
<ul>
<li><strong>更复杂的运动模型集成：</strong> 探索和集成更先进或更精细的运动模型，以应对更极端或更复杂的运动场景。</li>
<li><strong>自适应运动模型选择：</strong> 研究如何更智能地、甚至端到端地学习选择或组合不同的运动模型，而不是预先定义一个固定的库。</li>
<li><strong>跨模态融合中的STA：</strong> 将HAT的思路扩展到融合激光雷达（LiDAR）和摄像头（Camera）等多种传感器数据的场景，研究跨模态的时空对齐。</li>
<li><strong>更精细的查询表示：</strong> 进一步研究如何从查询中提取更丰富的运动和语义信息，以支持更有效的多假设解码。</li>
<li><strong>实时性优化：</strong> 虽然HAT的计算开销相对较低，但对于追求极致实时性的应用，仍可进一步优化其计算效率。</li>
</ul>
<p>总而言之，这篇论文通过提出HAT模块，成功地解决了现有E2E自动驾驶感知系统中时空对齐的局限性，通过多假设生成和自适应解码，实现了更鲁棒、更准确的物体对齐，并显著提升了整体系统的性能，尤其是在运动状态复杂或语义信息受损的场景下。这为未来研究提供了新的视角，强调了显式运动建模在端到端感知中的持续重要性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision.</li>
<li>It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23635v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23635v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23628v1'></a></p>
<h2 id="memorization-in-3d-shape-generation-an-empirical-study"><a href="https://arxiv.org/abs/2512.23628v1">Memorization in 3D Shape Generation: An Empirical Study</a></h2>
<p><strong>Authors:</strong> Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析：</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> Memorization in 3D Shape Generation: An Empirical Study
<strong>Authors:</strong> Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu
<strong>Categories:</strong> cs.CV, cs.LG
<strong>Published Date:</strong> 2025-12-29</p>
<p><strong>Abstract:</strong>
Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.</p>
<hr />
<p><strong>您的中文分析：</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>这篇论文的核心贡献在于，它首次提出了一种量化评估 3D 生成模型中“记忆化”（memorization）现象的框架。通过该框架，研究人员对现有 3D 生成方法进行了实证分析，并深入探究了数据和模型设计如何影响记忆化程度。最终，论文提出了一些简单有效的策略来降低记忆化，同时保持生成质量。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>量化评估框架的提出：</strong> 这是本研究最核心的创新点。在 3D 生成领域，对模型是否过度记忆训练数据（即“记忆化”）的量化评估一直是一个挑战。论文设计了一个专门的框架来解决这个问题，使得对记忆化程度的衡量成为可能。</li>
<li><strong>系统性的实证研究：</strong> 研究人员不仅提出了框架，还将其应用于现有方法，并进行了一系列受控实验。特别是针对一种特定的 latent vector-set (Vecset) diffusion model，他们系统地分析了数据（模态、多样性、条件粒度）和模型（引导尺度、Vecset长度、数据增强）等因素对记忆化的影响。这种细致的实证分析为理解 3D 生成模型的行为提供了宝贵的见解。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升生成模型的可信度和鲁棒性：</strong> 明确理解和量化记忆化，有助于开发者构建更可靠的 3D 生成模型。这对于防止训练数据泄露（例如，生成与训练数据高度相似的样本）至关重要，尤其是在涉及敏感或专有数据的场景下。</li>
<li><strong>促进生成结果的多样性：</strong> 过度记忆化往往会导致生成结果缺乏新颖性，倾向于复制训练数据中的样本。通过论文提出的方法减少记忆化，有望显著提升生成 3D 形状的多样性和创造性。</li>
<li><strong>指导模型设计和数据准备：</strong> 论文的发现为未来 3D 生成模型的研发提供了明确的方向。例如，在数据准备阶段，可以根据研究结果调整数据多样性或模态；在模型设计阶段，可以优化引导尺度、Vecset长度等超参数，并考虑引入特定的数据增强策略。</li>
<li><strong>推动 3D 生成领域的标准化评估：</strong> 该量化框架有望成为未来评估 3D 生成模型记忆化程度的行业标准，促进研究的公平性和可比性。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>3D 内容创作：</strong> 游戏开发、虚拟现实 (VR)、增强现实 (AR) 等领域需要大量高质量的 3D 模型。减少记忆化可以生成更具原创性和多样性的内容，丰富虚拟世界。</li>
<li><strong>计算机辅助设计 (CAD) 和工程：</strong> 在产品设计、建筑设计等领域，生成模型可以辅助设计师快速生成概念模型。确保生成模型的原创性对于知识产权保护和避免抄袭至关重要。</li>
<li><strong>医学影像和生物学：</strong> 生成模型可用于合成医学图像或分子结构，以扩充数据集或进行模拟。防止记忆化有助于生成更具临床或科学价值的、非直接复制的样本。</li>
<li><strong>数据增强和合成：</strong> 在数据稀缺的 3D 任务中，生成模型常用于数据增强。理解和控制记忆化可以确保增强的数据具有足够的独特性，从而更有效地提升下游任务的性能。</li>
<li><strong>隐私保护：</strong> 在使用包含敏感信息的 3D 数据进行训练时，防止模型记忆化是保护数据隐私的关键。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>聚焦于特定模型类型：</strong> 论文在深入分析时，主要使用了“latent vector-set (Vecset) diffusion model”。虽然这是一种先进的 3D 生成模型，但其研究结果的普适性可能需要进一步验证，即是否适用于其他类型的 3D 生成模型（如 GANs, VAEs, NeRF-based models 等）。</li>
<li><strong>“记忆化”的定义和衡量：</strong> 尽管论文提出了量化框架，但“记忆化”本身是一个相对复杂的概念。摘要中并未详细说明该框架的具体衡量指标和计算方法，其有效性和鲁棒性有待论文全文的详细阐述。</li>
<li><strong>“不降代生成质量”的权衡：</strong> 论文声称提出的策略可以在降低记忆化的同时不损害生成质量。然而，在实际应用中，降低记忆化与保持生成质量之间可能存在微妙的权衡。摘要中并未提供具体的量化证据来支持这一论点，需要通过论文中的实验结果来评估。</li>
<li><strong>数据模态的广泛性：</strong> 论文提到“数据模态”影响记忆化，但摘要并未具体说明研究中使用了哪些数据模态（例如，点云、网格、体素、隐式表示等），以及不同模态之间的具体影响差异。</li>
<li><strong>“指导尺度”的“适中”定义：</strong> 摘要中提到记忆化在“适中”的引导尺度下达到峰值。这个“适中”的范围是相对的，具体取决于模型和数据集，需要论文进一步明确。</li>
</ul>
<p>总而言之，这篇论文在 3D 生成领域具有重要的理论和实践意义。它通过提出创新的量化框架和进行深入的实证研究，为理解和控制 3D 生成模型的记忆化问题提供了关键洞察，并为构建更可靠、更多样化的 3D 生成模型指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23628v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23628v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23592v1'></a></p>
<h2 id="same-or-not-enhancing-visual-perception-in-vision-language-models"><a href="https://arxiv.org/abs/2512.23592v1">Same or Not? Enhancing Visual Perception in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Same or Not? Enhancing Visual Perception in Vision-Language Models”的全面摘要，重点关注其研究问题、创新之处、主要结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Same or Not? Enhancing Visual Perception in Vision-Language Models</p>
<p><strong>作者：</strong> Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkixoari</p>
<p><strong>摘要：</strong></p>
<p>这篇论文旨在解决当前视觉语言模型（VLMs）在细粒度视觉理解方面存在的不足。尽管VLMs在广泛的视觉推理方面表现出色，但它们往往过于粗粒度，存在视觉偏见，并且会忽略图像中的细微差别。现有的训练语料库往往侧重于一般的识别任务（如“这是猫还是狗？”），而非细粒度的感知能力，这加剧了这一局限性。</p>
<p><strong>1. 主要研究问题：</strong>
该研究的核心问题是：如何提升视觉语言模型对图像中细微视觉线索的感知能力，使其能够区分视觉上相似但并非完全相同的物体实例？</p>
<p><strong>2. 关键创新与方法论贡献：</strong></p>
<ul>
<li><strong>TWIN 数据集：</strong> 作者引入了一个大规模的图像对查询数据集，名为 TWIN（TWo-image INstance comparisons）。该数据集包含 561,000 个查询，要求模型判断两张视觉上相似的图像是否描绘的是同一个物体实例。TWIN 数据集涵盖了各种日常用品，并包含多样化的背景、视角和光照条件，旨在鼓励模型关注细微的视觉线索，如形状、纹理和局部几何特征。数据集的构建强调了“硬负样本”（hard negative pairs），即视觉上相似但属于不同实例的物体对，这对于训练模型区分细微差异至关重要。</li>
<li><strong>FGVQA 基准测试集：</strong> 为了评估模型在细粒度视觉理解方面的能力，作者提出了 FGVQA（Fine-grained Visual Question Answering）基准测试集。FGVQA 由 12,000 个查询组成，整合了来自多个领域的现有细粒度识别和检索数据集（如艺术品、动物、植物、地标和零售产品），旨在评估模型在不同领域泛化细粒度理解的能力。</li>
<li><strong>基于强化学习的后训练方法：</strong> 作者采用强化学习（RL）方法，在 TWIN 数据集上对现有的开源 VLMs（如 Qwen2.5-VL 和 InternVL3.5）进行后训练。RL 方法仅基于最终答案的正确性来计算奖励，这有助于模型在不牺牲通用 VQA 能力的情况下，提升细粒度感知能力。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>显著的细粒度理解提升：</strong> 在 TWIN 数据集上进行后训练的 VLMs 在 FGVQA 基准测试集上取得了显著的性能提升，最高可达 19.3%。这种提升不仅体现在模型对细微差别的识别能力上，还体现在模型能够更好地理解物体实例的细微之处，例如区分颜色、形状和纹理的细微差异。</li>
<li><strong>跨领域泛化能力：</strong> 后训练模型在 TWIN 数据集（主要包含日常用品）上获得的细粒度理解能力，能够有效泛化到 FGVQA 中未包含的领域，如艺术品、动物、植物和地标，这表明 TWIN 训练能够提升模型通用的细粒度感知能力。</li>
<li><strong>规模效应的重要性：</strong> 研究表明，TWIN 数据集的规模对于提升模型性能至关重要。随着训练样本数量的增加，模型在 FGVQA 上的准确率也随之提高，这支持了作者构建大规模数据集的决策。</li>
<li><strong>RL 方法的优势：</strong> 与监督微调（SFT）相比，基于 RL 的后训练方法在跨领域泛化方面表现更优，并且能更好地保留模型的通用能力，这与现有研究结果一致。</li>
<li><strong>对通用 VQA 性能无负面影响：</strong> 后训练模型在通用 VQA 基准测试集上的性能基本保持不变，甚至略有提升，这表明 TWIN 的训练并未导致模型在通用视觉推理能力上的退化。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>对极端视角变化的挑战：</strong> 尽管模型在细粒度理解上有所提升，但在处理极端视角变化时仍然存在挑战。</li>
<li><strong>对不完整视图的推断困难：</strong> 模型在处理动物等不完整视图时，仍然难以准确推断。</li>
<li><strong>对颜色和纹理细微差别的敏感度仍需提高：</strong> 在某些情况下，模型仍然难以区分颜色和纹理上的细微差异。</li>
<li><strong>未来研究方向的提示：</strong> 作者提到，可以探索更复杂的奖励函数（如多模态验证器），以提供更丰富的学习信号；研究更有效的“硬负样本”挖掘技术；以及整合 3D 表示来编码空间结构，以进一步提升模型性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的奖励函数设计：</strong> 探索使用多模态验证器等更复杂的奖励机制，以提供更丰富的学习信号，从而提升模型的感知能力。</li>
<li><strong>自动化“硬负样本”挖掘：</strong> 开发更有效的方法来自动挖掘“硬负样本”，例如利用模型驱动的数据引擎，以降低数据收集成本并提高训练效率。</li>
<li><strong>整合 3D 表示：</strong> 将 3D 表示方法融入模型中，以更显式地编码空间结构，从而更好地处理具有复杂几何形状的物体和场景。</li>
<li><strong>扩展 TWIN 数据集：</strong> 进一步扩展 TWIN 数据集的规模和多样性，涵盖更多类型的物体和场景，以应对更广泛的细粒度视觉理解挑战。</li>
<li><strong>开发更通用的细粒度理解模型：</strong> 基于 TWIN 和 FGVQA 的研究成果，开发能够处理更广泛细粒度视觉理解任务的通用模型。</li>
</ul>
<p>总而言之，这篇论文通过引入 TWIN 数据集和 FGVQA 基准测试集，并提出一种有效的基于 RL 的后训练方法，显著提升了视觉语言模型在细粒度视觉理解方面的能力。这项工作为推动 VLM 在更精细的视觉感知方面的发展奠定了重要基础，并为未来的研究提供了明确的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues.</li>
<li>To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23592v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23592v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23576v1'></a></p>
<h2 id="livetalk-real-time-multimodal-interactive-video-diffusion-via-improved-on-policy-distillation"><a href="https://arxiv.org/abs/2512.23576v1">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</a></h2>
<p><strong>Authors:</strong> Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation”论文的中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</p>
<p><strong>作者：</strong> Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在解决当前视频生成模型在实现实时、多模态交互式应用方面存在的瓶颈。现有的扩散模型虽然能生成高质量视频，但其迭代式双向注意力机制导致推理成本高昂，无法满足实时交互的需求。现有的蒸馏方法主要集中在文本到视频生成，在处理包含文本、图像和音频的多模态条件时，容易出现视觉伪影（如闪烁、黑帧、质量下降），导致人机交互不自然且效率低下。因此，研究的核心问题是如何在保持高质量视频生成的同时，实现低延迟、多模态条件下的实时视频扩散。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
*   <strong>改进的蒸馏方法：</strong> 论文提出了一种改进的在线策略蒸馏（on-policy distillation）方法，特别针对多模态视频扩散模型。该方法在现有“Self Forcing”蒸馏框架的基础上，重点优化了以下三个方面：
    *   <strong>精炼多模态条件：</strong> 强调了高质量条件输入的重要性，通过精心策划的参考图像（如使用Qwen-Image生成高质量帧）和文本提示（强调动态和面部表情）来提供更干净的训练信号。
    *   <strong>收敛的ODE初始化：</strong> 延长了ODE初始化阶段的训练时间，确保学生模型在进入在线DMD蒸馏之前，已经充分学习了去噪过程，为后续训练奠定稳固基础。
    *   <strong>最大化有限窗口内的学习：</strong> 采用了更积极的学习率策略（2倍基线）和调整的分类器引导（CFG）尺度，以在模型性能下降之前，最大化学习效率，尤其是在音频-视频同步方面。
*   <strong>Anchor-Heavy Identity Sinks (AHIS)：</strong> 提出了一种训练无关（training-free）的技术，用于在长时视频流中保持说话人身份的一致性。AHIS通过将KV缓存的一部分用作“身份锚点”（identity anchors），永久存储早期的高保真说话人帧，同时保留滚动KV令牌以维持上下文连贯性，从而有效抑制身份漂移。
*   <strong>并行化视频去噪和解码：</strong> 为了实现无缝的实时渲染，采用了流水线并行（pipeline parallelism）技术，在当前帧去噪的同时解码前一帧，显著降低了每帧的延迟。
*   <strong>LiveTalk系统：</strong> 基于上述改进的蒸馏模型，构建了一个名为LiveTalk的实时多模态交互式虚拟形象系统。该系统集成了Qwen3-Omni语言模型进行推理和音频生成，并利用改进的视频扩散模型生成同步的视频响应。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的效率提升：</strong> 论文提出的蒸馏模型实现了20倍的推理成本和延迟降低，吞吐量达到24.82 FPS，首帧延迟降至亚秒级（250倍加速）。这使得实时交互式视频生成成为可能。
*   <strong>高质量的视频生成：</strong> 蒸馏后的模型在视觉质量、美学和唇语同步方面，能够媲美甚至超越同等或更大规模的双向、多步基线模型（如OmniAvatar-1.3B）。
*   <strong>优越的多轮交互性能：</strong> 在新提出的多轮交互基准测试中，LiveTalk系统在多视频连贯性和内容质量方面显著优于Sora2和Veo3等先进模型，实现了从几分钟到实时响应的巨大飞跃， enabling seamless human-AI multimodal interaction。
*   <strong>身份保持能力：</strong> AHIS技术有效地解决了长时视频生成中的身份漂移问题，在几分钟的生成视频中保持了说话人外观的一致性。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>多模态条件下的不稳定性：</strong> 论文指出，在将“Self Forcing”蒸馏方法应用于多模态视频扩散模型时，存在显著的训练不稳定性，表现为视觉伪影。这表明多模态条件下的蒸馏比文本到视频更具挑战性。
*   <strong>数据质量的影响：</strong> 实验发现，参考图像的质量对蒸馏稳定性至关重要，低质量的图像会导致生成质量下降。
*   <strong>有限的学习窗口：</strong> 多模态条件下的DMD训练存在一个相对较短的有效学习窗口，模型在达到峰值性能后容易退化。
*   <strong>对条件输入的敏感性：</strong> 尽管论文强调了高质量条件的重要性，但对于不同类型和质量的输入条件，其影响程度可能需要进一步研究。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更广泛的多模态条件：</strong> 探索更复杂的、更具挑战性的多模态条件组合，例如包含更丰富的场景信息、动作捕捉数据等。
*   <strong>更鲁棒的蒸馏方法：</strong> 进一步研究和开发能够更稳定、更有效地处理多模态数据不确定性和复杂性的蒸馏技术。
*   <strong>更长时序的身份保持：</strong> 探索更先进的技术来应对更长时间跨度的身份保持问题，以及更复杂的面部表情和动作变化。
*   <strong>交互式内容生成：</strong> 将实时视频生成能力与更强大的对话和推理模型相结合，实现更具创造性和个性化的交互式内容生成。
*   <strong>系统级优化：</strong> 进一步优化整个LiveTalk系统的端到端性能，包括模型推理、音频处理和用户接口的协同工作。</p>
<p>总而言之，这篇论文通过提出一种改进的在线策略蒸馏方法和一系列关键技术，成功地解决了多模态视频扩散模型的实时性和交互性问题，并构建了一个性能卓越的LiveTalk系统，为未来实时人机交互式视频应用开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23576v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23576v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23568v1'></a></p>
<h2 id="thinkgen-generalized-thinking-for-visual-generation"><a href="https://arxiv.org/abs/2512.23568v1">ThinkGen: Generalized Thinking for Visual Generation</a></h2>
<p><strong>Authors:</strong> Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ThinkGen: Generalized Thinking for Visual Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> ThinkGen: Generalized Thinking for Visual Generation</p>
<p><strong>作者：</strong> Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei</p>
<p><strong>1. 研究问题与背景</strong></p>
<ul>
<li><strong>核心问题：</strong> 当前的多模态大语言模型（MLLMs）在理解任务中展现出了强大的链式思考（Chain-of-Thought, CoT）推理能力，但将其应用于生成任务时，仍面临挑战。现有的方法往往是场景特定的，缺乏泛化性和适应性，需要手动干预才能激活 CoT 推理，限制了其在多样化生成场景中的应用。</li>
<li><strong>背景：</strong> CoT 推理在数学、编程和视觉-语言理解等领域取得了显著进展。研究人员正积极探索如何将 CoT 推理应用于生成任务，以提升生成质量和智能化水平。</li>
</ul>
<p><strong>2. 主要创新与方法贡献</strong></p>
<ul>
<li><strong>ThinkGen 框架：</strong> 论文提出了 ThinkGen，这是首个“思考驱动”的视觉生成框架，能够显式地利用 MLLM 的 CoT 推理能力来处理各种生成场景。</li>
<li><strong>解耦架构：</strong> ThinkGen 采用解耦的架构，包含一个预训练的 MLLM 和一个扩散 Transformer（DiT）。MLLM 负责根据用户意图生成定制化的指令，DiT 则根据这些指令生成高质量图像。</li>
<li><strong>VGI-refine 模块：</strong> 为了解决 MLLM CoT 输出中的冗余信息，引入了 Visual Generation Instruction Refinement (VGI-refine) 模块。该模块提取 MLLM 推理链中的简洁指令信息，并与可学习的 Prepadding States 拼接，以自适应地调整 MLLM 的表示分布，使其更好地与 DiT 对齐。</li>
<li><strong>SepGRPO 训练范式：</strong> 提出了一种可分离的基于 GRPO（Proximal Policy Optimization）的训练范式（SepGRPO）。该范式交替进行 MLLM 和 DiT 的强化学习，允许在不同数据集上进行联合训练，从而实现对多种生成场景的有效 CoT 推理。这种分离设计提供了灵活的奖励设计和降低了学习复杂度。</li>
<li><strong>伪 CoT 注释生成：</strong> 针对现有生成数据集缺乏显式 <code>&lt;think&gt;</code> 标签的问题，开发了一种数据模板，从图像-文本对生成伪 CoT 注释，以优化 DiT 的推理能力。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>卓越的性能：</strong> 大量实验表明，ThinkGen 在多个生成基准上取得了稳健的、最先进（state-of-the-art）的性能。</li>
<li><strong>推理能力提升：</strong> 启用 CoT 推理后，ThinkGen 在推理基准（如 WISE: 0.55 → 0.76, RISEBench: 3.6 → 13.0）上取得了显著的性能提升。</li>
<li><strong>泛化性：</strong> ThinkGen 能够处理多种视觉生成任务，包括文本到图像生成、文本渲染、图像编辑和反思（reflection）等，展现了其强大的泛化能力。</li>
<li><strong>意义：</strong> 该研究是构建更智能、更通用的生成模型的重要一步，它有效地将推理能力与生成能力相结合，为视觉生成领域带来了新的思路。</li>
</ul>
<p><strong>4. 局限性</strong></p>
<ul>
<li><strong>对场景特定机制的依赖：</strong> 论文指出，现有方法在 CoT 机制上存在场景特定性，这限制了其泛化能力。ThinkGen 通过 SepGRPO 试图解决这一问题，但其在不同场景下的具体表现仍需进一步验证。</li>
<li><strong>数据需求：</strong> 尽管论文通过数据模板生成了伪 CoT 注释，但高质量的 CoT 数据仍然是训练的关键。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<ul>
<li><strong>更精细的 CoT 对齐：</strong> 进一步探索 MLLM 的 CoT 输出与 DiT 之间的更精细对齐机制，以优化指令遵循能力。</li>
<li><strong>更广泛的生成任务：</strong> 将 ThinkGen 的 CoT 推理能力扩展到更广泛的视觉生成任务，如视频生成、3D 内容生成等。</li>
<li><strong>模型效率优化：</strong> 探索更高效的训练和推理方法，以降低计算成本，使其更易于部署和应用。</li>
<li><strong>可解释性增强：</strong> 进一步研究 ThinkGen 的 CoT 推理过程，提高其可解释性，以便更好地理解模型的决策过程。</li>
</ul>
<p><strong>总结：</strong></p>
<p>ThinkGen 是一个创新的视觉生成框架，它通过显式地整合 MLLM 的 CoT 推理能力，显著提升了生成任务的智能化水平和性能。其解耦架构、VGI-refine 模块和 SepGRPO 训练范式是关键的技术贡献，使得模型能够有效地处理多样化的生成场景，并在多个基准上取得了最先进的成果。这标志着将推理能力融入生成模型的一个重要进展，为未来更强大、更通用的视觉生成模型奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios.</li>
<li>Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23568v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23568v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.23565v1'></a></p>
<h2 id="rxnbench-a-multimodal-benchmark-for-evaluating-large-language-models-on-chemical-reaction-understanding-from-scientific-literature"><a href="https://arxiv.org/abs/2512.23565v1">RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</a></h2>
<p><strong>Authors:</strong> Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke</p>
<p><strong>Published:</strong> 2025-12-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature”的全面摘要，重点关注其在计算机视觉领域的贡献：</p>
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决当前多模态大语言模型（MLLMs）在理解化学文献中密集、图形化的反应信息方面的能力不足。尽管MLLMs在科学发现领域展现出巨大潜力，但它们在解析包含复杂化学结构、反应条件和产物的反应方案图以及整合来自PDF文档中多模态信息（文本、图表、表格）以进行深入化学推理方面仍存在显著的“能力鸿沟”。现有基准测试往往侧重于文本或静态概念识别，未能充分模拟化学家阅读文献进行推理的真实认知过程。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>作者提出了 <strong>RxnBench</strong>，一个多层次的基准测试框架，专门用于评估MLLMs在化学反应理解方面的能力。RxnBench包含两个核心任务：</p>
<ul>
<li><strong>单图问答 (SF-QA):</strong> 专注于“感知”层面，包含1,525个问题，源自305个精心挑选的反应方案图。该任务旨在评估模型对化学图形语言的精细视觉感知和局部机制推理能力，要求模型精确提取反应物结构（SMILES）、反应条件、产率和立体化学信息。</li>
<li><strong>全文问答 (FD-QA):</strong> 专注于“推理”层面，是首个要求模型在完整的PDF文献上下文中回答问题的化学基准测试。它包含108篇文章的540个问题，要求模型整合文本描述、反应机理图和实验数据表，进行跨模态信息检索和综合推理。</li>
</ul>
<p>RxnBench的构建过程强调了领域专家的参与，通过精细的数据策选、标注和对抗性编辑，确保了问题的化学相关性和高难度，以避免模型利用测试技巧而非真正的化学理解。</p>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>能力鸿沟：</strong> 评估结果揭示了一个关键能力差距：MLLMs在提取显式文本信息方面表现出色，但在深度化学逻辑和精确结构识别方面存在困难。</li>
<li><strong>推理模型的优势：</strong> 具备推理能力的模型（如“Think”模型）在SF-QA任务上显著优于标准架构，尤其是在需要多步推理的复杂任务中。</li>
<li><strong>结构识别瓶颈：</strong> 尽管在推理方面有所提升，但所有模型在“结构识别”任务上都表现出显著的性能下降，这表明精确的视觉分子编码仍然是一个普遍的瓶颈。</li>
<li><strong>FD-QA的挑战性：</strong> FD-QA任务的难度极高，即使是顶尖模型也难以达到50%的准确率，凸显了综合理解长上下文多模态科学文献的挑战。</li>
<li><strong>领域特定编码器的需求：</strong> 研究结果强调了开发领域特定的视觉编码器以解决结构识别瓶颈的紧迫性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>结构识别的普遍瓶颈：</strong> 即使是领先的模型，在精确识别和解析化学结构方面也存在显著不足，这限制了它们在需要精细结构理解的任务上的表现。</li>
<li><strong>FD-QA的低准确率：</strong> 没有模型能在FD-QA任务上达到50%的准确率，表明当前MLLMs在处理复杂、长上下文的科学文献综合推理方面仍有很大提升空间。</li>
<li><strong>模型在中文处理上的细微差异：</strong> 虽然大多数顶尖模型在英中文基准测试上表现出高度一致性，但一些较小的模型在处理中文提示时会出现轻微的性能下降。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>领域特定视觉编码器：</strong> 开发能够实现像素级精确的化学分子图谱识别的专用视觉编码器。</li>
<li><strong>集成外部化学工具：</strong> 将MLLMs与外部化学工具（如RDKit、计算器代理）集成，以验证模型生成的化学信息，减少幻觉。</li>
<li><strong>主动式智能体工作流：</strong> 从被动问答转向更主动的智能体工作流，使模型能够自主导航、查询和验证文献数据，以实现真正的自主AI化学家。</li>
</ul>
<p><strong>对计算机视觉领域的贡献：</strong></p>
<p>RxnBench为评估MLLMs在理解复杂、非自然图像（化学反应方案图）方面的能力提供了一个新颖且具有挑战性的基准。它不仅测试了基本的图像识别能力，更深入地考察了模型如何将视觉信息与领域知识相结合，进行精细的结构解析、机制推理和跨模态信息综合。该基准测试的建立和评估结果，突出了当前计算机视觉模型在理解高度结构化、语义丰富的科学图像方面的局限性，并为未来开发更强大的视觉理解模型指明了方向，尤其是在需要精确结构识别和深度逻辑推理的科学领域。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.23565v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.23565v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-31 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
