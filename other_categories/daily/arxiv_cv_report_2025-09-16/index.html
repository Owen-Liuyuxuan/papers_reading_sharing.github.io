<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-16 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-15/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-17/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-16">Arxiv Computer Vision Papers - 2025-09-16</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-15" class="nav-link">Arxiv 计算机视觉每日报告执行摘要 (2025-09-15)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review" class="nav-link">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a>
                </li>
                <li class="nav-item">
                    <a href="#domain-adaptive-pretraining-improves-primate-behavior-recognition" class="nav-link">Domain-Adaptive Pretraining Improves Primate Behavior Recognition</a>
                </li>
                <li class="nav-item">
                    <a href="#railsafenet-visual-scene-understanding-for-tram-safety" class="nav-link">RailSafeNet: Visual Scene Understanding for Tram Safety</a>
                </li>
                <li class="nav-item">
                    <a href="#fs-sam2-adapting-segment-anything-model-2-for-few-shot-semantic-segmentation-via-low-rank-adaptation" class="nav-link">FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</a>
                </li>
                <li class="nav-item">
                    <a href="#ram-robust-representation-learning-via-adaptive-mask-for-all-in-one-image-restoration" class="nav-link">RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</a>
                </li>
                <li class="nav-item">
                    <a href="#integrating-prior-observations-for-incremental-3d-scene-graph-prediction" class="nav-link">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#sam-ttt-segment-anything-model-via-reverse-parameter-configuration-and-test-time-training-for-camouflaged-object-detection" class="nav-link">SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#drv-a-hierarchical-perception-temporal-cognition-framework-to-diagnose-video-hallucination-by-fine-grained-spatial-temporal-grounding" class="nav-link">Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</a>
                </li>
                <li class="nav-item">
                    <a href="#specvlm-fast-speculative-decoding-in-vision-language-models" class="nav-link">SpecVLM: Fast Speculative Decoding in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#seg2track-sam2-sam2-based-multi-object-tracking-and-segmentation-for-zero-shot-generalization" class="nav-link">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-16">Arxiv Computer Vision Papers - 2025-09-16</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-15">Arxiv 计算机视觉每日报告执行摘要 (2025-09-15)</h2>
<p><strong>概述：</strong></p>
<p>今天的 Arxiv 计算机视觉论文主要围绕 <strong>基础模型（特别是 SAM 及其变体）的适应与增强、多模态学习（尤其是视觉-语言模型）、3D 感知与场景理解以及鲁棒性与泛化能力</strong> 这几个核心主题展开。显著趋势是研究人员致力于将强大的预训练模型（如 SAM）应用于更具体的下游任务，并探索其在少样本、零样本和领域适应场景下的潜力。</p>
<p><strong>主要主题与趋势：</strong></p>
<ol>
<li><strong>SAM (Segment Anything Model) 的持续演进与应用：</strong> 今天的报告中有四篇论文直接或间接涉及 SAM 或其下一代版本 SAM2。这表明 SAM 仍然是计算机视觉领域的热点，研究重点已从模型本身转向如何高效地将其适应到特定任务（如少样本语义分割、伪装目标检测、多目标跟踪与分割），以及如何解决其在特定场景下的局限性。</li>
<li><strong>多模态学习与视觉-语言模型 (VLM)：</strong> SpecVLM 和 Dr.V 两篇论文凸显了 VLM 在效率和可靠性方面的研究进展。SpecVLM 关注推理速度优化，而 Dr.V 则致力于诊断和解决 VLM 中的“幻觉”问题，这对于提升 VLM 的实际应用价值至关重要。</li>
<li><strong>3D 感知与场景理解：</strong> 3D 人体姿态与形状估计以及增量式 3D 场景图预测是该领域的亮点。这表明对真实世界三维信息的理解和建模仍然是重要的研究方向，尤其是在机器人、自动驾驶等应用中。</li>
<li><strong>鲁棒性、泛化与领域适应：</strong> 多篇论文关注模型在不同领域、不同条件下的鲁棒性和泛化能力。例如，域适应预训练用于灵长类行为识别，以及 RAM++ 旨在通过自适应掩码实现全能图像恢复的鲁棒性。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation" (Bernardo Forni et al.)：</strong> 这篇论文代表了将强大的基础模型（SAM2）高效适应到数据稀缺任务（少样本语义分割）的最新尝试。通过低秩适应，它有望在保持模型性能的同时显著降低微调成本，具有很高的实用价值。</li>
<li><strong>"Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding" (Meng Luo et al.)：</strong> 解决 VLM 的“幻觉”问题是当前 VLM 领域面临的关键挑战。Dr.V 提出的分层框架，通过细粒度的时空定位来诊断视频幻觉，为提升 VLM 的可靠性和可信度提供了新的思路。</li>
<li><strong>"SpecVLM: Fast Speculative Decoding in Vision-Language Models" (Haiduo Huang et al.)：</strong> 随着 VLM 模型的规模不断扩大，推理效率成为瓶颈。SpecVLM 引入的推测解码技术，有望显著加速 VLM 的推理过程，对于 VLM 的实际部署具有重要意义。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>SAM/SAM2 的轻量化与高效适应：</strong> 随着 SAM 模型的普及，如何以更低的计算成本和更少的数据将其适应到特定任务，将是未来的重要方向（如 FS-SAM2 中的低秩适应）。</li>
<li><strong>VLM 的可靠性与可解释性：</strong> 解决 VLM 的“幻觉”问题（如 Dr.V）以及提升其决策过程的可解释性，将是 VLM 走向更广泛应用的关键。</li>
<li><strong>多模态融合的鲁棒性：</strong> 如何在复杂多变的环境中，有效地融合不同模态（如 LiDAR 点云、图像）的信息，并保持模型的鲁棒性，仍是活跃的研究领域。</li>
<li><strong>增量式学习在3D场景理解中的应用：</strong> 随着机器人和自动驾驶系统在动态环境中运行，如何让模型能够持续学习和更新其对3D场景的理解，将变得越来越重要。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于关注基础模型适应和高效利用的研究人员：
*   <strong>"FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation"</strong>
*   <strong>"Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization"</strong></p>
<p>对于关注多模态学习和 VLM 挑战的研究人员：
*   <strong>"Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding"</strong>
*   <strong>"SpecVLM: Fast Speculative Decoding in Vision-Language Models"</strong></p>
<p>对于关注 3D 感知和场景理解的研究人员：
*   <strong>"3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review"</strong> (作为该领域的综述，有助于全面了解)
*   <strong>"Integrating Prior Observations for Incremental 3D Scene Graph Prediction"</strong></p>
<p>这份摘要旨在帮助您快速把握今日 Arxiv 计算机视觉领域的最新动态和重要进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></li>
<li><a href="#2509.12193v1">Domain-Adaptive Pretraining Improves Primate Behavior Recognition</a></li>
<li><a href="#2509.12125v1">RailSafeNet: Visual Scene Understanding for Tram Safety</a></li>
<li><a href="#2509.12105v1">FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</a></li>
<li><a href="#2509.12039v1">RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</a></li>
<li><a href="#2509.11895v1">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a></li>
<li><a href="#2509.11884v1">SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection</a></li>
<li><a href="#2509.11866v1">Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</a></li>
<li><a href="#2509.11815v1">SpecVLM: Fast Speculative Decoding in Vision-Language Models</a></li>
<li><a href="#2509.11772v1">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.12197v1'></a></p>
<h2 id="3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review"><a href="https://arxiv.org/abs/2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></h2>
<p><strong>Authors:</strong> Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet撰写的论文“3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review”的全面摘要。</p>
<hr />
<p><strong>论文摘要：3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</strong></p>
<p>这篇综述论文全面回顾了从“野外”（in-the-wild）LiDAR点云中进行3D人体姿态估计（3D HPE）和人体网格恢复（HMR）的现有方法。作者旨在为该领域提供一个结构化的分类和深入的分析，以促进LiDAR-based人体理解的进一步发展。</p>
<p><strong>1. 主要问题或研究问题：</strong>
论文主要关注的核心问题是如何从稀疏、不规则、有噪声且可能存在遮挡的“野外”LiDAR点云中准确地估计3D人体姿态和恢复详细的人体网格。尽管LiDAR在自动驾驶和城市环境感知中具有隐私保护、精确深度测量和对光照条件鲁棒等优势，但其数据特性（如稀疏性、不均匀采样、不完整性）给3D人体理解带来了巨大挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文的主要贡献在于：
*   <strong>结构化分类法：</strong> 提出了一个结构化的分类法，将现有方法根据其学习范式（监督、弱监督、无监督）、输入模态（仅LiDAR或多模态融合）、网络架构（如PointNet变体、Transformer）以及处理稀疏性和时间信息的方式进行分类。
*   <strong>方法分析：</strong> 详细分析了每种方法的优势、局限性和设计选择，特别是它们如何应对LiDAR点云的稀疏性和不规则性。例如，针对稀疏性，方法包括结构化投影、全稀疏体素处理、混合BEV+体素融合、密度感知注意力Transformer等。
*   <strong>数据集和评估指标的统一：</strong> 对三个最广泛使用的LiDAR数据集（Waymo Open Dataset, SLOPER4D, Human-M3）进行了定量比较，详细阐述了它们的特性。同时，统一了所有评估指标的定义，确保了公平比较的基础。
*   <strong>基准测试：</strong> 建立了针对3D HPE和HMR任务在这三个数据集上的基准测试表，为该领域的进展提供了参考。
*   <strong>开放挑战和未来方向：</strong> 概述了LiDAR-based人体理解面临的开放挑战和潜在研究方向。
*   <strong>配套网页：</strong> 维护了一个配套网页，根据论文的分类法组织论文，并持续更新。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文通过对现有方法的全面梳理和基准测试，揭示了以下重要发现：
*   <strong>多模态融合的有效性：</strong> 许多弱监督方法通过融合LiDAR与RGB图像或IMU信号来弥补LiDAR数据的不足，提高了姿态估计的鲁棒性。
*   <strong>Transformer架构的兴起：</strong> Transformer在处理LiDAR点云的长期依赖性和不规则结构方面展现出巨大潜力，成为许多最新方法的骨干。
*   <strong>合成数据的重要性：</strong> 鉴于真实LiDAR数据集的稀缺性，合成数据生成和数据增强是训练模型、特别是预训练模型以学习人体中心先验的关键策略。
*   <strong>弱监督学习的潜力：</strong> 弱监督方法通过利用2D标注、伪标签、投影一致性等策略，有效缓解了对大量3D标注的需求。
*   <strong>时间一致性：</strong> 建模时间序列信息对于提高姿态估计的准确性和鲁棒性至关重要，尤其是在处理遮挡和运动预测时。</p>
<p>这些结果为研究人员提供了该领域当前技术水平的清晰视图，并指出了未来研究的有效路径。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据稀缺性：</strong> 缺乏大规模、高质量标注的LiDAR人体姿态和网格数据集是核心挑战。
*   <strong>多模态方法的依赖性：</strong> 现有弱监督方法仍高度依赖RGB图像或IMU信号等辅助模态，降低对这些模态的依赖是未来的方向。
*   <strong>相机参数依赖：</strong> 当前多模态方法严重依赖精确的相机参数进行2D-3D对应，这在实际应用中带来了挑战。
*   <strong>域间隙问题：</strong> 不同LiDAR传感器特性（如点密度、范围、噪声模式）和扫描模式（NRS与RMB）导致的数据集之间存在域间隙，影响模型的泛化能力。
*   <strong>合成数据的局限性：</strong> 当前合成数据生成存在域不匹配（如AMASS主要包含室内姿态，而WOD和SLOPER4D是室外场景）和真实感差距（射线投射未能完全捕捉真实LiDAR传感器的噪声、稀疏性和视角特性）。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>减少对辅助模态的依赖：</strong> 探索仅使用LiDAR数据实现弱监督HPE和HMR的方法，例如通过伪标签、自训练或对比学习。
*   <strong>整合时间信息：</strong> 进一步利用LiDAR帧序列中的时间线索，以增强姿态估计的准确性，而无需额外监督。
*   <strong>更真实的合成数据生成：</strong> 开发能够直接从真实世界分布中生成合成LiDAR数据的方法，特别是利用扩散模型等生成式模型。
*   <strong>数据高效学习：</strong> 采用自监督预训练，然后用少量监督数据对模型进行微调，以提高LiDAR-based HPE/HMR的数据效率。
*   <strong>弱监督HMR：</strong> 针对HMR任务开发弱监督方法，这是一个尚未充分探索但前景广阔的方向。
*   <strong>消除相机参数依赖：</strong> 用可学习模块取代对显式校准的需求，实现端到端的对齐。
*   <strong>域适应技术：</strong> 开发能够处理不同LiDAR传感器特性和扫描模式之间域间隙的鲁棒架构和适应策略，以提高模型的泛化能力。</p>
<hr />
<p>这篇综述为LiDAR-based 3D人体理解领域提供了一个全面的路线图，不仅总结了现有成就，更清晰地指明了未来的研究方向和挑战，对于推动该领域的发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds.</li>
<li>Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12197v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12197v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12193v1'></a></p>
<h2 id="domain-adaptive-pretraining-improves-primate-behavior-recognition"><a href="https://arxiv.org/abs/2509.12193v1">Domain-Adaptive Pretraining Improves Primate Behavior Recognition</a></h2>
<p><strong>Authors:</strong> Felix B. Mueller, Timo Lueddecke, Richard Vogg, Alexander S. Ecker</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, I.4.8; I.2.10; I.5</p>
<p><strong>Abstract:</strong></p>
<p>Computer vision for animal behavior offers promising tools to aid research in
ecology, cognition, and to support conservation efforts. Video camera traps
allow for large-scale data collection, but high labeling costs remain a
bottleneck to creating large-scale datasets. We thus need data-efficient
learning approaches. In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior. On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.
mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data. We show that most of the performance gain stems from the
DAP. Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples. Code is available at
https://github.com/ecker-lab/dap-behavior</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Felix B. Mueller等人撰写的论文“Domain-Adaptive Pretraining Improves Primate Behavior Recognition”的全面摘要。</p>
<hr />
<h3 id="domain-adaptive-pretraining-improves-primate-behavior-recognition_1">论文摘要：Domain-Adaptive Pretraining Improves Primate Behavior Recognition</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决动物行为识别领域中的一个核心挑战：如何利用计算机视觉技术，在面对高昂的标注成本和缺乏大规模标注数据集的情况下，实现对灵长类动物行为的高效、准确识别。特别是，视频相机陷阱（camera traps）能够收集大量野外数据，但这些数据的标注成本极高，限制了数据驱动学习方法的应用。因此，论文寻求数据高效的学习方法来改进灵长类行为识别。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文的核心创新在于结合了自监督学习和领域自适应预训练（Domain-Adaptive Pretraining, DAP）来解决上述问题。具体贡献包括：
*   <strong>利用预训练的V-JEPA模型：</strong> 研究首先利用一个在人类视频数据上预训练好的V-JEPA（Video Joint Embedding Predictive Architecture）模型作为骨干网络。V-JEPA是一种基于掩码自编码器（masked autoencoding）的自监督学习方法，能够学习视频数据的有效表示。
*   <strong>引入领域自适应预训练（DAP）：</strong> 在V-JEPA模型的基础上，研究进一步在无标签的灵长类动物视频数据上进行DAP。这意味着模型在目标领域（灵长类行为）的未标注数据上继续进行自监督预训练，以更好地适应目标领域的特征分布。
*   <strong>灵长类中心采样策略：</strong> 针对灵长类数据集的特点（如视频中可能包含多个灵长类个体或个体较小），论文采用了一种灵长类中心采样策略。通过使用Grounding DINO等开放词汇目标检测器，裁剪出视频中包含灵长类动物的部分，从而在不增加输入尺寸的情况下捕获更精细的个体细节。
*   <strong>注意力分类器：</strong> 在冻结的预训练骨干网络之上，训练了一个多头交叉注意力（multihead cross-attention）分类器，用于最终的行为识别任务。</p>
<p><strong>3. 主要结果及其重要性：</strong>
该方法在两个大猿行为数据集（PanAf和ChimpACT）上取得了显著的性能提升：
*   <strong>PanAf500数据集：</strong> 在Top-1准确率上，相比之前发布的最新模型，性能提升了6.1个百分点。
*   <strong>ChimpACT数据集：</strong> 在平均精度均值（mAP）上，性能提升了6.3个百分点。
*   <strong>DAP的关键作用：</strong> 论文明确指出，大部分性能提升来源于领域自适应预训练（DAP），而非仅仅使用预训练的V-JEPA模型。这强调了在目标领域数据上进行无标签预训练的重要性。
*   <strong>数据高效性：</strong> 结果表明，DAP不需要标注样本，这使其成为一种非常有前景的数据高效学习方法，极大地降低了创建大规模标注数据集的成本。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>依赖边界框裁剪：</strong> 当前方法依赖于将输入视频裁剪到边界框内。虽然这有助于更好地利用ViT模型有限的输入分辨率，但它忽略了全局上下文信息，并且对于每个感兴趣区域都需要进行一次前向传播。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>DAP的规模化：</strong> 未来的工作可以探索如何进一步扩展DAP的应用，结合更大、更多样化的动物数据源，以学习更好的表示并辅助行为识别。
*   <strong>尾部类别性能提升：</strong> 结合更多数据源有望改善对尾部类别（即数据量较少的行为类别）的识别性能，因为这些类别通常难以从少量数据中学习到良好的表示。
*   <strong>全局上下文的利用：</strong> 解决当前方法因裁剪而忽略全局上下文的问题，可能通过引入多尺度特征融合或更复杂的注意力机制来实现。</p>
<hr />
<p>总而言之，这篇论文成功地展示了自监督学习与领域自适应预训练相结合，能够显著提升灵长类动物行为识别的性能，尤其是在数据标注成本高昂的野外环境中。DAP作为一种无需标注样本的方法，为动物行为研究和保护提供了强大的新工具。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior.</li>
<li>On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt.</li>
<li>We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data.</li>
<li>We show that most of the performance gain stems from the
DAP.</li>
<li>Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12193v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12193v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12125v1'></a></p>
<h2 id="railsafenet-visual-scene-understanding-for-tram-safety"><a href="https://arxiv.org/abs/2509.12125v1">RailSafeNet: Visual Scene Understanding for Tram Safety</a></h2>
<p><strong>Authors:</strong> Ing. Ondrej Valach, Ing. Ivan Gruber</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, 68T45 (Primary), 68T07, I.4.8</p>
<p><strong>Abstract:</strong></p>
<p>Tram-human interaction safety is an important challenge, given that trams
frequently operate in densely populated areas, where collisions can range from
minor injuries to fatal outcomes. This paper addresses the issue from the
perspective of designing a solution leveraging digital image processing, deep
learning, and artificial intelligence to improve the safety of pedestrians,
drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions. Using only
monocular video, the system identifies rails, localises nearby objects and
classifies their risk by comparing projected distances with the standard 1435mm
rail gauge. Experiments on the diverse RailSem19 dataset show that a
class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),
while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated
at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore
delivers accurate, annotation-light scene understanding that can warn drivers
before dangerous situations escalate. Code available at
https://github.com/oValach/RailSafeNet.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ing. Ondrej Valach和Ing. Ivan Gruber撰写的论文“RailSafeNet: Visual Scene Understanding for Tram Safety”的全面摘要。</p>
<hr />
<p><strong>论文摘要：RailSafeNet: 有轨电车安全视觉场景理解</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决有轨电车与行人、骑自行车者、驾驶员、宠物和有轨电车乘客之间的安全互动问题。鉴于有轨电车经常在人口稠密地区运行，碰撞可能导致从轻微伤害到致命后果，因此提高有轨电车运行安全性是一个重要的挑战。核心研究问题是如何利用数字图像处理、深度学习和人工智能，通过实时场景理解来预警潜在的轨道入侵，从而提高安全性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
RailSafeNet框架提出了以下关键创新和方法论贡献：
*   <strong>实时融合框架：</strong> 提出了一个名为RailSafeNet的实时框架，它融合了语义分割、目标检测和一个基于规则的“距离评估器”（Distance Assessor）来识别轨道入侵。
*   <strong>单目视频距离估计：</strong> 引入了一种无需任何关于捕获、相机或设置参数的先验知识，仅通过单目图像即可进行轨道距离估计的方法。这通过利用标准1435毫米的轨距作为可靠的距离参考来实现。
*   <strong>定制分割处理：</strong> 采用了一种定制的分割处理方法，包括数据类别过滤和掩码后处理，这在RailSem19数据集上取得了优于原始论文的分割结果。
*   <strong>距离评估器系统：</strong> 提出了一个“距离评估器”系统，它处理场景分割和目标检测的输出，以准确估计物体与轨道的距离，并根据其接近程度对物体进行风险分类（例如，使用颜色编码区分不同级别的危险）。
*   <strong>无需深度传感器或LiDAR：</strong> 该方法不需要相机校准、深度传感器或LiDAR，使其成为一种低成本且易于部署的解决方案。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>语义分割性能：</strong> 在多样化的RailSem19数据集上，经过类别过滤的SegFormer B3模型在交并比（IoU）方面达到了65%，超越了现有基准。
*   <strong>目标检测性能：</strong> 经过微调的YOLOv8模型在IoU阈值为0.50时，平均精度（mAP）达到了75.6%。
*   <strong>距离评估准确性：</strong> 实际验证实验表明，即使在弯曲轨道或倾斜相机视角等挑战性条件下，系统也能准确估计距离，偏差仅限于几厘米。
*   <strong>实际意义：</strong> RailSafeNet能够提供准确、轻量级标注的场景理解，可以在危险情况升级之前向驾驶员发出警告，从而显著降低事故风险和严重性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>训练数据质量：</strong> 论文指出，尽管进行了类别过滤和掩码后处理，但原始RailSem19数据集的分割掩码仍存在不理想之处，例如不准确的标注、未检测或部分检测的物体，以及不理想分割类别之间看似任意的边缘。这影响了模型的性能。
*   <strong>类别不平衡：</strong> 目标检测模型在训练时面临类别不平衡问题，例如“人”和“汽车”等关键物体类别在数据集中代表性不足，导致这些关键物体的准确性较低。
*   <strong>复杂场景的偏差：</strong> 在弯曲轨道等复杂场景中，由于不完美的轨道分割，水平横截面测量可能导致轨道显得更宽，略微夸大距离估计，尤其是在图像的右侧。左侧的窄分割可能使关键区域向内偏移，导致约3-4厘米的偏差。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>训练数据集增强：</strong> 未来的改进将通过增强训练数据集来实现，特别是更精确的轨道和轨床标注。
*   <strong>集成到自动制动系统：</strong> 提出的框架有望作为有轨电车操作员的实用日常工具，并有可能集成到自动制动系统等中，以进一步降低事故风险和严重性。
*   <strong>更广泛的部署和验证：</strong> 尽管在比尔森进行了实际验证，但未来可以在更多样化的真实世界场景中进行部署和验证，以进一步证明其鲁棒性。</p>
<hr />
<p>总而言之，RailSafeNet为有轨电车安全提供了一个创新且实用的计算机视觉解决方案，通过融合深度学习模型和基于规则的距离评估，实现了对轨道入侵的实时、准确预警，为城市交通安全带来了显著的进步。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12125v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12125v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12105v1'></a></p>
<h2 id="fs-sam2-adapting-segment-anything-model-2-for-few-shot-semantic-segmentation-via-low-rank-adaptation"><a href="https://arxiv.org/abs/2509.12105v1">FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</a></h2>
<p><strong>Authors:</strong> Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Few-shot semantic segmentation has recently attracted great attention. The
goal is to develop a model capable of segmenting unseen classes using only a
few annotated samples. Most existing approaches adapt a pre-trained model by
training from scratch an additional module. Achieving optimal performance with
these approaches requires extensive training on large-scale datasets. The
Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and
video segmentation with a modular design. In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank
Adaptation (LoRA) to the original modules in order to handle the diverse images
typically found in standard datasets, unlike the temporally connected frames
used in SAM2's pre-training. With this approach, only a small number of
parameters is meta-trained, which effectively adapts SAM2 while benefiting from
its impressive segmentation performance. Our method supports any K-shot
configuration. We evaluate FS-SAM2 on the PASCAL-5<script type="math/tex">^i</script>, COCO-20<script type="math/tex">^i</script> and
FSS-1000 datasets, achieving remarkable results and demonstrating excellent
computational efficiency during inference. Code is available at
https://github.com/fornib/FS-SAM2</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Bernardo Forni等人撰写的论文“FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：FS-SAM2: 通过低秩适应将Segment Anything Model 2应用于少样本语义分割</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决少样本语义分割（Few-Shot Semantic Segmentation, FSS）的挑战。FSS的目标是开发一个模型，能够仅使用少量带标注的样本来分割未见过的类别。现有方法通常需要从头开始训练额外的模块，这需要在大规模数据集上进行大量训练才能达到最佳性能。作者提出，如何有效地将像Segment Anything Model 2 (SAM2) 这样的基础模型适应到FSS任务中，使其在处理多样化图像（而非SAM2预训练中使用的时序连接帧）时仍能保持高性能和计算效率，是一个关键问题。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
该论文提出了FS-SAM2，其核心创新点在于：
*   <strong>SAM2的视频能力重定向：</strong> FS-SAM2巧妙地将SAM2的视频分割能力直接重新用于FSS任务。它将支持图像视为带标注的视频帧，而查询图像则作为后续待分割的帧，利用SAM2的内存注意力模块进行像素级匹配。
*   <strong>低秩适应（LoRA）的应用：</strong> 为了高效且鲁棒地适应SAM2模型，作者将LoRA应用于SAM2的原始模块，包括图像编码器、内存编码器和内存注意力模块。LoRA通过引入少量可训练参数（仅元训练这些参数，而保持原始参数冻结）来微调选定的线性层，从而在处理标准数据集中常见的各种图像时，有效适应模型并增强特征提取，而无需从头训练任何特定模块。
*   <strong>K-shot配置的通用支持：</strong> 该方法支持任何K-shot配置，无需为不同的K值训练不同的模型。所有K个支持图像都被视为先前标注的帧，并以相同框架处理。</p>
<p><strong>3. 主要结果及其意义</strong>
FS-SAM2在PASCAL-5²、COCO-20²和FSS-1000数据集上进行了广泛评估，取得了显著成果：
*   <strong>卓越的性能：</strong> 在PASCAL-5²数据集的1-shot场景中，FS-SAM2实现了73.4%的mIoU，超越了最可比较的方法VRP-SAM 1.5%。在COCO-20²数据集上，FS-SAM2在1-shot基准测试中优于VRP-SAM 1.4%，并在5-shot基准测试中表现出显著提升。
*   <strong>计算效率：</strong> 该方法在推理过程中表现出卓越的计算效率，这得益于LoRA仅微调少量参数，并避免了额外骨干网络或模块的引入。
*   <strong>泛化能力和鲁棒性：</strong> 在FSS-1000数据集和域迁移场景（训练数据与测试数据存在显著域差距）上的评估表明，FS-SAM2具有很强的鲁棒性，LoRA成功地将模型适应到少样本设置。
*   <strong>定性结果：</strong> 定性比较显示，FS-SAM2能够生成准确完整的掩码，捕捉精细细节和整个对象区域，甚至能修正地面真值标注中存在的微小不准确性，而SAM2在处理不相似的支持-查询对时则表现不佳。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>5-shot训练的优化空间：</strong> 论文指出，虽然FS-SAM2在5-shot设置中有所改进，但通过明确地在5-shot机制下训练模型，可能会获得进一步的提升。
*   <strong>跨类别通信能力：</strong> SAM2本身缺乏跨类别通信能力，这可能限制了FS-SAM2在多类别少样本分割任务中的表现，而这是一个相对未被充分探索但具有实际优势的任务。
*   <strong>与DINOv2骨干网络的比较：</strong> 论文提到，像GF-SAM这样使用DINOv2-L骨干网络（在包含PASCAL-VOC的数据集上预训练）的方法，虽然资源密集度更高，但在某些情况下仍能取得领先性能，这暗示了基础模型选择和预训练数据的重要性。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>替代的5-shot训练方法：</strong> 探索更有效的5-shot训练策略，以进一步提升模型在多样本场景下的性能。
*   <strong>更参数高效的微调策略：</strong> 深入研究其他参数高效的微调方法，以在保持高性能的同时，进一步减少可训练参数的数量。
*   <strong>扩展到多类别设置：</strong> 将FS-SAM2方法扩展到多类别少样本语义分割任务，解决SAM2在跨类别通信方面的限制，以满足更广泛的实际应用需求。</p>
<hr />
<p>总而言之，这篇论文通过巧妙地重定向SAM2的视频分割能力并结合LoRA进行高效微调，为少样本语义分割提供了一个简单而有效的框架。FS-SAM2在保持计算效率的同时，展现出与现有最先进方法相当甚至更优的性能，为利用大型基础模型解决低数据量计算机视觉任务开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task.</li>
<li>Our method supports any K-shot
configuration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12105v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12105v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12039v1'></a></p>
<h2 id="ram-robust-representation-learning-via-adaptive-mask-for-all-in-one-image-restoration"><a href="https://arxiv.org/abs/2509.12039v1">RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</a></h2>
<p><strong>Authors:</strong> Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, Chongyi Li</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This work presents Robust Representation Learning via Adaptive Mask (RAM++),
a two-stage framework for all-in-one image restoration. RAM++ integrates
high-level semantic understanding with low-level texture generation to achieve
content-oriented robust restoration. It addresses the limitations of existing
degradation-oriented methods in extreme scenarios (e.g., degradations strongly
coupled with image structures). RAM++ also mitigates common challenges such as
unbalanced performance across tasks, overfitting to seen degradations, and weak
generalization to unseen ones through three key designs: 1) Adaptive
Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level
masks to semantically rich and textured regions. This design enables the
network to learn both generative priors and image content priors from various
degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy
that leverages DINOv2's semantically consistent and degradation-invariant
representations, together with efficient feature fusion, to achieve faithful
and semantically coherent restoration. With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations. Our code and model will be released at
https://github.com/DragonisCV/RAM</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zilong Zhang等人撰写的论文“RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration”的全面摘要。</p>
<hr />
<h3 id="ram">论文摘要：RAM++: 基于自适应掩码的全能图像恢复的鲁棒表示学习</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决全能图像恢复（all-in-one image restoration）领域中的核心挑战。现有的退化导向方法在极端场景（例如，退化与图像结构强耦合）下表现不佳，并且普遍存在任务间性能不平衡、对已知退化过拟合以及对未知退化泛化能力弱等问题。作者认为，图像恢复的本质在于学习鲁棒的内在表示，能够从退化输入中恢复基本图像信息，而不是仅仅去除退化模式。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
RAM++ 提出了一个两阶段框架，通过整合高级语义理解和低级纹理生成，实现内容导向的鲁棒恢复。其主要创新点包括：</p>
<ul>
<li><strong>自适应语义感知掩码 (AdaSAM)：</strong> 一种预训练策略，它对语义丰富和纹理区域应用像素级掩码。这使得网络能够从各种退化中学习生成先验和图像内容先验，从而在统一的潜在空间中编码多样化的退化。与简单随机像素级掩码不同，AdaSAM 结合了像素级恢复和区域级语义理解，专注于难以重建的区域。</li>
<li><strong>掩码属性传导 (MAC)：</strong> 一种选择性微调策略，通过评估每个网络层在弥合掩码预训练和全图像微调之间完整性差距方面的贡献，调整贡献较高的层，同时保留已学习的先验。这允许模型在仅更新少量层（例如30%）的情况下实现高性能。</li>
<li><strong>鲁棒特征正则化 (RFR)：</strong> 该策略利用 DINOv2 的语义一致性和退化不变性表示，结合高效的特征融合，实现忠实且语义连贯的恢复。DINOv2 的特征被整合到微调过程中，以增强图像恢复性能，尤其是在复杂退化场景下，并弥补恢复网络在预训练阶段学习主要内容能力的不足。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
RAM++ 在已知、未知、极端和混合退化场景下均实现了鲁棒、均衡且最先进的性能。</p>
<ul>
<li><strong>性能提升：</strong> 在3任务和7任务图像恢复设置下，RAM++ 在PSNR和SSIM等指标上超越了现有最先进方法。例如，在7任务设置下，RAM++ 在完全微调（100%）时，比次优方法提升了0.70dB。</li>
<li><strong>均衡性能：</strong> 随着任务数量的增加，RAM++ 策略不仅提升了整体性能，还将原始任务的性能下降限制在5.07%以内，并在七个任务中表现出最低的方差（4.83），表明其在多任务场景下的性能平衡性。</li>
<li><strong>泛化能力：</strong> 在分布外（OOD）退化评估中，RAM++ 在未知噪声类型上实现了显著的PSNR增益，并有效处理了水下图像增强等任务，展示了强大的泛化能力和鲁棒性。</li>
<li><strong>可解释性分析：</strong> 通过因果效应图（CEM）分析，论文揭示了RAM++的四个显著特性：有效的语义理解、稳定的全局信息获取、准确的正负信息判别以及优先的背景结构重建。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
尽管RAM++表现出色，但仍存在局限性：</p>
<ul>
<li><strong>任务间冲突：</strong> 在包含多样化退化的混合数据集上进行微调，不可避免地会面临任务间的固有冲突。</li>
<li><strong>细节推断挑战：</strong> 掩码图像建模的特性使得在内核去模糊等任务中推断细节更具挑战性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的研究方向包括：</p>
<ul>
<li><strong>多任务学习和优化数据混合策略：</strong> 进一步探索如何更好地处理任务间的冲突和数据混合。</li>
<li><strong>扩展到视频恢复：</strong> 将该框架扩展到视频恢复领域，并结合时间一致性，以增强其通用恢复能力。</li>
</ul>
<hr />
<p>总而言之，RAM++ 通过引入 AdaSAM、MAC 和 RFR 等创新设计，为全能图像恢复提供了一种新颖的内容导向视角。它成功地解决了现有方法在极端场景、性能不平衡和泛化能力方面的局限性，并在多个基准测试中取得了显著的、最先进的成果，为该领域未来的发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors.</li>
<li>With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12039v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12039v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11895v1'></a></p>
<h2 id="integrating-prior-observations-for-incremental-3d-scene-graph-prediction"><a href="https://arxiv.org/abs/2509.11895v1">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a></h2>
<p><strong>Authors:</strong> Marian Renz, Felix Igelbrink, Martin Atzmueller</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Marian Renz, Felix Igelbrink, Martin Atzmueller撰写的论文“Integrating Prior Observations for Incremental 3D Scene Graph Prediction”的全面摘要。</p>
<hr />
<h3 id="3d">论文摘要：整合先验观测以实现增量式3D场景图预测</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前3D语义场景图（3DSSG）生成方法主要依赖传感器数据，且通常假设能够访问完整的场景重建，这限制了它们在真实世界、增量式环境中的应用。这些方法未能有效整合来自语义丰富环境的额外信息，也无法在场景数据流实时获取的增量式设置中进行预测和解释。因此，核心问题是如何开发一种能够灵活整合多模态信息（特别是先验观测）的增量式3DSSG预测模型，同时避免对完整场景重建的依赖。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>新型异构图模型：</strong> 论文提出了一种新颖的异构图模型，用于增量式3DSSG预测。该模型将场景图构建所需的子任务整合到一个多层架构中，从而无需专门模块即可灵活地整合多模态信息。
*   <strong>全局-局部场景表示整合：</strong> 该方法的核心是一个异构场景图设计，它融合了传感器数据和来自先前时间步的观测结果，通过全局层（提供空间、几何和语义上下文）和局部层（整合当前传感器数据）实现。
*   <strong>先验观测的直接整合：</strong> 模型通过将实例从当前帧链接到先前预测的节点，直接将先验预测整合到消息传递过程中，从而利用早期观测信息进行预测，而无需存储完整的分割点云。
*   <strong>多模态特征嵌入：</strong> 模型通过将空间、几何和语义特征直接嵌入到消息传递过程中，高效地存储和整合这些特征，避免了存储大量的点云段或时间序列数据。特别是，全局节点可以包含基于类别标签的独热编码或CLIP嵌入的文本标签。
*   <strong>异构GNN的应用：</strong> 论文探索了使用异构图神经网络（GNNs）来改进语义相关信息的整合，并评估了GraphSAGE和HGT等不同GNN架构。
*   <strong>额外的边缘类型：</strong> 为了评估模型的灵活性，引入了一种额外的全局节点间边缘类型，该边缘类型基于几何碰撞检查和谐波中心性，提供了拓扑上不同的子图。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能提升：</strong> 在3DSSG数据集上的评估表明，通过语义嵌入（如CLIP）和先验观测增强的GNNs为复杂、真实世界的环境提供了可扩展且通用的解决方案。
*   <strong>异构模型的优势：</strong> 异构模型在关系预测方面表现出色，尤其是在整合了CLIP嵌入后，HGT+CLIP模型在关系预测方面达到了最高性能。这表明异构模型非常适合捕获3DSSG中丰富的语义结构。
*   <strong>对错误先验预测的鲁棒性：</strong> 即使在全局层引入20%或50%的错误标签，模型在节点和边缘分类任务上的性能下降相对较小，表明模型具有一定的鲁棒性。这说明学习到的先验特征即使在适度的标签损坏下仍然具有信息量。
*   <strong>多模态信息整合的灵活性：</strong> 结果表明，所提出的模型能够无缝地将多模态信息整合到消息传递过程中，而无需外部模块，这验证了其架构的灵活性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>对先验观测的依赖：</strong> 尽管模型对适度的标签损坏具有鲁棒性，但关系预测的性能下降表明模型对先验观测的强烈依赖，这在实际应用场景中需要缓解。
*   <strong>GNN层限制：</strong> 由于所使用的GNN层的限制，边缘特征在消息传递过程中未被使用。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>全尺度3D语义映射：</strong> 未来的工作将探索将该架构应用于全尺度3D语义映射，以支持真实世界的机器人任务。
*   <strong>整合额外先验知识源：</strong> 进一步整合额外的先验知识源，以增强推理能力和支持可解释性。</p>
<hr />
<p>这篇论文通过引入一个新颖的异构图模型，有效地解决了增量式3DSSG预测中整合多模态信息和先验观测的挑战。其贡献在于提供了一个灵活、可扩展的框架，能够处理复杂、动态的真实世界环境，并为机器人和具身AI系统提供了更准确的环境理解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process.</li>
<li>We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11895v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11895v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11884v1'></a></p>
<h2 id="sam-ttt-segment-anything-model-via-reverse-parameter-configuration-and-test-time-training-for-camouflaged-object-detection"><a href="https://arxiv.org/abs/2509.11884v1">SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection</a></h2>
<p><strong>Authors:</strong> Zhenni Yu, Li Zhao, Guobao Xiao, Xiaoqin Zhang</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT. While most
existing SAM-based COD models primarily focus on enhancing SAM by extracting
favorable features and amplifying its advantageous parameters, a crucial gap is
identified: insufficient attention to adverse parameters that impair SAM's
semantic understanding in downstream tasks. To tackle this issue, the Reverse
SAM Parameter Configuration Module is proposed to effectively mitigate the
influence of adverse parameters in a train-free manner by configuring SAM's
parameters. Building on this foundation, the T-Visioner Module is unveiled to
strengthen advantageous parameters by integrating Test-Time Training layers,
originally developed for language tasks, into vision tasks. Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state. By integrating two modules,
SAM-TTT simultaneously suppresses adverse parameters while reinforcing
advantageous ones, significantly improving SAM's semantic understanding in COD
task. Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field. The code will be available at
https://github.com/guobaoxiao/SAM-TTT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zhenni Yu, Li Zhao, Guobao Xiao, Xiaoqin Zhang撰写的论文“SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection”的全面摘要。</p>
<hr />
<h3 id="sam-ttt">SAM-TTT: 通过逆向参数配置和测试时训练实现伪装目标检测的通用分割模型</h3>
<p><strong>摘要：</strong></p>
<p>这篇论文介绍了一种名为SAM-TTT的新型通用分割模型（Segment Anything Model, SAM），它通过结合<strong>逆向参数配置（Reverse Parameter Configuration）</strong>和<strong>测试时训练（Test-Time Training, TTT）</strong>来显著提升其在伪装目标检测（Camouflaged Object Detection, COD）任务中的性能。</p>
<p><strong>1. 主要问题或研究问题：</strong>
现有的基于SAM的COD模型主要关注通过提取有利特征和放大优势参数来增强SAM，但作者指出一个关键的不足：<strong>对损害SAM在下游任务中语义理解的不利参数关注不足</strong>。SAM在COD任务中表现出的语义缺陷，例如分割掩码与预期语义不符，以及对目标内部的分割不完整，是由于SAM的零样本能力源于SA-1B数据集与COD数据集之间的领域差距。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
SAM-TTT的核心创新在于其双模块设计，旨在同时抑制不利参数并强化有利参数：</p>
<ul>
<li>
<p><strong>逆向SAM参数配置模块（Reverse SAM Parameter Configuration Module, R-SAMPC）：</strong></p>
<ul>
<li><strong>目的：</strong> 有效减轻不利参数的影响，以无训练（train-free）方式配置SAM的参数。</li>
<li><strong>机制：</strong> R-SAMPC被设计为一个不更新参数的卷积模块，类似于一种参数层面的随机掩码或dropout。它通过引入噪声直接降低SAM中不利于COD任务的参数影响，从而缓解语义缺陷。这与传统方法通过引入额外编码模块来补偿语义信息不同，R-SAMPC直接作用于参数，且在推理时不参与。</li>
<li><strong>“逆向”含义：</strong> 传统方法强调增强参数，而R-SAMPC则侧重于削弱参数，故名“逆向”。</li>
</ul>
</li>
<li>
<p><strong>T-Visioner模块（TVM）：</strong></p>
<ul>
<li><strong>目的：</strong> 强化有利参数，以补偿R-SAMPC在削弱不利参数时可能对有利参数造成的干扰。</li>
<li><strong>机制：</strong> TVM将测试时训练（TTT）层（特别是TTT-Linear，一种具有线性复杂度和高度表达性隐藏状态的RNN层）首次引入视觉任务。它通过DWT（离散小波变换）提取图像嵌入的高频分量，并调整特征维度以适应RNN层的要求，从而提取并强调有利特征。</li>
<li><strong>结构：</strong> SAM-TTT采用<strong>先并行后融合</strong>的结构，R-SAMPC和TVM在并行阶段独立运行，避免相互干扰，然后在融合阶段（通过COMPrompter的混合提示方法）结合两者的功能，以实现多尺度上下文信息的有效捕获和精确融合。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能卓越：</strong> SAM-TTT在多个COD基准数据集（CAMO, COD10K, NC4K）上取得了最先进的性能，超越了现有SOTA方法，为该领域树立了新基准。
*   <strong>语义理解提升：</strong> 通过同时抑制不利参数和强化有利参数，SAM-TTT显著改善了SAM在COD任务中的语义理解能力，能够生成更详细、更准确的分割掩码，尤其在处理微小、被遮挡或与背景相似的伪装目标时表现出色。
*   <strong>效率：</strong> 尽管SAM-TTT的总参数量较大（96.32M），但其可训练参数仅为6.65M（约为FSEL的十分之一），保持了较低的计算开销，同时实现了优于全监督方法的性能。
*   <strong>模块有效性：</strong> 消融实验证明了R-SAMPC和TVM的有效性。R-SAMPC显著提升了模型性能，尤其在正向指标上。TVM进一步提升了性能，并被证实能有效补偿R-SAMPC对有利参数的削弱作用，且TTT相比Mamba在聚焦有利特征方面表现更强。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   在CAMO数据集上，SAM-TTT的表现略逊于其他方法，这可能与SAM知识增强的泛化能力和R-SAMPC扰动以牺牲学习能力为代价有关，以及该数据集较高的训练-测试比（1000:250）可能进一步放大了这种影响。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   进一步探索如何改进削弱不利参数和强调有利参数的组合方式。
*   将这种“逆向参数配置”和“测试时训练”的概念扩展到其他大型模型在下游任务中的应用。</p>
<hr />
<p>总而言之，SAM-TTT通过其独特的R-SAMPC和TVM双模块设计，开创性地解决了SAM在伪装目标检测中语义理解不足的问题，通过拓宽有利参数和不利参数之间的“效应距离”，实现了性能上的突破，并为将测试时训练引入计算机视觉领域奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT.</li>
<li>Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state.</li>
<li>Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11884v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11884v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11866v1'></a></p>
<h2 id="drv-a-hierarchical-perception-temporal-cognition-framework-to-diagnose-video-hallucination-by-fine-grained-spatial-temporal-grounding"><a href="https://arxiv.org/abs/2509.11866v1">Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</a></h2>
<p><strong>Authors:</strong> Meng Luo, Shengqiong Wu, Liqiang Jing, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee, Wynne Hsu</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in large video models (LVMs) have significantly enhance
video understanding. However, these models continue to suffer from
hallucinations, producing content that conflicts with input videos. To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding. Dr.V comprises of two key components: a benchmark
dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes
10k instances drawn from 4,974 videos spanning diverse tasks, each enriched
with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in
LVMs by systematically applying fine-grained spatial-temporal grounding at the
perceptive and temporal levels, followed by cognitive level reasoning. This
step-by-step pipeline mirrors human-like video comprehension and effectively
identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is
effective in diagnosing hallucination while enhancing interpretability and
reliability, offering a practical blueprint for robust video understanding in
real-world scenarios. All our data and code are available at
https://github.com/Eurekaleo/Dr.V.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding”的综合摘要，内容涵盖了您要求的所有要点。</p>
<hr />
<p><strong>论文摘要：Dr.V：一种用于通过细粒度时空接地诊断视频幻觉的层次感知-时间-认知框架</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决大型视频模型（LVMs）在视频理解中普遍存在的“幻觉”问题。LVMs在生成内容时，经常会产生与输入视频不符或事实错误的信息，这严重损害了其可靠性和可信度。现有的幻觉评估方法存在碎片化、不完整的分类体系以及缺乏细粒度标注和分析的问题，无法进行全面的诊断和根本原因分析。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了Dr.V框架，包含两个核心组件：
*   <strong>Dr.V-Bench基准数据集：</strong> 这是一个新颖且全面的视频幻觉评估基准。它引入了一个分层的幻觉分类法，涵盖感知、时间、认知三个层次共14种细粒度幻觉类型。数据集包含10,000个实例，来自4,974个视频，涵盖多种任务，并富含详细的细粒度时空标注，支持精确的诊断分析。
*   <strong>Dr.V-Agent卫星视频代理：</strong> 这是一个新颖的诊断模型，它模仿人类的视频理解机制，采用“从感知到时间再到认知”的链式分层推理过程来诊断LVMs中的幻觉。Dr.V-Agent系统性地利用细粒度时空接地（通过调用先进的外部工具如Grounded SAM2和YOLO-World进行对象识别和跟踪，以及CG-STVG和Grounded-VideoLLM进行时间接地）来验证感知和时间层面的信息，随后进行认知层面的推理。这种逐步的诊断流程能够生成结构化的诊断反馈，指导LVMs纠正其响应。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>幻觉普遍存在：</strong> 实验表明，所有测试的LVMs都存在显著的幻觉问题，即使是顶级的闭源模型也未能幸免，这凸显了视频幻觉是一个严重且尚未解决的问题。
*   <strong>性能分层：</strong> LVMs在感知任务上表现最佳，但在时间任务和认知任务上的准确性显著下降，表明当前LVMs在处理复杂视频分析所需的严格时空理解和高级推理方面存在不足。
*   <strong>Dr.V-Agent的有效性：</strong> 广泛的实验证明，Dr.V-Agent在诊断幻觉方面非常有效，并显著提高了LVMs的解释性和可靠性。与基线自纠正策略（Self-PEP）相比，Dr.V-Agent在所有测试模型和幻觉类型上都取得了显著且稳健的性能提升，尤其是在低性能LVMs上。
*   <strong>诊断能力：</strong> Dr.V-Agent通过将模型的推理与精确的、外部验证的时空信息相结合，克服了自纠正的局限性，从而在特定幻觉类型（如对象、静态关系、OCR和动态关系）上实现了实质性改进。
*   <strong>效率和可扩展性：</strong> Dr.V-Agent采用免训练范式，通过智能地组合现有专家工具来运行，避免了昂贵的预训练和微调，具有固有的灵活性和面向未来的可扩展性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>对外部工具性能的依赖：</strong> Dr.V-Agent的有效性严重依赖于其调用的外部工具（如对象检测、时空接地工具）的性能。这些工具固有的局限性可能会影响最终诊断和幻觉缓解的准确性。
*   <strong>系统复杂性和计算开销：</strong> 为了实现细粒度推理，多代理方法调用了多达八个外部模型，这增加了显著的系统复杂性和计算开销。与端到端模型相比，这种多步骤、顺序处理过程可能导致更高的延迟。
*   <strong>基准标注成本和可扩展性：</strong> Dr.V-Bench的构建依赖于细粒度的人工时空标注。虽然这种细节水平对于准确诊断幻觉至关重要，但高昂的标注成本和时间投入限制了基准的可扩展性，使其难以快速扩展到更大规模或更多样化的视频领域。
*   <strong>生成能力间接评估：</strong> Dr.V-Bench对生成能力的评估被改编为基于QA的框架（特别是“字幕生成QA”）。这种设计简化了幻觉的针对性评估，但牺牲了对模型自由形式生成能力（如流畅性、连贯性和创造力）的直接和全面评估。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提升外部工具的鲁棒性：</strong> 鉴于对外部工具性能的依赖，未来的研究可以专注于开发更鲁棒、更准确的感知和时间接地工具，以进一步提高Dr.V-Agent的整体性能。
*   <strong>优化系统效率：</strong> 探索减少系统复杂性和计算开销的方法，例如通过更高效的工具集成、并行处理或模型蒸馏，以降低延迟并提高实际应用性。
*   <strong>扩展基准规模和多样性：</strong> 尽管Dr.V-Bench已经很全面，但未来的工作可以投入资源，通过半自动化标注或众包等方式，进一步扩大数据集的规模，涵盖更多视频领域和场景，以应对更广泛的幻觉类型。
*   <strong>直接评估生成能力：</strong> 开发更直接、更全面的方法来评估LVMs的自由形式生成能力，超越基于QA的框架，以更好地捕捉模型在流畅性、连贯性和创造力方面的表现。
*   <strong>整合外部世界知识：</strong> 进一步探索如何更有效地整合外部世界知识和领域特定专业知识，以增强LVMs在认知推理任务（如反事实预测和知识驱动解释）中的表现。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11866v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11866v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11815v1'></a></p>
<h2 id="specvlm-fast-speculative-decoding-in-vision-language-models"><a href="https://arxiv.org/abs/2509.11815v1">SpecVLM: Fast Speculative Decoding in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Haiduo Huang等人撰写的论文“SpecVLM: Fast Speculative Decoding in Vision-Language Models”的全面摘要。</p>
<hr />
<h3 id="specvlm-fast-speculative-decoding-in-vision-language-models_1">论文摘要：SpecVLM: Fast Speculative Decoding in Vision-Language Models</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决视觉-语言模型（VLMs）中自回归解码的效率瓶颈。尽管推测解码（speculative decoding）已成功加速大型语言模型（LLMs），但将其直接应用于VLMs面临独特挑战。特别是，预填充（prefill）阶段由视觉token主导，其数量随图像分辨率和视频长度急剧增加，导致计算和内存（尤其是键值（KV）缓存）开销巨大。这严重影响了VLMs的吞吐量和延迟，阻碍了实时和大规模部署。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
SpecVLM通过以下两项核心创新来解决上述问题：</p>
<ul>
<li><strong>弹性视觉压缩器（Elastic Visual Compressor）：</strong> SpecVLM引入了一种自适应的视觉压缩器，能够根据输入动态选择剪枝（pruning）、池化（pooling）、卷积（convolution）和重采样（resampler）等基本操作，以平衡FLOPs/参数和每个输入的准确性。这有助于在预填充阶段有效减少视觉token数量，从而缓解计算和KV缓存的压力。</li>
<li><strong>在线Logit蒸馏协议（Online-Logit Distillation Protocol）：</strong> 为了避免昂贵的离线蒸馏语料库，SpecVLM提出了一种在线Logit蒸馏协议。该协议使用实时的教师模型Logit和倒数第二层特征来训练草稿模型（draft model），结合了交叉熵和Smooth L1目标函数。这种方法消除了存储和预处理的需要，同时保持了计算效率，并揭示了训练时间扩展效应。</li>
</ul>
<p>此外，SpecVLM首先建立了一个强大的EAGLE-2风格基线模型EagleVLM，该模型本身就比完全自回归推理提供了显著的加速。SpecVLM在此基础上进一步提升了性能。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的端到端加速：</strong> EagleVLM在完全自回归推理的基础上实现了1.5-2.3倍的端到端加速。在此基础上，SpecVLM通过其创新进一步实现了额外的加速，在LLaVA和MMMU基准测试上，在5个epoch内达到了2.5-2.9倍的端到端加速。
*   <strong>无损解码：</strong> SpecVLM在加速的同时，保持了目标模型的输出分布（无损解码），确保了输出质量。
*   <strong>训练时间扩展效应：</strong> 在线Logit蒸馏协议揭示了一个重要的训练时间扩展效应：更长的在线训练单调地增加了草稿模型的平均接受长度，从而提高了推测解码的效率。这表明，对于多模态推测解码，延长有针对性的训练时间可以带来显著的效率提升。
*   <strong>跨分辨率和任务难度的一致性：</strong> SpecVLM在不同图像分辨率和任务难度下均表现出一致的性能提升，验证了其方法的鲁棒性。</p>
<p>这些结果表明SpecVLM为VLMs的推理加速提供了一个实用且可扩展的框架，在保持输出质量的同时显著提升了效率。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>视觉压缩器的压缩比例手动配置：</strong> 当前的视觉压缩器的压缩比例仍需手动配置，未来可以探索实例级的自适应压缩比例调整机制。
*   <strong>KV缓存的动态压缩：</strong> 论文提到，在推理时探索草稿模型KV缓存的动态压缩可以进一步提高效率。
*   <strong>其他扩展轴的探索不足：</strong> 论文主要关注训练时间，但更大的训练语料库和更深的草稿模型等其他扩展轴也可能提高推测解码效率。
*   <strong>超参数调优：</strong> 论文未详尽调优草稿模型的超参数（如深度、树Top-K、节点数量）或优化设置（如学习率、批次大小），因此报告的性能可能不是最优的。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   开发实例级的视觉压缩比例自适应调整机制。
*   探索推理时草稿模型KV缓存的动态压缩。
*   研究更大训练语料库和更深草稿模型对推测解码效率的影响，以揭示VLMs中推测解码的更广泛扩展规律。
*   对草稿模型超参数和优化设置进行更详尽的调优，以识别最佳设置。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11815v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11815v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11772v1'></a></p>
<h2 id="seg2track-sam2-sam2-based-multi-object-tracking-and-segmentation-for-zero-shot-generalization"><a href="https://arxiv.org/abs/2509.11772v1">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></h2>
<p><strong>Authors:</strong> Diogo Mendonça, Tiago Barros, Cristiano Premebida, Urbano J. Nunes</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to
operate reliably in dynamic environments. MOT ensures consistent object
identity assignment and precise spatial delineation. Recent advances in
foundation models, such as SAM2, have demonstrated strong zero-shot
generalization for video segmentation, but their direct application to MOTS
(MOT+Segmentation) remains limited by insufficient identity management and
memory efficiency. This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement. The
proposed approach requires no fine-tuning and remains detector-agnostic.
Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA). Furthermore, a sliding-window
memory strategy reduces memory usage by up to 75% with negligible performance
degradation, supporting deployment under resource constraints. These results
confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot
tracking, enhanced identity preservation, and efficient memory utilization. The
code is available at https://github.com/hcmr-lab/Seg2Track-SAM2</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Diogo Mendonça, Tiago Barros, Cristiano Premebida, Urbano J. Nunes撰写的论文“Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization”的全面摘要。</p>
<hr />
<h3 id="seg2track-sam2-sam2">论文摘要：Seg2Track-SAM2: 基于SAM2的零样本泛化多目标跟踪与分割</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决自主系统在动态环境中对鲁棒多目标跟踪与分割（MOTS）的需求。尽管基础模型如SAM2在视频分割方面展现出强大的零样本泛化能力，但其直接应用于MOTS时，面临身份管理不足和内存效率低下的限制。因此，核心问题是如何将SAM2的零样本分割能力与高效的身份管理和内存利用相结合，以实现鲁棒的零样本MOTS。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
Seg2Track-SAM2框架通过以下关键创新解决了上述问题：
*   <strong>集成预训练检测器与SAM2：</strong> 框架首先利用YOLOv11等预训练的深度学习检测器生成初始目标提议（bounding boxes），然后将这些提议作为SAM2的提示，生成高质量的分割掩码。这种方法使其能够处理不同类别的对象，并保持检测器无关性。
*   <strong>新型Seg2Track模块：</strong> 引入了一个专门的Seg2Track模块，用于处理轨迹初始化、轨迹管理和强化。该模块包含：
    *   <strong>轨迹质量评估（Track Quality Assessment, TQA）：</strong> 根据SAM2提供的IoU置信度分数评估掩码质量，将掩码分类为“高”、“不确定”或“低”状态，以决定轨迹的维护、移除或强化。
    *   <strong>二值掩码生成（Binary Mask Generation）：</strong> 将前一帧的单个对象掩码合并成一个统一的二值掩码，用于匹配过程，以识别与新提议的重叠。
    *   <strong>对象关联与过滤（Object Association and Filtering, OAF）：</strong> 这是一个两阶段过程，利用传入的对象提议和轨迹质量评估来决定是否初始化新轨迹或强化现有轨迹。
*   <strong>滑动窗口内存策略：</strong> 针对SAM2原始实现中内存无限制增长的问题，Seg2Track-SAM2引入了滑动窗口机制，限制了计算掩码时考虑的过去状态数量，显著降低了内存使用量，同时保持了跟踪性能。
*   <strong>零样本泛化能力：</strong> 整个框架设计为零样本范式，无需对特定数据集进行微调，即可在不同场景下工作。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能（SOTA）：</strong> 在KITTI MOT和KITTI MOTS基准测试中，Seg2Track-SAM2在汽车和行人类别上均取得了SOTA性能，在KITTI MOTS上总体排名第四。
*   <strong>关联准确度（AssA）新基准：</strong> 该方法在关联准确度（AssA）指标上建立了新基准，显著优于现有方法，表明其在身份保持方面表现出色，减少了身份切换率。
*   <strong>高效内存利用：</strong> 滑动窗口内存策略将内存使用量减少了高达75%，而性能下降可忽略不计，这对于资源受限系统（如机器人和自动驾驶）的部署至关重要。
*   <strong>鲁棒性：</strong> 结果证实了Seg2Track-SAM2通过结合鲁棒的零样本跟踪、增强的身份保持和高效的内存利用，显著推动了MOTS领域的发展。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>检测准确度（DetA）的限制：</strong> 相较于其他SOTA方法，Seg2Track-SAM2的检测准确度（DetA）较低。这主要是由于虚假正例的传播（SAM2错误地将虚假检测初始化为新对象，并在多帧中保持）以及依赖通用检测器（YOLOv11）而非针对KITTI数据集专门训练的检测器。
*   <strong>2D MOT基准测试中的性能下降：</strong> 在KITTI 2D MOT基准测试中，由于将分割掩码转换为2D边界框，当对象部分或完全被遮挡时，性能会下降。特别是对于空间范围较大的汽车，边界框转换过程中更容易出现失真，导致DetA显著下降。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进边界框转换策略：</strong> 针对2D MOT任务中边界框转换的局限性，可以探索更复杂的转换策略，例如非模态边界框估计或跨帧时间平滑，以更好地近似被遮挡对象的完整空间范围，并减少失真。但这需要权衡零样本设计理念与额外模型假设或训练数据。
*   <strong>优化虚假正例抑制：</strong> 进一步研究如何有效抑制SAM2可能产生的虚假正例，以提高检测准确度（DetA）。
*   <strong>集成更先进的检测器：</strong> 尽管该框架是检测器无关的，但集成针对特定数据集进行微调的更先进检测器可能会进一步提高整体性能。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement.</li>
<li>Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11772v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11772v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-16 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
