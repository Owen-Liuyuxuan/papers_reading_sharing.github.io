<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-16 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-15/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-17/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-16">Arxiv Computer Vision Papers - 2025-09-16</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-15" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-15)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review" class="nav-link">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a>
                </li>
                <li class="nav-item">
                    <a href="#domain-adaptive-pretraining-improves-primate-behavior-recognition" class="nav-link">Domain-Adaptive Pretraining Improves Primate Behavior Recognition</a>
                </li>
                <li class="nav-item">
                    <a href="#railsafenet-visual-scene-understanding-for-tram-safety" class="nav-link">RailSafeNet: Visual Scene Understanding for Tram Safety</a>
                </li>
                <li class="nav-item">
                    <a href="#fs-sam2-adapting-segment-anything-model-2-for-few-shot-semantic-segmentation-via-low-rank-adaptation" class="nav-link">FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</a>
                </li>
                <li class="nav-item">
                    <a href="#ram-robust-representation-learning-via-adaptive-mask-for-all-in-one-image-restoration" class="nav-link">RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</a>
                </li>
                <li class="nav-item">
                    <a href="#integrating-prior-observations-for-incremental-3d-scene-graph-prediction" class="nav-link">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#sam-ttt-segment-anything-model-via-reverse-parameter-configuration-and-test-time-training-for-camouflaged-object-detection" class="nav-link">SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#drv-a-hierarchical-perception-temporal-cognition-framework-to-diagnose-video-hallucination-by-fine-grained-spatial-temporal-grounding" class="nav-link">Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</a>
                </li>
                <li class="nav-item">
                    <a href="#specvlm-fast-speculative-decoding-in-vision-language-models" class="nav-link">SpecVLM: Fast Speculative Decoding in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#seg2track-sam2-sam2-based-multi-object-tracking-and-segmentation-for-zero-shot-generalization" class="nav-link">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-16">Arxiv Computer Vision Papers - 2025-09-16</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-15">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-15)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»å¤©ç Arxiv è®¡ç®æºè§è§è®ºæä¸»è¦å´ç» <strong>åºç¡æ¨¡åï¼ç¹å«æ¯ SAM åå¶åä½ï¼çéåºä¸å¢å¼ºãå¤æ¨¡æå­¦ä¹ ï¼å°¤å¶æ¯è§è§-è¯­è¨æ¨¡åï¼ã3D æç¥ä¸åºæ¯çè§£ä»¥åé²æ£æ§ä¸æ³åè½å</strong> è¿å ä¸ªæ ¸å¿ä¸»é¢å±å¼ãæ¾èè¶å¿æ¯ç ç©¶äººåè´åäºå°å¼ºå¤§çé¢è®­ç»æ¨¡åï¼å¦ SAMï¼åºç¨äºæ´å·ä½çä¸æ¸¸ä»»å¡ï¼å¹¶æ¢ç´¢å¶å¨å°æ ·æ¬ãé¶æ ·æ¬åé¢åéåºåºæ¯ä¸çæ½åã</p>
<p><strong>ä¸»è¦ä¸»é¢ä¸è¶å¿ï¼</strong></p>
<ol>
<li><strong>SAM (Segment Anything Model) çæç»­æ¼è¿ä¸åºç¨ï¼</strong> ä»å¤©çæ¥åä¸­æåç¯è®ºæç´æ¥æé´æ¥æ¶å SAM æå¶ä¸ä¸ä»£çæ¬ SAM2ãè¿è¡¨æ SAM ä»ç¶æ¯è®¡ç®æºè§è§é¢åçç­ç¹ï¼ç ç©¶éç¹å·²ä»æ¨¡åæ¬èº«è½¬åå¦ä½é«æå°å°å¶éåºå°ç¹å®ä»»å¡ï¼å¦å°æ ·æ¬è¯­ä¹åå²ãä¼ªè£ç®æ æ£æµãå¤ç®æ è·è¸ªä¸åå²ï¼ï¼ä»¥åå¦ä½è§£å³å¶å¨ç¹å®åºæ¯ä¸çå±éæ§ã</li>
<li><strong>å¤æ¨¡æå­¦ä¹ ä¸è§è§-è¯­è¨æ¨¡å (VLM)ï¼</strong> SpecVLM å Dr.V ä¸¤ç¯è®ºæå¸æ¾äº VLM å¨æçåå¯é æ§æ¹é¢çç ç©¶è¿å±ãSpecVLM å³æ³¨æ¨çéåº¦ä¼åï¼è Dr.V åè´åäºè¯æ­åè§£å³ VLM ä¸­çâå¹»è§âé®é¢ï¼è¿å¯¹äºæå VLM çå®éåºç¨ä»·å¼è³å³éè¦ã</li>
<li><strong>3D æç¥ä¸åºæ¯çè§£ï¼</strong> 3D äººä½å§¿æä¸å½¢ç¶ä¼°è®¡ä»¥åå¢éå¼ 3D åºæ¯å¾é¢æµæ¯è¯¥é¢åçäº®ç¹ãè¿è¡¨æå¯¹çå®ä¸çä¸ç»´ä¿¡æ¯ççè§£åå»ºæ¨¡ä»ç¶æ¯éè¦çç ç©¶æ¹åï¼å°¤å¶æ¯å¨æºå¨äººãèªå¨é©¾é©¶ç­åºç¨ä¸­ã</li>
<li><strong>é²æ£æ§ãæ³åä¸é¢åéåºï¼</strong> å¤ç¯è®ºæå³æ³¨æ¨¡åå¨ä¸åé¢åãä¸åæ¡ä»¶ä¸çé²æ£æ§åæ³åè½åãä¾å¦ï¼åéåºé¢è®­ç»ç¨äºçµé¿ç±»è¡ä¸ºè¯å«ï¼ä»¥å RAM++ æ¨å¨éè¿èªéåºæ©ç å®ç°å¨è½å¾åæ¢å¤çé²æ£æ§ã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation" (Bernardo Forni et al.)ï¼</strong> è¿ç¯è®ºæä»£è¡¨äºå°å¼ºå¤§çåºç¡æ¨¡åï¼SAM2ï¼é«æéåºå°æ°æ®ç¨ç¼ºä»»å¡ï¼å°æ ·æ¬è¯­ä¹åå²ï¼çææ°å°è¯ãéè¿ä½ç§©éåºï¼å®ææå¨ä¿ææ¨¡åæ§è½çåæ¶æ¾èéä½å¾®è°ææ¬ï¼å·æå¾é«çå®ç¨ä»·å¼ã</li>
<li><strong>"Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding" (Meng Luo et al.)ï¼</strong> è§£å³ VLM çâå¹»è§âé®é¢æ¯å½å VLM é¢åé¢ä¸´çå³é®ææãDr.V æåºçåå±æ¡æ¶ï¼éè¿ç»ç²åº¦çæ¶ç©ºå®ä½æ¥è¯æ­è§é¢å¹»è§ï¼ä¸ºæå VLM çå¯é æ§åå¯ä¿¡åº¦æä¾äºæ°çæè·¯ã</li>
<li><strong>"SpecVLM: Fast Speculative Decoding in Vision-Language Models" (Haiduo Huang et al.)ï¼</strong> éç VLM æ¨¡åçè§æ¨¡ä¸æ­æ©å¤§ï¼æ¨çæçæä¸ºç¶é¢ãSpecVLM å¼å¥çæ¨æµè§£ç ææ¯ï¼æææ¾èå é VLM çæ¨çè¿ç¨ï¼å¯¹äº VLM çå®éé¨ç½²å·æéè¦æä¹ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>SAM/SAM2 çè½»éåä¸é«æéåºï¼</strong> éç SAM æ¨¡åçæ®åï¼å¦ä½ä»¥æ´ä½çè®¡ç®ææ¬åæ´å°çæ°æ®å°å¶éåºå°ç¹å®ä»»å¡ï¼å°æ¯æªæ¥çéè¦æ¹åï¼å¦ FS-SAM2 ä¸­çä½ç§©éåºï¼ã</li>
<li><strong>VLM çå¯é æ§ä¸å¯è§£éæ§ï¼</strong> è§£å³ VLM çâå¹»è§âé®é¢ï¼å¦ Dr.Vï¼ä»¥åæåå¶å³ç­è¿ç¨çå¯è§£éæ§ï¼å°æ¯ VLM èµ°åæ´å¹¿æ³åºç¨çå³é®ã</li>
<li><strong>å¤æ¨¡æèåçé²æ£æ§ï¼</strong> å¦ä½å¨å¤æå¤åçç¯å¢ä¸­ï¼ææå°èåä¸åæ¨¡æï¼å¦ LiDAR ç¹äºãå¾åï¼çä¿¡æ¯ï¼å¹¶ä¿ææ¨¡åçé²æ£æ§ï¼ä»æ¯æ´»è·çç ç©¶é¢åã</li>
<li><strong>å¢éå¼å­¦ä¹ å¨3Dåºæ¯çè§£ä¸­çåºç¨ï¼</strong> éçæºå¨äººåèªå¨é©¾é©¶ç³»ç»å¨å¨æç¯å¢ä¸­è¿è¡ï¼å¦ä½è®©æ¨¡åè½å¤æç»­å­¦ä¹ åæ´æ°å¶å¯¹3Dåºæ¯ççè§£ï¼å°åå¾è¶æ¥è¶éè¦ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå³æ³¨åºç¡æ¨¡åéåºåé«æå©ç¨çç ç©¶äººåï¼
*   <strong>"FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation"</strong>
*   <strong>"Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization"</strong></p>
<p>å¯¹äºå³æ³¨å¤æ¨¡æå­¦ä¹ å VLM ææçç ç©¶äººåï¼
*   <strong>"Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding"</strong>
*   <strong>"SpecVLM: Fast Speculative Decoding in Vision-Language Models"</strong></p>
<p>å¯¹äºå³æ³¨ 3D æç¥ååºæ¯çè§£çç ç©¶äººåï¼
*   <strong>"3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review"</strong> (ä½ä¸ºè¯¥é¢åçç»¼è¿°ï¼æå©äºå¨é¢äºè§£)
*   <strong>"Integrating Prior Observations for Incremental 3D Scene Graph Prediction"</strong></p>
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥ Arxiv è®¡ç®æºè§è§é¢åçææ°å¨æåéè¦è¿å±ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></li>
<li><a href="#2509.12193v1">Domain-Adaptive Pretraining Improves Primate Behavior Recognition</a></li>
<li><a href="#2509.12125v1">RailSafeNet: Visual Scene Understanding for Tram Safety</a></li>
<li><a href="#2509.12105v1">FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</a></li>
<li><a href="#2509.12039v1">RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</a></li>
<li><a href="#2509.11895v1">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a></li>
<li><a href="#2509.11884v1">SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection</a></li>
<li><a href="#2509.11866v1">Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</a></li>
<li><a href="#2509.11815v1">SpecVLM: Fast Speculative Decoding in Vision-Language Models</a></li>
<li><a href="#2509.11772v1">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.12197v1'></a></p>
<h2 id="3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review"><a href="https://arxiv.org/abs/2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></h2>
<p><strong>Authors:</strong> Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Salma Galaaoui, Eduardo Valle, David Picard, Nermin Sametæ°åçè®ºæâ3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Reviewâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</strong></p>
<p>è¿ç¯ç»¼è¿°è®ºæå¨é¢åé¡¾äºä»âéå¤âï¼in-the-wildï¼LiDARç¹äºä¸­è¿è¡3Däººä½å§¿æä¼°è®¡ï¼3D HPEï¼åäººä½ç½æ ¼æ¢å¤ï¼HMRï¼çç°ææ¹æ³ãä½èæ¨å¨ä¸ºè¯¥é¢åæä¾ä¸ä¸ªç»æåçåç±»åæ·±å¥çåæï¼ä»¥ä¿è¿LiDAR-basedäººä½çè§£çè¿ä¸æ­¥åå±ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è®ºæä¸»è¦å³æ³¨çæ ¸å¿é®é¢æ¯å¦ä½ä»ç¨çãä¸è§åãæåªå£°ä¸å¯è½å­å¨é®æ¡çâéå¤âLiDARç¹äºä¸­åç¡®å°ä¼°è®¡3Däººä½å§¿æåæ¢å¤è¯¦ç»çäººä½ç½æ ¼ãå°½ç®¡LiDARå¨èªå¨é©¾é©¶ååå¸ç¯å¢æç¥ä¸­å·æéç§ä¿æ¤ãç²¾ç¡®æ·±åº¦æµéåå¯¹åç§æ¡ä»¶é²æ£ç­ä¼å¿ï¼ä½å¶æ°æ®ç¹æ§ï¼å¦ç¨çæ§ãä¸ååéæ ·ãä¸å®æ´æ§ï¼ç»3Däººä½çè§£å¸¦æ¥äºå·¨å¤§ææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæçä¸»è¦è´¡ç®å¨äºï¼
*   <strong>ç»æååç±»æ³ï¼</strong> æåºäºä¸ä¸ªç»æåçåç±»æ³ï¼å°ç°ææ¹æ³æ ¹æ®å¶å­¦ä¹ èå¼ï¼çç£ãå¼±çç£ãæ çç£ï¼ãè¾å¥æ¨¡æï¼ä»LiDARæå¤æ¨¡æèåï¼ãç½ç»æ¶æï¼å¦PointNetåä½ãTransformerï¼ä»¥åå¤çç¨çæ§åæ¶é´ä¿¡æ¯çæ¹å¼è¿è¡åç±»ã
*   <strong>æ¹æ³åæï¼</strong> è¯¦ç»åæäºæ¯ç§æ¹æ³çä¼å¿ãå±éæ§åè®¾è®¡éæ©ï¼ç¹å«æ¯å®ä»¬å¦ä½åºå¯¹LiDARç¹äºçç¨çæ§åä¸è§åæ§ãä¾å¦ï¼éå¯¹ç¨çæ§ï¼æ¹æ³åæ¬ç»æåæå½±ãå¨ç¨çä½ç´ å¤çãæ··åBEV+ä½ç´ èåãå¯åº¦æç¥æ³¨æåTransformerç­ã
*   <strong>æ°æ®éåè¯ä¼°ææ çç»ä¸ï¼</strong> å¯¹ä¸ä¸ªæå¹¿æ³ä½¿ç¨çLiDARæ°æ®éï¼Waymo Open Dataset, SLOPER4D, Human-M3ï¼è¿è¡äºå®éæ¯è¾ï¼è¯¦ç»éè¿°äºå®ä»¬çç¹æ§ãåæ¶ï¼ç»ä¸äºææè¯ä¼°ææ çå®ä¹ï¼ç¡®ä¿äºå¬å¹³æ¯è¾çåºç¡ã
*   <strong>åºåæµè¯ï¼</strong> å»ºç«äºéå¯¹3D HPEåHMRä»»å¡å¨è¿ä¸ä¸ªæ°æ®éä¸çåºåæµè¯è¡¨ï¼ä¸ºè¯¥é¢åçè¿å±æä¾äºåèã
*   <strong>å¼æ¾ææåæªæ¥æ¹åï¼</strong> æ¦è¿°äºLiDAR-basedäººä½çè§£é¢ä¸´çå¼æ¾ææåæ½å¨ç ç©¶æ¹åã
*   <strong>éå¥ç½é¡µï¼</strong> ç»´æ¤äºä¸ä¸ªéå¥ç½é¡µï¼æ ¹æ®è®ºæçåç±»æ³ç»ç»è®ºæï¼å¹¶æç»­æ´æ°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¯¹ç°ææ¹æ³çå¨é¢æ¢³çååºåæµè¯ï¼æ­ç¤ºäºä»¥ä¸éè¦åç°ï¼
*   <strong>å¤æ¨¡æèåçæææ§ï¼</strong> è®¸å¤å¼±çç£æ¹æ³éè¿èåLiDARä¸RGBå¾åæIMUä¿¡å·æ¥å¼¥è¡¥LiDARæ°æ®çä¸è¶³ï¼æé«äºå§¿æä¼°è®¡çé²æ£æ§ã
*   <strong>Transformeræ¶æçå´èµ·ï¼</strong> Transformerå¨å¤çLiDARç¹äºçé¿æä¾èµæ§åä¸è§åç»ææ¹é¢å±ç°åºå·¨å¤§æ½åï¼æä¸ºè®¸å¤ææ°æ¹æ³çéª¨å¹²ã
*   <strong>åææ°æ®çéè¦æ§ï¼</strong> é´äºçå®LiDARæ°æ®éçç¨ç¼ºæ§ï¼åææ°æ®çæåæ°æ®å¢å¼ºæ¯è®­ç»æ¨¡åãç¹å«æ¯é¢è®­ç»æ¨¡åä»¥å­¦ä¹ äººä½ä¸­å¿åéªçå³é®ç­ç¥ã
*   <strong>å¼±çç£å­¦ä¹ çæ½åï¼</strong> å¼±çç£æ¹æ³éè¿å©ç¨2Dæ æ³¨ãä¼ªæ ç­¾ãæå½±ä¸è´æ§ç­ç­ç¥ï¼ææç¼è§£äºå¯¹å¤§é3Dæ æ³¨çéæ±ã
*   <strong>æ¶é´ä¸è´æ§ï¼</strong> å»ºæ¨¡æ¶é´åºåä¿¡æ¯å¯¹äºæé«å§¿æä¼°è®¡çåç¡®æ§åé²æ£æ§è³å³éè¦ï¼å°¤å¶æ¯å¨å¤çé®æ¡åè¿å¨é¢æµæ¶ã</p>
<p>è¿äºç»æä¸ºç ç©¶äººåæä¾äºè¯¥é¢åå½åææ¯æ°´å¹³çæ¸æ°è§å¾ï¼å¹¶æåºäºæªæ¥ç ç©¶çææè·¯å¾ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®ç¨ç¼ºæ§ï¼</strong> ç¼ºä¹å¤§è§æ¨¡ãé«è´¨éæ æ³¨çLiDARäººä½å§¿æåç½æ ¼æ°æ®éæ¯æ ¸å¿ææã
*   <strong>å¤æ¨¡ææ¹æ³çä¾èµæ§ï¼</strong> ç°æå¼±çç£æ¹æ³ä»é«åº¦ä¾èµRGBå¾åæIMUä¿¡å·ç­è¾å©æ¨¡æï¼éä½å¯¹è¿äºæ¨¡æçä¾èµæ¯æªæ¥çæ¹åã
*   <strong>ç¸æºåæ°ä¾èµï¼</strong> å½åå¤æ¨¡ææ¹æ³ä¸¥éä¾èµç²¾ç¡®çç¸æºåæ°è¿è¡2D-3Då¯¹åºï¼è¿å¨å®éåºç¨ä¸­å¸¦æ¥äºææã
*   <strong>åé´éé®é¢ï¼</strong> ä¸åLiDARä¼ æå¨ç¹æ§ï¼å¦ç¹å¯åº¦ãèå´ãåªå£°æ¨¡å¼ï¼åæ«ææ¨¡å¼ï¼NRSä¸RMBï¼å¯¼è´çæ°æ®éä¹é´å­å¨åé´éï¼å½±åæ¨¡åçæ³åè½åã
*   <strong>åææ°æ®çå±éæ§ï¼</strong> å½ååææ°æ®çæå­å¨åä¸å¹éï¼å¦AMASSä¸»è¦åå«å®¤åå§¿æï¼èWODåSLOPER4Dæ¯å®¤å¤åºæ¯ï¼åçå®æå·®è·ï¼å°çº¿æå°æªè½å®å¨ææçå®LiDARä¼ æå¨çåªå£°ãç¨çæ§åè§è§ç¹æ§ï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>åå°å¯¹è¾å©æ¨¡æçä¾èµï¼</strong> æ¢ç´¢ä»ä½¿ç¨LiDARæ°æ®å®ç°å¼±çç£HPEåHMRçæ¹æ³ï¼ä¾å¦éè¿ä¼ªæ ç­¾ãèªè®­ç»æå¯¹æ¯å­¦ä¹ ã
*   <strong>æ´åæ¶é´ä¿¡æ¯ï¼</strong> è¿ä¸æ­¥å©ç¨LiDARå¸§åºåä¸­çæ¶é´çº¿ç´¢ï¼ä»¥å¢å¼ºå§¿æä¼°è®¡çåç¡®æ§ï¼èæ éé¢å¤çç£ã
*   <strong>æ´çå®çåææ°æ®çæï¼</strong> å¼åè½å¤ç´æ¥ä»çå®ä¸çåå¸ä¸­çæåæLiDARæ°æ®çæ¹æ³ï¼ç¹å«æ¯å©ç¨æ©æ£æ¨¡åç­çæå¼æ¨¡åã
*   <strong>æ°æ®é«æå­¦ä¹ ï¼</strong> éç¨èªçç£é¢è®­ç»ï¼ç¶åç¨å°éçç£æ°æ®å¯¹æ¨¡åè¿è¡å¾®è°ï¼ä»¥æé«LiDAR-based HPE/HMRçæ°æ®æçã
*   <strong>å¼±çç£HMRï¼</strong> éå¯¹HMRä»»å¡å¼åå¼±çç£æ¹æ³ï¼è¿æ¯ä¸ä¸ªå°æªååæ¢ç´¢ä½åæ¯å¹¿éçæ¹åã
*   <strong>æ¶é¤ç¸æºåæ°ä¾èµï¼</strong> ç¨å¯å­¦ä¹ æ¨¡ååä»£å¯¹æ¾å¼æ ¡åçéæ±ï¼å®ç°ç«¯å°ç«¯çå¯¹é½ã
*   <strong>åéåºææ¯ï¼</strong> å¼åè½å¤å¤çä¸åLiDARä¼ æå¨ç¹æ§åæ«ææ¨¡å¼ä¹é´åé´éçé²æ£æ¶æåéåºç­ç¥ï¼ä»¥æé«æ¨¡åçæ³åè½åã</p>
<hr />
<p>è¿ç¯ç»¼è¿°ä¸ºLiDAR-based 3Däººä½çè§£é¢åæä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»æ»ç»äºç°ææå°±ï¼æ´æ¸æ°å°ææäºæªæ¥çç ç©¶æ¹ååææï¼å¯¹äºæ¨å¨è¯¥é¢åçåå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds.</li>
<li>Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12197v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12197v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12193v1'></a></p>
<h2 id="domain-adaptive-pretraining-improves-primate-behavior-recognition"><a href="https://arxiv.org/abs/2509.12193v1">Domain-Adaptive Pretraining Improves Primate Behavior Recognition</a></h2>
<p><strong>Authors:</strong> Felix B. Mueller, Timo Lueddecke, Richard Vogg, Alexander S. Ecker</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, I.4.8; I.2.10; I.5</p>
<p><strong>Abstract:</strong></p>
<p>Computer vision for animal behavior offers promising tools to aid research in
ecology, cognition, and to support conservation efforts. Video camera traps
allow for large-scale data collection, but high labeling costs remain a
bottleneck to creating large-scale datasets. We thus need data-efficient
learning approaches. In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior. On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.
mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data. We show that most of the performance gain stems from the
DAP. Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples. Code is available at
https://github.com/ecker-lab/dap-behavior</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Felix B. Muellerç­äººæ°åçè®ºæâDomain-Adaptive Pretraining Improves Primate Behavior Recognitionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="domain-adaptive-pretraining-improves-primate-behavior-recognition_1">è®ºææè¦ï¼Domain-Adaptive Pretraining Improves Primate Behavior Recognition</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³å¨ç©è¡ä¸ºè¯å«é¢åä¸­çä¸ä¸ªæ ¸å¿ææï¼å¦ä½å©ç¨è®¡ç®æºè§è§ææ¯ï¼å¨é¢å¯¹é«æçæ æ³¨ææ¬åç¼ºä¹å¤§è§æ¨¡æ æ³¨æ°æ®éçæåµä¸ï¼å®ç°å¯¹çµé¿ç±»å¨ç©è¡ä¸ºçé«æãåç¡®è¯å«ãç¹å«æ¯ï¼è§é¢ç¸æºé·é±ï¼camera trapsï¼è½å¤æ¶éå¤§ééå¤æ°æ®ï¼ä½è¿äºæ°æ®çæ æ³¨ææ¬æé«ï¼éå¶äºæ°æ®é©±å¨å­¦ä¹ æ¹æ³çåºç¨ãå æ­¤ï¼è®ºæå¯»æ±æ°æ®é«æçå­¦ä¹ æ¹æ³æ¥æ¹è¿çµé¿ç±»è¡ä¸ºè¯å«ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæçæ ¸å¿åæ°å¨äºç»åäºèªçç£å­¦ä¹ åé¢åèªéåºé¢è®­ç»ï¼Domain-Adaptive Pretraining, DAPï¼æ¥è§£å³ä¸è¿°é®é¢ãå·ä½è´¡ç®åæ¬ï¼
*   <strong>å©ç¨é¢è®­ç»çV-JEPAæ¨¡åï¼</strong> ç ç©¶é¦åå©ç¨ä¸ä¸ªå¨äººç±»è§é¢æ°æ®ä¸é¢è®­ç»å¥½çV-JEPAï¼Video Joint Embedding Predictive Architectureï¼æ¨¡åä½ä¸ºéª¨å¹²ç½ç»ãV-JEPAæ¯ä¸ç§åºäºæ©ç èªç¼ç å¨ï¼masked autoencodingï¼çèªçç£å­¦ä¹ æ¹æ³ï¼è½å¤å­¦ä¹ è§é¢æ°æ®çææè¡¨ç¤ºã
*   <strong>å¼å¥é¢åèªéåºé¢è®­ç»ï¼DAPï¼ï¼</strong> å¨V-JEPAæ¨¡åçåºç¡ä¸ï¼ç ç©¶è¿ä¸æ­¥å¨æ æ ç­¾ççµé¿ç±»å¨ç©è§é¢æ°æ®ä¸è¿è¡DAPãè¿æå³çæ¨¡åå¨ç®æ é¢åï¼çµé¿ç±»è¡ä¸ºï¼çæªæ æ³¨æ°æ®ä¸ç»§ç»­è¿è¡èªçç£é¢è®­ç»ï¼ä»¥æ´å¥½å°éåºç®æ é¢åçç¹å¾åå¸ã
*   <strong>çµé¿ç±»ä¸­å¿éæ ·ç­ç¥ï¼</strong> éå¯¹çµé¿ç±»æ°æ®éçç¹ç¹ï¼å¦è§é¢ä¸­å¯è½åå«å¤ä¸ªçµé¿ç±»ä¸ªä½æä¸ªä½è¾å°ï¼ï¼è®ºæéç¨äºä¸ç§çµé¿ç±»ä¸­å¿éæ ·ç­ç¥ãéè¿ä½¿ç¨Grounding DINOç­å¼æ¾è¯æ±ç®æ æ£æµå¨ï¼è£åªåºè§é¢ä¸­åå«çµé¿ç±»å¨ç©çé¨åï¼ä»èå¨ä¸å¢å è¾å¥å°ºå¯¸çæåµä¸æè·æ´ç²¾ç»çä¸ªä½ç»èã
*   <strong>æ³¨æååç±»å¨ï¼</strong> å¨å»ç»çé¢è®­ç»éª¨å¹²ç½ç»ä¹ä¸ï¼è®­ç»äºä¸ä¸ªå¤å¤´äº¤åæ³¨æåï¼multihead cross-attentionï¼åç±»å¨ï¼ç¨äºæç»çè¡ä¸ºè¯å«ä»»å¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
è¯¥æ¹æ³å¨ä¸¤ä¸ªå¤§ç¿è¡ä¸ºæ°æ®éï¼PanAfåChimpACTï¼ä¸åå¾äºæ¾èçæ§è½æåï¼
*   <strong>PanAf500æ°æ®éï¼</strong> å¨Top-1åç¡®çä¸ï¼ç¸æ¯ä¹ååå¸çææ°æ¨¡åï¼æ§è½æåäº6.1ä¸ªç¾åç¹ã
*   <strong>ChimpACTæ°æ®éï¼</strong> å¨å¹³åç²¾åº¦åå¼ï¼mAPï¼ä¸ï¼æ§è½æåäº6.3ä¸ªç¾åç¹ã
*   <strong>DAPçå³é®ä½ç¨ï¼</strong> è®ºææç¡®æåºï¼å¤§é¨åæ§è½æåæ¥æºäºé¢åèªéåºé¢è®­ç»ï¼DAPï¼ï¼èéä»ä»ä½¿ç¨é¢è®­ç»çV-JEPAæ¨¡åãè¿å¼ºè°äºå¨ç®æ é¢åæ°æ®ä¸è¿è¡æ æ ç­¾é¢è®­ç»çéè¦æ§ã
*   <strong>æ°æ®é«ææ§ï¼</strong> ç»æè¡¨æï¼DAPä¸éè¦æ æ³¨æ ·æ¬ï¼è¿ä½¿å¶æä¸ºä¸ç§éå¸¸æåæ¯çæ°æ®é«æå­¦ä¹ æ¹æ³ï¼æå¤§å°éä½äºåå»ºå¤§è§æ¨¡æ æ³¨æ°æ®éçææ¬ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ä¾èµè¾¹çæ¡è£åªï¼</strong> å½åæ¹æ³ä¾èµäºå°è¾å¥è§é¢è£åªå°è¾¹çæ¡åãè½ç¶è¿æå©äºæ´å¥½å°å©ç¨ViTæ¨¡åæéçè¾å¥åè¾¨çï¼ä½å®å¿½ç¥äºå¨å±ä¸ä¸æä¿¡æ¯ï¼å¹¶ä¸å¯¹äºæ¯ä¸ªæå´è¶£åºåé½éè¦è¿è¡ä¸æ¬¡ååä¼ æ­ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>DAPçè§æ¨¡åï¼</strong> æªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢å¦ä½è¿ä¸æ­¥æ©å±DAPçåºç¨ï¼ç»åæ´å¤§ãæ´å¤æ ·åçå¨ç©æ°æ®æºï¼ä»¥å­¦ä¹ æ´å¥½çè¡¨ç¤ºå¹¶è¾å©è¡ä¸ºè¯å«ã
*   <strong>å°¾é¨ç±»å«æ§è½æåï¼</strong> ç»åæ´å¤æ°æ®æºæææ¹åå¯¹å°¾é¨ç±»å«ï¼å³æ°æ®éè¾å°çè¡ä¸ºç±»å«ï¼çè¯å«æ§è½ï¼å ä¸ºè¿äºç±»å«éå¸¸é¾ä»¥ä»å°éæ°æ®ä¸­å­¦ä¹ å°è¯å¥½çè¡¨ç¤ºã
*   <strong>å¨å±ä¸ä¸æçå©ç¨ï¼</strong> è§£å³å½åæ¹æ³å è£åªèå¿½ç¥å¨å±ä¸ä¸æçé®é¢ï¼å¯è½éè¿å¼å¥å¤å°ºåº¦ç¹å¾èåææ´å¤æçæ³¨æåæºå¶æ¥å®ç°ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºææåå°å±ç¤ºäºèªçç£å­¦ä¹ ä¸é¢åèªéåºé¢è®­ç»ç¸ç»åï¼è½å¤æ¾èæåçµé¿ç±»å¨ç©è¡ä¸ºè¯å«çæ§è½ï¼å°¤å¶æ¯å¨æ°æ®æ æ³¨ææ¬é«æçéå¤ç¯å¢ä¸­ãDAPä½ä¸ºä¸ç§æ éæ æ³¨æ ·æ¬çæ¹æ³ï¼ä¸ºå¨ç©è¡ä¸ºç ç©¶åä¿æ¤æä¾äºå¼ºå¤§çæ°å·¥å·ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior.</li>
<li>On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt.</li>
<li>We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data.</li>
<li>We show that most of the performance gain stems from the
DAP.</li>
<li>Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12193v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12193v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12125v1'></a></p>
<h2 id="railsafenet-visual-scene-understanding-for-tram-safety"><a href="https://arxiv.org/abs/2509.12125v1">RailSafeNet: Visual Scene Understanding for Tram Safety</a></h2>
<p><strong>Authors:</strong> Ing. Ondrej Valach, Ing. Ivan Gruber</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, 68T45 (Primary), 68T07, I.4.8</p>
<p><strong>Abstract:</strong></p>
<p>Tram-human interaction safety is an important challenge, given that trams
frequently operate in densely populated areas, where collisions can range from
minor injuries to fatal outcomes. This paper addresses the issue from the
perspective of designing a solution leveraging digital image processing, deep
learning, and artificial intelligence to improve the safety of pedestrians,
drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions. Using only
monocular video, the system identifies rails, localises nearby objects and
classifies their risk by comparing projected distances with the standard 1435mm
rail gauge. Experiments on the diverse RailSem19 dataset show that a
class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),
while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated
at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore
delivers accurate, annotation-light scene understanding that can warn drivers
before dangerous situations escalate. Code available at
https://github.com/oValach/RailSafeNet.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ing. Ondrej ValachåIng. Ivan Gruberæ°åçè®ºæâRailSafeNet: Visual Scene Understanding for Tram Safetyâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼RailSafeNet: æè½¨çµè½¦å®å¨è§è§åºæ¯çè§£</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æè½¨çµè½¦ä¸è¡äººãéªèªè¡è½¦èãé©¾é©¶åãå® ç©åæè½¨çµè½¦ä¹å®¢ä¹é´çå®å¨äºå¨é®é¢ãé´äºæè½¨çµè½¦ç»å¸¸å¨äººå£ç¨ å¯å°åºè¿è¡ï¼ç¢°æå¯è½å¯¼è´ä»è½»å¾®ä¼¤å®³å°è´å½åæï¼å æ­¤æé«æè½¨çµè½¦è¿è¡å®å¨æ§æ¯ä¸ä¸ªéè¦çææãæ ¸å¿ç ç©¶é®é¢æ¯å¦ä½å©ç¨æ°å­å¾åå¤çãæ·±åº¦å­¦ä¹ åäººå·¥æºè½ï¼éè¿å®æ¶åºæ¯çè§£æ¥é¢è­¦æ½å¨çè½¨éå¥ä¾µï¼ä»èæé«å®å¨æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
RailSafeNetæ¡æ¶æåºäºä»¥ä¸å³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼
*   <strong>å®æ¶èåæ¡æ¶ï¼</strong> æåºäºä¸ä¸ªåä¸ºRailSafeNetçå®æ¶æ¡æ¶ï¼å®èåäºè¯­ä¹åå²ãç®æ æ£æµåä¸ä¸ªåºäºè§åçâè·ç¦»è¯ä¼°å¨âï¼Distance Assessorï¼æ¥è¯å«è½¨éå¥ä¾µã
*   <strong>åç®è§é¢è·ç¦»ä¼°è®¡ï¼</strong> å¼å¥äºä¸ç§æ éä»»ä½å³äºæè·ãç¸æºæè®¾ç½®åæ°çåéªç¥è¯ï¼ä»éè¿åç®å¾åå³å¯è¿è¡è½¨éè·ç¦»ä¼°è®¡çæ¹æ³ãè¿éè¿å©ç¨æ å1435æ¯«ç±³çè½¨è·ä½ä¸ºå¯é çè·ç¦»åèæ¥å®ç°ã
*   <strong>å®å¶åå²å¤çï¼</strong> éç¨äºä¸ç§å®å¶çåå²å¤çæ¹æ³ï¼åæ¬æ°æ®ç±»å«è¿æ»¤åæ©ç åå¤çï¼è¿å¨RailSem19æ°æ®éä¸åå¾äºä¼äºåå§è®ºæçåå²ç»æã
*   <strong>è·ç¦»è¯ä¼°å¨ç³»ç»ï¼</strong> æåºäºä¸ä¸ªâè·ç¦»è¯ä¼°å¨âç³»ç»ï¼å®å¤çåºæ¯åå²åç®æ æ£æµçè¾åºï¼ä»¥åç¡®ä¼°è®¡ç©ä½ä¸è½¨éçè·ç¦»ï¼å¹¶æ ¹æ®å¶æ¥è¿ç¨åº¦å¯¹ç©ä½è¿è¡é£é©åç±»ï¼ä¾å¦ï¼ä½¿ç¨é¢è²ç¼ç åºåä¸åçº§å«çå±é©ï¼ã
*   <strong>æ éæ·±åº¦ä¼ æå¨æLiDARï¼</strong> è¯¥æ¹æ³ä¸éè¦ç¸æºæ ¡åãæ·±åº¦ä¼ æå¨æLiDARï¼ä½¿å¶æä¸ºä¸ç§ä½ææ¬ä¸æäºé¨ç½²çè§£å³æ¹æ¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>è¯­ä¹åå²æ§è½ï¼</strong> å¨å¤æ ·åçRailSem19æ°æ®éä¸ï¼ç»è¿ç±»å«è¿æ»¤çSegFormer B3æ¨¡åå¨äº¤å¹¶æ¯ï¼IoUï¼æ¹é¢è¾¾å°äº65%ï¼è¶è¶äºç°æåºåã
*   <strong>ç®æ æ£æµæ§è½ï¼</strong> ç»è¿å¾®è°çYOLOv8æ¨¡åå¨IoUéå¼ä¸º0.50æ¶ï¼å¹³åç²¾åº¦ï¼mAPï¼è¾¾å°äº75.6%ã
*   <strong>è·ç¦»è¯ä¼°åç¡®æ§ï¼</strong> å®ééªè¯å®éªè¡¨æï¼å³ä½¿å¨å¼¯æ²è½¨éæå¾æç¸æºè§è§ç­æææ§æ¡ä»¶ä¸ï¼ç³»ç»ä¹è½åç¡®ä¼°è®¡è·ç¦»ï¼åå·®ä»éäºå åç±³ã
*   <strong>å®éæä¹ï¼</strong> RailSafeNetè½å¤æä¾åç¡®ãè½»éçº§æ æ³¨çåºæ¯çè§£ï¼å¯ä»¥å¨å±é©æåµåçº§ä¹ååé©¾é©¶åååºè­¦åï¼ä»èæ¾èéä½äºæé£é©åä¸¥éæ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>è®­ç»æ°æ®è´¨éï¼</strong> è®ºææåºï¼å°½ç®¡è¿è¡äºç±»å«è¿æ»¤åæ©ç åå¤çï¼ä½åå§RailSem19æ°æ®éçåå²æ©ç ä»å­å¨ä¸çæ³ä¹å¤ï¼ä¾å¦ä¸åç¡®çæ æ³¨ãæªæ£æµæé¨åæ£æµçç©ä½ï¼ä»¥åä¸çæ³åå²ç±»å«ä¹é´çä¼¼ä»»æçè¾¹ç¼ãè¿å½±åäºæ¨¡åçæ§è½ã
*   <strong>ç±»å«ä¸å¹³è¡¡ï¼</strong> ç®æ æ£æµæ¨¡åå¨è®­ç»æ¶é¢ä¸´ç±»å«ä¸å¹³è¡¡é®é¢ï¼ä¾å¦âäººâåâæ±½è½¦âç­å³é®ç©ä½ç±»å«å¨æ°æ®éä¸­ä»£è¡¨æ§ä¸è¶³ï¼å¯¼è´è¿äºå³é®ç©ä½çåç¡®æ§è¾ä½ã
*   <strong>å¤æåºæ¯çåå·®ï¼</strong> å¨å¼¯æ²è½¨éç­å¤æåºæ¯ä¸­ï¼ç±äºä¸å®ç¾çè½¨éåå²ï¼æ°´å¹³æ¨ªæªé¢æµéå¯è½å¯¼è´è½¨éæ¾å¾æ´å®½ï¼ç¥å¾®å¤¸å¤§è·ç¦»ä¼°è®¡ï¼å°¤å¶æ¯å¨å¾åçå³ä¾§ãå·¦ä¾§ççªåå²å¯è½ä½¿å³é®åºååååç§»ï¼å¯¼è´çº¦3-4åç±³çåå·®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è®­ç»æ°æ®éå¢å¼ºï¼</strong> æªæ¥çæ¹è¿å°éè¿å¢å¼ºè®­ç»æ°æ®éæ¥å®ç°ï¼ç¹å«æ¯æ´ç²¾ç¡®çè½¨éåè½¨åºæ æ³¨ã
*   <strong>éæå°èªå¨å¶å¨ç³»ç»ï¼</strong> æåºçæ¡æ¶ææä½ä¸ºæè½¨çµè½¦æä½åçå®ç¨æ¥å¸¸å·¥å·ï¼å¹¶æå¯è½éæå°èªå¨å¶å¨ç³»ç»ç­ä¸­ï¼ä»¥è¿ä¸æ­¥éä½äºæé£é©åä¸¥éæ§ã
*   <strong>æ´å¹¿æ³çé¨ç½²åéªè¯ï¼</strong> å°½ç®¡å¨æ¯å°æ£®è¿è¡äºå®ééªè¯ï¼ä½æªæ¥å¯ä»¥å¨æ´å¤æ ·åççå®ä¸çåºæ¯ä¸­è¿è¡é¨ç½²åéªè¯ï¼ä»¥è¿ä¸æ­¥è¯æå¶é²æ£æ§ã</p>
<hr />
<p>æ»èè¨ä¹ï¼RailSafeNetä¸ºæè½¨çµè½¦å®å¨æä¾äºä¸ä¸ªåæ°ä¸å®ç¨çè®¡ç®æºè§è§è§£å³æ¹æ¡ï¼éè¿èåæ·±åº¦å­¦ä¹ æ¨¡åååºäºè§åçè·ç¦»è¯ä¼°ï¼å®ç°äºå¯¹è½¨éå¥ä¾µçå®æ¶ãåç¡®é¢è­¦ï¼ä¸ºåå¸äº¤éå®å¨å¸¦æ¥äºæ¾èçè¿æ­¥ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12125v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12125v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12105v1'></a></p>
<h2 id="fs-sam2-adapting-segment-anything-model-2-for-few-shot-semantic-segmentation-via-low-rank-adaptation"><a href="https://arxiv.org/abs/2509.12105v1">FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</a></h2>
<p><strong>Authors:</strong> Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Few-shot semantic segmentation has recently attracted great attention. The
goal is to develop a model capable of segmenting unseen classes using only a
few annotated samples. Most existing approaches adapt a pre-trained model by
training from scratch an additional module. Achieving optimal performance with
these approaches requires extensive training on large-scale datasets. The
Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and
video segmentation with a modular design. In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank
Adaptation (LoRA) to the original modules in order to handle the diverse images
typically found in standard datasets, unlike the temporally connected frames
used in SAM2's pre-training. With this approach, only a small number of
parameters is meta-trained, which effectively adapts SAM2 while benefiting from
its impressive segmentation performance. Our method supports any K-shot
configuration. We evaluate FS-SAM2 on the PASCAL-5<script type="math/tex">^i</script>, COCO-20<script type="math/tex">^i</script> and
FSS-1000 datasets, achieving remarkable results and demonstrating excellent
computational efficiency during inference. Code is available at
https://github.com/fornib/FS-SAM2</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Bernardo Forniç­äººæ°åçè®ºæâFS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼FS-SAM2: éè¿ä½ç§©éåºå°Segment Anything Model 2åºç¨äºå°æ ·æ¬è¯­ä¹åå²</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³å°æ ·æ¬è¯­ä¹åå²ï¼Few-Shot Semantic Segmentation, FSSï¼çææãFSSçç®æ æ¯å¼åä¸ä¸ªæ¨¡åï¼è½å¤ä»ä½¿ç¨å°éå¸¦æ æ³¨çæ ·æ¬æ¥åå²æªè§è¿çç±»å«ãç°ææ¹æ³éå¸¸éè¦ä»å¤´å¼å§è®­ç»é¢å¤çæ¨¡åï¼è¿éè¦å¨å¤§è§æ¨¡æ°æ®éä¸è¿è¡å¤§éè®­ç»æè½è¾¾å°æä½³æ§è½ãä½èæåºï¼å¦ä½ææå°å°åSegment Anything Model 2 (SAM2) è¿æ ·çåºç¡æ¨¡åéåºå°FSSä»»å¡ä¸­ï¼ä½¿å¶å¨å¤çå¤æ ·åå¾åï¼èéSAM2é¢è®­ç»ä¸­ä½¿ç¨çæ¶åºè¿æ¥å¸§ï¼æ¶ä»è½ä¿æé«æ§è½åè®¡ç®æçï¼æ¯ä¸ä¸ªå³é®é®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
è¯¥è®ºææåºäºFS-SAM2ï¼å¶æ ¸å¿åæ°ç¹å¨äºï¼
*   <strong>SAM2çè§é¢è½åéå®åï¼</strong> FS-SAM2å·§å¦å°å°SAM2çè§é¢åå²è½åç´æ¥éæ°ç¨äºFSSä»»å¡ãå®å°æ¯æå¾åè§ä¸ºå¸¦æ æ³¨çè§é¢å¸§ï¼èæ¥è¯¢å¾ååä½ä¸ºåç»­å¾åå²çå¸§ï¼å©ç¨SAM2çåå­æ³¨æåæ¨¡åè¿è¡åç´ çº§å¹éã
*   <strong>ä½ç§©éåºï¼LoRAï¼çåºç¨ï¼</strong> ä¸ºäºé«æä¸é²æ£å°éåºSAM2æ¨¡åï¼ä½èå°LoRAåºç¨äºSAM2çåå§æ¨¡åï¼åæ¬å¾åç¼ç å¨ãåå­ç¼ç å¨ååå­æ³¨æåæ¨¡åãLoRAéè¿å¼å¥å°éå¯è®­ç»åæ°ï¼ä»åè®­ç»è¿äºåæ°ï¼èä¿æåå§åæ°å»ç»ï¼æ¥å¾®è°éå®ççº¿æ§å±ï¼ä»èå¨å¤çæ åæ°æ®éä¸­å¸¸è§çåç§å¾åæ¶ï¼ææéåºæ¨¡åå¹¶å¢å¼ºç¹å¾æåï¼èæ éä»å¤´è®­ç»ä»»ä½ç¹å®æ¨¡åã
*   <strong>K-shotéç½®çéç¨æ¯æï¼</strong> è¯¥æ¹æ³æ¯æä»»ä½K-shotéç½®ï¼æ éä¸ºä¸åçKå¼è®­ç»ä¸åçæ¨¡åãææKä¸ªæ¯æå¾åé½è¢«è§ä¸ºååæ æ³¨çå¸§ï¼å¹¶ä»¥ç¸åæ¡æ¶å¤çã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
FS-SAM2å¨PASCAL-5Â²ãCOCO-20Â²åFSS-1000æ°æ®éä¸è¿è¡äºå¹¿æ³è¯ä¼°ï¼åå¾äºæ¾èææï¼
*   <strong>åè¶çæ§è½ï¼</strong> å¨PASCAL-5Â²æ°æ®éç1-shotåºæ¯ä¸­ï¼FS-SAM2å®ç°äº73.4%çmIoUï¼è¶è¶äºæå¯æ¯è¾çæ¹æ³VRP-SAM 1.5%ãå¨COCO-20Â²æ°æ®éä¸ï¼FS-SAM2å¨1-shotåºåæµè¯ä¸­ä¼äºVRP-SAM 1.4%ï¼å¹¶å¨5-shotåºåæµè¯ä¸­è¡¨ç°åºæ¾èæåã
*   <strong>è®¡ç®æçï¼</strong> è¯¥æ¹æ³å¨æ¨çè¿ç¨ä¸­è¡¨ç°åºåè¶çè®¡ç®æçï¼è¿å¾çäºLoRAä»å¾®è°å°éåæ°ï¼å¹¶é¿åäºé¢å¤éª¨å¹²ç½ç»ææ¨¡åçå¼å¥ã
*   <strong>æ³åè½ååé²æ£æ§ï¼</strong> å¨FSS-1000æ°æ®éååè¿ç§»åºæ¯ï¼è®­ç»æ°æ®ä¸æµè¯æ°æ®å­å¨æ¾èåå·®è·ï¼ä¸çè¯ä¼°è¡¨æï¼FS-SAM2å·æå¾å¼ºçé²æ£æ§ï¼LoRAæåå°å°æ¨¡åéåºå°å°æ ·æ¬è®¾ç½®ã
*   <strong>å®æ§ç»æï¼</strong> å®æ§æ¯è¾æ¾ç¤ºï¼FS-SAM2è½å¤çæåç¡®å®æ´çæ©ç ï¼ææç²¾ç»ç»èåæ´ä¸ªå¯¹è±¡åºåï¼çè³è½ä¿®æ­£å°é¢çå¼æ æ³¨ä¸­å­å¨çå¾®å°ä¸åç¡®æ§ï¼èSAM2å¨å¤çä¸ç¸ä¼¼çæ¯æ-æ¥è¯¢å¯¹æ¶åè¡¨ç°ä¸ä½³ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>5-shotè®­ç»çä¼åç©ºé´ï¼</strong> è®ºææåºï¼è½ç¶FS-SAM2å¨5-shotè®¾ç½®ä¸­æææ¹è¿ï¼ä½éè¿æç¡®å°å¨5-shotæºå¶ä¸è®­ç»æ¨¡åï¼å¯è½ä¼è·å¾è¿ä¸æ­¥çæåã
*   <strong>è·¨ç±»å«éä¿¡è½åï¼</strong> SAM2æ¬èº«ç¼ºä¹è·¨ç±»å«éä¿¡è½åï¼è¿å¯è½éå¶äºFS-SAM2å¨å¤ç±»å«å°æ ·æ¬åå²ä»»å¡ä¸­çè¡¨ç°ï¼èè¿æ¯ä¸ä¸ªç¸å¯¹æªè¢«ååæ¢ç´¢ä½å·æå®éä¼å¿çä»»å¡ã
*   <strong>ä¸DINOv2éª¨å¹²ç½ç»çæ¯è¾ï¼</strong> è®ºææå°ï¼åGF-SAMè¿æ ·ä½¿ç¨DINOv2-Léª¨å¹²ç½ç»ï¼å¨åå«PASCAL-VOCçæ°æ®éä¸é¢è®­ç»ï¼çæ¹æ³ï¼è½ç¶èµæºå¯éåº¦æ´é«ï¼ä½å¨æäºæåµä¸ä»è½åå¾é¢åæ§è½ï¼è¿æç¤ºäºåºç¡æ¨¡åéæ©åé¢è®­ç»æ°æ®çéè¦æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>æ¿ä»£ç5-shotè®­ç»æ¹æ³ï¼</strong> æ¢ç´¢æ´ææç5-shotè®­ç»ç­ç¥ï¼ä»¥è¿ä¸æ­¥æåæ¨¡åå¨å¤æ ·æ¬åºæ¯ä¸çæ§è½ã
*   <strong>æ´åæ°é«æçå¾®è°ç­ç¥ï¼</strong> æ·±å¥ç ç©¶å¶ä»åæ°é«æçå¾®è°æ¹æ³ï¼ä»¥å¨ä¿æé«æ§è½çåæ¶ï¼è¿ä¸æ­¥åå°å¯è®­ç»åæ°çæ°éã
*   <strong>æ©å±å°å¤ç±»å«è®¾ç½®ï¼</strong> å°FS-SAM2æ¹æ³æ©å±å°å¤ç±»å«å°æ ·æ¬è¯­ä¹åå²ä»»å¡ï¼è§£å³SAM2å¨è·¨ç±»å«éä¿¡æ¹é¢çéå¶ï¼ä»¥æ»¡è¶³æ´å¹¿æ³çå®éåºç¨éæ±ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å·§å¦å°éå®åSAM2çè§é¢åå²è½åå¹¶ç»åLoRAè¿è¡é«æå¾®è°ï¼ä¸ºå°æ ·æ¬è¯­ä¹åå²æä¾äºä¸ä¸ªç®åèææçæ¡æ¶ãFS-SAM2å¨ä¿æè®¡ç®æççåæ¶ï¼å±ç°åºä¸ç°ææåè¿æ¹æ³ç¸å½çè³æ´ä¼çæ§è½ï¼ä¸ºå©ç¨å¤§ååºç¡æ¨¡åè§£å³ä½æ°æ®éè®¡ç®æºè§è§ä»»å¡å¼è¾äºæ°çéå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task.</li>
<li>Our method supports any K-shot
configuration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12105v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12105v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12039v1'></a></p>
<h2 id="ram-robust-representation-learning-via-adaptive-mask-for-all-in-one-image-restoration"><a href="https://arxiv.org/abs/2509.12039v1">RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</a></h2>
<p><strong>Authors:</strong> Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, Chongyi Li</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This work presents Robust Representation Learning via Adaptive Mask (RAM++),
a two-stage framework for all-in-one image restoration. RAM++ integrates
high-level semantic understanding with low-level texture generation to achieve
content-oriented robust restoration. It addresses the limitations of existing
degradation-oriented methods in extreme scenarios (e.g., degradations strongly
coupled with image structures). RAM++ also mitigates common challenges such as
unbalanced performance across tasks, overfitting to seen degradations, and weak
generalization to unseen ones through three key designs: 1) Adaptive
Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level
masks to semantically rich and textured regions. This design enables the
network to learn both generative priors and image content priors from various
degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy
that leverages DINOv2's semantically consistent and degradation-invariant
representations, together with efficient feature fusion, to achieve faithful
and semantically coherent restoration. With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations. Our code and model will be released at
https://github.com/DragonisCV/RAM</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zilong Zhangç­äººæ°åçè®ºæâRAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restorationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="ram">è®ºææè¦ï¼RAM++: åºäºèªéåºæ©ç çå¨è½å¾åæ¢å¤çé²æ£è¡¨ç¤ºå­¦ä¹ </h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨è½å¾åæ¢å¤ï¼all-in-one image restorationï¼é¢åä¸­çæ ¸å¿ææãç°æçéåå¯¼åæ¹æ³å¨æç«¯åºæ¯ï¼ä¾å¦ï¼éåä¸å¾åç»æå¼ºè¦åï¼ä¸è¡¨ç°ä¸ä½³ï¼å¹¶ä¸æ®éå­å¨ä»»å¡é´æ§è½ä¸å¹³è¡¡ãå¯¹å·²ç¥éåè¿æåä»¥åå¯¹æªç¥éåæ³åè½åå¼±ç­é®é¢ãä½èè®¤ä¸ºï¼å¾åæ¢å¤çæ¬è´¨å¨äºå­¦ä¹ é²æ£çåå¨è¡¨ç¤ºï¼è½å¤ä»éåè¾å¥ä¸­æ¢å¤åºæ¬å¾åä¿¡æ¯ï¼èä¸æ¯ä»ä»å»é¤éåæ¨¡å¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
RAM++ æåºäºä¸ä¸ªä¸¤é¶æ®µæ¡æ¶ï¼éè¿æ´åé«çº§è¯­ä¹çè§£åä½çº§çº¹ççæï¼å®ç°åå®¹å¯¼åçé²æ£æ¢å¤ãå¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>èªéåºè¯­ä¹æç¥æ©ç  (AdaSAM)ï¼</strong> ä¸ç§é¢è®­ç»ç­ç¥ï¼å®å¯¹è¯­ä¹ä¸°å¯åçº¹çåºååºç¨åç´ çº§æ©ç ãè¿ä½¿å¾ç½ç»è½å¤ä»åç§éåä¸­å­¦ä¹ çæåéªåå¾ååå®¹åéªï¼ä»èå¨ç»ä¸çæ½å¨ç©ºé´ä¸­ç¼ç å¤æ ·åçéåãä¸ç®åéæºåç´ çº§æ©ç ä¸åï¼AdaSAM ç»åäºåç´ çº§æ¢å¤ååºåçº§è¯­ä¹çè§£ï¼ä¸æ³¨äºé¾ä»¥éå»ºçåºåã</li>
<li><strong>æ©ç å±æ§ä¼ å¯¼ (MAC)ï¼</strong> ä¸ç§éæ©æ§å¾®è°ç­ç¥ï¼éè¿è¯ä¼°æ¯ä¸ªç½ç»å±å¨å¼¥åæ©ç é¢è®­ç»åå¨å¾åå¾®è°ä¹é´å®æ´æ§å·®è·æ¹é¢çè´¡ç®ï¼è°æ´è´¡ç®è¾é«çå±ï¼åæ¶ä¿çå·²å­¦ä¹ çåéªãè¿åè®¸æ¨¡åå¨ä»æ´æ°å°éå±ï¼ä¾å¦30%ï¼çæåµä¸å®ç°é«æ§è½ã</li>
<li><strong>é²æ£ç¹å¾æ­£åå (RFR)ï¼</strong> è¯¥ç­ç¥å©ç¨ DINOv2 çè¯­ä¹ä¸è´æ§åéåä¸åæ§è¡¨ç¤ºï¼ç»åé«æçç¹å¾èåï¼å®ç°å¿ å®ä¸è¯­ä¹è¿è´¯çæ¢å¤ãDINOv2 çç¹å¾è¢«æ´åå°å¾®è°è¿ç¨ä¸­ï¼ä»¥å¢å¼ºå¾åæ¢å¤æ§è½ï¼å°¤å¶æ¯å¨å¤æéååºæ¯ä¸ï¼å¹¶å¼¥è¡¥æ¢å¤ç½ç»å¨é¢è®­ç»é¶æ®µå­¦ä¹ ä¸»è¦åå®¹è½åçä¸è¶³ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
RAM++ å¨å·²ç¥ãæªç¥ãæç«¯åæ··åéååºæ¯ä¸åå®ç°äºé²æ£ãåè¡¡ä¸æåè¿çæ§è½ã</p>
<ul>
<li><strong>æ§è½æåï¼</strong> å¨3ä»»å¡å7ä»»å¡å¾åæ¢å¤è®¾ç½®ä¸ï¼RAM++ å¨PSNRåSSIMç­ææ ä¸è¶è¶äºç°ææåè¿æ¹æ³ãä¾å¦ï¼å¨7ä»»å¡è®¾ç½®ä¸ï¼RAM++ å¨å®å¨å¾®è°ï¼100%ï¼æ¶ï¼æ¯æ¬¡ä¼æ¹æ³æåäº0.70dBã</li>
<li><strong>åè¡¡æ§è½ï¼</strong> éçä»»å¡æ°éçå¢å ï¼RAM++ ç­ç¥ä¸ä»æåäºæ´ä½æ§è½ï¼è¿å°åå§ä»»å¡çæ§è½ä¸ééå¶å¨5.07%ä»¥åï¼å¹¶å¨ä¸ä¸ªä»»å¡ä¸­è¡¨ç°åºæä½çæ¹å·®ï¼4.83ï¼ï¼è¡¨æå¶å¨å¤ä»»å¡åºæ¯ä¸çæ§è½å¹³è¡¡æ§ã</li>
<li><strong>æ³åè½åï¼</strong> å¨åå¸å¤ï¼OODï¼éåè¯ä¼°ä¸­ï¼RAM++ å¨æªç¥åªå£°ç±»åä¸å®ç°äºæ¾èçPSNRå¢çï¼å¹¶ææå¤çäºæ°´ä¸å¾åå¢å¼ºç­ä»»å¡ï¼å±ç¤ºäºå¼ºå¤§çæ³åè½ååé²æ£æ§ã</li>
<li><strong>å¯è§£éæ§åæï¼</strong> éè¿å ææåºå¾ï¼CEMï¼åæï¼è®ºææ­ç¤ºäºRAM++çåä¸ªæ¾èç¹æ§ï¼ææçè¯­ä¹çè§£ãç¨³å®çå¨å±ä¿¡æ¯è·åãåç¡®çæ­£è´ä¿¡æ¯å¤å«ä»¥åä¼åçèæ¯ç»æéå»ºã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡RAM++è¡¨ç°åºè²ï¼ä½ä»å­å¨å±éæ§ï¼</p>
<ul>
<li><strong>ä»»å¡é´å²çªï¼</strong> å¨åå«å¤æ ·åéåçæ··åæ°æ®éä¸è¿è¡å¾®è°ï¼ä¸å¯é¿åå°ä¼é¢ä¸´ä»»å¡é´çåºæå²çªã</li>
<li><strong>ç»èæ¨æ­ææï¼</strong> æ©ç å¾åå»ºæ¨¡çç¹æ§ä½¿å¾å¨åæ ¸å»æ¨¡ç³ç­ä»»å¡ä¸­æ¨æ­ç»èæ´å·æææ§ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çç ç©¶æ¹ååæ¬ï¼</p>
<ul>
<li><strong>å¤ä»»å¡å­¦ä¹ åä¼åæ°æ®æ··åç­ç¥ï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¦ä½æ´å¥½å°å¤çä»»å¡é´çå²çªåæ°æ®æ··åã</li>
<li><strong>æ©å±å°è§é¢æ¢å¤ï¼</strong> å°è¯¥æ¡æ¶æ©å±å°è§é¢æ¢å¤é¢åï¼å¹¶ç»åæ¶é´ä¸è´æ§ï¼ä»¥å¢å¼ºå¶éç¨æ¢å¤è½åã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼RAM++ éè¿å¼å¥ AdaSAMãMAC å RFR ç­åæ°è®¾è®¡ï¼ä¸ºå¨è½å¾åæ¢å¤æä¾äºä¸ç§æ°é¢çåå®¹å¯¼åè§è§ãå®æåå°è§£å³äºç°ææ¹æ³å¨æç«¯åºæ¯ãæ§è½ä¸å¹³è¡¡åæ³åè½åæ¹é¢çå±éæ§ï¼å¹¶å¨å¤ä¸ªåºåæµè¯ä¸­åå¾äºæ¾èçãæåè¿çææï¼ä¸ºè¯¥é¢åæªæ¥çåå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors.</li>
<li>With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12039v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12039v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11895v1'></a></p>
<h2 id="integrating-prior-observations-for-incremental-3d-scene-graph-prediction"><a href="https://arxiv.org/abs/2509.11895v1">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a></h2>
<p><strong>Authors:</strong> Marian Renz, Felix Igelbrink, Martin Atzmueller</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Marian Renz, Felix Igelbrink, Martin Atzmuelleræ°åçè®ºæâIntegrating Prior Observations for Incremental 3D Scene Graph Predictionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="3d">è®ºææè¦ï¼æ´ååéªè§æµä»¥å®ç°å¢éå¼3Dåºæ¯å¾é¢æµ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½å3Dè¯­ä¹åºæ¯å¾ï¼3DSSGï¼çææ¹æ³ä¸»è¦ä¾èµä¼ æå¨æ°æ®ï¼ä¸éå¸¸åè®¾è½å¤è®¿é®å®æ´çåºæ¯éå»ºï¼è¿éå¶äºå®ä»¬å¨çå®ä¸çãå¢éå¼ç¯å¢ä¸­çåºç¨ãè¿äºæ¹æ³æªè½æææ´åæ¥èªè¯­ä¹ä¸°å¯ç¯å¢çé¢å¤ä¿¡æ¯ï¼ä¹æ æ³å¨åºæ¯æ°æ®æµå®æ¶è·åçå¢éå¼è®¾ç½®ä¸­è¿è¡é¢æµåè§£éãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¼åä¸ç§è½å¤çµæ´»æ´åå¤æ¨¡æä¿¡æ¯ï¼ç¹å«æ¯åéªè§æµï¼çå¢éå¼3DSSGé¢æµæ¨¡åï¼åæ¶é¿åå¯¹å®æ´åºæ¯éå»ºçä¾èµã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>æ°åå¼æå¾æ¨¡åï¼</strong> è®ºææåºäºä¸ç§æ°é¢çå¼æå¾æ¨¡åï¼ç¨äºå¢éå¼3DSSGé¢æµãè¯¥æ¨¡åå°åºæ¯å¾æå»ºæéçå­ä»»å¡æ´åå°ä¸ä¸ªå¤å±æ¶æä¸­ï¼ä»èæ éä¸é¨æ¨¡åå³å¯çµæ´»å°æ´åå¤æ¨¡æä¿¡æ¯ã
*   <strong>å¨å±-å±é¨åºæ¯è¡¨ç¤ºæ´åï¼</strong> è¯¥æ¹æ³çæ ¸å¿æ¯ä¸ä¸ªå¼æåºæ¯å¾è®¾è®¡ï¼å®èåäºä¼ æå¨æ°æ®åæ¥èªååæ¶é´æ­¥çè§æµç»æï¼éè¿å¨å±å±ï¼æä¾ç©ºé´ãå ä½åè¯­ä¹ä¸ä¸æï¼åå±é¨å±ï¼æ´åå½åä¼ æå¨æ°æ®ï¼å®ç°ã
*   <strong>åéªè§æµçç´æ¥æ´åï¼</strong> æ¨¡åéè¿å°å®ä¾ä»å½åå¸§é¾æ¥å°ååé¢æµçèç¹ï¼ç´æ¥å°åéªé¢æµæ´åå°æ¶æ¯ä¼ éè¿ç¨ä¸­ï¼ä»èå©ç¨æ©æè§æµä¿¡æ¯è¿è¡é¢æµï¼èæ éå­å¨å®æ´çåå²ç¹äºã
*   <strong>å¤æ¨¡æç¹å¾åµå¥ï¼</strong> æ¨¡åéè¿å°ç©ºé´ãå ä½åè¯­ä¹ç¹å¾ç´æ¥åµå¥å°æ¶æ¯ä¼ éè¿ç¨ä¸­ï¼é«æå°å­å¨åæ´åè¿äºç¹å¾ï¼é¿åäºå­å¨å¤§éçç¹äºæ®µææ¶é´åºåæ°æ®ãç¹å«æ¯ï¼å¨å±èç¹å¯ä»¥åå«åºäºç±»å«æ ç­¾çç¬ç­ç¼ç æCLIPåµå¥çææ¬æ ç­¾ã
*   <strong>å¼æGNNçåºç¨ï¼</strong> è®ºææ¢ç´¢äºä½¿ç¨å¼æå¾ç¥ç»ç½ç»ï¼GNNsï¼æ¥æ¹è¿è¯­ä¹ç¸å³ä¿¡æ¯çæ´åï¼å¹¶è¯ä¼°äºGraphSAGEåHGTç­ä¸åGNNæ¶æã
*   <strong>é¢å¤çè¾¹ç¼ç±»åï¼</strong> ä¸ºäºè¯ä¼°æ¨¡åççµæ´»æ§ï¼å¼å¥äºä¸ç§é¢å¤çå¨å±èç¹é´è¾¹ç¼ç±»åï¼è¯¥è¾¹ç¼ç±»ååºäºå ä½ç¢°ææ£æ¥åè°æ³¢ä¸­å¿æ§ï¼æä¾äºææä¸ä¸åçå­å¾ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æåï¼</strong> å¨3DSSGæ°æ®éä¸çè¯ä¼°è¡¨æï¼éè¿è¯­ä¹åµå¥ï¼å¦CLIPï¼ååéªè§æµå¢å¼ºçGNNsä¸ºå¤æãçå®ä¸ççç¯å¢æä¾äºå¯æ©å±ä¸éç¨çè§£å³æ¹æ¡ã
*   <strong>å¼ææ¨¡åçä¼å¿ï¼</strong> å¼ææ¨¡åå¨å³ç³»é¢æµæ¹é¢è¡¨ç°åºè²ï¼å°¤å¶æ¯å¨æ´åäºCLIPåµå¥åï¼HGT+CLIPæ¨¡åå¨å³ç³»é¢æµæ¹é¢è¾¾å°äºæé«æ§è½ãè¿è¡¨æå¼ææ¨¡åéå¸¸éåæè·3DSSGä¸­ä¸°å¯çè¯­ä¹ç»æã
*   <strong>å¯¹éè¯¯åéªé¢æµçé²æ£æ§ï¼</strong> å³ä½¿å¨å¨å±å±å¼å¥20%æ50%çéè¯¯æ ç­¾ï¼æ¨¡åå¨èç¹åè¾¹ç¼åç±»ä»»å¡ä¸çæ§è½ä¸éç¸å¯¹è¾å°ï¼è¡¨ææ¨¡åå·æä¸å®çé²æ£æ§ãè¿è¯´æå­¦ä¹ å°çåéªç¹å¾å³ä½¿å¨éåº¦çæ ç­¾æåä¸ä»ç¶å·æä¿¡æ¯éã
*   <strong>å¤æ¨¡æä¿¡æ¯æ´åççµæ´»æ§ï¼</strong> ç»æè¡¨æï¼ææåºçæ¨¡åè½å¤æ ç¼å°å°å¤æ¨¡æä¿¡æ¯æ´åå°æ¶æ¯ä¼ éè¿ç¨ä¸­ï¼èæ éå¤é¨æ¨¡åï¼è¿éªè¯äºå¶æ¶æççµæ´»æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¯¹åéªè§æµçä¾èµï¼</strong> å°½ç®¡æ¨¡åå¯¹éåº¦çæ ç­¾æåå·æé²æ£æ§ï¼ä½å³ç³»é¢æµçæ§è½ä¸éè¡¨ææ¨¡åå¯¹åéªè§æµçå¼ºçä¾èµï¼è¿å¨å®éåºç¨åºæ¯ä¸­éè¦ç¼è§£ã
*   <strong>GNNå±éå¶ï¼</strong> ç±äºæä½¿ç¨çGNNå±çéå¶ï¼è¾¹ç¼ç¹å¾å¨æ¶æ¯ä¼ éè¿ç¨ä¸­æªè¢«ä½¿ç¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨å°ºåº¦3Dè¯­ä¹æ å°ï¼</strong> æªæ¥çå·¥ä½å°æ¢ç´¢å°è¯¥æ¶æåºç¨äºå¨å°ºåº¦3Dè¯­ä¹æ å°ï¼ä»¥æ¯æçå®ä¸ççæºå¨äººä»»å¡ã
*   <strong>æ´åé¢å¤åéªç¥è¯æºï¼</strong> è¿ä¸æ­¥æ´åé¢å¤çåéªç¥è¯æºï¼ä»¥å¢å¼ºæ¨çè½ååæ¯æå¯è§£éæ§ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥ä¸ä¸ªæ°é¢çå¼æå¾æ¨¡åï¼ææå°è§£å³äºå¢éå¼3DSSGé¢æµä¸­æ´åå¤æ¨¡æä¿¡æ¯ååéªè§æµçææãå¶è´¡ç®å¨äºæä¾äºä¸ä¸ªçµæ´»ãå¯æ©å±çæ¡æ¶ï¼è½å¤å¤çå¤æãå¨æççå®ä¸çç¯å¢ï¼å¹¶ä¸ºæºå¨äººåå·èº«AIç³»ç»æä¾äºæ´åç¡®çç¯å¢çè§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process.</li>
<li>We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11895v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11895v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11884v1'></a></p>
<h2 id="sam-ttt-segment-anything-model-via-reverse-parameter-configuration-and-test-time-training-for-camouflaged-object-detection"><a href="https://arxiv.org/abs/2509.11884v1">SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection</a></h2>
<p><strong>Authors:</strong> Zhenni Yu, Li Zhao, Guobao Xiao, Xiaoqin Zhang</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT. While most
existing SAM-based COD models primarily focus on enhancing SAM by extracting
favorable features and amplifying its advantageous parameters, a crucial gap is
identified: insufficient attention to adverse parameters that impair SAM's
semantic understanding in downstream tasks. To tackle this issue, the Reverse
SAM Parameter Configuration Module is proposed to effectively mitigate the
influence of adverse parameters in a train-free manner by configuring SAM's
parameters. Building on this foundation, the T-Visioner Module is unveiled to
strengthen advantageous parameters by integrating Test-Time Training layers,
originally developed for language tasks, into vision tasks. Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state. By integrating two modules,
SAM-TTT simultaneously suppresses adverse parameters while reinforcing
advantageous ones, significantly improving SAM's semantic understanding in COD
task. Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field. The code will be available at
https://github.com/guobaoxiao/SAM-TTT.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhenni Yu, Li Zhao, Guobao Xiao, Xiaoqin Zhangæ°åçè®ºæâSAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detectionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="sam-ttt">SAM-TTT: éè¿éååæ°éç½®åæµè¯æ¶è®­ç»å®ç°ä¼ªè£ç®æ æ£æµçéç¨åå²æ¨¡å</h3>
<p><strong>æè¦ï¼</strong></p>
<p>è¿ç¯è®ºæä»ç»äºä¸ç§åä¸ºSAM-TTTçæ°åéç¨åå²æ¨¡åï¼Segment Anything Model, SAMï¼ï¼å®éè¿ç»å<strong>éååæ°éç½®ï¼Reverse Parameter Configurationï¼</strong>å<strong>æµè¯æ¶è®­ç»ï¼Test-Time Training, TTTï¼</strong>æ¥æ¾èæåå¶å¨ä¼ªè£ç®æ æ£æµï¼Camouflaged Object Detection, CODï¼ä»»å¡ä¸­çæ§è½ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçåºäºSAMçCODæ¨¡åä¸»è¦å³æ³¨éè¿æåæå©ç¹å¾åæ¾å¤§ä¼å¿åæ°æ¥å¢å¼ºSAMï¼ä½ä½èæåºä¸ä¸ªå³é®çä¸è¶³ï¼<strong>å¯¹æå®³SAMå¨ä¸æ¸¸ä»»å¡ä¸­è¯­ä¹çè§£çä¸å©åæ°å³æ³¨ä¸è¶³</strong>ãSAMå¨CODä»»å¡ä¸­è¡¨ç°åºçè¯­ä¹ç¼ºé·ï¼ä¾å¦åå²æ©ç ä¸é¢æè¯­ä¹ä¸ç¬¦ï¼ä»¥åå¯¹ç®æ åé¨çåå²ä¸å®æ´ï¼æ¯ç±äºSAMçé¶æ ·æ¬è½åæºäºSA-1Bæ°æ®éä¸CODæ°æ®éä¹é´çé¢åå·®è·ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
SAM-TTTçæ ¸å¿åæ°å¨äºå¶åæ¨¡åè®¾è®¡ï¼æ¨å¨åæ¶æå¶ä¸å©åæ°å¹¶å¼ºåæå©åæ°ï¼</p>
<ul>
<li>
<p><strong>éåSAMåæ°éç½®æ¨¡åï¼Reverse SAM Parameter Configuration Module, R-SAMPCï¼ï¼</strong></p>
<ul>
<li><strong>ç®çï¼</strong> ææåè½»ä¸å©åæ°çå½±åï¼ä»¥æ è®­ç»ï¼train-freeï¼æ¹å¼éç½®SAMçåæ°ã</li>
<li><strong>æºå¶ï¼</strong> R-SAMPCè¢«è®¾è®¡ä¸ºä¸ä¸ªä¸æ´æ°åæ°çå·ç§¯æ¨¡åï¼ç±»ä¼¼äºä¸ç§åæ°å±é¢çéæºæ©ç ædropoutãå®éè¿å¼å¥åªå£°ç´æ¥éä½SAMä¸­ä¸å©äºCODä»»å¡çåæ°å½±åï¼ä»èç¼è§£è¯­ä¹ç¼ºé·ãè¿ä¸ä¼ ç»æ¹æ³éè¿å¼å¥é¢å¤ç¼ç æ¨¡åæ¥è¡¥å¿è¯­ä¹ä¿¡æ¯ä¸åï¼R-SAMPCç´æ¥ä½ç¨äºåæ°ï¼ä¸å¨æ¨çæ¶ä¸åä¸ã</li>
<li><strong>âéåâå«ä¹ï¼</strong> ä¼ ç»æ¹æ³å¼ºè°å¢å¼ºåæ°ï¼èR-SAMPCåä¾§éäºåå¼±åæ°ï¼æåâéåâã</li>
</ul>
</li>
<li>
<p><strong>T-Visioneræ¨¡åï¼TVMï¼ï¼</strong></p>
<ul>
<li><strong>ç®çï¼</strong> å¼ºåæå©åæ°ï¼ä»¥è¡¥å¿R-SAMPCå¨åå¼±ä¸å©åæ°æ¶å¯è½å¯¹æå©åæ°é æçå¹²æ°ã</li>
<li><strong>æºå¶ï¼</strong> TVMå°æµè¯æ¶è®­ç»ï¼TTTï¼å±ï¼ç¹å«æ¯TTT-Linearï¼ä¸ç§å·æçº¿æ§å¤æåº¦åé«åº¦è¡¨è¾¾æ§éèç¶æçRNNå±ï¼é¦æ¬¡å¼å¥è§è§ä»»å¡ãå®éè¿DWTï¼ç¦»æ£å°æ³¢åæ¢ï¼æåå¾ååµå¥çé«é¢åéï¼å¹¶è°æ´ç¹å¾ç»´åº¦ä»¥éåºRNNå±çè¦æ±ï¼ä»èæåå¹¶å¼ºè°æå©ç¹å¾ã</li>
<li><strong>ç»æï¼</strong> SAM-TTTéç¨<strong>åå¹¶è¡åèå</strong>çç»æï¼R-SAMPCåTVMå¨å¹¶è¡é¶æ®µç¬ç«è¿è¡ï¼é¿åç¸äºå¹²æ°ï¼ç¶åå¨èåé¶æ®µï¼éè¿COMPrompterçæ··åæç¤ºæ¹æ³ï¼ç»åä¸¤èçåè½ï¼ä»¥å®ç°å¤å°ºåº¦ä¸ä¸æä¿¡æ¯çæææè·åç²¾ç¡®èåã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½åè¶ï¼</strong> SAM-TTTå¨å¤ä¸ªCODåºåæ°æ®éï¼CAMO, COD10K, NC4Kï¼ä¸åå¾äºæåè¿çæ§è½ï¼è¶è¶äºç°æSOTAæ¹æ³ï¼ä¸ºè¯¥é¢åæ ç«äºæ°åºåã
*   <strong>è¯­ä¹çè§£æåï¼</strong> éè¿åæ¶æå¶ä¸å©åæ°åå¼ºåæå©åæ°ï¼SAM-TTTæ¾èæ¹åäºSAMå¨CODä»»å¡ä¸­çè¯­ä¹çè§£è½åï¼è½å¤çææ´è¯¦ç»ãæ´åç¡®çåå²æ©ç ï¼å°¤å¶å¨å¤çå¾®å°ãè¢«é®æ¡æä¸èæ¯ç¸ä¼¼çä¼ªè£ç®æ æ¶è¡¨ç°åºè²ã
*   <strong>æçï¼</strong> å°½ç®¡SAM-TTTçæ»åæ°éè¾å¤§ï¼96.32Mï¼ï¼ä½å¶å¯è®­ç»åæ°ä»ä¸º6.65Mï¼çº¦ä¸ºFSELçååä¹ä¸ï¼ï¼ä¿æäºè¾ä½çè®¡ç®å¼éï¼åæ¶å®ç°äºä¼äºå¨çç£æ¹æ³çæ§è½ã
*   <strong>æ¨¡åæææ§ï¼</strong> æ¶èå®éªè¯æäºR-SAMPCåTVMçæææ§ãR-SAMPCæ¾èæåäºæ¨¡åæ§è½ï¼å°¤å¶å¨æ­£åææ ä¸ãTVMè¿ä¸æ­¥æåäºæ§è½ï¼å¹¶è¢«è¯å®è½ææè¡¥å¿R-SAMPCå¯¹æå©åæ°çåå¼±ä½ç¨ï¼ä¸TTTç¸æ¯Mambaå¨èç¦æå©ç¹å¾æ¹é¢è¡¨ç°æ´å¼ºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   å¨CAMOæ°æ®éä¸ï¼SAM-TTTçè¡¨ç°ç¥éäºå¶ä»æ¹æ³ï¼è¿å¯è½ä¸SAMç¥è¯å¢å¼ºçæ³åè½ååR-SAMPCæ°å¨ä»¥çºç²å­¦ä¹ è½åä¸ºä»£ä»·æå³ï¼ä»¥åè¯¥æ°æ®éè¾é«çè®­ç»-æµè¯æ¯ï¼1000:250ï¼å¯è½è¿ä¸æ­¥æ¾å¤§äºè¿ç§å½±åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   è¿ä¸æ­¥æ¢ç´¢å¦ä½æ¹è¿åå¼±ä¸å©åæ°åå¼ºè°æå©åæ°çç»åæ¹å¼ã
*   å°è¿ç§âéååæ°éç½®âåâæµè¯æ¶è®­ç»âçæ¦å¿µæ©å±å°å¶ä»å¤§åæ¨¡åå¨ä¸æ¸¸ä»»å¡ä¸­çåºç¨ã</p>
<hr />
<p>æ»èè¨ä¹ï¼SAM-TTTéè¿å¶ç¬ç¹çR-SAMPCåTVMåæ¨¡åè®¾è®¡ï¼å¼åæ§å°è§£å³äºSAMå¨ä¼ªè£ç®æ æ£æµä¸­è¯­ä¹çè§£ä¸è¶³çé®é¢ï¼éè¿æå®½æå©åæ°åä¸å©åæ°ä¹é´çâæåºè·ç¦»âï¼å®ç°äºæ§è½ä¸ççªç ´ï¼å¹¶ä¸ºå°æµè¯æ¶è®­ç»å¼å¥è®¡ç®æºè§è§é¢åå¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT.</li>
<li>Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state.</li>
<li>Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11884v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11884v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11866v1'></a></p>
<h2 id="drv-a-hierarchical-perception-temporal-cognition-framework-to-diagnose-video-hallucination-by-fine-grained-spatial-temporal-grounding"><a href="https://arxiv.org/abs/2509.11866v1">Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</a></h2>
<p><strong>Authors:</strong> Meng Luo, Shengqiong Wu, Liqiang Jing, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee, Wynne Hsu</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in large video models (LVMs) have significantly enhance
video understanding. However, these models continue to suffer from
hallucinations, producing content that conflicts with input videos. To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding. Dr.V comprises of two key components: a benchmark
dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes
10k instances drawn from 4,974 videos spanning diverse tasks, each enriched
with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in
LVMs by systematically applying fine-grained spatial-temporal grounding at the
perceptive and temporal levels, followed by cognitive level reasoning. This
step-by-step pipeline mirrors human-like video comprehension and effectively
identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is
effective in diagnosing hallucination while enhancing interpretability and
reliability, offering a practical blueprint for robust video understanding in
real-world scenarios. All our data and code are available at
https://github.com/Eurekaleo/Dr.V.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâDr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Groundingâçç»¼åæè¦ï¼åå®¹æ¶µçäºæ¨è¦æ±çææè¦ç¹ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Dr.Vï¼ä¸ç§ç¨äºéè¿ç»ç²åº¦æ¶ç©ºæ¥å°è¯æ­è§é¢å¹»è§çå±æ¬¡æç¥-æ¶é´-è®¤ç¥æ¡æ¶</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤§åè§é¢æ¨¡åï¼LVMsï¼å¨è§é¢çè§£ä¸­æ®éå­å¨çâå¹»è§âé®é¢ãLVMså¨çæåå®¹æ¶ï¼ç»å¸¸ä¼äº§çä¸è¾å¥è§é¢ä¸ç¬¦æäºå®éè¯¯çä¿¡æ¯ï¼è¿ä¸¥éæå®³äºå¶å¯é æ§åå¯ä¿¡åº¦ãç°æçå¹»è§è¯ä¼°æ¹æ³å­å¨ç¢çåãä¸å®æ´çåç±»ä½ç³»ä»¥åç¼ºä¹ç»ç²åº¦æ æ³¨ååæçé®é¢ï¼æ æ³è¿è¡å¨é¢çè¯æ­åæ ¹æ¬åå åæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäºDr.Væ¡æ¶ï¼åå«ä¸¤ä¸ªæ ¸å¿ç»ä»¶ï¼
*   <strong>Dr.V-Benchåºåæ°æ®éï¼</strong> è¿æ¯ä¸ä¸ªæ°é¢ä¸å¨é¢çè§é¢å¹»è§è¯ä¼°åºåãå®å¼å¥äºä¸ä¸ªåå±çå¹»è§åç±»æ³ï¼æ¶µçæç¥ãæ¶é´ãè®¤ç¥ä¸ä¸ªå±æ¬¡å±14ç§ç»ç²åº¦å¹»è§ç±»åãæ°æ®éåå«10,000ä¸ªå®ä¾ï¼æ¥èª4,974ä¸ªè§é¢ï¼æ¶µçå¤ç§ä»»å¡ï¼å¹¶å¯å«è¯¦ç»çç»ç²åº¦æ¶ç©ºæ æ³¨ï¼æ¯æç²¾ç¡®çè¯æ­åæã
*   <strong>Dr.V-Agentå«æè§é¢ä»£çï¼</strong> è¿æ¯ä¸ä¸ªæ°é¢çè¯æ­æ¨¡åï¼å®æ¨¡ä»¿äººç±»çè§é¢çè§£æºå¶ï¼éç¨âä»æç¥å°æ¶é´åå°è®¤ç¥âçé¾å¼åå±æ¨çè¿ç¨æ¥è¯æ­LVMsä¸­çå¹»è§ãDr.V-Agentç³»ç»æ§å°å©ç¨ç»ç²åº¦æ¶ç©ºæ¥å°ï¼éè¿è°ç¨åè¿çå¤é¨å·¥å·å¦Grounded SAM2åYOLO-Worldè¿è¡å¯¹è±¡è¯å«åè·è¸ªï¼ä»¥åCG-STVGåGrounded-VideoLLMè¿è¡æ¶é´æ¥å°ï¼æ¥éªè¯æç¥åæ¶é´å±é¢çä¿¡æ¯ï¼éåè¿è¡è®¤ç¥å±é¢çæ¨çãè¿ç§éæ­¥çè¯æ­æµç¨è½å¤çæç»æåçè¯æ­åé¦ï¼æå¯¼LVMsçº æ­£å¶ååºã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å¹»è§æ®éå­å¨ï¼</strong> å®éªè¡¨æï¼æææµè¯çLVMsé½å­å¨æ¾èçå¹»è§é®é¢ï¼å³ä½¿æ¯é¡¶çº§çé­æºæ¨¡åä¹æªè½å¹¸åï¼è¿å¸æ¾äºè§é¢å¹»è§æ¯ä¸ä¸ªä¸¥éä¸å°æªè§£å³çé®é¢ã
*   <strong>æ§è½åå±ï¼</strong> LVMså¨æç¥ä»»å¡ä¸è¡¨ç°æä½³ï¼ä½å¨æ¶é´ä»»å¡åè®¤ç¥ä»»å¡ä¸çåç¡®æ§æ¾èä¸éï¼è¡¨æå½åLVMså¨å¤çå¤æè§é¢åææéçä¸¥æ ¼æ¶ç©ºçè§£åé«çº§æ¨çæ¹é¢å­å¨ä¸è¶³ã
*   <strong>Dr.V-Agentçæææ§ï¼</strong> å¹¿æ³çå®éªè¯æï¼Dr.V-Agentå¨è¯æ­å¹»è§æ¹é¢éå¸¸ææï¼å¹¶æ¾èæé«äºLVMsçè§£éæ§åå¯é æ§ãä¸åºçº¿èªçº æ­£ç­ç¥ï¼Self-PEPï¼ç¸æ¯ï¼Dr.V-Agentå¨æææµè¯æ¨¡ååå¹»è§ç±»åä¸é½åå¾äºæ¾èä¸ç¨³å¥çæ§è½æåï¼å°¤å¶æ¯å¨ä½æ§è½LVMsä¸ã
*   <strong>è¯æ­è½åï¼</strong> Dr.V-Agentéè¿å°æ¨¡åçæ¨çä¸ç²¾ç¡®çãå¤é¨éªè¯çæ¶ç©ºä¿¡æ¯ç¸ç»åï¼åæäºèªçº æ­£çå±éæ§ï¼ä»èå¨ç¹å®å¹»è§ç±»åï¼å¦å¯¹è±¡ãéæå³ç³»ãOCRåå¨æå³ç³»ï¼ä¸å®ç°äºå®è´¨æ§æ¹è¿ã
*   <strong>æçåå¯æ©å±æ§ï¼</strong> Dr.V-Agentéç¨åè®­ç»èå¼ï¼éè¿æºè½å°ç»åç°æä¸å®¶å·¥å·æ¥è¿è¡ï¼é¿åäºæè´µçé¢è®­ç»åå¾®è°ï¼å·æåºæççµæ´»æ§åé¢åæªæ¥çå¯æ©å±æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¯¹å¤é¨å·¥å·æ§è½çä¾èµï¼</strong> Dr.V-Agentçæææ§ä¸¥éä¾èµäºå¶è°ç¨çå¤é¨å·¥å·ï¼å¦å¯¹è±¡æ£æµãæ¶ç©ºæ¥å°å·¥å·ï¼çæ§è½ãè¿äºå·¥å·åºæçå±éæ§å¯è½ä¼å½±åæç»è¯æ­åå¹»è§ç¼è§£çåç¡®æ§ã
*   <strong>ç³»ç»å¤ææ§åè®¡ç®å¼éï¼</strong> ä¸ºäºå®ç°ç»ç²åº¦æ¨çï¼å¤ä»£çæ¹æ³è°ç¨äºå¤è¾¾å«ä¸ªå¤é¨æ¨¡åï¼è¿å¢å äºæ¾èçç³»ç»å¤ææ§åè®¡ç®å¼éãä¸ç«¯å°ç«¯æ¨¡åç¸æ¯ï¼è¿ç§å¤æ­¥éª¤ãé¡ºåºå¤çè¿ç¨å¯è½å¯¼è´æ´é«çå»¶è¿ã
*   <strong>åºåæ æ³¨ææ¬åå¯æ©å±æ§ï¼</strong> Dr.V-Benchçæå»ºä¾èµäºç»ç²åº¦çäººå·¥æ¶ç©ºæ æ³¨ãè½ç¶è¿ç§ç»èæ°´å¹³å¯¹äºåç¡®è¯æ­å¹»è§è³å³éè¦ï¼ä½é«æçæ æ³¨ææ¬åæ¶é´æå¥éå¶äºåºåçå¯æ©å±æ§ï¼ä½¿å¶é¾ä»¥å¿«éæ©å±å°æ´å¤§è§æ¨¡ææ´å¤æ ·åçè§é¢é¢åã
*   <strong>çæè½åé´æ¥è¯ä¼°ï¼</strong> Dr.V-Benchå¯¹çæè½åçè¯ä¼°è¢«æ¹ç¼ä¸ºåºäºQAçæ¡æ¶ï¼ç¹å«æ¯âå­å¹çæQAâï¼ãè¿ç§è®¾è®¡ç®åäºå¹»è§çéå¯¹æ§è¯ä¼°ï¼ä½çºç²äºå¯¹æ¨¡åèªç±å½¢å¼çæè½åï¼å¦æµçæ§ãè¿è´¯æ§ååé åï¼çç´æ¥åå¨é¢è¯ä¼°ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æåå¤é¨å·¥å·çé²æ£æ§ï¼</strong> é´äºå¯¹å¤é¨å·¥å·æ§è½çä¾èµï¼æªæ¥çç ç©¶å¯ä»¥ä¸æ³¨äºå¼åæ´é²æ£ãæ´åç¡®çæç¥åæ¶é´æ¥å°å·¥å·ï¼ä»¥è¿ä¸æ­¥æé«Dr.V-Agentçæ´ä½æ§è½ã
*   <strong>ä¼åç³»ç»æçï¼</strong> æ¢ç´¢åå°ç³»ç»å¤ææ§åè®¡ç®å¼éçæ¹æ³ï¼ä¾å¦éè¿æ´é«æçå·¥å·éæãå¹¶è¡å¤çææ¨¡åè¸é¦ï¼ä»¥éä½å»¶è¿å¹¶æé«å®éåºç¨æ§ã
*   <strong>æ©å±åºåè§æ¨¡åå¤æ ·æ§ï¼</strong> å°½ç®¡Dr.V-Benchå·²ç»å¾å¨é¢ï¼ä½æªæ¥çå·¥ä½å¯ä»¥æå¥èµæºï¼éè¿åèªå¨åæ æ³¨æä¼åç­æ¹å¼ï¼è¿ä¸æ­¥æ©å¤§æ°æ®éçè§æ¨¡ï¼æ¶µçæ´å¤è§é¢é¢åååºæ¯ï¼ä»¥åºå¯¹æ´å¹¿æ³çå¹»è§ç±»åã
*   <strong>ç´æ¥è¯ä¼°çæè½åï¼</strong> å¼åæ´ç´æ¥ãæ´å¨é¢çæ¹æ³æ¥è¯ä¼°LVMsçèªç±å½¢å¼çæè½åï¼è¶è¶åºäºQAçæ¡æ¶ï¼ä»¥æ´å¥½å°æææ¨¡åå¨æµçæ§ãè¿è´¯æ§ååé åæ¹é¢çè¡¨ç°ã
*   <strong>æ´åå¤é¨ä¸çç¥è¯ï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¦ä½æ´ææå°æ´åå¤é¨ä¸çç¥è¯åé¢åç¹å®ä¸ä¸ç¥è¯ï¼ä»¥å¢å¼ºLVMså¨è®¤ç¥æ¨çä»»å¡ï¼å¦åäºå®é¢æµåç¥è¯é©±å¨è§£éï¼ä¸­çè¡¨ç°ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11866v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11866v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11815v1'></a></p>
<h2 id="specvlm-fast-speculative-decoding-in-vision-language-models"><a href="https://arxiv.org/abs/2509.11815v1">SpecVLM: Fast Speculative Decoding in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Haiduo Huangç­äººæ°åçè®ºæâSpecVLM: Fast Speculative Decoding in Vision-Language Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="specvlm-fast-speculative-decoding-in-vision-language-models_1">è®ºææè¦ï¼SpecVLM: Fast Speculative Decoding in Vision-Language Models</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è§è§-è¯­è¨æ¨¡åï¼VLMsï¼ä¸­èªåå½è§£ç çæçç¶é¢ãå°½ç®¡æ¨æµè§£ç ï¼speculative decodingï¼å·²æåå éå¤§åè¯­è¨æ¨¡åï¼LLMsï¼ï¼ä½å°å¶ç´æ¥åºç¨äºVLMsé¢ä¸´ç¬ç¹ææãç¹å«æ¯ï¼é¢å¡«åï¼prefillï¼é¶æ®µç±è§è§tokenä¸»å¯¼ï¼å¶æ°ééå¾ååè¾¨çåè§é¢é¿åº¦æ¥å§å¢å ï¼å¯¼è´è®¡ç®ååå­ï¼å°¤å¶æ¯é®å¼ï¼KVï¼ç¼å­ï¼å¼éå·¨å¤§ãè¿ä¸¥éå½±åäºVLMsçååéåå»¶è¿ï¼é»ç¢äºå®æ¶åå¤§è§æ¨¡é¨ç½²ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
SpecVLMéè¿ä»¥ä¸ä¸¤é¡¹æ ¸å¿åæ°æ¥è§£å³ä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>å¼¹æ§è§è§åç¼©å¨ï¼Elastic Visual Compressorï¼ï¼</strong> SpecVLMå¼å¥äºä¸ç§èªéåºçè§è§åç¼©å¨ï¼è½å¤æ ¹æ®è¾å¥å¨æéæ©åªæï¼pruningï¼ãæ± åï¼poolingï¼ãå·ç§¯ï¼convolutionï¼åééæ ·ï¼resamplerï¼ç­åºæ¬æä½ï¼ä»¥å¹³è¡¡FLOPs/åæ°åæ¯ä¸ªè¾å¥çåç¡®æ§ãè¿æå©äºå¨é¢å¡«åé¶æ®µææåå°è§è§tokenæ°éï¼ä»èç¼è§£è®¡ç®åKVç¼å­çååã</li>
<li><strong>å¨çº¿Logitè¸é¦åè®®ï¼Online-Logit Distillation Protocolï¼ï¼</strong> ä¸ºäºé¿åæè´µçç¦»çº¿è¸é¦è¯­æåºï¼SpecVLMæåºäºä¸ç§å¨çº¿Logitè¸é¦åè®®ãè¯¥åè®®ä½¿ç¨å®æ¶çæå¸æ¨¡åLogitååæ°ç¬¬äºå±ç¹å¾æ¥è®­ç»èç¨¿æ¨¡åï¼draft modelï¼ï¼ç»åäºäº¤åçµåSmooth L1ç®æ å½æ°ãè¿ç§æ¹æ³æ¶é¤äºå­å¨åé¢å¤ççéè¦ï¼åæ¶ä¿æäºè®¡ç®æçï¼å¹¶æ­ç¤ºäºè®­ç»æ¶é´æ©å±æåºã</li>
</ul>
<p>æ­¤å¤ï¼SpecVLMé¦åå»ºç«äºä¸ä¸ªå¼ºå¤§çEAGLE-2é£æ ¼åºçº¿æ¨¡åEagleVLMï¼è¯¥æ¨¡åæ¬èº«å°±æ¯å®å¨èªåå½æ¨çæä¾äºæ¾èçå éãSpecVLMå¨æ­¤åºç¡ä¸è¿ä¸æ­¥æåäºæ§è½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçç«¯å°ç«¯å éï¼</strong> EagleVLMå¨å®å¨èªåå½æ¨ççåºç¡ä¸å®ç°äº1.5-2.3åçç«¯å°ç«¯å éãå¨æ­¤åºç¡ä¸ï¼SpecVLMéè¿å¶åæ°è¿ä¸æ­¥å®ç°äºé¢å¤çå éï¼å¨LLaVAåMMMUåºåæµè¯ä¸ï¼å¨5ä¸ªepochåè¾¾å°äº2.5-2.9åçç«¯å°ç«¯å éã
*   <strong>æ æè§£ç ï¼</strong> SpecVLMå¨å éçåæ¶ï¼ä¿æäºç®æ æ¨¡åçè¾åºåå¸ï¼æ æè§£ç ï¼ï¼ç¡®ä¿äºè¾åºè´¨éã
*   <strong>è®­ç»æ¶é´æ©å±æåºï¼</strong> å¨çº¿Logitè¸é¦åè®®æ­ç¤ºäºä¸ä¸ªéè¦çè®­ç»æ¶é´æ©å±æåºï¼æ´é¿çå¨çº¿è®­ç»åè°å°å¢å äºèç¨¿æ¨¡åçå¹³åæ¥åé¿åº¦ï¼ä»èæé«äºæ¨æµè§£ç çæçãè¿è¡¨æï¼å¯¹äºå¤æ¨¡ææ¨æµè§£ç ï¼å»¶é¿æéå¯¹æ§çè®­ç»æ¶é´å¯ä»¥å¸¦æ¥æ¾èçæçæåã
*   <strong>è·¨åè¾¨çåä»»å¡é¾åº¦çä¸è´æ§ï¼</strong> SpecVLMå¨ä¸åå¾ååè¾¨çåä»»å¡é¾åº¦ä¸åè¡¨ç°åºä¸è´çæ§è½æåï¼éªè¯äºå¶æ¹æ³çé²æ£æ§ã</p>
<p>è¿äºç»æè¡¨æSpecVLMä¸ºVLMsçæ¨çå éæä¾äºä¸ä¸ªå®ç¨ä¸å¯æ©å±çæ¡æ¶ï¼å¨ä¿æè¾åºè´¨éçåæ¶æ¾èæåäºæçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è§è§åç¼©å¨çåç¼©æ¯ä¾æå¨éç½®ï¼</strong> å½åçè§è§åç¼©å¨çåç¼©æ¯ä¾ä»éæå¨éç½®ï¼æªæ¥å¯ä»¥æ¢ç´¢å®ä¾çº§çèªéåºåç¼©æ¯ä¾è°æ´æºå¶ã
*   <strong>KVç¼å­çå¨æåç¼©ï¼</strong> è®ºææå°ï¼å¨æ¨çæ¶æ¢ç´¢èç¨¿æ¨¡åKVç¼å­çå¨æåç¼©å¯ä»¥è¿ä¸æ­¥æé«æçã
*   <strong>å¶ä»æ©å±è½´çæ¢ç´¢ä¸è¶³ï¼</strong> è®ºæä¸»è¦å³æ³¨è®­ç»æ¶é´ï¼ä½æ´å¤§çè®­ç»è¯­æåºåæ´æ·±çèç¨¿æ¨¡åç­å¶ä»æ©å±è½´ä¹å¯è½æé«æ¨æµè§£ç æçã
*   <strong>è¶åæ°è°ä¼ï¼</strong> è®ºææªè¯¦å°½è°ä¼èç¨¿æ¨¡åçè¶åæ°ï¼å¦æ·±åº¦ãæ Top-Kãèç¹æ°éï¼æä¼åè®¾ç½®ï¼å¦å­¦ä¹ çãæ¹æ¬¡å¤§å°ï¼ï¼å æ­¤æ¥åçæ§è½å¯è½ä¸æ¯æä¼çã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å¼åå®ä¾çº§çè§è§åç¼©æ¯ä¾èªéåºè°æ´æºå¶ã
*   æ¢ç´¢æ¨çæ¶èç¨¿æ¨¡åKVç¼å­çå¨æåç¼©ã
*   ç ç©¶æ´å¤§è®­ç»è¯­æåºåæ´æ·±èç¨¿æ¨¡åå¯¹æ¨æµè§£ç æççå½±åï¼ä»¥æ­ç¤ºVLMsä¸­æ¨æµè§£ç çæ´å¹¿æ³æ©å±è§å¾ã
*   å¯¹èç¨¿æ¨¡åè¶åæ°åä¼åè®¾ç½®è¿è¡æ´è¯¦å°½çè°ä¼ï¼ä»¥è¯å«æä½³è®¾ç½®ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11815v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11815v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.11772v1'></a></p>
<h2 id="seg2track-sam2-sam2-based-multi-object-tracking-and-segmentation-for-zero-shot-generalization"><a href="https://arxiv.org/abs/2509.11772v1">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></h2>
<p><strong>Authors:</strong> Diogo MendonÃ§a, Tiago Barros, Cristiano Premebida, Urbano J. Nunes</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to
operate reliably in dynamic environments. MOT ensures consistent object
identity assignment and precise spatial delineation. Recent advances in
foundation models, such as SAM2, have demonstrated strong zero-shot
generalization for video segmentation, but their direct application to MOTS
(MOT+Segmentation) remains limited by insufficient identity management and
memory efficiency. This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement. The
proposed approach requires no fine-tuning and remains detector-agnostic.
Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA). Furthermore, a sliding-window
memory strategy reduces memory usage by up to 75% with negligible performance
degradation, supporting deployment under resource constraints. These results
confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot
tracking, enhanced identity preservation, and efficient memory utilization. The
code is available at https://github.com/hcmr-lab/Seg2Track-SAM2</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Diogo MendonÃ§a, Tiago Barros, Cristiano Premebida, Urbano J. Nunesæ°åçè®ºæâSeg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalizationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="seg2track-sam2-sam2">è®ºææè¦ï¼Seg2Track-SAM2: åºäºSAM2çé¶æ ·æ¬æ³åå¤ç®æ è·è¸ªä¸åå²</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³èªä¸»ç³»ç»å¨å¨æç¯å¢ä¸­å¯¹é²æ£å¤ç®æ è·è¸ªä¸åå²ï¼MOTSï¼çéæ±ãå°½ç®¡åºç¡æ¨¡åå¦SAM2å¨è§é¢åå²æ¹é¢å±ç°åºå¼ºå¤§çé¶æ ·æ¬æ³åè½åï¼ä½å¶ç´æ¥åºç¨äºMOTSæ¶ï¼é¢ä¸´èº«ä»½ç®¡çä¸è¶³ååå­æçä½ä¸çéå¶ãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å°SAM2çé¶æ ·æ¬åå²è½åä¸é«æçèº«ä»½ç®¡çååå­å©ç¨ç¸ç»åï¼ä»¥å®ç°é²æ£çé¶æ ·æ¬MOTSã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
Seg2Track-SAM2æ¡æ¶éè¿ä»¥ä¸å³é®åæ°è§£å³äºä¸è¿°é®é¢ï¼
*   <strong>éæé¢è®­ç»æ£æµå¨ä¸SAM2ï¼</strong> æ¡æ¶é¦åå©ç¨YOLOv11ç­é¢è®­ç»çæ·±åº¦å­¦ä¹ æ£æµå¨çæåå§ç®æ æè®®ï¼bounding boxesï¼ï¼ç¶åå°è¿äºæè®®ä½ä¸ºSAM2çæç¤ºï¼çæé«è´¨éçåå²æ©ç ãè¿ç§æ¹æ³ä½¿å¶è½å¤å¤çä¸åç±»å«çå¯¹è±¡ï¼å¹¶ä¿ææ£æµå¨æ å³æ§ã
*   <strong>æ°åSeg2Trackæ¨¡åï¼</strong> å¼å¥äºä¸ä¸ªä¸é¨çSeg2Trackæ¨¡åï¼ç¨äºå¤çè½¨è¿¹åå§åãè½¨è¿¹ç®¡çåå¼ºåãè¯¥æ¨¡ååå«ï¼
    *   <strong>è½¨è¿¹è´¨éè¯ä¼°ï¼Track Quality Assessment, TQAï¼ï¼</strong> æ ¹æ®SAM2æä¾çIoUç½®ä¿¡åº¦åæ°è¯ä¼°æ©ç è´¨éï¼å°æ©ç åç±»ä¸ºâé«âãâä¸ç¡®å®âæâä½âç¶æï¼ä»¥å³å®è½¨è¿¹çç»´æ¤ãç§»é¤æå¼ºåã
    *   <strong>äºå¼æ©ç çæï¼Binary Mask Generationï¼ï¼</strong> å°åä¸å¸§çåä¸ªå¯¹è±¡æ©ç åå¹¶æä¸ä¸ªç»ä¸çäºå¼æ©ç ï¼ç¨äºå¹éè¿ç¨ï¼ä»¥è¯å«ä¸æ°æè®®çéå ã
    *   <strong>å¯¹è±¡å³èä¸è¿æ»¤ï¼Object Association and Filtering, OAFï¼ï¼</strong> è¿æ¯ä¸ä¸ªä¸¤é¶æ®µè¿ç¨ï¼å©ç¨ä¼ å¥çå¯¹è±¡æè®®åè½¨è¿¹è´¨éè¯ä¼°æ¥å³å®æ¯å¦åå§åæ°è½¨è¿¹æå¼ºåç°æè½¨è¿¹ã
*   <strong>æ»å¨çªå£åå­ç­ç¥ï¼</strong> éå¯¹SAM2åå§å®ç°ä¸­åå­æ éå¶å¢é¿çé®é¢ï¼Seg2Track-SAM2å¼å¥äºæ»å¨çªå£æºå¶ï¼éå¶äºè®¡ç®æ©ç æ¶èèçè¿å»ç¶ææ°éï¼æ¾èéä½äºåå­ä½¿ç¨éï¼åæ¶ä¿æäºè·è¸ªæ§è½ã
*   <strong>é¶æ ·æ¬æ³åè½åï¼</strong> æ´ä¸ªæ¡æ¶è®¾è®¡ä¸ºé¶æ ·æ¬èå¼ï¼æ éå¯¹ç¹å®æ°æ®éè¿è¡å¾®è°ï¼å³å¯å¨ä¸ååºæ¯ä¸å·¥ä½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼SOTAï¼ï¼</strong> å¨KITTI MOTåKITTI MOTSåºåæµè¯ä¸­ï¼Seg2Track-SAM2å¨æ±½è½¦åè¡äººç±»å«ä¸ååå¾äºSOTAæ§è½ï¼å¨KITTI MOTSä¸æ»ä½æåç¬¬åã
*   <strong>å³èåç¡®åº¦ï¼AssAï¼æ°åºåï¼</strong> è¯¥æ¹æ³å¨å³èåç¡®åº¦ï¼AssAï¼ææ ä¸å»ºç«äºæ°åºåï¼æ¾èä¼äºç°ææ¹æ³ï¼è¡¨æå¶å¨èº«ä»½ä¿ææ¹é¢è¡¨ç°åºè²ï¼åå°äºèº«ä»½åæ¢çã
*   <strong>é«æåå­å©ç¨ï¼</strong> æ»å¨çªå£åå­ç­ç¥å°åå­ä½¿ç¨éåå°äºé«è¾¾75%ï¼èæ§è½ä¸éå¯å¿½ç¥ä¸è®¡ï¼è¿å¯¹äºèµæºåéç³»ç»ï¼å¦æºå¨äººåèªå¨é©¾é©¶ï¼çé¨ç½²è³å³éè¦ã
*   <strong>é²æ£æ§ï¼</strong> ç»æè¯å®äºSeg2Track-SAM2éè¿ç»åé²æ£çé¶æ ·æ¬è·è¸ªãå¢å¼ºçèº«ä»½ä¿æåé«æçåå­å©ç¨ï¼æ¾èæ¨å¨äºMOTSé¢åçåå±ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ£æµåç¡®åº¦ï¼DetAï¼çéå¶ï¼</strong> ç¸è¾äºå¶ä»SOTAæ¹æ³ï¼Seg2Track-SAM2çæ£æµåç¡®åº¦ï¼DetAï¼è¾ä½ãè¿ä¸»è¦æ¯ç±äºèåæ­£ä¾çä¼ æ­ï¼SAM2éè¯¯å°å°èåæ£æµåå§åä¸ºæ°å¯¹è±¡ï¼å¹¶å¨å¤å¸§ä¸­ä¿æï¼ä»¥åä¾èµéç¨æ£æµå¨ï¼YOLOv11ï¼èééå¯¹KITTIæ°æ®éä¸é¨è®­ç»çæ£æµå¨ã
*   <strong>2D MOTåºåæµè¯ä¸­çæ§è½ä¸éï¼</strong> å¨KITTI 2D MOTåºåæµè¯ä¸­ï¼ç±äºå°åå²æ©ç è½¬æ¢ä¸º2Dè¾¹çæ¡ï¼å½å¯¹è±¡é¨åæå®å¨è¢«é®æ¡æ¶ï¼æ§è½ä¼ä¸éãç¹å«æ¯å¯¹äºç©ºé´èå´è¾å¤§çæ±½è½¦ï¼è¾¹çæ¡è½¬æ¢è¿ç¨ä¸­æ´å®¹æåºç°å¤±çï¼å¯¼è´DetAæ¾èä¸éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¹è¿è¾¹çæ¡è½¬æ¢ç­ç¥ï¼</strong> éå¯¹2D MOTä»»å¡ä¸­è¾¹çæ¡è½¬æ¢çå±éæ§ï¼å¯ä»¥æ¢ç´¢æ´å¤æçè½¬æ¢ç­ç¥ï¼ä¾å¦éæ¨¡æè¾¹çæ¡ä¼°è®¡æè·¨å¸§æ¶é´å¹³æ»ï¼ä»¥æ´å¥½å°è¿ä¼¼è¢«é®æ¡å¯¹è±¡çå®æ´ç©ºé´èå´ï¼å¹¶åå°å¤±çãä½è¿éè¦æè¡¡é¶æ ·æ¬è®¾è®¡çå¿µä¸é¢å¤æ¨¡ååè®¾æè®­ç»æ°æ®ã
*   <strong>ä¼åèåæ­£ä¾æå¶ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æææå¶SAM2å¯è½äº§ççèåæ­£ä¾ï¼ä»¥æé«æ£æµåç¡®åº¦ï¼DetAï¼ã
*   <strong>éææ´åè¿çæ£æµå¨ï¼</strong> å°½ç®¡è¯¥æ¡æ¶æ¯æ£æµå¨æ å³çï¼ä½éæéå¯¹ç¹å®æ°æ®éè¿è¡å¾®è°çæ´åè¿æ£æµå¨å¯è½ä¼è¿ä¸æ­¥æé«æ´ä½æ§è½ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement.</li>
<li>Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.11772v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.11772v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-16 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
