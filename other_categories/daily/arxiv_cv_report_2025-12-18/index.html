<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-18 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-17/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-19/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-18">Arxiv Computer Vision Papers - 2025-12-18</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#mmgr-multi-modal-generative-reasoning" class="nav-link">MMGR: Multi-Modal Generative Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#spatia-video-generation-with-updatable-spatial-memory" class="nav-link">Spatia: Video Generation with Updatable Spatial Memory</a>
                </li>
                <li class="nav-item">
                    <a href="#in-pursuit-of-pixel-supervision-for-visual-pre-training" class="nav-link">In Pursuit of Pixel Supervision for Visual Pre-training</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusionvl-translating-any-autoregressive-models-into-diffusion-vision-language-models" class="nav-link">DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#multi-view-foundation-models" class="nav-link">Multi-View Foundation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#gatefusion-hierarchical-gated-cross-modal-fusion-for-active-speaker-detection" class="nav-link">GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#end-to-end-training-for-autoregressive-video-diffusion-via-self-resampling" class="nav-link">End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</a>
                </li>
                <li class="nav-item">
                    <a href="#vlic-vision-language-models-as-perceptual-judges-for-human-aligned-image-compression" class="nav-link">VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</a>
                </li>
                <li class="nav-item">
                    <a href="#skyra-ai-generated-video-detection-via-grounded-artifact-reasoning" class="nav-link">Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#mimic-video-video-action-models-for-generalizable-robot-control-beyond-vlas" class="nav-link">mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-18">Arxiv Computer Vision Papers - 2025-12-18</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域近期论文的执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月16日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>多模态理解与生成</strong>、<strong>视频内容生成与分析</strong>以及<strong>视觉预训练新范式</strong>方面的显著进展。特别值得注意的是，研究人员正积极探索如何将<strong>生成模型（尤其是扩散模型）</strong>与<strong>视觉语言任务</strong>深度融合，并致力于提升模型的<strong>泛化能力</strong>和<strong>可控性</strong>。同时，<strong>利用视觉语言模型进行评估和推理</strong>也成为一个新兴的研究方向。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>MMGR: Multi-Modal Generative Reasoning</strong> 提出了一种多模态生成推理框架，预示着模型在整合不同模态信息进行复杂推理方面迈出了重要一步。</li>
<li><strong>Spatia: Video Generation with Updatable Spatial Memory</strong> 在视频生成领域引入了“可更新空间记忆”的概念，有望显著提升视频生成的一致性和连贯性。</li>
<li><strong>DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</strong> 提供了一种通用的方法，将现有的自回归模型转化为扩散视觉语言模型，极大地扩展了扩散模型在视觉语言任务中的应用范围。</li>
<li><strong>VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</strong> 巧妙地利用视觉语言模型作为“感知裁判”，为图像压缩任务引入了更符合人类视觉偏好的评估标准，具有重要的实际应用价值。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>扩散模型在视觉语言任务中的广泛应用：</strong> 从文本到视频生成，再到模型转换，扩散模型正成为构建强大视觉语言模型的核心技术。</li>
<li><strong>视频生成的可控性与记忆机制：</strong> 通过引入空间记忆等机制，研究人员正努力使视频生成更加可控，并能更好地处理长时序依赖。</li>
<li><strong>视觉预训练的像素级监督探索：</strong> 论文“In Pursuit of Pixel Supervision for Visual Pre-training”表明，对像素级信息的深入挖掘是提升视觉预训练模型能力的关键。</li>
<li><strong>多模态基础模型（Multi-View Foundation Models）：</strong> 预示着未来将出现更强大的、能够处理多视角信息的通用基础模型。</li>
<li><strong>AI生成内容的检测与溯源：</strong> “Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning”关注AI生成内容的检测，是应对内容真实性挑战的重要研究方向。</li>
<li><strong>机器人控制的通用化：</strong> “mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs”展示了将视频理解能力应用于更广泛的机器人控制任务的潜力。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其创新性和潜在影响力，以下论文值得深入阅读：</p>
<ol>
<li><strong>MMGR: Multi-Modal Generative Reasoning</strong> (多模态推理的通用框架)</li>
<li><strong>Spatia: Video Generation with Updatable Spatial Memory</strong> (视频生成在一致性与可控性上的突破)</li>
<li><strong>DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</strong> (通用模型转换方法，扩展扩散模型应用)</li>
<li><strong>VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</strong> (视觉语言模型在评估领域的创新应用)</li>
</ol>
<hr />
<p>希望这份执行摘要能帮助您快速了解近期 Arxiv 计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.14691v2">MMGR: Multi-Modal Generative Reasoning</a></li>
<li><a href="#2512.15716v1">Spatia: Video Generation with Updatable Spatial Memory</a></li>
<li><a href="#2512.15715v1">In Pursuit of Pixel Supervision for Visual Pre-training</a></li>
<li><a href="#2512.15713v1">DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</a></li>
<li><a href="#2512.15708v1">Multi-View Foundation Models</a></li>
<li><a href="#2512.15707v1">GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</a></li>
<li><a href="#2512.15702v1">End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</a></li>
<li><a href="#2512.15701v1">VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</a></li>
<li><a href="#2512.15693v1">Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</a></li>
<li><a href="#2512.15692v1">mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.14691v2'></a></p>
<h2 id="mmgr-multi-modal-generative-reasoning"><a href="https://arxiv.org/abs/2512.14691v2">MMGR: Multi-Modal Generative Reasoning</a></h2>
<p><strong>Authors:</strong> Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Wen Xiao, Jiuxiang Gu, Nanyun Peng, Junjie Hu</p>
<p><strong>Published:</strong> 2025-12-16</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：MMGR: Multi-Modal Generative Reasoning</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本研究提出了MMGR，一个多模态生成推理的评估框架和基准，旨在弥补现有视频生成模型评估指标（如FVD）在衡量物理、逻辑和空间约束方面的不足。MMGR通过评估物理、逻辑、3D空间、2D空间和时间五种推理能力，在抽象推理、具身导航和物理常识三个领域对领先的生成模型进行了全面基准测试，揭示了当前模型在推理能力上的显著差距。该框架为开发更可靠、更具世界模拟能力的生成模型提供了方向。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>多模态生成推理评估框架 (MMGR)：</strong> 这是本研究的核心创新。MMGR不再仅仅关注生成内容的视觉逼真度和时间连贯性，而是引入了对生成内容中蕴含的“推理”能力的系统性评估。</li>
<li><strong>五种核心推理能力：</strong> 论文明确定义了五种关键的推理能力：物理（Physical）、逻辑（Logical）、3D空间（3D Spatial）、2D空间（2D Spatial）和时间（Temporal）。这种细粒度的能力划分有助于更深入地诊断模型在不同推理维度上的表现。</li>
<li><strong>三个多样化的评估领域：</strong> MMGR在三个具有代表性的领域进行评估：<ul>
<li><strong>抽象推理 (Abstract Reasoning):</strong> 例如ARC-AGI和Sudoku，测试模型在符号操作和规则遵循方面的能力。</li>
<li><strong>具身导航 (Embodied Navigation):</strong> 模拟真实世界中的3D导航和定位任务，评估模型在理解和规划空间路径的能力。</li>
<li><strong>物理常识 (Physical Commonsense):</strong> 例如体育场景和组合式交互，测试模型对物理定律和物体间相互作用的理解。</li>
</ul>
</li>
<li><strong>跨模态（视频和图像）的细粒度评估：</strong> MMGR要求生成内容在视频和图像层面都展现出整体的正确性，这使得评估更加全面和严格。</li>
<li><strong>对现有模型的深入诊断：</strong> 通过在MMGR框架下对Sora-2、Veo-3、GPT-4o-image等领先模型进行基准测试，论文揭示了它们在不同推理任务上的具体优劣势，为模型改进提供了明确的指导。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>推动生成模型向“理解”和“推理”发展：</strong> MMGR的出现将促使研究者从单纯追求视觉效果转向更关注生成内容的内在逻辑和物理一致性，从而推动生成模型向更具智能和可靠性的“世界模拟器”迈进。</li>
<li><strong>建立新的评估标准和研究方向：</strong> MMGR提供了一个标准化的、多维度的评估框架，有望成为未来视频和多模态生成模型研究的基石，引导新的研究方向，例如如何设计能够有效学习和执行推理任务的模型架构和训练策略。</li>
<li><strong>加速具身智能和物理模拟的发展：</strong> 对于需要精确物理和空间理解的应用（如机器人、虚拟现实），MMGR的评估方法和发现将直接促进相关领域的发展。</li>
<li><strong>提升生成内容的可靠性和可信度：</strong> 通过强调推理能力，MMGR有助于减少生成内容中的“幻觉”和不合逻辑的错误，提高生成内容的可靠性和在实际应用中的可信度。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>具身AI和机器人学：</strong> 机器人需要在复杂环境中进行导航、规划和与物理世界交互，MMGR的具身导航和物理常识评估对机器人训练至关重要。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 创建逼真且符合物理规律的虚拟环境需要强大的空间和物理推理能力，MMGR的评估方法可以指导VR/AR内容生成。</li>
<li><strong>教育和模拟训练：</strong> 用于生成教学内容、模拟实验或训练场景，需要确保内容的准确性和逻辑性。</li>
<li><strong>内容创作和影视制作：</strong> 生成更具说服力、逻辑严谨的视频内容，减少后期修正的工作量。</li>
<li><strong>科学研究：</strong> 用于模拟物理过程、化学反应或生物现象，需要高度的物理准确性。</li>
<li><strong>自动驾驶：</strong> 理解交通规则、预测其他车辆行为等都需要强大的逻辑和物理推理能力。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>评估的全面性仍有待检验：</strong> 尽管MMGR提出了五种推理能力和三个评估领域，但“世界模拟”是一个极其广泛的概念，可能还有其他未被涵盖的推理维度（例如，社会常识、因果关系中的更深层次理解等）。</li>
<li><strong>评估的自动化程度：</strong> 摘要提到“需要整体正确性”，这可能意味着部分评估需要人工干预或复杂的后处理，其自动化程度和可扩展性可能是一个挑战。</li>
<li><strong>基准测试的覆盖范围：</strong> 虽然提到了“领先模型”，但模型的选择可能存在一定的局限性，未来需要更广泛的模型覆盖来验证MMGR的普适性。</li>
<li><strong>对“推理”的定义和量化：</strong> “推理”本身是一个复杂且难以精确量化的概念。MMGR的五种能力和具体指标的有效性，以及它们是否能完全捕捉到“推理”的本质，仍需进一步的实验验证和社区讨论。</li>
<li><strong>模型在抽象推理上的极端劣势：</strong> 摘要指出模型在ARC-AGI上准确率低于10%，这表明当前模型在处理高度抽象和符号化的推理任务上存在根本性困难，可能需要全新的模型架构或训练范式来解决。</li>
<li><strong>对“物理常识”的定义和评估：</strong> 虽然提到了体育和组合式交互，但“物理常识”的边界和评估的细致程度也可能影响结果的解释。</li>
</ul>
<p>总而言之，这篇论文通过引入MMGR框架，为评估和提升多模态生成模型的世界模拟能力提供了一个重要且具有前瞻性的方向。它不仅指出了当前模型的关键短板，也为未来的研究提供了清晰的路线图，尤其是在从“感知”向“理解”和“推理”转型的过程中，MMGR具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.14691v2">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.14691v2">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15716v1'></a></p>
<h2 id="spatia-video-generation-with-updatable-spatial-memory"><a href="https://arxiv.org/abs/2512.15716v1">Spatia: Video Generation with Updatable Spatial Memory</a></h2>
<p><strong>Authors:</strong> Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行分析。</p>
<p><strong>论文分析：Spatia: Video Generation with Updatable Spatial Memory</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了一种名为 Spatia 的新型视频生成框架，其核心贡献在于通过引入可更新的 3D 场景点云作为持久化空间记忆，显著提升了视频生成在长期空间和时间上的一致性。Spatia 结合了条件生成与视觉 SLAM（Simultaneous Localization and Mapping）的动态更新机制，实现了生成内容与几何场景的解耦，从而克服了现有模型在处理高维视频信号时面临的挑战。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>Spatia 的关键创新在于其<strong>空间记忆驱动的视频生成范式</strong>。具体来说：</p>
<ul>
<li><strong>显式 3D 场景点云作为空间记忆：</strong> 与以往主要依赖隐式表示或仅关注时间序列的模型不同，Spatia 将一个 3D 场景点云作为核心的、持久化的“空间记忆”。这为生成过程提供了一个明确的几何约束。</li>
<li><strong>迭代生成与空间记忆更新：</strong> 模型并非一次性生成整个视频，而是<strong>迭代地</strong>生成视频片段，并且<strong>每次生成都以当前的空间记忆为条件</strong>。更重要的是，它通过<strong>视觉 SLAM 技术</strong>来<strong>持续更新</strong>这个空间记忆。这意味着模型能够感知并适应场景的变化，即使在生成过程中。</li>
<li><strong>动态-静态解耦设计：</strong> 这种方法实现了动态内容（如运动的物体）与静态场景几何（由点云表示）的解耦。这使得模型既能保持场景的几何一致性，又能生成逼真且具有物理合理性的动态元素。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<p>Spatia 的研究可能对视频生成领域产生深远影响：</p>
<ul>
<li><strong>提升长期一致性：</strong> 解决了当前视频生成模型在长视频中容易出现的空间扭曲、物体漂移等问题，使得生成的视频在视觉上更加连贯和可信。</li>
<li><strong>实现更可控的生成：</strong> 通过显式的 3D 场景表示，为视频生成提供了更强的可控性。研究人员可以更精确地控制相机视角、场景布局等，从而生成符合特定需求的视频。</li>
<li><strong>推动 3D 场景理解与生成融合：</strong> 将 3D 场景重建（通过 SLAM）与视频生成紧密结合，为构建更具几何感知能力的生成模型开辟了新路径。</li>
<li><strong>为下游应用奠定基础：</strong> 这种几何基础的生成框架为诸如 3D 场景编辑、虚拟现实内容生成等应用提供了更坚实的基础。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 生成逼真且几何一致的虚拟场景和交互式内容。</li>
<li><strong>电影和游戏制作：</strong> 自动化生成复杂场景的视频片段，或辅助进行场景设计和动画制作。</li>
<li><strong>机器人学：</strong> 生成模拟环境以训练机器人，或在动态环境中进行导航和规划。</li>
<li><strong>自动驾驶：</strong> 生成逼真的交通场景，用于训练和测试自动驾驶算法。</li>
<li><strong>3D 内容创作：</strong> 简化从 2D 视频到 3D 场景的转换，或直接生成 3D 可编辑的视频内容。</li>
<li><strong>数字孪生：</strong> 构建和更新动态的数字孪生场景。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<p>尽管摘要描绘了一个非常有前景的框架，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算复杂度：</strong> 维护和更新一个 3D 点云，并结合视觉 SLAM 和视频生成，很可能需要大量的计算资源和时间。这可能会限制其在实时应用中的部署。</li>
<li><strong>SLAM 的鲁棒性：</strong> 视觉 SLAM 本身在某些复杂场景（如纹理稀疏、光照剧烈变化、快速运动）下可能存在精度和鲁棒性问题。Spatia 的性能将很大程度上依赖于其底层 SLAM 模块的性能。</li>
<li><strong>点云表示的局限性：</strong> 点云是一种稀疏的几何表示，可能难以捕捉精细的表面细节或拓扑结构。如何有效地将点云信息转化为高质量的视频纹理和细节是一个挑战。</li>
<li><strong>动态内容与静态场景的融合难度：</strong> 尽管提出了解耦设计，但如何无缝地将逼真的动态内容（如人物、车辆）与几何精确的静态场景融合，使其看起来自然且物理一致，仍然是一个技术难题。</li>
<li><strong>训练数据的需求：</strong> 训练这样一个复杂的模型可能需要大量的、带有精确 3D 几何标注的视频数据，这可能难以获取。</li>
</ul>
<p>总而言之，Spatia 提出的空间记忆驱动的视频生成方法，通过显式的 3D 几何表示和动态更新机制，为解决视频生成中的长期一致性问题提供了一个创新的解决方案，并为未来的可控、3D 感知视频生成开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15716v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15716v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15715v1'></a></p>
<h2 id="in-pursuit-of-pixel-supervision-for-visual-pre-training"><a href="https://arxiv.org/abs/2512.15715v1">In Pursuit of Pixel Supervision for Visual Pre-training</a></h2>
<p><strong>Authors:</strong> Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang, Abdelrahman Mohamed, Hengshuang Zhao, Hu Xu</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“In Pursuit of Pixel Supervision for Visual Pre-training”的全面中文摘要：</p>
<p><strong>论文题目：</strong> In Pursuit of Pixel Supervision for Visual Pre-training (追求像素监督以实现视觉预训练)</p>
<p><strong>作者：</strong> Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang, Abdelrahman Mohamed, Hengshuang Zhao, Hu Xu</p>
<p><strong>摘要：</strong></p>
<p>这篇论文探讨了在视觉表示学习领域，像素作为最基础的视觉信息来源，是否能够作为一种简单、稳定且高效的自监督学习信号。作者认为，像素包含了从低级属性到高级概念的全部视觉信息，并提出了一种名为“Pixio”的增强型掩码自编码器（MAE）模型，旨在利用像素级别的监督信号进行视觉预训练。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>论文的核心研究问题在于，在当前主流的自监督学习方法（如DINO系列）倾向于利用更抽象的潜在空间表示时，基于像素的自监督学习方法（如MAE）是否仍然具有竞争力，并能否产生与先进方法相媲美的、适用于广泛下游任务的强大视觉表示。研究还关注如何改进MAE以适应大规模数据和模型，并克服其在处理复杂视觉信息时的局限性。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li><strong>Pixio 模型：</strong> 作者提出了Pixio，一个在MAE基础上进行了四项关键改进的模型。这些改进包括：<ul>
<li><strong>更深的解码器 (Deeper Decoder)：</strong> 增强解码器的容量，以更好地处理像素回归任务，从而减轻编码器在低级细节建模上的负担，使其能更专注于高级语义理解。</li>
<li><strong>更大的掩码块 (Larger Mask Block)：</strong> 采用4x4的局部掩码块而非单像素掩码，以提供更丰富的局部上下文信息，防止模型通过简单复制可见块来完成重建，从而强制模型进行更深入的理解。</li>
<li><strong>更多的类别标记 (More [CLS] Tokens)：</strong> 引入多个类别标记（class tokens），以捕捉图像更丰富的全局视觉属性，而非单个标记的局限性。</li>
<li><strong>大规模、低人工干预的数据集：</strong> 收集了20亿张网页爬取图像，并采用一种基于重构损失的软自策策略进行数据筛选，以减少对人工策展的依赖，避免数据偏差，并获得更具多样性的训练数据。</li>
</ul>
</li>
<li><strong>像素监督的优势：</strong> 论文强调了像素监督的优势在于其更少的人为偏见，因为它直接来源于物理世界的观察，相比于人类定义的类别或文本描述，它更少受到人类认知和语言的限制。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>下游任务表现优异：</strong> Pixio 在包括单目深度估计（如Depth Anything）、前馈3D重建（如MapAnything）、语义分割和机器人学习等多个下游任务上，取得了与甚至超越同等规模的DINOv3模型相当的性能。</li>
<li><strong>证明像素监督的潜力：</strong> 研究结果有力地证明了基于像素的自监督学习方法在今天仍然具有竞争力，并且能够产生强大且通用的视觉表示。这表明像素空间自监督学习可以作为一种有前景的替代方案，并能与潜在空间方法互补。</li>
<li><strong>大规模数据的重要性：</strong> 论文强调了大规模、多样化且经过适当策展的数据集对于提升自监督学习模型性能的关键作用。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>掩码的固有局限性：</strong> 论文承认，即使是像素级别的自监督学习，掩码操作本身也是一种人为的失真，可能引入不必要的偏差。低掩码率会导致信息泄露，高掩码率则可能导致上下文不足和训练/推理分布的偏移。</li>
<li><strong>视频数据的潜力：</strong> 作者指出，静态图像作为视觉信息的载体存在固有局限性，因为它们是孤立的快照。而视频数据通过捕捉事件的自然进程和因果关系，提供了更丰富的时序信息，可能带来更强大的预测性目标，从而减少对人工掩码的需求。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>扩展到视频数据：</strong> 作者计划将像素监督的方法扩展到大规模视频数据，利用视频的时序丰富性和自然预测目标，开发更强大、更少偏差的视觉基础模型。</li>
<li><strong>进一步探索数据策展策略：</strong> 虽然论文提出了低人工干预的数据策展方法，但未来仍有空间探索更有效的策略，以平衡数据多样性和模型性能。</li>
<li><strong>理解掩码的理论基础：</strong> 论文中也提到了对掩码自编码器理论的探索，未来可以进一步深入研究掩码比率、掩码策略等对模型性能的影响。</li>
</ul>
<p>总而言之，这篇论文通过提出Pixio模型并进行大规模实验，成功地证明了像素级别的自监督学习在现代视觉表示学习中依然扮演着重要角色，并为未来的研究提供了新的方向和思路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15715v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15715v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15713v1'></a></p>
<h2 id="diffusionvl-translating-any-autoregressive-models-into-diffusion-vision-language-models"><a href="https://arxiv.org/abs/2512.15713v1">DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</a></h2>
<p><strong>Authors:</strong> Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models”的全面中文摘要：</p>
<p><strong>论文标题：</strong> DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</p>
<p><strong>作者：</strong> Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
近年来，扩散模型（Diffusion Models）作为一种新兴的范式，在多模态理解领域展现出潜力，尤其是在解码方面具有独特优势。然而，现有的扩散视觉语言模型（dVLM）在性能上普遍落后于主流的自回归视觉语言模型（AR-VLM）。这引发了一个关键问题：是否能够将现有的强大自回归模型（包括AR-VLM和AR-LM）有效地转化为高性能的扩散视觉语言模型？</p>
<p><strong>2. 主要创新与方法贡献：</strong>
本文提出了 <strong>DiffusionVL</strong>，一个能够将任何强大的自回归模型转化为扩散视觉语言模型的框架。其核心贡献在于：</p>
<ul>
<li><strong>统一的扩散微调（Diffusion Finetuning）：</strong> 提出了一种简单的微调方法，能够将自回归模型的下一个词预测（NTP）范式转化为扩散范式，而无需修改原始模型的架构。</li>
<li><strong>两种转换路径：</strong><ul>
<li><strong>范式迁移（Paradigm Shift）：</strong> 对于已经具备视觉语言对齐的AR-VLM，直接进行扩散微调即可得到dVLM。</li>
<li><strong>模态与范式迁移（Modality and Paradigm Shift）：</strong> 对于纯AR-LM，采用两阶段方法：首先通过一个可训练的连接器对齐视觉和文本空间，然后进行扩散微调，实现模态和范式的双重转换。</li>
</ul>
</li>
<li><strong>块解码设计（Block-Decoding Design）：</strong> 引入了块解码策略，支持任意长度的生成，并能有效复用KV缓存，显著提升了推理速度。</li>
<li><strong>广泛的实验验证：</strong> 证明了该方法不仅适用于AR-VLM，也适用于AR-LM，并且在数据量远小于先前方法的情况下，取得了优异的性能。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升显著：</strong> DiffusionVL在MMMU-Pro（vision）基准上提升了34.4%，在MME (Cog.) 基准上提升了37.5%。</li>
<li><strong>推理速度提升：</strong> 实现了2倍的推理速度提升。</li>
<li><strong>数据效率高：</strong> 仅使用了先前方法所需数据的5%进行训练，就取得了SOTA（State-of-the-Art）的扩散模型性能。</li>
<li><strong>缩小与AR-VLM的差距：</strong> DiffusionVL显著缩小了现有dVLM与先进AR-VLM之间的性能差距。</li>
<li><strong>AR-LM到dVLM的可行性：</strong> 首次证明了将AR-LM直接转化为高性能dVLM是可行的，并且性能可以与LLaVA风格的视觉指令微调模型相媲美。</li>
<li><strong>模型与代码开源：</strong> 论文发布了模型和代码，促进了相关研究的进一步发展。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>AR-VLM的优势：</strong> 论文也指出，虽然DiffusionVL能够有效转化，但直接从AR-VLM迁移到dVLM的性能优势，部分归功于其基础模型已经进行了广泛且高质量的视觉语言对齐训练。</li>
<li><strong>AR-LM的潜力：</strong> 作者认为，经过更长、更高质量视觉微调的AR-LM，也有潜力构建出与AR-VLM媲美的dVLM。</li>
<li><strong>块大小的权衡：</strong> 实验表明，较小的训练块大小能带来略好的性能，但会牺牲一定的并行性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>AR-LM的进一步优化：</strong> 探索如何通过更精细的视觉微调策略，进一步提升AR-LM转化为dVLM的性能上限。</li>
<li><strong>更复杂的模态与范式转换：</strong> 探索将更多类型的自回归模型（如多模态AR模型）转化为扩散模型。</li>
<li><strong>动态低置信度重掩码策略的深入研究：</strong> 进一步优化动态重掩码策略，以在加速和质量之间找到更好的平衡点。</li>
<li><strong>与其他扩散模型技术的结合：</strong> 探索将DiffusionVL与最新的扩散模型技术（如更高效的噪声调度、注意力机制等）相结合，以进一步提升性能和效率。</li>
</ul>
<p><strong>总结：</strong>
DiffusionVL论文的核心贡献在于提出了一种通用且高效的方法，能够将现有的自回归模型（包括视觉语言模型和纯语言模型）转化为高性能的扩散视觉语言模型。通过创新的扩散微调和块解码设计，DiffusionVL在显著提升推理速度的同时，大幅缩小了与先进自回归模型的性能差距，并且在数据效率方面表现出色。这项工作不仅为构建高性能dVLM提供了一条低成本、高效率的途径，也为理解和融合自回归与扩散模型在多模态领域的潜力开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models.</li>
<li>Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15713v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15713v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15708v1'></a></p>
<h2 id="multi-view-foundation-models"><a href="https://arxiv.org/abs/2512.15708v1">Multi-View Foundation Models</a></h2>
<p><strong>Authors:</strong> Leo Segre, Or Hirschorn, Shai Avidan</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Multi-View Foundation Models</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种将现有的单视图基础模型（Foundation Models）转化为多视图基础模型的方法，使其能够处理同一3D场景的多个RGB图像输入。其核心在于通过引入3D感知注意力层，确保不同视图下对应3D点的特征表示具有高度一致性，从而在图像空间内实现特征的跨视图匹配，而无需显式构建3D模型。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<ul>
<li><strong>核心创新：</strong> 将单视图基础模型扩展到多视图场景，并解决多视图特征不一致的问题。</li>
<li><strong>方法论：</strong><ul>
<li><strong>“3D-aware attention layers”：</strong> 这是论文的核心技术创新。通过在Transformer-based基础模型（如DINO, SAM, CLIP）的中间层插入这些注意力层，模型能够学习到如何根据不同视图之间的几何关系来匹配特征。这种设计允许模型在不显式构建3D几何模型的情况下，隐式地理解3D结构和点之间的对应关系。</li>
<li><strong>图像空间操作：</strong> 论文强调其方法直接在图像空间进行操作，避免了构建复杂3D模型或进行3D重建的开销，这使得方法更具通用性和效率。</li>
<li><strong>特征一致性优化：</strong> 目标是使不同视图下对应3D点的特征尽可能一致，这是衡量多视图特征融合效果的关键指标。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升多视图理解能力：</strong> 使得基础模型能够更好地理解和利用来自同一场景的多个视角信息，从而在各种下游任务中获得更鲁棒和准确的结果。</li>
<li><strong>降低多视图任务的门槛：</strong> 通过直接增强现有成熟的基础模型，降低了开发高性能多视图应用的门槛，无需从头开始设计专门的多视图模型。</li>
<li><strong>推动3D感知研究：</strong> 尽管避免了显式3D建模，但该方法通过隐式3D感知来提升特征匹配，这可能为未来更高效、更通用的3D感知方法提供新的思路。</li>
<li><strong>通用性：</strong> 该方法可以应用于多种Transformer-based基础模型，具有良好的通用性，有望成为多视图计算机视觉领域的一个标准化增强技术。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>3D重建与场景理解：</strong> 更一致的多视图特征可以显著提升多视图立体（MVS）和场景流（Scene Flow）等任务的准确性。</li>
<li><strong>机器人导航与感知：</strong> 机器人通常依赖于多个传感器（如多个摄像头）获取信息，一致的多视图特征对于路径规划、障碍物检测和环境建图至关重要。</li>
<li><strong>增强现实（AR）与虚拟现实（VR）：</strong> 在AR/VR应用中，准确地对齐和融合来自不同视角的真实世界信息是实现沉浸式体验的关键。</li>
<li><strong>自动驾驶：</strong> 自动驾驶车辆需要处理来自多个摄像头和传感器的信息，以全面感知周围环境，一致的多视图特征有助于提高感知系统的鲁棒性。</li>
<li><strong>多视角图像检索与匹配：</strong> 能够更准确地匹配不同视角下的相同物体或场景。</li>
<li><strong>物体识别与分割：</strong> 在遮挡或视角变化较大的情况下，多视图信息可以提供更全面的物体信息，提高识别和分割的准确性。</li>
<li><strong>医学影像分析：</strong> 在某些医学成像技术中，会获取同一病灶的多个视图，一致的特征表示有助于更精确的诊断和分析。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>对初始单视图基础模型的依赖：</strong> 该方法的性能在很大程度上取决于底层单视图基础模型的质量和能力。如果基础模型本身存在缺陷，其多视图扩展也可能受到限制。</li>
<li><strong>“3D-aware attention layers”的实现细节：</strong> 摘要中并未详细说明这些注意力层的具体结构和训练方式。其有效性可能取决于这些层的设计是否能够有效地捕捉跨视图的几何关系。</li>
<li><strong>计算开销的增加：</strong> 引入额外的注意力层可能会增加模型的计算复杂度和推理时间，尽管论文声称避免了显式3D建模，但计算量的增加仍是潜在的考虑因素。</li>
<li><strong>对相机姿态的隐式或显式要求：</strong> 虽然论文提到“bypasses the need to build a consistent 3D model”，但为了有效地匹配特征，模型可能仍然需要某种形式的相机姿态信息（即使是隐式的学习）来理解视图之间的相对关系。如果相机姿态信息不可用或不准确，效果可能会打折扣。</li>
<li><strong>“as consistent as possible”的度量：</strong> 摘要中提到“as consistent as possible”，这表明特征一致性可能是一个优化目标，但达到完全一致可能仍然是一个挑战，其“一致性”的程度和评估标准需要进一步的实验验证。</li>
<li><strong>实验验证的范围：</strong> 摘要中提到了“surface normal estimation and multi-view segmentation tasks”作为示例，但其在更广泛的多视图任务上的泛化能力仍需进一步验证。</li>
</ul>
<p>总而言之，这篇论文提出了一种非常有前景的方法，通过引入“3D-aware attention layers”来增强现有基础模型的多视图理解能力，有望在多个计算机视觉领域带来显著的性能提升，尤其是在处理真实世界中常见的、具有多视角信息的场景时。其最大的亮点在于在不进行显式3D重建的前提下，实现了跨视图特征的有效对齐和一致性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a way to convert a Foundation Model into a Multi-View Foundation Model.</li>
<li>Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views.</li>
<li>As leading examples, we show surface normal estimation and multi-view segmentation tasks.</li>
<li>Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15708v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15708v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15707v1'></a></p>
<h2 id="gatefusion-hierarchical-gated-cross-modal-fusion-for-active-speaker-detection"><a href="https://arxiv.org/abs/2512.15707v1">GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</a></h2>
<p><strong>Authors:</strong> Yu Wang, Juhyung Ha, Frangil M. Ramirez, Yuchen Wang, David J. Crandall</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection”的全面中文摘要：</p>
<p><strong>论文题目：</strong> GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection (GateFusion：用于主动说话人检测的分层门控跨模态融合)</p>
<p><strong>作者：</strong> Yu Wang, Juhyung Ha, Frangil M. Ramirez, Yuchen Wang, David J. Crandall</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
主动说话人检测（ASD）旨在识别视频中每一帧正在说话的人。现有的大多数先进方法依赖于“晚期融合”（late fusion），即将视觉和音频特征在最后阶段进行合并。然而，这种方法往往难以捕捉细粒度的跨模态交互，而这种交互对于在非约束场景下实现鲁棒的性能至关重要。当一种模态信息不足或模糊时（例如，说话人不可见，或多人声音相似），晚期融合的局限性尤为明显。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了解决上述问题，本文提出了<strong>GateFusion</strong>，一种新颖的架构，其核心创新在于：</p>
<ul>
<li><strong>分层门控融合解码器 (HiGate)：</strong> GateFusion引入了一个名为HiGate的解码器，它能够实现渐进式的、多深度的跨模态融合。HiGate通过在Transformer骨干网络的多个层级，自适应地将一种模态的上下文特征注入到另一种模态中来实现融合。这种注入过程由可学习的、双模态条件控制的门控机制（gates）引导，从而实现细粒度的、上下文感知的跨模态交互。HiGate的设计是对称的，允许音频和视觉模态互相扮演主导或上下文角色，支持灵活的双向信息流。</li>
<li><strong>两个辅助训练目标：</strong><ul>
<li><strong>掩码对齐损失 (MAL)：</strong> 旨在使单模态的预测结果与多模态的预测结果对齐，但仅限于有主动说话人的帧。这有助于增强单模态分支在处理模糊或有噪声情况下的可靠性，并鼓励它们学习跨模态依赖关系。</li>
<li><strong>过正惩罚 (OPP)：</strong> 专门用于抑制视频分支产生的过多的假阳性激活，尤其是在视觉信息模糊或退化的场景下。这有助于提高模型在视觉信息不确定时的鲁棒性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
GateFusion在多个具有挑战性的ASD基准测试中取得了最先进（state-of-the-art）的性能，具体表现为：
*   在Ego4D-ASD上达到 <strong>77.8% mAP</strong>（比之前最佳方法提高9.4%）。
*   在UniTalk上达到 <strong>86.1% mAP</strong>（比之前最佳方法提高2.9%）。
*   在WASD上达到 <strong>96.1% mAP</strong>（比之前最佳方法提高0.5%）。
*   在AVA-ActiveSpeaker上也取得了具有竞争力的性能。</p>
<p>此外，论文进行了跨领域（out-of-domain）实验，证明了GateFusion在不同数据集上的<strong>泛化能力</strong>。消融实验也充分展示了HiGate、MAL和OPP等组件的<strong>互补效益</strong>，共同提升了模型的性能和鲁棒性。这些结果表明，GateFusion在处理非约束和具有挑战性的音频-视觉场景方面具有显著优势。</p>
<p><strong>4. 提及的局限性：</strong>
论文中并未明确列出具体的局限性，但从其研究方向和提出的方法来看，可以推断出一些潜在的方面：
*   <strong>计算成本：</strong> 尽管论文在效率分析中展示了GateFusion相比于某些基线模型（如LoCoNet）具有更高的效率，但分层融合和门控机制本身可能比简单的晚期融合方法有更高的计算开销。
*   <strong>超参数敏感性：</strong> 辅助损失的权重（λMAL和λOPP）需要仔细调整，以平衡它们与主分类损失的作用。
*   <strong>对预训练模型的依赖：</strong> GateFusion依赖于强大的预训练单模态编码器，其性能在一定程度上受限于这些编码器的能力。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于论文的研究内容，可以推测以下潜在的未来研究方向：
*   <strong>更精细的门控机制：</strong> 探索更复杂的门控机制，以更精细地控制跨模态信息的注入和交互。
*   <strong>多说话人场景的扩展：</strong> 虽然论文提到了处理多人场景的挑战，但进一步优化模型以在更复杂的、多人同时说话的场景下实现更精确的检测和区分。
*   <strong>实时性优化：</strong> 进一步探索模型压缩和加速技术，以满足更严格的实时性要求。
*   <strong>跨模态信息互补的深入研究：</strong> 探索如何更有效地利用一种模态的缺失信息来推断另一种模态，或者在模态信息质量差异较大的情况下进行更鲁棒的融合。
*   <strong>应用于其他跨模态任务：</strong> 将GateFusion的层级门控融合思想应用于其他需要细粒度跨模态交互的任务，如视频字幕生成、情感识别等。</p>
<p>总而言之，GateFusion通过引入创新的HiGate分层门控融合机制和有效的辅助训练目标，显著提升了主动说话人检测的性能和鲁棒性，尤其是在非约束和具有挑战性的场景下，为跨模态融合在计算机视觉领域的研究提供了新的思路和方法。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios.</li>
<li>In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate).</li>
<li>To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations.</li>
<li>GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15707v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15707v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15702v1'></a></p>
<h2 id="end-to-end-training-for-autoregressive-video-diffusion-via-self-resampling"><a href="https://arxiv.org/abs/2512.15702v1">End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</a></h2>
<p><strong>Authors:</strong> Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei, Zhenheng Yang, Weilin Huang, Dahua Lin</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“End-to-End Training for Autoregressive Video Diffusion via Self-Resampling”的全面中文摘要：</p>
<p><strong>论文题目：</strong> End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</p>
<p><strong>作者：</strong> Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei, Zhenheng Yang, Weilin Huang, Dahua Lin</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>Resampling Forcing</strong> 的新颖框架，旨在解决自回归视频扩散模型在训练过程中面临的 <strong>曝光偏差（exposure bias）</strong> 问题，并实现端到端的训练。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>自回归视频扩散模型在模拟世界和预测未来状态方面展现出巨大潜力，但其训练方式（教师强制，teacher forcing）与推理方式存在不匹配。在训练时，模型依赖于真实的、无误的历史帧进行预测；而在推理时，模型必须依赖于自身生成的、可能包含误差的历史帧。这种“训练-测试不匹配”会导致模型预测误差的累积，尤其是在生成长视频时，可能导致视频质量的灾难性下降（即视频崩溃）。现有方法通常采用训练后蒸馏或引入在线判别器来缓解此问题，但这些方法难以实现从头开始的大规模端到端训练，且可能引入额外的复杂性或泄露未来信息。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>Resampling Forcing 框架：</strong> 提出了一种教师无关（teacher-free）的端到端训练框架，允许自回归视频扩散模型从零开始进行大规模训练。</li>
<li><strong>自重采样（Self-Resampling）机制：</strong> 这是该方法的核心。在训练过程中，该机制模拟了推理时模型可能产生的误差。具体而言，它通过在真实历史帧上引入一定程度的噪声（模拟模型预测误差），然后使用在线模型权重来完成剩余的去噪步骤，从而生成一个包含模型误差的“退化”历史帧。模型随后基于这些退化历史帧来预测下一帧。这种方式迫使模型学习在不完美的输入下保持鲁棒性，从而缓解误差累积。</li>
<li><strong>并行训练与因果掩码：</strong> 结合退化历史帧和因果掩码（sparse causal mask），实现了并行训练，并利用逐帧扩散损失（frame-level diffusion loss）来保证时间因果性。</li>
<li><strong>历史路由（History Routing）机制：</strong> 为了解决长视频生成中注意力机制复杂度随历史帧数增长而急剧增加的问题，论文引入了一个参数无关（parameter-free）的动态历史路由机制。该机制能够为每个查询（query）动态地检索最相关的 top-k 个历史帧进行注意力计算，从而将注意力复杂度维持在近乎恒定的水平，有效支持长时序生成。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能媲美蒸馏基线：</strong> 实验结果表明，Resampling Forcing 在生成质量上与基于蒸馏的先进基线模型相当。</li>
<li><strong>长视频生成优势：</strong> 由于采用了原生长视频训练（native-length training），该方法在生成长视频时表现出优越的时间一致性，显著优于那些通过截断或外插长视频的基线方法。</li>
<li><strong>严格的时间因果性：</strong> 与蒸馏基线相比，该模型更严格地遵守了因果依赖关系。</li>
<li><strong>高效的长时序上下文：</strong> 历史路由机制在保持可忽略的质量损失的情况下，实现了稀疏上下文，为长时序生成提供了可行的内存设计方案。</li>
<li><strong>意义：</strong> 该工作为自回归视频扩散模型的端到端、可扩展训练提供了新的途径，并解决了长时序视频生成中的关键挑战，有望推动未来视频世界模型的进步。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>推理速度：</strong> 作为一种扩散模型，其推理过程需要迭代去噪步骤，实时性可能需要后处理加速（如少步蒸馏）或改进采样器。</li>
<li><strong>训练效率：</strong> 训练过程需要同时处理扩散样本和干净历史帧，这可能可以通过架构优化来进一步提高效率。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>加速推理：</strong> 探索更有效的后处理技术或采样方法来提高推理速度，以实现实时生成。</li>
<li><strong>训练优化：</strong> 研究更优的架构设计，以进一步提高训练效率，可能通过更紧凑地处理扩散样本和干净历史帧。</li>
<li><strong>更长时序的探索：</strong> 进一步探索和优化历史路由机制，以支持更长、更复杂的视频生成任务。</li>
<li><strong>多模态融合：</strong> 将该框架扩展到更广泛的多模态视频生成任务中。</li>
</ul>
<p>总而言之，这篇论文通过创新的自重采样和历史路由机制，成功地解决了自回归视频扩散模型在训练和长时序生成中的核心挑战，为构建更强大、更可靠的视频生成模型奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale.</li>
<li>Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training.</li>
<li>Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15702v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15702v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15701v1'></a></p>
<h2 id="vlic-vision-language-models-as-perceptual-judges-for-human-aligned-image-compression"><a href="https://arxiv.org/abs/2512.15701v1">VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</a></h2>
<p><strong>Authors:</strong> Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski, Li Fei-Fei, Jiajun Wu, Jason Zhang</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression”的全面中文摘要，重点突出其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</p>
<p><strong>作者：</strong> Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Hołyński, Li Fei-Fei, Jiajun Wu, Jason Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
传统的图像压缩模型在评估图像质量时，往往依赖于均方误差（MSE）等失真度量，这些度量与人类的感知判断存在显著差距。为了使图像压缩模型更好地对齐人类感知，以往的研究通常依赖于使用大量人类心理视觉判断数据训练的可微分感知损失网络。然而，收集这些数据成本高昂，且训练出的感知模型可能存在泛化性问题。本研究的核心问题是如何在不依赖大量人工标注数据的情况下，构建一个能够准确评估图像感知质量并用于指导图像压缩模型训练的系统。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了一种名为 <strong>VLIC (Vision-Language Models for Image Compression)</strong> 的新颖图像压缩系统，其核心创新在于：</p>
<ul>
<li><strong>利用视觉语言模型（VLMs）作为零样本感知裁判：</strong> 作者发现，现成的先进VLMs（如Gemini 2.5-Flash）能够零样本（zero-shot）地准确复制人类在二选一强制选择（2AFC）任务中的视觉相似性判断。这意味着VLMs具备了强大的、无需额外训练的感知理解能力。</li>
<li><strong>基于VLM偏好进行扩散模型后训练：</strong> VLIC系统将VLMs的判断能力直接应用于扩散模型（Diffusion Model）的后训练阶段。具体而言，利用VLM对同一潜在编码生成的两个不同重建图像进行排序，并将这种偏好信号通过 <strong>Diffusion DPO (Direct Preference Optimization)</strong> 技术来优化扩散模型，使其生成更符合人类感知的图像。这种方法避免了将VLM的判断蒸馏成一个独立的感知损失网络，而是直接利用了VLM的推理能力。</li>
<li><strong>VLM与LPIPS的集成：</strong> 为了进一步提高奖励信号的鲁棒性和一致性，VLIC将VLM的偏好与传统的感知度量LPIPS（Learned Perceptual Image Patch Similarity）相结合。只有当两者达成一致时，才使用该偏好对进行训练，这有效减少了VLM可能产生的噪声和幻觉。</li>
<li><strong>改进的VLM奖励设计：</strong> 为了提高VLM判断的可靠性，作者提出了一系列策略，包括：对同一对图像进行双向评估（反转顺序），对多个随机种子下的VLM判断进行自集成（self-ensembling），以及与LPIPS进行一致性检查。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
VLIC在多个标准图像压缩数据集上取得了令人鼓舞的结果：</p>
<ul>
<li><strong>竞争性或领先的性能：</strong> VLIC在人类感知对齐的评估指标上，根据数据集的不同，达到了与现有最先进（state-of-the-art）的图像压缩方法相当甚至更好的性能。</li>
<li><strong>在MS-COCO数据集上的优势：</strong> VLIC在包含大量人脸、文本等人类敏感特征的MS-COCO数据集上表现尤为出色，这表明其对人类感知细节的捕捉能力更强。</li>
<li><strong>验证了VLM作为感知裁判的潜力：</strong> 研究结果有力地证明了VLMs作为零样本感知裁判的有效性，为未来图像压缩领域的研究提供了一种新的范式，即利用大型多模态模型来指导感知优化。</li>
<li><strong>对VLM后训练的深入分析：</strong> 文章还提供了关于VLM奖励设计和训练程序的详细分析，为未来使用VLMs进行模型对齐提供了宝贵的实践指导。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>扩散模型的固有延迟：</strong> 与GANs等方法相比，扩散模型在推理时存在一定的延迟，尽管这一点在其他基于扩散的模型中也存在。
*   <strong>VLM奖励计算成本：</strong> 使用VLM作为奖励函数比使用小型感知网络计算成本更高。
*   <strong>VLM的幻觉和不一致性：</strong> 尽管采取了多种缓解措施，但VLMs在处理高度相似的图像时仍可能出现不一致或错误的判断（如图6所示）。
*   <strong>对低比特率下PSNR的牺牲：</strong> VLIC为了更好地对齐人类感知，可能会在像素级度量（如PSNR）上有所牺牲，这在固定比特率下是权衡的结果。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>利用更强大的VLMs：</strong> 随着VLMs的不断发展和改进，其零样本感知能力将进一步增强，有望为VLIC带来更优的性能。
*   <strong>探索更广泛的VLM应用：</strong> VLMs的感知能力不仅限于图像质量评估，还可以用于指导图像编辑、风格迁移等更复杂的任务。
*   <strong>降低VLM推理成本：</strong> 研究更高效的VLM推理方法或模型压缩技术，以降低VLIC的计算开销。
*   <strong>更精细的奖励设计：</strong> 进一步探索如何设计更鲁棒、更具区分度的VLM奖励信号，以应对各种复杂的图像内容和失真类型。
*   <strong>多模态信息融合：</strong> 探索将文本描述等其他模态信息与图像内容结合，以实现更全面的人类感知对齐。</p>
<p><strong>总结：</strong></p>
<p>这篇论文的核心贡献在于开创性地将大型视觉语言模型（VLMs）引入图像压缩领域，并成功地利用其强大的零样本感知理解能力，通过Diffusion DPO技术对扩散模型进行后训练，构建了VLIC系统。VLIC在不依赖大量人工标注数据的情况下，实现了与现有最先进方法相当甚至更优的人类感知对齐性能，尤其在包含人脸和文本等敏感特征的图像上表现突出。这项工作不仅为图像压缩领域提供了一种新颖且高效的训练范式，也为未来利用大型多模态模型指导计算机视觉任务的研究开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images.</li>
<li>Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments.</li>
<li>We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15701v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15701v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15693v1'></a></p>
<h2 id="skyra-ai-generated-video-detection-via-grounded-artifact-reasoning"><a href="https://arxiv.org/abs/2512.15693v1">Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</a></h2>
<p><strong>Authors:</strong> Yifei Li, Wenzhao Zheng, Yanran Zhang, Runze Sun, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本研究提出了Skyra，一个专门用于检测AI生成视频的多模态大型语言模型（MLLM）。Skyra的核心贡献在于其能够识别并利用人类可感知的视觉伪影作为检测和解释的依据，从而克服了现有方法仅限于二元分类且缺乏可解释性的局限。为此，论文构建了首个大规模AI生成视频伪影数据集ViF-CoT-4K，并提出了一种两阶段训练策略来提升模型的时空伪影感知、解释能力和检测准确性。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>多模态大型语言模型（MLLM）的应用：</strong> 将MLLM的能力引入AI生成视频检测领域，使其能够同时处理视觉信息和语言解释。</li>
<li><strong>基于“接地伪影推理”（Grounded Artifact Reasoning）的检测范式：</strong> 这是本研究最核心的创新点。Skyra不是简单地进行二元分类，而是主动寻找并分析AI生成视频中存在的、人类肉眼可见的视觉伪影（如不自然的纹理、运动不连贯、物体变形等）。这些伪影被视为“接地证据”，用于支持检测结果，并能生成可解释的说明。</li>
<li><strong>首个大规模AI生成视频伪影数据集（ViF-CoT-4K）：</strong> 专门为训练模型识别和理解伪影而构建，包含细粒度的人类标注，这对于监督微调（SFT）至关重要。</li>
<li><strong>两阶段训练策略：</strong> 旨在系统性地提升模型在时空伪影感知、解释能力和最终检测准确性方面的表现。这可能意味着模型在不同阶段侧重于不同的学习目标。</li>
<li><strong>专门的评估基准（ViF-Bench）：</strong> 包含来自十余个SOTA视频生成器的3K高质量样本，为全面评估Skyra的性能提供了标准。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升AI生成视频检测的可靠性和可信度：</strong> 通过提供可解释的检测结果，Skyra有望增强用户对检测结果的信任，并帮助区分真实视频和虚假视频。</li>
<li><strong>推动可解释AI（XAI）在多媒体安全领域的应用：</strong> 本研究展示了如何将XAI的理念融入到AI生成内容检测中，为其他内容安全问题提供借鉴。</li>
<li><strong>为AI生成视频的伦理和法律监管提供技术支持：</strong> 准确且可解释的检测工具对于打击虚假信息、保护个人隐私以及制定相关法规至关重要。</li>
<li><strong>促进AI视频生成技术的发展：</strong> 通过揭示当前生成技术中存在的伪影，可以为研究人员提供改进生成模型的方向。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>媒体内容审核与事实核查：</strong> 社交媒体平台、新闻机构等可以利用Skyra来识别和标记AI生成的虚假视频内容。</li>
<li><strong>数字取证：</strong> 在法律和调查领域，Skyra可以帮助识别视频证据的真实性。</li>
<li><strong>内容创作者和平台：</strong> 帮助内容创作者理解其生成内容的潜在问题，并为平台提供内容安全工具。</li>
<li><strong>网络安全：</strong> 防范利用AI生成视频进行的欺诈、诽谤或政治操纵。</li>
<li><strong>教育和研究：</strong> 为AI生成内容检测和可解释AI的研究提供新的工具和数据集。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>对“人类可感知”伪影的依赖：</strong> 尽管这是其优势，但也意味着如果AI生成技术发展到能够完全消除人类肉眼难以察觉的伪影，Skyra的有效性可能会受到影响。</li>
<li><strong>数据集和基准的覆盖范围：</strong> ViF-CoT-4K和ViF-Bench虽然规模较大，但可能仍无法覆盖所有现有的AI视频生成技术和可能出现的伪影类型。随着新生成模型的出现，数据集和基准可能需要不断更新。</li>
<li><strong>计算资源需求：</strong> 作为基于MLLM的模型，Skyra的训练和部署可能需要大量的计算资源。</li>
<li><strong>解释的深度和准确性：</strong> 虽然论文强调了解释能力，但解释的详细程度、准确性以及是否能被所有用户理解，仍需在实际应用中进一步验证。</li>
<li><strong>泛化能力：</strong> 模型在未见过的新型AI生成视频上的泛化能力，以及对不同领域（如电影、纪录片、个人Vlog等）视频的适应性，是需要进一步考察的。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Skyra这篇论文在AI生成视频检测领域提出了一个非常有前景的新方向。通过将MLLM的能力与对视觉伪影的“接地推理”相结合，并辅以高质量的数据集和评估基准，该研究有望显著提升AI生成视频检测的准确性和可解释性。这对于应对日益严峻的AI生成内容滥用问题具有重要的理论和实践意义。其核心创新在于从“黑盒”分类转向“白盒”的伪影分析和解释，这正是当前AI安全和可信AI领域所急需的突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation.</li>
<li>To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15693v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15693v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.15692v1'></a></p>
<h2 id="mimic-video-video-action-models-for-generalizable-robot-control-beyond-vlas"><a href="https://arxiv.org/abs/2512.15692v1">mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</a></h2>
<p><strong>Authors:</strong> Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</p>
<p><strong>Published:</strong> 2025-12-17</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs”的全面中文摘要，重点突出了其新颖性和重要性：</p>
<p><strong>论文摘要：mimic-video：超越VLA的通用机器人控制视频-动作模型</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>当前主流的视觉-语言-动作（VLA）模型在机器人操控领域取得了显著进展，但它们主要依赖于在海量但静态的互联网图像-文本数据上预训练的视觉-语言模型（VLMs）。这种范式导致模型虽然在语义理解上表现良好，但必须从稀疏且昂贵的机器人轨迹数据中隐式地学习复杂的物理动力学和时间依赖关系。这带来了巨大的数据负担，需要持续收集大量专家演示数据来弥补模型对物理因果关系的先天理解不足。论文的核心问题在于：如何更有效地利用预训练模型来学习机器人控制，特别是如何解决当前VLA模型在物理理解和数据效率上的瓶颈。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>本文提出了一种名为 <strong>mimic-video</strong> 的新型 <strong>视频-动作模型（VAM）</strong>，其核心创新在于：</p>
<ul>
<li><strong>利用视频预训练的物理动力学先验：</strong> mimic-video 摒弃了仅依赖静态图像-文本的VLM，而是利用预训练的互联网规模视频模型。视频数据天然地包含了“事物如何发生”的动态信息，能够捕捉物体运动、形变和交互的物理过程。</li>
<li><strong>解耦规划与控制：</strong> mimic-video 将视频模型的长时序规划能力（生成部分去噪的视频“视觉计划”）与一个轻量级的动作解码器（作为逆动力学模型，IDM）相结合。视频模型负责生成未来场景的潜在表示，而动作解码器则专注于将这些潜在表示转化为低维度的机器人动作。这种解耦使得动作解码器无需从头学习复杂的未来分布，而能专注于更简单的逆动力学问题。</li>
<li><strong>部分去噪策略：</strong> 在推理阶段，mimic-video 采用一种“部分去噪”策略，即仅对视频模型生成的中间状态进行去噪，而不是完全去噪到最终的清晰视频。这种策略在实验中被证明能带来更好的策略性能，并显著加速推理速度，因为它只需要视频模型的一次前向传播。</li>
<li><strong>条件流匹配（CFM）框架：</strong> 论文利用条件流匹配（CFM）框架来训练视频模型和动作解码器，这是一种用于学习数据分布的有效方法。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>状态-艺术性能：</strong> mimic-video 在模拟和真实世界的机器人操控任务上取得了最先进的性能，包括精细的操纵任务。</li>
<li><strong>显著提升数据效率：</strong> 与传统的VLA模型相比，mimic-video 将样本效率提高了 <strong>10倍</strong>。这意味着模型可以用更少的数据来达到相似或更好的性能。</li>
<li><strong>加速收敛速度：</strong> 动作解码器的训练速度提高了 <strong>2倍</strong>，这得益于视频模型提供的更丰富、更具物理意义的先验信息。</li>
<li><strong>泛化能力：</strong> mimic-video 在多种机器人实体（从单臂到双臂、模拟到真实世界）上展现了良好的泛化能力。</li>
<li><strong>对视频质量的洞察：</strong> 研究发现，过高的视频保真度（完全去噪）反而可能导致性能下降，因为不完美的视频生成可能引入与训练数据分布不符的噪声，而适度的“噪声”可以作为一种有效的训练和测试时增强。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>单视角视频模型：</strong> 当前模型依赖于单视角视频模型，这限制了其在需要多视角理解的任务中的空间推理和遮挡鲁棒性。</li>
<li><strong>未实现统一的跨实体模型：</strong> 论文尚未将VAM范式应用于训练一个统一的、大规模的、跨实体的模型，这被认为是解锁视频基础模型全部泛化能力的关键一步。</li>
<li><strong>真实世界实验任务有限：</strong> 当前的真实世界实验仅限于一组特定的任务，未来需要扩展到更多样化的操纵行为。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>探索多视角视频模型：</strong> 集成多视角视频模型以增强空间推理和遮挡鲁棒性。</li>
<li><strong>构建统一的跨实体VAM模型：</strong> 训练一个能够处理多种机器人实体和任务的通用VAM模型。</li>
<li><strong>扩展到更广泛的真实世界任务：</strong> 将该方法应用于更广泛、更复杂的机器人操纵场景。</li>
<li><strong>进一步优化推理效率：</strong> 探索更高效的视频生成和动作解码策略，以实现实时控制。</li>
</ul>
<p><strong>总结：</strong></p>
<p>mimic-video 是一项重要的研究工作，它成功地将预训练视频模型的丰富物理动力学先验引入机器人控制领域，显著克服了现有VLA模型在数据效率和物理理解上的瓶颈。通过解耦规划与控制以及创新的部分去噪策略，mimic-video 在多种机器人任务上取得了优异的性能，并为未来更通用、更高效的机器人学习奠定了基础。这项工作强调了视频数据在机器人控制中的关键作用，并为如何有效利用这些数据提供了新的视角。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations.</li>
<li>Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.15692v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.15692v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-18 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
