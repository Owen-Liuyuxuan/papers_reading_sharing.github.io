<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-13 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-12/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-14/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-13">Arxiv Computer Vision Papers - 2026-01-13</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-generation-models-in-robotics-applications-research-challenges-future-directions" class="nav-link">Video Generation Models in Robotics - Applications, Research Challenges, Future Directions</a>
                </li>
                <li class="nav-item">
                    <a href="#tuning-free-visual-effect-transfer-across-videos" class="nav-link">Tuning-free Visual Effect Transfer across Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#mhla-restoring-expressivity-of-linear-attention-via-token-level-multi-head" class="nav-link">MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</a>
                </li>
                <li class="nav-item">
                    <a href="#more-images-more-problems-a-controlled-analysis-of-vlm-failure-modes" class="nav-link">More Images, More Problems? A Controlled Analysis of VLM Failure Modes</a>
                </li>
                <li class="nav-item">
                    <a href="#os-symphony-a-holistic-framework-for-robust-and-generalist-computer-using-agent" class="nav-link">OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-external-guidance-unleashing-the-semantic-richness-inside-diffusion-transformers-for-improved-training" class="nav-link">Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training</a>
                </li>
                <li class="nav-item">
                    <a href="#video-evidence-to-reasoning-efficient-video-understanding-via-explicit-evidence-grounding" class="nav-link">Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding</a>
                </li>
                <li class="nav-item">
                    <a href="#evaluating-the-encoding-competence-of-visual-language-models-using-uncommon-actions" class="nav-link">Evaluating the encoding competence of visual language models using uncommon actions</a>
                </li>
                <li class="nav-item">
                    <a href="#smooth-operator-smooth-verifiable-reward-activates-spatial-reasoning-ability-of-vision-language-model" class="nav-link">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</a>
                </li>
                <li class="nav-item">
                    <a href="#leveraging-3d-representation-alignment-and-rgb-pretrained-priors-for-lidar-scene-generation" class="nav-link">Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-13">Arxiv Computer Vision Papers - 2026-01-13</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域论文的每日报告执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2026-01-12)</strong></p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>视频理解与生成</strong>、<strong>视觉语言模型 (VLM) 的能力评估与提升</strong>，以及<strong>3D 视觉与机器人应用</strong>。其中，视频生成在机器人领域的应用、视频效果迁移、以及利用扩散模型进行更高效的视频理解是突出亮点。同时，对 VLM 的鲁棒性、泛化能力和特定推理能力的深入探究也占据重要位置。</p>
<p><strong>2. 显著或创新性论文：</strong></p>
<ul>
<li><strong>"OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent"</strong> 展现了构建通用且鲁棒的计算机使用智能体的潜力，可能为自动化任务和人机交互带来突破。</li>
<li><strong>"MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head"</strong> 提出了一种改进线性注意力机制的方法，有望提升 Transformer 在处理长序列时的效率和表达能力。</li>
<li><strong>"Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation"</strong> 结合了 3D 表示对齐和 RGB 预训练先验，为 LiDAR 场景生成提供了新的思路，对自动驾驶和机器人感知至关重要。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>视频生成在机器人领域的融合应用：</strong> 论文 1 明确指出了这一方向的潜力和挑战，预示着未来机器人将更依赖于视频生成来规划和执行任务。</li>
<li><strong>无需微调的视频效果迁移：</strong> 论文 2 展示了在不进行模型微调的情况下实现跨视频效果迁移的可能性，这对于内容创作和视频编辑具有重要意义。</li>
<li><strong>Diffusion Transformer 的内部语义挖掘：</strong> 论文 6 探索了如何更有效地利用扩散 Transformer 内部的语义信息进行训练，可能带来更强大的生成和理解能力。</li>
<li><strong>显式证据关联的视频理解：</strong> 论文 7 提出的通过显式证据关联来提升视频理解能力，强调了模型对视频内容进行推理和解释的重要性。</li>
<li><strong>VLM 的空间推理能力激活：</strong> 论文 9 提出了一种通过平滑可验证奖励来激活 VLM 空间推理能力的方法，是提升 VLM 在复杂场景下理解能力的关键。</li>
</ul>
<p><strong>4. 建议深入阅读的论文：</strong></p>
<p>考虑到其潜在的广泛影响和技术创新性，以下论文值得深入阅读：</p>
<ul>
<li><strong>"OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent"</strong> (论文 5)：对于构建更智能、更通用的 AI 代理具有重要意义。</li>
<li><strong>"Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation"</strong> (论文 10)：对于自动驾驶、机器人感知和 3D 内容生成领域的研究者至关重要。</li>
<li><strong>"Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding"</strong> (论文 7)：对于提升视频理解的深度和可解释性具有启发性。</li>
<li><strong>"More Images, More Problems? A Controlled Analysis of VLM Failure Modes"</strong> (论文 4) 和 <strong>"Evaluating the encoding competence of visual language models using uncommon actions"</strong> (论文 8)：这两篇论文都聚焦于 VLM 的评估和局限性，对于理解和改进 VLM 的鲁棒性至关重要。</li>
</ul>
<hr />
<p>希望这份执行摘要能帮助您快速了解本期 Arxiv 论文的重点内容。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.07823v1">Video Generation Models in Robotics - Applications, Research Challenges, Future Directions</a></li>
<li><a href="#2601.07833v1">Tuning-free Visual Effect Transfer across Videos</a></li>
<li><a href="#2601.07832v1">MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</a></li>
<li><a href="#2601.07812v1">More Images, More Problems? A Controlled Analysis of VLM Failure Modes</a></li>
<li><a href="#2601.07779v1">OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent</a></li>
<li><a href="#2601.07773v1">Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training</a></li>
<li><a href="#2601.07761v1">Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding</a></li>
<li><a href="#2601.07737v1">Evaluating the encoding competence of visual language models using uncommon actions</a></li>
<li><a href="#2601.07695v1">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</a></li>
<li><a href="#2601.07692v1">Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.07823v1'></a></p>
<h2 id="video-generation-models-in-robotics-applications-research-challenges-future-directions"><a href="https://arxiv.org/abs/2601.07823v1">Video Generation Models in Robotics - Applications, Research Challenges, Future Directions</a></h2>
<p><strong>Authors:</strong> Zhiting Mei, Tenny Yin, Ola Shorinwa, Apurva Badithela, Zhonghe Zheng, Joseph Bruno, Madison Bland, Lihan Zha, Asher Hancock, Jaime Fernández Fisac, Philip Dames, Anirudha Majumdar</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> eess.SY, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video Generation Models in Robotics: Applications, Research Challenges, Future Directions”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Video Generation Models in Robotics: Applications, Research Challenges, Future Directions (机器人中的视频生成模型：应用、研究挑战与未来方向)</p>
<p><strong>作者：</strong> Zhiting Mei, Tenny Yin, Ola Shorinwa, Apurva Badithela, Zhonghe Zheng, Joseph Bruno, Madison Bland, Lihan Zha, Asher Hancock, Jaime Fernández Fisac, Philip Dames, Anirudha Majumdar</p>
<p><strong>摘要：</strong></p>
<p>这篇综述论文深入探讨了视频生成模型在机器人领域的应用、面临的研究挑战以及未来的发展方向。作者们认为，视频生成模型作为高保真度的物理世界模型，能够合成捕捉精细交互的视频，为机器人领域带来了革命性的进步，尤其是在解决传统物理模拟器存在的局限性方面。</p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>论文旨在全面回顾视频生成模型在机器人领域的最新进展，重点关注其作为“具身世界模型”（embodied world models）的角色。研究的核心问题在于：</p>
<ul>
<li>视频生成模型如何能够有效地模拟物理世界，并为机器人提供精细、逼真的环境表征？</li>
<li>这些模型在机器人领域的具体应用有哪些，例如数据生成、策略学习、策略评估和视觉规划？</li>
<li>当前视频生成模型在机器人应用中面临哪些关键挑战和局限性？</li>
<li>未来应如何克服这些挑战，推动视频生成模型在机器人领域的更广泛、更可靠的应用？</li>
</ul>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<p>该论文的主要贡献在于其<strong>全面的综述视角</strong>，系统地梳理了视频生成模型在机器人领域的现状。其方法论贡献体现在：</p>
<ul>
<li><strong>分类与梳理：</strong> 将视频生成模型分为隐式和显式世界模型，并详细介绍了其在机器人领域的四类主要应用：<ul>
<li><strong>模仿学习中的数据生成与动作预测：</strong> 利用视频模型生成逼真的专家演示，降低数据收集成本。</li>
<li><strong>强化学习中的动力学与奖励建模：</strong> 利用视频模型提供更精确的动力学预测和奖励信号，提升强化学习效率。</li>
<li><strong>可扩展的策略评估：</strong> 通过模拟真实世界交互，实现高效、可复现的策略评估。</li>
<li><strong>视觉规划：</strong> 利用视频模型生成未来场景预测，指导机器人规划和决策。</li>
</ul>
</li>
<li><strong>架构与技术回顾：</strong> 详细介绍了当前主流的视频生成模型架构，包括基于扩散/流匹配的模型（如 DiTs 和 U-Nets）以及联合嵌入预测架构（JEPAs）。</li>
<li><strong>挑战与未来方向的系统性分析：</strong> 深入剖析了视频模型在机器人应用中遇到的挑战，如<strong>幻觉与物理定律违背、不确定性量化、指令遵循困难、安全内容生成、安全机器人交互、动作估计、长视频生成以及数据策展和训练/推理成本</strong>等。并为每个挑战提出了具体的未来研究方向。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>论文的主要结果体现在对视频生成模型在机器人领域潜力的全面展示和对其发展瓶颈的清晰界定：</p>
<ul>
<li><strong>高保真度世界建模：</strong> 视频生成模型能够生成高度逼真、物理一致的视频，有效弥补了传统物理模拟器在处理复杂动力学（如可变形体模拟）方面的不足，为机器人提供了更可靠的“世界模型”。</li>
<li><strong>赋能机器人学习：</strong> 这些模型极大地促进了机器人学习的进步，使得更高效的数据生成、更鲁棒的策略学习和更可靠的策略评估成为可能。</li>
<li><strong>推动具身智能发展：</strong> 视频生成模型作为具身世界模型，是实现更通用、更智能机器人系统的关键技术之一，能够帮助机器人更好地理解和与物理世界交互。</li>
<li><strong>明确研究方向：</strong> 通过系统地梳理挑战，论文为未来的研究提供了清晰的路线图，指明了需要重点攻克的科学问题。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<p>尽管视频生成模型展现出巨大潜力，但论文也指出了其在机器人应用中面临的显著局限性：</p>
<ul>
<li><strong>幻觉与物理定律违背：</strong> 模型常生成不符合物理现实的视频，如物体消失、变形或违反物理定律。</li>
<li><strong>指令遵循困难：</strong> 模型难以精确理解和执行复杂的语言指令，尤其是在长时序任务中。</li>
<li><strong>不确定性量化不足：</strong> 模型难以表达其预测的不确定性，影响了其在安全关键领域的可靠性。</li>
<li><strong>数据策展、训练和推理成本高昂：</strong> 训练高质量的视频生成模型需要大量数据和计算资源，限制了其可及性。</li>
<li><strong>安全内容生成问题：</strong> 模型可能生成不安全或有害的内容，需要更强的安全防护机制。</li>
<li><strong>长视频生成挑战：</strong> 当前模型生成的视频时长有限，难以满足机器人任务的长期规划需求。</li>
<li><strong>动作估计精度不足：</strong> 从视频中估计机器人动作的精度仍有待提高，尤其是在精细化任务中。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>论文最后提出了多个未来研究方向，以期克服现有挑战并推动视频生成模型在机器人领域的广泛应用：</p>
<ul>
<li><strong>提升物理真实性：</strong> 探索集成物理先验、物理模拟器或利用 LLMs 改进模型，以增强视频的物理一致性。</li>
<li><strong>不确定性量化：</strong> 开发更高效、可解释的不确定性量化方法，使模型能够表达其置信度。</li>
<li><strong>改进指令遵循：</strong> 利用多模态条件、更强的指令理解能力或内在推理机制来提升模型对用户指令的遵循能力。</li>
<li><strong>增强安全性：</strong> 开发更通用、更有效的安全防护机制和安全基准测试，以防止生成不安全内容。</li>
<li><strong>安全机器人交互：</strong> 将视频模型应用于评估和预测机器人动作的安全性，避免碰撞和潜在危险。</li>
<li><strong>高精度动作估计：</strong> 探索更具解释性、可扩展性的动作估计方法，如改进的逆动力学模型和半监督学习。</li>
<li><strong>长视频生成：</strong> 设计更高效的架构和训练策略，以生成更长、更连贯、更具物理一致性的视频。</li>
<li><strong>降低数据成本：</strong> 开发更有效的视频分割、过滤和标注技术，并探索利用 LLMs 进行更可靠的视频描述。</li>
<li><strong>加速训练与推理：</strong> 研究更高效的模型架构、压缩技术和优化算法，以降低训练和推理成本。</li>
<li><strong>机器人中心评估：</strong> 开发针对机器人任务的、多维度的、量化的评估指标和基准测试。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇综述论文为理解视频生成模型在机器人领域的现状、挑战和未来机遇提供了一个全面的框架。它强调了视频生成模型作为具身世界模型的巨大潜力，并为研究人员指明了未来需要重点关注的研究方向，尤其是在提升模型的物理真实性、可靠性和安全性方面。这对于推动机器人技术的发展，特别是实现更智能、更自主的机器人系统具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07823v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07823v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07833v1'></a></p>
<h2 id="tuning-free-visual-effect-transfer-across-videos"><a href="https://arxiv.org/abs/2601.07833v1">Tuning-free Visual Effect Transfer across Videos</a></h2>
<p><strong>Authors:</strong> Maxwell Jones, Rameen Abdal, Or Patashnik, Ruslan Salakhutdinov, Sergey Tulyakov, Jun-Yan Zhu, Kuan-Chieh Jackson Wang</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website <script type="math/tex">\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\ this\ URL}</script>.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Tuning-free Visual Effect Transfer across Videos”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Tuning-free Visual Effect Transfer across Videos (无微调的跨视频视觉效果迁移)</p>
<p><strong>作者：</strong> Maxwell Jones, Rameen Abdal, Or Patashnik, Ruslan Salakhutdinov, Sergey Tulyakov, Jun-Yan Zhu, Kuan-Chieh Jackson Wang</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决现有视频编辑方法在处理<strong>动态、随时间变化的视觉效果</strong>时遇到的困难。这些效果，如动态光照变化、角色变形或复杂的材质转换，难以通过文本提示或静态关键帧来准确描述和迁移。核心挑战在于如何将参考视频中的复杂时间动态无缝地融入到目标视频或图像的内容和运动中，同时保持视觉一致性和时间连贯性。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li><strong>RefVFX 框架：</strong> 提出了一种名为 RefVFX 的新颖框架，能够以<strong>前馈（feed-forward）</strong>的方式，将参考视频中的复杂时间视觉效果迁移到目标视频或图像上。</li>
<li><strong>大规模数据集构建：</strong> 核心贡献之一是构建了一个<strong>大规模、效果对齐的视频三元组数据集</strong>。该数据集包含超过 120,000 个三元组，涵盖了 1,700 多种不同的视觉效果。数据集的生成过程是自动化的，并且能够保留目标视频的运动和结构，同时应用固定的、可重复的效果。数据集的构建结合了三种互补的来源：<ul>
<li><strong>LoRA 驱动的图像到视频（I2V）效果：</strong> 利用预训练的 LoRA 模型生成。</li>
<li><strong>可扩展的视频到视频（V2V）转换：</strong> 通过一个自动化的流水线生成，这是首次实现大规模的 V2V 效果迁移数据生成方法。</li>
<li><strong>程序化生成的合成效果：</strong> 通过代码实现，提供多样化的效果。</li>
</ul>
</li>
<li><strong>多源条件化模型：</strong> RefVFX 模型基于最新的文本到视频（text-to-video）扩散模型（如 Wan [74]），并进行了扩展，使其能够<strong>联合条件化</strong>于三个输入：<ul>
<li><strong>参考视频：</strong> 提供时间视觉效果。</li>
<li><strong>输入图像或视频：</strong> 定义场景内容和运动。</li>
<li><strong>文本提示：</strong> 提供高级语义指导。
这种多源条件化使得模型能够和谐地整合参考视频的时间动态与输入视频的外观和运动。</li>
</ul>
</li>
<li><strong>无微调（Tuning-free）设计：</strong> 论文强调其方法是“无微调”的，意味着在推理时不需要对模型进行额外的优化，从而提高了效率。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>高质量的视觉效果迁移：</strong> RefVFX 能够生成<strong>视觉上一致且时间上连贯</strong>的编辑视频，忠实地捕捉并迁移参考视频中的动态效果。</li>
<li><strong>泛化能力：</strong> 模型在<strong>未见过（unseen）的效果类别</strong>上表现出良好的泛化能力。</li>
<li><strong>优于基线模型：</strong> 在定性、定量和用户偏好研究中，RefVFX 均<strong>显著优于</strong>仅基于文本提示或静态参考的基线模型。用户研究表明，参与者普遍偏好 RefVFX 生成的结果，尤其是在参考视频的遵循度和输入视频的保持度方面。</li>
<li><strong>建立新基准：</strong> 所构建的大规模数据集为未来参考视频效果迁移的研究奠定了新的基准。</li>
<li><strong>可控性：</strong> 论文还展示了如何通过调整不同引导（guidance）的权重，实现对原始视频保持程度和参考效果影响程度的<strong>可控编辑</strong>。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>遮挡和复杂交互：</strong> 模型在处理<strong>精细的遮挡</strong>或<strong>主体间的复杂交互</strong>时可能存在困难，有时会导致部分融合或错位。</li>
<li><strong>数据集偏向：</strong> 数据集主要关注<strong>以人为中心和前景为主</strong>的场景，这可能限制了模型在更广泛的场景下的泛化能力。</li>
<li><strong>推理时间：</strong> 由于需要同时处理输入视频和参考视频的条件化，推理时间大约是单源基线模型的<strong>两倍</strong>。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>改进遮挡和复杂交互的处理：</strong> 进一步提升模型在处理复杂场景下的遮挡和主体间交互的能力。</li>
<li><strong>扩展数据集的多样性：</strong> 增加数据集的场景类型和效果范围，以提高模型的泛化能力。</li>
<li><strong>优化推理效率：</strong> 探索更高效的模型架构或推理策略，以缩短生成时间。</li>
</ul>
<hr />
<p><strong>总结：</strong></p>
<p>这篇论文提出了 RefVFX，一个创新的无微调框架，用于将复杂的动态视觉效果从一个视频迁移到另一个视频或图像。其核心贡献在于构建了一个大规模、多样化的数据集，并设计了一个能够联合处理参考视频、输入视频和文本提示的多源条件化模型。实验结果表明，RefVFX 在生成视觉上一致、时间上连贯的编辑视频方面取得了显著的成功，并且在未见过效果上表现出良好的泛化能力，超越了现有方法。该工作为视频效果迁移领域开辟了新的可能性，并为未来的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner.</li>
<li>Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance.</li>
<li>% To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect.</li>
<li>To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect.</li>
<li>Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones.</li>
<li>Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07833v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07833v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07832v1'></a></p>
<h2 id="mhla-restoring-expressivity-of-linear-attention-via-token-level-multi-head"><a href="https://arxiv.org/abs/2601.07832v1">MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</a></h2>
<p><strong>Authors:</strong> Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head”的全面摘要：</p>
<p><strong>论文题目：</strong> MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</p>
<p><strong>作者：</strong> Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
Transformer 模型在众多领域取得了巨大成功，但其自注意力机制的二次方计算复杂度限制了其在大规模应用中的扩展性。线性注意力作为一种高效的替代方案，虽然降低了计算复杂度，但直接应用往往会导致性能下降，现有修复方法（如引入额外的卷积或门控模块）又会增加计算开销，违背了线性注意力的初衷。论文识别出线性注意力性能下降的关键原因在于“全局上下文塌陷”（global context collapse），即模型丧失了表示多样性。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决全局上下文塌陷问题，论文提出了<strong>多头线性注意力（Multi-Head Linear Attention, MHLA）</strong>。MHLA 的核心思想是将 token 维度进行划分，在每个“头”内独立计算注意力，从而保留了表示多样性。具体来说：
*   <strong>分块处理：</strong> 将输入序列沿着 token 维度划分为多个非重叠的块（“头”）。
*   <strong>局部 KV 摘要：</strong> 为每个块计算局部键值（KV）摘要。
*   <strong>多头混合（Multi-Head Mixing）：</strong> 引入一个可学习的混合系数矩阵，使得每个查询块能够根据自身需求，以查询为条件，对这些局部 KV 摘要进行加权混合，生成一个定制化的上下文表示。
*   <strong>恢复查询条件性：</strong> 通过这种方式，MHLA 恢复了查询条件性（query-conditioned selectivity）和 token 级别的加权能力，而无需引入额外的计算密集型模块。
*   <strong>线性复杂度：</strong> MHLA 在保持线性计算复杂度的同时，显著提升了模型的表达能力。</p>
<p><strong>3. 主要结果与意义：</strong>
MHLA 在多个领域进行了广泛验证，并取得了显著成果：
*   <strong>图像分类：</strong> 在 ImageNet 分类任务上，MHLA 相比于标准自注意力模型提升了 <strong>3.6%</strong> 的准确率。
*   <strong>自然语言处理（NLP）：</strong> 在 NLP 任务上，MHLA 带来了 <strong>6.3%</strong> 的增益。
*   <strong>图像生成：</strong> 在图像生成任务中，MHLA 使 DiT 架构的性能提升了 <strong>12.6%</strong>。
*   <strong>视频生成：</strong> 在视频生成任务中，MHLA 实现了 <strong>41%</strong> 的显著提升，尤其是在处理超长序列时，表现远超 vanilla 线性注意力。
*   <strong>效率：</strong> MHLA 在保持线性复杂度的前提下，恢复了自注意力模型的表达能力，并且在实际应用中展现出良好的性能和效率平衡。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   虽然 MHLA 显著提升了线性注意力的性能，但论文也提到，在某些情况下，当 MHLA 的头数 M 远大于序列长度 N 的平方根时，可能会引入一些额外的计算开销（尽管总体复杂度仍为线性）。
*   在某些实验中，MHLA 结合额外的模块（如 CPE 和 output gating）在小型模型上能带来进一步的性能提升，但这些模块的收益会随着模型规模的增大而减弱，甚至可能导致性能下降，这表明 MHLA 本身具有强大的内在优势。</p>
<p><strong>5. 未来研究方向：</strong>
论文的结论部分展望了 MHLA 作为一种基础性注意力机制的潜力，可以应用于更广泛的下游任务，例如：
*   <strong>高质量图像生成</strong>
*   <strong>长时序视频合成</strong>
*   <strong>大规模语言建模</strong></p>
<p>MHLA 的提出为解决 Transformer 的效率和表达能力之间的权衡问题提供了一个有前景的解决方案，尤其是在处理长序列和大规模数据时，具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07832v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07832v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07812v1'></a></p>
<h2 id="more-images-more-problems-a-controlled-analysis-of-vlm-failure-modes"><a href="https://arxiv.org/abs/2601.07812v1">More Images, More Problems? A Controlled Analysis of VLM Failure Modes</a></h2>
<p><strong>Authors:</strong> Anurag Das, Adrian Bulat, Alberto Baldrati, Ioannis Maniadis Metaxas, Bernt Schiele, Georgios Tzimiropoulos, Brais Martinez</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“More Images, More Problems? A Controlled Analysis of VLM Failure Modes”的全面中文摘要：</p>
<p><strong>论文题目：</strong> More Images, More Problems? A Controlled Analysis of VLM Failure Modes.
<strong>作者：</strong> Anurag Das, Adrian Bulat, Alberto Baldrati, Ioannis Maniadis Metaxas, Bernt Schiele, Georgios Tzimiropoulos, Brais Martinez</p>
<p><strong>摘要：</strong></p>
<p>这篇论文深入探讨了大型视觉语言模型（LVLM）在处理多图像场景时的局限性，并提出了相应的解决方案。尽管LVLM在单图像理解方面取得了显著进展，但它们在整合和推理多个图像信息方面的能力仍未得到充分探索。现有基准测试在评估多图像模型方面尚显不足，缺乏对模型核心弱点及其根源的全面分析。</p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>论文的核心研究问题是：<strong>大型视觉语言模型（LVLM）在理解和推理多图像场景时存在哪些根本性的缺陷，以及这些缺陷的根源是什么？</strong> 作者指出，现有LVLM在以下几个方面表现不佳：
*   <strong>信息聚合困难：</strong> 无法有效地整合来自不同图像的信息。
*   <strong>多概念跟踪能力弱：</strong> 难以同时跟踪或关注多个概念。
*   <strong>对干扰敏感：</strong> 容易受到无关或干扰性图像的影响。
*   <strong>序列长度偏见：</strong> 模型性能随输入图像数量的增加而显著下降，这可能与处理长序列的能力有关。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>为了解决上述问题，作者提出了两项主要创新：</p>
<ul>
<li><strong>MIMIC基准测试：</strong> 作者引入了一个名为MIMIC（Multi-Image Model Insights and Challenges）的新基准测试。MIMIC通过程序化生成多图像序列，并对信息分布、查询复杂度和干扰物等维度进行精确控制，从而能够对LVLM的多图像能力进行严格、细致的诊断性评估。MIMIC包含四个核心任务：计数（Counting）、列表（Listing）、共同（Common）和择一（Odd-One），旨在隔离和分析模型在不同多图像推理方面的能力。</li>
<li><strong>两种互补的改进策略：</strong><ul>
<li><strong>数据驱动策略：</strong> 提出了一种程序化数据生成策略，将单图像标注合成为丰富、有针对性的多图像训练示例，为模型提供更强的跨图像推理监督。</li>
<li><strong>优化驱动策略：</strong> 通过分析模型层级注意力模式，提出了一种针对多图像输入的注意力掩码（attention-masking）方案。该方案限制了模型在特定层级中跨图像的注意力，从而鼓励模型更专注于同一图像内的信息整合，并减轻了序列长度带来的负担。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>诊断性发现：</strong> 通过在MIMIC基准上进行的实验，作者发现当前最先进的LVLM在多图像场景下普遍存在信息聚合和多概念跟踪的困难，并且对视觉干扰物非常敏感。研究还表明，性能下降主要源于处理长序列（即大量图像）的挑战，而非简单地处理多个独立图像。</li>
<li><strong>性能提升：</strong> 作者提出的数据生成策略和注意力掩码策略显著提高了LVLM在多图像场景下的性能。特别是注意力掩码策略，在计算效率上也有显著提升。</li>
<li><strong>状态艺术（SOTA）成果：</strong> 经过微调的模型在现有的多图像基准测试（如MuirBench、Blink等）上取得了新的最先进（SOTA）结果，证明了其有效性。</li>
<li><strong>对未来研究的启示：</strong> 研究结果表明，未来的LVLM研究应更加关注多图像信息整合、长序列处理能力以及对干扰物的鲁棒性。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>基准领域限制：</strong> MIMIC基准主要基于MS-COCO数据集构建，以精确控制变量。虽然这有利于“单元测试”式的模型推理分析，但将其扩展到更专业的领域（如密集文档或医学影像）仍需进一步研究。</li>
<li><strong>分辨率权衡：</strong> 论文发现缩短序列长度可以提高性能，但对于需要像素级精确感知极小细节的任务，可能需要自适应分辨率策略，这超出了本研究的范围。</li>
<li><strong>模型范围：</strong> 分析主要集中在开源模型上。虽然作者认为结论可能适用于闭源模型，但需要额外的验证。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li>将MIMIC基准扩展到更专业的领域，如密集文档或医学影像。</li>
<li>探索自适应分辨率策略，以解决需要像素级精确感知的问题。</li>
<li>对闭源模型进行验证，以确认研究结论的普适性。</li>
<li>进一步研究模型在处理极长序列时的内在机制和优化方法。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文对LVLM在多图像理解方面的挑战进行了系统性的分析，并提出了创新的MIMIC基准测试和有效的改进策略。研究揭示了当前LVLM在信息聚合、多概念跟踪和序列长度处理方面的关键弱点，并证明了其提出的方法能够显著提升模型性能，为未来多图像视觉语言模型的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously.</li>
<li>To address these failures, we propose two novel complementary remedies.</li>
<li>On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07812v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07812v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07779v1'></a></p>
<h2 id="os-symphony-a-holistic-framework-for-robust-and-generalist-computer-using-agent"><a href="https://arxiv.org/abs/2601.07779v1">OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent</a></h2>
<p><strong>Authors:</strong> Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, JingJing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.MA, cs.AI, cs.CL, cs.CV, cs.HC</p>
<p><strong>Abstract:</strong></p>
<p>While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent”的全面中文摘要：</p>
<p><strong>论文摘要：OS-Symphony：一个用于鲁棒和通用计算机使用代理的整体框架</strong></p>
<p><strong>1. 研究问题与挑战</strong></p>
<p>该论文主要关注当前计算机使用代理（Computer-Using Agents, CUAs）在处理长时序任务和泛化到新领域时所面临的挑战。尽管视觉语言模型（VLMs）的进步显著推动了CUAs的发展，但现有框架在以下两个方面存在局限性：</p>
<ul>
<li><strong>历史视觉上下文的粒度控制不足：</strong> 现有机制难以有效地管理和筛选长时序任务中的历史视觉信息，导致代理容易丢失关键上下文，影响决策和纠错能力。</li>
<li><strong>缺乏视觉感知能力的教程检索：</strong> 现有方法在检索教程时，往往过度依赖文本信息，忽略了视觉线索，难以在未见过或复杂场景下生成高保真度的教程。</li>
</ul>
<p>这些局限性阻碍了CUAs实现鲁棒性和通用性，限制了其在实际应用中的潜力。</p>
<p><strong>2. 主要创新与方法论贡献</strong></p>
<p>为了解决上述问题，论文提出了<strong>OS-Symphony</strong>，一个整体性的CUA框架，其核心创新在于引入了两个关键组件：</p>
<ul>
<li>
<p><strong>反射-记忆代理（Reflection-Memory Agent, RMA）：</strong></p>
<ul>
<li><strong>里程碑驱动的长时记忆：</strong> RMA通过选择性地存储关键的“里程碑”截图和抽象轨迹，构建了长时记忆，有效缓解了长时序任务中的视觉上下文丢失问题。</li>
<li><strong>轨迹级别自我纠错：</strong> 基于里程碑记忆，RMA生成详细的轨迹级别反思（reflection），并通过结构化的消息协议（Message Protocol）提供给Orchestrator，实现对代理行为的有效纠错，例如识别意图漂移、循环行为等。</li>
<li><strong>辅助检测方法：</strong> 论文详细介绍了用于RMA的两个辅助检测方法：<strong>Step Summary</strong>（用于单步操作的正确性验证）和<strong>Loop Detection</strong>（用于识别重复或循环行为）。</li>
</ul>
</li>
<li>
<p><strong>多功能工具代理（Versatile Tool Agents）：</strong></p>
<ul>
<li><strong>多模态搜索器（Multimodal Searcher）：</strong> 这是一个创新的“视觉中心搜索即工具”（Visual-Centric Search as a Tool）范式。它采用“看-做”（See-Act）策略，在一个独立的浏览器沙箱环境中自主导航网页，并结合视觉信息和空间布局来合成实时、视觉对齐的教程。这解决了传统RAG方法在视觉信息处理上的不足，并能生成适用于未见过场景的教程。</li>
<li><strong>其他工具代理：</strong> 除了搜索器，框架还包含<strong>Grounder</strong>（用于精确的UI元素定位）和<strong>Coder</strong>（用于执行代码和文件操作），它们协同工作以高效地完成复杂任务。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<p>OS-Symphony在多个基准测试中展现了卓越的性能：</p>
<ul>
<li><strong>OSWorld：</strong> 在OSWorld基准上，OS-Symphony取得了新的SOTA（State-of-the-Art）结果，使用GPT-5模型在100步限制下达到了<strong>65.84%</strong>的成功率，比主要基线Agent S3高出约3%。在Workflow领域，其优势更为明显，提升了7%。</li>
<li><strong>WindowsAgentArena和MacOSArena：</strong> 在这两个跨平台基准上，OS-Symphony也取得了显著的性能提升，尤其是在MacOSArena上，即使使用较小的模型，也能实现大幅超越。</li>
<li><strong>模型规模和成本效益：</strong> 实验表明，OS-Symphony在不同模型规模下都能有效提升性能，并且与更强大的模型相比，使用GPT-5-Mini等成本效益更高的模型也能取得有竞争力的结果，显示了其强大的通用性和成本效益。</li>
<li><strong>对开源VLMs的赋能：</strong> 该框架能够显著提升开源VLMs在长时序和未见过任务上的能力，降低了实现高级CUA的门槛。</li>
</ul>
<p>这些结果证明了OS-Symphony在提高CUAs的鲁棒性、泛化能力和效率方面的有效性。</p>
<p><strong>4. 局限性</strong></p>
<p>论文也坦诚地指出了OS-Symphony的局限性：</p>
<ul>
<li><strong>环境泛化性：</strong> 当前评估主要集中在桌面环境，其在移动平台（如Android、iOS）上的适应性尚未验证，需要针对移动端进行不同的动作空间适配。</li>
<li><strong>结构复杂性与效率：</strong> 多代理系统引入了固有的开销，导致执行速度比人类慢几个数量级，目前尚不支持实时部署。</li>
<li><strong>视觉感知粒度：</strong> RMA在处理细微的视觉线索（如高亮或重叠窗口）时仍可能出现困难，导致误报。</li>
<li><strong>指令模糊性与评估指标：</strong> 某些任务失败是由于指令本身模糊或评估函数过于严格，而非代理能力不足。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<p>基于上述局限性，论文提出了未来的研究方向：</p>
<ul>
<li><strong>跨平台通用性：</strong> 探索在移动平台上的适配和通用性。</li>
<li><strong>效率优化：</strong> 改进多代理系统的通信和执行效率，实现实时部署。</li>
<li><strong>视觉感知增强：</strong> 通过更精细的提示工程或图像后处理技术，提升代理对细微视觉线索的理解能力。</li>
<li><strong>指令与评估的改进：</strong> 探索更鲁棒的指令理解和更灵活的评估机制。</li>
<li><strong>“安全即设计”原则：</strong> 在开发更强大的CUAs的同时，必须同步发展安全和隐私保护机制，确保代理的可靠性、透明度和伦理合规性。</li>
<li><strong>混合范式：</strong> 探索结合端到端CUA和模块化代理的混合范式，以突破当前框架的瓶颈。</li>
</ul>
<p><strong>总结</strong></p>
<p>OS-Symphony通过引入创新的<strong>反射-记忆代理（RMA）</strong>和<strong>多模态搜索器</strong>，有效解决了现有计算机使用代理在长时序任务中的上下文丢失和在新场景下的教程生成问题。该框架在多个基准测试中取得了SOTA性能，并证明了其在不同模型规模下的鲁棒性和通用性。尽管存在一些局限性，OS-Symphony为构建更强大、更可靠的未来计算机使用代理提供了坚实的基础和有价值的见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains.</li>
<li>To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios.</li>
<li>Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07779v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07779v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07773v1'></a></p>
<h2 id="beyond-external-guidance-unleashing-the-semantic-richness-inside-diffusion-transformers-for-improved-training"><a href="https://arxiv.org/abs/2601.07773v1">Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training</a></h2>
<p><strong>Authors:</strong> Lingchen Sun, Rongyuan Wu, Zhengqiang Zhang, Ruibin Li, Yujing Sun, Shuaizheng Liu, Lei Zhang</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent works such as REPA have shown that guiding diffusion models with external semantic features (e.g., DINO) can significantly accelerate the training of diffusion transformers (DiTs). However, this requires the use of pretrained external networks, introducing additional dependencies and reducing flexibility. In this work, we argue that DiTs actually have the power to guide the training of themselves, and propose \textbf{Self-Transcendence}, a simple yet effective method that achieves fast convergence using internal feature supervision only. It is found that the slow convergence in DiT training primarily stems from the difficulty of representation learning in shallow layers. To address this, we initially train the DiT model by aligning its shallow features with the latent representations from the pretrained VAE for a short phase (e.g., 40 epochs), then apply classifier-free guidance to the intermediate features, enhancing their discriminative capability and semantic expressiveness. These enriched internal features, learned entirely within the model, are used as supervision signals to guide a new DiT training. Compared to existing self-contained methods, our approach brings a significant performance boost. It can even surpass REPA in terms of generation quality and convergence speed, but without the need for any external pretrained models. Our method is not only more flexible for different backbones but also has the potential to be adopted for a wider range of diffusion-based generative tasks. The source code of our method can be found at https://github.com/csslc/Self-Transcendence.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并根据您的要求提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本研究提出了一种名为“Self-Transcendence”的新颖方法，旨在解决Diffusion Transformers (DiTs) 训练收敛速度慢的问题。该方法通过利用模型自身的内部特征进行监督，无需依赖外部预训练模型，实现了更快的训练收敛和更高的生成质量，甚至在某些方面超越了依赖外部指导的方法。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>内部特征监督（Internal Feature Supervision）</strong>: 这是该方法的核心创新。与依赖外部语义特征（如DINO）的REPA等方法不同，Self-Transcendence完全利用DiT模型自身的中间层特征作为监督信号。</li>
<li><strong>两阶段训练策略</strong>:<ul>
<li><strong>浅层特征对齐</strong>: 在训练初期，将DiT模型的浅层特征与预训练VAE的潜在表示对齐，以解决浅层表示学习困难的问题。这一阶段持续时间较短（例如40个epoch）。</li>
<li><strong>中间特征增强与引导</strong>: 在此之后，对中间特征应用Classifier-Free Guidance (CFG)，以增强其判别能力和语义表达能力。这些增强后的内部特征随后被用作监督信号来指导新的DiT训练。</li>
</ul>
</li>
<li><strong>解决浅层表示学习瓶颈</strong>: 研究者明确指出，DiT训练收敛慢的主要原因是浅层表示学习的困难，并针对性地提出了浅层特征对齐的解决方案。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>加速DiT训练</strong>: 该方法有望显著缩短DiT模型的训练时间，降低研究和应用的门槛。</li>
<li><strong>提高模型灵活性和可移植性</strong>: 摆脱对外部预训练模型的依赖，使得DiT模型可以更灵活地应用于不同的骨干网络，并且更容易在资源受限的环境中部署。</li>
<li><strong>提升生成质量</strong>: 论文声称其方法在生成质量上可以超越REPA，这意味着在保持高效训练的同时，也能获得更优的生成结果。</li>
<li><strong>推动自监督/无监督学习在生成模型中的应用</strong>: 该研究展示了仅通过内部信号进行有效训练的可能性，为未来更纯粹的自监督或无监督生成模型研究提供了新的思路。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>图像生成</strong>: 这是最直接的应用领域，包括文本到图像生成、图像编辑、图像修复等。</li>
<li><strong>视频生成</strong>: DiTs在视频生成领域也展现出潜力，该方法同样适用于加速视频生成模型的训练。</li>
<li><strong>三维内容生成</strong>: 扩散模型在三维形状和场景生成方面也有应用，该方法有望提升其训练效率。</li>
<li><strong>其他基于扩散模型的生成任务</strong>: 任何使用扩散模型进行生成任务的研究和应用，如音频生成、分子设计等，都可能从该方法中受益。</li>
<li><strong>模型压缩与高效训练</strong>: 对于需要快速迭代和部署的模型，该方法提供的加速训练能力尤为重要。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>对VAE的依赖（初期阶段）</strong>: 虽然最终目标是完全摆脱外部模型，但在初期阶段，该方法仍然依赖于一个预训练的VAE。VAE的质量和特性可能会影响DiT的初始学习效果。</li>
<li><strong>超参数敏感性</strong>: Classifier-Free Guidance和特征对齐阶段的 epoch 数等超参数的设置可能对最终性能有显著影响，需要仔细调优。</li>
<li><strong>“简单 yet effective”的定义</strong>: 尽管作者声称方法简单有效，但“简单”的程度需要通过阅读完整论文来评估，其实现复杂度可能仍然高于一些基础的DiT训练方法。</li>
<li><strong>通用性验证</strong>: 摘要中提到“potential to be adopted for a wider range of diffusion-based generative tasks”，这暗示了其在所有扩散模型任务上的通用性可能还需要进一步的广泛验证。</li>
<li><strong>理论解释的深度</strong>: 摘要主要关注方法论和实验结果，关于“为什么”浅层特征对齐和CFG对中间特征有效，以及其背后的理论解释，可能需要在论文正文中深入探讨。</li>
</ul>
<p>总而言之，这篇论文提出的“Self-Transcendence”方法通过巧妙地利用模型自身的内部信息进行监督，为解决DiT训练效率低下这一关键问题提供了一个有前景的解决方案。其最大的亮点在于摆脱了对外部预训练模型的依赖，从而带来了更高的灵活性和潜在的性能优势，这对于推动扩散模型在各个领域的应用具有重要的意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>These enriched internal features, learned entirely within the model, are used as supervision signals to guide a new DiT training.</li>
<li>Compared to existing self-contained methods, our approach brings a significant performance boost.</li>
<li>Our method is not only more flexible for different backbones but also has the potential to be adopted for a wider range of diffusion-based generative tasks.</li>
<li>The source code of our method can be found at https://github.com/csslc/Self-Transcendence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07773v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07773v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07761v1'></a></p>
<h2 id="video-evidence-to-reasoning-efficient-video-understanding-via-explicit-evidence-grounding"><a href="https://arxiv.org/abs/2601.07761v1">Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding</a></h2>
<p><strong>Authors:</strong> Yanxiang Huang, Guohua Gao, Zhaoyang Wei, Jianyuan Ni</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video Evidence to Reasoning: Efficient Video Understanding via Explicit Evidence Grounding”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Video Evidence to Reasoning: Efficient Video Understanding via Explicit Evidence Grounding (视频证据到推理：通过显式证据关联实现高效视频理解)</p>
<p><strong>作者：</strong> Yanxiang Huang, Guohua Gao, Zhaoyang Wei, Jianyuan Ni</p>
<p><strong>摘要：</strong></p>
<p>这篇论文解决了大型视觉语言模型（LVLMs）在视频推理中面临的一个核心困境：如何在保证推理准确性的同时，避免因冗长推理带来的高昂计算成本，以及如何克服高效但容易产生幻觉的非关联性方法。作者提出了一个名为“Chain of Evidence (CoE)”的新型框架，该框架通过架构上的解耦和联合优化，实现了感知关联（perceptual grounding）与推理效率的平衡。</p>
<p><strong>1. 主要研究问题：</strong></p>
<p>LVLMs在视频推理中存在一个根本性的权衡：
*   <strong>高计算成本与准确性：</strong> 详细的推理过程（如思维链 CoT）能提高准确性，但会消耗大量计算资源，不适合实际应用。
*   <strong>效率与幻觉风险：</strong> 追求效率的方法可能导致推理过程脱离视觉证据，产生事实性错误（幻觉）。</p>
<p>论文旨在解决如何在视频理解中实现既高效又准确、且推理过程可追溯的推理。</p>
<p><strong>2. 关键创新与方法贡献：</strong></p>
<ul>
<li><strong>Chain of Evidence (CoE) 框架：</strong> 提出了一种新颖的视频推理范式，它将感知关联与推理效率解耦并联合优化。CoE 包含两个核心创新：<ul>
<li><strong>轻量级证据关联模块 (Evidence Grounding Module, EGM)：</strong> 这是一个查询引导的过滤器，能够动态地识别和提取视频中与用户查询相关的、高保真度的视觉证据。它通过一个浅层的 M 层交叉注意力网络实现，将原始视频帧特征压缩成一个紧凑的、与查询相关的证据特征序列。</li>
<li><strong>证据关联协议 (Evidence-Anchoring Protocol)：</strong> 该协议通过强化学习进行优化，强制模型在推理过程中严格引用识别出的时间锚点，从而显著减少幻觉。它包含三个步骤：显式关联（Explicit Anchoring）、证据交错推导（Evidence-Interleaved Deduction）和结论（Conclusion）。</li>
</ul>
</li>
<li><strong>CoE-Instruct 数据集：</strong> 构建了一个大规模（164k 样本）的数据集，采用新颖的双重标注模式，为感知关联和推理分别提供独立的监督信号。</li>
<li><strong>解耦联合训练策略：</strong> 采用多任务训练策略，包括监督微调（SFT）和强化学习（RL）阶段。SFT 阶段使用 CoE-Instruct-SFT 数据集，分别监督 EGM 的证据定位和 LLM 的推理过程。RL 阶段使用 CoE-Instruct-RL 数据集，通过一个复合奖励机制来优化模型的推理策略，该机制同时考虑答案准确性、证据关联性和推理过程的连贯性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 在五个具有挑战性的视频理解基准测试（包括 Video-MME, MVBench, VSI-Bench）上，CoE 增强的模型取得了新的 SOTA 性能。</li>
<li><strong>准确性显著提高：</strong> CoE 模型在准确性上显著优于现有方法。</li>
<li><strong>效率提升：</strong> 在实现高准确性的同时，显著减少了 token 使用量和推理延迟。</li>
<li><strong>可解释性增强：</strong> CoE 框架提供了多层次的可解释性，包括感知层面的帧重要性评分可视化，以及逻辑层面的推理草稿，使得模型的决策过程更加透明。</li>
<li><strong>鲁棒性：</strong> CoE 模型在处理不同长度的视频时表现出更强的鲁棒性，即使在长视频中也能保持较高的性能。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li>论文主要在 InternVL 模型上进行了实验，虽然其性能强大，但对其他架构的适用性有待进一步探索。</li>
<li>虽然 CoE 显著提高了效率，但与非常简单的非推理模型相比，仍可能存在一定的计算开销。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li>将 CoE 范式推广到其他视觉语言模型架构和多模态领域。</li>
<li>进一步探索更高效的 EGM 设计和训练方法。</li>
<li>研究如何将 CoE 的显式证据关联机制应用于更复杂的推理任务，如多模态对话和长视频理解。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文提出的 CoE 框架通过将视频推理分解为“证据关联”和“推理”两个阶段，并设计了相应的模块（EGM）和训练策略（CoE-Instruct 数据集、解耦训练、RL 优化），成功地解决了 LVLMs 在视频推理中的效率与准确性之间的矛盾。其核心贡献在于通过显式地将推理过程与视觉证据关联起来，不仅提高了模型的准确性和效率，还增强了模型的可解释性，为实现更可靠、更强大的视频理解系统提供了一个有力的范式。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency.</li>
<li>To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision.</li>
<li>Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07761v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07761v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07737v1'></a></p>
<h2 id="evaluating-the-encoding-competence-of-visual-language-models-using-uncommon-actions"><a href="https://arxiv.org/abs/2601.07737v1">Evaluating the encoding competence of visual language models using uncommon actions</a></h2>
<p><strong>Authors:</strong> Chen Ling, Nai Ding</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model's competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Evaluating the encoding competence of visual language models using uncommon actions”的全面中文摘要，其中包含了您要求的五个方面：</p>
<p><strong>论文题目：</strong> Evaluating the encoding competence of visual language models using uncommon actions
<strong>作者：</strong> Chen Ling, Nai Ding</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前主流的视觉语言模型（VLMs）在理解和推理视觉场景中的语义信息方面取得了显著进展，尤其是在处理常见的、符合统计规律的图像-文本对时表现出色。然而，当面对语义上不合常理、违反常识或逻辑颠倒的“非常规”动作场景时，这些模型的理解能力会急剧下降。论文的核心研究问题在于：<strong>如何系统地评估和提升VLMs在理解和推理这些非常规动作场景中的语义能力，特别是它们对主体-客体关系和物理可行性的深层理解能力。</strong></p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>提出UAIT数据集：</strong> 论文的核心贡献是构建了一个名为UAIT（Uncommon-sense Action Image-Text）的新型评估基准。该数据集专门设计用于测试VLMs在非常规动作场景下的语义理解能力，其特点是包含语法正确但语义上反常识的图像-文本对。
*   <strong>半自动化合成方法：</strong> 为了构建高质量的非常规场景数据，论文提出了一种半自动化的合成流程，结合了大型语言模型（LLMs）、少样本提示工程（few-shot prompt engineering）和文本到图像生成技术（如Stable Diffusion）。这种方法有效避免了数据稀缺和版权问题。
*   <strong>精细化评估机制：</strong> UAIT数据集中的每个图像都配有精心设计的选择题，旨在测试模型在细粒度推理方面的能力，特别是对动作角色动态（主体-客体关系）的理解。
*   <strong>关注语义角色反转：</strong> 论文特别强调了对“语义角色反转”（例如，主体和客体互换导致意义完全改变）的关注，这是一种被忽视但至关重要的能力，能够更深入地诊断VLMs的理解局限性。
*   <strong>引入人类基准：</strong> 为了提供一个可靠的比较基准，论文邀请了两位未受过专业标注训练的参与者对数据集进行评估，以衡量人类在非常规场景下的表现。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>模型表现远低于人类：</strong> 实验结果表明，在处理非常规动作场景的语义判断任务时，所有评估的VLMs（包括先进模型和基于对比学习的模型）的表现都显著低于人类水平。尤其是在区分语法正确性与语义合理性方面，模型存在巨大差距。
*   <strong>揭示模型局限性：</strong> 研究结果清晰地揭示了当前VLMs在理解和推理非常规语义场景方面的关键弱点，它们过度依赖训练数据中的统计规律和常见模式，而缺乏对深层语义逻辑和因果关系的真正理解。
*   <strong>微调的潜力：</strong> 实验还表明，即使是轻量级模型，通过针对性地微调（如使用LoRA），其在理解复杂语义场景方面的能力也能得到显著提升，这为资源受限环境下的模型改进提供了重要启示。
*   <strong>诊断工具和研究方向：</strong> UAIT数据集和实验结果为理解VLMs的局限性提供了诊断工具，并为开发具有真实视觉语义推理能力的鲁棒模型指明了研究方向。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>模型对统计偏差的依赖：</strong> 论文指出，当前模型在面对非常规场景时，容易受到训练数据中固有统计偏差的影响，例如“老虎追逐兔子”比“兔子追逐老虎”更常见，这可能导致模型做出错误的判断。
*   <strong>语义理解的深度不足：</strong> 模型对视觉细节的理解仍然停留在表面，难以准确解析动态和逻辑关系，尤其是在处理主体-客体角色转换时表现乏力。
*   <strong>微调的依赖性：</strong> 虽然微调能提升模型性能，但其效果很大程度上依赖于数据构建策略，并且无法从根本上解决模型在“超越常识”的图形推理任务中泛化能力不足的问题。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展UAIT数据集的规模和多样性：</strong> 增加数据集的语义广度和文化多样性，通过大规模自动生成和人工筛选来构建更大规模的基准集，提高统计稳定性和挑战性。
*   <strong>构建多维度评估体系：</strong> 引入更丰富的评估形式，如开放式回答、图文生成匹配评估、动作语义定位等，构建更全面的评估矩阵，不仅判断选择是否正确，还关注“为何正确”和“是否可解释”。
*   <strong>引入更多样的反常识场景：</strong> 探索更多类型的反常识逻辑，如物理反常识场景，以从不同层面挑战模型的常识系统、因果推理和多模态融合能力。
*   <strong>整合多语言语境：</strong> 研究构建UAIT的多语言版本，引入中文等语言，并进行跨文化模型在不同文化背景下的“非常规”判断能力比较研究，以检验模型的普适性并解决AI的本地化部署问题。
*   <strong>构建训练和增强机制：</strong> 探索基于UAIT的训练方法，如构建反常识语料库增强模块，引入角色逻辑对抗学习策略，帮助模型从“统计理解”向“因果推理”过渡，特别是在参数规模或资源有限的情况下，这种定向训练策略有望显著提升模型的表现和可靠性。</p>
<p><strong>论文的独特性和重要性：</strong>
这篇论文的重要性和新颖性在于，它<strong>首次</strong>系统地构建了一个专门针对“非常规”动作场景的评估基准（UAIT），并深入分析了当前主流视觉语言模型在处理这类挑战性任务时的核心缺陷。通过引入反常识的图像-文本对和精细化的评估方法，论文不仅揭示了模型在语义角色理解、因果推理和常识判断方面的不足，还为推动多模态模型从“表面匹配”走向“深度理解”提供了重要的理论和实践指导，为未来开发更具鲁棒性和人类水平推理能力的AI系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes.</li>
<li>We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07737v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07737v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07695v1'></a></p>
<h2 id="smooth-operator-smooth-verifiable-reward-activates-spatial-reasoning-ability-of-vision-language-model"><a href="https://arxiv.org/abs/2601.07695v1">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</a></h2>
<p><strong>Authors:</strong> Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes "near-miss" samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model (平滑算子：平滑可验证奖励激活视觉-语言模型空间推理能力)</p>
<p><strong>作者：</strong> Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</p>
<hr />
<p><strong>全面摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文聚焦于视觉-语言模型（VLMs）在三维（3D）场景理解中面临的一个关键瓶颈：<strong>精确的数值预测能力不足</strong>。现有的基于相对排序的强化学习（RL）方法，如标准GRPO（Group Relative Policy Optimization），在处理3D场景理解任务时存在<strong>奖励稀疏性</strong>和<strong>梯度不稳定性</strong>的问题。特别地，这些方法在处理“接近但未完全正确”（near-miss）的样本时，由于相对归一化机制，会导致<strong>优势（advantage）坍塌</strong>，使得大量有价值的边界样本被浪费，从而造成了严重的数据利用效率低下。这阻碍了VLMs有效利用3D物理约束提供的可验证信号。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>为了解决上述问题，作者提出了两项核心创新：</p>
<ul>
<li><strong>平滑数值奖励激活（SNRA）算子：</strong> SNRA算子通过一个动态参数化的Sigmoid函数，将原始的、可能稀疏或离散的反馈信号（如平方误差或离散评分）转化为一个<strong>密集、连续的奖励区间 [0, 1]</strong>。这使得奖励信号更加平滑，梯度更加稳定，尤其是在训练早期，鼓励模型进行探索。SNRA算子中的“锐度”（sharpness）参数 <code>k</code> 可以动态调整，在训练初期鼓励探索，在后期强制模型进行精确对齐。</li>
<li><strong>绝对值保持GRPO（AP-GRPO）框架：</strong> AP-GRPO是对标准GRPO的改进，它引入了<strong>绝对值标量梯度</strong>来缓解传统相对排序机制中数值信息丢失的问题。具体而言，AP-GRPO将原始奖励 <code>r</code> 的绝对值信息（通过 <code>r^α</code> 形式）整合到优势函数的计算中。这使得模型在保留GRPO的组内相对排序优势的同时，能够更好地利用绝对奖励信息，从而锚定优势函数在一个物理上有意义的尺度上，避免了“near-miss”样本的优势坍塌。</li>
</ul>
<p>此外，作者还构建了一个名为<strong>Numerical3D-50k</strong>的数据集，包含约50,000个可验证的3D子任务，用于训练和评估模型。</p>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> AP-GRPO框架在多个3D空间推理基准测试中取得了与大型监督方法相当的性能，同时<strong>数据效率更高</strong>。</li>
<li><strong>数据效率：</strong> 实验表明，AP-GRPO在仅使用50K目标样本的情况下，取得了与使用数百万样本的基线方法（如VST）相当甚至略优的性能，显著提高了数据利用效率。这得益于SNRA算子和AP-GRPO能够有效利用“near-miss”样本。</li>
<li><strong>激活3D推理能力：</strong> 该方法能够有效激活VLMs中潜藏的3D空间推理能力，而<strong>无需进行架构修改</strong>，仅通过改进奖励信号和优化框架实现。</li>
<li><strong>鲁棒性与稳定性：</strong> 通过动态锐度调度（Dynamic Sharpness Scheduling），模型能够从早期的大范围探索平稳过渡到后期的高精度对齐，有效解决了梯度消失和精度瓶颈问题。AP-GRPO在稀疏奖励和高精度场景下均表现出更好的方差抑制和收敛性。</li>
<li><strong>数据集贡献：</strong> Numerical3D-50k数据集为3D空间理解研究提供了一个有价值的资源。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li>论文中提到，在某些情况下，过大的 <code>kmax</code> 值（终端锐度）可能导致奖励稀疏和高方差梯度，表明 <code>kmax</code> 的选择需要仔细权衡。</li>
<li>虽然AP-GRPO在数据效率上表现出色，但其在<strong>百万级规模的监督预训练</strong>基线面前仍需进一步提升。</li>
<li>论文主要关注数值预测和空间推理，对于更复杂的3D场景理解任务（如场景重建、交互等）的全面性可能仍需进一步探索。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更复杂的3D任务：</strong> 将AP-GRPO框架扩展到更广泛的3D任务，如3D场景生成、交互式3D理解等。</li>
<li><strong>更精细的奖励设计：</strong> 探索更先进的奖励函数设计，以进一步提升模型在复杂3D场景中的表现。</li>
<li><strong>跨模态融合：</strong> 结合其他模态信息（如点云、深度图）来增强3D空间推理能力。</li>
<li><strong>模型架构的协同优化：</strong> 虽然本文强调无需架构修改，但未来可以探索SNRA和AP-GRPO与特定3D感知架构的协同优化，以期获得更大的性能飞跃。</li>
<li><strong>更具挑战性的数据集：</strong> 构建更大、更多样化、更具挑战性的3D空间理解数据集，以推动领域发展。</li>
</ul>
<p><strong>总结：</strong></p>
<p>“Smooth Operator”论文提出了一种新颖的强化学习框架（AP-GRPO）和奖励激活算子（SNRA），有效解决了视觉-语言模型在3D场景理解中数值预测能力不足的问题。通过将SNRA的平滑、连续奖励与AP-GRPO的绝对值保持机制相结合，该方法显著提高了数据效率和训练稳定性，并激活了模型潜在的3D空间推理能力，而无需改变模型架构。这项工作为提升VLMs在3D领域的感知和推理能力提供了一种有效且实用的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07695v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07695v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.07692v1'></a></p>
<h2 id="leveraging-3d-representation-alignment-and-rgb-pretrained-priors-for-lidar-scene-generation"><a href="https://arxiv.org/abs/2601.07692v1">Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation</a></h2>
<p><strong>Authors:</strong> Nicolas Sereyjol-Garros, Ellington Kirby, Victor Besnier, Nermin Samet</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>LiDAR scene synthesis is an emerging solution to scarcity in 3D data for robotic tasks such as autonomous driving. Recent approaches employ diffusion or flow matching models to generate realistic scenes, but 3D data remains limited compared to RGB datasets with millions of samples. We introduce R3DPA, the first LiDAR scene generation method to unlock image-pretrained priors for LiDAR point clouds, and leverage self-supervised 3D representations for state-of-the-art results. Specifically, we (i) align intermediate features of our generative model with self-supervised 3D features, which substantially improves generation quality; (ii) transfer knowledge from large-scale image-pretrained generative models to LiDAR generation, mitigating limited LiDAR datasets; and (iii) enable point cloud control at inference for object inpainting and scene mixing with solely an unconditional model. On the KITTI-360 benchmark R3DPA achieves state of the art performance. Code and pretrained models are available at https://github.com/valeoai/R3DPA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面中文摘要。</p>
<p><strong>论文题目：</strong> Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation</p>
<p><strong>作者：</strong> Nicolas Sereyjol-Garros, Ellington Kirby, Victor Besnier, Nermin Samet</p>
<p><strong>摘要：</strong></p>
<p>这篇论文<strong>“Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation”</strong>（利用3D表示对齐和RGB预训练先验进行LiDAR场景生成）旨在解决<strong>3D LiDAR场景生成中数据稀缺的问题</strong>，并<strong>提升生成场景的真实感和质量</strong>。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<ul>
<li><strong>3D LiDAR数据稀缺：</strong> 与拥有数百万样本的RGB数据集相比，用于机器人任务（如自动驾驶）的3D LiDAR数据集规模有限，这阻碍了真实感LiDAR场景生成模型的发展。</li>
<li><strong>跨模态知识迁移困难：</strong> 如何有效地将从大规模RGB图像数据中学到的知识迁移到LiDAR场景生成任务中，以弥补LiDAR数据的不足，是一个关键挑战。</li>
<li><strong>提升生成质量：</strong> 现有的LiDAR场景生成方法通常从头开始训练，未能充分利用强大的自监督3D表示，导致生成质量有待提高。</li>
</ul>
<p><strong>2. 主要创新/方法贡献：</strong></p>
<p>该论文提出了<strong>R3DPA</strong>（<strong>R</strong>GB-<strong>3D</strong> <strong>P</strong>retrained <strong>A</strong>lignment），这是<strong>首个</strong>能够解锁<strong>RGB预训练先验</strong>并结合<strong>自监督3D表示</strong>用于LiDAR场景生成的模型。其核心创新点包括：</p>
<ul>
<li><strong>RGB预训练先验的迁移：</strong> R3DPA首次将预训练的自然图像流匹配（Flow Matching, FM）模型的权重迁移到LiDAR场景生成任务中。通过一种创新的<strong>VAE对齐（VAE Alignment）训练策略</strong>，解决了跨模态域迁移带来的性能衰减问题，有效地利用了大规模RGB图像数据中的丰富先验知识。</li>
<li><strong>3D表示对齐（3D Representation Alignment）：</strong> 模型将生成器（FM模型）的中间特征与从预训练的3D骨干网络提取的自监督3D特征进行对齐。这种对齐机制显著提高了生成质量，并使得模型能够理解和利用3D场景的结构信息。</li>
<li><strong>端到端联合训练（End-to-End Training）：</strong> 论文提出了一种端到端的训练框架，联合优化VAE（变分自编码器）和FM模型。通过在训练过程中引入3D表示对齐损失，不仅指导了FM模型的生成过程，还反向传播梯度以优化VAE的编码器，从而实现更具表现力的潜在空间。</li>
<li><strong>可控的场景编辑能力：</strong> 由于模型在训练过程中与3D表示对齐，这使得在推理时能够实现<strong>免费的场景编辑</strong>，例如<strong>物体修复（Object Inpainting）</strong>和<strong>场景混合（Scene Mixing）</strong>，仅需一个无条件的模型即可实现。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>SOTA性能：</strong> 在KITTI-360基准测试上，R3DPA取得了<strong>最先进（State-of-the-Art）的性能</strong>，在多个评估指标上显著优于现有方法，例如在FLD（Fréchet Localization Distance）上取得了显著提升，在点云指标上也表现出色。</li>
<li><strong>提升生成质量：</strong> 通过结合RGB预训练先验和3D表示对齐，R3DPA能够生成<strong>更高质量、更具多样性和真实感</strong>的LiDAR点云场景。</li>
<li><strong>数据效率：</strong> 有效地利用了大规模RGB数据，缓解了LiDAR数据稀缺的问题，为未来研究提供了新的方向。</li>
<li><strong>可控生成：</strong> 实现了推理时的场景编辑能力，为自动驾驶等应用场景提供了更灵活的工具。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中并未明确列出具体的局限性，但可以推断出：</p>
<ul>
<li><strong>计算成本：</strong> 尽管引入了VAE和FM模型，但端到端训练和3D表示对齐仍然需要大量的计算资源。</li>
<li><strong>对齐的鲁棒性：</strong> 3D表示的质量和预训练3D骨干网络的性能直接影响对齐效果，可能在某些复杂场景下仍存在挑战。</li>
<li><strong>场景编辑的局限性：</strong> 虽然展示了物体修复和场景混合的潜力，但论文也指出这些应用是“说明性的例子”，而非声称达到了SOTA性能，暗示其在复杂编辑任务上仍有提升空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更复杂的场景编辑：</strong> 进一步探索和优化推理时的场景编辑能力，例如更精细的物体替换、场景属性控制等。</li>
<li><strong>更广泛的3D表示：</strong> 探索更多样化、更强大的3D自监督表示方法，并将其集成到模型中。</li>
<li><strong>实时性提升：</strong> 进一步优化模型结构和推理过程，以满足实时应用的需求。</li>
<li><strong>跨领域迁移的泛化性：</strong> 研究模型在不同传感器、不同环境下的泛化能力。</li>
<li><strong>与其他生成模型结合：</strong> 探索将R3DPA的理念与GANs、自回归模型等其他生成模型相结合的可能性。</li>
</ul>
<p>总而言之，这篇论文在LiDAR场景生成领域做出了重要贡献，通过巧妙地结合RGB预训练先验和3D表示对齐，显著提升了生成质量，并开辟了可控生成的新途径，为自动驾驶等相关研究提供了强大的新工具和研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce R3DPA, the first LiDAR scene generation method to unlock image-pretrained priors for LiDAR point clouds, and leverage self-supervised 3D representations for state-of-the-art results.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07692v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07692v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-13 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
