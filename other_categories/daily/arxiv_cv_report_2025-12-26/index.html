<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-26 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-25/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-29/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-26">Arxiv Computer Vision Papers - 2025-12-26</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#histream-efficient-high-resolution-video-generation-via-redundancy-eliminated-streaming" class="nav-link">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-memorization-a-multi-modal-ordinal-regression-benchmark-to-expose-popularity-bias-in-vision-language-models" class="nav-link">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#streaming-video-instruction-tuning" class="nav-link">Streaming Video Instruction Tuning</a>
                </li>
                <li class="nav-item">
                    <a href="#gridit-factorized-grid-based-diffusion-for-efficient-long-image-sequence-generation" class="nav-link">GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#acd-direct-conditional-control-for-video-diffusion-models-via-attention-supervision" class="nav-link">ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</a>
                </li>
                <li class="nav-item">
                    <a href="#dreamontage-arbitrary-frame-guided-one-shot-video-generation" class="nav-link">DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#lookplangraph-embodied-instruction-following-method-with-vlm-graph-augmentation" class="nav-link">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#leveraging-lightweight-entity-extraction-for-scalable-event-based-image-retrieval" class="nav-link">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#latent-implicit-visual-reasoning" class="nav-link">Latent Implicit Visual Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#human-motion-estimation-with-everyday-wearables" class="nav-link">Human Motion Estimation with Everyday Wearables</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-26">Arxiv Computer Vision Papers - 2025-12-26</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域近期论文的每日报告执行摘要。</p>
<hr />
<p><strong>每日报告执行摘要：Arxiv 计算机视觉领域论文 (2025-12-24)</strong></p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集主要聚焦于<strong>高效视频生成</strong>和<strong>多模态理解</strong>两大核心领域。在视频生成方面，研究人员正积极探索如何克服高分辨率、长序列以及任意帧引导等挑战，通过流式处理、注意力机制和扩散模型等技术实现更高效、更可控的生成。多模态领域则关注如何提升视觉语言模型在理解复杂指令、处理流行度偏差以及进行视觉推理方面的能力。此外，利用日常可穿戴设备进行人体运动估计也展现了新的研究方向。</p>
<p><strong>2. 亮点与创新：</strong></p>
<ul>
<li><strong>HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</strong> 和 <strong>Streaming Video Instruction Tuning</strong> 均在<strong>视频生成效率</strong>方面取得了显著进展。HiStream 通过消除冗余的流式处理技术，有望大幅提升高分辨率视频生成的效率。Streaming Video Instruction Tuning 则将指令微调的概念引入视频领域，为更具交互性的视频生成奠定基础。</li>
<li><strong>GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</strong> 提出了一种新颖的基于网格的扩散模型，为<strong>长图像序列生成</strong>提供了高效解决方案。</li>
<li><strong>ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</strong> 和 <strong>DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</strong> 在<strong>视频生成的可控性</strong>方面表现突出。ACD 通过注意力监督实现对视频扩散模型的直接条件控制，而 DreaMontage 则实现了任意帧引导的单次视频生成，极大地增强了用户对生成内容的掌控力。</li>
<li><strong>Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</strong> 针对视觉语言模型中的<strong>流行度偏差</strong>问题提出了新的评估基准，对于提升模型的泛化能力和公平性具有重要意义。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>流式处理技术在视频生成中的应用：</strong> HiStream 和 Streaming Video Instruction Tuning 表明，流式处理是解决高分辨率和长序列视频生成效率问题的关键技术。</li>
<li><strong>扩散模型在视频生成中的精细化控制：</strong> ACD 和 GriDiT 展示了如何通过注意力机制和网格化等方法，进一步提升扩散模型在视频生成中的可控性和效率。</li>
<li><strong>多模态模型对偏差的鲁棒性研究：</strong> Beyond Memorization 指出，评估和解决视觉语言模型中的流行度偏差是未来研究的重要方向。</li>
<li><strong>具身智能与视觉语言模型结合：</strong> LookPlanGraph 探索了将视觉语言模型与图增强相结合，以实现更强大的具身指令跟随能力。</li>
<li><strong>轻量级实体提取在检索中的应用：</strong> Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval 提出了一种高效的图像检索方法，预示着轻量级模型在特定任务中的潜力。</li>
<li><strong>日常可穿戴设备在人体运动估计中的应用：</strong> Human Motion Estimation with Everyday Wearables 开启了利用低成本设备进行人体运动分析的新可能。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>考虑到其在<strong>视频生成效率</strong>和<strong>可控性</strong>方面的突破性进展，以及对<strong>多模态模型评估</strong>的创新性贡献，以下论文值得优先阅读全文：</p>
<ul>
<li><strong>HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</strong> (视频生成效率)</li>
<li><strong>ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</strong> (视频生成可控性)</li>
<li><strong>DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</strong> (视频生成可控性)</li>
<li><strong>Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</strong> (多模态模型评估与偏差)</li>
<li><strong>GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</strong> (长序列图像生成效率)</li>
</ul>
<hr />
<p>希望这份摘要能帮助您快速了解该领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.21338v1">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</a></li>
<li><a href="#2512.21337v1">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</a></li>
<li><a href="#2512.21334v1">Streaming Video Instruction Tuning</a></li>
<li><a href="#2512.21276v1">GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</a></li>
<li><a href="#2512.21268v1">ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</a></li>
<li><a href="#2512.21252v1">DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</a></li>
<li><a href="#2512.21243v1">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</a></li>
<li><a href="#2512.21221v1">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</a></li>
<li><a href="#2512.21218v1">Latent Implicit Visual Reasoning</a></li>
<li><a href="#2512.21209v1">Human Motion Estimation with Everyday Wearables</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.21338v1'></a></p>
<h2 id="histream-efficient-high-resolution-video-generation-via-redundancy-eliminated-streaming"><a href="https://arxiv.org/abs/2512.21338v1">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</a></h2>
<p><strong>Authors:</strong> Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, Juan-Manuel Perez-Rua</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了一种名为 HiStream 的高效自回归框架，旨在解决高分辨率视频生成中扩散模型固有的二次计算复杂度问题。通过在空间、时间和时间步长三个维度上系统地消除冗余，HiStream 实现了显著的推理加速，同时保持了出色的视觉质量，使得高分辨率视频生成在实践中变得可行且可扩展。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>HiStream 的核心创新在于其“消除冗余”的策略，具体体现在以下三个维度：</p>
<ul>
<li><strong>空间压缩 (Spatial Compression):</strong> 这是其核心思想之一。它不是直接在高分辨率下进行所有去噪操作，而是先在低分辨率下进行大部分去噪，然后利用缓存的低分辨率特征来指导高分辨率的精炼过程。这种“先低后高”的策略大大减少了在高分辨率下需要处理的信息量。</li>
<li><strong>时间压缩 (Temporal Compression):</strong> 采用“分块处理”（chunk-by-chunk）的策略，并引入一个固定大小的“锚点缓存”（anchor cache）。这意味着模型不会一次性处理整个视频序列，而是以小块为单位进行生成，并通过锚点缓存来维持不同时间块之间的连贯性。这种方法确保了推理速度的稳定性，避免了随着视频长度增加而导致的计算量爆炸。</li>
<li><strong>时间步长压缩 (Timestep Compression):</strong> 对于后续的、依赖于缓存的视频块，应用更少的时间步长进行去噪。这意味着模型可以利用之前已生成块的信息来更快地完成当前块的生成，进一步加速了过程。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<p>HiStream 的出现可能对高分辨率视频生成领域产生重大影响：</p>
<ul>
<li><strong>实用性提升：</strong> 解决了当前扩散模型在高分辨率视频生成上的计算瓶颈，使得高质量、高分辨率视频的生成不再是实验室里的昂贵实验，而是可以进入实际应用。</li>
<li><strong>可扩展性增强：</strong> 76.2x 甚至 107.5x 的加速比意味着研究人员和开发者可以更轻松地探索和生成更长、更高分辨率的视频内容，推动了该领域的可扩展性。</li>
<li><strong>推动新应用：</strong> 更快的生成速度将加速诸如电影制作、虚拟现实内容创作、游戏开发、数字人生成等领域的创新和发展。</li>
<li><strong>研究方向引导：</strong> HiStream 的成功可能会启发其他研究者探索类似的“消除冗余”策略来优化其他计算密集型的生成模型。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>电影和视觉特效 (VFX):</strong> 生成逼真的、高分辨率的电影场景、角色动画和特效。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 创建沉浸式、高保真的虚拟环境和交互式内容。</li>
<li><strong>游戏开发:</strong> 生成高质量的游戏过场动画和动态游戏场景。</li>
<li><strong>数字人生成:</strong> 创建更逼真、更流畅的数字人形象和对话。</li>
<li><strong>内容创作平台:</strong> 为内容创作者提供更强大的工具，以生成高质量的视频内容。</li>
<li><strong>医学影像:</strong> 在某些需要生成高分辨率医学视频的场景下（如模拟手术过程）。</li>
<li><strong>科学可视化:</strong> 生成复杂科学现象的高分辨率动态模拟。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<p>尽管摘要强调了 HiStream 的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>质量与速度的权衡 (Trade-off):</strong> 摘要中提到 HiStream+（应用所有三个优化）“提供了一个引人注目的速度与质量之间的权衡”。这暗示着在追求极致速度时，可能仍然存在一定程度的质量损失，尽管论文声称“可忽略不计的质量损失”。具体损失的程度和可接受性取决于具体的应用场景。</li>
<li><strong>锚点缓存的限制:</strong> 固定大小的锚点缓存虽然保证了速度稳定，但也可能成为一个瓶颈。如果视频内容在时间上存在非常长距离的依赖性，或者需要全局一致性，固定大小的缓存可能不足以捕捉所有关键信息，从而影响长时序的连贯性。</li>
<li><strong>实现复杂度:</strong> 虽然框架旨在提高效率，但其多阶段、多维度的优化策略（低分辨率去噪、特征缓存、分块处理、锚点缓存）可能会增加模型的实现和调试复杂度。</li>
<li><strong>对特定类型视频的适应性:</strong> 摘要提到在 1080p 基准上取得了 SOTA 质量。但对于不同风格、不同内容（例如，高度动态的动作场景 vs. 静态场景）的视频，其性能表现可能有所差异。</li>
<li><strong>“冗余消除”的定义和边界:</strong> 摘要中对“冗余”的定义是基于其优化策略。但“冗余”本身是一个相对概念，如何更普适、更智能地识别和消除冗余，可能仍是未来研究的方向。</li>
</ul>
<p>总而言之，HiStream 是一项非常有前景的研究，它通过巧妙地分解和优化高分辨率视频生成过程中的计算瓶颈，为该领域带来了巨大的进步。其核心在于对信息冗余的深刻理解和多维度上的有效利用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks.</li>
<li>On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21338v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21338v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21337v1'></a></p>
<h2 id="beyond-memorization-a-multi-modal-ordinal-regression-benchmark-to-expose-popularity-bias-in-vision-language-models"><a href="https://arxiv.org/abs/2512.21337v1">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</p>
<p><strong>作者：</strong> Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在揭示当前最先进的视觉语言模型（VLMs）在建筑年代预测任务中存在的显著“名气偏见”（popularity bias）。研究发现，这些模型在预测著名建筑时，准确率比普通建筑高出34%，这表明模型可能依赖于对著名地标的记忆，而非真正理解建筑的结构和历史特征。这种偏见阻碍了模型在实际应用中的泛化能力和可靠性。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
*   <strong>YearGuessr 数据集：</strong> 作者构建了迄今为止最大的开放式建筑年代预测数据集。该数据集包含来自157个国家的55,546张建筑立面图像，并附带多模态属性，包括：
    *   连续的建筑建造年份标签（1001-2024 CE）。
    *   GPS坐标信息。
    *   维基百科页面浏览量，作为衡量建筑“名气”的代理指标。
    *   文本描述，包含建筑风格、屋顶、墙体等信息。
*   <strong>Ordinal Regression 框架：</strong> 将建筑年代预测任务重新定义为序数回归（ordinal regression）问题，而非简单的分类或回归。这能更好地捕捉年代之间的有序关系。
*   <strong>Popularity-Aware Metrics：</strong> 引入了新的评估指标，如“名气感知区间准确率”（popularity-aware interval accuracy），用于量化和分析模型在不同名气水平建筑上的表现差异。
*   <strong>YearCLIP 模型：</strong> 作者提出了一个名为 YearCLIP 的新模型，该模型结合了 CLIP 的强大视觉-语言对齐能力，并引入了序数回归的粗粒度到细粒度（coarse-to-fine）策略。YearCLIP 还融合了 GPS 位置信息和预定义的推理提示（reasoning prompts），以提供可解释的建筑年代预测和人类可验证的理由。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>揭示名气偏见：</strong> 通过在包含30多个模型的基准测试中，作者证实了 VLMs 在处理高人气、易于记忆的建筑时表现出色，但在面对普通、不熟悉的对象时则显著挣扎。这种现象表明模型的能力更多地源于记忆而非真正的推理。
*   <strong>YearCLIP 的性能：</strong> YearCLIP 模型在 YearGuessr 数据集上取得了优异的性能，其 MAE（平均绝对误差）优于许多现有的先进模型，并且通过引入位置信息和推理提示，显著提升了预测的准确性和可解释性。
*   <strong>数据集的价值：</strong> YearGuessr 数据集为建筑年代预测领域提供了一个大规模、多模态、全球覆盖的开放基准，极大地推动了该领域的研究。
*   <strong>模型局限性：</strong> 研究表明，即使是先进的 VLMs，其性能也高度依赖于训练数据的分布，在地理和时间上存在偏差（例如，对现代建筑和美洲建筑的预测更好）。</p>
<p><strong>4. 论文提及的局限性：</strong>
*   <strong>数据偏差：</strong> YearGuessr 数据集在地理和时间分布上存在偏差，对现代建筑和美洲地区的覆盖更广，这可能导致模型在代表性不足的地区和早期风格上表现不佳。
*   <strong>标签的局限性：</strong> 建筑年代标签主要基于原始建造年份，即使建筑经过大规模翻新或重建，也可能保留原始年份，这会引入噪声。
*   <strong>模型泛化能力：</strong> 尽管 YearCLIP 表现出色，但研究仍指出，许多模型在处理风格模糊、历史记录不详或经过多次改造的建筑时仍面临挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展数据覆盖：</strong> 增加对非西方地区和早期建筑风格的覆盖，例如整合其他数据集或进行有针对性的数据收集。
*   <strong>更精细的标签：</strong> 引入更详细的标签，如明确区分翻新和重建的建筑，以及更精确的时间段划分。
*   <strong>数据增强与去偏：</strong> 利用扩散模型等技术进行合成数据增强，并开发更有效的去偏方法，以提高模型在数据稀疏和有偏见情况下的鲁棒性。
*   <strong>主动学习与专家验证：</strong> 探索主动学习策略以更有效地利用标注数据，并结合专家知识进行模型验证和改进。
*   <strong>多模态融合的进一步探索：</strong> 研究更先进的多模态融合技术，以更有效地整合图像、文本、地理信息等多种模态的数据。</p>
<p>总而言之，这篇论文通过构建大规模数据集和提出新颖的评估框架，有力地揭示了当前视觉语言模型在建筑年代预测任务中的“名气偏见”问题，并提出了 YearCLIP 模型作为一种更具泛化能力和可解释性的解决方案。研究成果为理解和改进多模态模型在真实世界应用中的鲁棒性提供了重要见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding.</li>
<li>To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21337v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21337v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21334v1'></a></p>
<h2 id="streaming-video-instruction-tuning"><a href="https://arxiv.org/abs/2512.21334v1">Streaming Video Instruction Tuning</a></h2>
<p><strong>Authors:</strong> Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是论文“Streaming Video Instruction Tuning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Streaming Video Instruction Tuning</p>
<p><strong>作者：</strong> Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
现有视频大语言模型（LLMs）主要针对离线视频进行分析，即需要完整的视频作为输入才能生成输出。然而，实时交互式AI助手需要处理连续、无界限的视频流，并根据事件的发生实时响应指令，同时受到严格的延迟限制。这带来了两大挑战：1）如何在不丢失上下文的情况下处理连续、无界限的数据流；2）如何管理跨多个任务的可变响应时序和粒度，这可能需要帧级或更长时间尺度的时序推理。现有的离线模型难以满足流式视频的实时交互需求。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>Streamo模型：</strong> 作者提出了Streamo，一个端到端的实时流式视频LLM，它将决策制定和响应生成统一起来。通过在模型内部嵌入帧级响应状态预测（Silence, Standby, Response），Streamo能够实时监控视频流并做出精细的判断，从而实现一次推理即可生成响应，显著提高了响应时序的准确性和生成效率。
*   <strong>Streamo-Instruct-465K数据集：</strong> 为了解决现有数据集在时序对齐和多任务响应行为上的不一致性问题，作者构建了一个大规模、多任务的指令遵循数据集Streamo-Instruct-465K。该数据集为流式视频理解和交互量身定制，标准化了三个响应粒度级别，提供了统一的时序边界标注，并涵盖了实时叙述、动作和事件描述、时序事件定位以及时序问答等多种任务。
*   <strong>Streamo-Bench基准：</strong> 作者还提出了Streamo-Bench，一个全面的流式视频指令遵循基准，用于评估模型在多样化交互任务上的指令理解和响应能力，弥补了现有基准主要依赖问答（QA）形式的局限性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能优越：</strong> Streamo在流式和离线视频基准测试中均表现出色，超越了现有的在线方法，展现出强大的时序推理能力、响应式交互能力和广泛的泛化能力。
*   <strong>弥合鸿沟：</strong> Streamo成功地弥合了离线视频感知模型与实时多模态助手之间的差距，朝着实现统一、智能的连续视频流理解迈出了重要一步。
*   <strong>数据集和基准的价值：</strong> Streamo-Instruct-465K数据集为流式视频理解研究提供了宝贵的资源，而Streamo-Bench则为评估和推动该领域的发展提供了新的标准。
*   <strong>框架的兼容性：</strong> 作者证明了其端到端训练框架能够有效地将多种先进的离线模型转化为流式视频助手，并且在转换后仍能保持强大的离线感知能力。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>长序列优化不足：</strong> 尽管模型在准确性方面表现良好，但其当前流水线在处理无界限时序上下文时，缺乏专门的长序列优化，导致序列长度增长时内存和延迟成本显著增加，可能变得难以承受。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>计算效率提升：</strong> 作者提出可以通过集成KV-cache管理和视觉令牌剪枝来降低计算开销。
*   <strong>上下文管理增强：</strong> 探索滑动窗口注意力（sliding-window attention）和自适应帧压缩（adaptive frame compression）等技术来改进上下文管理。
*   <strong>无界限数据流处理：</strong> 进一步研究如何实现真正无界限的实时数据流处理。</p>
<p><strong>总结：</strong>
这篇论文的核心贡献在于提出了Streamo模型和Streamo-Instruct-465K数据集，成功地解决了实时流式视频理解和交互的挑战。通过创新的端到端训练框架和精心设计的数据集，Streamo能够高效地处理连续视频流并响应复杂的指令，为构建通用的实时AI助手奠定了基础。该研究不仅在技术上取得了显著进展，也为未来的相关研究提供了重要的资源和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21334v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21334v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21276v1'></a></p>
<h2 id="gridit-factorized-grid-based-diffusion-for-efficient-long-image-sequence-generation"><a href="https://arxiv.org/abs/2512.21276v1">GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</a></h2>
<p><strong>Authors:</strong> Snehal Singh Tomar, Alexandros Graikos, Arjun Krishna, Dimitris Samaras, Klaus Mueller</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本研究提出了一种名为 GriDiT 的新颖方法，用于高效生成长图像序列。其核心贡献在于将图像序列生成过程分解为两个阶段：首先在低分辨率下生成序列的粗糙结构，然后独立地对每个帧进行高分辨率超分辨率处理。这种分解策略显著提高了生成质量、序列连贯性，并带来了更高的推理效率和数据利用率。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>因子化生成（Factorized Generation）：</strong> 这是 GriDiT 最核心的创新。论文摒弃了将图像序列视为单一高维张量的传统做法，而是将其分解为“低分辨率序列结构”和“高分辨率帧细节”两个独立但相互关联的部分。</li>
<li><strong>基于网格的低分辨率序列生成：</strong> 论文训练了一个生成模型，仅在由子采样帧组成的“网格图像”上进行训练。这里的“网格图像”可以理解为一种将序列帧以某种空间排列（例如，将连续帧堆叠成一个更大的二维图像）的方式呈现给模型，使其能够学习到帧之间的时序关联。</li>
<li><strong>利用 Diffusion Transformer (DiT) 的自注意力机制：</strong> 尽管模型在低分辨率网格图像上训练，但 GriDiT 利用了 DiT 强大的自注意力机制来捕捉序列帧之间的时序相关性。这使得一个本质上是二维图像生成器能够扩展为一个低分辨率的三维（序列）图像生成器，而无需修改其核心架构。</li>
<li><strong>独立帧超分辨率：</strong> 在生成低分辨率序列结构后，论文采用了一个独立的超分辨率模型来提升每个帧的细节，从而实现高保真度的最终输出。这种分离使得高分辨率细节的生成更加高效且独立于序列的整体结构。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升长序列生成效率：</strong> 传统的生成模型在处理长序列时面临计算量爆炸和内存瓶颈。GriDiT 的因子化方法通过降低中间表示的维度和独立处理高分辨率细节，有望显著提高生成长序列的效率，使其在实际应用中更具可行性。</li>
<li><strong>改善生成质量和连贯性：</strong> 通过显式地建模序列的粗糙结构和帧间的时序关联，GriDiT 有望生成更具视觉质量和时间连贯性的图像序列，减少伪影和不自然的过渡。</li>
<li><strong>降低数据和计算需求：</strong> 论文提到该方法提高了训练数据的使用效率，并且在推理速度上实现了至少两倍的提升。这意味着使用更少的数据和更短的训练时间，就能达到甚至超越现有 SoTA 的性能。</li>
<li><strong>增强泛化能力：</strong> 论文指出 GriDiT 能够有效地泛化到不同的数据领域，而无需额外的先验知识或监督。这表明其方法论具有更强的普适性，可以应用于更广泛的图像序列生成任务。</li>
<li><strong>为视频生成等领域开辟新思路：</strong> 这种将序列建模分解为结构和细节的方法，可能为其他需要处理时序数据的生成任务（如视频生成、动作生成等）提供新的视角和技术路线。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用：</strong></p>
<ul>
<li><strong>视频生成：</strong> 这是最直接的应用领域，可以用于生成逼真的短视频、动画片段等。</li>
<li><strong>视频预测：</strong> 预测未来帧的序列，用于自动驾驶、监控分析等。</li>
<li><strong>图像动画化：</strong> 将静态图像转化为具有动态效果的短视频。</li>
<li><strong>医学影像序列分析：</strong> 如 MRI、CT 等医学扫描的动态序列生成和预测。</li>
<li><strong>科学可视化：</strong> 生成模拟结果的动态序列，帮助理解复杂现象。</li>
<li><strong>游戏和虚拟现实：</strong> 生成逼真的动态场景和角色动画。</li>
<li><strong>内容创作：</strong> 为艺术家和创作者提供更高效的工具来生成动态内容。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>对“网格图像”的具体实现细节未知：</strong> 摘要中提到“grid images comprising subsampled frames”，但具体如何将子采样帧组织成“网格图像”以供模型学习，以及这种组织方式是否会引入新的偏见或限制，需要进一步的论文内容来阐明。</li>
<li><strong>超分辨率模型的独立性：</strong> 虽然独立超分辨率提高了效率，但也可能意味着超分辨率模型无法完全利用序列的全局时序信息来优化细节。如果序列中存在需要全局时序一致性的精细动态，独立超分辨率可能无法完美捕捉。</li>
<li><strong>对长序列的定义和处理能力：</strong> 论文强调“long image sequence generation”，但“long”的具体长度界限以及模型在处理极长序列（例如，成千上万帧）时的性能和稳定性，仍需验证。</li>
<li><strong>计算资源的权衡：</strong> 尽管推理效率有所提升，但引入了两个阶段（低分辨率生成和高分辨率超分辨率），总体的计算资源需求和延迟可能仍是一个需要权衡的因素，尤其是在对实时性要求极高的场景下。</li>
<li><strong>潜在的“分辨率鸿沟”：</strong> 低分辨率生成和高分辨率超分辨率之间的信息传递和融合是否会产生信息损失或不一致，是需要关注的问题。</li>
</ul>
<p>总而言之，GriDiT 提出的因子化生成策略，特别是将序列生成分解为结构和细节两步，并利用 DiT 的自注意力机制在低分辨率网格上学习时序关联，是一项非常有前景的研究。它有望在效率、质量和泛化能力上带来显著的提升，对图像序列生成领域具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, is this straightforward representation ideal given the current state-of-the-art (SoTA)?</li>
<li>Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution.</li>
<li>Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences.</li>
<li>Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context.</li>
<li>Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21276v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21276v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21268v1'></a></p>
<h2 id="acd-direct-conditional-control-for-video-diffusion-models-via-attention-supervision"><a href="https://arxiv.org/abs/2512.21268v1">ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision</a></h2>
<p><strong>Authors:</strong> Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision”的全面中文摘要，重点突出了其研究问题、方法、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision (ACD：通过注意力监督实现视频扩散模型的直接条件控制)</p>
<p><strong>作者：</strong> Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</p>
<p><strong>研究问题/核心挑战：</strong>
视频生成领域的一个关键挑战是实现<strong>精确的可控性</strong>，即生成的视频能够准确地对齐给定的条件信号。现有的方法，如<strong>无分类器引导 (Classifier-Free Guidance, CFG)</strong>，通过间接建模数据和条件的联合分布来实现控制，但往往导致控制能力有限。而<strong>基于分类器的引导 (Classifier-based Guidance)</strong> 虽然能强制执行条件，但容易产生<strong>对抗性伪影</strong>，模型可能为了提高分类器分数而并未真正满足条件。因此，如何实现<strong>直接、可靠且语义一致</strong>的视频条件控制是亟待解决的问题。</p>
<p><strong>关键创新/方法贡献：</strong></p>
<ol>
<li>
<p><strong>Attention-Conditional Diffusion (ACD) 框架：</strong> 论文提出了一种新颖的框架，<strong>ACD</strong>，用于视频扩散模型的<strong>直接条件控制</strong>。其核心思想是将<strong>条件信号直接注入到模型的注意力机制</strong>中，而不是在输出或分数层面进行引导。通过使模型的注意力图与外部控制信号对齐，ACD 实现了更强的可控性。</p>
</li>
<li>
<p><strong>稀疏 3D 感知对象布局作为条件信号：</strong> 为了支持 ACD，论文引入了一种<strong>稀疏的 3D 感知对象布局</strong>作为一种高效的条件表示。这种布局自然地捕捉了对象的几何形状和空间关系，为场景构成和相机视角提供了直观的控制。</p>
</li>
<li>
<p><strong>Layout ControlNet 和自动化标注流水线：</strong> 论文设计了一个<strong>专门的 Layout ControlNet</strong> 来将布局信息注入到扩散模型中。此外，还开发了一个<strong>自动化的标注流水线</strong>，用于大规模地集成布局信息，解决了获取高质量 3D 布局数据的难题。</p>
</li>
<li>
<p><strong>注意力层面的监督：</strong> ACD 的关键在于其<strong>注意力层面的监督机制</strong>。通过在 Transformer 架构的注意力层中强制执行条件信号与生成内容之间的对齐，ACD 确保了条件信息能够直接影响生成过程，从而实现更精确的语义控制。</p>
</li>
</ol>
<p><strong>主要结果与意义：</strong></p>
<ul>
<li><strong>卓越的可控性：</strong> 大量实验表明，ACD 在<strong>条件信号对齐方面表现出优越性</strong>，能够生成与稀疏 3D 对象布局和相机轨迹高度一致的视频。</li>
<li><strong>高质量视频生成：</strong> ACD 在实现高可控性的同时，<strong>保持了视频的时间连贯性和视觉保真度</strong>，生成了视觉上令人信服的视频。</li>
<li><strong>克服现有方法的局限性：</strong> 与现有方法相比，ACD <strong>避免了对抗性伪影</strong>（如基于分类器的引导）和<strong>有限的控制能力</strong>（如无分类器引导），提供了一种更可靠的条件视频合成范式。</li>
<li><strong>用户研究验证：</strong> 用户研究结果显示，ACD 在感知相似性、时间连贯性和相机引导准确性方面均优于其他先进方法。</li>
</ul>
<p><strong>论文提及的局限性：</strong></p>
<ul>
<li><strong>静态场景为主：</strong> 目前的稀疏 3D 对象布局主要设计用于<strong>静态室内场景</strong>，这限制了其在动态或室外环境中的应用，这些环境需要显式地建模时间动态和物体形变。</li>
<li><strong>近似对齐：</strong> 稀疏布局提供的是<strong>近似的对象放置</strong>，而非像素级别的对齐。在复杂场景下，如长距离相机轨迹，这可能导致对齐上的不精确。</li>
<li><strong>数据依赖性：</strong> 尽管自动化标注流水线有所帮助，但训练仍然<strong>依赖于标注好的布局数据</strong>。扩展到更大、更多样化的数据集可能仍然具有挑战性。</li>
</ul>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>动态和室外场景：</strong> 将 ACD 扩展到能够处理动态场景和室外环境，需要显式地建模时间动态和物体形变。</li>
<li><strong>更精确的对齐：</strong> 探索更精细的控制信号或方法，以实现像素级别的对象对齐，尤其是在复杂场景下。</li>
<li><strong>弱监督或自监督学习：</strong> 研究弱监督或自监督策略，以减轻对大量标注数据的依赖，从而实现更大规模和更多样化的数据集训练。</li>
<li><strong>更精细的几何先验：</strong> 引入更精确的几何先验，如密集深度图或场景流，以进一步提高在复杂场景下的对齐精度。</li>
</ul>
<p>总而言之，这篇论文提出了一种名为 ACD 的新颖框架，通过将条件信号直接注入到视频扩散模型的注意力机制中，实现了视频生成的可控性。其核心贡献在于利用稀疏 3D 对象布局作为控制信号，并设计了相应的 ControlNet 和训练策略。实验结果表明，ACD 在保持视频质量的同时，显著提高了条件控制的准确性和可靠性，为可控视频生成领域开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision.</li>
<li>To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21252v1'></a></p>
<h2 id="dreamontage-arbitrary-frame-guided-one-shot-video-generation"><a href="https://arxiv.org/abs/2512.21252v1">DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation</a></h2>
<p><strong>Authors:</strong> Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation”论文的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation (DreaMontage：任意帧引导的单镜头视频生成)</p>
<p><strong>作者：</strong> Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>电影制作中的“单镜头”（one-shot）或“长镜头”（long take）技术以其沉浸式的连续性而著称，但其在现实中实现成本高昂且对专业技能要求极高。现有的AI视频生成模型虽然提供了虚拟替代方案，但通常采用简单的片段拼接（clip concatenation），这难以保证视频在视觉上的流畅性和时间上的连贯性，常常导致不自然的转场和断裂感。因此，研究如何生成<strong>无缝、富有表现力且时长可控的任意帧引导的单镜头视频</strong>是本文要解决的核心问题。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<p>DreaMontage 框架通过三个主要维度解决了上述挑战：</p>
<ul>
<li>
<p><strong>轻量级中间条件注入机制 (Intermediate-Conditioning Mechanism)：</strong></p>
<ul>
<li>在DiT（Diffusion Transformer）架构中集成了一种轻量级的中间条件注入方法。</li>
<li>采用<strong>自适应调优 (Adaptive Tuning)</strong> 策略，有效利用基础训练数据，实现了强大的任意帧控制能力。</li>
<li>针对超分辨率模型，提出了<strong>共享 RoPE (Shared-RoPE)</strong> 策略，通过序列级条件注入，有效缓解了因条件与生成内容之间的细微差异导致的闪烁和跨帧颜色偏移问题。</li>
</ul>
</li>
<li>
<p><strong>视觉表现力增强 (Visual Expression Enhancement)：</strong></p>
<ul>
<li>精心策划了一个高质量的数据集，并实施了<strong>视觉表现力 SFT (Visual Expression SFT)</strong> 阶段，以提升视频的视觉保真度和电影表现力。</li>
<li>针对“突兀转场”和“主体运动不合理”等关键问题，设计了特定的<strong>成对数据集</strong>，并应用了<strong>定制化 DPO (Tailored DPO)</strong> 训练。这显著提高了生成内容的成功率和可用性。</li>
</ul>
</li>
<li>
<p><strong>高效的长视频生成策略 (Efficient Long-Video Generation Strategy)：</strong></p>
<ul>
<li>设计了<strong>分段自回归 (Segment-wise Auto-Regressive, SAR)</strong> 推理策略，将长视频生成分解为多个段落，在内存效率和计算效率之间取得了最佳平衡，同时保持了单镜头内容的完整性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>生成效果：</strong> DreaMontage 能够生成视觉上引人注目且无缝衔接的单镜头视频，有效克服了现有方法的断裂感和不连贯性。</li>
<li><strong>控制能力：</strong> 模型实现了对任意帧（包括图像和视频片段）的精确时间控制，用户可以灵活地将零散的视觉素材整合成连贯的电影体验。</li>
<li><strong>效率：</strong> SAR 推理策略使得生成长视频在计算和内存上更加高效。</li>
<li><strong>性能超越：</strong> 在与 SOTA（State-of-the-Art）模型的定量和定性比较中，DreaMontage 在多关键帧和首尾帧引导的单镜头视频生成任务上均取得了显著优势，尤其在提示跟随（Prompt Following）方面表现突出。</li>
<li><strong>意义：</strong> 该框架极大地降低了创作高质量单镜头视频的门槛，为电影制作、游戏过场动画、动态广告等领域提供了强大的新工具，使创作者能够以前所未有的灵活性和效率实现复杂的叙事和视觉效果。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中并未明确列出具体的局限性，但从其研究方向和方法来看，可以推断出潜在的挑战：</p>
<ul>
<li><strong>数据依赖性：</strong> 高质量数据集的构建和标注对于 SFT 和 DPO 阶段至关重要，数据的质量和多样性直接影响模型的表现。</li>
<li><strong>计算资源：</strong> 尽管 SAR 策略提高了效率，但训练和生成高质量、长时长的视频仍然需要大量的计算资源。</li>
<li><strong>复杂场景的泛化性：</strong> 对于极其复杂或高度抽象的场景，模型可能仍会遇到挑战，尤其是在处理非常规的物理运动或细微的情感表达时。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的控制：</strong> 进一步探索更细粒度的控制机制，例如对特定物体、动作或情感的精确控制。</li>
<li><strong>实时交互生成：</strong> 探索将 DreaMontage 应用于实时交互式视频生成场景，允许用户进行实时的修改和调整。</li>
<li><strong>多模态融合：</strong> 进一步融合更多模态的信息，如音频、音乐等，以生成更具沉浸感和表现力的视频。</li>
<li><strong>模型效率优化：</strong> 继续研究更高效的模型架构和训练方法，以进一步降低计算成本，使其在更多设备上可用。</li>
<li><strong>艺术风格迁移：</strong> 探索将 DreaMontage 与艺术风格迁移技术结合，生成具有特定艺术风格的单镜头视频。</li>
</ul>
<p>总而言之，DreaMontage 是一项重要的研究成果，它通过创新的技术手段，显著提升了任意帧引导的单镜头视频生成能力，为AI驱动的视频创作领域开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs.</li>
<li>Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21252v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21252v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21243v1'></a></p>
<h2 id="lookplangraph-embodied-instruction-following-method-with-vlm-graph-augmentation"><a href="https://arxiv.org/abs/2512.21243v1">LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation</a></h2>
<p><strong>Authors:</strong> Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation
<strong>作者：</strong> Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究挑战：</strong>
该论文主要解决了在动态环境中，机器人如何有效地遵循人类指令的问题。现有的大型语言模型（LLM）在执行指令时，通常依赖于预先构建的静态场景图。然而，这些静态场景图无法应对环境中可能发生的物体位置变化或新物体的出现，导致LLM在实际执行任务时出现规划失败。因此，核心挑战在于如何让LLM的规划能力与动态变化的环境保持一致，并实现对环境的实时感知和更新。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
作者提出了 <strong>LookPlanGraph</strong> 方法，其核心创新在于：</p>
<ul>
<li><strong>动态场景图更新：</strong> LookPlanGraph 引入了一个 <strong>图增强模块</strong>，利用 <strong>视觉语言模型（VLM）</strong> 处理机器人的第一视角图像，动态地更新场景图。这包括验证现有物体先验信息或发现新实体，从而使场景图能够实时反映环境变化。</li>
<li><strong>记忆图（Memory Graph）：</strong> 方法的核心是一个 <strong>记忆图</strong>，它包含三个关键部分：通用场景结构、已交互过的不可移动资产（assets）以及可移动物体的可能位置（priors）。这为LLM提供了更具上下文感知能力的规划基础。</li>
<li><strong>场景图模拟器（Scene Graph Simulator - SGS）：</strong> SGS 用于验证LLM生成的动作的可行性，并根据环境反馈修正动作，同时更新记忆图，确保LLM能够基于当前环境状态做出动态决策。</li>
<li><strong>GraSIF 数据集：</strong> 为了评估此类方法，作者构建了一个名为 <strong>GraSIF（Graph Scenes for Instruction Following）</strong> 的新数据集，包含 514 个任务，涵盖了家庭环境中的操作任务，并提供了自动化的验证框架。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在模拟环境（VirtualHome, OmniGibson）和真实世界实验中，LookPlanGraph 在动态场景下显著优于基于静态场景图的方法，成功率（SR）和平均规划精度（APP）均有提升。
*   <strong>动态环境适应性：</strong> 该方法能够有效地处理物体位置变化等动态环境因素，这是传统静态规划方法难以做到的。
*   <strong>通用性：</strong> LookPlanGraph 在不同规模的模型（如 GPT-4o, Llama3.2-90b, Gemma3-12b）上都表现出良好的性能，证明了其模型适应性。
*   <strong>数据集贡献：</strong> GraSIF 数据集的发布为未来研究提供了标准化的评估平台，促进了该领域的研究。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>低层动作执行依赖：</strong> 该方法在很大程度上依赖于低层动作的精确执行。在真实世界中，传感器噪声、抓取失败和导航错误等问题可能会影响整体性能。
*   <strong>动作幻觉与无效发现：</strong> LLM 有时会生成语义上无效的动作，例如尝试“发现”已知物体，或将发现操作应用于不恰当的节点类型，这可能导致任务失败。
*   <strong>领域知识限制：</strong> LLM 在处理领域特定的推理时存在局限性，例如在“派对后清洁咖啡房间”的任务中，无法区分一次性用品和可重复使用的物品，导致不当的处理。
*   <strong>模型能力影响：</strong> 虽然 LookPlanGraph 具有模型适应性，但较小的模型（如 Gemma3-12b）在性能上会有显著下降。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>鲁棒的错误恢复机制：</strong> 集成更强大的错误恢复机制，以及从低层控制器获取反馈，以增强系统在执行失败时的韧性。
*   <strong>动态规划的潜力：</strong> 探索动态规划在真实世界机器人系统中的应用，以实现更无缝的人机协作和更高效的实时重规划。
*   <strong>更精细的领域知识整合：</strong> 进一步提升 LLM 在领域特定推理方面的能力，使其能够更好地理解和处理复杂的现实世界场景。</p>
<p><strong>对计算机视觉领域的新颖性/重要性：</strong>
LookPlanGraph 的主要贡献在于将 <strong>VLM 的视觉感知能力</strong> 与 <strong>LLM 的规划能力</strong> 有效结合，并解决了 <strong>动态环境下的场景理解和更新</strong> 问题。它通过 <strong>动态场景图增强</strong> 的方式，使得机器人能够实时感知和适应环境变化，这对于需要与真实世界进行复杂交互的机器人应用至关重要。该方法不仅提升了机器人在动态环境中的指令遵循能力，还通过引入 GraSIF 数据集，为该领域的研究提供了重要的基准和推动力。它展示了如何利用 VLM 来弥合感知与规划之间的鸿沟，为构建更智能、更具适应性的机器人系统开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors.</li>
<li>During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities.</li>
<li>We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs.</li>
<li>To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting.</li>
<li>Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21243v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21243v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21221v1'></a></p>
<h2 id="leveraging-lightweight-entity-extraction-for-scalable-event-based-image-retrieval"><a href="https://arxiv.org/abs/2512.21221v1">Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</a></h2>
<p><strong>Authors:</strong> Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval”的全面中文摘要：</p>
<p><strong>论文摘要：利用轻量级实体提取实现可扩展的事件驱动图像检索</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决现实世界中图像-文本检索面临的挑战，特别是针对自然语言描述的图像检索任务。现有方法在处理模糊、依赖上下文的查询、语言多样性以及需要可扩展解决方案方面存在困难。尤其是在事件驱动的场景下，查询往往更复杂，包含多个命名实体、时间信息和上下文依赖，这使得传统的基于短文本描述的检索模型难以有效工作。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<p>作者提出了一种<strong>轻量级的两阶段检索流水线</strong>，该流水线利用<strong>事件驱动的实体提取</strong>来整合真实世界标题中的时间与上下文信号。</p>
<ul>
<li>
<p><strong>第一阶段：事件驱动的候选过滤</strong></p>
<ul>
<li>利用<strong>spaCy</strong>等工具进行<strong>命名实体识别 (NER)</strong>，提取查询中的关键实体（如人名、地点、时间）。</li>
<li>基于提取出的<strong>显著实体</strong>，使用<strong>BM25</strong>算法进行高效的候选文章过滤。这大大减少了计算开销，同时保留了语义相关性。</li>
<li>通过<strong>加权实体类型匹配</strong>和<strong>NLTK的WordNet</strong>进行查询扩展，增强了实体匹配的鲁棒性。</li>
<li>最后，通过<strong>互惠排名融合 (Reciprocal Rank Fusion)</strong> 融合来自实体匹配和文本匹配（BM25）的结果，选出 top-K 的候选文章。</li>
</ul>
</li>
<li>
<p><strong>第二阶段：多模态图像重排序</strong></p>
<ul>
<li>利用<strong>BEiT-3</strong>模型（一种强大的视觉-语言Transformer模型）来捕捉深层的多模态语义。</li>
<li>采用<strong>双模型策略</strong>：<ul>
<li><strong>事件对齐的BEiT-3</strong>：针对事件查询和图像文本之间的字面相似性进行微调，特别擅长匹配命名实体、时间参考和事实描述。</li>
<li><strong>BEiT-3 ITC (Image-Text Contrastive)</strong>：利用对比学习目标进行预训练，编码超越文本重叠的高层视觉语义，用于检索具有潜在视觉线索（如情感、象征意义）的图像。</li>
</ul>
</li>
<li>通过<strong>sigmoid增强</strong>机制，结合了图像的<strong>原始相似度得分</strong>和<strong>文章的排名位置</strong>，对候选图像进行重排序。</li>
<li>最终通过<strong>互惠排名融合</strong>整合两个BEiT-3模型的重排序结果，得到最终的排名。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li>在<strong>OpenEvents v1</strong>基准测试上，该方法取得了<strong>0.559的平均精度 (mAP)</strong>，显著优于现有基线方法（最佳基线mAP为0.323），相对提升了73%。</li>
<li>在公共测试集上，该方法取得了0.559的mAP、0.559的mRR和0.760的R@10。在私有测试集上，mAP为0.521，R@10为0.705，排名第四。</li>
<li>这些结果表明，将<strong>事件驱动的过滤</strong>与<strong>长文本视觉-语言建模</strong>相结合，能够实现<strong>准确且高效</strong>的复杂、真实世界场景下的图像检索。该方法在处理长、实体密集且依赖上下文的查询方面表现出色。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>实体提取的挑战</strong>：尽管spaCy模型能识别命名实体和动作，但对于用自然语言描述的事件（如动词或隐含在子句中的事件）识别存在困难，可能导致关键的时间和上下文线索被忽略。</li>
<li><strong>抽象/象征性描述的困难</strong>：对于不直接对应具体对象或可观察特征，而是反映潜在语义（如情感、象征意义、隐含叙事）的查询，当前的视觉-语言模型（包括BEiT-3）仍难以准确表示和对齐。这凸显了将文本与图像的细微、人类水平的解释对齐的难度。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更高级的事件提取方法</strong>：探索利用大型语言模型（LLMs）来改进对隐式或非字面事件表达的捕捉能力，以弥补传统NLP工具的不足。</li>
<li><strong>针对抽象/细微线索的模型</strong>：开发专门用于识别抽象或细微事件线索，并将其有效整合到检索过程中的轻量级模型。</li>
<li><strong>检索增强生成 (RAG) 和文本重排序</strong>：探索集成RAG或其他文本重排序技术，以进一步缩小文章池，提高准确性并减少图像匹配阶段的噪声。</li>
<li><strong>知识增强推理</strong>：结合符号过滤、上下文抽象和知识增强推理，有望进一步推动多模态事件检索的发展。</li>
</ul>
<p>总而言之，这篇论文提出了一种创新的两阶段检索框架，通过轻量级的实体提取和强大的多模态模型，有效解决了事件驱动图像检索中的关键挑战，并在真实世界数据集上取得了显著的性能提升。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions.</li>
<li>Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21221v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21221v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21218v1'></a></p>
<h2 id="latent-implicit-visual-reasoning"><a href="https://arxiv.org/abs/2512.21218v1">Latent Implicit Visual Reasoning</a></h2>
<p><strong>Authors:</strong> Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Latent Implicit Visual Reasoning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Latent Implicit Visual Reasoning (潜在隐式视觉推理)</p>
<p><strong>作者：</strong> Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig</p>
<hr />
<p><strong>1. 研究背景与问题 (The main problem or research question being addressed)</strong></p>
<p>大型多模态模型（LMMs）在视觉理解方面取得了显著进展，但它们在处理以视觉为主的推理任务时仍存在局限性。这主要是因为现有LMMs在很大程度上是“以文本为中心”的，依赖语言作为核心推理模态。虽然一些近期方法尝试通过监督中间视觉步骤（如使用辅助图像、深度图或图像裁剪）来增强模型的视觉能力，但这些策略存在几个缺点：它们引入了对“有用”视觉抽象的严格先验，增加了大量标注成本，并且难以泛化到不同任务。因此，论文旨在解决<strong>如何让LMMs在没有显式监督的情况下，自主地发现和利用视觉推理的中间表示，以提升其在视觉密集型推理任务上的表现</strong>这一核心问题。</p>
<p><strong>2. 关键创新与方法论贡献 (The key innovations or methodological contributions)</strong></p>
<p>该论文提出了一种名为<strong>Latent Implicit Visual Reasoning (LIVR)</strong> 的新方法，其核心创新在于：</p>
<ul>
<li><strong>任务无关的隐式视觉推理机制：</strong> LIVR引入了一种任务无关的机制，使LMM能够自主发现和使用视觉推理的“潜在（latent）”标记（tokens），而无需显式监督。</li>
<li><strong>视觉瓶颈（Visual Bottlenecking）：</strong> 论文设计了一种新颖的视觉瓶颈方法。通过修改注意力掩码，强制视觉信息必须通过新引入的潜在标记进行传递。这意味着答案标记（answer tokens）和提示标记（prompt tokens）只能关注潜在标记，而不能直接关注原始图像标记。这迫使潜在标记成为承载和处理视觉信息的核心通道。</li>
<li><strong>潜在标记（Latent Tokens）：</strong> LIVR在LMM的词汇表中添加了K个新的特殊标记，这些标记被初始化并与模型一起进行训练。它们不直接生成，而是学习如何有效地利用视觉信息来解决任务。</li>
<li><strong>两阶段训练（Multi-Stage Training）：</strong><ul>
<li><strong>第一阶段（视觉瓶颈）：</strong> 使用上述视觉瓶颈和注意力掩码，仅在答案标记上计算损失，以优化潜在标记捕获最有用的视觉信息。</li>
<li><strong>第二阶段（标准掩码）：</strong> 恢复标准的注意力掩码，允许答案标记同时关注原始图像标记和已“富集”的潜在标记，以联合利用两者来回答问题。</li>
</ul>
</li>
</ul>
<p>这种方法避免了昂贵的任务特定标注，并且能够适应各种视觉推理任务，包括那些难以定义中间抽象的任务。</p>
<p><strong>3. 主要结果与意义 (The main results and their significance)</strong></p>
<ul>
<li><strong>性能提升：</strong> LIVR在九个感知密集型任务上取得了显著的性能提升。在单任务微调设置下，LIVR超越了直接监督微调（Direct SFT），并在多个任务上达到了最先进（state-of-the-art）的结果。例如，在Jigsaw任务上平均提升了6.24%，在Functional Correspondence任务上提升了13.02%。</li>
<li><strong>泛化能力：</strong> LIVR在多任务指令微调设置下也展现了强大的泛化能力，优于直接监督微调。</li>
<li><strong>对难以指定抽象任务的优势：</strong> 该方法在Art Style、Visual Similarity和Relative Reflectance等任务上表现尤为突出，这些任务的中间视觉抽象难以用人工方式明确定义。LIVR提供了一种学习这些抽象的有效途径。</li>
<li><strong>与现有方法的对比：</strong> 相较于Mirage等依赖显式视觉监督的方法，LIVR在Jigsaw和Visual Spatial Planning任务上取得了显著的性能优势（分别提升了19.40%和20.00%），证明了其任务无关性和无需显式监督的优势。</li>
<li><strong>消融实验验证：</strong> 消融实验证实了潜在标记和视觉瓶颈都是LIVR成功的关键组成部分。仅添加潜在标记而不进行瓶颈训练，或仅使用瓶颈而不添加潜在标记，都无法达到LIVR的性能。</li>
</ul>
<p><strong>意义：</strong> LIVR为LMMs提供了一种更有效、更通用的视觉推理能力增强途径，克服了现有方法的局限性，为构建更强大的视觉推理模型开辟了新方向。</p>
<p><strong>4. 论文中提到的局限性 (Any limitations mentioned in the paper)</strong></p>
<ul>
<li><strong>可解释性：</strong> 论文提到，一个潜在的局限性是潜在标记可能不如文本解释那样容易被人类理解。</li>
<li><strong>模型规模与数据：</strong> 未来工作可以考虑扩展到更大的模型，增加潜在标记的容量，并在更大的数据集上进行训练。</li>
</ul>
<p><strong>5. 潜在的未来研究方向 (Potential future research directions)</strong></p>
<ul>
<li><strong>模型规模扩展：</strong> 将LIVR应用于更大规模的模型。</li>
<li><strong>增加潜在标记容量：</strong> 探索增加潜在标记数量或维度以提升模型能力。</li>
<li><strong>更大规模数据集训练：</strong> 在更广泛、更多样化的数据集上进行训练。</li>
<li><strong>架构归纳偏置：</strong> 论文强调了通过架构归纳偏置（如LIVR）来增强视觉推理，而非仅依赖显式监督，这为未来研究提供了方向。</li>
</ul>
<hr />
<p><strong>总结：</strong></p>
<p>这篇论文的核心贡献是提出了<strong>Latent Implicit Visual Reasoning (LIVR)</strong> 方法，通过引入<strong>任务无关的潜在视觉推理标记</strong>和创新的<strong>视觉瓶颈机制</strong>，使得大型多模态模型（LMMs）能够在<strong>无需显式监督</strong>的情况下，自主地学习和利用视觉信息进行推理。LIVR克服了现有方法在标注成本、泛化能力和处理难以定义中间抽象任务方面的挑战，并在多项视觉密集型任务上取得了显著的性能提升，证明了其有效性和普适性。尽管存在可解释性方面的挑战，LIVR为提升LMMs的视觉推理能力提供了一个有前景且通用的新框架。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision.</li>
<li>Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21218v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21218v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.21209v1'></a></p>
<h2 id="human-motion-estimation-with-everyday-wearables"><a href="https://arxiv.org/abs/2512.21209v1">Human Motion Estimation with Everyday Wearables</a></h2>
<p><strong>Authors:</strong> Siqi Zhu, Yixuan Li, Junfu Li, Qi Wu, Zan Wang, Haozhe Ma, Wei Liang</p>
<p><strong>Published:</strong> 2025-12-24</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Human Motion Estimation with Everyday Wearables</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>该论文提出了一种名为 EveryWear 的新型人体运动估计方法，该方法完全依赖于日常可穿戴设备（智能手机、智能手表、耳机和配备摄像头的智能眼镜），无需显式校准即可实现轻量级且实用的全身运动捕捉。为了支持该研究，作者构建了一个名为 Ego-Elec 的大规模真实世界数据集，并采用了一种多模态教师-学生框架，融合了来自主观视角摄像头的视觉信息和消费级设备的惯性信号，从而有效解决了现有方法的穿戴不便、硬件昂贵和校准繁琐等问题。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>完全基于日常可穿戴设备：</strong> 这是最核心的创新点。论文摆脱了对专业运动捕捉设备（如 Vicon、OptiTrack 等）的依赖，而是巧妙地利用了人们日常生活中已经普遍拥有的设备。这极大地降低了门槛，使得运动捕捉技术能够真正融入日常生活。</li>
<li><strong>无需显式校准：</strong> 现有的大多数运动捕捉系统都需要耗时且繁琐的校准过程。EveryWear 的“即插即用”特性是其“实用性”的关键体现，极大地提升了用户体验和部署的便捷性。</li>
<li><strong>Ego-Elec 数据集：</strong> 构建一个大规模、多样化且包含真实世界场景的运动捕捉数据集是至关重要的。Ego-Elec 覆盖了 56 种日常活动和 17 种环境，并提供了 MoCap 级别的地面真实三维标注，这为该领域的研究和基准测试提供了宝贵的资源，尤其是在解决“模拟到真实”的鸿沟方面。</li>
<li><strong>多模态教师-学生框架：</strong> 论文采用了这种先进的训练范式，将来自不同传感器（egocentric cameras 和 inertial signals）的信息进行有效融合。<ul>
<li><strong>Egocentric Cameras (主观视角摄像头):</strong> 智能眼镜上的摄像头提供了第一人称视角下的视觉信息，这对于理解身体的相对运动和姿态至关重要。</li>
<li><strong>Inertial Signals (惯性信号):</strong> 来自智能手机、智能手表和耳机的 IMU（惯性测量单元）数据提供了关于加速度和角速度的信息，这些信息对于捕捉动态运动和姿态变化非常有用。</li>
<li><strong>Teacher-Student Framework:</strong> 这种框架通常用于知识蒸馏，其中一个更强大（可能是更复杂的模型或使用更精确的传感器）的“教师”模型将知识传递给一个更轻量级的“学生”模型。在这里，它可能意味着利用更精确的传感器（如 MoCap）或更复杂的模型来指导学生模型在日常可穿戴设备上的训练。</li>
</ul>
</li>
<li><strong>直接在真实世界数据上训练：</strong> 避免了模拟数据，直接在真实数据上训练模型，从而有效消除了“模拟到真实”（sim-to-real）的差距，这是许多基于模拟训练的计算机视觉方法面临的普遍挑战。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>普及运动捕捉技术：</strong> EveryWear 的方法有望将高精度的运动捕捉技术从专业实验室带入普通人的生活，极大地拓展其应用范围。</li>
<li><strong>推动 XR 交互的进步：</strong> 如摘要所述，XR（扩展现实）交互是该技术的重要驱动力。更自然、更低成本的全身运动捕捉将显著提升 XR 体验的沉浸感和交互性。</li>
<li><strong>降低研究门槛：</strong> 易于获取的硬件和无需复杂校准的特性，将吸引更多研究者进入运动估计领域，加速相关技术的创新。</li>
<li><strong>催生新的应用场景：</strong> 除了 XR，该技术还可以应用于虚拟健身、游戏、康复医疗、行为分析、人机交互等众多领域。</li>
<li><strong>推动多模态融合和轻量级模型的发展：</strong> 该研究展示了如何有效地融合来自不同类型消费级传感器的信息，并训练出在实际部署中表现良好的轻量级模型。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>扩展现实 (XR) / 虚拟现实 (VR) / 增强现实 (AR)：</strong> 实现更自然、更具沉浸感的虚拟形象控制和环境交互。</li>
<li><strong>游戏和娱乐：</strong> 允许玩家通过身体动作直接控制游戏角色，提升游戏体验。</li>
<li><strong>虚拟健身和运动分析：</strong> 实时监测用户的运动姿态，提供反馈和指导，帮助用户更有效地锻炼。</li>
<li><strong>康复医疗：</strong> 远程监测患者的运动恢复情况，提供个性化的康复方案。</li>
<li><strong>行为分析和人体姿态识别：</strong> 用于安防监控、人机交互、老年人跌倒检测等。</li>
<li><strong>动画和虚拟角色制作：</strong> 为艺术家提供更便捷的动作捕捉工具。</li>
<li><strong>智能家居和人机交互：</strong> 通过身体姿态和动作来控制智能设备。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>精度限制：</strong> 虽然论文声称“ outperforms baseline models”，但与专业的 MoCap 系统相比，基于日常可穿戴设备的运动估计在精度上可能仍存在一定差距，尤其是在捕捉精细的动作细节或快速、复杂的运动时。摘要中未明确说明其精度与专业 MoCap 系统的量化对比。</li>
<li><strong>传感器覆盖范围和鲁棒性：</strong> 智能眼镜上的摄像头数量有限（一个前置，两个下置），可能无法完全捕捉到所有身体部位的运动，例如背部或脚部。此外，在复杂的光照条件、遮挡或快速运动的情况下，摄像头的视觉信息可能会受到影响。</li>
<li><strong>惯性信号的累积误差：</strong> IMU 数据容易受到漂移和累积误差的影响，尤其是在长时间的运动中。虽然多模态融合可能有所缓解，但仍是一个潜在的挑战。</li>
<li><strong>“日常活动”的定义：</strong> 摘要中提到了“56 种日常活动”，但这些活动的复杂度和多样性可能仍有限。对于一些高度专业化或非常规的动作，该方法的表现可能需要进一步验证。</li>
<li><strong>用户体验的潜在问题：</strong> 尽管声称“wearability”，但长时间佩戴智能眼镜、智能手表、耳机和携带智能手机，对于某些用户来说可能仍然存在舒适度或便利性的问题。</li>
<li><strong>隐私问题：</strong> 使用带有摄像头的智能眼镜进行运动捕捉，可能会引发用户对隐私的担忧，尤其是在公共场合。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文的亮点在于其<strong>极高的实用性和普适性</strong>。通过巧妙地利用现有消费级可穿戴设备，并结合先进的多模态融合技术和真实世界数据训练，它有望打破运动捕捉技术的壁垒，使其真正走向大众。Ego-Elec 数据集的构建也为该领域的研究提供了坚实的基础。尽管可能存在精度和鲁棒性方面的挑战，但其提出的 EveryWear 方法无疑是人体运动估计领域一个令人兴奋的进展，预示着一个更加便捷和普及的运动捕捉时代的到来。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use.</li>
<li>We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction.</li>
<li>Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices.</li>
<li>Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.21209v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.21209v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-26 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
