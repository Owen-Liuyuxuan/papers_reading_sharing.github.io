<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-17 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-16/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-18/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-17">Arxiv Computer Vision Papers - 2025-09-17</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era" class="nav-link">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a>
                </li>
                <li class="nav-item">
                    <a href="#maps-for-autonomous-driving-full-process-survey-and-frontiers" class="nav-link">Maps for Autonomous Driving: Full-process Survey and Frontiers</a>
                </li>
                <li class="nav-item">
                    <a href="#deep-learning-for-3d-point-cloud-processing-from-approaches-tasks-to-its-implications-on-urban-and-environmental-applications" class="nav-link">Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</a>
                </li>
                <li class="nav-item">
                    <a href="#3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review" class="nav-link">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a>
                </li>
                <li class="nav-item">
                    <a href="#vi-safe-a-spatial-temporal-framework-for-efficient-violence-detection-in-public-surveillance" class="nav-link">Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</a>
                </li>
                <li class="nav-item">
                    <a href="#whu-stree-a-multi-modal-benchmark-dataset-for-street-tree-inventory" class="nav-link">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a>
                </li>
                <li class="nav-item">
                    <a href="#advancing-real-world-parking-slot-detection-with-large-scale-dataset-and-semi-supervised-baseline" class="nav-link">Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</a>
                </li>
                <li class="nav-item">
                    <a href="#using-kl-divergence-to-focus-frequency-information-in-low-light-image-enhancement" class="nav-link">Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</a>
                </li>
                <li class="nav-item">
                    <a href="#dual-stage-reweighted-moe-for-long-tailed-egocentric-mistake-detection" class="nav-link">Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#runge-kutta-approximation-and-decoupled-attention-for-rectified-flow-inversion-and-semantic-editing" class="nav-link">Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-17">Arxiv Computer Vision Papers - 2025-09-17</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月16日Arxiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解最新进展。</p>
<hr />
<p><strong>执行摘要：Arxiv 计算机视觉每日报告 (2025-09-16)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文集呈现出计算机视觉领域几个关键且相互关联的趋势：</p>
<ul>
<li><strong>具身智能与三维感知：</strong> 具身智能（Embodied AI）和自动驾驶是核心驱动力，对全向视觉、三维点云处理、三维人体姿态与形状估计以及高精度地图的需求日益增长。这表明研究正从纯粹的图像理解转向更复杂的、与物理世界交互的感知系统。</li>
<li><strong>数据驱动与基准建设：</strong> 多个工作致力于构建大规模、多模态或特定场景的数据集（如街景树木、停车位），以推动特定任务的进展。这反映了高质量数据在深度学习时代的重要性，以及对更真实世界场景的关注。</li>
<li><strong>效率与鲁棒性：</strong> 在实际应用中，对效率（如暴力检测）和鲁棒性（如低光照增强、长尾分布处理）的关注持续存在，研究人员正在探索新的模型架构和训练策略来解决这些挑战。</li>
<li><strong>生成模型与编辑：</strong> 扩散模型（Rectified Flow）在图像生成和语义编辑方面的应用显示出其在内容创作和图像操作领域的潜力。</li>
</ul>
<p><strong>2. 特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)：</strong> 这篇综述性论文非常及时且具有前瞻性。它系统地梳理了全向视觉在具身智能中的重要性、挑战和未来方向，对于理解该领域宏观发展至关重要。其创新性在于对一个新兴且关键领域的全面展望。</li>
<li><strong>"Maps for Autonomous Driving: Full-process Survey and Frontiers" (Pengxin Chen et al.)：</strong> 另一篇高质量的综述，深入探讨了自动驾驶地图的整个生命周期。考虑到自动驾驶的复杂性和对地图的依赖，这篇论文为研究人员提供了宝贵的知识体系和未来研究方向。</li>
<li><strong>"Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing" (Weiming Chen et al.)：</strong> 这篇论文在生成模型领域展现了技术创新。通过引入Runge-Kutta近似和解耦注意力，它提升了Rectified Flow模型在图像反演和语义编辑方面的性能，为高质量图像生成和操作提供了新的工具。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>全向视觉（Omnidirectional Vision）：</strong> 随着具身智能和VR/AR的发展，全向视觉将成为一个越来越重要的研究方向，涵盖数据采集、模型设计和应用。</li>
<li><strong>LiDAR点云在人体感知中的应用：</strong> "3D Human Pose and Shape Estimation from LiDAR Point Clouds" 指出LiDAR在隐私保护和全天候感知方面的优势，预示着LiDAR在人体感知领域的潜力。</li>
<li><strong>扩散模型（Rectified Flow）的精细化控制与应用：</strong> "Runge-Kutta Approximation and Decoupled Attention..." 展示了如何通过算法优化来提升扩散模型在特定任务（如语义编辑）上的表现，预示着未来对生成模型更精细化控制的研究。</li>
<li><strong>多模态融合与特定场景数据集：</strong> WHU-STree 和 Advancing Real-World Parking Slot Detection 等工作强调了为特定复杂场景构建多模态、大规模数据集的重要性，以及如何利用这些数据解决实际问题。</li>
</ul>
<p><strong>4. 建议完整阅读的论文：</strong></p>
<p>为了全面了解当前趋势和潜在突破，建议完整阅读以下论文：</p>
<ul>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)：</strong> 提供宏观视角，理解具身智能背景下的全向视觉。</li>
<li><strong>"Maps for Autonomous Driving: Full-process Survey and Frontiers" (Pengxin Chen et al.)：</strong> 深入了解自动驾驶核心技术之一的地图。</li>
<li><strong>"Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing" (Weiming Chen et al.)：</strong> 了解生成模型前沿技术及其在图像编辑中的应用。</li>
<li><strong>"Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications" (Zhenxin Zhang et al.)：</strong> 对3D点云处理的全面综述，对于理解三维感知基础至关重要。</li>
</ul>
<hr />
<p>这份摘要旨在为您的研究提供一个快速导航，帮助您优先关注与您研究方向最相关的最新进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></li>
<li><a href="#2509.12632v1">Maps for Autonomous Driving: Full-process Survey and Frontiers</a></li>
<li><a href="#2509.12452v1">Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</a></li>
<li><a href="#2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></li>
<li><a href="#2509.13210v1">Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</a></li>
<li><a href="#2509.13172v1">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a></li>
<li><a href="#2509.13133v1">Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</a></li>
<li><a href="#2509.13083v1">Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</a></li>
<li><a href="#2509.12990v1">Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</a></li>
<li><a href="#2509.12888v1">Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.12989v1'></a></p>
<h2 id="panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era"><a href="https://arxiv.org/abs/2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></h2>
<p><strong>Authors:</strong> Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Omnidirectional vision, using 360-degree vision to understand the
environment, has become increasingly critical across domains like robotics,
industrial inspection, and environmental monitoring. Compared to traditional
pinhole vision, omnidirectional vision provides holistic environmental
awareness, significantly enhancing the completeness of scene perception and the
reliability of decision-making. However, foundational research in this area has
historically lagged behind traditional pinhole vision. This talk presents an
emerging trend in the embodied AI era: the rapid development of omnidirectional
vision, driven by growing industrial demand and academic interest. We highlight
recent breakthroughs in omnidirectional generation, omnidirectional perception,
omnidirectional understanding, and related datasets. Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.
Moreover, we offer in-depth opinions related to emerging trends and
cross-community impacts at the intersection of panoramic vision and embodied
AI, along with the future roadmap and open challenges. This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供由Xu Zheng等人撰写的论文“PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era”的全面摘要。</p>
<hr />
<p><strong>论文摘要：PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决在具身AI时代，全向视觉（360度全景视觉）在理解环境方面日益增长的重要性与该领域基础研究长期落后于传统针孔视觉之间的差距。传统针孔视觉提供的是狭窄的、视锥受限的视角，而全向视觉能提供更全面的环境感知，这对于具身AI中更复杂的任务（如室内/室外导航）至关重要。论文探讨了如何克服数据瓶颈、模型能力限制和应用空白，以充分释放全向视觉在具身AI中的巨大潜力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>PANORAMA系统架构：</strong> 论文提出了一个理想的全景系统架构——PANORAMA，由四个关键子系统组成：
    *   <strong>数据采集与预处理（Data Acquisition &amp; Pre-processing）：</strong> 负责捕获原始全向数据并转换为计算处理格式，包括数据捕获、格式转换、同步与校准。
    *   <strong>感知（Perception）：</strong> 对预处理后的全景数据进行基础场景感知，利用专门的深度学习模型（如球面CNN、Transformer）提取丰富、结构化的信息，执行特征提取和环境感知（语义分割、目标检测、深度估计）。
    *   <strong>应用（Application）：</strong> 将感知洞察转化为具身AI智能体的行动，服务于特定下游任务，如导航与SLAM、人机交互、数字孪生与3D重建。
    *   <strong>加速与部署（Acceleration &amp; Employment）：</strong> 解决高分辨率全景数据处理的计算挑战，通过软件加速（模型量化、剪枝）和硬件部署（边缘计算平台）确保整个流程的计算可行性。
*   <strong>全向视觉技术突破的综合概述：</strong> 论文系统地总结了全向生成、全向感知、全向理解以及相关数据集的最新进展，突出了该领域在工业需求和学术兴趣驱动下的快速发展。
*   <strong>具身AI时代全向视觉的路线图：</strong> 论文提出了一个分阶段的未来路线图，包括数据集整合、多模态扩展、推理与具身数据、统一模型预训练、评估与基准测试、部署与泛化，旨在构建一个理想的、统一的全向任务模型。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>全向视觉的潜力：</strong> 论文强调全向视觉在机器人、工业检测和环境监测等领域的重要性，因为它提供了比针孔视觉更全面的环境感知，显著提升了场景感知的完整性和决策的可靠性。
*   <strong>挑战的系统性分类：</strong> 将全向视觉面临的问题归结为数据瓶颈、模型能力和应用空白三大类，为后续研究提供了清晰的框架。
*   <strong>技术进展的梳理：</strong> 详细介绍了全向生成（如Dream360、PanoDiffusion、OmniDrag）、全向感知（如GoodSAM、OmniSAM）和全向理解（如OSR-Bench、OmniVQA）的最新技术，展示了该领域在克服几何畸变、适应模型和构建数据集方面的努力。
*   <strong>数据集的全面回顾：</strong> 提供了室内、室外和无人机/飞行等领域23个代表性全向数据集的概述，涵盖了RGB全景图、深度、相机姿态和语义标签等模态，为研究人员提供了宝贵的资源。
*   <strong>PANORAMA架构的愿景：</strong> 提出的PANORAMA系统架构为具身AI中全向视觉的集成提供了一个全面的、端到端的解决方案，有望推动具身智能的发展。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据瓶颈：</strong> 全景图像的标注成本高昂，且由于等距矩形投影（ERP）等几何畸变，传统自动化标注工具效率低下，阻碍了大规模高质量数据集的开发。
*   <strong>模型能力限制：</strong> 现有预训练模型（主要针对针孔图像设计）的归纳偏置（如平移不变性）不适用于全景图像的畸变特性，导致性能显著下降。
*   <strong>应用空白：</strong> 缺乏跨学科人才以及现有全景数据和模型的不足，导致全景生产安全检查、全景森林火灾检测等特定应用领域探索不足。
*   <strong>泛化性和鲁棒性：</strong> 当前模型多专注于特定场景或投影方法，难以泛化到多样化的全景传感器规格、应用场景和投影方法。
*   <strong>动态畸变处理：</strong> 现有方法将全景图像的畸变视为与帧无关的几何问题，未能充分考虑真实世界场景中畸变的动态性和在全向视频序列中的演变。
*   <strong>缺乏大规模多模态预训练资源：</strong> 现有方法在模型泛化方面受限，缺乏大规模多模态预训练资源，阻碍了具身AI的广泛发展。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>数据集建设：</strong> 规划和发布大规模、多任务的全向数据集，涵盖真实世界场景的复杂性，包括室内外、通用和具身智能场景。
*   <strong>算法创新：</strong> 超越基于针孔模型的简单适配，创建具有全向信息的新颖架构和动态学习范式，以应对全向视觉的独特挑战。
*   <strong>应用探索：</strong> 探索和展示全向感知在真实世界机器人和交互系统中的优势，弥合实验室研究与实际应用之间的鸿沟。
*   <strong>泛化与鲁棒性：</strong> 开发能够泛化到不同全景传感器规格、应用场景和投影方法的模型，并利用投影无关表示和自监督学习技术从无标签全向信息中学习不变特征。
*   <strong>动态畸变处理：</strong> 明确考虑全向视频序列中畸变的动态性和时间一致性。
*   <strong>以行动为导向的表示学习：</strong> 使模型能够学习全景图像中以行动为导向的表示，从而实现更有效和高效的机器人决策。
*   <strong>可扩展的统一架构：</strong> 创建专门为全向视觉设计的统一、多任务基础模型，通过在大量全景数据上进行预训练，捕捉全向几何和语义的基本理解。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.</li>
<li>This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12989v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12989v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12632v1'></a></p>
<h2 id="maps-for-autonomous-driving-full-process-survey-and-frontiers"><a href="https://arxiv.org/abs/2509.12632v1">Maps for Autonomous Driving: Full-process Survey and Frontiers</a></h2>
<p><strong>Authors:</strong> Pengxin Chen, Zhipeng Luo, Xiaoqi Jiang, Zhangcai Yin, Jonathan Li</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Maps have always been an essential component of autonomous driving. With the
advancement of autonomous driving technology, both the representation and
production process of maps have evolved substantially. The article categorizes
the evolution of maps into three stages: High-Definition (HD) maps, Lightweight
(Lite) maps, and Implicit maps. For each stage, we provide a comprehensive
review of the map production workflow, with highlighting technical challenges
involved and summarizing relevant solutions proposed by the academic community.
Furthermore, we discuss cutting-edge research advances in map representations
and explore how these innovations can be integrated into end-to-end autonomous
driving frameworks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Pengxin Chen等人撰写的论文“Maps for Autonomous Driving: Full-process Survey and Frontiers”的全面摘要。</p>
<hr />
<p><strong>论文摘要：自动驾驶地图：全流程综述与前沿</strong></p>
<p><strong>1. 论文解决的主要问题或研究问题：</strong>
该论文旨在全面回顾自动驾驶地图（Maps for Autonomous Driving, MAD）的演变、生产流程、技术挑战以及未来发展方向。核心问题是，随着自动驾驶技术的发展，地图的表示和生产过程如何演变，以满足从传统导航到端到端自动驾驶系统不断增长的需求，并克服高成本、低更新频率和复杂性等限制。论文将地图演变分为三个阶段：高精地图（HD maps）、轻量化地图（Lite maps）和隐式地图（Implicit maps），并对每个阶段的生产流程、技术挑战和学术解决方案进行了深入分析。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要贡献在于其全面的视角和对地图演变阶段的划分，这在现有综述中是独有的。具体方法论贡献包括：
*   <strong>三阶段演进框架：</strong> 首次系统地将自动驾驶地图的演进划分为HD Map、Lite Map和Implicit Map三个关键阶段，并详细阐述了每个阶段的特点、技术需求和挑战。
*   <strong>HD Map阶段的全面回顾：</strong> 详细介绍了HD Map的生产流程（测绘、感知、地图编译），并深入探讨了定位（GNSS、IMU、轮式里程计、视觉/激光雷达里程计、地面控制点）、多行程测绘（协同SLAM、闭环检测、SD地图匹配）、静态感知（路面、路标、车道线、交通标志、护栏、杆状物、行道树提取）和拓扑生成（图搜索、深度学习、分层推理、多模态融合）等关键技术。
*   <strong>Lite Map阶段的重点关注：</strong> 强调了Lite Map作为HD Map的轻量化替代方案，在解决城市道路自动驾驶挑战中的作用。重点介绍了在线矢量化（单帧检测、长序列建模）、众包地图维护（在线变化检测、奖励路线、交通流轨迹挖掘）等创新方法。
*   <strong>Implicit Map阶段的前沿探索：</strong> 深入探讨了隐式地图作为端到端自动驾驶系统发展趋势的一部分，包括查询表示方法（生成式与目标导向、上下文条件、统一跨任务）、潜在空间方法（结构化场景表示、动态感知潜在建模、鲁棒性增强）、神经辐射场（NeRF）方法（场景重建、语义理解、效率与鲁棒性增强）和世界模型（环境表示、动态演化、模型增强）等。
*   <strong>工业视角的整合：</strong> 结合了作者在工业界的经验，提供了对地图发展里程碑和关键公司技术的洞察，使综述更具实践指导意义。
*   <strong>开源工作总结：</strong> 附录中提供了隐式地图和在线矢量化相关开源工作的列表，方便研究人员快速查阅。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>HD Map的奠基作用：</strong> HD Map通过提供厘米级精度的车道级信息，首次使L2-L4级高速公路自动驾驶成为可能，奠定了地图在自动驾驶中的基础地位。
*   <strong>Lite Map的实用性突破：</strong> Lite Map通过众包数据和自动化生成技术，显著降低了生产成本并提高了更新频率，使其能够覆盖城市道路，推动了自动驾驶的商业化落地。
*   <strong>Implicit Map的未来潜力：</strong> 隐式地图作为端到端自动驾驶系统的一部分，通过将环境知识隐式编码到神经网络中，有望实现更像人类的、上下文感知的决策制定，并促进可微分处理和反向传播，为联合学习系统提供支持。
*   <strong>地图在AD系统中不可或缺的地位：</strong> 论文强调，无论地图形式如何演变，它始终是自动驾驶系统中不可或缺的元素，从传统的模块化架构到现代的端到端学习系统，地图都扮演着关键角色。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>HD Map的局限性：</strong> 生产成本高昂、更新频率低、维护成本高，难以扩展到复杂的城市道路环境。
*   <strong>Lite Map的局限性：</strong> 尽管有所改进，但在线矢量化在地图精度和元素丰富度方面仍远未达到HD Map的水平，感知范围有限，且在复杂遮挡场景下性能受影响。众包维护面临数据传输量大、用户隐私和地图新鲜度等挑战。
*   <strong>Implicit Map的局限性：</strong>
    *   <strong>查询表示方法：</strong> 计算效率与表示能力之间存在权衡，依赖高质量输入，且缺乏正式的安全验证机制。跨模态接地存在模糊性，因果可解释性不足，时间鲁棒性有限。
    *   <strong>潜在空间方法：</strong> BEV投影受深度估计误差和遮挡伪影影响，图基方法依赖预定义拓扑模板，对象中心方法易受上游检测误差影响。动态感知潜在建模计算复杂，非线性运动建模不足，对数据稀疏性敏感。鲁棒性增强方法依赖高质量真实数据，对抗训练和域适应缺乏可量化鲁棒性边界，合成数据安全验证机制缺失。
    *   <strong>NeRF方法：</strong> 瞬态动态处理导致伪影，语义分辨率限制小物体识别，校准误差影响传感器融合。
    *   <strong>世界模型：</strong> 密集表示计算和内存开销大，对象中心方法易受上游感知模块误差传播影响，图基模型依赖定义关系的完整性，纯视觉模型在恶劣条件下鲁棒性有限。物理模型计算成本高，博弈论方法复杂且模型简化，生成模型推理速度慢且物理可控性差，贝叶斯方法计算密集且难以区分不确定性。模型增强方法验证范围有限，训练复杂且不稳定，元学习依赖元训练数据多样性，LLM模型推理延迟高、易“幻觉”，合成数据存在“现实鸿沟”。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>全自动轻量化地图（Fully Automatic Lite Map）：</strong> 持续改进Lite Map，实现数据采集、处理和更新的无缝集成，无需人工干预，以实现可扩展和经济高效的地图维护。
*   <strong>零样本地图学习（Zero-shot Map Learning）：</strong> 开发能够识别稀有或新型道路特征的模型，减少对标注数据的依赖，提高模型对未知环境的泛化能力，并结合不确定性估计以提高安全性和可靠性。
*   <strong>VLA中的隐式地图（Implicit Map in VLA）：</strong> 探索如何将地图经验存储在云端作为“记忆知识库”或“经验层”，以辅助车辆更好地理解驾驶场景，并支持VLA模型通过多模态训练数据隐式理解地图结构。
*   <strong>基础模型（Foundation Model）的应用：</strong> 利用基础模型学习潜在空间结构和语义上下文，实现多模态集成（视觉、语言、时间上下文），并通过Transformer的注意力机制动态检索空间和语义相关信息，从而支持隐式定位和地图构建。
*   <strong>混合建模方法：</strong> 结合结构化先验知识与神经网络，开发可解释的表示学习技术用于关键场景分析，并制定标准化评估指标以兼顾性能和安全要求。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>For each stage, we provide a comprehensive
review of the map production workflow, with highlighting technical challenges
involved and summarizing relevant solutions proposed by the academic community.</li>
<li>Furthermore, we discuss cutting-edge research advances in map representations
and explore how these innovations can be integrated into end-to-end autonomous
driving frameworks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12632v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12632v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12452v1'></a></p>
<h2 id="deep-learning-for-3d-point-cloud-processing-from-approaches-tasks-to-its-implications-on-urban-and-environmental-applications"><a href="https://arxiv.org/abs/2509.12452v1">Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</a></h2>
<p><strong>Authors:</strong> Zhenxin Zhang, Zhihua Xu, Yuwei Cao, Ningli Xu, Shuye Wang, Shen'ao Cui, Zhen Li, Rongjun Qin</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Point cloud processing as a fundamental task in the field of geomatics and
computer vision, has been supporting tasks and applications at different scales
from air to ground, including mapping, environmental monitoring, urban/tree
structure modeling, automated driving, robotics, disaster responses etc. Due to
the rapid development of deep learning, point cloud processing algorithms have
nowadays been almost explicitly dominated by learning-based approaches, most of
which are yet transitioned into real-world practices. Existing surveys
primarily focus on the ever-updating network architecture to accommodate
unordered point clouds, largely ignoring their practical values in typical
point cloud processing applications, in which extra-large volume of data,
diverse scene contents, varying point density, data modality need to be
considered. In this paper, we provide a meta review on deep learning approaches
and datasets that cover a selection of critical tasks of point cloud processing
in use such as scene completion, registration, semantic segmentation, and
modeling. By reviewing a broad range of urban and environmental applications
these tasks can support, we identify gaps to be closed as these methods
transformed into applications and draw concluding remarks in both the
algorithmic and practical aspects of the surveyed methods.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文“Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications”由Zhenxin Zhang等人撰写，对深度学习在3D点云处理及其在城市和环境应用中的作用进行了全面的元综述。</p>
<p>以下是该论文的摘要：</p>
<ol>
<li>
<p><strong>主要问题或研究问题：</strong>
    该论文旨在解决现有关于点云处理深度学习综述的不足。现有综述主要关注不断更新的网络架构以适应无序点云，但往往忽略了这些方法在实际点云处理应用中的实用价值，尤其是在处理超大数据量、多样场景内容、不同点云密度和数据模态时。因此，该研究的核心问题是：如何提供一个全面的元综述，连接深度学习算法与实际点云处理任务及其在城市和环境应用中的潜在价值，并识别这些方法在转化为实际应用时需要弥补的差距。</p>
</li>
<li>
<p><strong>关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>元综述方法：</strong> 论文采用元综述的方法，而非仅仅关注最新的网络架构，而是将深度学习方法和数据集与点云处理的关键任务（如场景补全、配准、语义分割和建模）联系起来。</li>
<li><strong>数据源的全面概述：</strong> 详细介绍了各种点云数据采集方法，包括LiDAR、摄影测量、结构光系统、光度立体、SAR干涉测量和混合系统，并分析了它们的特点、成本、复杂性和精度。</li>
<li><strong>任务与应用的连接：</strong> 论文明确地将点云处理任务与广泛的城市和环境应用（如城市建模、林业、农业、生态学和公用事业测绘）联系起来，突出了深度学习在这些领域中的实际支持作用。</li>
<li><strong>识别差距和挑战：</strong> 论文不仅总结了现有方法的成就，还明确指出了这些方法在推广到实际应用中时面临的挑战，例如泛化能力、处理大规模数据、计算效率和模型可解释性。</li>
</ul>
</li>
<li>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>深度学习在点云处理中的主导地位：</strong> 论文强调，深度学习方法已几乎完全主导了点云处理算法，显著提升了场景补全、配准、语义分割和几何建模等任务的性能。</li>
<li><strong>广泛的应用潜力：</strong> 深度学习在点云处理中的应用范围广泛，从智能城市基础设施管理、能源管理、文化遗产保护、道路网络测绘到灾害管理、林业、农业和生态监测等，都展现出巨大的潜力。</li>
<li><strong>方法论的同质性：</strong> 尽管点云任务多样，但特征提取过程大多遵循基于体素、基于视图和基于点的方法，这表明研究人员和开发者可以在不同任务之间利用相似的算法和概念。</li>
<li><strong>性能提升：</strong> 现有工作表明，不断更新的网络架构和共享训练数据的增加，使得深度学习在基准数据集上的性能得到了显著提升。</li>
</ul>
</li>
<li>
<p><strong>局限性：</strong></p>
<ul>
<li><strong>泛化能力不足：</strong> 尽管在基准数据集上表现良好，但这些方法在应用于未见过的数据集时，其泛化能力和性能一致性尚未得到充分验证。</li>
<li><strong>大规模数据处理挑战：</strong> 现有方法在处理地理空间尺度上的超大数据量时，计算效率和内存消耗仍然是主要挑战，尤其是在精细配准任务中。</li>
<li><strong>模型可解释性：</strong> 深度学习模型的“黑箱”性质阻碍了其在智能城市控制系统等领域的解释性和信任度。</li>
<li><strong>数据质量和可用性：</strong> 模型的准确性和完整性高度依赖于高质量和可用的大规模训练数据集，这在实际应用中可能难以获得。</li>
<li><strong>手动工作量：</strong> 即使有深度学习的辅助，许多建模过程（如LoD3模型的生成）仍然需要大量手动工作。</li>
</ul>
</li>
<li>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提升泛化能力：</strong> 未来的研究应侧重于开发具有更强泛化能力的算法，使其在应用于不同领域和场景的未见过数据集时能保持一致的性能。</li>
<li><strong>优化大规模数据处理：</strong> 需要开发更高效的算法和架构，以处理地理空间尺度上的超大数据量，并优化计算效率和内存使用。</li>
<li><strong>增强模型可解释性：</strong> 探索提高深度学习模型可解释性的方法，以增强用户对模型的信任和理解，尤其是在关键决策领域。</li>
<li><strong>多源数据融合：</strong> 进一步研究如何有效融合多源点云数据（如LiDAR、图像、多光谱数据等），以获取更丰富、更全面的场景信息。</li>
<li><strong>自动化与实时应用：</strong> 推动算法向更高自动化水平发展，并优化推理时间，以支持实时交通管理和灾害响应等应用。</li>
<li><strong>填补数据差距：</strong> 解决数据稀缺和专有数据格式问题，以促进高质量训练数据集的创建和共享。</li>
</ul>
</li>
</ol>
<p>总而言之，这篇论文为深度学习在3D点云处理领域的应用提供了一个全面的视角，强调了其在城市和环境应用中的巨大潜力，同时也明确指出了当前方法在实际部署中面临的挑战和未来研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we provide a meta review on deep learning approaches
and datasets that cover a selection of critical tasks of point cloud processing
in use such as scene completion, registration, semantic segmentation, and
modeling.</li>
<li>By reviewing a broad range of urban and environmental applications
these tasks can support, we identify gaps to be closed as these methods
transformed into applications and draw concluding remarks in both the
algorithmic and practical aspects of the surveyed methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12452v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12452v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12197v1'></a></p>
<h2 id="3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review"><a href="https://arxiv.org/abs/2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></h2>
<p><strong>Authors:</strong> Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet撰写的论文“3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review”的全面摘要。</p>
<hr />
<p><strong>论文摘要：3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</strong></p>
<p>这篇论文对从“野外”LiDAR点云中进行3D人体姿态估计（3D HPE）和人体网格恢复（HMR）的现有方法进行了全面回顾。随着LiDAR传感器在自动驾驶和机器人领域的普及，以及相关数据集的发布，利用LiDAR数据进行人体理解成为一个日益重要的研究方向。</p>
<p><strong>1. 主要问题或研究问题：</strong>
论文主要关注如何从稀疏、不规则、可能存在遮挡和噪声的“野外”LiDAR点云中准确地估计3D人体姿态和恢复详细的人体网格。传统基于图像和视频的方法在深度信息、隐私保护和鲁棒性方面存在局限性，而LiDAR虽然提供了精确的3D几何信息，但也带来了自身的数据挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>结构化分类法：</strong> 论文提出了一个结构化的分类法，用于对现有的3D HPE和HMR方法进行分类，并分析了它们的优势、局限性和设计选择。该分类法基于学习范式（监督、弱监督、无监督）、输入模态（仅LiDAR或多模态融合）和网络架构（如Transformer、PointNet变体等）。
*   <strong>方法分析：</strong> 详细分析了32项2019年至2025年间发表的研究，涵盖了稀疏性处理、Transformer作为骨干网络、超越3D姿态的辅助监督任务、合成数据学习、弱监督下的标注鸿沟弥合以及多模态融合等关键方面。
*   <strong>数据集定量比较：</strong> 对Waymo Open Dataset、SLOPER4D和Human-M3这三个最广泛使用的LiDAR人体姿态数据集进行了定量比较，详细阐述了它们的特性、采集方式、数据格式和内在属性（如点云密度、人体-传感器距离、3D姿态多样性等）。
*   <strong>统一评估指标：</strong> 整理并统一了所有用于评估LiDAR点云3D HPE和HMR方法的评估指标定义，包括MPJPE、PA-MPJPE、PCK、PEM、MPVPE、MPERE、ADE、LAE、LLE、Accel Error和Chamfer Distance等。
*   <strong>基准测试表：</strong> 建立了在这三个数据集上针对3D HPE和HMR任务的基准测试表，旨在促进公平比较和领域进展。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文通过对现有方法的深入分析和基准测试，揭示了LiDAR-based 3D HPE和HMR领域的最新进展和挑战。研究表明，多模态融合（特别是LiDAR与RGB图像或IMU的融合）和利用合成数据进行预训练是解决数据稀疏性和标注不足的关键策略。Transformer架构因其全局感受野和处理不规则数据的能力，在LiDAR-based任务中表现出强大潜力。弱监督方法通过2D伪标签、投影一致性和辅助任务有效弥合了标注鸿沟。这些发现为研究人员提供了清晰的路线图，以理解当前技术水平并指导未来研究方向。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据稀缺性：</strong> 缺乏大规模、高质量标注的LiDAR数据集是核心挑战。
*   <strong>对辅助模态的依赖：</strong> 多数弱监督方法仍严重依赖RGB图像或IMU信号等辅助数据模态。
*   <strong>合成数据生成：</strong> 当前合成数据生成管道存在领域不匹配（室内姿态与室外场景）和真实性差距（未能完全捕捉真实LiDAR传感器的噪声、稀疏性和视角特性）。
*   <strong>相机参数依赖：</strong> 多数多模态方法高度依赖精确的相机参数进行2D-3D对应，这在实际应用中带来了挑战。
*   <strong>传感器域差异：</strong> 不同LiDAR传感器特性（点云密度、范围、噪声模式）导致模型在不同数据集之间泛化能力差。
*   <strong>扫描模式差异：</strong> 不同LiDAR扫描模式（NRS和RMB）产生结构不同的点云分布，需要鲁棒的架构或适应策略。
*   <strong>弱监督HMR的探索不足：</strong> 相比3D HPE，弱监督HMR方法的研究相对较少。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>减少对辅助模态的依赖：</strong> 探索仅使用LiDAR数据实现弱监督HPE和HMR的方法，例如通过伪标签、自训练或对比学习。
*   <strong>整合时间信息：</strong> 利用LiDAR帧序列的时间连贯性，提取时间线索以增强姿态估计精度，而无需额外监督。
*   <strong>更真实的合成数据生成：</strong> 发展能够直接从真实世界分布中生成合成LiDAR数据的方法，特别是利用扩散模型等生成式模型。
*   <strong>数据高效学习：</strong> 预训练模型并以最少监督进行微调，以提高LiDAR-based HPE和HMR的数据效率。
*   <strong>弱监督3D HMR：</strong> 深入探索专门针对HMR的弱监督方法。
*   <strong>消除相机参数依赖：</strong> 开发可学习模块，实现端到端对齐，无需显式校准。
*   <strong>域适应技术：</strong> 针对不同LiDAR传感器特性和扫描模式之间的域差异，开发域适应技术，以提高模型的泛化能力。</p>
<hr />
<p>这篇综述论文为LiDAR-based 3D人体理解领域提供了一个全面的概览，不仅系统地分类和分析了现有方法，还通过量化比较和基准测试为未来的研究奠定了基础，并指明了关键的开放挑战和有前景的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds.</li>
<li>Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12197v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12197v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13210v1'></a></p>
<h2 id="vi-safe-a-spatial-temporal-framework-for-efficient-violence-detection-in-public-surveillance"><a href="https://arxiv.org/abs/2509.13210v1">Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</a></h2>
<p><strong>Authors:</strong> Ligang Chang, Shengkai Xu, Liangchang Shen, Binhan Xu, Junqiao Wang, Tianyu Shi, Yanhui Du</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV, I.2.10; I.4.8</p>
<p><strong>Abstract:</strong></p>
<p>Violence detection in public surveillance is critical for public safety. This
study addresses challenges such as small-scale targets, complex environments,
and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal
framework that integrates an enhanced YOLOv8 with a Temporal Segment Network
(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as
a lightweight backbone, an exponential moving average (EMA) attention
mechanism, and pruning to reduce computational cost while maintaining accuracy.
YOLOv8 and TSN are trained separately on pedestrian and violence datasets,
where YOLOv8 extracts human regions and TSN performs binary classification of
violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE
achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming
existing methods in both accuracy and efficiency, demonstrating its
effectiveness for public safety surveillance. Code is available at
https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ligang Chang等人撰写的论文“Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance”的全面摘要。</p>
<hr />
<h3 id="vi-safe">论文《Vi-SAFE: 一种用于公共监控中高效暴力检测的时空框架》摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决公共监控中高效暴力行为检测的挑战。具体来说，它关注于如何克服小目标、复杂环境以及实时时序分析的难题，以提高公共安全水平。现有的方法往往在准确性、计算效率或实时性方面存在不足，尤其是在资源受限的边缘设备上部署时。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
Vi-SAFE框架的核心创新在于其独特的时空集成方法和对YOLOv8模型的优化：
*   <strong>时空集成框架 (Vi-SAFE)：</strong> 论文提出了一种新颖的端到端时空框架，将优化的YOLOv8s目标检测器与时序分割网络 (TSN) 相结合。YOLOv8s负责在视频帧中准确地定位潜在暴力活动中的个体（即提取人体区域），而TSN则对这些感兴趣区域 (ROIs) 的时序动态进行建模，以判断暴力行为。这种双2D CNN结构在实现高准确性的同时，保持了较低的计算复杂度。
*   <strong>YOLOv8s模型优化 (GE-YOLOv8)：</strong>
    *   <strong>轻量级骨干网络：</strong> 将YOLOv8s的骨干网络替换为GhostNetV3，通过廉价的线性操作生成额外的特征图，显著减少了FLOPs和参数，同时保持了准确的特征表示。
    *   <strong>EMA注意力机制：</strong> 在骨干网络的后期层中引入了指数移动平均 (EMA) 注意力机制，通过重新加权特征图来突出信息区域并抑制背景噪声，从而提高对小目标或遮挡目标的检测，并增强复杂场景中的时序一致性。
    *   <strong>剪枝技术：</strong> 应用基于GroupNorm重要性的通道剪枝技术，进一步压缩模型，在保持准确性的同时降低了参数和GFLOPs。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>GE-YOLOv8的性能：</strong> 经过GhostNetV3、EMA注意力机制和剪枝优化后的GE-YOLOv8模型，在保持高准确性（mAP 0.737）的同时，显著减少了参数（减少约一半）和GFLOPs（减少约一半），使其非常适合实时边缘部署。
*   <strong>Vi-SAFE的整体性能：</strong> 在RWF-2000数据集上的实验表明，Vi-SAFE实现了0.88的准确率，显著优于单独的TSN (0.77) 以及其他主流方法，如U-Net+LSTM (0.820) 和C3D (0.828)，甚至略微超越了Openpose+ST-GCN (0.878)。
*   <strong>意义：</strong> 这些结果证明了Vi-SAFE在公共安全监控中进行暴力检测的有效性、高效性和鲁棒性。其模块化架构也确保了可扩展性，便于集成额外的模块以适应更广泛的应用场景。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确指出当前Vi-SAFE框架的局限性，但从未来研究方向可以推断出一些潜在的改进空间：
*   <strong>泛化能力：</strong> 尽管Vi-SAFE在RWF-2000数据集上表现出色，但其在更复杂、更多样化的环境中的泛化能力仍有待进一步验证。
*   <strong>优化策略：</strong> 尽管已经进行了模型优化，但可能仍有进一步提升效率和准确性的空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出了以下未来研究方向：
*   <strong>评估更多样化的数据集：</strong> 在更复杂、更多样化的环境中评估Vi-SAFE的泛化能力。
*   <strong>探索先进的优化策略：</strong> 进一步研究和应用先进的优化策略，以持续提高模型的效率和性能。</p>
<hr />
<p>这份摘要突出了Vi-SAFE框架在解决公共监控中暴力检测问题上的创新性，特别是在平衡准确性和计算效率方面的贡献，使其成为边缘设备部署的有力候选。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Vi-SAFE, a spatial-temporal
framework that integrates an enhanced YOLOv8 with a Temporal Segment Network
(TSN) for video surveillance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13210v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13210v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13172v1'></a></p>
<h2 id="whu-stree-a-multi-modal-benchmark-dataset-for-street-tree-inventory"><a href="https://arxiv.org/abs/2509.13172v1">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a></h2>
<p><strong>Authors:</strong> Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Street trees are vital to urban livability, providing ecological and social
benefits. Establishing a detailed, accurate, and dynamically updated street
tree inventory has become essential for optimizing these multifunctional assets
within space-constrained urban environments. Given that traditional field
surveys are time-consuming and labor-intensive, automated surveys utilizing
Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing
MMS-acquired tree datasets are limited by small-scale scene, limited
annotation, or single modality, restricting their utility for comprehensive
analysis. To address these limitations, we introduce WHU-STree, a cross-city,
richly annotated, and multi-modal urban street tree dataset. Collected across
two distinct cities, WHU-STree integrates synchronized point clouds and
high-resolution images, encompassing 21,007 annotated tree instances across 50
species and 2 morphological parameters. Leveraging the unique characteristics,
WHU-STree concurrently supports over 10 tasks related to street tree inventory.
We benchmark representative baselines for two key tasks--tree species
classification and individual tree segmentation. Extensive experiments and
in-depth analysis demonstrate the significant potential of multi-modal data
fusion and underscore cross-domain applicability as a critical prerequisite for
practical algorithm deployment. In particular, we identify key challenges and
outline potential future works for fully exploiting WHU-STree, encompassing
multi-modal fusion, multi-task collaboration, cross-domain generalization,
spatial pattern learning, and Multi-modal Large Language Model for street tree
asset management. The WHU-STree dataset is accessible at:
https://github.com/WHU-USI3DV/WHU-STree.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ruifei Ding等人撰写的论文“WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory”的全面摘要。</p>
<hr />
<p><strong>论文摘要：WHU-STree：用于行道树盘点任务的多模态基准数据集</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现有行道树数据集在规模、标注丰富性和模态单一性方面的局限性，这些局限性阻碍了对城市行道树进行全面分析和自动化盘点。传统的实地调查耗时费力，而现有的移动测绘系统（MMS）获取的树木数据集往往场景规模小、标注不完整或仅限于单一模态，无法支持复杂的分析任务。因此，研究者们需要一个大规模、多模态、标注丰富的行道树数据集，以推动基于深度学习的自动化行道树盘点算法的发展。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
该论文的核心贡献是引入了WHU-STree数据集，其关键创新和方法学贡献包括：
*   <strong>跨城市、多模态数据集成：</strong> WHU-STree数据集首次整合了来自中国两个气候区和城市规划差异显著的城市（南京和沈阳）的同步点云和高分辨率图像数据。这种跨城市覆盖有助于评估算法的泛化能力和鲁棒性。
*   <strong>丰富的标注：</strong> 数据集包含21,007个标注的树木实例，涵盖50种树木物种和2个形态参数（树高和胸径）。这些详细标注支持多种任务，包括树种分类、单棵树分割和形态参数估计。
*   <strong>多任务支持：</strong> 凭借其独特的特性，WHU-STree能够同时支持超过10个与行道树盘点相关的任务，包括单模态或多模态输入、单任务或多任务学习、以及跨区域或区域内评估。这为开发综合性行道树盘点解决方案提供了基础。
*   <strong>基准测试与分析：</strong> 论文对树种分类和单棵树分割这两个关键任务的代表性基线算法进行了基准测试，并提供了深入的实验分析，强调了多模态数据融合和跨领域适用性的重要性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>多模态融合的优越性：</strong> 实验结果表明，多模态方法（如TSCMDL）在树种分类任务中显著优于单一模态方法，尤其是在mIoU指标上有所提升（例如，TSCMDL比PointMLP提高了2.49%）。这强调了利用图像的纹理和光谱信息作为点云几何信息的补充，对于提高分类准确性的重要性。
*   <strong>跨领域泛化能力：</strong> 在跨城市评估中，所有算法都表现出稳健的性能，尤其是在WHU-STree-SY数据集上。多模态方法LCPS在F1分数上甚至超越了单模态方法，这表明多模态融合有助于缓解点云数据中的领域漂移，提高了算法在异构城市环境中的鲁棒性。
*   <strong>多任务协作的潜力：</strong> 初步实验表明，将树种分类整合到树木分割任务中，可以提高网络的细粒度特征捕获能力，从而间接改善整体分割效果（F1分数从69.2%提高到82.2%）。这为开发端到端的多任务学习框架提供了有力证据。
*   <strong>挑战与机遇：</strong> 尽管取得了进展，但算法在处理家具干扰、树冠重叠和块合并失败等复杂场景时仍面临挑战。最高的mIoU仅为64.09%，表明区分特定树种仍是一个难题。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>现有数据集的局限：</strong> 论文指出，现有的MMS获取的树木数据集通常场景规模小、标注有限或仅限于单一模态，限制了其在全面分析中的实用性。
*   <strong>算法性能的局限：</strong> 尽管多模态融合有所改进，但现有算法的整体性能仍不足以满足实际部署的需求。例如，树种分类的最高mIoU仅为64.09%，表明在区分特定树种方面仍存在显著挑战。
*   <strong>特定场景的挑战：</strong> 算法在处理城市道路家具干扰、密集树冠重叠导致的欠分割以及块合并失败等复杂场景时仍存在问题。
*   <strong>跨领域泛化的挑战：</strong> 尽管WHU-STree-NJ数据集的丰富性有助于泛化，但不同城市间树种组成和形态的显著差异仍可能限制检测的泛化能力。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>多模态融合策略：</strong> 进一步研究任务特定的多模态融合策略，以更好地利用点云和图像数据之间的互补优势，解决投影误差和多视角不一致等问题。
*   <strong>多任务协作框架：</strong> 开发端到端的多任务学习模型，同时执行分类、分割和参数估计，促进跨任务知识迁移，并支持更全面的行道树盘点。
*   <strong>跨领域泛化：</strong> 探索能够处理领域漂移和提高算法在异构城市环境中鲁棒性的方法，例如开放词汇识别和新类别发现。未来将扩展WHU-STree到更多城市，并增加沈阳数据集的树种标注。
*   <strong>空间模式学习：</strong> 结合行道树的空间分布模式（如线性排列、种植间隔、拓扑关系）作为先验知识，以提高分割和分类性能，并支持城市绿化规划。
*   <strong>多模态大语言模型（MLLM）应用：</strong> 利用MLLM的推理和跨模态交互能力，整合专家知识和政府政策，构建一个无缝的“感知-分析-决策”闭环框架，实现智能化的行道树资产管理，包括健康状况评估、维护需求和风险评估。</p>
<hr />
<p>这份摘要突出了WHU-STree数据集作为计算机视觉领域新颖且重要的基准，为解决城市行道树盘点中的实际挑战提供了坚实基础，并为未来的多模态、多任务和跨领域研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we introduce WHU-STree, a cross-city,
richly annotated, and multi-modal urban street tree dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13172v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13172v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13133v1'></a></p>
<h2 id="advancing-real-world-parking-slot-detection-with-large-scale-dataset-and-semi-supervised-baseline"><a href="https://arxiv.org/abs/2509.13133v1">Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</a></h2>
<p><strong>Authors:</strong> Zhihao Zhang, Chunyu Lin, Lang Nie, Jiyuan Wang, Yao Zhao</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As automatic parking systems evolve, the accurate detection of parking slots
has become increasingly critical. This study focuses on parking slot detection
using surround-view cameras, which offer a comprehensive bird's-eye view of the
parking environment. However, the current datasets are limited in scale, and
the scenes they contain are seldom disrupted by real-world noise (e.g., light,
occlusion, etc.). Moreover, manual data annotation is prone to errors and
omissions due to the complexity of real-world conditions, significantly
increasing the cost of annotating large-scale datasets. To address these
issues, we first construct a large-scale parking slot detection dataset (named
CRPS-D), which includes various lighting distributions, diverse weather
conditions, and challenging parking slot variants. Compared with existing
datasets, the proposed dataset boasts the largest data scale and consists of a
higher density of parking slots, particularly featuring more slanted parking
slots. Additionally, we develop a semi-supervised baseline for parking slot
detection, termed SS-PSD, to further improve performance by exploiting
unlabeled data. To our knowledge, this is the first semi-supervised approach in
parking slot detection, which is built on the teacher-student model with
confidence-guided mask consistency and adaptive feature perturbation.
Experimental results demonstrate the superiority of SS-PSD over the existing
state-of-the-art (SoTA) solutions on both the proposed dataset and the existing
dataset. Particularly, the more unlabeled data there is, the more significant
the gains brought by our semi-supervised scheme. The relevant source codes and
the dataset have been made publicly available at
https://github.com/zzh362/CRPS-D.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zhihao Zhang等人的论文“Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline”的全面摘要。</p>
<hr />
<h3 id="advancing-real-world-parking-slot-detection-with-large-scale-dataset-and-semi-supervised-baseline_1">论文摘要：Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决自动泊车系统中停车位检测的准确性问题，尤其是在真实世界复杂场景下的挑战。现有数据集规模有限，且缺乏真实世界的噪声（如光照、遮挡等），导致模型泛化能力不足。此外，手动标注大规模数据集成本高昂且易出错。因此，核心问题是如何在复杂多变的真实世界环境中，实现高精度、鲁棒的停车位检测，并有效利用未标注数据以降低标注成本。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>CRPS-D大规模数据集：</strong> 论文构建了一个名为CRPS-D的大规模真实世界停车位检测数据集。该数据集在规模上远超现有数据集，包含更多样化的光照分布、天气条件和更具挑战性的停车位变体（特别是更多倾斜停车位），场景密度更高，旨在更好地反映真实世界的复杂性。
*   <strong>SS-PSD半监督基线模型：</strong> 首次提出了用于停车位检测的半监督方法SS-PSD（Semi-Supervised Parking Slot Detection）。该模型基于教师-学生（Teacher-Student）框架，并引入了两项关键创新：
    *   <strong>置信度引导掩码一致性（Confidence-Guided Mask Consistency, CGM）：</strong> 针对未标注数据，通过可训练的置信度图为不同区域分配权重，并掩盖低置信度区域，以确保预测一致性，避免潜在错误预测的误导。
    *   <strong>自适应特征扰动（Adaptive Feature Perturbation, Adaptive-VAT）：</strong> 引入了一种自适应选择性的特征扰动机制，根据教师或学生模型对扰动的鲁棒性，生成更强但合理的对抗性噪声，以促进学生模型的有效训练。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据集优势：</strong> CRPS-D数据集在图像和停车位数量上远超现有数据集，且停车位密度更高，倾斜停车位比例显著增加（11.75%），为真实世界停车位检测提供了更具挑战性和代表性的基准。
*   <strong>SS-PSD性能优越性：</strong> 实验结果表明，SS-PSD在CRPS-D和现有数据集上均超越了现有最先进（SoTA）的全监督解决方案。尤其是在标注数据较少的情况下（例如1/24的标注比例），SS-PSD相较于DMPR-PS和GCN在APparking-slot上分别取得了19.02%和16.49%的显著提升。
*   <strong>半监督学习的有效性：</strong> 论文强调，SS-PSD能够有效利用未标注数据，随着未标注数据量的增加，性能提升越显著，证明了其在标签稀缺场景下的实用性和可扩展性。
*   <strong>组件有效性：</strong> 消融实验证实了CGM一致性和Adaptive-VAT对模型性能的积极贡献，CGM一致性使APparking-slot提升了3.60%，Adaptive-VAT进一步提升了0.82%。</p>
<p><strong>4. 论文中提及的局限性：</strong>
尽管SS-PSD模型在大多数场景下表现出色，但在极端环境条件下仍存在局限性。论文中列举了以下失败案例：
*   <strong>复杂光照条件：</strong> 地下车库中光照不足、阴影和地面反射会导致误检。
*   <strong>户外眩光：</strong> 强烈的户外眩光会影响停车位检测。
*   <strong>恶劣天气（大雨）：</strong> 降低对比度、表面反射和部分遮挡的停车位标记会降低检测准确性。
*   <strong>标记磨损：</strong> 褪色或严重损坏的停车位标记难以辨别，导致漏检。
*   <strong>严重遮挡：</strong> 车辆或其他物体遮挡停车位视图时，模型性能下降。
*   <strong>夜间场景：</strong> 有限的光照和传感器噪声显著降低视觉清晰度，导致漏检。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展功能：</strong> 将模型扩展到包含占用检测，以判断停车位是否被占用或空闲，从而构建更全面实用的自动泊车系统。
*   <strong>多源数据融合：</strong> 结合其他数据源，如视频序列中的时间线索或传感器融合，以提高模型在恶劣条件下的鲁棒性。
*   <strong>V2X技术集成：</strong> 探索将V2X技术（如RSU辅助的协同定位）集成到系统中，以增强在GNSS受限环境（如地下停车场和城市峡谷）中的系统鲁棒性，可能同时改善定位和检测性能。
*   <strong>模型优化：</strong> 进一步优化模型以处理更广泛的环境条件，并扩展其功能以支持其他交通相关应用。</p>
<hr />
<p>这篇论文通过引入大规模、高挑战性的CRPS-D数据集和创新的SS-PSD半监督学习框架，为真实世界停车位检测领域做出了重要贡献。其半监督方法在标签稀缺场景下表现出显著优势，为未来自动泊车系统的发展提供了有力的技术支持和研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Additionally, we develop a semi-supervised baseline for parking slot
detection, termed SS-PSD, to further improve performance by exploiting
unlabeled data.</li>
<li>Experimental results demonstrate the superiority of SS-PSD over the existing
state-of-the-art (SoTA) solutions on both the proposed dataset and the existing
dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13133v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13133v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13083v1'></a></p>
<h2 id="using-kl-divergence-to-focus-frequency-information-in-low-light-image-enhancement"><a href="https://arxiv.org/abs/2509.13083v1">Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</a></h2>
<p><strong>Authors:</strong> Yan Xingyang, Huang Xiaohong, Zhang Zhao, You Tian, Xu Ziheng</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In the Fourier domain, luminance information is primarily encoded in the
amplitude spectrum, while spatial structures are captured in the phase
components. The traditional Fourier Frequency information fitting employs
pixel-wise loss functions, which tend to focus excessively on local information
and may lead to global information loss. In this paper, we present LLFDisc, a
U-shaped deep enhancement network that integrates cross-attention and gating
mechanisms tailored for frequency-aware enhancement. We propose a novel
distribution-aware loss that directly fits the Fourier-domain information and
minimizes their divergence using a closed-form KL-Divergence objective. This
enables the model to align Fourier-domain information more robustly than with
conventional MSE-based losses. Furthermore, we enhance the perceptual loss
based on VGG by embedding KL-Divergence on extracted deep features, enabling
better structural fidelity. Extensive experiments across multiple benchmarks
demonstrate that LLFDisc achieves state-of-the-art performance in both
qualitative and quantitative evaluations. Our code will be released at:
https://github.com/YanXY000/LLFDisc</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Yan Xingyang等人撰写的论文“Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement”的全面摘要。</p>
<hr />
<h3 id="using-kl-divergence-to-focus-frequency-information-in-low-light-image-enhancement_1">论文《Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement》全面摘要</h3>
<p><strong>1. 论文主要问题或研究问题：</strong>
低光照图像增强（LLIE）是一个核心的计算机视觉任务，但现有方法在处理低光照图像时存在局限性。具体来说，传统的基于傅里叶频率信息的增强方法通常采用像素级损失函数（如MSE），这导致模型过度关注局部信息，可能忽略全局频率分布，从而影响增强图像的整体质量和结构保真度。此外，现有的傅里IE增强方法未能充分利用振幅信息，并且在空间域特征提取时容易放大噪声，而基于Retinex的方法可能导致纹理失真和泛化能力差。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了一个名为LLFDisc的U形深度增强网络，并引入了多项关键创新：</p>
<ul>
<li><strong>傅里叶KL散度损失（Fourier KL-Divergence Loss）：</strong> 针对传统像素级损失的局限性，作者提出了一种新颖的、分布感知的傅里叶域损失函数。该损失函数将预测图像和真实图像的振幅和相位谱建模为高斯分布，并通过计算它们之间的KL散度来最小化差异。这种方法使得模型能够更鲁棒地对齐傅里叶域信息，捕获跨频率的联合分布结构，而非独立处理每个频率分量，从而避免了局部偏差和全局信息损失。</li>
<li><strong>增强型VGG感知损失（Enhanced VGG Perceptual Loss with KL-Divergence）：</strong> 为了进一步提升结构保真度，作者将KL散度嵌入到VGG感知损失中。通过在VGG网络提取的深层特征上应用KL散度，模型能够衡量高层特征表示之间的分布相似性，从而更好地捕捉图像的感知质量和结构细节。</li>
<li><strong>LLFDisc网络架构：</strong> 提出了一种流线型的U形编码器-解码器网络LLFDisc，它集成了交叉注意力（Cross-Attention）和门控机制，这些机制专为频率感知增强而设计。网络包含三个特征提取阶段，每个阶段都集成了增强型轻量级交叉注意力（EnhancedLCA）模块。</li>
<li><strong>EnhancedLCA模块：</strong> 该模块是LCA的改进，包含DANCE（暗区和噪声校正增强）、IEL（信息增强层）、SE（Squeeze-and-Excitation）和CAB（交叉注意力块）模块。<ul>
<li><strong>DANCE模块：</strong> 这是一个新提出的模块，用于暗区和噪声校正增强，通过噪声感知模块抑制噪声，暗区增强模块恢复低光照区域的细节，以及通道注意力模块重新加权特征。</li>
<li><strong>IEL模块：</strong> 通过门控机制增强特征选择性，动态调节特征流，捕获重要信息并抑制无关或噪声特征。</li>
<li><strong>CAB模块：</strong> 通过Query和Key-Value对机制选择性地融合特征，处理大尺度光照不均匀性，并动态调整注意力权重。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> 在多个基准数据集（包括LOLv1、LOLv2-Real、LOLv2-Synthetic和LSRW-Huawei）上进行了广泛的实验。LLFDisc在定性和定量评估（PSNR、SSIM、LPIPS、NIQE）方面均取得了最先进的性能。
*   <strong>傅里叶域拟合的有效性：</strong> 实验证明，基于KL散度的损失函数在傅里叶域信息拟合方面显著优于基于MSE的损失，在定量指标和感知质量上均表现出色。
*   <strong>结构保真度提升：</strong> 结合KL散度的VGG感知损失显著提升了模型的表示能力和结构保真度。
*   <strong>效率和鲁棒性：</strong> LLFDisc网络设计流线型，参数量和计算复杂度较低（例如，在LOLv1上FLOPS为10.93G，参数量为0.923M），同时在不同光照条件下表现出良好的泛化能力和鲁棒性。
*   <strong>CAM可视化：</strong> CAM可视化结果显示，使用完整损失函数训练的模型（Full-CAM）生成的注意力热图与真实图像（GT-CAM）高度相似，表明模型有效捕捉了与真实激活对齐的感兴趣区域。
*   <strong>消融研究：</strong> 消融实验验证了所提出模块（CAB、IEL、SE、DANCE）和损失函数（傅里叶KL损失、VGG KL损失）的有效性，每个组件都对提升图像增强性能有显著贡献。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确提及当前方法的具体局限性。然而，从其创新点和改进方向来看，可以推断出：
*   虽然KL散度损失在傅里叶域和感知域表现出色，但其计算复杂性可能高于简单的像素级损失，尽管论文强调了其闭式解的优势。
*   将振幅和相位谱建模为高斯分布是一种简化，可能无法完全捕捉所有复杂的频率分布特征。
*   模型在处理极端复杂或特定类型的低光照场景（如极度黑暗、强噪声干扰等）时，可能仍有进一步优化的空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更复杂的频率分布建模：</strong> 探索除了高斯分布之外，更复杂的概率分布模型来拟合傅里叶域的振幅和相位信息，以期捕捉更精细的频率特征。
*   <strong>自适应损失权重：</strong> 进一步研究自适应地调整复合损失函数中各项损失权重的策略，使其能够根据输入图像的特性或训练阶段动态优化。
*   <strong>跨模态融合：</strong> 结合其他模态信息（如深度、热成像等）来辅助低光照图像增强，尤其是在极端低光照或复杂场景下。
*   <strong>实时应用优化：</strong> 进一步优化网络架构和损失函数，以满足实时低光照图像增强应用的需求，例如在自动驾驶或监控系统中。
*   <strong>无监督/自监督学习：</strong> 探索在没有配对低光照-正常光照图像的情况下，利用KL散度进行无监督或自监督的低光照图像增强方法。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present LLFDisc, a
U-shaped deep enhancement network that integrates cross-attention and gating
mechanisms tailored for frequency-aware enhancement.</li>
<li>We propose a novel
distribution-aware loss that directly fits the Fourier-domain information and
minimizes their divergence using a closed-form KL-Divergence objective.</li>
<li>Extensive experiments across multiple benchmarks
demonstrate that LLFDisc achieves state-of-the-art performance in both
qualitative and quantitative evaluations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13083v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13083v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12990v1'></a></p>
<h2 id="dual-stage-reweighted-moe-for-long-tailed-egocentric-mistake-detection"><a href="https://arxiv.org/abs/2509.12990v1">Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</a></h2>
<p><strong>Authors:</strong> Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Sicong Li, Qingming Huang</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>In this report, we address the problem of determining whether a user performs
an action incorrectly from egocentric video data. To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework. In the first stage, features are
extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are
combined through a feature-level expert module. In the second stage, three
classifiers are trained with different objectives: reweighted cross-entropy to
mitigate class imbalance, AUC loss to improve ranking under skewed
distributions, and label-aware loss with sharpness-aware minimization to
enhance calibration and generalization. Their predictions are fused using a
classification-level expert module. The proposed method achieves strong
performance, particularly in identifying rare and ambiguous mistake instances.
The code is available at https://github.com/boyuh/DR-MoE.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Boyu Han等人撰写的论文“Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection”的全面摘要。</p>
<hr />
<h3 id="dual-stage-reweighted-moe-for-long-tailed-egocentric-mistake-detection_1">论文摘要：Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</h3>
<p>这篇论文提出了一种名为“双阶段重加权专家混合（Dual-Stage Reweighted Mixture-of-Experts, DR-MoE）”的框架，旨在解决从第一视角视频数据中检测用户操作错误的问题。该任务的挑战在于错误通常是细微、不频繁且数据分布严重不平衡的。</p>
<p><strong>1. 主要问题或研究问题：</strong>
论文的核心研究问题是：如何有效地从第一视角视频数据中识别用户是否执行了不正确的操作（即“错误检测”），尤其是在错误事件稀有、细微且数据分布高度不平衡的情况下。传统的动作识别方法难以应对这种需要更精细时间分析和对用户行为细微偏差高度敏感的任务。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
DR-MoE框架通过以下两个阶段的创新性结合来解决上述挑战：</p>
<ul>
<li>
<p><strong>第一阶段：特征级专家混合（Feature Mixture-of-Experts, F-MoE）</strong></p>
<ul>
<li>该阶段利用两个基于ViViT的模型作为特征提取专家：一个<strong>冻结的ViViT模型</strong>用于捕捉通用的时空语义先验（粗粒度动作特征），另一个是经过<strong>LoRA（低秩适应）微调的ViViT模型</strong>，专注于对错误敏感的细粒度线索。</li>
<li>这两个专家的输出通过一个可学习的F-MoE模块进行融合，该模块根据输入特性动态调整每个专家的贡献，生成统一的联合特征表示。</li>
</ul>
</li>
<li>
<p><strong>第二阶段：分类级专家混合（Classification Mixture-of-Experts, C-MoE）</strong></p>
<ul>
<li>融合后的特征被送入三个独立优化的分类器，每个分类器都针对长尾识别问题采用不同的优化目标：<ul>
<li><strong>重加权交叉熵损失（Reweighted Cross-Entropy Loss）</strong>：通过为稀有错误类别分配更高的权重来缓解类别不平衡问题，提高对欠表示错误实例的召回率。</li>
<li><strong>AUC损失（AUC Loss）</strong>：直接优化ROC曲线下面积，旨在提高正负实例之间的排名质量，这在错误类别欠表示时至关重要。</li>
<li><strong>结合锐度感知最小化（SAM）的标签感知损失（Label-Aware Loss, LA Loss）</strong>：通过调整logits并明确优化平坦最小值，促进更校准和鲁棒的决策边界，增强泛化能力。</li>
</ul>
</li>
<li>这些分类器的预测结果通过一个C-MoE模块进行自适应融合，该模块根据输入动态加权每个专家的贡献，从而在多样化的数据条件下实现灵活的决策。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
论文在HoloAssist 2025比赛的错误检测任务上进行了实验，并取得了显著的性能提升。
*   与Random和TimeSformer等基线模型相比，DR-MoE方法在F-score上实现了显著改进。
*   特别是在识别稀有和模糊的错误实例方面，该方法表现出强大的性能。
*   值得注意的是，DR-MoE仅使用RGB模态输入，就达到了甚至超越了依赖多模态输入的模型（如UNICT Solution）的竞争力，尤其是在错误召回率方面有显著提升（从UNICT Solution的0.09提升到DR-MoE的0.63）。
这些结果表明，DR-MoE框架通过结合互补的建模策略（包括特征提取和分类层面），能够有效应对长尾和细微错误检测的挑战，提高了模型的鲁棒性和准确性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确提及当前方法的具体局限性。然而，从其方法论的复杂性来看，潜在的局限性可能包括：
*   <strong>计算成本：</strong> 结合多个ViViT模型、LoRA微调以及多专家分类器可能会增加模型的训练和推理成本。
*   <strong>超参数调优：</strong> 多个损失函数和专家混合模块引入了更多的超参数，可能需要精细的调优才能达到最佳性能。
*   <strong>泛化性：</strong> 尽管论文强调了泛化能力，但对于HoloAssist数据集之外的更广泛、更多样化的第一视角错误检测场景，其泛化能力仍需进一步验证。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出未来的研究方向，但基于其贡献和潜在局限性，可以推断出以下方向：
*   <strong>模型效率优化：</strong> 探索更轻量级的专家模型或更高效的专家混合机制，以降低计算成本，使其更适用于实时或资源受限的应用。
*   <strong>多模态融合的进一步探索：</strong> 尽管当前方法仅使用RGB模态就表现出色，但结合其他模态（如深度、手部姿态、眼动追踪）的更高级融合策略，可能会进一步提升性能，尤其是在更复杂或更模糊的错误场景中。
*   <strong>可解释性：</strong> 深入研究F-MoE和C-MoE模块中专家权重分配的机制，以提高模型决策的可解释性，帮助理解模型在不同情况下如何权衡不同信息源。
*   <strong>自监督或半监督学习：</strong> 鉴于错误事件的稀有性，探索利用未标记数据进行自监督或半监督学习的方法，以进一步缓解数据不平衡问题。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12990v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12990v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12888v1'></a></p>
<h2 id="runge-kutta-approximation-and-decoupled-attention-for-rectified-flow-inversion-and-semantic-editing"><a href="https://arxiv.org/abs/2509.12888v1">Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</a></h2>
<p><strong>Authors:</strong> Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Rectified flow (RF) models have recently demonstrated superior generative
performance compared to DDIM-based diffusion models. However, in real-world
applications, they suffer from two major challenges: (1) low inversion accuracy
that hinders the consistency with the source image, and (2) entangled
multimodal attention in diffusion transformers, which hinders precise attention
control. To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations. To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control. Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability. Code is
available at https://github.com/wmchen/RKSovler_DDTA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He撰写的论文“Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Runge-Kutta近似与解耦注意力在整流流反演和语义编辑中的应用</strong></p>
<p>这篇论文由Weiming Chen等人撰写，旨在解决整流流（Rectified Flow, RF）模型在实际应用中面临的两个关键挑战：低反演精度和多模态注意力纠缠。RF模型在生成性能上已超越基于DDIM的扩散模型，但在图像重建和语义编辑任务中仍存在不足。</p>
<p><strong>1. 主要问题或研究问题：</strong>
论文主要关注RF模型在实际应用中的两个核心问题：
*   <strong>低反演精度：</strong> RF模型在将给定图像反演回其对应的噪声样本时，难以保持与源图像的一致性，这限制了其在图像重建和编辑中的应用。
*   <strong>多模态注意力纠缠：</strong> 在多模态扩散Transformer（MM-DiT）中，文本和图像注意力紧密耦合，导致难以进行精确的语义控制，从而影响了文本引导图像编辑的灵活性和准确性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述挑战，论文提出了两项主要创新：
*   <strong>基于Runge-Kutta（RK）求解器的高阶反演方法：</strong> 针对低反演精度问题，作者将数值分析中的Runge-Kutta方法引入RF采样过程，提出了一种高效的高阶求解器，用于RF模型的微分方程过程。这种方法能够更精确地近似微分轨迹，从而提高反演保真度。
*   <strong>解耦扩散Transformer注意力（Decoupled Diffusion Transformer Attention, DDTA）：</strong> 为了解决多模态注意力纠缠问题，论文引入了DDTA机制。该机制通过深入MM-DiT的内部结构，将文本和图像注意力解耦，从而实现更精确的语义控制。这使得在文本引导的图像编辑中，能够更灵活地利用源信息，平衡保真度和可编辑性之间的权衡。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文通过在图像重建和文本引导编辑任务上的广泛实验，验证了所提方法的有效性：
*   <strong>图像重建任务：</strong> RK求解器显著提高了反演保真度，在PSNR（峰值信噪比）上实现了高达2.39 dB的显著增益，优于现有RF反演方法。这表明高阶反演技术能有效克服RF潜在空间的稀疏性问题。
*   <strong>文本引导编辑任务：</strong> 结合RK求解器和DDTA的方法在保真度和可编辑性方面均达到了最先进的性能。DDTA机制通过解耦文本和图像注意力，实现了对语义编辑的精细控制，从而在保持源图像一致性的同时，提高了编辑的准确性。用户研究也进一步证实了该方法在编辑质量和忠实度方面的优越性。
*   <strong>效率：</strong> 论文的方法在显著减少采样步骤的情况下，实现了最佳的整体性能，表明了其卓越的效率。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文也坦诚地指出了当前方法的局限性：
*   <strong>计算开销：</strong> 尽管RK求解器显著提高了保真度，但其高阶建模引入了额外的计算开销。
*   <strong>内存消耗：</strong> 保存解耦注意力图会带来显著的内存消耗。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了未来的研究方向：
*   <strong>低计算开销的高阶求解器：</strong> 开发计算开销更低的高阶求解器，以提高RF模型的实用性。
*   <strong>高效的注意力保持机制：</strong> 设计更高效的注意力保持机制，以进一步优化内存使用和计算效率。</p>
<hr />
<p>总而言之，这篇论文为RF模型在图像反演和语义编辑中的应用提供了重要的进展。通过引入Runge-Kutta近似和解耦注意力机制，作者有效地解决了RF模型在实际应用中面临的精度和控制性挑战，为未来基于扩散模型的图像生成和编辑技术开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations.</li>
<li>To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control.</li>
<li>Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12888v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12888v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-17 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
