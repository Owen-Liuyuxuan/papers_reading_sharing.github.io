<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-17 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-16/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-18/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-17">Arxiv Computer Vision Papers - 2025-09-17</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era" class="nav-link">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a>
                </li>
                <li class="nav-item">
                    <a href="#maps-for-autonomous-driving-full-process-survey-and-frontiers" class="nav-link">Maps for Autonomous Driving: Full-process Survey and Frontiers</a>
                </li>
                <li class="nav-item">
                    <a href="#deep-learning-for-3d-point-cloud-processing-from-approaches-tasks-to-its-implications-on-urban-and-environmental-applications" class="nav-link">Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</a>
                </li>
                <li class="nav-item">
                    <a href="#3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review" class="nav-link">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a>
                </li>
                <li class="nav-item">
                    <a href="#vi-safe-a-spatial-temporal-framework-for-efficient-violence-detection-in-public-surveillance" class="nav-link">Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</a>
                </li>
                <li class="nav-item">
                    <a href="#whu-stree-a-multi-modal-benchmark-dataset-for-street-tree-inventory" class="nav-link">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a>
                </li>
                <li class="nav-item">
                    <a href="#advancing-real-world-parking-slot-detection-with-large-scale-dataset-and-semi-supervised-baseline" class="nav-link">Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</a>
                </li>
                <li class="nav-item">
                    <a href="#using-kl-divergence-to-focus-frequency-information-in-low-light-image-enhancement" class="nav-link">Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</a>
                </li>
                <li class="nav-item">
                    <a href="#dual-stage-reweighted-moe-for-long-tailed-egocentric-mistake-detection" class="nav-link">Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#runge-kutta-approximation-and-decoupled-attention-for-rectified-flow-inversion-and-semantic-editing" class="nav-link">Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-17">Arxiv Computer Vision Papers - 2025-09-17</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ16æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>æ§è¡æè¦ï¼Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥å (2025-09-16)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæéåç°åºè®¡ç®æºè§è§é¢åå ä¸ªå³é®ä¸ç¸äºå³èçè¶å¿ï¼</p>
<ul>
<li><strong>å·èº«æºè½ä¸ä¸ç»´æç¥ï¼</strong> å·èº«æºè½ï¼Embodied AIï¼åèªå¨é©¾é©¶æ¯æ ¸å¿é©±å¨åï¼å¯¹å¨åè§è§ãä¸ç»´ç¹äºå¤çãä¸ç»´äººä½å§¿æä¸å½¢ç¶ä¼°è®¡ä»¥åé«ç²¾åº¦å°å¾çéæ±æ¥çå¢é¿ãè¿è¡¨æç ç©¶æ­£ä»çº¯ç²¹çå¾åçè§£è½¬åæ´å¤æçãä¸ç©çä¸çäº¤äºçæç¥ç³»ç»ã</li>
<li><strong>æ°æ®é©±å¨ä¸åºåå»ºè®¾ï¼</strong> å¤ä¸ªå·¥ä½è´åäºæå»ºå¤§è§æ¨¡ãå¤æ¨¡ææç¹å®åºæ¯çæ°æ®éï¼å¦è¡æ¯æ æ¨ãåè½¦ä½ï¼ï¼ä»¥æ¨å¨ç¹å®ä»»å¡çè¿å±ãè¿åæ äºé«è´¨éæ°æ®å¨æ·±åº¦å­¦ä¹ æ¶ä»£çéè¦æ§ï¼ä»¥åå¯¹æ´çå®ä¸çåºæ¯çå³æ³¨ã</li>
<li><strong>æçä¸é²æ£æ§ï¼</strong> å¨å®éåºç¨ä¸­ï¼å¯¹æçï¼å¦æ´åæ£æµï¼åé²æ£æ§ï¼å¦ä½åç§å¢å¼ºãé¿å°¾åå¸å¤çï¼çå³æ³¨æç»­å­å¨ï¼ç ç©¶äººåæ­£å¨æ¢ç´¢æ°çæ¨¡åæ¶æåè®­ç»ç­ç¥æ¥è§£å³è¿äºææã</li>
<li><strong>çææ¨¡åä¸ç¼è¾ï¼</strong> æ©æ£æ¨¡åï¼Rectified Flowï¼å¨å¾åçæåè¯­ä¹ç¼è¾æ¹é¢çåºç¨æ¾ç¤ºåºå¶å¨åå®¹åä½åå¾åæä½é¢åçæ½åã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)ï¼</strong> è¿ç¯ç»¼è¿°æ§è®ºæéå¸¸åæ¶ä¸å·æåç»æ§ãå®ç³»ç»å°æ¢³çäºå¨åè§è§å¨å·èº«æºè½ä¸­çéè¦æ§ãææåæªæ¥æ¹åï¼å¯¹äºçè§£è¯¥é¢åå®è§åå±è³å³éè¦ãå¶åæ°æ§å¨äºå¯¹ä¸ä¸ªæ°å´ä¸å³é®é¢åçå¨é¢å±æã</li>
<li><strong>"Maps for Autonomous Driving: Full-process Survey and Frontiers" (Pengxin Chen et al.)ï¼</strong> å¦ä¸ç¯é«è´¨éçç»¼è¿°ï¼æ·±å¥æ¢è®¨äºèªå¨é©¾é©¶å°å¾çæ´ä¸ªçå½å¨æãèèå°èªå¨é©¾é©¶çå¤ææ§åå¯¹å°å¾çä¾èµï¼è¿ç¯è®ºæä¸ºç ç©¶äººåæä¾äºå®è´µçç¥è¯ä½ç³»åæªæ¥ç ç©¶æ¹åã</li>
<li><strong>"Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing" (Weiming Chen et al.)ï¼</strong> è¿ç¯è®ºæå¨çææ¨¡åé¢åå±ç°äºææ¯åæ°ãéè¿å¼å¥Runge-Kuttaè¿ä¼¼åè§£è¦æ³¨æåï¼å®æåäºRectified Flowæ¨¡åå¨å¾ååæ¼åè¯­ä¹ç¼è¾æ¹é¢çæ§è½ï¼ä¸ºé«è´¨éå¾åçæåæä½æä¾äºæ°çå·¥å·ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>å¨åè§è§ï¼Omnidirectional Visionï¼ï¼</strong> éçå·èº«æºè½åVR/ARçåå±ï¼å¨åè§è§å°æä¸ºä¸ä¸ªè¶æ¥è¶éè¦çç ç©¶æ¹åï¼æ¶µçæ°æ®ééãæ¨¡åè®¾è®¡ååºç¨ã</li>
<li><strong>LiDARç¹äºå¨äººä½æç¥ä¸­çåºç¨ï¼</strong> "3D Human Pose and Shape Estimation from LiDAR Point Clouds" æåºLiDARå¨éç§ä¿æ¤åå¨å¤©åæç¥æ¹é¢çä¼å¿ï¼é¢ç¤ºçLiDARå¨äººä½æç¥é¢åçæ½åã</li>
<li><strong>æ©æ£æ¨¡åï¼Rectified Flowï¼çç²¾ç»åæ§å¶ä¸åºç¨ï¼</strong> "Runge-Kutta Approximation and Decoupled Attention..." å±ç¤ºäºå¦ä½éè¿ç®æ³ä¼åæ¥æåæ©æ£æ¨¡åå¨ç¹å®ä»»å¡ï¼å¦è¯­ä¹ç¼è¾ï¼ä¸çè¡¨ç°ï¼é¢ç¤ºçæªæ¥å¯¹çææ¨¡åæ´ç²¾ç»åæ§å¶çç ç©¶ã</li>
<li><strong>å¤æ¨¡æèåä¸ç¹å®åºæ¯æ°æ®éï¼</strong> WHU-STree å Advancing Real-World Parking Slot Detection ç­å·¥ä½å¼ºè°äºä¸ºç¹å®å¤æåºæ¯æå»ºå¤æ¨¡æãå¤§è§æ¨¡æ°æ®éçéè¦æ§ï¼ä»¥åå¦ä½å©ç¨è¿äºæ°æ®è§£å³å®éé®é¢ã</li>
</ul>
<p><strong>4. å»ºè®®å®æ´éè¯»çè®ºæï¼</strong></p>
<p>ä¸ºäºå¨é¢äºè§£å½åè¶å¿åæ½å¨çªç ´ï¼å»ºè®®å®æ´éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>"PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era" (Xu Zheng et al.)ï¼</strong> æä¾å®è§è§è§ï¼çè§£å·èº«æºè½èæ¯ä¸çå¨åè§è§ã</li>
<li><strong>"Maps for Autonomous Driving: Full-process Survey and Frontiers" (Pengxin Chen et al.)ï¼</strong> æ·±å¥äºè§£èªå¨é©¾é©¶æ ¸å¿ææ¯ä¹ä¸çå°å¾ã</li>
<li><strong>"Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing" (Weiming Chen et al.)ï¼</strong> äºè§£çææ¨¡ååæ²¿ææ¯åå¶å¨å¾åç¼è¾ä¸­çåºç¨ã</li>
<li><strong>"Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications" (Zhenxin Zhang et al.)ï¼</strong> å¯¹3Dç¹äºå¤ççå¨é¢ç»¼è¿°ï¼å¯¹äºçè§£ä¸ç»´æç¥åºç¡è³å³éè¦ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨ä¸ºæ¨çç ç©¶æä¾ä¸ä¸ªå¿«éå¯¼èªï¼å¸®å©æ¨ä¼åå³æ³¨ä¸æ¨ç ç©¶æ¹åæç¸å³çææ°è¿å±ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></li>
<li><a href="#2509.12632v1">Maps for Autonomous Driving: Full-process Survey and Frontiers</a></li>
<li><a href="#2509.12452v1">Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</a></li>
<li><a href="#2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></li>
<li><a href="#2509.13210v1">Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</a></li>
<li><a href="#2509.13172v1">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a></li>
<li><a href="#2509.13133v1">Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</a></li>
<li><a href="#2509.13083v1">Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</a></li>
<li><a href="#2509.12990v1">Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</a></li>
<li><a href="#2509.12888v1">Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.12989v1'></a></p>
<h2 id="panorama-the-rise-of-omnidirectional-vision-in-the-embodied-ai-era"><a href="https://arxiv.org/abs/2509.12989v1">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></h2>
<p><strong>Authors:</strong> Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Omnidirectional vision, using 360-degree vision to understand the
environment, has become increasingly critical across domains like robotics,
industrial inspection, and environmental monitoring. Compared to traditional
pinhole vision, omnidirectional vision provides holistic environmental
awareness, significantly enhancing the completeness of scene perception and the
reliability of decision-making. However, foundational research in this area has
historically lagged behind traditional pinhole vision. This talk presents an
emerging trend in the embodied AI era: the rapid development of omnidirectional
vision, driven by growing industrial demand and academic interest. We highlight
recent breakthroughs in omnidirectional generation, omnidirectional perception,
omnidirectional understanding, and related datasets. Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.
Moreover, we offer in-depth opinions related to emerging trends and
cross-community impacts at the intersection of panoramic vision and embodied
AI, along with the future roadmap and open challenges. This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾ç±Xu Zhengç­äººæ°åçè®ºæâPANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Eraâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨å·èº«AIæ¶ä»£ï¼å¨åè§è§ï¼360åº¦å¨æ¯è§è§ï¼å¨çè§£ç¯å¢æ¹é¢æ¥çå¢é¿çéè¦æ§ä¸è¯¥é¢ååºç¡ç ç©¶é¿æè½åäºä¼ ç»éå­è§è§ä¹é´çå·®è·ãä¼ ç»éå­è§è§æä¾çæ¯ç­çªçãè§é¥åéçè§è§ï¼èå¨åè§è§è½æä¾æ´å¨é¢çç¯å¢æç¥ï¼è¿å¯¹äºå·èº«AIä¸­æ´å¤æçä»»å¡ï¼å¦å®¤å/å®¤å¤å¯¼èªï¼è³å³éè¦ãè®ºææ¢è®¨äºå¦ä½åææ°æ®ç¶é¢ãæ¨¡åè½åéå¶ååºç¨ç©ºç½ï¼ä»¥ååéæ¾å¨åè§è§å¨å·èº«AIä¸­çå·¨å¤§æ½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>PANORAMAç³»ç»æ¶æï¼</strong> è®ºææåºäºä¸ä¸ªçæ³çå¨æ¯ç³»ç»æ¶æââPANORAMAï¼ç±åä¸ªå³é®å­ç³»ç»ç»æï¼
    *   <strong>æ°æ®ééä¸é¢å¤çï¼Data Acquisition &amp; Pre-processingï¼ï¼</strong> è´è´£æè·åå§å¨åæ°æ®å¹¶è½¬æ¢ä¸ºè®¡ç®å¤çæ ¼å¼ï¼åæ¬æ°æ®æè·ãæ ¼å¼è½¬æ¢ãåæ­¥ä¸æ ¡åã
    *   <strong>æç¥ï¼Perceptionï¼ï¼</strong> å¯¹é¢å¤çåçå¨æ¯æ°æ®è¿è¡åºç¡åºæ¯æç¥ï¼å©ç¨ä¸é¨çæ·±åº¦å­¦ä¹ æ¨¡åï¼å¦çé¢CNNãTransformerï¼æåä¸°å¯ãç»æåçä¿¡æ¯ï¼æ§è¡ç¹å¾æååç¯å¢æç¥ï¼è¯­ä¹åå²ãç®æ æ£æµãæ·±åº¦ä¼°è®¡ï¼ã
    *   <strong>åºç¨ï¼Applicationï¼ï¼</strong> å°æç¥æ´å¯è½¬åä¸ºå·èº«AIæºè½ä½çè¡å¨ï¼æå¡äºç¹å®ä¸æ¸¸ä»»å¡ï¼å¦å¯¼èªä¸SLAMãäººæºäº¤äºãæ°å­å­ªçä¸3Déå»ºã
    *   <strong>å éä¸é¨ç½²ï¼Acceleration &amp; Employmentï¼ï¼</strong> è§£å³é«åè¾¨çå¨æ¯æ°æ®å¤ççè®¡ç®ææï¼éè¿è½¯ä»¶å éï¼æ¨¡åéåãåªæï¼åç¡¬ä»¶é¨ç½²ï¼è¾¹ç¼è®¡ç®å¹³å°ï¼ç¡®ä¿æ´ä¸ªæµç¨çè®¡ç®å¯è¡æ§ã
*   <strong>å¨åè§è§ææ¯çªç ´çç»¼åæ¦è¿°ï¼</strong> è®ºæç³»ç»å°æ»ç»äºå¨åçæãå¨åæç¥ãå¨åçè§£ä»¥åç¸å³æ°æ®éçææ°è¿å±ï¼çªåºäºè¯¥é¢åå¨å·¥ä¸éæ±åå­¦æ¯å´è¶£é©±å¨ä¸çå¿«éåå±ã
*   <strong>å·èº«AIæ¶ä»£å¨åè§è§çè·¯çº¿å¾ï¼</strong> è®ºææåºäºä¸ä¸ªåé¶æ®µçæªæ¥è·¯çº¿å¾ï¼åæ¬æ°æ®éæ´åãå¤æ¨¡ææ©å±ãæ¨çä¸å·èº«æ°æ®ãç»ä¸æ¨¡åé¢è®­ç»ãè¯ä¼°ä¸åºåæµè¯ãé¨ç½²ä¸æ³åï¼æ¨å¨æå»ºä¸ä¸ªçæ³çãç»ä¸çå¨åä»»å¡æ¨¡åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å¨åè§è§çæ½åï¼</strong> è®ºæå¼ºè°å¨åè§è§å¨æºå¨äººãå·¥ä¸æ£æµåç¯å¢çæµç­é¢åçéè¦æ§ï¼å ä¸ºå®æä¾äºæ¯éå­è§è§æ´å¨é¢çç¯å¢æç¥ï¼æ¾èæåäºåºæ¯æç¥çå®æ´æ§åå³ç­çå¯é æ§ã
*   <strong>ææçç³»ç»æ§åç±»ï¼</strong> å°å¨åè§è§é¢ä¸´çé®é¢å½ç»ä¸ºæ°æ®ç¶é¢ãæ¨¡åè½åååºç¨ç©ºç½ä¸å¤§ç±»ï¼ä¸ºåç»­ç ç©¶æä¾äºæ¸æ°çæ¡æ¶ã
*   <strong>ææ¯è¿å±çæ¢³çï¼</strong> è¯¦ç»ä»ç»äºå¨åçæï¼å¦Dream360ãPanoDiffusionãOmniDragï¼ãå¨åæç¥ï¼å¦GoodSAMãOmniSAMï¼åå¨åçè§£ï¼å¦OSR-BenchãOmniVQAï¼çææ°ææ¯ï¼å±ç¤ºäºè¯¥é¢åå¨åæå ä½ç¸åãéåºæ¨¡ååæå»ºæ°æ®éæ¹é¢çåªåã
*   <strong>æ°æ®éçå¨é¢åé¡¾ï¼</strong> æä¾äºå®¤åãå®¤å¤åæ äººæº/é£è¡ç­é¢å23ä¸ªä»£è¡¨æ§å¨åæ°æ®éçæ¦è¿°ï¼æ¶µçäºRGBå¨æ¯å¾ãæ·±åº¦ãç¸æºå§¿æåè¯­ä¹æ ç­¾ç­æ¨¡æï¼ä¸ºç ç©¶äººåæä¾äºå®è´µçèµæºã
*   <strong>PANORAMAæ¶æçæ¿æ¯ï¼</strong> æåºçPANORAMAç³»ç»æ¶æä¸ºå·èº«AIä¸­å¨åè§è§çéææä¾äºä¸ä¸ªå¨é¢çãç«¯å°ç«¯çè§£å³æ¹æ¡ï¼æææ¨å¨å·èº«æºè½çåå±ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®ç¶é¢ï¼</strong> å¨æ¯å¾åçæ æ³¨ææ¬é«æï¼ä¸ç±äºç­è·ç©å½¢æå½±ï¼ERPï¼ç­å ä½ç¸åï¼ä¼ ç»èªå¨åæ æ³¨å·¥å·æçä½ä¸ï¼é»ç¢äºå¤§è§æ¨¡é«è´¨éæ°æ®éçå¼åã
*   <strong>æ¨¡åè½åéå¶ï¼</strong> ç°æé¢è®­ç»æ¨¡åï¼ä¸»è¦éå¯¹éå­å¾åè®¾è®¡ï¼çå½çº³åç½®ï¼å¦å¹³ç§»ä¸åæ§ï¼ä¸éç¨äºå¨æ¯å¾åçç¸åç¹æ§ï¼å¯¼è´æ§è½æ¾èä¸éã
*   <strong>åºç¨ç©ºç½ï¼</strong> ç¼ºä¹è·¨å­¦ç§äººæä»¥åç°æå¨æ¯æ°æ®åæ¨¡åçä¸è¶³ï¼å¯¼è´å¨æ¯çäº§å®å¨æ£æ¥ãå¨æ¯æ£®æç«ç¾æ£æµç­ç¹å®åºç¨é¢åæ¢ç´¢ä¸è¶³ã
*   <strong>æ³åæ§åé²æ£æ§ï¼</strong> å½åæ¨¡åå¤ä¸æ³¨äºç¹å®åºæ¯ææå½±æ¹æ³ï¼é¾ä»¥æ³åå°å¤æ ·åçå¨æ¯ä¼ æå¨è§æ ¼ãåºç¨åºæ¯åæå½±æ¹æ³ã
*   <strong>å¨æç¸åå¤çï¼</strong> ç°ææ¹æ³å°å¨æ¯å¾åçç¸åè§ä¸ºä¸å¸§æ å³çå ä½é®é¢ï¼æªè½ååèèçå®ä¸çåºæ¯ä¸­ç¸åçå¨ææ§åå¨å¨åè§é¢åºåä¸­çæ¼åã
*   <strong>ç¼ºä¹å¤§è§æ¨¡å¤æ¨¡æé¢è®­ç»èµæºï¼</strong> ç°ææ¹æ³å¨æ¨¡åæ³åæ¹é¢åéï¼ç¼ºä¹å¤§è§æ¨¡å¤æ¨¡æé¢è®­ç»èµæºï¼é»ç¢äºå·èº«AIçå¹¿æ³åå±ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ°æ®éå»ºè®¾ï¼</strong> è§åååå¸å¤§è§æ¨¡ãå¤ä»»å¡çå¨åæ°æ®éï¼æ¶µççå®ä¸çåºæ¯çå¤ææ§ï¼åæ¬å®¤åå¤ãéç¨åå·èº«æºè½åºæ¯ã
*   <strong>ç®æ³åæ°ï¼</strong> è¶è¶åºäºéå­æ¨¡åçç®åééï¼åå»ºå·æå¨åä¿¡æ¯çæ°é¢æ¶æåå¨æå­¦ä¹ èå¼ï¼ä»¥åºå¯¹å¨åè§è§çç¬ç¹ææã
*   <strong>åºç¨æ¢ç´¢ï¼</strong> æ¢ç´¢åå±ç¤ºå¨åæç¥å¨çå®ä¸çæºå¨äººåäº¤äºç³»ç»ä¸­çä¼å¿ï¼å¼¥åå®éªå®¤ç ç©¶ä¸å®éåºç¨ä¹é´çé¸¿æ²ã
*   <strong>æ³åä¸é²æ£æ§ï¼</strong> å¼åè½å¤æ³åå°ä¸åå¨æ¯ä¼ æå¨è§æ ¼ãåºç¨åºæ¯åæå½±æ¹æ³çæ¨¡åï¼å¹¶å©ç¨æå½±æ å³è¡¨ç¤ºåèªçç£å­¦ä¹ ææ¯ä»æ æ ç­¾å¨åä¿¡æ¯ä¸­å­¦ä¹ ä¸åç¹å¾ã
*   <strong>å¨æç¸åå¤çï¼</strong> æç¡®èèå¨åè§é¢åºåä¸­ç¸åçå¨ææ§åæ¶é´ä¸è´æ§ã
*   <strong>ä»¥è¡å¨ä¸ºå¯¼åçè¡¨ç¤ºå­¦ä¹ ï¼</strong> ä½¿æ¨¡åè½å¤å­¦ä¹ å¨æ¯å¾åä¸­ä»¥è¡å¨ä¸ºå¯¼åçè¡¨ç¤ºï¼ä»èå®ç°æ´ææåé«æçæºå¨äººå³ç­ã
*   <strong>å¯æ©å±çç»ä¸æ¶æï¼</strong> åå»ºä¸é¨ä¸ºå¨åè§è§è®¾è®¡çç»ä¸ãå¤ä»»å¡åºç¡æ¨¡åï¼éè¿å¨å¤§éå¨æ¯æ°æ®ä¸è¿è¡é¢è®­ç»ï¼ææå¨åå ä½åè¯­ä¹çåºæ¬çè§£ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.</li>
<li>This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12989v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12989v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12632v1'></a></p>
<h2 id="maps-for-autonomous-driving-full-process-survey-and-frontiers"><a href="https://arxiv.org/abs/2509.12632v1">Maps for Autonomous Driving: Full-process Survey and Frontiers</a></h2>
<p><strong>Authors:</strong> Pengxin Chen, Zhipeng Luo, Xiaoqi Jiang, Zhangcai Yin, Jonathan Li</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Maps have always been an essential component of autonomous driving. With the
advancement of autonomous driving technology, both the representation and
production process of maps have evolved substantially. The article categorizes
the evolution of maps into three stages: High-Definition (HD) maps, Lightweight
(Lite) maps, and Implicit maps. For each stage, we provide a comprehensive
review of the map production workflow, with highlighting technical challenges
involved and summarizing relevant solutions proposed by the academic community.
Furthermore, we discuss cutting-edge research advances in map representations
and explore how these innovations can be integrated into end-to-end autonomous
driving frameworks.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Pengxin Chenç­äººæ°åçè®ºæâMaps for Autonomous Driving: Full-process Survey and Frontiersâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼èªå¨é©¾é©¶å°å¾ï¼å¨æµç¨ç»¼è¿°ä¸åæ²¿</strong></p>
<p><strong>1. è®ºæè§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨å¨é¢åé¡¾èªå¨é©¾é©¶å°å¾ï¼Maps for Autonomous Driving, MADï¼çæ¼åãçäº§æµç¨ãææ¯ææä»¥åæªæ¥åå±æ¹åãæ ¸å¿é®é¢æ¯ï¼éçèªå¨é©¾é©¶ææ¯çåå±ï¼å°å¾çè¡¨ç¤ºåçäº§è¿ç¨å¦ä½æ¼åï¼ä»¥æ»¡è¶³ä»ä¼ ç»å¯¼èªå°ç«¯å°ç«¯èªå¨é©¾é©¶ç³»ç»ä¸æ­å¢é¿çéæ±ï¼å¹¶åæé«ææ¬ãä½æ´æ°é¢çåå¤ææ§ç­éå¶ãè®ºæå°å°å¾æ¼ååä¸ºä¸ä¸ªé¶æ®µï¼é«ç²¾å°å¾ï¼HD mapsï¼ãè½»éåå°å¾ï¼Lite mapsï¼åéå¼å°å¾ï¼Implicit mapsï¼ï¼å¹¶å¯¹æ¯ä¸ªé¶æ®µççäº§æµç¨ãææ¯ææåå­¦æ¯è§£å³æ¹æ¡è¿è¡äºæ·±å¥åæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçä¸»è¦è´¡ç®å¨äºå¶å¨é¢çè§è§åå¯¹å°å¾æ¼åé¶æ®µçååï¼è¿å¨ç°æç»¼è¿°ä¸­æ¯ç¬æçãå·ä½æ¹æ³è®ºè´¡ç®åæ¬ï¼
*   <strong>ä¸é¶æ®µæ¼è¿æ¡æ¶ï¼</strong> é¦æ¬¡ç³»ç»å°å°èªå¨é©¾é©¶å°å¾çæ¼è¿ååä¸ºHD MapãLite MapåImplicit Mapä¸ä¸ªå³é®é¶æ®µï¼å¹¶è¯¦ç»éè¿°äºæ¯ä¸ªé¶æ®µçç¹ç¹ãææ¯éæ±åææã
*   <strong>HD Mapé¶æ®µçå¨é¢åé¡¾ï¼</strong> è¯¦ç»ä»ç»äºHD Mapççäº§æµç¨ï¼æµç»ãæç¥ãå°å¾ç¼è¯ï¼ï¼å¹¶æ·±å¥æ¢è®¨äºå®ä½ï¼GNSSãIMUãè½®å¼éç¨è®¡ãè§è§/æ¿åé·è¾¾éç¨è®¡ãå°é¢æ§å¶ç¹ï¼ãå¤è¡ç¨æµç»ï¼ååSLAMãé­ç¯æ£æµãSDå°å¾å¹éï¼ãéææç¥ï¼è·¯é¢ãè·¯æ ãè½¦éçº¿ãäº¤éæ å¿ãæ¤æ ãæç¶ç©ãè¡éæ æåï¼åææçæï¼å¾æç´¢ãæ·±åº¦å­¦ä¹ ãåå±æ¨çãå¤æ¨¡æèåï¼ç­å³é®ææ¯ã
*   <strong>Lite Mapé¶æ®µçéç¹å³æ³¨ï¼</strong> å¼ºè°äºLite Mapä½ä¸ºHD Mapçè½»éåæ¿ä»£æ¹æ¡ï¼å¨è§£å³åå¸éè·¯èªå¨é©¾é©¶ææä¸­çä½ç¨ãéç¹ä»ç»äºå¨çº¿ç¢éåï¼åå¸§æ£æµãé¿åºåå»ºæ¨¡ï¼ãä¼åå°å¾ç»´æ¤ï¼å¨çº¿ååæ£æµãå¥å±è·¯çº¿ãäº¤éæµè½¨è¿¹ææï¼ç­åæ°æ¹æ³ã
*   <strong>Implicit Mapé¶æ®µçåæ²¿æ¢ç´¢ï¼</strong> æ·±å¥æ¢è®¨äºéå¼å°å¾ä½ä¸ºç«¯å°ç«¯èªå¨é©¾é©¶ç³»ç»åå±è¶å¿çä¸é¨åï¼åæ¬æ¥è¯¢è¡¨ç¤ºæ¹æ³ï¼çæå¼ä¸ç®æ å¯¼åãä¸ä¸ææ¡ä»¶ãç»ä¸è·¨ä»»å¡ï¼ãæ½å¨ç©ºé´æ¹æ³ï¼ç»æååºæ¯è¡¨ç¤ºãå¨ææç¥æ½å¨å»ºæ¨¡ãé²æ£æ§å¢å¼ºï¼ãç¥ç»è¾å°åºï¼NeRFï¼æ¹æ³ï¼åºæ¯éå»ºãè¯­ä¹çè§£ãæçä¸é²æ£æ§å¢å¼ºï¼åä¸çæ¨¡åï¼ç¯å¢è¡¨ç¤ºãå¨ææ¼åãæ¨¡åå¢å¼ºï¼ç­ã
*   <strong>å·¥ä¸è§è§çæ´åï¼</strong> ç»åäºä½èå¨å·¥ä¸ççç»éªï¼æä¾äºå¯¹å°å¾åå±éç¨ç¢åå³é®å¬å¸ææ¯çæ´å¯ï¼ä½¿ç»¼è¿°æ´å·å®è·µæå¯¼æä¹ã
*   <strong>å¼æºå·¥ä½æ»ç»ï¼</strong> éå½ä¸­æä¾äºéå¼å°å¾åå¨çº¿ç¢éåç¸å³å¼æºå·¥ä½çåè¡¨ï¼æ¹ä¾¿ç ç©¶äººåå¿«éæ¥éã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>HD Mapçå¥ åºä½ç¨ï¼</strong> HD Mapéè¿æä¾åç±³çº§ç²¾åº¦çè½¦éçº§ä¿¡æ¯ï¼é¦æ¬¡ä½¿L2-L4çº§é«éå¬è·¯èªå¨é©¾é©¶æä¸ºå¯è½ï¼å¥ å®äºå°å¾å¨èªå¨é©¾é©¶ä¸­çåºç¡å°ä½ã
*   <strong>Lite Mapçå®ç¨æ§çªç ´ï¼</strong> Lite Mapéè¿ä¼åæ°æ®åèªå¨åçæææ¯ï¼æ¾èéä½äºçäº§ææ¬å¹¶æé«äºæ´æ°é¢çï¼ä½¿å¶è½å¤è¦çåå¸éè·¯ï¼æ¨å¨äºèªå¨é©¾é©¶çåä¸åè½å°ã
*   <strong>Implicit Mapçæªæ¥æ½åï¼</strong> éå¼å°å¾ä½ä¸ºç«¯å°ç«¯èªå¨é©¾é©¶ç³»ç»çä¸é¨åï¼éè¿å°ç¯å¢ç¥è¯éå¼ç¼ç å°ç¥ç»ç½ç»ä¸­ï¼ææå®ç°æ´åäººç±»çãä¸ä¸ææç¥çå³ç­å¶å®ï¼å¹¶ä¿è¿å¯å¾®åå¤çåååä¼ æ­ï¼ä¸ºèåå­¦ä¹ ç³»ç»æä¾æ¯æã
*   <strong>å°å¾å¨ADç³»ç»ä¸­ä¸å¯æç¼ºçå°ä½ï¼</strong> è®ºæå¼ºè°ï¼æ è®ºå°å¾å½¢å¼å¦ä½æ¼åï¼å®å§ç»æ¯èªå¨é©¾é©¶ç³»ç»ä¸­ä¸å¯æç¼ºçåç´ ï¼ä»ä¼ ç»çæ¨¡ååæ¶æå°ç°ä»£çç«¯å°ç«¯å­¦ä¹ ç³»ç»ï¼å°å¾é½æ®æ¼çå³é®è§è²ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>HD Mapçå±éæ§ï¼</strong> çäº§ææ¬é«æãæ´æ°é¢çä½ãç»´æ¤ææ¬é«ï¼é¾ä»¥æ©å±å°å¤æçåå¸éè·¯ç¯å¢ã
*   <strong>Lite Mapçå±éæ§ï¼</strong> å°½ç®¡æææ¹è¿ï¼ä½å¨çº¿ç¢éåå¨å°å¾ç²¾åº¦ååç´ ä¸°å¯åº¦æ¹é¢ä»è¿æªè¾¾å°HD Mapçæ°´å¹³ï¼æç¥èå´æéï¼ä¸å¨å¤æé®æ¡åºæ¯ä¸æ§è½åå½±åãä¼åç»´æ¤é¢ä¸´æ°æ®ä¼ è¾éå¤§ãç¨æ·éç§åå°å¾æ°é²åº¦ç­ææã
*   <strong>Implicit Mapçå±éæ§ï¼</strong>
    *   <strong>æ¥è¯¢è¡¨ç¤ºæ¹æ³ï¼</strong> è®¡ç®æçä¸è¡¨ç¤ºè½åä¹é´å­å¨æè¡¡ï¼ä¾èµé«è´¨éè¾å¥ï¼ä¸ç¼ºä¹æ­£å¼çå®å¨éªè¯æºå¶ãè·¨æ¨¡ææ¥å°å­å¨æ¨¡ç³æ§ï¼å æå¯è§£éæ§ä¸è¶³ï¼æ¶é´é²æ£æ§æéã
    *   <strong>æ½å¨ç©ºé´æ¹æ³ï¼</strong> BEVæå½±åæ·±åº¦ä¼°è®¡è¯¯å·®åé®æ¡ä¼ªå½±å½±åï¼å¾åºæ¹æ³ä¾èµé¢å®ä¹æææ¨¡æ¿ï¼å¯¹è±¡ä¸­å¿æ¹æ³æåä¸æ¸¸æ£æµè¯¯å·®å½±åãå¨ææç¥æ½å¨å»ºæ¨¡è®¡ç®å¤æï¼éçº¿æ§è¿å¨å»ºæ¨¡ä¸è¶³ï¼å¯¹æ°æ®ç¨çæ§ææãé²æ£æ§å¢å¼ºæ¹æ³ä¾èµé«è´¨éçå®æ°æ®ï¼å¯¹æè®­ç»ååéåºç¼ºä¹å¯éåé²æ£æ§è¾¹çï¼åææ°æ®å®å¨éªè¯æºå¶ç¼ºå¤±ã
    *   <strong>NeRFæ¹æ³ï¼</strong> ç¬æå¨æå¤çå¯¼è´ä¼ªå½±ï¼è¯­ä¹åè¾¨çéå¶å°ç©ä½è¯å«ï¼æ ¡åè¯¯å·®å½±åä¼ æå¨èåã
    *   <strong>ä¸çæ¨¡åï¼</strong> å¯éè¡¨ç¤ºè®¡ç®ååå­å¼éå¤§ï¼å¯¹è±¡ä¸­å¿æ¹æ³æåä¸æ¸¸æç¥æ¨¡åè¯¯å·®ä¼ æ­å½±åï¼å¾åºæ¨¡åä¾èµå®ä¹å³ç³»çå®æ´æ§ï¼çº¯è§è§æ¨¡åå¨æ¶å£æ¡ä»¶ä¸é²æ£æ§æéãç©çæ¨¡åè®¡ç®ææ¬é«ï¼åå¼è®ºæ¹æ³å¤æä¸æ¨¡åç®åï¼çææ¨¡åæ¨çéåº¦æ¢ä¸ç©çå¯æ§æ§å·®ï¼è´å¶æ¯æ¹æ³è®¡ç®å¯éä¸é¾ä»¥åºåä¸ç¡®å®æ§ãæ¨¡åå¢å¼ºæ¹æ³éªè¯èå´æéï¼è®­ç»å¤æä¸ä¸ç¨³å®ï¼åå­¦ä¹ ä¾èµåè®­ç»æ°æ®å¤æ ·æ§ï¼LLMæ¨¡åæ¨çå»¶è¿é«ãæâå¹»è§âï¼åææ°æ®å­å¨âç°å®é¸¿æ²âã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨èªå¨è½»éåå°å¾ï¼Fully Automatic Lite Mapï¼ï¼</strong> æç»­æ¹è¿Lite Mapï¼å®ç°æ°æ®ééãå¤çåæ´æ°çæ ç¼éæï¼æ éäººå·¥å¹²é¢ï¼ä»¥å®ç°å¯æ©å±åç»æµé«æçå°å¾ç»´æ¤ã
*   <strong>é¶æ ·æ¬å°å¾å­¦ä¹ ï¼Zero-shot Map Learningï¼ï¼</strong> å¼åè½å¤è¯å«ç¨æææ°åéè·¯ç¹å¾çæ¨¡åï¼åå°å¯¹æ æ³¨æ°æ®çä¾èµï¼æé«æ¨¡åå¯¹æªç¥ç¯å¢çæ³åè½åï¼å¹¶ç»åä¸ç¡®å®æ§ä¼°è®¡ä»¥æé«å®å¨æ§åå¯é æ§ã
*   <strong>VLAä¸­çéå¼å°å¾ï¼Implicit Map in VLAï¼ï¼</strong> æ¢ç´¢å¦ä½å°å°å¾ç»éªå­å¨å¨äºç«¯ä½ä¸ºâè®°å¿ç¥è¯åºâæâç»éªå±âï¼ä»¥è¾å©è½¦è¾æ´å¥½å°çè§£é©¾é©¶åºæ¯ï¼å¹¶æ¯æVLAæ¨¡åéè¿å¤æ¨¡æè®­ç»æ°æ®éå¼çè§£å°å¾ç»æã
*   <strong>åºç¡æ¨¡åï¼Foundation Modelï¼çåºç¨ï¼</strong> å©ç¨åºç¡æ¨¡åå­¦ä¹ æ½å¨ç©ºé´ç»æåè¯­ä¹ä¸ä¸æï¼å®ç°å¤æ¨¡æéæï¼è§è§ãè¯­è¨ãæ¶é´ä¸ä¸æï¼ï¼å¹¶éè¿Transformerçæ³¨æåæºå¶å¨ææ£ç´¢ç©ºé´åè¯­ä¹ç¸å³ä¿¡æ¯ï¼ä»èæ¯æéå¼å®ä½åå°å¾æå»ºã
*   <strong>æ··åå»ºæ¨¡æ¹æ³ï¼</strong> ç»åç»æååéªç¥è¯ä¸ç¥ç»ç½ç»ï¼å¼åå¯è§£éçè¡¨ç¤ºå­¦ä¹ ææ¯ç¨äºå³é®åºæ¯åæï¼å¹¶å¶å®æ ååè¯ä¼°ææ ä»¥å¼é¡¾æ§è½åå®å¨è¦æ±ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>For each stage, we provide a comprehensive
review of the map production workflow, with highlighting technical challenges
involved and summarizing relevant solutions proposed by the academic community.</li>
<li>Furthermore, we discuss cutting-edge research advances in map representations
and explore how these innovations can be integrated into end-to-end autonomous
driving frameworks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12632v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12632v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12452v1'></a></p>
<h2 id="deep-learning-for-3d-point-cloud-processing-from-approaches-tasks-to-its-implications-on-urban-and-environmental-applications"><a href="https://arxiv.org/abs/2509.12452v1">Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</a></h2>
<p><strong>Authors:</strong> Zhenxin Zhang, Zhihua Xu, Yuwei Cao, Ningli Xu, Shuye Wang, Shen'ao Cui, Zhen Li, Rongjun Qin</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Point cloud processing as a fundamental task in the field of geomatics and
computer vision, has been supporting tasks and applications at different scales
from air to ground, including mapping, environmental monitoring, urban/tree
structure modeling, automated driving, robotics, disaster responses etc. Due to
the rapid development of deep learning, point cloud processing algorithms have
nowadays been almost explicitly dominated by learning-based approaches, most of
which are yet transitioned into real-world practices. Existing surveys
primarily focus on the ever-updating network architecture to accommodate
unordered point clouds, largely ignoring their practical values in typical
point cloud processing applications, in which extra-large volume of data,
diverse scene contents, varying point density, data modality need to be
considered. In this paper, we provide a meta review on deep learning approaches
and datasets that cover a selection of critical tasks of point cloud processing
in use such as scene completion, registration, semantic segmentation, and
modeling. By reviewing a broad range of urban and environmental applications
these tasks can support, we identify gaps to be closed as these methods
transformed into applications and draw concluding remarks in both the
algorithmic and practical aspects of the surveyed methods.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæâDeep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applicationsâç±Zhenxin Zhangç­äººæ°åï¼å¯¹æ·±åº¦å­¦ä¹ å¨3Dç¹äºå¤çåå¶å¨åå¸åç¯å¢åºç¨ä¸­çä½ç¨è¿è¡äºå¨é¢çåç»¼è¿°ã</p>
<p>ä»¥ä¸æ¯è¯¥è®ºæçæè¦ï¼</p>
<ol>
<li>
<p><strong>ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
    è¯¥è®ºææ¨å¨è§£å³ç°æå³äºç¹äºå¤çæ·±åº¦å­¦ä¹ ç»¼è¿°çä¸è¶³ãç°æç»¼è¿°ä¸»è¦å³æ³¨ä¸æ­æ´æ°çç½ç»æ¶æä»¥éåºæ åºç¹äºï¼ä½å¾å¾å¿½ç¥äºè¿äºæ¹æ³å¨å®éç¹äºå¤çåºç¨ä¸­çå®ç¨ä»·å¼ï¼å°¤å¶æ¯å¨å¤çè¶å¤§æ°æ®éãå¤æ ·åºæ¯åå®¹ãä¸åç¹äºå¯åº¦åæ°æ®æ¨¡ææ¶ãå æ­¤ï¼è¯¥ç ç©¶çæ ¸å¿é®é¢æ¯ï¼å¦ä½æä¾ä¸ä¸ªå¨é¢çåç»¼è¿°ï¼è¿æ¥æ·±åº¦å­¦ä¹ ç®æ³ä¸å®éç¹äºå¤çä»»å¡åå¶å¨åå¸åç¯å¢åºç¨ä¸­çæ½å¨ä»·å¼ï¼å¹¶è¯å«è¿äºæ¹æ³å¨è½¬åä¸ºå®éåºç¨æ¶éè¦å¼¥è¡¥çå·®è·ã</p>
</li>
<li>
<p><strong>å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong></p>
<ul>
<li><strong>åç»¼è¿°æ¹æ³ï¼</strong> è®ºæéç¨åç»¼è¿°çæ¹æ³ï¼èéä»ä»å³æ³¨ææ°çç½ç»æ¶æï¼èæ¯å°æ·±åº¦å­¦ä¹ æ¹æ³åæ°æ®éä¸ç¹äºå¤ççå³é®ä»»å¡ï¼å¦åºæ¯è¡¥å¨ãéåãè¯­ä¹åå²åå»ºæ¨¡ï¼èç³»èµ·æ¥ã</li>
<li><strong>æ°æ®æºçå¨é¢æ¦è¿°ï¼</strong> è¯¦ç»ä»ç»äºåç§ç¹äºæ°æ®ééæ¹æ³ï¼åæ¬LiDARãæå½±æµéãç»æåç³»ç»ãååº¦ç«ä½ãSARå¹²æ¶æµéåæ··åç³»ç»ï¼å¹¶åæäºå®ä»¬çç¹ç¹ãææ¬ãå¤ææ§åç²¾åº¦ã</li>
<li><strong>ä»»å¡ä¸åºç¨çè¿æ¥ï¼</strong> è®ºææç¡®å°å°ç¹äºå¤çä»»å¡ä¸å¹¿æ³çåå¸åç¯å¢åºç¨ï¼å¦åå¸å»ºæ¨¡ãæä¸ãåä¸ãçæå­¦åå¬ç¨äºä¸æµç»ï¼èç³»èµ·æ¥ï¼çªåºäºæ·±åº¦å­¦ä¹ å¨è¿äºé¢åä¸­çå®éæ¯æä½ç¨ã</li>
<li><strong>è¯å«å·®è·åææï¼</strong> è®ºæä¸ä»æ»ç»äºç°ææ¹æ³çæå°±ï¼è¿æç¡®æåºäºè¿äºæ¹æ³å¨æ¨å¹¿å°å®éåºç¨ä¸­æ¶é¢ä¸´çææï¼ä¾å¦æ³åè½åãå¤çå¤§è§æ¨¡æ°æ®ãè®¡ç®æçåæ¨¡åå¯è§£éæ§ã</li>
</ul>
</li>
<li>
<p><strong>ä¸»è¦ç»æåå¶æä¹ï¼</strong></p>
<ul>
<li><strong>æ·±åº¦å­¦ä¹ å¨ç¹äºå¤çä¸­çä¸»å¯¼å°ä½ï¼</strong> è®ºæå¼ºè°ï¼æ·±åº¦å­¦ä¹ æ¹æ³å·²å ä¹å®å¨ä¸»å¯¼äºç¹äºå¤çç®æ³ï¼æ¾èæåäºåºæ¯è¡¥å¨ãéåãè¯­ä¹åå²åå ä½å»ºæ¨¡ç­ä»»å¡çæ§è½ã</li>
<li><strong>å¹¿æ³çåºç¨æ½åï¼</strong> æ·±åº¦å­¦ä¹ å¨ç¹äºå¤çä¸­çåºç¨èå´å¹¿æ³ï¼ä»æºè½åå¸åºç¡è®¾æ½ç®¡çãè½æºç®¡çãæåéäº§ä¿æ¤ãéè·¯ç½ç»æµç»å°ç¾å®³ç®¡çãæä¸ãåä¸åçæçæµç­ï¼é½å±ç°åºå·¨å¤§çæ½åã</li>
<li><strong>æ¹æ³è®ºçåè´¨æ§ï¼</strong> å°½ç®¡ç¹äºä»»å¡å¤æ ·ï¼ä½ç¹å¾æåè¿ç¨å¤§å¤éµå¾ªåºäºä½ç´ ãåºäºè§å¾ååºäºç¹çæ¹æ³ï¼è¿è¡¨æç ç©¶äººååå¼åèå¯ä»¥å¨ä¸åä»»å¡ä¹é´å©ç¨ç¸ä¼¼çç®æ³åæ¦å¿µã</li>
<li><strong>æ§è½æåï¼</strong> ç°æå·¥ä½è¡¨æï¼ä¸æ­æ´æ°çç½ç»æ¶æåå±äº«è®­ç»æ°æ®çå¢å ï¼ä½¿å¾æ·±åº¦å­¦ä¹ å¨åºåæ°æ®éä¸çæ§è½å¾å°äºæ¾èæåã</li>
</ul>
</li>
<li>
<p><strong>å±éæ§ï¼</strong></p>
<ul>
<li><strong>æ³åè½åä¸è¶³ï¼</strong> å°½ç®¡å¨åºåæ°æ®éä¸è¡¨ç°è¯å¥½ï¼ä½è¿äºæ¹æ³å¨åºç¨äºæªè§è¿çæ°æ®éæ¶ï¼å¶æ³åè½ååæ§è½ä¸è´æ§å°æªå¾å°ååéªè¯ã</li>
<li><strong>å¤§è§æ¨¡æ°æ®å¤çææï¼</strong> ç°ææ¹æ³å¨å¤çå°çç©ºé´å°ºåº¦ä¸çè¶å¤§æ°æ®éæ¶ï¼è®¡ç®æçååå­æ¶èä»ç¶æ¯ä¸»è¦ææï¼å°¤å¶æ¯å¨ç²¾ç»éåä»»å¡ä¸­ã</li>
<li><strong>æ¨¡åå¯è§£éæ§ï¼</strong> æ·±åº¦å­¦ä¹ æ¨¡åçâé»ç®±âæ§è´¨é»ç¢äºå¶å¨æºè½åå¸æ§å¶ç³»ç»ç­é¢åçè§£éæ§åä¿¡ä»»åº¦ã</li>
<li><strong>æ°æ®è´¨éåå¯ç¨æ§ï¼</strong> æ¨¡åçåç¡®æ§åå®æ´æ§é«åº¦ä¾èµäºé«è´¨éåå¯ç¨çå¤§è§æ¨¡è®­ç»æ°æ®éï¼è¿å¨å®éåºç¨ä¸­å¯è½é¾ä»¥è·å¾ã</li>
<li><strong>æå¨å·¥ä½éï¼</strong> å³ä½¿ææ·±åº¦å­¦ä¹ çè¾å©ï¼è®¸å¤å»ºæ¨¡è¿ç¨ï¼å¦LoD3æ¨¡åççæï¼ä»ç¶éè¦å¤§éæå¨å·¥ä½ã</li>
</ul>
</li>
<li>
<p><strong>æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong></p>
<ul>
<li><strong>æåæ³åè½åï¼</strong> æªæ¥çç ç©¶åºä¾§éäºå¼åå·ææ´å¼ºæ³åè½åçç®æ³ï¼ä½¿å¶å¨åºç¨äºä¸åé¢åååºæ¯çæªè§è¿æ°æ®éæ¶è½ä¿æä¸è´çæ§è½ã</li>
<li><strong>ä¼åå¤§è§æ¨¡æ°æ®å¤çï¼</strong> éè¦å¼åæ´é«æçç®æ³åæ¶æï¼ä»¥å¤çå°çç©ºé´å°ºåº¦ä¸çè¶å¤§æ°æ®éï¼å¹¶ä¼åè®¡ç®æçååå­ä½¿ç¨ã</li>
<li><strong>å¢å¼ºæ¨¡åå¯è§£éæ§ï¼</strong> æ¢ç´¢æé«æ·±åº¦å­¦ä¹ æ¨¡åå¯è§£éæ§çæ¹æ³ï¼ä»¥å¢å¼ºç¨æ·å¯¹æ¨¡åçä¿¡ä»»åçè§£ï¼å°¤å¶æ¯å¨å³é®å³ç­é¢åã</li>
<li><strong>å¤æºæ°æ®èåï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½ææèåå¤æºç¹äºæ°æ®ï¼å¦LiDARãå¾åãå¤åè°±æ°æ®ç­ï¼ï¼ä»¥è·åæ´ä¸°å¯ãæ´å¨é¢çåºæ¯ä¿¡æ¯ã</li>
<li><strong>èªå¨åä¸å®æ¶åºç¨ï¼</strong> æ¨å¨ç®æ³åæ´é«èªå¨åæ°´å¹³åå±ï¼å¹¶ä¼åæ¨çæ¶é´ï¼ä»¥æ¯æå®æ¶äº¤éç®¡çåç¾å®³ååºç­åºç¨ã</li>
<li><strong>å¡«è¡¥æ°æ®å·®è·ï¼</strong> è§£å³æ°æ®ç¨ç¼ºåä¸ææ°æ®æ ¼å¼é®é¢ï¼ä»¥ä¿è¿é«è´¨éè®­ç»æ°æ®éçåå»ºåå±äº«ã</li>
</ul>
</li>
</ol>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºæ·±åº¦å­¦ä¹ å¨3Dç¹äºå¤çé¢åçåºç¨æä¾äºä¸ä¸ªå¨é¢çè§è§ï¼å¼ºè°äºå¶å¨åå¸åç¯å¢åºç¨ä¸­çå·¨å¤§æ½åï¼åæ¶ä¹æç¡®æåºäºå½åæ¹æ³å¨å®éé¨ç½²ä¸­é¢ä¸´çææåæªæ¥ç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we provide a meta review on deep learning approaches
and datasets that cover a selection of critical tasks of point cloud processing
in use such as scene completion, registration, semantic segmentation, and
modeling.</li>
<li>By reviewing a broad range of urban and environmental applications
these tasks can support, we identify gaps to be closed as these methods
transformed into applications and draw concluding remarks in both the
algorithmic and practical aspects of the surveyed methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12452v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12452v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12197v1'></a></p>
<h2 id="3d-human-pose-and-shape-estimation-from-lidar-point-clouds-a-review"><a href="https://arxiv.org/abs/2509.12197v1">3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</a></h2>
<p><strong>Authors:</strong> Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet</p>
<p><strong>Published:</strong> 2025-09-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Salma Galaaoui, Eduardo Valle, David Picard, Nermin Sametæ°åçè®ºæâ3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Reviewâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</strong></p>
<p>è¿ç¯è®ºæå¯¹ä»âéå¤âLiDARç¹äºä¸­è¿è¡3Däººä½å§¿æä¼°è®¡ï¼3D HPEï¼åäººä½ç½æ ¼æ¢å¤ï¼HMRï¼çç°ææ¹æ³è¿è¡äºå¨é¢åé¡¾ãéçLiDARä¼ æå¨å¨èªå¨é©¾é©¶åæºå¨äººé¢åçæ®åï¼ä»¥åç¸å³æ°æ®éçåå¸ï¼å©ç¨LiDARæ°æ®è¿è¡äººä½çè§£æä¸ºä¸ä¸ªæ¥çéè¦çç ç©¶æ¹åã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è®ºæä¸»è¦å³æ³¨å¦ä½ä»ç¨çãä¸è§åãå¯è½å­å¨é®æ¡ååªå£°çâéå¤âLiDARç¹äºä¸­åç¡®å°ä¼°è®¡3Däººä½å§¿æåæ¢å¤è¯¦ç»çäººä½ç½æ ¼ãä¼ ç»åºäºå¾ååè§é¢çæ¹æ³å¨æ·±åº¦ä¿¡æ¯ãéç§ä¿æ¤åé²æ£æ§æ¹é¢å­å¨å±éæ§ï¼èLiDARè½ç¶æä¾äºç²¾ç¡®ç3Då ä½ä¿¡æ¯ï¼ä½ä¹å¸¦æ¥äºèªèº«çæ°æ®ææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»æååç±»æ³ï¼</strong> è®ºææåºäºä¸ä¸ªç»æåçåç±»æ³ï¼ç¨äºå¯¹ç°æç3D HPEåHMRæ¹æ³è¿è¡åç±»ï¼å¹¶åæäºå®ä»¬çä¼å¿ãå±éæ§åè®¾è®¡éæ©ãè¯¥åç±»æ³åºäºå­¦ä¹ èå¼ï¼çç£ãå¼±çç£ãæ çç£ï¼ãè¾å¥æ¨¡æï¼ä»LiDARæå¤æ¨¡æèåï¼åç½ç»æ¶æï¼å¦TransformerãPointNetåä½ç­ï¼ã
*   <strong>æ¹æ³åæï¼</strong> è¯¦ç»åæäº32é¡¹2019å¹´è³2025å¹´é´åè¡¨çç ç©¶ï¼æ¶µçäºç¨çæ§å¤çãTransformerä½ä¸ºéª¨å¹²ç½ç»ãè¶è¶3Då§¿æçè¾å©çç£ä»»å¡ãåææ°æ®å­¦ä¹ ãå¼±çç£ä¸çæ æ³¨é¸¿æ²å¼¥åä»¥åå¤æ¨¡æèåç­å³é®æ¹é¢ã
*   <strong>æ°æ®éå®éæ¯è¾ï¼</strong> å¯¹Waymo Open DatasetãSLOPER4DåHuman-M3è¿ä¸ä¸ªæå¹¿æ³ä½¿ç¨çLiDARäººä½å§¿ææ°æ®éè¿è¡äºå®éæ¯è¾ï¼è¯¦ç»éè¿°äºå®ä»¬çç¹æ§ãééæ¹å¼ãæ°æ®æ ¼å¼ååå¨å±æ§ï¼å¦ç¹äºå¯åº¦ãäººä½-ä¼ æå¨è·ç¦»ã3Då§¿æå¤æ ·æ§ç­ï¼ã
*   <strong>ç»ä¸è¯ä¼°ææ ï¼</strong> æ´çå¹¶ç»ä¸äºææç¨äºè¯ä¼°LiDARç¹äº3D HPEåHMRæ¹æ³çè¯ä¼°ææ å®ä¹ï¼åæ¬MPJPEãPA-MPJPEãPCKãPEMãMPVPEãMPEREãADEãLAEãLLEãAccel ErroråChamfer Distanceç­ã
*   <strong>åºåæµè¯è¡¨ï¼</strong> å»ºç«äºå¨è¿ä¸ä¸ªæ°æ®éä¸éå¯¹3D HPEåHMRä»»å¡çåºåæµè¯è¡¨ï¼æ¨å¨ä¿è¿å¬å¹³æ¯è¾åé¢åè¿å±ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¯¹ç°ææ¹æ³çæ·±å¥åæååºåæµè¯ï¼æ­ç¤ºäºLiDAR-based 3D HPEåHMRé¢åçææ°è¿å±åææãç ç©¶è¡¨æï¼å¤æ¨¡æèåï¼ç¹å«æ¯LiDARä¸RGBå¾åæIMUçèåï¼åå©ç¨åææ°æ®è¿è¡é¢è®­ç»æ¯è§£å³æ°æ®ç¨çæ§åæ æ³¨ä¸è¶³çå³é®ç­ç¥ãTransformeræ¶æå å¶å¨å±æåéåå¤çä¸è§åæ°æ®çè½åï¼å¨LiDAR-basedä»»å¡ä¸­è¡¨ç°åºå¼ºå¤§æ½åãå¼±çç£æ¹æ³éè¿2Dä¼ªæ ç­¾ãæå½±ä¸è´æ§åè¾å©ä»»å¡ææå¼¥åäºæ æ³¨é¸¿æ²ãè¿äºåç°ä¸ºç ç©¶äººåæä¾äºæ¸æ°çè·¯çº¿å¾ï¼ä»¥çè§£å½åææ¯æ°´å¹³å¹¶æå¯¼æªæ¥ç ç©¶æ¹åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®ç¨ç¼ºæ§ï¼</strong> ç¼ºä¹å¤§è§æ¨¡ãé«è´¨éæ æ³¨çLiDARæ°æ®éæ¯æ ¸å¿ææã
*   <strong>å¯¹è¾å©æ¨¡æçä¾èµï¼</strong> å¤æ°å¼±çç£æ¹æ³ä»ä¸¥éä¾èµRGBå¾åæIMUä¿¡å·ç­è¾å©æ°æ®æ¨¡æã
*   <strong>åææ°æ®çæï¼</strong> å½ååææ°æ®çæç®¡éå­å¨é¢åä¸å¹éï¼å®¤åå§¿æä¸å®¤å¤åºæ¯ï¼åçå®æ§å·®è·ï¼æªè½å®å¨ææçå®LiDARä¼ æå¨çåªå£°ãç¨çæ§åè§è§ç¹æ§ï¼ã
*   <strong>ç¸æºåæ°ä¾èµï¼</strong> å¤æ°å¤æ¨¡ææ¹æ³é«åº¦ä¾èµç²¾ç¡®çç¸æºåæ°è¿è¡2D-3Då¯¹åºï¼è¿å¨å®éåºç¨ä¸­å¸¦æ¥äºææã
*   <strong>ä¼ æå¨åå·®å¼ï¼</strong> ä¸åLiDARä¼ æå¨ç¹æ§ï¼ç¹äºå¯åº¦ãèå´ãåªå£°æ¨¡å¼ï¼å¯¼è´æ¨¡åå¨ä¸åæ°æ®éä¹é´æ³åè½åå·®ã
*   <strong>æ«ææ¨¡å¼å·®å¼ï¼</strong> ä¸åLiDARæ«ææ¨¡å¼ï¼NRSåRMBï¼äº§çç»æä¸åçç¹äºåå¸ï¼éè¦é²æ£çæ¶ææéåºç­ç¥ã
*   <strong>å¼±çç£HMRçæ¢ç´¢ä¸è¶³ï¼</strong> ç¸æ¯3D HPEï¼å¼±çç£HMRæ¹æ³çç ç©¶ç¸å¯¹è¾å°ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>åå°å¯¹è¾å©æ¨¡æçä¾èµï¼</strong> æ¢ç´¢ä»ä½¿ç¨LiDARæ°æ®å®ç°å¼±çç£HPEåHMRçæ¹æ³ï¼ä¾å¦éè¿ä¼ªæ ç­¾ãèªè®­ç»æå¯¹æ¯å­¦ä¹ ã
*   <strong>æ´åæ¶é´ä¿¡æ¯ï¼</strong> å©ç¨LiDARå¸§åºåçæ¶é´è¿è´¯æ§ï¼æåæ¶é´çº¿ç´¢ä»¥å¢å¼ºå§¿æä¼°è®¡ç²¾åº¦ï¼èæ éé¢å¤çç£ã
*   <strong>æ´çå®çåææ°æ®çæï¼</strong> åå±è½å¤ç´æ¥ä»çå®ä¸çåå¸ä¸­çæåæLiDARæ°æ®çæ¹æ³ï¼ç¹å«æ¯å©ç¨æ©æ£æ¨¡åç­çæå¼æ¨¡åã
*   <strong>æ°æ®é«æå­¦ä¹ ï¼</strong> é¢è®­ç»æ¨¡åå¹¶ä»¥æå°çç£è¿è¡å¾®è°ï¼ä»¥æé«LiDAR-based HPEåHMRçæ°æ®æçã
*   <strong>å¼±çç£3D HMRï¼</strong> æ·±å¥æ¢ç´¢ä¸é¨éå¯¹HMRçå¼±çç£æ¹æ³ã
*   <strong>æ¶é¤ç¸æºåæ°ä¾èµï¼</strong> å¼åå¯å­¦ä¹ æ¨¡åï¼å®ç°ç«¯å°ç«¯å¯¹é½ï¼æ éæ¾å¼æ ¡åã
*   <strong>åéåºææ¯ï¼</strong> éå¯¹ä¸åLiDARä¼ æå¨ç¹æ§åæ«ææ¨¡å¼ä¹é´çåå·®å¼ï¼å¼ååéåºææ¯ï¼ä»¥æé«æ¨¡åçæ³åè½åã</p>
<hr />
<p>è¿ç¯ç»¼è¿°è®ºæä¸ºLiDAR-based 3Däººä½çè§£é¢åæä¾äºä¸ä¸ªå¨é¢çæ¦è§ï¼ä¸ä»ç³»ç»å°åç±»ååæäºç°ææ¹æ³ï¼è¿éè¿éåæ¯è¾ååºåæµè¯ä¸ºæªæ¥çç ç©¶å¥ å®äºåºç¡ï¼å¹¶ææäºå³é®çå¼æ¾ææåæåæ¯çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds.</li>
<li>Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12197v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12197v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13210v1'></a></p>
<h2 id="vi-safe-a-spatial-temporal-framework-for-efficient-violence-detection-in-public-surveillance"><a href="https://arxiv.org/abs/2509.13210v1">Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</a></h2>
<p><strong>Authors:</strong> Ligang Chang, Shengkai Xu, Liangchang Shen, Binhan Xu, Junqiao Wang, Tianyu Shi, Yanhui Du</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV, I.2.10; I.4.8</p>
<p><strong>Abstract:</strong></p>
<p>Violence detection in public surveillance is critical for public safety. This
study addresses challenges such as small-scale targets, complex environments,
and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal
framework that integrates an enhanced YOLOv8 with a Temporal Segment Network
(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as
a lightweight backbone, an exponential moving average (EMA) attention
mechanism, and pruning to reduce computational cost while maintaining accuracy.
YOLOv8 and TSN are trained separately on pedestrian and violence datasets,
where YOLOv8 extracts human regions and TSN performs binary classification of
violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE
achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming
existing methods in both accuracy and efficiency, demonstrating its
effectiveness for public safety surveillance. Code is available at
https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ligang Changç­äººæ°åçè®ºæâVi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillanceâçå¨é¢æè¦ã</p>
<hr />
<h3 id="vi-safe">è®ºæãVi-SAFE: ä¸ç§ç¨äºå¬å±çæ§ä¸­é«ææ´åæ£æµçæ¶ç©ºæ¡æ¶ãæè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¬å±çæ§ä¸­é«ææ´åè¡ä¸ºæ£æµçææãå·ä½æ¥è¯´ï¼å®å³æ³¨äºå¦ä½åæå°ç®æ ãå¤æç¯å¢ä»¥åå®æ¶æ¶åºåæçé¾é¢ï¼ä»¥æé«å¬å±å®å¨æ°´å¹³ãç°æçæ¹æ³å¾å¾å¨åç¡®æ§ãè®¡ç®æçæå®æ¶æ§æ¹é¢å­å¨ä¸è¶³ï¼å°¤å¶æ¯å¨èµæºåéçè¾¹ç¼è®¾å¤ä¸é¨ç½²æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
Vi-SAFEæ¡æ¶çæ ¸å¿åæ°å¨äºå¶ç¬ç¹çæ¶ç©ºéææ¹æ³åå¯¹YOLOv8æ¨¡åçä¼åï¼
*   <strong>æ¶ç©ºéææ¡æ¶ (Vi-SAFE)ï¼</strong> è®ºææåºäºä¸ç§æ°é¢çç«¯å°ç«¯æ¶ç©ºæ¡æ¶ï¼å°ä¼åçYOLOv8sç®æ æ£æµå¨ä¸æ¶åºåå²ç½ç» (TSN) ç¸ç»åãYOLOv8sè´è´£å¨è§é¢å¸§ä¸­åç¡®å°å®ä½æ½å¨æ´åæ´»å¨ä¸­çä¸ªä½ï¼å³æåäººä½åºåï¼ï¼èTSNåå¯¹è¿äºæå´è¶£åºå (ROIs) çæ¶åºå¨æè¿è¡å»ºæ¨¡ï¼ä»¥å¤æ­æ´åè¡ä¸ºãè¿ç§å2D CNNç»æå¨å®ç°é«åç¡®æ§çåæ¶ï¼ä¿æäºè¾ä½çè®¡ç®å¤æåº¦ã
*   <strong>YOLOv8sæ¨¡åä¼å (GE-YOLOv8)ï¼</strong>
    *   <strong>è½»éçº§éª¨å¹²ç½ç»ï¼</strong> å°YOLOv8sçéª¨å¹²ç½ç»æ¿æ¢ä¸ºGhostNetV3ï¼éè¿å»ä»·ççº¿æ§æä½çæé¢å¤çç¹å¾å¾ï¼æ¾èåå°äºFLOPsååæ°ï¼åæ¶ä¿æäºåç¡®çç¹å¾è¡¨ç¤ºã
    *   <strong>EMAæ³¨æåæºå¶ï¼</strong> å¨éª¨å¹²ç½ç»çåæå±ä¸­å¼å¥äºææ°ç§»å¨å¹³å (EMA) æ³¨æåæºå¶ï¼éè¿éæ°å æç¹å¾å¾æ¥çªåºä¿¡æ¯åºåå¹¶æå¶èæ¯åªå£°ï¼ä»èæé«å¯¹å°ç®æ æé®æ¡ç®æ çæ£æµï¼å¹¶å¢å¼ºå¤æåºæ¯ä¸­çæ¶åºä¸è´æ§ã
    *   <strong>åªæææ¯ï¼</strong> åºç¨åºäºGroupNorméè¦æ§çééåªæææ¯ï¼è¿ä¸æ­¥åç¼©æ¨¡åï¼å¨ä¿æåç¡®æ§çåæ¶éä½äºåæ°åGFLOPsã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>GE-YOLOv8çæ§è½ï¼</strong> ç»è¿GhostNetV3ãEMAæ³¨æåæºå¶ååªæä¼ååçGE-YOLOv8æ¨¡åï¼å¨ä¿æé«åç¡®æ§ï¼mAP 0.737ï¼çåæ¶ï¼æ¾èåå°äºåæ°ï¼åå°çº¦ä¸åï¼åGFLOPsï¼åå°çº¦ä¸åï¼ï¼ä½¿å¶éå¸¸éåå®æ¶è¾¹ç¼é¨ç½²ã
*   <strong>Vi-SAFEçæ´ä½æ§è½ï¼</strong> å¨RWF-2000æ°æ®éä¸çå®éªè¡¨æï¼Vi-SAFEå®ç°äº0.88çåç¡®çï¼æ¾èä¼äºåç¬çTSN (0.77) ä»¥åå¶ä»ä¸»æµæ¹æ³ï¼å¦U-Net+LSTM (0.820) åC3D (0.828)ï¼çè³ç¥å¾®è¶è¶äºOpenpose+ST-GCN (0.878)ã
*   <strong>æä¹ï¼</strong> è¿äºç»æè¯æäºVi-SAFEå¨å¬å±å®å¨çæ§ä¸­è¿è¡æ´åæ£æµçæææ§ãé«ææ§åé²æ£æ§ãå¶æ¨¡ååæ¶æä¹ç¡®ä¿äºå¯æ©å±æ§ï¼ä¾¿äºéæé¢å¤çæ¨¡åä»¥éåºæ´å¹¿æ³çåºç¨åºæ¯ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåºå½åVi-SAFEæ¡æ¶çå±éæ§ï¼ä½ä»æªæ¥ç ç©¶æ¹åå¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹è¿ç©ºé´ï¼
*   <strong>æ³åè½åï¼</strong> å°½ç®¡Vi-SAFEå¨RWF-2000æ°æ®éä¸è¡¨ç°åºè²ï¼ä½å¶å¨æ´å¤æãæ´å¤æ ·åçç¯å¢ä¸­çæ³åè½åä»æå¾è¿ä¸æ­¥éªè¯ã
*   <strong>ä¼åç­ç¥ï¼</strong> å°½ç®¡å·²ç»è¿è¡äºæ¨¡åä¼åï¼ä½å¯è½ä»æè¿ä¸æ­¥æåæçååç¡®æ§çç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>è¯ä¼°æ´å¤æ ·åçæ°æ®éï¼</strong> å¨æ´å¤æãæ´å¤æ ·åçç¯å¢ä¸­è¯ä¼°Vi-SAFEçæ³åè½åã
*   <strong>æ¢ç´¢åè¿çä¼åç­ç¥ï¼</strong> è¿ä¸æ­¥ç ç©¶ååºç¨åè¿çä¼åç­ç¥ï¼ä»¥æç»­æé«æ¨¡åçæçåæ§è½ã</p>
<hr />
<p>è¿ä»½æè¦çªåºäºVi-SAFEæ¡æ¶å¨è§£å³å¬å±çæ§ä¸­æ´åæ£æµé®é¢ä¸çåæ°æ§ï¼ç¹å«æ¯å¨å¹³è¡¡åç¡®æ§åè®¡ç®æçæ¹é¢çè´¡ç®ï¼ä½¿å¶æä¸ºè¾¹ç¼è®¾å¤é¨ç½²çæååéã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Vi-SAFE, a spatial-temporal
framework that integrates an enhanced YOLOv8 with a Temporal Segment Network
(TSN) for video surveillance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13210v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13210v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13172v1'></a></p>
<h2 id="whu-stree-a-multi-modal-benchmark-dataset-for-street-tree-inventory"><a href="https://arxiv.org/abs/2509.13172v1">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a></h2>
<p><strong>Authors:</strong> Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Street trees are vital to urban livability, providing ecological and social
benefits. Establishing a detailed, accurate, and dynamically updated street
tree inventory has become essential for optimizing these multifunctional assets
within space-constrained urban environments. Given that traditional field
surveys are time-consuming and labor-intensive, automated surveys utilizing
Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing
MMS-acquired tree datasets are limited by small-scale scene, limited
annotation, or single modality, restricting their utility for comprehensive
analysis. To address these limitations, we introduce WHU-STree, a cross-city,
richly annotated, and multi-modal urban street tree dataset. Collected across
two distinct cities, WHU-STree integrates synchronized point clouds and
high-resolution images, encompassing 21,007 annotated tree instances across 50
species and 2 morphological parameters. Leveraging the unique characteristics,
WHU-STree concurrently supports over 10 tasks related to street tree inventory.
We benchmark representative baselines for two key tasks--tree species
classification and individual tree segmentation. Extensive experiments and
in-depth analysis demonstrate the significant potential of multi-modal data
fusion and underscore cross-domain applicability as a critical prerequisite for
practical algorithm deployment. In particular, we identify key challenges and
outline potential future works for fully exploiting WHU-STree, encompassing
multi-modal fusion, multi-task collaboration, cross-domain generalization,
spatial pattern learning, and Multi-modal Large Language Model for street tree
asset management. The WHU-STree dataset is accessible at:
https://github.com/WHU-USI3DV/WHU-STree.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ruifei Dingç­äººæ°åçè®ºæâWHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventoryâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼WHU-STreeï¼ç¨äºè¡éæ çç¹ä»»å¡çå¤æ¨¡æåºåæ°æ®é</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æè¡éæ æ°æ®éå¨è§æ¨¡ãæ æ³¨ä¸°å¯æ§åæ¨¡æåä¸æ§æ¹é¢çå±éæ§ï¼è¿äºå±éæ§é»ç¢äºå¯¹åå¸è¡éæ è¿è¡å¨é¢åæåèªå¨åçç¹ãä¼ ç»çå®å°è°æ¥èæ¶è´¹åï¼èç°æçç§»å¨æµç»ç³»ç»ï¼MMSï¼è·åçæ æ¨æ°æ®éå¾å¾åºæ¯è§æ¨¡å°ãæ æ³¨ä¸å®æ´æä»éäºåä¸æ¨¡æï¼æ æ³æ¯æå¤æçåæä»»å¡ãå æ­¤ï¼ç ç©¶èä»¬éè¦ä¸ä¸ªå¤§è§æ¨¡ãå¤æ¨¡æãæ æ³¨ä¸°å¯çè¡éæ æ°æ®éï¼ä»¥æ¨å¨åºäºæ·±åº¦å­¦ä¹ çèªå¨åè¡éæ çç¹ç®æ³çåå±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿è´¡ç®æ¯å¼å¥äºWHU-STreeæ°æ®éï¼å¶å³é®åæ°åæ¹æ³å­¦è´¡ç®åæ¬ï¼
*   <strong>è·¨åå¸ãå¤æ¨¡ææ°æ®éæï¼</strong> WHU-STreeæ°æ®éé¦æ¬¡æ´åäºæ¥èªä¸­å½ä¸¤ä¸ªæ°ååºååå¸è§åå·®å¼æ¾èçåå¸ï¼åäº¬åæ²é³ï¼çåæ­¥ç¹äºåé«åè¾¨çå¾åæ°æ®ãè¿ç§è·¨åå¸è¦çæå©äºè¯ä¼°ç®æ³çæ³åè½ååé²æ£æ§ã
*   <strong>ä¸°å¯çæ æ³¨ï¼</strong> æ°æ®éåå«21,007ä¸ªæ æ³¨çæ æ¨å®ä¾ï¼æ¶µç50ç§æ æ¨ç©ç§å2ä¸ªå½¢æåæ°ï¼æ é«åè¸å¾ï¼ãè¿äºè¯¦ç»æ æ³¨æ¯æå¤ç§ä»»å¡ï¼åæ¬æ ç§åç±»ãåæ£µæ åå²åå½¢æåæ°ä¼°è®¡ã
*   <strong>å¤ä»»å¡æ¯æï¼</strong> å­åå¶ç¬ç¹çç¹æ§ï¼WHU-STreeè½å¤åæ¶æ¯æè¶è¿10ä¸ªä¸è¡éæ çç¹ç¸å³çä»»å¡ï¼åæ¬åæ¨¡ææå¤æ¨¡æè¾å¥ãåä»»å¡æå¤ä»»å¡å­¦ä¹ ãä»¥åè·¨åºåæåºååè¯ä¼°ãè¿ä¸ºå¼åç»¼åæ§è¡éæ çç¹è§£å³æ¹æ¡æä¾äºåºç¡ã
*   <strong>åºåæµè¯ä¸åæï¼</strong> è®ºæå¯¹æ ç§åç±»ååæ£µæ åå²è¿ä¸¤ä¸ªå³é®ä»»å¡çä»£è¡¨æ§åºçº¿ç®æ³è¿è¡äºåºåæµè¯ï¼å¹¶æä¾äºæ·±å¥çå®éªåæï¼å¼ºè°äºå¤æ¨¡ææ°æ®èååè·¨é¢åéç¨æ§çéè¦æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å¤æ¨¡æèåçä¼è¶æ§ï¼</strong> å®éªç»æè¡¨æï¼å¤æ¨¡ææ¹æ³ï¼å¦TSCMDLï¼å¨æ ç§åç±»ä»»å¡ä¸­æ¾èä¼äºåä¸æ¨¡ææ¹æ³ï¼å°¤å¶æ¯å¨mIoUææ ä¸æææåï¼ä¾å¦ï¼TSCMDLæ¯PointMLPæé«äº2.49%ï¼ãè¿å¼ºè°äºå©ç¨å¾åççº¹çååè°±ä¿¡æ¯ä½ä¸ºç¹äºå ä½ä¿¡æ¯çè¡¥åï¼å¯¹äºæé«åç±»åç¡®æ§çéè¦æ§ã
*   <strong>è·¨é¢åæ³åè½åï¼</strong> å¨è·¨åå¸è¯ä¼°ä¸­ï¼ææç®æ³é½è¡¨ç°åºç¨³å¥çæ§è½ï¼å°¤å¶æ¯å¨WHU-STree-SYæ°æ®éä¸ãå¤æ¨¡ææ¹æ³LCPSå¨F1åæ°ä¸çè³è¶è¶äºåæ¨¡ææ¹æ³ï¼è¿è¡¨æå¤æ¨¡æèåæå©äºç¼è§£ç¹äºæ°æ®ä¸­çé¢åæ¼ç§»ï¼æé«äºç®æ³å¨å¼æåå¸ç¯å¢ä¸­çé²æ£æ§ã
*   <strong>å¤ä»»å¡åä½çæ½åï¼</strong> åæ­¥å®éªè¡¨æï¼å°æ ç§åç±»æ´åå°æ æ¨åå²ä»»å¡ä¸­ï¼å¯ä»¥æé«ç½ç»çç»ç²åº¦ç¹å¾æè·è½åï¼ä»èé´æ¥æ¹åæ´ä½åå²ææï¼F1åæ°ä»69.2%æé«å°82.2%ï¼ãè¿ä¸ºå¼åç«¯å°ç«¯çå¤ä»»å¡å­¦ä¹ æ¡æ¶æä¾äºæåè¯æ®ã
*   <strong>ææä¸æºéï¼</strong> å°½ç®¡åå¾äºè¿å±ï¼ä½ç®æ³å¨å¤çå®¶å·å¹²æ°ãæ å éå åååå¹¶å¤±è´¥ç­å¤æåºæ¯æ¶ä»é¢ä¸´ææãæé«çmIoUä»ä¸º64.09%ï¼è¡¨æåºåç¹å®æ ç§ä»æ¯ä¸ä¸ªé¾é¢ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç°ææ°æ®éçå±éï¼</strong> è®ºææåºï¼ç°æçMMSè·åçæ æ¨æ°æ®ééå¸¸åºæ¯è§æ¨¡å°ãæ æ³¨æéæä»éäºåä¸æ¨¡æï¼éå¶äºå¶å¨å¨é¢åæä¸­çå®ç¨æ§ã
*   <strong>ç®æ³æ§è½çå±éï¼</strong> å°½ç®¡å¤æ¨¡æèåæææ¹è¿ï¼ä½ç°æç®æ³çæ´ä½æ§è½ä»ä¸è¶³ä»¥æ»¡è¶³å®éé¨ç½²çéæ±ãä¾å¦ï¼æ ç§åç±»çæé«mIoUä»ä¸º64.09%ï¼è¡¨æå¨åºåç¹å®æ ç§æ¹é¢ä»å­å¨æ¾èææã
*   <strong>ç¹å®åºæ¯çææï¼</strong> ç®æ³å¨å¤çåå¸éè·¯å®¶å·å¹²æ°ãå¯éæ å éå å¯¼è´çæ¬ åå²ä»¥åååå¹¶å¤±è´¥ç­å¤æåºæ¯æ¶ä»å­å¨é®é¢ã
*   <strong>è·¨é¢åæ³åçææï¼</strong> å°½ç®¡WHU-STree-NJæ°æ®éçä¸°å¯æ§æå©äºæ³åï¼ä½ä¸ååå¸é´æ ç§ç»æåå½¢æçæ¾èå·®å¼ä»å¯è½éå¶æ£æµçæ³åè½åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤æ¨¡æèåç­ç¥ï¼</strong> è¿ä¸æ­¥ç ç©¶ä»»å¡ç¹å®çå¤æ¨¡æèåç­ç¥ï¼ä»¥æ´å¥½å°å©ç¨ç¹äºåå¾åæ°æ®ä¹é´çäºè¡¥ä¼å¿ï¼è§£å³æå½±è¯¯å·®åå¤è§è§ä¸ä¸è´ç­é®é¢ã
*   <strong>å¤ä»»å¡åä½æ¡æ¶ï¼</strong> å¼åç«¯å°ç«¯çå¤ä»»å¡å­¦ä¹ æ¨¡åï¼åæ¶æ§è¡åç±»ãåå²ååæ°ä¼°è®¡ï¼ä¿è¿è·¨ä»»å¡ç¥è¯è¿ç§»ï¼å¹¶æ¯ææ´å¨é¢çè¡éæ çç¹ã
*   <strong>è·¨é¢åæ³åï¼</strong> æ¢ç´¢è½å¤å¤çé¢åæ¼ç§»åæé«ç®æ³å¨å¼æåå¸ç¯å¢ä¸­é²æ£æ§çæ¹æ³ï¼ä¾å¦å¼æ¾è¯æ±è¯å«åæ°ç±»å«åç°ãæªæ¥å°æ©å±WHU-STreeå°æ´å¤åå¸ï¼å¹¶å¢å æ²é³æ°æ®éçæ ç§æ æ³¨ã
*   <strong>ç©ºé´æ¨¡å¼å­¦ä¹ ï¼</strong> ç»åè¡éæ çç©ºé´åå¸æ¨¡å¼ï¼å¦çº¿æ§æåãç§æ¤é´éãææå³ç³»ï¼ä½ä¸ºåéªç¥è¯ï¼ä»¥æé«åå²ååç±»æ§è½ï¼å¹¶æ¯æåå¸ç»¿åè§åã
*   <strong>å¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼åºç¨ï¼</strong> å©ç¨MLLMçæ¨çåè·¨æ¨¡æäº¤äºè½åï¼æ´åä¸å®¶ç¥è¯åæ¿åºæ¿ç­ï¼æå»ºä¸ä¸ªæ ç¼çâæç¥-åæ-å³ç­âé­ç¯æ¡æ¶ï¼å®ç°æºè½åçè¡éæ èµäº§ç®¡çï¼åæ¬å¥åº·ç¶åµè¯ä¼°ãç»´æ¤éæ±åé£é©è¯ä¼°ã</p>
<hr />
<p>è¿ä»½æè¦çªåºäºWHU-STreeæ°æ®éä½ä¸ºè®¡ç®æºè§è§é¢åæ°é¢ä¸éè¦çåºåï¼ä¸ºè§£å³åå¸è¡éæ çç¹ä¸­çå®éæææä¾äºåå®åºç¡ï¼å¹¶ä¸ºæªæ¥çå¤æ¨¡æãå¤ä»»å¡åè·¨é¢åç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we introduce WHU-STree, a cross-city,
richly annotated, and multi-modal urban street tree dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13172v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13172v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13133v1'></a></p>
<h2 id="advancing-real-world-parking-slot-detection-with-large-scale-dataset-and-semi-supervised-baseline"><a href="https://arxiv.org/abs/2509.13133v1">Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</a></h2>
<p><strong>Authors:</strong> Zhihao Zhang, Chunyu Lin, Lang Nie, Jiyuan Wang, Yao Zhao</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As automatic parking systems evolve, the accurate detection of parking slots
has become increasingly critical. This study focuses on parking slot detection
using surround-view cameras, which offer a comprehensive bird's-eye view of the
parking environment. However, the current datasets are limited in scale, and
the scenes they contain are seldom disrupted by real-world noise (e.g., light,
occlusion, etc.). Moreover, manual data annotation is prone to errors and
omissions due to the complexity of real-world conditions, significantly
increasing the cost of annotating large-scale datasets. To address these
issues, we first construct a large-scale parking slot detection dataset (named
CRPS-D), which includes various lighting distributions, diverse weather
conditions, and challenging parking slot variants. Compared with existing
datasets, the proposed dataset boasts the largest data scale and consists of a
higher density of parking slots, particularly featuring more slanted parking
slots. Additionally, we develop a semi-supervised baseline for parking slot
detection, termed SS-PSD, to further improve performance by exploiting
unlabeled data. To our knowledge, this is the first semi-supervised approach in
parking slot detection, which is built on the teacher-student model with
confidence-guided mask consistency and adaptive feature perturbation.
Experimental results demonstrate the superiority of SS-PSD over the existing
state-of-the-art (SoTA) solutions on both the proposed dataset and the existing
dataset. Particularly, the more unlabeled data there is, the more significant
the gains brought by our semi-supervised scheme. The relevant source codes and
the dataset have been made publicly available at
https://github.com/zzh362/CRPS-D.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhihao Zhangç­äººçè®ºæâAdvancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baselineâçå¨é¢æè¦ã</p>
<hr />
<h3 id="advancing-real-world-parking-slot-detection-with-large-scale-dataset-and-semi-supervised-baseline_1">è®ºææè¦ï¼Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³èªå¨æ³è½¦ç³»ç»ä¸­åè½¦ä½æ£æµçåç¡®æ§é®é¢ï¼å°¤å¶æ¯å¨çå®ä¸çå¤æåºæ¯ä¸çææãç°ææ°æ®éè§æ¨¡æéï¼ä¸ç¼ºä¹çå®ä¸ççåªå£°ï¼å¦åç§ãé®æ¡ç­ï¼ï¼å¯¼è´æ¨¡åæ³åè½åä¸è¶³ãæ­¤å¤ï¼æå¨æ æ³¨å¤§è§æ¨¡æ°æ®éææ¬é«æä¸æåºéãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¨å¤æå¤åççå®ä¸çç¯å¢ä¸­ï¼å®ç°é«ç²¾åº¦ãé²æ£çåè½¦ä½æ£æµï¼å¹¶ææå©ç¨æªæ æ³¨æ°æ®ä»¥éä½æ æ³¨ææ¬ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>CRPS-Då¤§è§æ¨¡æ°æ®éï¼</strong> è®ºææå»ºäºä¸ä¸ªåä¸ºCRPS-Dçå¤§è§æ¨¡çå®ä¸çåè½¦ä½æ£æµæ°æ®éãè¯¥æ°æ®éå¨è§æ¨¡ä¸è¿è¶ç°ææ°æ®éï¼åå«æ´å¤æ ·åçåç§åå¸ãå¤©æ°æ¡ä»¶åæ´å·æææ§çåè½¦ä½åä½ï¼ç¹å«æ¯æ´å¤å¾æåè½¦ä½ï¼ï¼åºæ¯å¯åº¦æ´é«ï¼æ¨å¨æ´å¥½å°åæ çå®ä¸ççå¤ææ§ã
*   <strong>SS-PSDåçç£åºçº¿æ¨¡åï¼</strong> é¦æ¬¡æåºäºç¨äºåè½¦ä½æ£æµçåçç£æ¹æ³SS-PSDï¼Semi-Supervised Parking Slot Detectionï¼ãè¯¥æ¨¡ååºäºæå¸-å­¦çï¼Teacher-Studentï¼æ¡æ¶ï¼å¹¶å¼å¥äºä¸¤é¡¹å³é®åæ°ï¼
    *   <strong>ç½®ä¿¡åº¦å¼å¯¼æ©ç ä¸è´æ§ï¼Confidence-Guided Mask Consistency, CGMï¼ï¼</strong> éå¯¹æªæ æ³¨æ°æ®ï¼éè¿å¯è®­ç»çç½®ä¿¡åº¦å¾ä¸ºä¸ååºååéæéï¼å¹¶æ©çä½ç½®ä¿¡åº¦åºåï¼ä»¥ç¡®ä¿é¢æµä¸è´æ§ï¼é¿åæ½å¨éè¯¯é¢æµçè¯¯å¯¼ã
    *   <strong>èªéåºç¹å¾æ°å¨ï¼Adaptive Feature Perturbation, Adaptive-VATï¼ï¼</strong> å¼å¥äºä¸ç§èªéåºéæ©æ§çç¹å¾æ°å¨æºå¶ï¼æ ¹æ®æå¸æå­¦çæ¨¡åå¯¹æ°å¨çé²æ£æ§ï¼çææ´å¼ºä½åççå¯¹ææ§åªå£°ï¼ä»¥ä¿è¿å­¦çæ¨¡åçææè®­ç»ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ°æ®éä¼å¿ï¼</strong> CRPS-Dæ°æ®éå¨å¾åååè½¦ä½æ°éä¸è¿è¶ç°ææ°æ®éï¼ä¸åè½¦ä½å¯åº¦æ´é«ï¼å¾æåè½¦ä½æ¯ä¾æ¾èå¢å ï¼11.75%ï¼ï¼ä¸ºçå®ä¸çåè½¦ä½æ£æµæä¾äºæ´å·æææ§åä»£è¡¨æ§çåºåã
*   <strong>SS-PSDæ§è½ä¼è¶æ§ï¼</strong> å®éªç»æè¡¨æï¼SS-PSDå¨CRPS-Dåç°ææ°æ®éä¸åè¶è¶äºç°ææåè¿ï¼SoTAï¼çå¨çç£è§£å³æ¹æ¡ãå°¤å¶æ¯å¨æ æ³¨æ°æ®è¾å°çæåµä¸ï¼ä¾å¦1/24çæ æ³¨æ¯ä¾ï¼ï¼SS-PSDç¸è¾äºDMPR-PSåGCNå¨APparking-slotä¸åå«åå¾äº19.02%å16.49%çæ¾èæåã
*   <strong>åçç£å­¦ä¹ çæææ§ï¼</strong> è®ºæå¼ºè°ï¼SS-PSDè½å¤ææå©ç¨æªæ æ³¨æ°æ®ï¼éçæªæ æ³¨æ°æ®éçå¢å ï¼æ§è½æåè¶æ¾èï¼è¯æäºå¶å¨æ ç­¾ç¨ç¼ºåºæ¯ä¸çå®ç¨æ§åå¯æ©å±æ§ã
*   <strong>ç»ä»¶æææ§ï¼</strong> æ¶èå®éªè¯å®äºCGMä¸è´æ§åAdaptive-VATå¯¹æ¨¡åæ§è½çç§¯æè´¡ç®ï¼CGMä¸è´æ§ä½¿APparking-slotæåäº3.60%ï¼Adaptive-VATè¿ä¸æ­¥æåäº0.82%ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡SS-PSDæ¨¡åå¨å¤§å¤æ°åºæ¯ä¸è¡¨ç°åºè²ï¼ä½å¨æç«¯ç¯å¢æ¡ä»¶ä¸ä»å­å¨å±éæ§ãè®ºæä¸­åä¸¾äºä»¥ä¸å¤±è´¥æ¡ä¾ï¼
*   <strong>å¤æåç§æ¡ä»¶ï¼</strong> å°ä¸è½¦åºä¸­åç§ä¸è¶³ãé´å½±åå°é¢åå°ä¼å¯¼è´è¯¯æ£ã
*   <strong>æ·å¤ç©åï¼</strong> å¼ºççæ·å¤ç©åä¼å½±ååè½¦ä½æ£æµã
*   <strong>æ¶å£å¤©æ°ï¼å¤§é¨ï¼ï¼</strong> éä½å¯¹æ¯åº¦ãè¡¨é¢åå°åé¨åé®æ¡çåè½¦ä½æ è®°ä¼éä½æ£æµåç¡®æ§ã
*   <strong>æ è®°ç£¨æï¼</strong> è¤ªè²æä¸¥éæåçåè½¦ä½æ è®°é¾ä»¥è¾¨å«ï¼å¯¼è´æ¼æ£ã
*   <strong>ä¸¥éé®æ¡ï¼</strong> è½¦è¾æå¶ä»ç©ä½é®æ¡åè½¦ä½è§å¾æ¶ï¼æ¨¡åæ§è½ä¸éã
*   <strong>å¤é´åºæ¯ï¼</strong> æéçåç§åä¼ æå¨åªå£°æ¾èéä½è§è§æ¸æ°åº¦ï¼å¯¼è´æ¼æ£ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±åè½ï¼</strong> å°æ¨¡åæ©å±å°åå«å ç¨æ£æµï¼ä»¥å¤æ­åè½¦ä½æ¯å¦è¢«å ç¨æç©ºé²ï¼ä»èæå»ºæ´å¨é¢å®ç¨çèªå¨æ³è½¦ç³»ç»ã
*   <strong>å¤æºæ°æ®èåï¼</strong> ç»åå¶ä»æ°æ®æºï¼å¦è§é¢åºåä¸­çæ¶é´çº¿ç´¢æä¼ æå¨èåï¼ä»¥æé«æ¨¡åå¨æ¶å£æ¡ä»¶ä¸çé²æ£æ§ã
*   <strong>V2Xææ¯éæï¼</strong> æ¢ç´¢å°V2Xææ¯ï¼å¦RSUè¾å©çååå®ä½ï¼éæå°ç³»ç»ä¸­ï¼ä»¥å¢å¼ºå¨GNSSåéç¯å¢ï¼å¦å°ä¸åè½¦åºååå¸å³¡è°·ï¼ä¸­çç³»ç»é²æ£æ§ï¼å¯è½åæ¶æ¹åå®ä½åæ£æµæ§è½ã
*   <strong>æ¨¡åä¼åï¼</strong> è¿ä¸æ­¥ä¼åæ¨¡åä»¥å¤çæ´å¹¿æ³çç¯å¢æ¡ä»¶ï¼å¹¶æ©å±å¶åè½ä»¥æ¯æå¶ä»äº¤éç¸å³åºç¨ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥å¤§è§æ¨¡ãé«æææ§çCRPS-Dæ°æ®éååæ°çSS-PSDåçç£å­¦ä¹ æ¡æ¶ï¼ä¸ºçå®ä¸çåè½¦ä½æ£æµé¢åååºäºéè¦è´¡ç®ãå¶åçç£æ¹æ³å¨æ ç­¾ç¨ç¼ºåºæ¯ä¸è¡¨ç°åºæ¾èä¼å¿ï¼ä¸ºæªæ¥èªå¨æ³è½¦ç³»ç»çåå±æä¾äºæåçææ¯æ¯æåç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Additionally, we develop a semi-supervised baseline for parking slot
detection, termed SS-PSD, to further improve performance by exploiting
unlabeled data.</li>
<li>Experimental results demonstrate the superiority of SS-PSD over the existing
state-of-the-art (SoTA) solutions on both the proposed dataset and the existing
dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13133v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13133v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.13083v1'></a></p>
<h2 id="using-kl-divergence-to-focus-frequency-information-in-low-light-image-enhancement"><a href="https://arxiv.org/abs/2509.13083v1">Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement</a></h2>
<p><strong>Authors:</strong> Yan Xingyang, Huang Xiaohong, Zhang Zhao, You Tian, Xu Ziheng</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In the Fourier domain, luminance information is primarily encoded in the
amplitude spectrum, while spatial structures are captured in the phase
components. The traditional Fourier Frequency information fitting employs
pixel-wise loss functions, which tend to focus excessively on local information
and may lead to global information loss. In this paper, we present LLFDisc, a
U-shaped deep enhancement network that integrates cross-attention and gating
mechanisms tailored for frequency-aware enhancement. We propose a novel
distribution-aware loss that directly fits the Fourier-domain information and
minimizes their divergence using a closed-form KL-Divergence objective. This
enables the model to align Fourier-domain information more robustly than with
conventional MSE-based losses. Furthermore, we enhance the perceptual loss
based on VGG by embedding KL-Divergence on extracted deep features, enabling
better structural fidelity. Extensive experiments across multiple benchmarks
demonstrate that LLFDisc achieves state-of-the-art performance in both
qualitative and quantitative evaluations. Our code will be released at:
https://github.com/YanXY000/LLFDisc</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Yan Xingyangç­äººæ°åçè®ºæâUsing KL-Divergence to Focus Frequency Information in Low-Light Image Enhancementâçå¨é¢æè¦ã</p>
<hr />
<h3 id="using-kl-divergence-to-focus-frequency-information-in-low-light-image-enhancement_1">è®ºæãUsing KL-Divergence to Focus Frequency Information in Low-Light Image Enhancementãå¨é¢æè¦</h3>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ä½åç§å¾åå¢å¼ºï¼LLIEï¼æ¯ä¸ä¸ªæ ¸å¿çè®¡ç®æºè§è§ä»»å¡ï¼ä½ç°ææ¹æ³å¨å¤çä½åç§å¾åæ¶å­å¨å±éæ§ãå·ä½æ¥è¯´ï¼ä¼ ç»çåºäºåéå¶é¢çä¿¡æ¯çå¢å¼ºæ¹æ³éå¸¸éç¨åç´ çº§æå¤±å½æ°ï¼å¦MSEï¼ï¼è¿å¯¼è´æ¨¡åè¿åº¦å³æ³¨å±é¨ä¿¡æ¯ï¼å¯è½å¿½ç¥å¨å±é¢çåå¸ï¼ä»èå½±åå¢å¼ºå¾åçæ´ä½è´¨éåç»æä¿çåº¦ãæ­¤å¤ï¼ç°æçåéIEå¢å¼ºæ¹æ³æªè½ååå©ç¨æ¯å¹ä¿¡æ¯ï¼å¹¶ä¸å¨ç©ºé´åç¹å¾æåæ¶å®¹ææ¾å¤§åªå£°ï¼èåºäºRetinexçæ¹æ³å¯è½å¯¼è´çº¹çå¤±çåæ³åè½åå·®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäºä¸ä¸ªåä¸ºLLFDiscçUå½¢æ·±åº¦å¢å¼ºç½ç»ï¼å¹¶å¼å¥äºå¤é¡¹å³é®åæ°ï¼</p>
<ul>
<li><strong>åéå¶KLæ£åº¦æå¤±ï¼Fourier KL-Divergence Lossï¼ï¼</strong> éå¯¹ä¼ ç»åç´ çº§æå¤±çå±éæ§ï¼ä½èæåºäºä¸ç§æ°é¢çãåå¸æç¥çåéå¶åæå¤±å½æ°ãè¯¥æå¤±å½æ°å°é¢æµå¾ååçå®å¾åçæ¯å¹åç¸ä½è°±å»ºæ¨¡ä¸ºé«æ¯åå¸ï¼å¹¶éè¿è®¡ç®å®ä»¬ä¹é´çKLæ£åº¦æ¥æå°åå·®å¼ãè¿ç§æ¹æ³ä½¿å¾æ¨¡åè½å¤æ´é²æ£å°å¯¹é½åéå¶åä¿¡æ¯ï¼æè·è·¨é¢ççèååå¸ç»æï¼èéç¬ç«å¤çæ¯ä¸ªé¢çåéï¼ä»èé¿åäºå±é¨åå·®åå¨å±ä¿¡æ¯æå¤±ã</li>
<li><strong>å¢å¼ºåVGGæç¥æå¤±ï¼Enhanced VGG Perceptual Loss with KL-Divergenceï¼ï¼</strong> ä¸ºäºè¿ä¸æ­¥æåç»æä¿çåº¦ï¼ä½èå°KLæ£åº¦åµå¥å°VGGæç¥æå¤±ä¸­ãéè¿å¨VGGç½ç»æåçæ·±å±ç¹å¾ä¸åºç¨KLæ£åº¦ï¼æ¨¡åè½å¤è¡¡éé«å±ç¹å¾è¡¨ç¤ºä¹é´çåå¸ç¸ä¼¼æ§ï¼ä»èæ´å¥½å°ææå¾åçæç¥è´¨éåç»æç»èã</li>
<li><strong>LLFDiscç½ç»æ¶æï¼</strong> æåºäºä¸ç§æµçº¿åçUå½¢ç¼ç å¨-è§£ç å¨ç½ç»LLFDiscï¼å®éæäºäº¤åæ³¨æåï¼Cross-Attentionï¼åé¨æ§æºå¶ï¼è¿äºæºå¶ä¸ä¸ºé¢çæç¥å¢å¼ºèè®¾è®¡ãç½ç»åå«ä¸ä¸ªç¹å¾æåé¶æ®µï¼æ¯ä¸ªé¶æ®µé½éæäºå¢å¼ºåè½»éçº§äº¤åæ³¨æåï¼EnhancedLCAï¼æ¨¡åã</li>
<li><strong>EnhancedLCAæ¨¡åï¼</strong> è¯¥æ¨¡åæ¯LCAçæ¹è¿ï¼åå«DANCEï¼æåºååªå£°æ ¡æ­£å¢å¼ºï¼ãIELï¼ä¿¡æ¯å¢å¼ºå±ï¼ãSEï¼Squeeze-and-Excitationï¼åCABï¼äº¤åæ³¨æååï¼æ¨¡åã<ul>
<li><strong>DANCEæ¨¡åï¼</strong> è¿æ¯ä¸ä¸ªæ°æåºçæ¨¡åï¼ç¨äºæåºååªå£°æ ¡æ­£å¢å¼ºï¼éè¿åªå£°æç¥æ¨¡åæå¶åªå£°ï¼æåºå¢å¼ºæ¨¡åæ¢å¤ä½åç§åºåçç»èï¼ä»¥åééæ³¨æåæ¨¡åéæ°å æç¹å¾ã</li>
<li><strong>IELæ¨¡åï¼</strong> éè¿é¨æ§æºå¶å¢å¼ºç¹å¾éæ©æ§ï¼å¨æè°èç¹å¾æµï¼æè·éè¦ä¿¡æ¯å¹¶æå¶æ å³æåªå£°ç¹å¾ã</li>
<li><strong>CABæ¨¡åï¼</strong> éè¿QueryåKey-Valueå¯¹æºå¶éæ©æ§å°èåç¹å¾ï¼å¤çå¤§å°ºåº¦åç§ä¸ååæ§ï¼å¹¶å¨æè°æ´æ³¨æåæéã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> å¨å¤ä¸ªåºåæ°æ®éï¼åæ¬LOLv1ãLOLv2-RealãLOLv2-SyntheticåLSRW-Huaweiï¼ä¸è¿è¡äºå¹¿æ³çå®éªãLLFDiscå¨å®æ§åå®éè¯ä¼°ï¼PSNRãSSIMãLPIPSãNIQEï¼æ¹é¢ååå¾äºæåè¿çæ§è½ã
*   <strong>åéå¶åæåçæææ§ï¼</strong> å®éªè¯æï¼åºäºKLæ£åº¦çæå¤±å½æ°å¨åéå¶åä¿¡æ¯æåæ¹é¢æ¾èä¼äºåºäºMSEçæå¤±ï¼å¨å®éææ åæç¥è´¨éä¸åè¡¨ç°åºè²ã
*   <strong>ç»æä¿çåº¦æåï¼</strong> ç»åKLæ£åº¦çVGGæç¥æå¤±æ¾èæåäºæ¨¡åçè¡¨ç¤ºè½ååç»æä¿çåº¦ã
*   <strong>æçåé²æ£æ§ï¼</strong> LLFDiscç½ç»è®¾è®¡æµçº¿åï¼åæ°éåè®¡ç®å¤æåº¦è¾ä½ï¼ä¾å¦ï¼å¨LOLv1ä¸FLOPSä¸º10.93Gï¼åæ°éä¸º0.923Mï¼ï¼åæ¶å¨ä¸ååç§æ¡ä»¶ä¸è¡¨ç°åºè¯å¥½çæ³åè½ååé²æ£æ§ã
*   <strong>CAMå¯è§åï¼</strong> CAMå¯è§åç»ææ¾ç¤ºï¼ä½¿ç¨å®æ´æå¤±å½æ°è®­ç»çæ¨¡åï¼Full-CAMï¼çæçæ³¨æåç­å¾ä¸çå®å¾åï¼GT-CAMï¼é«åº¦ç¸ä¼¼ï¼è¡¨ææ¨¡åææææäºä¸çå®æ¿æ´»å¯¹é½çæå´è¶£åºåã
*   <strong>æ¶èç ç©¶ï¼</strong> æ¶èå®éªéªè¯äºææåºæ¨¡åï¼CABãIELãSEãDANCEï¼åæå¤±å½æ°ï¼åéå¶KLæå¤±ãVGG KLæå¤±ï¼çæææ§ï¼æ¯ä¸ªç»ä»¶é½å¯¹æåå¾åå¢å¼ºæ§è½ææ¾èè´¡ç®ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåå½åæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»å¶åæ°ç¹åæ¹è¿æ¹åæ¥çï¼å¯ä»¥æ¨æ­åºï¼
*   è½ç¶KLæ£åº¦æå¤±å¨åéå¶ååæç¥åè¡¨ç°åºè²ï¼ä½å¶è®¡ç®å¤ææ§å¯è½é«äºç®åçåç´ çº§æå¤±ï¼å°½ç®¡è®ºæå¼ºè°äºå¶é­å¼è§£çä¼å¿ã
*   å°æ¯å¹åç¸ä½è°±å»ºæ¨¡ä¸ºé«æ¯åå¸æ¯ä¸ç§ç®åï¼å¯è½æ æ³å®å¨ææææå¤æçé¢çåå¸ç¹å¾ã
*   æ¨¡åå¨å¤çæç«¯å¤ææç¹å®ç±»åçä½åç§åºæ¯ï¼å¦æåº¦é»æãå¼ºåªå£°å¹²æ°ç­ï¼æ¶ï¼å¯è½ä»æè¿ä¸æ­¥ä¼åçç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¤æçé¢çåå¸å»ºæ¨¡ï¼</strong> æ¢ç´¢é¤äºé«æ¯åå¸ä¹å¤ï¼æ´å¤æçæ¦çåå¸æ¨¡åæ¥æååéå¶åçæ¯å¹åç¸ä½ä¿¡æ¯ï¼ä»¥ææææ´ç²¾ç»çé¢çç¹å¾ã
*   <strong>èªéåºæå¤±æéï¼</strong> è¿ä¸æ­¥ç ç©¶èªéåºå°è°æ´å¤åæå¤±å½æ°ä¸­åé¡¹æå¤±æéçç­ç¥ï¼ä½¿å¶è½å¤æ ¹æ®è¾å¥å¾åçç¹æ§æè®­ç»é¶æ®µå¨æä¼åã
*   <strong>è·¨æ¨¡æèåï¼</strong> ç»åå¶ä»æ¨¡æä¿¡æ¯ï¼å¦æ·±åº¦ãç­æåç­ï¼æ¥è¾å©ä½åç§å¾åå¢å¼ºï¼å°¤å¶æ¯å¨æç«¯ä½åç§æå¤æåºæ¯ä¸ã
*   <strong>å®æ¶åºç¨ä¼åï¼</strong> è¿ä¸æ­¥ä¼åç½ç»æ¶æåæå¤±å½æ°ï¼ä»¥æ»¡è¶³å®æ¶ä½åç§å¾åå¢å¼ºåºç¨çéæ±ï¼ä¾å¦å¨èªå¨é©¾é©¶æçæ§ç³»ç»ä¸­ã
*   <strong>æ çç£/èªçç£å­¦ä¹ ï¼</strong> æ¢ç´¢å¨æ²¡æéå¯¹ä½åç§-æ­£å¸¸åç§å¾åçæåµä¸ï¼å©ç¨KLæ£åº¦è¿è¡æ çç£æèªçç£çä½åç§å¾åå¢å¼ºæ¹æ³ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present LLFDisc, a
U-shaped deep enhancement network that integrates cross-attention and gating
mechanisms tailored for frequency-aware enhancement.</li>
<li>We propose a novel
distribution-aware loss that directly fits the Fourier-domain information and
minimizes their divergence using a closed-form KL-Divergence objective.</li>
<li>Extensive experiments across multiple benchmarks
demonstrate that LLFDisc achieves state-of-the-art performance in both
qualitative and quantitative evaluations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.13083v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.13083v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12990v1'></a></p>
<h2 id="dual-stage-reweighted-moe-for-long-tailed-egocentric-mistake-detection"><a href="https://arxiv.org/abs/2509.12990v1">Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</a></h2>
<p><strong>Authors:</strong> Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Sicong Li, Qingming Huang</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>In this report, we address the problem of determining whether a user performs
an action incorrectly from egocentric video data. To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework. In the first stage, features are
extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are
combined through a feature-level expert module. In the second stage, three
classifiers are trained with different objectives: reweighted cross-entropy to
mitigate class imbalance, AUC loss to improve ranking under skewed
distributions, and label-aware loss with sharpness-aware minimization to
enhance calibration and generalization. Their predictions are fused using a
classification-level expert module. The proposed method achieves strong
performance, particularly in identifying rare and ambiguous mistake instances.
The code is available at https://github.com/boyuh/DR-MoE.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Boyu Hanç­äººæ°åçè®ºæâDual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detectionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="dual-stage-reweighted-moe-for-long-tailed-egocentric-mistake-detection_1">è®ºææè¦ï¼Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</h3>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸ºâåé¶æ®µéå æä¸å®¶æ··åï¼Dual-Stage Reweighted Mixture-of-Experts, DR-MoEï¼âçæ¡æ¶ï¼æ¨å¨è§£å³ä»ç¬¬ä¸è§è§è§é¢æ°æ®ä¸­æ£æµç¨æ·æä½éè¯¯çé®é¢ãè¯¥ä»»å¡çææå¨äºéè¯¯éå¸¸æ¯ç»å¾®ãä¸é¢ç¹ä¸æ°æ®åå¸ä¸¥éä¸å¹³è¡¡çã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è®ºæçæ ¸å¿ç ç©¶é®é¢æ¯ï¼å¦ä½ææå°ä»ç¬¬ä¸è§è§è§é¢æ°æ®ä¸­è¯å«ç¨æ·æ¯å¦æ§è¡äºä¸æ­£ç¡®çæä½ï¼å³âéè¯¯æ£æµâï¼ï¼å°¤å¶æ¯å¨éè¯¯äºä»¶ç¨æãç»å¾®ä¸æ°æ®åå¸é«åº¦ä¸å¹³è¡¡çæåµä¸ãä¼ ç»çå¨ä½è¯å«æ¹æ³é¾ä»¥åºå¯¹è¿ç§éè¦æ´ç²¾ç»æ¶é´åæåå¯¹ç¨æ·è¡ä¸ºç»å¾®åå·®é«åº¦ææçä»»å¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
DR-MoEæ¡æ¶éè¿ä»¥ä¸ä¸¤ä¸ªé¶æ®µçåæ°æ§ç»åæ¥è§£å³ä¸è¿°ææï¼</p>
<ul>
<li>
<p><strong>ç¬¬ä¸é¶æ®µï¼ç¹å¾çº§ä¸å®¶æ··åï¼Feature Mixture-of-Experts, F-MoEï¼</strong></p>
<ul>
<li>è¯¥é¶æ®µå©ç¨ä¸¤ä¸ªåºäºViViTçæ¨¡åä½ä¸ºç¹å¾æåä¸å®¶ï¼ä¸ä¸ª<strong>å»ç»çViViTæ¨¡å</strong>ç¨äºææéç¨çæ¶ç©ºè¯­ä¹åéªï¼ç²ç²åº¦å¨ä½ç¹å¾ï¼ï¼å¦ä¸ä¸ªæ¯ç»è¿<strong>LoRAï¼ä½ç§©éåºï¼å¾®è°çViViTæ¨¡å</strong>ï¼ä¸æ³¨äºå¯¹éè¯¯ææçç»ç²åº¦çº¿ç´¢ã</li>
<li>è¿ä¸¤ä¸ªä¸å®¶çè¾åºéè¿ä¸ä¸ªå¯å­¦ä¹ çF-MoEæ¨¡åè¿è¡èåï¼è¯¥æ¨¡åæ ¹æ®è¾å¥ç¹æ§å¨æè°æ´æ¯ä¸ªä¸å®¶çè´¡ç®ï¼çæç»ä¸çèåç¹å¾è¡¨ç¤ºã</li>
</ul>
</li>
<li>
<p><strong>ç¬¬äºé¶æ®µï¼åç±»çº§ä¸å®¶æ··åï¼Classification Mixture-of-Experts, C-MoEï¼</strong></p>
<ul>
<li>èååçç¹å¾è¢«éå¥ä¸ä¸ªç¬ç«ä¼åçåç±»å¨ï¼æ¯ä¸ªåç±»å¨é½éå¯¹é¿å°¾è¯å«é®é¢éç¨ä¸åçä¼åç®æ ï¼<ul>
<li><strong>éå æäº¤åçµæå¤±ï¼Reweighted Cross-Entropy Lossï¼</strong>ï¼éè¿ä¸ºç¨æéè¯¯ç±»å«åéæ´é«çæéæ¥ç¼è§£ç±»å«ä¸å¹³è¡¡é®é¢ï¼æé«å¯¹æ¬ è¡¨ç¤ºéè¯¯å®ä¾çå¬åçã</li>
<li><strong>AUCæå¤±ï¼AUC Lossï¼</strong>ï¼ç´æ¥ä¼åROCæ²çº¿ä¸é¢ç§¯ï¼æ¨å¨æé«æ­£è´å®ä¾ä¹é´çæåè´¨éï¼è¿å¨éè¯¯ç±»å«æ¬ è¡¨ç¤ºæ¶è³å³éè¦ã</li>
<li><strong>ç»åéåº¦æç¥æå°åï¼SAMï¼çæ ç­¾æç¥æå¤±ï¼Label-Aware Loss, LA Lossï¼</strong>ï¼éè¿è°æ´logitså¹¶æç¡®ä¼åå¹³å¦æå°å¼ï¼ä¿è¿æ´æ ¡ååé²æ£çå³ç­è¾¹çï¼å¢å¼ºæ³åè½åã</li>
</ul>
</li>
<li>è¿äºåç±»å¨çé¢æµç»æéè¿ä¸ä¸ªC-MoEæ¨¡åè¿è¡èªéåºèåï¼è¯¥æ¨¡åæ ¹æ®è¾å¥å¨æå ææ¯ä¸ªä¸å®¶çè´¡ç®ï¼ä»èå¨å¤æ ·åçæ°æ®æ¡ä»¶ä¸å®ç°çµæ´»çå³ç­ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæå¨HoloAssist 2025æ¯èµçéè¯¯æ£æµä»»å¡ä¸è¿è¡äºå®éªï¼å¹¶åå¾äºæ¾èçæ§è½æåã
*   ä¸RandomåTimeSformerç­åºçº¿æ¨¡åç¸æ¯ï¼DR-MoEæ¹æ³å¨F-scoreä¸å®ç°äºæ¾èæ¹è¿ã
*   ç¹å«æ¯å¨è¯å«ç¨æåæ¨¡ç³çéè¯¯å®ä¾æ¹é¢ï¼è¯¥æ¹æ³è¡¨ç°åºå¼ºå¤§çæ§è½ã
*   å¼å¾æ³¨æçæ¯ï¼DR-MoEä»ä½¿ç¨RGBæ¨¡æè¾å¥ï¼å°±è¾¾å°äºçè³è¶è¶äºä¾èµå¤æ¨¡æè¾å¥çæ¨¡åï¼å¦UNICT Solutionï¼çç«äºåï¼å°¤å¶æ¯å¨éè¯¯å¬åçæ¹é¢ææ¾èæåï¼ä»UNICT Solutionç0.09æåå°DR-MoEç0.63ï¼ã
è¿äºç»æè¡¨æï¼DR-MoEæ¡æ¶éè¿ç»åäºè¡¥çå»ºæ¨¡ç­ç¥ï¼åæ¬ç¹å¾æåååç±»å±é¢ï¼ï¼è½å¤ææåºå¯¹é¿å°¾åç»å¾®éè¯¯æ£æµçææï¼æé«äºæ¨¡åçé²æ£æ§ååç¡®æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåå½åæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»å¶æ¹æ³è®ºçå¤ææ§æ¥çï¼æ½å¨çå±éæ§å¯è½åæ¬ï¼
*   <strong>è®¡ç®ææ¬ï¼</strong> ç»åå¤ä¸ªViViTæ¨¡åãLoRAå¾®è°ä»¥åå¤ä¸å®¶åç±»å¨å¯è½ä¼å¢å æ¨¡åçè®­ç»åæ¨çææ¬ã
*   <strong>è¶åæ°è°ä¼ï¼</strong> å¤ä¸ªæå¤±å½æ°åä¸å®¶æ··åæ¨¡åå¼å¥äºæ´å¤çè¶åæ°ï¼å¯è½éè¦ç²¾ç»çè°ä¼æè½è¾¾å°æä½³æ§è½ã
*   <strong>æ³åæ§ï¼</strong> å°½ç®¡è®ºæå¼ºè°äºæ³åè½åï¼ä½å¯¹äºHoloAssistæ°æ®éä¹å¤çæ´å¹¿æ³ãæ´å¤æ ·åçç¬¬ä¸è§è§éè¯¯æ£æµåºæ¯ï¼å¶æ³åè½åä»éè¿ä¸æ­¥éªè¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½åºäºå¶è´¡ç®åæ½å¨å±éæ§ï¼å¯ä»¥æ¨æ­åºä»¥ä¸æ¹åï¼
*   <strong>æ¨¡åæçä¼åï¼</strong> æ¢ç´¢æ´è½»éçº§çä¸å®¶æ¨¡åææ´é«æçä¸å®¶æ··åæºå¶ï¼ä»¥éä½è®¡ç®ææ¬ï¼ä½¿å¶æ´éç¨äºå®æ¶æèµæºåéçåºç¨ã
*   <strong>å¤æ¨¡æèåçè¿ä¸æ­¥æ¢ç´¢ï¼</strong> å°½ç®¡å½åæ¹æ³ä»ä½¿ç¨RGBæ¨¡æå°±è¡¨ç°åºè²ï¼ä½ç»åå¶ä»æ¨¡æï¼å¦æ·±åº¦ãæé¨å§¿æãç¼å¨è¿½è¸ªï¼çæ´é«çº§èåç­ç¥ï¼å¯è½ä¼è¿ä¸æ­¥æåæ§è½ï¼å°¤å¶æ¯å¨æ´å¤æææ´æ¨¡ç³çéè¯¯åºæ¯ä¸­ã
*   <strong>å¯è§£éæ§ï¼</strong> æ·±å¥ç ç©¶F-MoEåC-MoEæ¨¡åä¸­ä¸å®¶æéåéçæºå¶ï¼ä»¥æé«æ¨¡åå³ç­çå¯è§£éæ§ï¼å¸®å©çè§£æ¨¡åå¨ä¸åæåµä¸å¦ä½æè¡¡ä¸åä¿¡æ¯æºã
*   <strong>èªçç£æåçç£å­¦ä¹ ï¼</strong> é´äºéè¯¯äºä»¶çç¨ææ§ï¼æ¢ç´¢å©ç¨æªæ è®°æ°æ®è¿è¡èªçç£æåçç£å­¦ä¹ çæ¹æ³ï¼ä»¥è¿ä¸æ­¥ç¼è§£æ°æ®ä¸å¹³è¡¡é®é¢ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12990v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12990v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.12888v1'></a></p>
<h2 id="runge-kutta-approximation-and-decoupled-attention-for-rectified-flow-inversion-and-semantic-editing"><a href="https://arxiv.org/abs/2509.12888v1">Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</a></h2>
<p><strong>Authors:</strong> Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He</p>
<p><strong>Published:</strong> 2025-09-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Rectified flow (RF) models have recently demonstrated superior generative
performance compared to DDIM-based diffusion models. However, in real-world
applications, they suffer from two major challenges: (1) low inversion accuracy
that hinders the consistency with the source image, and (2) entangled
multimodal attention in diffusion transformers, which hinders precise attention
control. To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations. To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control. Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability. Code is
available at https://github.com/wmchen/RKSovler_DDTA.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai Heæ°åçè®ºæâRunge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editingâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Runge-Kuttaè¿ä¼¼ä¸è§£è¦æ³¨æåå¨æ´æµæµåæ¼åè¯­ä¹ç¼è¾ä¸­çåºç¨</strong></p>
<p>è¿ç¯è®ºæç±Weiming Chenç­äººæ°åï¼æ¨å¨è§£å³æ´æµæµï¼Rectified Flow, RFï¼æ¨¡åå¨å®éåºç¨ä¸­é¢ä¸´çä¸¤ä¸ªå³é®ææï¼ä½åæ¼ç²¾åº¦åå¤æ¨¡ææ³¨æåçº ç¼ ãRFæ¨¡åå¨çææ§è½ä¸å·²è¶è¶åºäºDDIMçæ©æ£æ¨¡åï¼ä½å¨å¾åéå»ºåè¯­ä¹ç¼è¾ä»»å¡ä¸­ä»å­å¨ä¸è¶³ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è®ºæä¸»è¦å³æ³¨RFæ¨¡åå¨å®éåºç¨ä¸­çä¸¤ä¸ªæ ¸å¿é®é¢ï¼
*   <strong>ä½åæ¼ç²¾åº¦ï¼</strong> RFæ¨¡åå¨å°ç»å®å¾ååæ¼åå¶å¯¹åºçåªå£°æ ·æ¬æ¶ï¼é¾ä»¥ä¿æä¸æºå¾åçä¸è´æ§ï¼è¿éå¶äºå¶å¨å¾åéå»ºåç¼è¾ä¸­çåºç¨ã
*   <strong>å¤æ¨¡ææ³¨æåçº ç¼ ï¼</strong> å¨å¤æ¨¡ææ©æ£Transformerï¼MM-DiTï¼ä¸­ï¼ææ¬åå¾åæ³¨æåç´§å¯è¦åï¼å¯¼è´é¾ä»¥è¿è¡ç²¾ç¡®çè¯­ä¹æ§å¶ï¼ä»èå½±åäºææ¬å¼å¯¼å¾åç¼è¾ççµæ´»æ§ååç¡®æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼è®ºææåºäºä¸¤é¡¹ä¸»è¦åæ°ï¼
*   <strong>åºäºRunge-Kuttaï¼RKï¼æ±è§£å¨çé«é¶åæ¼æ¹æ³ï¼</strong> éå¯¹ä½åæ¼ç²¾åº¦é®é¢ï¼ä½èå°æ°å¼åæä¸­çRunge-Kuttaæ¹æ³å¼å¥RFéæ ·è¿ç¨ï¼æåºäºä¸ç§é«æçé«é¶æ±è§£å¨ï¼ç¨äºRFæ¨¡åçå¾®åæ¹ç¨è¿ç¨ãè¿ç§æ¹æ³è½å¤æ´ç²¾ç¡®å°è¿ä¼¼å¾®åè½¨è¿¹ï¼ä»èæé«åæ¼ä¿çåº¦ã
*   <strong>è§£è¦æ©æ£Transformeræ³¨æåï¼Decoupled Diffusion Transformer Attention, DDTAï¼ï¼</strong> ä¸ºäºè§£å³å¤æ¨¡ææ³¨æåçº ç¼ é®é¢ï¼è®ºæå¼å¥äºDDTAæºå¶ãè¯¥æºå¶éè¿æ·±å¥MM-DiTçåé¨ç»æï¼å°ææ¬åå¾åæ³¨æåè§£è¦ï¼ä»èå®ç°æ´ç²¾ç¡®çè¯­ä¹æ§å¶ãè¿ä½¿å¾å¨ææ¬å¼å¯¼çå¾åç¼è¾ä¸­ï¼è½å¤æ´çµæ´»å°å©ç¨æºä¿¡æ¯ï¼å¹³è¡¡ä¿çåº¦åå¯ç¼è¾æ§ä¹é´çæè¡¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¨å¾åéå»ºåææ¬å¼å¯¼ç¼è¾ä»»å¡ä¸çå¹¿æ³å®éªï¼éªè¯äºæææ¹æ³çæææ§ï¼
*   <strong>å¾åéå»ºä»»å¡ï¼</strong> RKæ±è§£å¨æ¾èæé«äºåæ¼ä¿çåº¦ï¼å¨PSNRï¼å³°å¼ä¿¡åªæ¯ï¼ä¸å®ç°äºé«è¾¾2.39 dBçæ¾èå¢çï¼ä¼äºç°æRFåæ¼æ¹æ³ãè¿è¡¨æé«é¶åæ¼ææ¯è½ææåæRFæ½å¨ç©ºé´çç¨çæ§é®é¢ã
*   <strong>ææ¬å¼å¯¼ç¼è¾ä»»å¡ï¼</strong> ç»åRKæ±è§£å¨åDDTAçæ¹æ³å¨ä¿çåº¦åå¯ç¼è¾æ§æ¹é¢åè¾¾å°äºæåè¿çæ§è½ãDDTAæºå¶éè¿è§£è¦ææ¬åå¾åæ³¨æåï¼å®ç°äºå¯¹è¯­ä¹ç¼è¾çç²¾ç»æ§å¶ï¼ä»èå¨ä¿ææºå¾åä¸è´æ§çåæ¶ï¼æé«äºç¼è¾çåç¡®æ§ãç¨æ·ç ç©¶ä¹è¿ä¸æ­¥è¯å®äºè¯¥æ¹æ³å¨ç¼è¾è´¨éåå¿ å®åº¦æ¹é¢çä¼è¶æ§ã
*   <strong>æçï¼</strong> è®ºæçæ¹æ³å¨æ¾èåå°éæ ·æ­¥éª¤çæåµä¸ï¼å®ç°äºæä½³çæ´ä½æ§è½ï¼è¡¨æäºå¶åè¶çæçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹å¦è¯å°æåºäºå½åæ¹æ³çå±éæ§ï¼
*   <strong>è®¡ç®å¼éï¼</strong> å°½ç®¡RKæ±è§£å¨æ¾èæé«äºä¿çåº¦ï¼ä½å¶é«é¶å»ºæ¨¡å¼å¥äºé¢å¤çè®¡ç®å¼éã
*   <strong>åå­æ¶èï¼</strong> ä¿å­è§£è¦æ³¨æåå¾ä¼å¸¦æ¥æ¾èçåå­æ¶èã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºæªæ¥çç ç©¶æ¹åï¼
*   <strong>ä½è®¡ç®å¼éçé«é¶æ±è§£å¨ï¼</strong> å¼åè®¡ç®å¼éæ´ä½çé«é¶æ±è§£å¨ï¼ä»¥æé«RFæ¨¡åçå®ç¨æ§ã
*   <strong>é«æçæ³¨æåä¿ææºå¶ï¼</strong> è®¾è®¡æ´é«æçæ³¨æåä¿ææºå¶ï¼ä»¥è¿ä¸æ­¥ä¼ååå­ä½¿ç¨åè®¡ç®æçã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºRFæ¨¡åå¨å¾ååæ¼åè¯­ä¹ç¼è¾ä¸­çåºç¨æä¾äºéè¦çè¿å±ãéè¿å¼å¥Runge-Kuttaè¿ä¼¼åè§£è¦æ³¨æåæºå¶ï¼ä½èææå°è§£å³äºRFæ¨¡åå¨å®éåºç¨ä¸­é¢ä¸´çç²¾åº¦åæ§å¶æ§ææï¼ä¸ºæªæ¥åºäºæ©æ£æ¨¡åçå¾åçæåç¼è¾ææ¯å¼è¾äºæ°çéè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations.</li>
<li>To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control.</li>
<li>Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.12888v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.12888v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-17 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
