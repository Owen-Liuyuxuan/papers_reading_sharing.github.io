<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-20 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-19/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-21/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-20">Arxiv Computer Vision Papers - 2025-11-20</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#roma-v2-harder-better-faster-denser-feature-matching" class="nav-link">RoMa v2: Harder Better Faster Denser Feature Matching</a>
                </li>
                <li class="nav-item">
                    <a href="#in-n-on-scaling-egocentric-manipulation-with-in-the-wild-and-on-task-data" class="nav-link">In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data</a>
                </li>
                <li class="nav-item">
                    <a href="#think-visually-reason-textually-vision-language-synergy-in-arc" class="nav-link">Think Visually, Reason Textually: Vision-Language Synergy in ARC</a>
                </li>
                <li class="nav-item">
                    <a href="#first-frame-is-the-place-to-go-for-video-content-customization" class="nav-link">First Frame Is the Place to Go for Video Content Customization</a>
                </li>
                <li class="nav-item">
                    <a href="#modes-accelerating-mixture-of-experts-multimodal-large-language-models-via-dynamic-expert-skipping" class="nav-link">MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</a>
                </li>
                <li class="nav-item">
                    <a href="#visplay-self-evolving-vision-language-models-from-images" class="nav-link">VisPlay: Self-Evolving Vision-Language Models from Images</a>
                </li>
                <li class="nav-item">
                    <a href="#the-sa-fari-dataset-segment-anything-in-footage-of-animals-for-recognition-and-identification" class="nav-link">The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification</a>
                </li>
                <li class="nav-item">
                    <a href="#flashmesh-faster-and-better-autoregressive-mesh-synthesis-via-structured-speculation" class="nav-link">FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation</a>
                </li>
                <li class="nav-item">
                    <a href="#when-to-think-and-when-to-look-uncertainty-guided-lookback" class="nav-link">When to Think and When to Look: Uncertainty-Guided Lookback</a>
                </li>
                <li class="nav-item">
                    <a href="#srpo-self-referential-policy-optimization-for-vision-language-action-models" class="nav-link">SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-20">Arxiv Computer Vision Papers - 2025-11-20</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年11月19日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年11月19日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的进展：</p>
<ul>
<li><strong>多模态融合与理解：</strong> 视觉信息与文本、语言的深度融合是核心趋势，旨在实现更强大的视觉推理和内容生成能力。</li>
<li><strong>高效模型与加速技术：</strong> 针对大型模型（如 MoE）的效率提升，以及更快的推理和合成技术是研究热点。</li>
<li><strong>具身智能与机器人操作：</strong> 提升机器人在真实世界中的感知、理解和操作能力，尤其是在以自我为中心的视角下。</li>
<li><strong>数据驱动的自适应与定制：</strong> 利用数据驱动的方法，实现模型在特定任务或内容上的自适应和定制化。</li>
<li><strong>精细化视觉任务：</strong> 在特征匹配、网格合成等精细化视觉任务上追求更高的精度和效率。</li>
</ul>
<p><strong>特别值得关注的论文：</strong></p>
<ul>
<li><strong>"RoMa v2: Harder Better Faster Denser Feature Matching"</strong>：在特征匹配这一基础但关键的视觉任务上，RoMa v2 展现了在难度、性能、速度和密度上的显著提升，预示着更鲁棒的视觉匹配技术。</li>
<li><strong>"In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data"</strong>：该论文通过结合野外数据和任务导向数据，为扩展以自我为中心的操纵任务提供了新的思路，对具身智能和机器人领域具有重要意义。</li>
<li><strong>"Think Visually, Reason Textually: Vision-Language Synergy in ARC"</strong>：ARC 任务的视觉-语言协同方法，强调了“视觉思考，文本推理”的范式，是推动视觉语言模型向更深层次理解迈进的重要一步。</li>
<li><strong>"MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping"</strong>：MoDES 提出了动态专家跳过机制，显著加速了 MoE 多模态大语言模型，是解决大型模型效率瓶颈的创新方案。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>动态专家选择与稀疏激活：</strong> 在 MoE 模型中，动态地选择和激活专家，以提高计算效率和模型性能。</li>
<li><strong>自演化与自适应模型：</strong> 利用图像数据实现视觉语言模型的自我演化和适应，减少对人工标注的依赖。</li>
<li><strong>不确定性引导的注意力机制：</strong> 通过不确定性来指导模型何时进行“思考”和“观察”，实现更智能的决策过程。</li>
<li><strong>结构化推断与加速合成：</strong> 在网格合成等任务中，利用结构化推断来加速生成过程。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其在基础任务上的突破、对具身智能的推动、以及在多模态模型效率上的创新，以下论文建议优先阅读全文：</p>
<ol>
<li><strong>"RoMa v2: Harder Better Faster Denser Feature Matching"</strong>：对于任何关注视觉匹配、SLAM、3D重建等领域的研究人员都至关重要。</li>
<li><strong>"In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data"</strong>：对机器人学、具身智能以及需要理解和执行复杂操作的研究人员具有直接价值。</li>
<li><strong>"Think Visually, Reason Textually: Vision-Language Synergy in ARC"</strong>：对于深入理解视觉语言模型如何进行复杂推理的研究人员来说，是必读之作。</li>
<li><strong>"MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping"</strong>：对于关注大型多模态模型效率和部署的研究人员，该论文提供了关键的技术洞察。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速了解本期 Arxiv 论文的亮点，并指导您进一步深入研究。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.15706v1">RoMa v2: Harder Better Faster Denser Feature Matching</a></li>
<li><a href="#2511.15704v1">In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data</a></li>
<li><a href="#2511.15703v1">Think Visually, Reason Textually: Vision-Language Synergy in ARC</a></li>
<li><a href="#2511.15700v1">First Frame Is the Place to Go for Video Content Customization</a></li>
<li><a href="#2511.15690v1">MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</a></li>
<li><a href="#2511.15661v1">VisPlay: Self-Evolving Vision-Language Models from Images</a></li>
<li><a href="#2511.15622v1">The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification</a></li>
<li><a href="#2511.15618v1">FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation</a></li>
<li><a href="#2511.15613v1">When to Think and When to Look: Uncertainty-Guided Lookback</a></li>
<li><a href="#2511.15605v1">SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.15706v1'></a></p>
<h2 id="roma-v2-harder-better-faster-denser-feature-matching"><a href="https://arxiv.org/abs/2511.15706v1">RoMa v2: Harder Better Faster Denser Feature Matching</a></h2>
<p><strong>Authors:</strong> Johan Edstedt, David Nordström, Yushan Zhang, Georg Bökman, Jonathan Astermark, Viktor Larsson, Anders Heyden, Fredrik Kahl, Mårten Wadenbäck, Michael Felsberg</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RoMa v2: Harder Better Faster Denser Feature Matching”的全面中文摘要：</p>
<p><strong>论文题目：</strong> RoMa v2: Harder Better Faster Denser Feature Matching</p>
<p><strong>作者：</strong> Johan Edstedt, David Nordström, Yushan Zhang, Georg Bökman, Jonathan Astermark, Viktor Larsson, Anders Heyden, Fredrik Kahl, Mårten Wadenbäck, Michael Felsberg</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决当前密集特征匹配（Dense Feature Matching）技术在处理复杂真实世界场景时遇到的挑战。尽管密集匹配已成为估计三维场景中像素间对应关系的黄金标准，但现有方法在许多困难场景下仍会失效或表现不佳。此外，高精度模型通常计算成本高昂且速度慢，限制了其在大规模应用中的可行性。具体而言，研究人员希望开发一个在准确性、鲁棒性、速度和内存占用方面都得到显著提升的密集特征匹配模型。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>RoMa v2 在多个方面进行了系统性改进，以克服现有方法的局限性：</p>
<ul>
<li><strong>新颖的匹配架构和损失函数：</strong> 引入了一种新的匹配架构和损失函数，特别是结合了“warp”和“correlation-based”损失，使得粗匹配器能够学习到多视图上下文信息。</li>
<li><strong>解耦的两阶段匹配-精炼流水线：</strong> 采用了一种解耦的两阶段（匹配-精炼）训练范式，这使得实验迭代更加快速，并显著降低了精炼阶段的内存占用。</li>
<li><strong>高效的自定义 CUDA 核：</strong> 开发了一个自定义的 CUDA 核，用于加速局部相关性计算，从而大幅减少精炼阶段的内存消耗。</li>
<li><strong>利用 DINOv3 基础模型：</strong> 集成了最新的 DINOv3 基础模型作为特征提取器，提高了模型的鲁棒性和泛化能力，并保持了特征提取器的冻结以增强鲁棒性。</li>
<li><strong>多样化的训练数据分布：</strong> 构建了一个包含宽基线和窄基线数据集的混合训练集，旨在平衡模型在极端视角变化下的鲁棒性以及在各种复杂匹配任务中的亚像素精度。</li>
<li><strong>预测像素级误差协方差：</strong> 引入了预测像素级误差协方差（或精度矩阵）的能力，这可以用于下游任务中的几何精炼，为匹配结果提供不确定性估计。</li>
<li><strong>改进的精炼器：</strong> 引入了更轻量级的精炼器，并采用指数移动平均（EMA）来缓解训练过程中出现的亚像素偏差问题。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>通过上述改进，RoMa v2 在多项基准测试中取得了最先进（state-of-the-art）的性能。实验结果表明：</p>
<ul>
<li><strong>显著的精度提升：</strong> RoMa v2 在 MegaDepth-1500 和 ScanNet-1500 等基准上，在相对位姿估计任务上显著优于所有先前的方法，包括一些三维重建方法。</li>
<li><strong>更强的鲁棒性：</strong> 在 WxBS 等包含极端视角、光照和模态变化的挑战性基准上，RoMa v2 表现出更强的鲁棒性。</li>
<li><strong>更快的速度和更低的内存占用：</strong> RoMa v2 的运行速度比 RoMa 快 1.7 倍，同时内存占用更小，使其更适用于大规模应用。</li>
<li><strong>在特定任务上的优势：</strong> 在密集匹配任务上，RoMa v2 在多个数据集上均优于 RoMa 和 UFM，尤其在 AerialMegaDepth 数据集上，其 EPE（端点误差）降低了 84%。</li>
<li><strong>新的贡献：</strong> 论文还引入了一个新的 SatAst 基准，用于匹配宇航员拍摄的图像与卫星图像，并展示了 RoMa v2 在此任务上的潜力。</li>
</ul>
<p>这些结果表明 RoMa v2 是一个更强大、更快速、更精确的密集特征匹配模型，为计算机视觉中的下游任务（如三维重建、视觉定位等）提供了更可靠的基础。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模态变化下的鲁棒性：</strong> 论文提到，与 RoMa 相比，RoMa v2 在处理极端模态变化（如 WxBS 基准中的红外到 RGB 图像匹配）时，鲁棒性略有下降，尽管它仍然优于 UFM。</li>
<li><strong>AerialMegaDepth 中的天空区域偏差：</strong> 在 AerialMegaDepth 数据集上，RoMa v2 有时会在纹理贫乏的天空区域产生错误的置信度估计，这可能是由于数据集本身存在深度信息泄露到天空区域的问题。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>进一步提升模态变化下的鲁棒性：</strong> 探索如何进一步提高模型在跨模态匹配任务中的鲁棒性，以缩小与 RoMa 在此方面的差距。</li>
<li><strong>解决数据集偏差问题：</strong> 针对 AerialMegaDepth 数据集中的天空区域偏差问题，研究如何通过数据增强、模型架构调整或后处理来缓解。</li>
<li><strong>更广泛的应用探索：</strong> 将 RoMa v2 应用于更广泛的计算机视觉任务，例如更复杂的机器人导航、增强现实和三维重建场景。</li>
<li><strong>实时性能优化：</strong> 尽管 RoMa v2 已经比 RoMa 快，但进一步优化以实现更高级别的实时性能，尤其是在资源受限的设备上，仍有研究空间。</li>
<li><strong>不确定性估计的深入应用：</strong> 探索预测的误差协方差在各种下游任务中的更深入应用，例如用于主动学习、不确定性感知几何估计等。</li>
</ul>
<p>总而言之，RoMa v2 代表了密集特征匹配领域的一项重要进展，它通过多方面的创新，显著提升了模型的性能和效率，为解决更具挑战性的计算机视觉问题奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks.</li>
<li>In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15706v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15706v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15704v1'></a></p>
<h2 id="in-n-on-scaling-egocentric-manipulation-with-in-the-wild-and-on-task-data"><a href="https://arxiv.org/abs/2511.15704v1">In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data</a></h2>
<p><strong>Authors:</strong> Xiongyi Cai, Ri-Zhao Qiu, Geng Chen, Lai Wei, Isabella Liu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data”的全面中文摘要，重点突出其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data</p>
<p><strong>作者：</strong> Xiongyi Cai, Ri-Zhao Qiu, Geng Chen, Lai Wei, Isabella Liu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决利用<strong>第一人称视角（Egocentric）的人类视频数据来训练机器人操纵策略</strong>时面临的<strong>数据异质性（data heterogeneity）</strong>问题。现有方法通常仅将人类数据用于简单的预训练，未能充分挖掘其潜力。研究的核心问题是如何有效地整合和利用不同类型的人类数据，以提升机器人在真实世界中的操纵能力和泛化性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>数据分类与收集策略：</strong> 论文提出了一个<strong>可扩展的数据收集和使用方案</strong>，将人类数据分为两类：<strong>“野外数据”（in-the-wild data）</strong>和<strong>“任务数据”（on-task data）</strong>。
    *   <strong>“野外数据”</strong>：指日常活动、非特定任务的视频，易于收集且多样，适合用于<strong>引导基础模型的预训练</strong>。
    *   <strong>“任务数据”</strong>：指与目标操纵任务直接相关的、经过精心策划的人类演示，更贴近目标分布，适合用于<strong>任务特定的后训练（post-training）</strong>。
*   <strong>大规模数据集 PHSD：</strong> 论文<strong>创建了一个名为 PHSD（Physical Humans-Humanoids Dataset）的大规模数据集</strong>，其中包含超过1000小时的“野外”第一人称视角人类数据，以及超过20小时与目标操纵任务直接对齐的“任务”数据。
*   <strong>统一的人类中心状态-动作空间：</strong> 为了解决不同机器人形态的差异，论文定义了一个<strong>统一的人类中心状态-动作空间</strong>，并开发了一套软件套件（Retargeting Software Suite）来将人类和人形机器人数据转换为此统一空间，从而实现跨具身（cross-embodiment）的学习。
*   <strong>Egocentric 语言条件流匹配模型 Humano：</strong> 基于上述数据和空间定义，论文训练了一个<strong>大规模的第一人称视角语言条件流匹配（language-conditioned flow matching）策略模型，命名为 Humano</strong>。
*   <strong>领域自适应（Domain Adaptation）：</strong> 为了解决人类和人形机器人之间的具身差异，论文引入了<strong>领域自适应技术（通过梯度反转层 GRL）</strong>，鼓励模型学习具身不变的表征，从而最小化人类与人形机器人之间的差距，防止模型“作弊”式地区分数据来源。
*   <strong>两阶段训练框架：</strong> 论文采用了一个<strong>两阶段的训练流程</strong>：首先使用大规模的“野外”人类和机器人数据进行预训练，然后使用任务对齐的“任务”数据进行后训练。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>语言指令遵循能力：</strong> Humano 在<strong>遵循仅存在于人类数据中的、机器人训练数据中未出现的语言指令方面表现出显著的能力</strong>。这克服了现有视觉-语言-动作（VLA）模型在处理未见指令时的弱点。
*   <strong>少样本学习（Few-shot Learning）：</strong> 即使仅使用<strong>极少量（如1个演示）的机器人数据</strong>，Humano 也能实现有效的学习，这表明人类数据提供了强大的先验知识，能够极大地加速机器人学习新任务的过程。
*   <strong>鲁棒性提升：</strong> 使用“任务数据”进行后训练，显著<strong>提高了模型在复杂、长时序任务中的鲁棒性</strong>，例如在汉堡组装任务中，即使面对未见过的食材或不同的背景，模型也能保持较高的成功率。
*   <strong>跨具身迁移能力：</strong> 通过统一的状态-动作空间和领域自适应，Humano 能够有效地将从人类数据中学到的知识迁移到人形机器人上，<strong>缩小了人类与人形机器人之间的性能差距</strong>。
*   <strong>数据驱动的机器人操纵新范式：</strong> 该研究展示了一种<strong>利用大规模、多样化的人类第一人称视角数据来训练通用机器人操纵策略</strong>的新范式，为解决机器人泛化性问题提供了新的思路。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>零样本行为学习的边界：</strong> 尽管模型在语言遵循和少样本学习方面表现出色，但论文指出，在当前训练规模下，<strong>机器人尚不能完全从人类数据中学到全新的行为（零样本行为学习）</strong>。
*   <strong>感知和长时序控制的挑战：</strong> 在某些场景下，尤其是在光照变化或复杂背景下，模型偶尔会出现<strong>物体定位错误或颜色混淆</strong>，导致抓取失败或碰撞。在需要工具使用或精确操纵的任务中，早期微小的抓取不准确可能会累积并导致最终任务失败。
*   <strong>具身细节的细微影响：</strong> 尽管领域自适应技术有效，但<strong>细微的具身差异（如关节限制或接触不稳定的抓取）仍然可能影响模型的表现</strong>，表明模型表征尚未完全对所有具身细节保持不变。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步扩大数据规模：</strong> 继续增加人类数据的规模和多样性，以期实现更强的零样本行为学习能力。
*   <strong>测试更多机器人平台：</strong> 将模型部署和测试到更多不同类型的人形机器人或其他机器人平台上，以验证其跨具身泛化能力。
*   <strong>更精细的领域自适应：</strong> 探索更先进的领域自适应技术，以更全面地解决人类与机器人之间的具身差异。
*   <strong>更复杂的任务和场景：</strong> 探索在更具挑战性的、需要更精细操纵、更长时序规划或更复杂交互的任务中应用该方法。
*   <strong>结合其他模态数据：</strong> 探索将其他类型的数据（如触觉、力反馈等）与第一人称视角数据结合，以进一步提升模型的感知和控制能力。</p>
<p><strong>总结：</strong></p>
<p>“In-N-On”论文提出了一种创新的、可扩展的策略，通过对人类第一人称视角数据进行“野外”预训练和“任务”后训练，并结合统一的状态-动作空间和领域自适应技术，成功训练了一个名为 Humano 的大规模语言条件操纵模型。该模型在遵循人类特有的语言指令、实现高效的少样本学习以及提升在复杂任务中的鲁棒性方面取得了显著成果，为机器人操纵领域带来了新的突破，并为未来利用大规模人类数据训练通用机器人智能指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15704v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15704v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15703v1'></a></p>
<h2 id="think-visually-reason-textually-vision-language-synergy-in-arc"><a href="https://arxiv.org/abs/2511.15703v1">Think Visually, Reason Textually: Vision-Language Synergy in ARC</a></h2>
<p><strong>Authors:</strong> Beichen Zhang, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Think Visually, Reason Textually: Vision-Language Synergy in ARC”的全面中文摘要，重点关注其在计算机视觉领域的贡献：</p>
<p><strong>论文题目：</strong> Think Visually, Reason Textually: Vision-Language Synergy in ARC</p>
<p><strong>作者：</strong> Beichen Zhang, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文聚焦于当前大型基础模型（如GPT-5和Grok 4）在抽象推理能力上的局限性，特别是它们难以从少量示例中推断出结构化规则，而这正是人类智能的关键特征。ARC-AGI（Abstraction and Reasoning Corpus for Artificial General Intelligence）作为评估此类能力的基准，要求模型进行概念规则的归纳和迁移。现有方法大多将ARC-AGI视为纯文本任务，忽视了人类在解决此类谜题时对视觉抽象的依赖。论文的核心问题在于，如何有效地结合视觉和语言的优势，以提升模型在抽象推理任务上的表现。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
作者提出了一个核心假设：视觉和语言在推理的不同阶段具有互补的优势。视觉擅长全局模式的抽象和验证，而语言则擅长符号规则的制定和精确执行。基于此洞察，论文引入了两个协同策略：</p>
<ul>
<li><strong>视觉-语言协同推理 (Vision-Language Synergy Reasoning, VLSR)：</strong> 该方法将ARC-AGI任务分解为两个模态对齐的子任务。在<strong>规则归纳（Rule Summarization）</strong>阶段，将ARC-AGI的示例输入-输出矩阵对可视化为彩色编码的2D网格，利用模型的全局视觉感知能力来提取转换模式。在<strong>规则应用（Rule Application）</strong>阶段，则切换回文本表示，以便模型能够精确地执行元素级操作。这种模态匹配的分解策略利用了各模态的固有优势。</li>
<li><strong>模态切换自校正 (Modality-Switch Self-Correction, MSSC)：</strong> 针对模型在同一模态内进行内在自我纠错的挑战，MSSC采用不同模态进行前向推理和后向验证。在生成候选输出后，MSSC将测试输入和预测输出可视化为图像，利用视觉的模式一致性验证能力来检查预测转换是否与示例图像中的模式匹配。若检测到不一致，模型会收到明确的反馈并进行另一轮文本推理。这种跨模态验证实现了有效的内在自我纠错，无需外部真实标签。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>显著性能提升：</strong> 实验表明，VLSR和MSSC策略能够显著提升模型在ARC-AGI任务上的性能。在旗舰模型（如Gemini-2.5-Pro、GPT-40、o4-mini）上，作者的方法平均比纯文本基线模型提高了 <strong>4.33%</strong> 的准确率，最高可达 <strong>7.25%</strong> 的提升。
*   <strong>模态互补性验证：</strong> 论文通过详细的定量和定性分析，证实了视觉在规则归纳阶段（提供全局感知和2D结构理解）的优势，以及文本在规则应用阶段（提供精确的元素级操作）的优势。反之，在规则应用阶段直接使用图像表示反而会因精度问题导致性能下降。
*   <strong>自校正的有效性：</strong> MSSC策略在迭代改进方面表现出色，能够持续提升性能，而纯文本自校正则效果有限甚至可能导致性能下降。这表明跨模态验证是实现有效内在自我纠错的关键。
*   <strong>对未来研究的启示：</strong> 研究结果有力地证明，将视觉抽象与语言推理相结合是实现更通用、更类人智能的基础模型迈出的关键一步。这为未来研究如何更深层次地融合多模态信息以解决复杂推理问题提供了重要方向。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>视觉表示的精度问题：</strong> 论文明确指出，在规则应用阶段，直接将ARC-AGI网格渲染为图像会导致模型在定位和识别特定单元格时出现精度问题，从而降低性能。这强调了在不同推理阶段选择合适模态的重要性。
*   <strong>纯文本自校正的不足：</strong> 论文通过实验证明，仅使用文本模态进行自校正，模型容易陷入“确认偏见”，难以发现自身错误，有时甚至会降低性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更精细的模态协同策略：</strong> 论文提出的VLSR和MSSC是初步的协同策略，未来可以探索更复杂的模态交互和融合方式，以进一步挖掘视觉和语言的协同潜力。
*   <strong>扩展到其他抽象推理任务：</strong> 该方法在ARC-AGI上的成功表明其潜力，未来可以将其应用于其他需要抽象推理能力的任务，如数学推理、逻辑谜题等。
*   <strong>模型微调的融合：</strong> 论文在附录中展示了将VLSR策略应用于模型微调（fine-tuning）场景，取得了显著效果。未来可以进一步研究如何在训练阶段更有效地整合视觉和语言信息，以训练出更强大的基础模型。
*   <strong>理解人类视觉推理机制：</strong> 论文的分析部分深入探讨了视觉和文本推理的差异，未来可以基于这些洞察，设计更符合人类认知过程的模型。</p>
<p><strong>对计算机视觉领域的贡献：</strong>
这篇论文对计算机视觉领域的重要贡献在于，它<strong>首次系统性地揭示了视觉信息在抽象推理任务（如ARC-AGI）中的独特价值，并提出了一种创新的方法来将其有效地整合到大型语言模型（LLM）的推理过程中。</strong> 论文不仅证明了视觉在<strong>全局模式识别、2D结构理解和空间关系捕捉</strong>方面的优势，这些是纯文本表示难以充分捕捉的，而且通过<strong>VLSR和MSSC</strong>等具体方法，展示了如何利用这些优势来<strong>提升规则归纳的准确性并实现有效的自我纠错</strong>。这为计算机视觉模型在更高级的认知任务中扮演更重要的角色开辟了新的途径，超越了传统的图像识别和理解范畴，迈向了更具通用智能的AI系统。论文强调了“视觉思考”在抽象推理中的不可或缺性，为多模态AI的发展提供了重要的理论和实践指导。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks.</li>
<li>Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction.</li>
<li>Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15703v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15703v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15700v1'></a></p>
<h2 id="first-frame-is-the-place-to-go-for-video-content-customization"><a href="https://arxiv.org/abs/2511.15700v1">First Frame Is the Place to Go for Video Content Customization</a></h2>
<p><strong>Authors:</strong> Jingxi Chen, Zongxia Li, Zhichao Liu, Guangyao Shi, Xiyang Wu, Fuxiao Liu, Cornelia Fermuller, Brandon Y. Feng, Yiannis Aloimonos</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> First Frame Is the Place to Go for Video Content Customization
<strong>Authors:</strong> Jingxi Chen, Zongxia Li, Zhichao Liu, Guangyao Shi, Xiyang Wu, Fuxiao Liu, Cornelia Fermuller, Brandon Y. Feng, Yiannis Aloimonos
<strong>Categories:</strong> cs.CV
<strong>Published Date:</strong> 2025-11-19</p>
<hr />
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）：</strong></p>
<p>本研究颠覆了传统上将视频生成模型的第一帧视为简单空间-时间起点的认知，揭示了第一帧实际上充当了模型内部的“概念记忆缓冲区”，用于存储和重用视觉实体。基于这一发现，论文提出了一种高效的视频内容定制方法，仅需少量（20-50个）训练样本，无需修改模型架构或进行大规模微调，即可在各种场景下实现鲁棒且泛化的参考视频定制。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>核心洞察：</strong> 论文的关键创新在于其对视频生成模型内部机制的深刻洞察——即第一帧不仅仅是起点，更是一个“概念记忆缓冲区”。这意味着模型在生成后续帧时，会主动从第一帧中提取和重用关键的视觉信息（如物体、场景元素等），而不是完全从头开始生成。</li>
<li><strong>方法论：</strong> 基于这一洞察，论文提出了一种“参考视频定制”的方法。其核心在于利用第一帧的“记忆”能力，通过提供少量参考样本（20-50个），引导模型生成符合特定内容或风格的视频。这种方法不需要对现有视频生成模型进行复杂的架构修改或耗时的大规模微调，而是巧妙地利用了模型已有的能力。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>降低视频定制门槛：</strong> 这项研究有望极大地降低视频内容定制的门槛。目前，高质量的视频生成和定制通常需要大量的训练数据、强大的计算资源和专业的知识。如果该方法能够广泛应用，将使得普通用户或小型团队也能相对容易地生成个性化视频。</li>
<li><strong>提升视频生成模型的效率和可控性：</strong> 揭示第一帧的“记忆”作用，为理解和控制视频生成过程提供了新的视角。这可能促使研究人员开发更高效、更易于控制的视频生成模型，并为解决视频生成中的一致性、可控性等难题提供新的思路。</li>
<li><strong>推动“少样本学习”在视频生成领域的应用：</strong> 论文展示了在视频生成领域实现高效“少样本学习”的可能性，这对于数据稀缺的应用场景具有重要意义。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>个性化视频内容创作：</strong> 例如，为社交媒体用户生成定制化的短视频，为品牌营销创建个性化广告。</li>
<li><strong>虚拟现实/增强现实（VR/AR）内容生成：</strong> 快速生成符合特定场景或用户需求的动态内容。</li>
<li><strong>电影和游戏制作：</strong> 辅助艺术家快速生成概念视频、场景草图或动态元素。</li>
<li><strong>教育和培训：</strong> 创建定制化的教学视频，以适应不同学习者的需求。</li>
<li><strong>视频编辑和后期制作：</strong> 提供更智能、更高效的视频编辑工具。</li>
<li><strong>内容审核和分析：</strong> 通过理解视频内容的关键元素，可能有助于更精细的内容分析。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>“概念记忆缓冲区”的精确机制未知：</strong> 摘要中提到“模型隐式地将第一帧视为概念记忆缓冲区”，但并未详细说明这一“记忆”是如何被编码、存储和检索的。其具体实现细节和工作原理仍需论文正文来阐述。</li>
<li><strong>“鲁棒性”和“泛化性”的定义和评估标准：</strong> 摘要声称方法具有“鲁棒性”和“泛化性”，但并未提供具体的评估指标或实验场景。这些特性的具体表现和局限性需要通过论文的实验结果来验证。</li>
<li><strong>“20-50个训练样本”的适用范围：</strong> 尽管数量少，但这些样本的质量、多样性以及它们与目标定制内容的相关性可能对最终效果产生显著影响。摘要并未说明样本的选取标准。</li>
<li><strong>对模型架构的依赖性：</strong> 论文强调“不进行架构更改”，这暗示了该方法可能对现有视频生成模型的架构有一定的依赖性。对于某些特定架构的模型，该方法的效果可能有所不同。</li>
<li><strong>定制的深度和复杂性：</strong> 摘要提到“视频内容定制”，但并未明确定制的深度和复杂性。例如，是仅限于物体替换、风格迁移，还是可以进行更复杂的场景重构或叙事改变。</li>
<li><strong>计算成本：</strong> 虽然避免了大规模微调，但参考视频定制过程本身的计算成本（如推理时间）并未在摘要中提及。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文的摘要非常有吸引力，因为它提出了一个关于视频生成模型内部工作机制的全新视角，并将这一洞察转化为一种实用的、高效的视频内容定制方法。如果其宣称的效果能够得到实验的充分验证，那么这项研究将对视频生成和内容创作领域产生深远的影响，有望 democratize（普及化）视频定制的门槛。其核心在于利用了模型对第一帧的“记忆”能力，实现了在极少样本下的定制化生成，这在“少样本学习”和“可控生成”领域都具有重要的理论和实践意义。然而，关于其具体机制、评估标准以及适用范围的细节，还需要进一步阅读论文原文来深入了解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15700v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15700v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15690v1'></a></p>
<h2 id="modes-accelerating-mixture-of-experts-multimodal-large-language-models-via-dynamic-expert-skipping"><a href="https://arxiv.org/abs/2511.15690v1">MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</a></h2>
<p><strong>Authors:</strong> Yushi Huang, Zining Wang, Zhihang Yuan, Yifu Ding, Ruihao Gong, Jinyang Guo, Xianglong Liu, Jun Zhang</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16<script type="math/tex">\times</script> and the decoding time by 1.26<script type="math/tex">\times</script>.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping”的全面中文摘要：</p>
<p><strong>论文题目：</strong> MoDES: 通过动态专家跳过加速混合专家多模态大语言模型</p>
<p><strong>作者：</strong> Yushi Huang, Zining Wang, Zhihang Yuan, Yifu Ding, Ruihao Gong, Jinyang Guo, Xianglong Liu, Jun Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
混合专家（MoE）多模态大语言模型（MLLMs）在视觉-语言任务中表现出色，但存在计算效率低下的问题。现有的专家跳过方法主要针对单模态大语言模型（LLMs）设计，直接应用于MLLMs会导致性能显著下降。这是因为这些方法未能充分考虑MoE层之间专家贡献的异质性以及不同模态token在这些层中的特定行为。因此，研究的核心问题是如何在不牺牲性能的前提下，为MoE MLLMs设计一种高效且准确的动态专家跳过框架。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
作者提出了<strong>MoDES（Multimodal Dynamic Expert Skipping）</strong>，这是首个无需训练即可实现MoE MLLM高效且准确推理的框架。其核心创新包括：</p>
<ul>
<li><strong>全局调制局部门控（GMLG）机制：</strong> 该机制结合了全局层级重要性（通过离线校准获得）和局部路由概率，以准确估计每个token的专家重要性。这解决了现有方法忽略专家跨层贡献不均的问题。</li>
<li><strong>双模态阈值（DMT）方法：</strong> 该方法分别处理来自不同模态（文本和视觉）的token，并根据其重要性分数和模态特定的阈值来决定是否跳过专家。这解决了现有方法未能考虑模态间差异的问题。</li>
<li><strong>前沿搜索算法：</strong> 为了确定最优的模态特定阈值，作者引入了一种利用单调性属性的前沿搜索算法，将原本需要数天的收敛时间缩短到数小时，显著提高了效率。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能提升：</strong> MoDES在3个模型系列和13个基准测试中，显著优于现有方法。例如，在Qwen3-VL-MoE-30B-A3B-Instruct模型上，当跳过88%的专家时，性能提升高达10.67%（97.33% vs. 86.66%），同时保持了高精度。
*   <strong>推理加速：</strong> MoDES显著提高了推理速度，在Qwen3-VL-MoE-30B-A3B-Instruct模型上，预填充时间提升了2.16倍，解码时间提升了1.26倍。
*   <strong>普适性：</strong> 实验表明，MoDES在不同模型骨干（backbones）上都表现出优越性，并且在不同跳过比例下都能保持高性能。
*   <strong>效率：</strong> 作者提出的前沿搜索算法将阈值优化的搜索时间从数天缩短到数小时，证明了其高效性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>离线校准：</strong> GMLG机制中的全局重要性因子α(l)需要离线校准，虽然不增加推理开销，但需要额外的计算步骤。
*   <strong>计算成本：</strong> 尽管MoDES显著提高了效率，但对于非常大的模型，计算和搜索最优阈值仍然需要一定的计算资源。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>与其他优化技术的结合：</strong> 作者提到未来将探索将MoDES与其他正交技术（如剪枝和蒸馏）结合，以进一步降低MoE MLLMs的计算需求。
*   <strong>更精细的模态交互：</strong> 虽然MoDES考虑了模态差异，但未来可以进一步探索更精细的模态交互机制，以更深入地理解和利用不同模态token的特性。
*   <strong>在线自适应：</strong> 目前的校准是离线的，未来可以探索在线自适应的全局重要性计算方法，以应对动态变化的数据分布。</p>
<p><strong>总结：</strong>
这篇论文成功地解决了MoE MLLMs在推理效率方面的关键挑战。通过引入创新的GMLG和DMT机制，以及高效的前沿搜索算法，MoDES实现了在不牺牲性能的前提下，显著加速MoE MLLMs的推理过程。其对模态差异和层级贡献的深入分析，为多模态模型的高效推理提供了新的视角，并为未来相关研究奠定了基础。这对于在资源受限的环境下部署和应用大型多模态模型具有重要的实际意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers.</li>
<li>Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference.</li>
<li>To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours.</li>
<li>Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15690v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15690v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15661v1'></a></p>
<h2 id="visplay-self-evolving-vision-language-models-from-images"><a href="https://arxiv.org/abs/2511.15661v1">VisPlay: Self-Evolving Vision-Language Models from Images</a></h2>
<p><strong>Authors:</strong> Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VisPlay: Self-Evolving Vision-Language Models from Images”的全面中文摘要：</p>
<p><strong>论文题目：</strong> VisPlay: Self-Evolving Vision-Language Models from Images</p>
<p><strong>作者：</strong> Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心问题：</strong>
当前，视觉语言模型（VLMs）在复杂推理任务上的提升主要依赖于强化学习（RL）。然而，现有的RL方法通常需要昂贵且难以扩展的人工标注标签或特定任务的启发式规则来定义可验证的奖励。这限制了VLMs在海量未标注图像数据上的自主学习和能力提升。论文旨在解决如何让VLMs能够仅凭海量未标注的图像数据，自主地提升其视觉推理能力，从而克服人工标注的瓶颈。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
论文提出了 <strong>VisPlay</strong>，一个<strong>自演化（self-evolving）的强化学习框架</strong>，使VLMs能够自主地从海量未标注图像数据中提升推理能力。其核心创新在于：</p>
<ul>
<li><strong>双角色协同演化：</strong> VisPlay将一个基础VLM分解为两个相互作用的角色：<ul>
<li><strong>图像条件化提问者（Image-Conditioned Questioner）：</strong> 负责根据输入图像生成具有挑战性且可回答的视觉问题。</li>
<li><strong>多模态推理者（Multimodal Reasoner）：</strong> 负责根据图像和生成的问题，产生“银质”（silver）回答。</li>
</ul>
</li>
<li><strong>联合训练机制：</strong> 两个角色通过<strong>组相对策略优化（Group Relative Policy Optimization, GRPO）</strong>进行联合训练。GRPO结合了<strong>多样性奖励</strong>和<strong>难度奖励</strong>，以平衡生成问题的难度和回答的质量，无需外部监督。</li>
<li><strong>自演化循环：</strong> 提问者被训练生成更具挑战性的问题，而推理者则被训练解决越来越难的问题，形成一个持续改进的闭环。</li>
<li><strong>伪标签生成：</strong> 针对提问者生成的问题，利用推理者自身生成多个回答，通过多数投票生成伪标签，并计算置信度作为问题难度的代理。</li>
<li><strong>不确定性奖励与多样性正则化：</strong> 提问者通过奖励与推理者不确定性（置信度接近0.5）相关的奖励来生成更难的问题，并通过多样性正则化避免生成重复性问题。</li>
</ul>
<p><strong>3. 主要结果及意义：</strong>
*   <strong>显著性能提升：</strong> 在Qwen2.5-VL和MiMo-VL等模型家族上进行训练，VisPlay在视觉推理、组合泛化和幻觉减少方面取得了<strong>持续的性能提升</strong>。例如，Qwen2.5-VL-3B的平均得分从基线30.61提升到迭代三次后的47.27。
*   <strong>跨模型和任务的普适性：</strong> VisPlay框架在不同模型和模型规模上都展现了良好的<strong>可扩展性和泛化能力</strong>。
*   <strong>多模态能力增强：</strong> 实验表明，VisPlay有效增强了<strong>任务特定推理</strong>和<strong>跨领域多模态泛化</strong>能力，尤其在<strong>幻觉检测</strong>方面表现突出，显著降低了模型产生不实信息的概率。
*   <strong>可扩展的自演化路径：</strong> VisPlay为实现<strong>可扩展的、自演化的多模态智能</strong>提供了一条有前景的路径，证明了在缺乏人工监督的情况下，模型也能显著提升其能力。
*   <strong>与人工标注数据媲美：</strong> 与使用人工标注数据训练的模型相比，VisPlay训练的模型在<strong>平均准确率上具有竞争力</strong>，并显著减少了幻觉，表明其自动化训练流程的有效性。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>计算资源限制：</strong> 实验主要集中在Qwen2.5-VL和MiMo-VL模型家族，对于更大规模的模型（如≥10B参数）的<strong>可扩展性</strong>仍是开放性问题。
*   <strong>自生成数据验证：</strong> 框架缺乏<strong>明确的验证方法</strong>来评估自生成数据的忠实度，尽管GRPO间接优化了质量，但开发更鲁棒的自动化方法来防止错误累积仍是未来研究的重点。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更大规模模型的验证：</strong> 探索VisPlay在更大规模VLMs上的表现。
*   <strong>更鲁棒的数据验证机制：</strong> 开发更有效的机制来评估和确保自生成数据的质量和可靠性。
*   <strong>更广泛的应用领域：</strong> 将VisPlay框架扩展到更广泛的多模态任务和应用场景。
*   <strong>探索更复杂的自演化策略：</strong> 研究更高级的自演化机制，以进一步提升模型的自主学习能力。</p>
<p><strong>总结：</strong>
VisPlay论文提出了一种创新的自演化强化学习框架，通过让VLM扮演提问者和回答者的角色并进行协同训练，成功地实现了在无人工标注数据的情况下，模型在视觉推理、组合泛化和减少幻觉方面的显著提升。该框架不仅展示了其在不同模型上的可扩展性，也为构建能够持续自主进化的多模态智能系统开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15661v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15661v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15622v1'></a></p>
<h2 id="the-sa-fari-dataset-segment-anything-in-footage-of-animals-for-recognition-and-identification"><a href="https://arxiv.org/abs/2511.15622v1">The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification</a></h2>
<p><strong>Authors:</strong> Dante Francisco Wasmuht, Otto Brookes, Maximillian Schall, Pablo Palencia, Chris Beirne, Tilo Burghardt, Majid Mirmehdi, Hjalmar Kühl, Mimi Arandjelovic, Sam Pottie, Peter Bermant, Brandon Asheim, Yi Jin Toh, Adam Elzinga, Jason Holmberg, Andrew Whitworth, Eleanor Flatt, Laura Gustafson, Chaitanya Ryali, Yuan-Ting Hu, Baishan Guo, Andrew Westbury, Kate Saenko, Didac Suris</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at <script type="math/tex">\href{https://www.conservationxlabs.com/sa-fari}{\text{conservationxlabs.com/SA-FARI}}</script>.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification”的全面中文摘要：</p>
<p><strong>论文题目：</strong> The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification</p>
<p><strong>作者：</strong> Dante Francisco Wasmuht, Otto Brookes, Maximillian Schall, Pablo Palencia, Chris Beirne, Tilo Burghardt, Majid Mirmehdi, Hjalmar Kühl, Mimi Arandjelovic, Sam Pottie, Peter Bermant, Brandon Asheim, Yi Jin Toh, Adam Elzinga, Jason Holmberg, Andrew Whitworth, Eleanor Flatt, Laura Gustafson, Chaitanya Ryali, Yuan-Ting Hu, Baishan Guo, Andrew Westbury, Kate Saenko, Didac Suris</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
论文的核心问题在于，当前用于野生动物多目标跟踪（Multi-Animal Tracking, MAT）的研究领域缺乏大规模、多样化且高质量的数据集。现有的数据集在规模、物种覆盖范围、时间和地理多样性方面存在显著限制，这阻碍了通用型MAT模型的训练和评估，而这些模型对于实现自动化野生动物监测至关重要。自动化视频分析在野生动物保护中扮演着关键角色，MAT是其中一项基础任务，它支撑着个体识别和行为识别等应用。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>SA-FARI 数据集：</strong> 作者提出了SA-FARI数据集，这是目前为止最大的开源野生动物MAT数据集。它包含来自741个独立采样点、横跨4大洲、历时约10年（2014-2024）收集的11,609个相机陷阱视频，涵盖了99个物种类别。
*   <strong>高质量标注：</strong> 数据集提供了详尽的时空标注，包括约46小时的密集标注视频，其中包含16,224个“masklet”（个体身份保持的分割掩码序列）以及942,702个个体边界框、分割掩码和物种标签。此外，还发布了匿名的相机陷阱位置信息。
*   <strong>多样性与规模：</strong> SA-FARI在总标注时长和物种多样性上均远超现有数据集，提供了前所未有的规模和多样性，使其成为训练和评估通用MAT模型的理想基准。
*   <strong>基准测试：</strong> 论文在SA-FARI数据集上对最先进的视觉语言模型（如SAM 3）和纯视觉方法进行了全面的基准测试，评估了它们在物种特定和通用动物提示下的检测和跟踪性能。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>模型性能提升：</strong> 研究表明，在SA-FARI数据集上进行训练或微调，能够显著提升现有SOTA模型（如SAM 3）的性能。例如，微调后的SAM 3模型在多个指标上比基线模型有大幅提升，证明了大规模、多样化标注数据的价值。
*   <strong>挑战与机遇：</strong> 基准测试结果揭示了在真实野生环境中进行MAT的固有挑战，尤其是在处理小尺寸掩码、遮挡、运动以及多动物场景时。
*   <strong>推动领域发展：</strong> SA-FARI数据集的发布为野生动物监测领域的研究人员提供了一个强大的新基础，有望加速通用、鲁棒的MAT系统的开发，从而更有效地支持生物多样性监测和保护工作。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>数据分布的长尾效应：</strong> 与真实世界的自然数据一样，SA-FARI数据集也呈现出长尾分布的特点，即少数物种的视频数量占比较大，而大多数物种的视频数量较少。
*   <strong>特定场景的挑战：</strong> 研究发现，小尺寸掩码、遮挡、运动以及夜间场景对模型检测和跟踪带来了更大的挑战。
*   <strong>地理偏差：</strong> 数据集主要来自南美和中美洲的录制地点，这可能导致一定的地理偏差。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>多模态融合：</strong> 数据集中包含的音频流为未来的多模态模型开发提供了机会，可以利用声音信息来增强检测、物种分类和跟踪的鲁棒性。
*   <strong>扩展数据模态：</strong> 数据集可以进一步扩展，例如加入动物姿态、深度信息以及自然语言描述等。
*   <strong>地理多样性增强：</strong> 未来数据收集应优先考虑更多样化的生态区域，以减少地理偏差，捕捉更广泛的物种。
*   <strong>模型改进：</strong> 针对数据集中发现的特定挑战（如小目标、遮挡、夜间等），开发更鲁棒的MAT模型。</p>
<p><strong>总结：</strong>
SA-FARI数据集的发布是野生动物多目标跟踪领域的一项重要贡献。它通过提供前所未有的规模、物种多样性和高质量的时空标注，解决了现有数据集的局限性，为训练和评估通用MAT模型奠定了坚实基础。研究结果表明，该数据集能够显著提升现有模型的性能，并为未来的研究指明了方向，有望加速自动化野生动物监测和保护技术的进步。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals.</li>
<li>Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts.</li>
<li>SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15622v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15622v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15618v1'></a></p>
<h2 id="flashmesh-faster-and-better-autoregressive-mesh-synthesis-via-structured-speculation"><a href="https://arxiv.org/abs/2511.15618v1">FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation</a></h2>
<p><strong>Authors:</strong> Tingrui Shen, Yiheng Zhang, Chen Tang, Chuan Ping, Zixing Zhao, Le Wan, Yuwang Wang, Ronggang Wang, Shengfeng He</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation
<strong>作者：</strong> Tingrui Shen, Yiheng Zhang, Chen Tang, Chuan Ping, Zixing Zhao, Le Wan, Yuwang Wang, Ronggang Wang, Shengfeng He</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>论文旨在解决当前基于自回归（autoregressive）模型生成三维网格（mesh）时存在的<strong>推理速度慢</strong>的问题。虽然自回归模型能够生成高质量的网格，但其逐个 token（如顶点、面）解码的串行化过程导致推理效率低下，这严重限制了其在交互式应用和大规模场景中的实用性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>为了克服这一挑战，作者提出了<strong>FlashMesh</strong>框架，其核心创新在于引入了一种新颖的<strong>“预测-修正-验证”（predict-correct-verify）范式</strong>来重构自回归解码过程。其关键洞察在于，网格数据本身具有强大的结构和几何相关性，这使得模型能够自信地进行<strong>多 token 的推测（speculation）</strong>。</p>
<p>FlashMesh 的具体贡献包括：</p>
<ul>
<li><strong>结构化推测解码（Structured Speculative Decoding）：</strong> 针对常用的 Hourglass Transformer 架构，设计了一种定制化的推测解码方案，能够跨越面（face）、点（point）和坐标（coordinate）三个层级并行预测多个未来的 token。</li>
<li><strong>分层推测模块（Hierarchical Speculative Modules）：</strong> 引入了 SP-Block（Speculative Prediction Block）和 HF-Block（Hierarchical Fusion Block）。SP-Block 负责并行生成多个草稿 token，而 HF-Block 则利用高层结构信息和低层局部上下文来精炼这些预测，并保持层级一致性。</li>
<li><strong>结构感知修正机制（Structure-Aware Correction Mechanism）：</strong> 针对并行预测可能引入的局部几何不一致性，设计了一个修正模块，利用网格连接性先验来强制执行顶点共享一致性，并调整几何坐标预测。</li>
<li><strong>验证阶段（Verification Stage）：</strong> 利用主干网络在一个前向传播中验证修正后的草稿 token，从而确保最终输出的忠实性，并加速推理。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著的推理加速：</strong> 实验表明，FlashMesh 相比于标准的自回归模型，推理速度最高可提升 <strong>2 倍</strong>。</li>
<li><strong>生成质量的提升：</strong> 在加速的同时，FlashMesh 还能<strong>提高生成网格的保真度（fidelity）</strong>，产生更优的几何和拓扑质量。</li>
<li><strong>有效利用结构先验：</strong> 研究证明，网格数据固有的结构化先验信息可以被系统地利用起来，以加速和增强自回归生成过程。</li>
<li><strong>更好的权衡：</strong> FlashMesh 在质量（CD, HD）、效率（TPS）和速度提升（Speed-up）之间取得了最佳的权衡。</li>
</ul>
<p>FlashMesh 的提出是迈向<strong>可扩展且易于访问的三维网格生成</strong>的重要一步，为解决自回归模型在实际应用中的瓶颈提供了有效途径。</p>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>继承自回归模型的局限性：</strong> FlashMesh 仍然继承了自回归模型的固有局限性，例如对<strong>早期预测错误敏感</strong>。</li>
<li><strong>潜在的精度损失（在极小模型下）：</strong> 在非常小的模型（如 0.5B 参数）下，虽然速度有所提升，但生成质量可能略有下降，作者将其归因于小模型在支持多 token 预测方面的表示能力和推理能力不足。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>探索混合解码策略：</strong> 结合其他解码策略，以进一步提升性能。</li>
<li><strong>集成更显式的几何先验：</strong> 更明确地将几何先验知识融入模型，以增强鲁棒性。</li>
<li><strong>进一步提升鲁棒性：</strong> 针对早期预测错误对生成质量的影响，探索更有效的缓解方法。</li>
</ul>
<p>总而言之，FlashMesh 通过创新的“预测-修正-验证”范式和结构化的多 token 推测解码，成功地解决了自回归网格生成中的速度瓶颈问题，并在保持甚至提升生成质量的同时，实现了显著的推理加速，为三维网格生成领域带来了重要的进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15618v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15618v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15613v1'></a></p>
<h2 id="when-to-think-and-when-to-look-uncertainty-guided-lookback"><a href="https://arxiv.org/abs/2511.15613v1">When to Think and When to Look: Uncertainty-Guided Lookback</a></h2>
<p><strong>Authors:</strong> Jing Bi, Filippos Bellos, Junjia Guo, Yayuan Li, Chao Huang,  Yunlong,  Tang, Luchuan Song, Susan Liang,  Zhongfei,  Zhang, Jason J. Corso, Chenliang Xu</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“When to Think and When to Look: Uncertainty-Guided Lookback”的全面摘要，重点关注其在计算机视觉领域的研究问题、方法、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> When to Think and When to Look: Uncertainty-Guided Lookback</p>
<p><strong>作者：</strong> Jing Bi, Filippos Bellos, Junjia Guo, Yayuan Li, Chao Huang, Yunlong (Yolo) Tang, Luchuan Song, Susan Liang, Zhongfei (Mark) Zhang, Jason J. Corso, Chenliang Xu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong></p>
<p>该论文的核心研究问题在于深入探究“测试时思考”（test-time thinking），即生成显式的中间推理链，在大型视觉语言模型（LVLMs）中的实际作用。尽管这种“思考”模式在提升模型性能方面展现出潜力，但目前缺乏对其如何影响视觉推理的系统性分析。具体来说，研究者们试图回答以下关键问题：
* <strong>思考何时真正有助于视觉推理？</strong> 思考的收益是否与模型容量、采样预算和任务类别相关？
* <strong>如何权衡思考的“广度”与“深度”？</strong> 如何最优地分配计算资源于生成更多的推理路径（广度）或使用更强的推理模式（深度）？
* <strong>能否在感知任务中自适应地控制思考？</strong> 是否存在一种机制，能够根据视觉线索和不确定性信号来动态调整思考过程，而非盲目地延长推理链？</p>
<p><strong>2. 主要创新与方法论贡献：</strong></p>
<p>该论文的主要贡献在于其对视觉思考的系统性分析以及提出的新颖的解码策略：</p>
<ul>
<li><strong>首次大规模、受控的视觉思考分析：</strong> 研究者对 InternVL3.5 和 Qwen3-VL 系列的十个 LVLM 变体进行了大规模、受控的比较分析，评估了在不同 token 预算和多轮解码下的思考效果。</li>
<li><strong>揭示“思考”的非线性影响：</strong> 研究发现，更多的思考并非总是更好。过长的推理链可能导致“长错”（long-wrong）的轨迹，忽略图像信息，甚至不如标准指令模式。</li>
<li><strong>发现“回看”（Lookback）短语的重要性：</strong> 分析表明，在成功的推理轨迹中，一些明确指向图像的简短“回看”短语（lookback phrases）显著富集，并与更好的视觉基础（visual grounding）相关。</li>
<li><strong>提出“不确定性引导的回看”（Uncertainty-Guided Lookback）策略：</strong> 这是一种训练免费、模型无关的解码策略。它结合了不确定性信号和自适应的回看提示（adaptive lookback prompts）以及广度搜索（breadth search）。该策略的核心在于：<ul>
<li><strong>Token-Level Visual Sensitivity Probe：</strong> 通过计算模型在真实图像、噪声图像和无图像三种视觉上下文下的每步困惑度（perplexity），来量化图像内容和图像存在本身对推理的影响。这有助于识别模型何时依赖图像内容（Acontent）和何时仅仅对视觉信号做出反应（Apresence）。</li>
<li><strong>挖掘“回看”短语和不确定性短语：</strong> 基于上述探测，研究者们挖掘出能够明确提示模型重新审视图像细节的“回看”短语，以及能够捕捉模型推理不确定性的短语。</li>
<li><strong>自适应触发机制：</strong> 当模型推理进入不确定性较高的阶段，或者检测到“长错”迹象时，该策略会插入预先挖掘的“回看”短语，强制模型重新聚焦于图像。</li>
<li><strong>并行回看采样（Parallel Lookback Sampling）：</strong> 在触发回看时，可以并行探索多个基于图像的推理分支，以提高找到正确路径的概率。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 该方法在 MMMUval 基准测试上显著提升了整体性能，尤其是在标准思考模式表现较弱的类别中，带来了最大的增益。</li>
<li><strong>超越基线：</strong> 在固定的模型家族和 token 预算下，该方法优于多个强大的解码基线，达到了新的最先进水平。</li>
<li><strong>泛化性：</strong> 该解码策略具有良好的泛化能力，在另外五个基准测试（包括两个广泛的多模态数据集和数学推理数据集）上均取得了持续的改进。</li>
<li><strong>效率提升：</strong> 在实现性能提升的同时，该方法通常能减少 token 的使用量，提高了计算效率。例如，在 Qwen3-VL 模型上，其回看变体在 Pass@1 提升的同时，token 使用量减少了约 40-60%。</li>
<li><strong>揭示模型行为：</strong> 研究深入揭示了模型容量、任务难度和模型家族对思考效果的影响，为理解 LVLM 的推理机制提供了重要见解。例如，模型容量越大，思考的效率越高；识别和检索类任务可能更适合简洁的指令模式，而需要复杂推理的任务则更能从思考中获益。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模型依赖性：</strong> 虽然提出的策略是模型无关的，但其效果仍受限于所使用的 LVLM 的基础能力。</li>
<li><strong>计算成本：</strong> 尽管该方法旨在提高效率，但“不确定性引导的回看”策略本身（如挖掘短语）需要离线计算，并且在推理时进行短语匹配和插入也需要一定的计算开销，尽管作者强调其开销相对较小。</li>
<li><strong>“回看”的潜在负面影响：</strong> 作者也提到，在某些情况下，回看可能会偶尔带来负面影响，尽管这种情况相对较少。</li>
<li><strong>对特定任务的适应性：</strong> 虽然在多个基准上表现良好，但对于某些高度专业化或非常规的任务，其效果可能需要进一步验证。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的自适应控制：</strong> 进一步探索更精细的机制来判断何时以及如何触发回看，以及选择何种回看短语。</li>
<li><strong>结合其他推理技术：</strong> 将不确定性引导的回看策略与自洽性（self-consistency）、反射（reflection）等其他先进的文本推理技术相结合，以期获得更强大的多模态推理能力。</li>
<li><strong>跨模型家族的通用性：</strong> 进一步验证该策略在更多不同架构和训练方法的 LVLM 上的有效性。</li>
<li><strong>更深入的错误分析：</strong> 深入分析模型在哪些具体场景下会产生“长错”或“静默错”（quiet-wrong）的推理，并针对性地优化策略。</li>
<li><strong>实时在线学习：</strong> 探索将部分探测和短语挖掘过程在线化，使模型能够实时适应新的任务或数据分布。</li>
</ul>
<p>总而言之，这篇论文通过对视觉思考的细致分析，揭示了其复杂性，并提出了一种创新的、基于不确定性和视觉基础的自适应解码策略，显著提升了 LVLM 在视觉推理任务上的性能和效率，为未来 LVLM 的推理研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode.</li>
<li>Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search.</li>
<li>Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15613v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15613v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.15605v1'></a></p>
<h2 id="srpo-self-referential-policy-optimization-for-vision-language-action-models"><a href="https://arxiv.org/abs/2511.15605v1">SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</a></h2>
<p><strong>Authors:</strong> Senyu Fei, Siyin Wang, Li Ji, Ao Li, Shiduo Zhang, Liming Liu, Jinlong Hou, Jingjing Gong, Xianzhong Zhao, Xipeng Qiu</p>
<p><strong>Published:</strong> 2025-11-19</p>
<p><strong>Categories:</strong> cs.RO, cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文分析：SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</strong></p>
<p><strong>1. 主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>该论文提出了一种新颖的视觉-语言-动作 (VLA) 模型强化学习 (RL) 框架——自参照策略优化 (SRPO)。SRPO 克服了现有 VLA-RL 方法中普遍存在的奖励稀疏性问题，通过利用模型自身在当前训练批次中生成的成功轨迹作为“自参照”，为失败的尝试分配了基于进度的奖励，从而显著提高了训练效率和性能。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>SRPO 的核心创新在于其<strong>自参照机制</strong>和<strong>基于潜在世界表示的进度度量</strong>。</p>
<ul>
<li>
<p><strong>自参照机制 (Self-Referential Policy Optimization):</strong></p>
<ul>
<li><strong>摆脱外部依赖:</strong> SRPO 不再依赖于专家演示或手动设计的奖励函数。</li>
<li><strong>利用内部成功信号:</strong> 它巧妙地利用模型在当前训练批次中已经生成的成功轨迹，将这些成功轨迹作为“黄金标准”或“参照点”。</li>
<li><strong>为失败轨迹分配进度奖励:</strong> 基于这些自参照的成功轨迹，SRPO 能够为那些未能完全成功的轨迹分配一个“进度奖励”，而不是简单地给予零奖励。这意味着即使一个尝试没有达到最终目标，但如果它在过程中表现出一定的进步，也能获得积极的反馈，从而更有效地指导策略学习。</li>
</ul>
</li>
<li>
<p><strong>基于潜在世界表示的进度度量 (Progress Measurement using Latent World Representations):</strong></p>
<ul>
<li><strong>鲁棒的进度评估:</strong> 为了准确衡量行为进度，SRPO 引入了使用<strong>世界模型 (world model) 的潜在空间表示 (latent representations)</strong>。</li>
<li><strong>抽象和可迁移性:</strong> 这些潜在表示是从原始像素中压缩而来，能够捕捉到环境的本质特征和动态，并且具有良好的可迁移性。这意味着它们不需要针对特定环境进行领域特定的微调。</li>
<li><strong>泛化能力:</strong> 这种方法使得模型能够跨不同环境准确地比较轨迹的进度模式，从而实现更泛化的学习。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>SRPO 的提出对 VLA 模型和机器人领域具有重要的潜在影响：</p>
<ul>
<li><strong>提升 VLA 模型在机器人任务中的实用性:</strong> 通过解决奖励稀疏性问题，SRPO 大大提高了 VLA 模型通过 RL 进行后训练的效率和效果，使其在复杂机器人操作任务中更具实用性。</li>
<li><strong>降低数据依赖性:</strong> 摆脱对大量专家演示的依赖，降低了数据收集的成本和难度，使得 VLA 模型更容易部署和应用。</li>
<li><strong>推动 RL 在机器人领域的普及:</strong> SRPO 的方法为解决 RL 在机器人领域面临的普遍挑战（如奖励设计和稀疏性）提供了新的思路，可能加速 RL 在机器人控制中的应用。</li>
<li><strong>促进更通用的机器人学习:</strong> 基于潜在世界表示的进度度量方法，有望实现更具泛化能力的机器人策略学习，使其能够适应更广泛的任务和环境。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>机器人操作 (Robotic Manipulation):</strong> 这是论文直接关注的领域，包括抓取、放置、组装等复杂任务。</li>
<li><strong>自动驾驶 (Autonomous Driving):</strong> 尽管不是直接的 VLA 模型，但自动驾驶也需要理解视觉信息、语言指令（如导航）和执行动作。SRPO 的进度度量思想可能有助于学习更平滑、更安全的驾驶策略。</li>
<li><strong>人机交互 (Human-Robot Interaction):</strong> 当机器人需要理解人类的语言指令并执行相应的动作时，SRPO 的方法可以帮助机器人更有效地学习和适应。</li>
<li><strong>游戏 AI (Game AI):</strong> 在需要通过观察游戏画面、理解游戏规则（语言）并执行操作来达成目标的场景，SRPO 的方法可以提高 AI 的学习效率。</li>
<li><strong>虚拟现实/增强现实 (VR/AR) 中的交互:</strong> 在这些环境中，用户可能通过语言指令与虚拟对象进行交互，SRPO 的方法可以用于训练更智能的虚拟代理。</li>
<li><strong>通用人工智能 (AGI) 的探索:</strong> SRPO 提出的自参照学习和泛化能力，是迈向更通用智能体的重要一步。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了显著的成果，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对世界模型的依赖:</strong> SRPO 的核心在于利用世界模型的潜在表示。因此，其性能在很大程度上取决于所使用的世界模型的质量和能力。如果世界模型无法准确地捕捉环境的动态或生成有意义的潜在表示，SRPO 的效果可能会受到限制。</li>
<li><strong>“自参照”的定义和稳定性:</strong> 摘要提到“利用模型自身的成功轨迹，生成在当前训练批次中”。这可能意味着在训练初期，如果模型尚未生成任何成功的轨迹，SRPO 的自参照机制可能无法有效启动。此外，如果模型在训练过程中出现不稳定的行为，其“自参照”的基准也可能随之波动。</li>
<li><strong>计算成本:</strong> 训练一个世界模型本身可能需要大量的计算资源。虽然 SRPO 声称提高了 RL 训练效率，但整体的训练流程（包括世界模型的训练）的计算成本仍需考虑。</li>
<li><strong>潜在的“局部最优”陷阱:</strong> 虽然 SRPO 能够为失败轨迹分配进度奖励，但如果“自参照”的成功轨迹本身就存在某种偏差或局限性，模型可能会被引导到次优的策略空间，即陷入“局部最优”。</li>
<li><strong>泛化能力的边界:</strong> 摘要提到“跨环境的准确、泛化的轨迹比较”，但这种泛化能力在多大程度上能够跨越非常大的环境差异或任务类型，仍需进一步验证。LIBERO benchmark 的成功可能是在相对相似的环境下进行的。</li>
<li><strong>对“成功”的定义:</strong> 尽管 SRPO 解决了奖励稀疏性，但最终的“成功”仍然需要一个明确的定义来评估。如果最终的成功标准过于苛刻或模糊，SRPO 的效果也可能受到影响。</li>
</ul>
<p>总而言之，SRPO 是一项令人兴奋的研究，它通过创新的自参照学习和利用世界模型潜在表示来解决 VLA 模型 RL 中的关键挑战。其在效率和性能上的显著提升，预示着其在机器人和相关领域具有广阔的应用前景。然而，对其依赖的世界模型质量、自参照机制的稳定性以及泛化能力的边界等方面的进一步研究和验证将是重要的。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework.</li>
<li>Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.15605v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.15605v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-20 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
