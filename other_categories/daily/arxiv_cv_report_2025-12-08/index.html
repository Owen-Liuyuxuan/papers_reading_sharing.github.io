<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-08 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-05/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-09/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-08">Arxiv Computer Vision Papers - 2025-12-08</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#editthinker-unlocking-iterative-reasoning-for-any-image-editor" class="nav-link">EditThinker: Unlocking Iterative Reasoning for Any Image Editor</a>
                </li>
                <li class="nav-item">
                    <a href="#m4-rag-a-massive-scale-multilingual-multi-cultural-multimodal-rag" class="nav-link">M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</a>
                </li>
                <li class="nav-item">
                    <a href="#simpact-simulation-enabled-action-planning-using-vision-language-models" class="nav-link">SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#correspondence-oriented-imitation-learning-flexible-visuomotor-control-with-3d-conditioning" class="nav-link">Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</a>
                </li>
                <li class="nav-item">
                    <a href="#zoom-in-click-out-unlocking-and-evaluating-the-potential-of-zooming-for-gui-grounding" class="nav-link">Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</a>
                </li>
                <li class="nav-item">
                    <a href="#measuring-the-effect-of-background-on-classification-and-feature-importance-in-deep-learning-for-av-perception" class="nav-link">Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#synset-signset-germany-a-synthetic-dataset-for-german-traffic-sign-recognition" class="nav-link">Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition</a>
                </li>
                <li class="nav-item">
                    <a href="#physically-based-simulation-of-automotive-lidar" class="nav-link">Physically-Based Simulation of Automotive LiDAR</a>
                </li>
                <li class="nav-item">
                    <a href="#world-models-that-know-when-they-dont-know-controllable-video-generation-with-calibrated-uncertainty" class="nav-link">World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</a>
                </li>
                <li class="nav-item">
                    <a href="#scail-towards-studio-grade-character-animation-via-in-context-learning-of-3d-consistent-pose-representations" class="nav-link">SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-08">Arxiv Computer Vision Papers - 2025-12-08</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月5日Arxiv计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月5日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>日期：</strong> 2025年12月5日</p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期Arxiv论文集中体现了计算机视觉领域在<strong>多模态理解与生成、具身智能与机器人控制、以及对模型鲁棒性和可解释性的追求</strong>等方面的显著进展。</p>
<ul>
<li><strong>多模态融合与生成：</strong> 论文普遍关注如何将视觉信息与语言、行为等其他模态进行更深度的融合，以实现更强大的理解和生成能力。例如，<strong>M4-RAG</strong>展示了大规模多语言多文化多模态检索增强生成（RAG）的潜力，而<strong>SCAIL</strong>则在3D角色动画生成方面取得了突破。</li>
<li><strong>具身智能与交互：</strong> 多个研究聚焦于让AI能够理解和执行物理世界的任务，特别是通过视觉进行规划和控制。<strong>SIMPACT</strong>利用视觉语言模型进行仿真驱动的动作规划，<strong>Correspondence-Oriented Imitation Learning</strong>则探索了灵活的视觉运动控制。<strong>Zoom in, Click out</strong>则关注了GUI的理解和交互。</li>
<li><strong>模型鲁棒性与可解释性：</strong> 研究人员开始更加关注模型在真实世界复杂场景下的表现，以及理解其决策过程。<strong>Measuring the Effect of Background</strong>探讨了背景对分类和特征重要性的影响，而<strong>World Models That Know When They Don't Know</strong>则强调了模型对自身不确定性的认知，这对于安全可靠的应用至关重要。</li>
<li><strong>合成数据与仿真：</strong> <strong>Synset Signset Germany</strong>和<strong>Physically-Based Simulation of Automotive LiDAR</strong>的出现表明，高质量的合成数据和物理仿真正在成为训练和评估视觉模型的重要手段，尤其是在自动驾驶等领域。</li>
</ul>
<p><strong>特别值得关注的论文：</strong></p>
<ul>
<li><strong>"EditThinker: Unlocking Iterative Reasoning for Any Image Editor"</strong>：该论文提出了一种新颖的迭代推理框架，能够增强现有图像编辑器的能力，预示着更智能、更具创造性的图像编辑新时代。</li>
<li><strong>"World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty"</strong>：这项工作在视频生成领域引入了“知道自己不知道”的能力，通过校准不确定性来实现可控的视频生成，对于生成可靠和可信的视频内容具有重要意义。</li>
<li><strong>"M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG"</strong>：其大规模、多语言、多文化、多模态的RAG方法，为构建更具普适性和包容性的AI系统奠定了基础。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>迭代式推理与编辑：</strong> 图像编辑不再是单次操作，而是通过迭代推理来优化结果。</li>
<li><strong>具身AI的规划与控制：</strong> 将视觉理解与物理世界的动作规划紧密结合，是迈向通用人工智能的关键一步。</li>
<li><strong>模型不确定性量化与利用：</strong> 让模型能够感知自身的局限性，并据此调整行为，是提升AI安全性和可靠性的重要方向。</li>
<li><strong>大规模多模态RAG：</strong> 结合检索增强生成，并将其扩展到更广泛的语言和文化背景，是提升模型泛化能力的关键。</li>
<li><strong>合成数据与物理仿真在复杂场景下的应用：</strong> 尤其是在自动驾驶等对安全性要求极高的领域。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其创新性和潜在影响力，以下论文值得深入阅读：</p>
<ol>
<li><strong>"EditThinker: Unlocking Iterative Reasoning for Any Image Editor"</strong>：对于图像编辑和内容生成领域的研究者。</li>
<li><strong>"World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty"</strong>：对于视频生成、生成模型以及AI安全性的研究者。</li>
<li><strong>"SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models"</strong>：对于机器人学、具身智能和强化学习的研究者。</li>
<li><strong>"M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG"</strong>：对于自然语言处理、多模态AI和信息检索的研究者。</li>
</ol>
<hr />
<p>希望这份执行摘要能帮助您快速了解近期Arxiv计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.05965v1">EditThinker: Unlocking Iterative Reasoning for Any Image Editor</a></li>
<li><a href="#2512.05959v1">M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</a></li>
<li><a href="#2512.05955v1">SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</a></li>
<li><a href="#2512.05953v1">Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</a></li>
<li><a href="#2512.05941v1">Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</a></li>
<li><a href="#2512.05937v1">Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</a></li>
<li><a href="#2512.05936v1">Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition</a></li>
<li><a href="#2512.05932v1">Physically-Based Simulation of Automotive LiDAR</a></li>
<li><a href="#2512.05927v1">World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</a></li>
<li><a href="#2512.05905v1">SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.05965v1'></a></p>
<h2 id="editthinker-unlocking-iterative-reasoning-for-any-image-editor"><a href="https://arxiv.org/abs/2512.05965v1">EditThinker: Unlocking Iterative Reasoning for Any Image Editor</a></h2>
<p><strong>Authors:</strong> Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于图像编辑的论文“EditThinker: Unlocking Iterative Reasoning for Any Image Editor”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> EditThinker: Unlocking Iterative Reasoning for Any Image Editor
<strong>作者：</strong> Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文主要关注<strong>指令驱动的图像编辑任务中指令遵循能力不足</strong>的问题。尽管现有的图像编辑模型（通常基于强大的图像生成基础模型）能够生成高质量的编辑结果，但它们在精确理解和执行用户指令方面仍然存在局限性。这种局限性主要源于<strong>单轮编辑的固有随机性和缺乏深思熟虑的迭代过程</strong>，导致模型难以纠正中间错误，从而产生不完整的编辑或遗漏关键细节。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
该论文的核心创新在于提出了一个名为 <strong>EditThinker</strong> 的<strong>“边思考边编辑”（Think-while-Edit）框架</strong>。该框架模拟了人类在创作过程中的认知循环，通过<strong>迭代的“批判-精炼-重复”（Critique-Refine-Repeat）循环</strong>来提升编辑的指令遵循能力。</p>
<ul>
<li><strong>核心机制：</strong> EditThinker 是一个<strong>多模态大型语言模型（MLLM）</strong>，它充当整个框架的<strong>推理引擎</strong>。在每个编辑迭代中，EditThinker 不仅执行编辑任务，还<strong>同时进行批判（评估当前编辑结果）、精炼（根据评估结果改进指令）和重复（将精炼后的指令提交给图像编辑器）</strong>。</li>
<li><strong>双重角色模型：</strong> EditThinker 被设计成一个<strong>双重角色模型</strong>，能够同时进行评估和规划，而不是采用分离的评估器和重写器。</li>
<li><strong>训练策略：</strong><ul>
<li><strong>监督微调（SFT）：</strong> 首先使用专家（如 GPT-4.1）演示数据对 EditThinker 进行微调，使其学习输出格式、基本推理和批判精炼指令的原则。</li>
<li><strong>强化学习（RL）：</strong> 接着通过强化学习来弥合 EditThinker 的推理能力与实际图像编辑模型之间的差距，使其推理过程与编辑模型的实际能力和失败模式对齐。这通过一个精心设计的奖励函数实现，该函数包含格式奖励、批判奖励（衡量自我评估的准确性）和编辑奖励（衡量指令改进带来的实际效果）。</li>
</ul>
</li>
<li><strong>数据集构建：</strong> 论文还构建了一个大规模的多轮指令精炼数据集 <strong>THINKEDIT-140k</strong>，该数据集通过自动化的流程生成，包含高质量的源图像、多样化的编辑请求以及详细的推理轨迹。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该方法在四个广泛使用的图像编辑基准测试（ImgEdit-Bench, GEdit-Bench-EN, RISE-Bench, Kris-Bench）上进行了评估。</p>
<ul>
<li><strong>显著性能提升：</strong> EditThinker 框架能够<strong>显著提升现有图像编辑模型（如 FLUX.1 Kontext, OmniGen2, Qwen-Image-Edit）的指令遵循能力</strong>。在多个基准测试中，EditThinker 带来的性能提升幅度很大，尤其是在需要复杂推理的任务上。</li>
<li><strong>通用性和可扩展性：</strong> 实验表明，EditThinker 框架具有<strong>通用性</strong>，可以与不同的图像编辑模型结合使用，并且其性能<strong>可扩展</strong>，与作为“思考者”的专家模型的强大程度成正比。</li>
<li><strong>迭代推理的重要性：</strong> 论文强调了<strong>多轮迭代推理</strong>的重要性，证明了通过逐步批判和精炼指令，可以有效地解决单轮编辑的局限性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>计算成本：</strong> 论文中提到，EditThinker 的训练过程需要大量的计算资源（例如，8 H800 GPU，约 48 小时）。虽然推理时可以与现有模型结合，但多轮迭代本身也会增加推理时间。
*   <strong>专家模型的依赖性：</strong> 框架的性能在一定程度上依赖于作为“思考者”的专家模型的质量。虽然论文展示了使用 GPT-4.1 等强大模型的效果，但使用能力较弱的专家模型可能会限制性能提升的幅度。
*   <strong>多模态输入限制：</strong> 在 Kris-Bench 的评估中，论文提到由于其方法目前不支持多模态输入，因此排除了“Temporal”子集以确保公平比较。</p>
<p><strong>5. 未来研究方向：</strong>
虽然论文没有明确列出未来研究方向，但可以推断出以下潜在的拓展：</p>
<ul>
<li><strong>更高效的训练和推理：</strong> 探索更高效的训练策略和模型架构，以降低计算成本，并加速推理过程。</li>
<li><strong>更强大的专家模型：</strong> 研究如何训练或利用更强大的、专门为图像编辑推理设计的 MLLM 作为“思考者”。</li>
<li><strong>更广泛的应用场景：</strong> 将 Think-while-Edit 框架扩展到其他需要迭代推理和精炼的任务，例如视频编辑、3D 内容生成等。</li>
<li><strong>用户交互的集成：</strong> 探索更精细的用户交互机制，允许用户在迭代过程中提供更直接的反馈，进一步优化编辑结果。</li>
<li><strong>自动化数据集的扩展：</strong> 进一步扩大 THINKEDIT 数据集的规模和多样性，覆盖更广泛的编辑场景和挑战。</li>
</ul>
<p><strong>总结：</strong>
该论文提出了一种新颖的“边思考边编辑”框架 EditThinker，通过引入一个多模态大型语言模型作为推理引擎，实现了图像编辑过程的迭代批判和精炼。该方法有效解决了现有图像编辑模型在指令遵循方面的不足，并在多个基准测试中取得了显著的性能提升。EditThinker 的贡献在于将图像编辑从一个单次执行的任务转变为一个需要深思熟虑的迭代推理过程，为未来更智能、更可靠的图像编辑系统奠定了基础。论文的发布也伴随着数据集和代码的公开，有望推动该领域的研究进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory.</li>
<li>Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05965v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05965v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05959v1'></a></p>
<h2 id="m4-rag-a-massive-scale-multilingual-multi-cultural-multimodal-rag"><a href="https://arxiv.org/abs/2512.05959v1">M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</a></h2>
<p><strong>Authors:</strong> David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</p>
<p><strong>作者：</strong> David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题与背景：</strong></p>
<p>当前主流的视觉语言模型（VLMs）在视觉问答（VQA）任务上表现出色，但其能力受限于静态的训练数据，难以处理时效性强、文化背景多样或多语言的信息。检索增强生成（RAG）技术通过引入外部知识来弥补这一不足，但多语言、多模态的RAG（multilingual multimodal RAG）领域仍有待深入探索。现实世界中的信息获取天然是多语言、多模态的，因此，评估和提升模型在这一复杂场景下的能力至关重要。</p>
<p><strong>2. 关键创新与方法论贡献：</strong></p>
<ul>
<li><strong>M4-RAG 基准的构建：</strong> 作者提出了 M4-RAG，一个大规模的多语言、多文化、多模态 RAG 评估框架。该框架涵盖了 <strong>42 种语言</strong>和 <strong>56 个地区方言及语域</strong>，并包含超过 <strong>80,000 对</strong>文化多样化的图像-问题对。</li>
<li><strong>真实世界的检索环境模拟：</strong> 为了平衡真实性和可复现性，研究人员构建了一个受控的检索环境，其中包含数百万条精心策划的多语言文档，这些文档与查询领域相关，旨在模拟真实世界的检索条件。</li>
<li><strong>多模态检索的探索：</strong> M4-RAG 评估了多种检索策略，包括纯文本检索、基于多模态嵌入（mmE5 和 B3）的检索，以及结合图像和文本信息的检索。</li>
<li><strong>系统性的模型评估：</strong> 研究人员对多种主流的开源多语言 VLM 系列（如 Qwen2.5-VL, Gemma 3, Qwen3-VL, Pangea）进行了广泛的实验，分析了模型规模、检索模式、语言和提示语对 VQA 性能的影响。</li>
</ul>
<p><strong>3. 主要研究结果与意义：</strong></p>
<ul>
<li><strong>RAG 对小型模型的益处：</strong> 研究发现，RAG 技术能够显著提升小型 VLM 的性能，使其在某些情况下能够媲美甚至超越更大的非 RAG 模型。这表明对于参数量有限的模型，外部知识的引入尤为重要。</li>
<li><strong>RAG 对大型模型的局限性：</strong> 令人意外的是，研究表明 RAG 技术对大型模型（尤其是参数量超过 30B 的模型）的益处并不显著，甚至可能导致性能下降。这揭示了模型规模与当前检索有效性之间存在不匹配，大型模型可能过度依赖其内部知识，对外部不完美信息表现出较低的整合能力。</li>
<li><strong>多语言提示与上下文的挑战：</strong> 实验结果显示，尽管 VLM 能够生成多语言响应，但它们在处理非英语指令和上下文时表现出明显的英语偏见。即使是文化相关的查询，使用非英语指令或上下文也常常导致性能下降，尤其是在低资源语言上。这表明当前的指令微调方法在真正实现跨文化、跨语言的无缝推理方面仍有不足。</li>
<li><strong>检索质量的重要性：</strong> 研究强调了检索到的上下文的质量对 RAG 系统成功至关重要。高质量的检索能够显著提高正确率和纠错率，而低质量或不相关的检索则可能误导模型。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>大型模型对检索的敏感度：</strong> 大型模型在面对不完美检索时，其性能下降幅度可能更大，这表明它们对检索质量的依赖性可能与小型模型不同。</li>
<li><strong>英语中心主义的偏见：</strong> 尽管模型支持多语言，但在指令和上下文的语言处理上，仍然存在明显的英语偏见，这限制了其在跨文化场景下的泛化能力。</li>
<li><strong>检索系统与模型规模的匹配问题：</strong> 当前的检索系统可能无法为大型模型提供足够高质量或足够相关的上下文，导致其优势无法充分发挥。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>下一代 RAG 系统的发展：</strong> M4-RAG 为开发能够无缝跨越语言、模态和文化背景进行推理的下一代 RAG 系统奠定了基础。</li>
<li><strong>提升多语言、多模态的整合能力：</strong> 需要进一步研究如何更好地整合跨语言检索和多模态表示，以及如何让模型更有效地利用非英语的上下文信息。</li>
<li><strong>解决大型模型与检索的失配问题：</strong> 需要探索新的方法来提高大型模型对外部知识的整合能力，或者开发更适合大型模型的检索策略。</li>
<li><strong>克服英语中心主义：</strong> 需要研究更有效的指令微调和上下文处理方法，以减少模型对英语的依赖，实现真正的语言无关性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>M4-RAG 是一个重要的贡献，它不仅提供了一个大规模、多维度（语言、文化、模态）的评估平台，还通过系统性的实验揭示了当前多语言多模态 RAG 领域面临的关键挑战，特别是大型模型在 RAG 中的表现以及模型在跨语言和跨文化场景下的局限性。这项工作为未来更强大、更具文化适应性的视觉语言模型的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05959v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05959v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05955v1'></a></p>
<h2 id="simpact-simulation-enabled-action-planning-using-vision-language-models"><a href="https://arxiv.org/abs/2512.05955v1">SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Haowen Liu, Shaoxiong Yao, Haonan Chen, Jiawei Gao, Jiayuan Mao, Jia-Bin Huang, Yilun Du</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</p>
<p><strong>作者：</strong> Haowen Liu, Shaoxiong Yao, Haonan Chen, Jiawei Gao, Jiayuan Mao, Jia-Bin Huang, Yilun Du</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决当前视觉语言模型（VLMs）在机器人操作任务中面临的关键挑战：尽管VLMs在常识推理和语义理解方面表现出色，但它们缺乏对物理动力学的深入理解。这是因为VLMs通常在静态的互联网数据上进行训练，这些数据缺乏因果交互和动作驱动的变化信息。因此，对于需要精确物理理解、推理和动作规划的精细机器人操作任务，VLMs往往难以有效应用。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了克服这一局限性，作者提出了<strong>SIMPACT</strong>（Simulation-Enabled Action Planning）框架。其核心创新在于：</p>
<ul>
<li><strong>测试时（Test-time）的仿真驱动物理推理：</strong> SIMPACT在测试时利用物理仿真来增强VLMs的物理推理能力，而无需对VLM进行任何额外的训练。</li>
<li><strong>高效的仿真构建管道：</strong> 该框架能够从单个RGB-D图像高效地构建多物理仿真环境。它利用预训练的视觉基础模型（如分割、3D生成和姿态估计）来重建物体几何形状和姿态，并利用VLM来推断所需的物理参数（如质量、摩擦力、弹性等），从而支持刚体和可变形物体的仿真。</li>
<li><strong>仿真-闭环的动作规划：</strong> SIMPACT将仿真环境集成到VLM的推理过程中。VLM首先提出初步的动作序列，然后通过仿真进行回放（rollouts）来评估这些动作的后果。VLM会根据仿真结果迭代地优化其动作规划，从而实现物理接地（physically grounded）的推理。</li>
<li><strong>符号化动作空间与VLM的结合：</strong> 框架定义了一套紧凑的符号化动作（如PUSH, GRASP, RELEASE），并结合连续控制参数，使VLM能够更有效地生成和优化高层动作序列。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
SIMPACT在五个具有挑战性的真实世界刚体和可变形物体操作任务上取得了<strong>最先进的性能</strong>，显著优于现有的通用机器人操作模型。实验结果表明：</p>
<ul>
<li>SIMPACT能够成功执行需要精细物理推理的任务，例如防止物体倾倒的推箱子任务、堆叠碗、旋转盒子、编织绳子和塑形橡皮泥。</li>
<li>通过将物理理解嵌入到VLM的测试时推理中，SIMPACT为实现可泛化的具身智能（embodied intelligence）提供了一条有前景的路径。</li>
<li>仿真与真实世界结果之间存在高度的一致性（89%的成功/失败匹配率），这表明SIMPACT构建的物理仿真是一个高保真度的世界模型。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>仿真质量依赖性：</strong> 仿真结果的准确性在一定程度上依赖于底层图像到3D重建的质量，尤其是在处理遮挡物体时。
*   <strong>物理参数估计的潜在不准确性：</strong> VLM对物理参数的估计可能存在不准确性，从而导致仿真与现实世界动力学之间的偏差。
*   <strong>开环执行：</strong> 当前系统执行的是开环动作序列，这可能使其容易受到累积误差和干扰的影响。
*   <strong>计算成本：</strong> VLM规划阶段是计算成本最高的环节，尤其是动作采样阶段，因为需要多次查询VLM以保证多样性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提升3D重建能力：</strong> 集成更先进的图像到3D重建模型，特别是针对遮挡物体和关节物体。
*   <strong>系统辨识（System Identification）：</strong> 集成系统辨识模块，利用真实世界交互数据来优化VLM估计的物理参数，从而提高仿真保真度。
*   <strong>闭环控制：</strong> 探索使用模型预测控制（MPC）风格的闭环控制，结合VLM生成的策略，以应对动态环境中的干扰和不确定性。
*   <strong>降低计算成本：</strong> 开发更高效的VLM，或针对机器人应用进行优化，以加快规划循环的速度。</p>
<p><strong>总结：</strong>
SIMPACT框架通过将物理仿真无缝集成到VLMs的测试时推理过程中，成功弥补了现有VLMs在物理动力学理解方面的不足。它实现了从单目RGB-D输入到物理接地动作规划的端到端流程，并在复杂的机器人操作任务中展现出强大的泛化能力和鲁棒性，为构建更智能、更具适应性的机器人系统开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training.</li>
<li>Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05955v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05955v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05953v1'></a></p>
<h2 id="correspondence-oriented-imitation-learning-flexible-visuomotor-control-with-3d-conditioning"><a href="https://arxiv.org/abs/2512.05953v1">Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</a></h2>
<p><strong>Authors:</strong> Yunhao Cao, Zubin Bhaumik, Jessie Jia, Xingyi He, Kuan Fang</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning”的全面中文摘要：</p>
<p><strong>论文摘要：Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</strong></p>
<p><strong>1. 研究问题/核心挑战</strong></p>
<p>本文旨在解决在机器人抓取和操作任务中，如何设计一种灵活且通用的任务表示方法，以实现对机器人视觉-运动控制策略的有效引导。现有的方法往往在任务表示的粒度（空间和时间）上存在局限性，难以适应用户多样化的意图和任务需求。同时，将高层任务语义有效地转化为低层、精确的机器人动作，尤其是在处理不确定性和复杂场景时，仍然是一个重大的挑战。</p>
<p><strong>2. 主要创新点/方法贡献</strong></p>
<ul>
<li><strong>Correspondence-Oriented Task Representation (COIL) 任务表示：</strong> 论文提出了一种新颖的“对应关系导向”的任务表示方法。该方法将任务定义为场景中选定物体关键点的三维预期运动轨迹。与以往方法不同的是，COIL 不强制要求固定数量的关键点或均匀的时间间隔，而是支持<strong>可变的空间和时间粒度</strong>，能够适应不同复杂度和意图的任务。</li>
<li><strong>Spatio-Temporal Attention Policy：</strong> 为了鲁棒地将这种对应关系表示转化为可执行动作，论文设计了一个条件策略，该策略采用<strong>时空注意力机制</strong>。该机制能够有效地融合来自多模态输入的信息（如点云、追踪的关键点和任务表示），跨越空间和时间维度进行推理，从而实现对任务的精确理解和执行。</li>
<li><strong>Scalable Self-Supervised Pipeline：</strong> 策略通过一个可扩展的<strong>自监督学习流程</strong>进行训练。该流程在模拟环境中收集大量演示数据，并通过<strong>事后（hindsight）对应关系估计</strong>自动生成任务标签，从而实现了完全的自监督训练，大大降低了数据标注的成本。</li>
<li><strong>3D 关键点追踪与融合：</strong> 论文详细阐述了如何结合 2D 在线追踪算法和深度估计，实现对三维关键点的鲁棒追踪。这些追踪到的关键点信息与任务表示和视觉观察一起，被输入到时空注意力编码器中进行融合。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>卓越的性能和泛化能力：</strong> COIL 在多种机器人操作任务（包括刚性和可变形物体、工具使用等）上展现出<strong>零样本（zero-shot）的强大泛化能力</strong>，能够处理未见过的物体和任务。</li>
<li><strong>对不同粒度的鲁棒性：</strong> 无论是在稀疏（如仅有起点和终点）还是密集（如详细的轨迹）的任务规范下，COIL 都能保持<strong>优异的性能</strong>，显著优于现有基线方法。</li>
<li><strong>与语言模型的结合：</strong> 论文展示了 COIL 可以与视觉语言模型（VLM）结合，直接从语言指令生成任务规范，进一步扩展了其应用范围。</li>
<li><strong>对设计组件的验证：</strong> 消融实验表明，时空注意力机制、归一化位置编码、流随机化以及关键点噪声增强等设计都对 COIL 的整体性能至关重要。</li>
</ul>
<p><strong>4. 论文中提到的局限性</strong></p>
<ul>
<li><strong>依赖精确的关键点追踪：</strong> COIL 的性能在测试时依赖于在线关键点追踪的准确性，而现有的追踪方法在遮挡或杂乱场景下可能引入噪声。</li>
<li><strong>未考虑任务表示的质量：</strong> 当前方法假设任务表示是外部提供的，并未主动推理任务意图的模糊性或表示的质量。</li>
<li><strong>缺乏多模态感知：</strong> 尽管使用 3D 视觉输入，但 COIL 目前尚未利用触觉或力觉等其他感知模态，而这些模态对于需要精细接触和顺应性的任务至关重要。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>联合推理和精炼任务表示：</strong> 开发能够主动推断和优化任务表示质量的方法，以提高自主性和鲁棒性。</li>
<li><strong>整合多模态感知：</strong> 将触觉、力觉等传感器信息融入到任务表示和控制框架中，以处理更广泛的机器人操作任务。</li>
<li><strong>改进关键点追踪：</strong> 研究更鲁棒的关键点追踪算法，以应对复杂和动态的环境。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文提出了一个名为 COIL 的创新性框架，通过引入一种灵活的 3D 对应关系导向的任务表示，并结合强大的时空注意力策略，显著提升了机器人视觉-运动控制的通用性和鲁棒性。其核心贡献在于能够处理不同粒度的任务规范，并通过自监督学习实现高效训练。COIL 在多项实验中取得了优异的零样本泛化性能，为未来更智能、更适应性强的机器人控制系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D.</li>
<li>At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05953v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05953v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05941v1'></a></p>
<h2 id="zoom-in-click-out-unlocking-and-evaluating-the-potential-of-zooming-for-gui-grounding"><a href="https://arxiv.org/abs/2512.05941v1">Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</a></h2>
<p><strong>Authors:</strong> Zhiyuan Jiang, Shenghao Xie, Wenyi Li, Wenqiang Zu, Peihang Li, Jiahao Qiu, Siqi Pei, Lei Ma, Tiejun Huang, Mengdi Wang, Shilong Liu</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding”的论文的中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文摘要：Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决图形用户界面（GUI）代理能力中的一个关键问题——<strong>GUI 基础（Grounding）</strong>。现有的方法虽然依赖大规模的边界框标注，但在跨平台泛化、复杂布局分析和精细化元素定位方面仍面临诸多挑战。论文的核心研究问题是：如何有效地利用“缩放”（zoom）这一强大的先验知识来提升 GUI 基础能力，并克服现有方法的局限性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
论文的主要贡献在于提出了一个名为 <strong>ZoomClick</strong> 的<strong>训练无关（training-free）</strong>方法，它通过系统地利用缩放的四个关键属性来解锁其全部潜力：
*   <strong>预缩放（Pre-zoom）：</strong> 在第一步通过全局预测与局部块预测的一致性来确保一个可靠的起始点，从而避免早期定位错误。
*   <strong>缩放深度（Depth）：</strong> 通过多步迭代的缩放过程，逐步缩小视野以精细化定位。
*   <strong>收缩尺寸（Shrink Size）：</strong> 在每次迭代中，以固定的收缩比例裁剪当前视野，以保持空间对齐并避免边界溢出。
*   <strong>最小裁剪尺寸（Minimal Crop Size）：</strong> 设置一个最小裁剪尺寸，以防止视野过度缩小而丢失关键的上下文信息，从而保持模型对全局布局的理解。</p>
<p>此外，论文还提出了 <strong>GUIZoom-Bench</strong>，一个专门用于评估模型缩放适应性的基准，旨在激发未来在缩放方面进一步的研究。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能提升显著：</strong> ZoomClick 方法显著提升了通用视觉语言模型（如 Qwen3-VL）和专用 GUI 基础模型（如 UI-Venus）的性能。
*   <strong>达到最先进水平（SOTA）：</strong> 在多个主流基准测试中取得了最先进的成果。例如，UI-Venus-72B 模型结合 ZoomClick 后在 ScreenSpot-Pro 上达到了 73.1% 的成功率。
*   <strong>模型规模效应减弱：</strong> 即使是较小的模型，通过 ZoomClick 也能达到甚至超越更大模型的性能，例如 UI-Venus-7B 结合 ZoomClick 后性能优于原始的 UI-Venus-72B。
*   <strong>GUIZoom-Bench 的价值：</strong> 该基准揭示了缩放在复杂布局、精细元素和分辨率不匹配场景下的未解决的挑战，为设计更鲁棒和通用的缩放方法提供了可解释的标准。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>模型固有能力限制：</strong> ZoomClick 的性能上限受限于模型自身的空间和语义先验。
*   <strong>桌面场景限制：</strong> GUIZoom-Bench 目前仅限于桌面规模的场景，未能直接推广到移动端界面或多步代理交互工作流。
*   <strong>隐私风险：</strong> GUI 交互可能暴露敏感的屏幕信息，带来隐私风险。
*   <strong>计算资源消耗：</strong> 该方法依赖大型模型，计算量较大，可能产生不可忽略的能源消耗。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>多分辨率/多尺度训练：</strong> 针对缩放引起的上下文变化，未来的模型应采用多分辨率或多尺度训练策略。
*   <strong>动态裁剪：</strong> 探索更智能的动态裁剪策略，以更好地适应不同场景和模型需求。
*   <strong>可训练的上下文引导：</strong> 将上下文信息视为可训练的引导信号，而不是简单的启发式方法，以提高鲁棒性和迭代校正能力。
*   <strong>更强的语言理解能力：</strong> 针对指令中的顺序或比较语义，需要提升模型的语言理解能力，以避免被视觉相似的干扰项误导。
*   <strong>分布适应性：</strong> 研究模型如何适应因裁剪而产生的分布偏移，以提高在非典型视觉布局或模式下的性能。</p>
<p>总而言之，这篇论文通过提出创新的 ZoomClick 方法和 GUIZoom-Bench 基准，有效地解锁了缩放技术在 GUI 基础领域的潜力，显著提升了现有模型的性能，并为未来的研究指明了方向。其训练无关的特性和对模型先验的充分利用，使其成为一个实用且高效的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro.</li>
<li>Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05941v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05941v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05937v1'></a></p>
<h2 id="measuring-the-effect-of-background-on-classification-and-feature-importance-in-deep-learning-for-av-perception"><a href="https://arxiv.org/abs/2512.05937v1">Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</a></h2>
<p><strong>Authors:</strong> Anne Sielemann, Valentin Barner, Stefan Wolf, Masoud Roschani, Jens Ziehn, Juergen Beyerer</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].   Download: synset.de/datasets/synset-signset-ger/background-effect</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“测量背景对自动驾驶感知中深度学习分类和特征重要性的影响”的论文的全面摘要，其中包含您要求的各个部分：</p>
<p><strong>论文题目：</strong> Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception (测量背景对自动驾驶感知中深度学习分类和特征重要性的影响)</p>
<p><strong>作者：</strong> Anne Sielemann, Valentin Barner, Stefan Wolf, Masoud Roschani, Jens Ziehn, Juergen Beyerer</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文旨在解决可解释人工智能（XAI）领域的一个核心挑战：如何量化和验证XAI方法（如SHAP和GradCAM）的解释是否准确反映了深度学习模型在自动驾驶感知任务中的真实学习行为，特别是关于模型是否过度拟合了背景中的虚假关联。现有XAI方法通常依赖于直观解释，但缺乏量化验证的手段，尤其是在真实世界数据中，背景与前景的关联难以人为控制和分离。因此，研究的核心问题是：背景的哪些属性（如相关性、相机变化程度、交通标志形状）会影响模型对背景的关注度，以及这种关注度如何影响分类性能，从而判断XAI的解释是否可靠。</p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<ul>
<li><strong>系统化的合成数据集生成：</strong> 论文的核心贡献在于系统性地生成了六个合成交通标志识别数据集。这些数据集通过控制背景相关性（相关/不相关）和相机变化程度（前置/中等/高）来隔离这些因素的影响。数据集基于Synset Signset Germany数据集的生成流程，并加入了GAN纹理生成，以保证一定程度的真实感。</li>
<li><strong>量化背景特征重要性：</strong> 论文利用Kernel SHAP (KS) 和 GradCAM (GC) 等XAI方法，计算了“像素比率”（pixel ratio），该指标衡量了图像中正向归因特征（即对模型预测有积极贡献的特征）在交通标志区域内的比例。这提供了一种量化模型对背景关注程度的方法。</li>
<li><strong>隔离变量的实验设计：</strong> 通过精心设计的合成数据集，论文能够独立地研究背景相关性、相机变化以及交通标志形状对模型特征重要性和分类性能的影响，这是在真实世界数据中难以实现的。</li>
<li><strong>提供可下载的数据集：</strong> 论文公开了生成的六个合成数据集，为后续XAI研究和模型评估提供了宝贵的资源。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>背景相关性的影响：</strong> 研究发现，在相关背景下训练的模型倾向于给予背景特征更高的重要性。这表明模型确实会利用背景信息进行分类，尤其是在训练和测试数据领域一致的情况下。</li>
<li><strong>相机变化的影响：</strong> 相机变化对背景关注度的影响并不显著，尤其是在不相关背景下，仅在某些特定架构和设置下观察到微弱趋势。</li>
<li><strong>交通标志形状的影响：</strong> 包含更多样化交通标志形状的数据集会增加模型对背景的关注度。这可能是因为区分不同形状的交通标志本身就需要更精细的特征提取，从而使得背景特征也可能被纳入考量。</li>
<li><strong>XAI解释的可靠性：</strong> 论文表明，背景关注度并不总是与模型性能差（即过度拟合）相关。现代的ConvNeXt等架构在相关背景下表现出更高的背景关注度，但同时也能获得更好的性能，这表明背景信息有时是“合理”的，而非“虚假”的。这挑战了“低背景关注度即健康分类器”的直观假设。</li>
<li><strong>合成数据在XAI研究中的价值：</strong> 研究证明了合成数据在量化和研究XAI方法方面的重要作用，能够提供一个可控的实验环境来验证XAI的假设。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>计算资源限制：</strong> 由于Kernel SHAP计算成本高昂，实验在测试集上仅使用了前200张图像（每个类别），并且仅在部分数据集上进行了详细的形状分析。</li>
<li><strong>特定任务和模型：</strong> 研究主要集中在交通标志识别这一特定任务上，并使用了ConvNeXt和ResNet50等几种网络架构。其结论的普适性可能需要进一步在其他任务和模型上验证。</li>
<li><strong>真实感量化：</strong> 虽然使用了GAN纹理生成，但合成数据集的真实感量化程度（与真实世界数据的差距）仍是一个需要持续关注的问题。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到更多场景和数据：</strong> 在测试数据集中引入更多“角落情况”（如雨、雪、雾、遮挡、过度曝光等），以增加数据集的难度，进一步凸显模型性能差异。</li>
<li><strong>更广泛的XAI方法和模型：</strong> 探索更多XAI方法（如LIME、Integrated Gradients等）以及更多种类的深度学习模型，以验证研究结果的普适性。</li>
<li><strong>更深入的真实感评估：</strong> 对合成数据集的真实感进行更细致的量化评估，并研究真实感对XAI解释的影响。</li>
<li><strong>跨领域研究：</strong> 将研究方法和发现推广到其他自动驾驶感知任务（如行人检测、车道线检测等）以及其他计算机视觉领域。</li>
<li><strong>背景关注度的设计考量：</strong> 进一步研究如何设计模型和训练策略，以在需要时有效利用背景信息，同时避免虚假关联。</li>
</ul>
<p>总而言之，这篇论文通过创新的合成数据生成和量化评估方法，深入剖析了背景信息对深度学习模型在自动驾驶感知任务中的分类性能和XAI解释的影响。其研究结果不仅挑战了关于XAI解释的一些直观假设，也为未来更可靠、更深入地理解和应用XAI方法提供了重要的理论和实践指导。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>[...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].</li>
<li>Download: synset.de/datasets/synset-signset-ger/background-effect</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05937v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05937v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05936v1'></a></p>
<h2 id="synset-signset-germany-a-synthetic-dataset-for-german-traffic-sign-recognition"><a href="https://arxiv.org/abs/2512.05936v1">Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition</a></h2>
<p><strong>Authors:</strong> Anne Sielemann, Lena Loercher, Max-Lion Schumacher, Stefan Wolf, Masoud Roschani, Jens Ziehn</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面中文摘要。</p>
<p><strong>论文题目：</strong> Synset Signset Germany: A Synthetic Dataset for German Traffic Sign Recognition</p>
<p><strong>作者：</strong> Anne Sielemann, Lena Loercher, Max-Lion Schumacher, Stefan Wolf, Masoud Roschani, Jens Ziehn, Juergen Beyerer</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>随着机器学习（ML）和人工智能（AI）的飞速发展，尤其是在深度学习领域，研究重点已从单纯追求模型性能转向关注性能的实现方式。这包括确保训练数据的质量、时效性和成本效益，以及理解系统在真实世界中的行为，即可解释性AI（XAI）和鲁棒性。在交通标志识别任务中，真实世界数据的获取成本高昂，且难以覆盖所有罕见或危险场景。因此，如何生成高质量、多样化且逼真的合成数据来弥补真实数据的不足，并用于模型训练和评估，是该研究的核心问题。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>该论文提出了一种新颖的合成流水线和数据集——<strong>Synset Signset Germany</strong>，用于交通标志识别任务。其核心创新在于结合了<strong>数据驱动</strong>和<strong>分析建模</strong>的优势：</p>
<ul>
<li><strong>GAN-based 纹理生成：</strong> 利用生成对抗网络（GAN），特别是Pix2Pix模型，生成数据驱动的污垢、磨损和褪色等真实交通标志表面瑕疵，增加了纹理的独特性和逼真度。</li>
<li><strong>分析性场景调制：</strong> 通过物理上正确的照明和详细的参数化，实现场景的精确控制。这包括对交通标志杆、相机姿态、环境光照（IBL）以及遮挡物（如树木）的模拟。</li>
<li><strong>多样的成像伪影模拟：</strong> 模拟了自动曝光控制（AEC）、白平衡（WB）、点扩散函数（PSF）、镜头光晕、运动模糊、色差、锐化以及Bayer BGGR去马赛克等多种数字成像伪影，以更全面地模拟真实世界的成像条件。</li>
<li><strong>详细的元数据：</strong> 为每张图像提供了丰富的元数据，包括随机选择的环境和成像效果参数，这对于XAI和鲁棒性分析至关重要。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>数据集规模与多样性：</strong> Synset Signset Germany 包含 <strong>105,500 张图像</strong>，涵盖了 <strong>211 种不同的德国交通标志类别</strong>，其中包括2020年发布的新型罕见交通标志。数据集在类别数量和多样性上均处于领先地位。</li>
<li><strong>训练与评估效果：</strong> 在与真实世界数据集 GTSRB 的比较评估中，使用 Synset Signset Germany 训练的模型在 GTSRB 上取得了 <strong>超过80%的Top-1准确率</strong>，与在真实数据上训练的模型得分接近，仅相差1.2个百分点。这表明该合成数据集具有很高的训练有效性。</li>
<li><strong>XAI与鲁棒性分析：</strong> 该数据集的详细参数化和成像伪影模拟能力，使其非常适合用于XAI和鲁棒性分析。通过引入特定参数的扰动，可以量化评估模型的解释质量和鲁棒性。实验表明，随着运动模糊等扰动强度的增加，模型的预测性能和解释质量都会下降，这为研究模型在不同条件下的行为提供了依据。</li>
<li><strong>与现有方法的比较：</strong> 与纯分析性生成的 CATERED 数据集相比，Synset Signset Germany 在跨数据集评估中表现出约20%的更高Top-1准确率，这归功于其更逼真的GAN纹理生成，从而降低了“sim-to-real”的差距。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>GAN纹理的局限性：</strong> 目前的GAN纹理生成方法仅使用了简单的概念，未能区分不同类型的污垢和损坏，也缺乏对灰色区域和反光材料的表示。</li>
<li><strong>遮挡物数量有限：</strong> 为了实现更复杂和多样的阴影投射和遮挡效果，需要增加遮挡物的数量。</li>
<li><strong>环境地图多样性：</strong> 为了获得更高的图像方差，需要收集更多的环境地图。</li>
<li><strong>模型选择有限：</strong> 本文的实验主要集中在少数深度学习模型上，更广泛的模型分析（包括经典模型）可以进一步验证数据集的适用性。</li>
<li><strong>Sim-to-Real 差距：</strong> 尽管实验表明该数据集在实际应用中具有较低的“sim-to-real”差距，但作为测试数据时，对逼真度的要求更高，仍有改进空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到国际交通标志：</strong> 将当前仅限于德国交通标志的限制扩展到国际交通标志。</li>
<li><strong>改进GAN纹理生成：</strong> 提升GAN模型对污垢、损坏、灰色区域和反光材料的表示能力。</li>
<li><strong>增加遮挡物和环境多样性：</strong> 引入更多类型的遮挡物，并使用更丰富的环境地图来提高场景的复杂性和多样性。</li>
<li><strong>更广泛的模型评估：</strong> 对更多类型的模型（包括经典ML模型）进行评估，以更全面地理解数据集的适用性。</li>
<li><strong>进一步缩小Sim-to-Real差距：</strong> 通过持续改进合成方法和评估策略，进一步降低合成数据与真实数据之间的差距。</li>
<li><strong>XAI和鲁棒性分析的深化：</strong> 利用数据集提供的详细参数，深入研究模型在各种扰动下的行为，并探索如何根据特定鲁棒性和解释性需求来调整合成参数。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Synset Signset Germany 是一个大规模、高多样性的德国交通标志合成数据集，它通过结合数据驱动的GAN纹理生成和分析性的场景模拟，显著提高了合成数据的逼真度。该数据集不仅为交通标志识别任务提供了丰富的训练和测试数据，尤其是在罕见类别方面，而且其详细的元数据和可控的参数化使其成为研究模型XAI和鲁棒性的宝贵工具。该研究为合成数据在计算机视觉领域的应用树立了新的标杆，并为未来的相关研究提供了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization.</li>
<li>In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments.</li>
<li>Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs.</li>
<li>We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05936v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05936v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05932v1'></a></p>
<h2 id="physically-based-simulation-of-automotive-lidar"><a href="https://arxiv.org/abs/2512.05932v1">Physically-Based Simulation of Automotive LiDAR</a></h2>
<p><strong>Authors:</strong> L. Dudzik, M. Roschani, A. Sielemann, K. Trampert, J. Ziehn, J. Beyerer, C. Neumann</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.   Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.   Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01° resolution, which marks the best available resolution for measuring the beam pattern.   The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Physically-Based Simulation of Automotive LiDAR”的全面中文摘要，重点关注其研究问题、方法创新、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Physically-Based Simulation of Automotive LiDAR (汽车 LiDAR 的物理基础模拟)</p>
<p><strong>研究问题/核心挑战：</strong>
随着自动驾驶技术的发展，对用于训练和验证安全关键系统（如自动驾驶汽车）的真实感数据需求日益增长。然而，真实世界数据的采集、标注成本高昂且耗时。因此，合成数据（主要通过模拟生成）成为一种有吸引力的替代方案。然而，为了确保合成数据能够充分替代真实数据，必须证明合成数据与真实世界数据之间的“领域差距”足够小且理解充分。尽管在基于图像的模拟方面取得了进展，但针对 LiDAR（激光雷达）数据的物理基础模拟，特别是考虑其光学现象（如“光晕效应”或“blooming”）和精确的传感器特性，仍然是一个挑战。</p>
<p><strong>关键创新/方法贡献：</strong>
1.  <strong>物理基础渲染 (PBR) 模型：</strong> 论文提出了一种基于物理基础渲染（PBR）的分析模型，用于模拟汽车 LiDAR 的时间飞行（ToF）数据。该模型在近红外（NIR）领域工作，并考虑了单次反射和后向反射。
2.  <strong>集成光学现象：</strong> 模型的核心创新在于系统地集成了 LiDAR 模拟中的关键光学现象，特别是：
    *   <strong>光晕效应 (Blooming)：</strong> 模拟了由于 LiDAR 光束边缘的散射导致目标轮廓看起来比实际更宽的现象，这是真实 LiDAR 数据中一个重要的伪影。
    *   <strong>回波脉冲宽度 (Echo Pulse Width)：</strong> 考虑了发射脉冲的非零长度以及其与接收信号强度的关系，这影响了回波的测量。
    *   <strong>环境光 (Ambient Light)：</strong> 将来自非相关光源（如阳光）的杂散光纳入模型，以模拟真实世界中的噪声背景。
3.  <strong>系统化的参数提取：</strong> 论文提出了一种通过光学实验室测量来系统地确定模型参数的方法。这包括使用测角仪（goniometer）在 0.01° 的高分辨率下测量目标表面的光度亮度，以精确获取 LiDAR 光束的扩散模式和接收二极管的灵敏度。
4.  <strong>灵活的计算方法：</strong> 模型提供了多种计算方法（均非实时），允许根据系统特性、计算能力和期望的输出属性进行选择，例如基于光束迭代或基于范围堆叠的算法。
5.  <strong>模型参数化：</strong> 模型参数涵盖了系统特定属性（如光束扩散、接收器灵敏度、发射强度、反射强度与回波脉冲宽度的转换）以及场景参数（如环境照明、目标位置和表面属性）。</p>
<p><strong>主要结果及其意义：</strong>
*   <strong>成功校准和验证：</strong> 该方法成功地应用于两种不同的汽车 LiDAR 系统（Valeo Scala Gen. 2 和 Blickfeld Cube 1），尽管它们在特性和接口方面存在显著差异。这表明该模型具有通用性。
*   <strong>重现真实世界伪影：</strong> 模拟结果能够重现真实 LiDAR 数据中的关键伪影，特别是光晕效应和由后向反射材料引起的形状失真。这对于理解和弥合领域差距至关重要。
*   <strong>提供高质量合成数据：</strong> 该模型能够生成具有物理准确性的 LiDAR 数据，包括复杂的伪影，为训练和测试自动驾驶 AI/ML 应用提供了高质量的合成数据集。
*   <strong>加速研究和开发：</strong> 通过提供一个可控且可重复的模拟环境，该模型可以加速 LiDAR 系统的开发、测试和验证过程，降低对真实世界数据的依赖。</p>
<p><strong>论文中提到的局限性：</strong>
*   <strong>计算成本高昂：</strong> 尽管进行了优化，但该模型在计算上仍然非常耗时，通常需要几秒钟才能生成一帧数据，这使得实时应用（如硬件在环测试）具有挑战性。
*   <strong>单次反射假设：</strong> 模型主要基于单次反射的假设，虽然在图 7 中展示了其对结果强度的影响相对较小，但在某些复杂场景下（如多次反射或折射），可能需要更全面的模型。
*   <strong>世界几何和 BRDF 的近似：</strong> 在实际应用中，世界几何和表面 BRDF 的参数通常是近似的，这会影响模拟的定量准确性。
*   <strong>传感器特性细节：</strong> 对于像 Scala 2 这样具有复杂二极管特性的传感器，其所有细节（如不同二极管的强度差异）并未完全建模，需要额外的实验室测量。
*   <strong>未来工作：</strong> 论文明确指出，需要对更多 LiDAR 系统进行更全面的演示和评估，并需要更详细地推导世界对象几何和表面参数。此外，对环境光接收和 AD 阈值与环境噪声之间关系的进一步研究也留待未来。</p>
<p><strong>潜在的未来研究方向：</strong>
*   <strong>实时性优化：</strong> 探索更进一步的算法优化或硬件加速，以实现实时或近实时的模拟，从而支持硬件在环（HiL）测试。
*   <strong>多重散射和折射：</strong> 扩展模型以包含多重散射和折射效应，以处理更复杂的场景和材料交互。
*   <strong>更精细的传感器模型：</strong> 开发更精细的传感器模型，以捕捉更复杂的传感器特性，如不同发射二极管的差异。
*   <strong>自动参数提取：</strong> 研究从真实 LiDAR 数据中自动提取模型参数的方法，以减少对实验室测量的依赖。
*   <strong>领域差距量化：</strong> 进一步量化模拟数据与真实数据之间的领域差距，并开发更有效的领域适应技术。
*   <strong>集成到更广泛的仿真平台：</strong> 将该模型集成到更广泛的自动驾驶仿真平台中，与其他传感器模型（如相机、雷达）协同工作。</p>
<p>总而言之，这篇论文在汽车 LiDAR 模拟领域做出了重要贡献，通过引入物理基础渲染和系统化的光学现象建模，显著提高了模拟数据的真实性，并为解决自动驾驶领域的数据挑战提供了有力的工具。其对光晕效应等关键伪影的精确模拟，以及通过实验室测量进行参数校准的方法，是其核心创新点。尽管存在计算成本和模型简化方面的局限性，但该研究为未来更逼真、更高效的 LiDAR 模拟奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05932v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05932v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05927v1'></a></p>
<h2 id="world-models-that-know-when-they-dont-know-controllable-video-generation-with-calibrated-uncertainty"><a href="https://arxiv.org/abs/2512.05927v1">World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</a></h2>
<p><strong>Authors:</strong> Zhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, Anirudha Majumdar</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文的摘要进行了深入分析。以下是我的评估：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>这篇论文提出了一种名为 C3 的新颖不确定性量化 (UQ) 方法，用于训练可控视频生成模型。C3 能够为生成的视频帧提供密集、校准过的子像素级别置信度估计，从而精确地定位模型的不确定性区域。这项工作解决了当前可控视频模型容易产生幻觉但缺乏表达置信度能力的问题，为机器人学习和规划等关键应用提供了更可靠的视频生成。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>C3 的核心创新在于其多方面的 UQ 方法：</p>
<ul>
<li><strong>基于严格正确评分规则的训练框架：</strong> 论文引入了一种新颖的训练框架，利用严格正确评分规则（strictly proper scoring rules）来同时优化视频模型的正确性和校准性。这意味着模型不仅要生成准确的视频，还要能够准确地表达其对生成结果的信心。</li>
<li><strong>潜空间不确定性估计：</strong> 为了避免像素空间 UQ 方法带来的训练不稳定性和高昂计算成本，C3 选择在潜空间（latent space）中估计不确定性。这是一种更高效且更稳定的方法。</li>
<li><strong>潜空间到像素空间的映射：</strong> C3 将潜空间中估计出的密集不确定性映射回可解释的 RGB 像素空间。这使得研究人员能够直观地可视化不确定性，生成高分辨率的不确定性热图，从而精确识别生成视频中不可信的区域。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>这项研究对可控视频生成领域具有重要的潜在影响：</p>
<ul>
<li><strong>提升模型可靠性：</strong> 通过提供可信度度量，C3 能够显著提升可控视频生成模型的可靠性，尤其是在对准确性要求极高的应用场景中。</li>
<li><strong>促进安全关键型应用：</strong> 对于机器人策略评估、规划以及需要精确物理交互的任务，能够识别和量化不确定性是至关重要的。C3 的方法为这些安全关键型应用铺平了道路。</li>
<li><strong>推动更鲁棒的视频模型：</strong> 能够识别模型何时“不知道”，意味着可以设计更智能的后处理机制，例如在不确定性高的区域进行人工干预或使用更保守的策略，从而构建更鲁棒的视频生成系统。</li>
<li><strong>促进模型可解释性：</strong> 高分辨率的不确定性热图提供了直观的可视化，增强了模型的可解释性，有助于理解模型在生成过程中哪些部分存在问题。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学：</strong><ul>
<li><strong>机器人策略评估与规划：</strong> 机器人需要准确预测未来状态以进行安全有效的规划。C3 的 UQ 能力可以帮助机器人识别其预测的不可靠区域，从而避免危险的行动。</li>
<li><strong>机器人模拟与训练：</strong> 在模拟环境中生成逼真的视频用于机器人训练时，能够量化模拟的真实性至关重要。</li>
<li><strong>指令驱动的机器人操作：</strong> 当机器人根据文本或动作指令执行任务时，C3 可以帮助识别指令执行过程中可能出现偏差的环节。</li>
</ul>
</li>
<li><strong>自动驾驶：</strong> 预测其他车辆或行人的未来轨迹时，量化不确定性对于安全决策至关重要。</li>
<li><strong>虚拟现实/增强现实：</strong> 在生成沉浸式体验时，确保生成内容的物理合理性非常重要。</li>
<li><strong>内容创作与编辑：</strong> 在进行视频编辑或生成时，用户可以根据不确定性热图来判断哪些区域需要人工修正。</li>
<li><strong>医学影像分析：</strong> 在生成医学影像序列时，量化模型的不确定性有助于医生做出更准确的诊断。</li>
<li><strong>科学模拟：</strong> 在模拟复杂物理过程时，量化模型预测的不确定性有助于评估模拟结果的可靠性。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要强调了 C3 的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算成本：</strong> 尽管作者声称在潜空间估计不确定性避免了像素空间的高昂成本，但训练一个能够进行密集 UQ 的视频模型，尤其是在大规模数据集上，仍然可能需要相当大的计算资源。</li>
<li><strong>校准的范围：</strong> 摘要提到“在训练分布内”提供了校准的不确定性估计，并能进行“有效的分布外检测”。这暗示了模型在<strong>完全未见过</strong>的、与训练数据分布差异巨大的情况下的不确定性估计的有效性可能仍然是一个研究方向，或者其性能会下降。</li>
<li><strong>“严格正确评分规则”的实现细节：</strong> 摘要提到了使用这些规则，但具体的规则选择、实现方式以及它们对模型性能的具体影响，在摘要中并未详述，这可能需要阅读全文来深入理解。</li>
<li><strong>“子像素级别”的定义和精度：</strong> 摘要提到了“子像素级别”的置信度估计，这听起来非常精细。但实际的“子像素级别”的精度和粒度，以及它在多大程度上能够真正捕捉到细微的物理不一致性，需要通过实验结果来验证。</li>
<li><strong>对“幻觉”的定义和缓解程度：</strong> 摘要指出模型“经常产生幻觉”，而 C3 旨在“缓解幻觉”。然而，C3 是否能完全消除幻觉，或者只是将其减少到可接受的水平，以及它对不同类型的幻觉（例如物理不一致、语义错误等）的缓解效果如何，摘要并未明确说明。</li>
</ul>
<p>总而言之，这篇论文提出的 C3 方法在可控视频生成领域是一个非常有前景的进展，它通过引入校准的不确定性量化，为提高模型的可靠性和安全性开辟了新的途径。其在潜空间进行 UQ 并映射到像素空间的策略，以及利用严格正确评分规则进行训练，是其关键的创新点。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation.</li>
<li>To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame.</li>
<li>First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules.</li>
<li>Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05927v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05927v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05905v1'></a></p>
<h2 id="scail-towards-studio-grade-character-animation-via-in-context-learning-of-3d-consistent-pose-representations"><a href="https://arxiv.org/abs/2512.05905v1">SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</a></h2>
<p><strong>Authors:</strong> Wenhao Yan, Sheng Ye, Zhuoyi Yang, Jiayan Teng, ZhenHui Dong, Kairui Wen, Xiaotao Gu, Yong-Jin Liu, Jie Tang</p>
<p><strong>Published:</strong> 2025-12-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \textbf{SCAIL} (\textbf{S}tudio-grade \textbf{C}haracter \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</p>
<p><strong>作者：</strong> Wenhao Yan, Sheng Ye, Zhuoyi Yang, Jiayan Teng, ZhenHui Dong, Kairui Wen, Xiaotao Gu, Yong-Jin Liu, Jie Tang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前角色动画技术在达到<strong>影院级制作标准</strong>方面面临的挑战。现有方法虽然能够将驱动视频的动作迁移到参考图像上，但在复杂场景下，如大幅度动作变化、风格化角色、多角色交互以及跨身份动画时，往往难以保持<strong>结构保真度</strong>和<strong>时间一致性</strong>，导致生成视频出现外观扭曲、不自然的姿态和错误的遮挡等问题。这表明现有的骨骼姿态表示和生成模型在捕捉3D动作结构、角色间空间关系和时间关联性方面存在不足。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
SCAIL（Studio-grade Character Animation via In-context Learning）框架提出了两项关键创新来解决上述挑战：</p>
<ul>
<li><strong>新颖的3D姿态表示：</strong> 论文提出了一种新的3D姿态表示方法，它结合了2D骨骼的关键点信息和骨骼的3D拓扑结构，并将骨骼表示为<strong>圆柱体</strong>。这种表示能够更鲁棒、更灵活地捕捉动作信号，并能保留2D投影中丢失的遮挡和深度信息，同时避免了SMPL模型固有的身份泄露问题。该表示通过<strong>光栅化</strong>生成2D运动引导信号，并结合姿态增强和重定向策略，实现了跨不同角色和场景的无缝迁移。</li>
<li><strong>全上下文姿态注入机制：</strong> 论文引入了一种<strong>全上下文姿态注入</strong>机制，该机制集成在<strong>Diffusion Transformer (DiT)</strong> 架构中。与传统的逐帧通道拼接方式不同，该机制允许模型在生成每一帧时都能<strong>关注整个姿态序列</strong>，从而实现有效的<strong>时空推理</strong>，更好地捕捉高层运动语义和时间上下文。为了应对序列长度增加的问题，论文还采用了<strong>空间下采样</strong>策略。此外，为了解决参考图像和驱动姿态之间的不匹配问题，引入了<strong>Pose-Shifted RoPE</strong>（旋转位置编码）机制，以增强模型在增强后的全序列姿态表示中检索驱动信号的能力。</li>
</ul>
<p>为了达到影院级生成质量，论文还开发了一个<strong>精选的数据管道</strong>，确保数据的多样性和高质量，并建立了一个<strong>全面的Studio-Bench基准</strong>用于系统性评估。</p>
<p><strong>3. 主要结果与意义：</strong>
实验结果表明，SCAIL在Studio-Bench基准上取得了<strong>最先进的性能</strong>，显著优于现有的基线方法。
*   在<strong>自驱动动画</strong>方面，SCAIL在PSNR、SSIM、LPIPS和FVD等指标上均表现出色。
*   在<strong>跨驱动动画</strong>方面，SCAIL在运动准确性（Mot-Acc）、运动学一致性（Kin-Consis）、物理一致性（Phy-Consis）和身份相似性（ID-Sim）等关键指标上取得了显著提升，证明了其在处理复杂场景、跨身份和跨域动画方面的强大能力。
*   定性结果也展示了SCAIL在生成结构稳定、动作流畅、姿态自然且能保持身份一致性的高质量动画方面的优势，尤其是在处理复杂交互和非标准角色时。</p>
<p>SCAIL的成功标志着角色动画技术向<strong>影院级可靠性和真实感</strong>迈出了重要一步，为电影制作等领域提供了更强大、更易用的工具。</p>
<p><strong>4. 提及的局限性：</strong>
论文中提到了一些局限性：
*   <strong>多人物姿态估计的精度：</strong> 尽管论文采用了相对有效的多人姿态提取管道，但与单人场景相比，多人物姿态估计的精度仍有待提高。
*   <strong>面部表情控制的精细度：</strong> 目前SCAIL主要依赖面部关键点进行面部控制，这种表示在精细的面部表情方面存在局限性。
*   <strong>潜在的滥用风险：</strong> 论文也承认，随着动画技术达到更高的真实感和表现力，其被用于生成误导性或有害数字内容的潜在风险也随之增加。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文指出了以下未来研究方向：
*   <strong>提升多人物姿态估计的精度：</strong> 期待该领域的研究进展，以进一步提高运动复制的保真度。
*   <strong>增强面部表情控制：</strong> 未来工作将专注于提高精细面部细节（如手部和面部表情）的准确性和保真度，以进一步提升模型的整体质量。
*   <strong>探索更精细的控制：</strong> 尽管论文主要关注解决运动不稳定性问题，但未来可以探索更精细的控制，例如更精细的手部和面部表情控制。
*   <strong>应对滥用风险：</strong> 尽管论文选择开源模型以促进透明度和社区发展，但未来需要考虑如何应对潜在的滥用风险。</p>
<p>总而言之，SCAIL框架通过创新的3D姿态表示和全上下文姿态注入机制，显著提升了角色动画的质量和鲁棒性，为实现影院级动画制作奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present \textbf{SCAIL} (\textbf{S}tudio-grade \textbf{C}haracter \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning), a framework designed to address these challenges from two key innovations.</li>
<li>First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal.</li>
<li>Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences.</li>
<li>To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation.</li>
<li>Experiments show that \textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05905v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05905v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-08 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
