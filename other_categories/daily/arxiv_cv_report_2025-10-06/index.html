<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-06 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-03/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-07/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-06">Arxiv Computer Vision Papers - 2025-10-06</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#monster-a-unified-model-for-motion-scene-text-retrieval" class="nav-link">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#taming-text-to-sounding-video-generation-via-advanced-modality-condition-and-interaction" class="nav-link">Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-scalable-and-consistent-3d-editing" class="nav-link">Towards Scalable and Consistent 3D Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#tit-score-evaluating-long-prompt-based-text-to-image-alignment-via-text-to-image-to-text-consistency" class="nav-link">TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency</a>
                </li>
                <li class="nav-item">
                    <a href="#one-patch-to-caption-them-all-a-unified-zero-shot-captioning-framework" class="nav-link">One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</a>
                </li>
                <li class="nav-item">
                    <a href="#work-zones-challenge-vlm-trajectory-planning-toward-mitigation-and-robust-autonomous-driving" class="nav-link">Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#maskcd-mitigating-lvlm-hallucinations-by-image-head-masked-contrastive-decoding" class="nav-link">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a>
                </li>
                <li class="nav-item">
                    <a href="#adard-key-adaptive-relevance-diversity-keyframe-sampling-for-long-form-video-understanding" class="nav-link">AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#mogic-boosting-motion-generation-via-intention-understanding-and-visual-context" class="nav-link">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a>
                </li>
                <li class="nav-item">
                    <a href="#memory-forcing-spatio-temporal-memory-for-consistent-scene-generation-on-minecraft" class="nav-link">Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-06">Arxiv Computer Vision Papers - 2025-10-06</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ3æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ§è¡æè¦ (2025-10-03)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong>
ä»å¤©çè®ºæéå±ç¤ºäºè®¡ç®æºè§è§é¢åå¨å¤æ¨¡æçè§£ãçæåç¼è¾æ¹é¢çæç»­å¿«éåå±ãæ ¸å¿è¶å¿åæ¬ï¼</p>
<ol>
<li><strong>å¤æ¨¡æèåä¸ç»ä¸æ¨¡åï¼</strong> æ¾èå³æ³¨äºæ´åææ¬ãå¾åãè§é¢åè¿å¨ç­å¤ç§æ¨¡æï¼ä»¥å®ç°æ´å¼ºå¤§çæ£ç´¢ãçæåçè§£è½åã</li>
<li><strong>çææ¨¡åçé«çº§æ§å¶ä¸ä¸è´æ§ï¼</strong> ææ¬å°å¾å/è§é¢çææ¨¡åæ­£å¨åªåè§£å³é¿æç¤ºå¯¹é½ãæ¶ç©ºä¸è´æ§ä»¥ååå°å¹»è§ç­ææã</li>
<li><strong>3Dåå®¹çæä¸ç¼è¾ï¼</strong> 3Dåºæ¯ååå®¹çå¯æ©å±ãä¸è´æ§ç¼è¾åçææ¯å¦ä¸ä¸ªéè¦æ¹åã</li>
<li><strong>é²æ£æ§ä¸å¯é æ§ï¼</strong> éå¯¹VLMå¨å¤æåºæ¯ï¼å¦èªå¨é©¾é©¶ï¼ä¸­çé²æ£æ§ä»¥åå¤§åè§è§è¯­è¨æ¨¡åï¼LVLMï¼å¹»è§çç¼è§£ç­ç¥åå°å³æ³¨ã</li>
<li><strong>æçä¸ä¼åï¼</strong> è§é¢çè§£ä¸­çå³é®å¸§éæ ·ä»¥åè¿å¨çæä¸­çæå¾çè§£æ¨å¨æé«æçåè´¨éã</li>
</ol>
<p><strong>ç¹å«æ¾èæåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"MonSTeR: a Unified Model for Motion, Scene, Text Retrieval" (Luca Collorone et al.)</strong>ï¼è¿ä»½è®ºæå å¶æåºä¸ä¸ª<strong>ç»ä¸æ¨¡å</strong>æ¥å¤çè¿å¨ãåºæ¯åææ¬æ£ç´¢èæ¾å¾å°¤ä¸ºçªåºãè¿ç§è·¨æ¨¡æçç»ä¸æ¹æ³å¨ä¿¡æ¯æ£ç´¢é¢åå·æå·¨å¤§çæ½åï¼å¯è½ä¸ºæªæ¥çå¤æ¨¡ææç´¢å¼æå¥ å®åºç¡ã</li>
<li><strong>"TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency" (Juntong Wang et al.)</strong>ï¼å¨ææ¬å°å¾åçææ¥çå¤æãé¿æç¤ºæä¸ºå¸¸æçèæ¯ä¸ï¼æåºä¸ä¸ª<strong>æ°çè¯ä¼°ææ </strong>æ¥è¡¡éé¿æç¤ºå¯¹é½åº¦è³å³éè¦ãTIT-Scoreéè¿âææ¬-å¾å-ææ¬âçä¸è´æ§æ¥è¯ä¼°ï¼æä¾äºä¸ç§æ´å¨é¢åå¯é çåº¦éæ¹æ³ï¼å¯¹çææ¨¡åçç ç©¶åå¼åå·æç´æ¥æå¯¼æä¹ã</li>
<li><strong>"MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding" (Jingyuan Deng, Yujiu Yang)</strong>ï¼éçLVLMçå¹¿æ³åºç¨ï¼å¹»è§é®é¢æ¯å¶å¯é æ§çä¸»è¦éç¢ãMaskCDæåºäºä¸ç§<strong>æ°é¢çè§£ç ç­ç¥</strong>æ¥ç¼è§£è¿ä¸é®é¢ï¼è¿å¯¹äºæåLVLMå¨å®éåºç¨ä¸­çå¯ä¿¡åº¦å·æéè¦æä¹ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>âææ¬-å¾å-ææ¬âä¸è´æ§è¯ä¼°ï¼</strong> TIT-Scoreçæåºè¡¨æï¼å¯¹çææ¨¡åè¾åºè´¨éçè¯ä¼°æ­£ä»åä¸æ¨¡ææç®åå¯¹é½è½¬åæ´å¤æçå¾ªç¯ä¸è´æ§éªè¯ã</li>
<li><strong>åºäºæå¾çè¿å¨çæï¼</strong> MoGICéè¿çè§£æå¾æ¥æåè¿å¨çæï¼é¢ç¤ºçæªæ¥çææ¨¡åå°æ´æ·±å¥å°çè§£ç¨æ·æç¯å¢çæ½å¨ç®æ ã</li>
<li><strong>æ¶ç©ºè®°å¿æºå¶ï¼</strong> Memory Forcingå¨Minecraftåºæ¯çæä¸­å¼å¥æ¶ç©ºè®°å¿ï¼å¼ºè°äºå¨é¿åºåæå¤æåºæ¯çæä¸­ä¿æä¸è´æ§åè¿è´¯æ§çéè¦æ§ã</li>
<li><strong>ç»ä¸çå¤æ¨¡ææ£ç´¢æ¶æï¼</strong> MonSTeRä»£è¡¨äºå°å¤ç§æ¨¡æï¼è¿å¨ãåºæ¯ãææ¬ï¼æ´åå°åä¸æ£ç´¢æ¡æ¶ä¸­çè¶å¿ï¼æ¨å¨å®ç°æ´å¨é¢çä¿¡æ¯è®¿é®ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<ol>
<li><strong>"MonSTeR: a Unified Model for Motion, Scene, Text Retrieval"</strong>ï¼å¯¹äºå¯¹å¤æ¨¡æä¿¡æ¯æ£ç´¢åç»ä¸æ¨¡åæ¶ææå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºææä¾äºæ½å¨ççªç ´æ§æ¹æ³ã</li>
<li><strong>"TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency"</strong>ï¼ä»»ä½ä»äºææ¬å°å¾åçææ¨¡åå¼åæè¯ä¼°çäººé½åºè¯¥æ·±å¥éè¯»ï¼å ä¸ºå®æä¾äºä¸ä¸ªå³é®çè¯ä¼°å·¥å·ã</li>
<li><strong>"MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding"</strong>ï¼å¯¹äºå³æ³¨å¤§åè§è§è¯­è¨æ¨¡åå¯é æ§ãå¹»è§é®é¢åå¶è§£å³æ¹æ¡çç ç©¶äººåæ¥è¯´ï¼è¿ç¯è®ºææ¯å¿è¯»çã</li>
<li><strong>"Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction"</strong>ï¼å¯¹äºè§é¢çæåå¤æ¨¡æäº¤äºæå´è¶£çå¢éï¼è¿ç¯è®ºæå±ç¤ºäºå¦ä½éè¿é«çº§æ¡ä»¶æ§å¶æ¥æåè§é¢çæè´¨éã</li>
</ol>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥Arxivè®¡ç®æºè§è§é¢åçå³é®è¿å±ï¼å¹¶è¯å«åºå¯¹æ¨ç ç©¶ææä»·å¼çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.03200v1">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a></li>
<li><a href="#2510.03117v1">Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</a></li>
<li><a href="#2510.02994v1">Towards Scalable and Consistent 3D Editing</a></li>
<li><a href="#2510.02987v1">TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency</a></li>
<li><a href="#2510.02898v1">One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</a></li>
<li><a href="#2510.02803v1">Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving</a></li>
<li><a href="#2510.02790v1">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a></li>
<li><a href="#2510.02778v1">AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</a></li>
<li><a href="#2510.02722v1">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a></li>
<li><a href="#2510.03198v1">Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.03200v1'></a></p>
<h2 id="monster-a-unified-model-for-motion-scene-text-retrieval"><a href="https://arxiv.org/abs/2510.03200v1">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a></h2>
<p><strong>Authors:</strong> Luca Collorone, Matteo Gioia, Massimiliano Pappa, Paolo Leoni, Giovanni Ficarra, Or Litany, Indro Spinelli, Fabio Galasso</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâMonSTeR: a Unified Model for Motion, Scene, Text Retrievalâè®ºæçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼MonSTeR: è¿å¨ãåºæ¯ãææ¬ç»ä¸æ£ç´¢æ¨¡å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
äººç±»å¨å¤æç¯å¢ä¸­çè¿å¨åæå¾é©±å¨ï¼ä½å¶å®ç°å¿é¡»ä¾èµå¨å´ç¯å¢çæ¯æãç°æç ç©¶ç¼ºä¹ææå·¥å·æ¥è¯ä¼°éª¨éª¼è¿å¨ï¼è¿å¨ï¼ãæå¾ï¼ææ¬ï¼åå¨å´ç¯å¢ï¼åºæ¯ï¼ä¸èä¹é´çä¸è´æ§ãè¿å¯¼è´äºå¨äººç±»è¿å¨çæåæ£ç´¢ä¸­ï¼ç¯å¢ä¸ä¸ææªè½è¢«ååå©ç¨ï¼ä»¥åäººç±»åºæ¯äº¤äºæ¨¡åç¼ºä¹å¨å±ä¸è´æ§è¯ä¼°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>é¦æ¬¡æåºç»ä¸çè¿å¨-åºæ¯-ææ¬æ£ç´¢æ¨¡åï¼MonSTeRï¼ï¼</strong> MonSTeRæ¯ç¬¬ä¸ä¸ªè½å¤å¨ä¸ä¸ªç»ä¸çæ½å¨ç©ºé´ä¸­è¯ä¼°ææ¬ãè¿å¨ååºæ¯ä¸æ¨¡æä¹é´ä¸è´æ§çæ¨¡åã
*   <strong>å»ºæ¨¡é«é¶å³ç³»ï¼</strong> åæææ·±åº¦å­¦ä¹ çå¯åï¼MonSTeRéè¿å»ºæ¨¡è¶è¶æå¯¹å³ç³»çæ´é«é¶äº¤äºæ¥æææ¨¡æé´çå¤æä¾èµãå®éè¿å¯¹åæ¨¡æåè·¨æ¨¡æè¡¨ç¤ºè¿è¡å¯¹é½ï¼æå»ºäºä¸ä¸ªç»ä¸çæ½å¨ç©ºé´ãå·ä½æ¥è¯´ï¼å®ä¸ä»å¯¹é½åæ¨¡æé¡¹ï¼t, s, mï¼ï¼è¿å¯¹é½æå¯¹çè·¨æ¨¡æé¡¹ï¼ts, sm, mtï¼ï¼ä»¥ææä¸æ¨¡æäº¤äºã
*   <strong>çµæ´»ä¸é²æ£çæ£ç´¢è½åï¼</strong> è¿ç§å»ºæ¨¡æ¹å¼ä½¿å¾MonSTeRè½å¤çµæ´»ä¸é²æ£å°æ§è¡åç§æ£ç´¢ä»»å¡ï¼åæ¬ç»å®ä¸ä¸ªæ¨¡ææ£ç´¢å¦ä¸ä¸ªæ¨¡æï¼æç»å®ä¸¤ä¸ªæ¨¡æçè¡¨ç¤ºæ£ç´¢ä¸ä¸ªæ¨¡æã
*   <strong>äººç±»åºæ¯äº¤äºæ¨¡åè¯ä¼°å·¥å·ï¼</strong> MonSTeRå¯ä½ä¸ºè¯ä¼°ææ¬æ¡ä»¶äººç±»åºæ¯äº¤äºæ¨¡åï¼HSIï¼çå·¥å·ï¼è½å¤è¯ä¼°è¿å¨è·¯å¾çåçæ§ãä¸åºæ¯çç¬¦ååº¦ï¼å¹¶ä¸äººç±»åå¥½é«åº¦ä¸è´ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ£ç´¢æ§è½ï¼</strong> MonSTeRå¨HUMANISE+åTRUMANS+æ°æ®éä¸çæ£ç´¢ä»»å¡ä¸­ï¼æ¾èä¼äºä»ä¾èµåæ¨¡æè¡¨ç¤ºçä¸æ¨¡ææ¨¡åãå¨âAllâåè®®çst2mä»»å¡ä¸­ï¼MonSTeRç¸å¯¹äºæä½³åºæ¯æç¥æ¨¡åæ§è½æåäº209%ã
*   <strong>ä¸äººç±»åå¥½å¯¹é½ï¼</strong> éè¿ç¨æ·ç ç©¶éªè¯ï¼MonSTeRçæ£ç´¢åæ°ä¸äººç±»åå¥½é«åº¦ä¸è´ï¼66.5%çå¯¹é½çï¼ï¼è¯æäºå¶è¯ä¼°ç»æçå¯é æ§ã
*   <strong>å¤åè½æ§ï¼</strong> MonSTeRçæ½å¨ç©ºé´å¨é¶æ ·æ¬åºæ¯åç©ä½æ¾ç½®ï¼In-Scene Object Placementï¼åè¿å¨æè¿°ï¼Motion Captioningï¼ç­ä¸æ¸¸ä»»å¡ä¸­å±ç°åºå¼ºå¤§çéç¨æ§ãå¨é¶æ ·æ¬ç©ä½æ¾ç½®ä»»å¡ä¸­ï¼å¹³åè¯¯å·®ä»ä¸º18cmï¼å¨è¿å¨æè¿°ä»»å¡ä¸­ï¼MonSTeR+GPT2å¨å¤é¡¹ææ ä¸è¶è¶äºMotionGPTã
*   <strong>åºæ¯-è¿å¨æ¥å°è½åï¼</strong> MonSTeRè½å¤åºåä¸åºæ¯åææ¬æå¾ä¸è´çè¿å¨è·¯å¾ä¸ä¸ä¸è´çè·¯å¾ï¼å¹¶è½è¯å«åºç©¿éåºæ¯ç©ä½çè¿å¨ï¼è¡¨æå¶æ½å¨ç©ºé´ååäºèªç¶è¿å¨ä¸åºç©¿éåºæ¯çç¥è¯ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è®¡ç®ææ¬ï¼</strong> è·¨æ¨¡æç¼ç å¨ä»å¨å¯¹é½çæ¨¡æå¯¹ä¸è¿è¡è®­ç»ï¼å ä¸ºå¨æªéå¯¹æ°æ®ä¸è¿è¡è®­ç»çè®¡ç®ææ¬è¿é«ã
*   <strong>éæåºæ¯ï¼</strong> å½åæ¨¡ååè®¾åºæ¯æ¯éæçï¼å³äººç±»å¨ä½ä¸ä¼æ¹ååºæ¯å¸å±ãè¿éå¶äºå¶å¨å¨æåºæ¯ä¸­çåºç¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨æåºæ¯ç¼ç å¨ï¼</strong> æ¢ç´¢åå¼åè½å¤å¤çå¨æåºæ¯çç¼ç å¨ï¼ä»¥æ©å±MonSTeRå¨æ´å¤æãäº¤äºæ§æ´å¼ºçç¯å¢ä¸­çåºç¨ã
*   <strong>æ´å¤æçæ¨¡æäº¤äºï¼</strong> è¿ä¸æ­¥ç ç©¶åå»ºæ¨¡æ´å¤æçé«é¶æ¨¡æäº¤äºï¼ä»¥æææ´ç»è´çä¾èµå³ç³»ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model.</li>
<li>Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations.</li>
<li>We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.03200v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.03200v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.03117v1'></a></p>
<h2 id="taming-text-to-sounding-video-generation-via-advanced-modality-condition-and-interaction"><a href="https://arxiv.org/abs/2510.03117v1">Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</a></h2>
<p><strong>Authors:</strong> Kaisi Guan, Xihua Wang, Zhengfeng Lai, Xin Cheng, Peng Zhang, XiaoJiang Liu, Ruihua Song, Meng Cao</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV, cs.SD</p>
<p><strong>Abstract:</strong></p>
<p>This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæãéè¿é«çº§æ¨¡ææ¡ä»¶åäº¤äºé©¯æææ¬å°æå£°è§é¢çæãè§£å³äºææ¬å°æå£°è§é¢ï¼T2SVï¼çæè¿ä¸å·ææææ§çä»»å¡ãT2SVçç®æ æ¯æ ¹æ®ææ¬æ¡ä»¶çæå·æåæ­¥é³é¢çè§é¢ï¼åæ¶ç¡®ä¿è§é¢åé³é¢æ¨¡æé½ä¸ææ¬å¯¹é½ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡å¨èåé³è§é¢è®­ç»æ¹é¢åå¾äºè¿å±ï¼ä½T2SVçæä»é¢ä¸´ä¸¤ä¸ªå³é®ææï¼
1. <strong>æ¨¡æå¹²æ°ï¼</strong> åä¸å±äº«ææ¬æè¿°ï¼è§é¢ææ¬ç­äºé³é¢ææ¬ï¼å¸¸å¸¸å¯¼è´æ¨¡æå¹²æ°ï¼æ··æ·é¢è®­ç»çéª¨å¹²ç½ç»ã
2. <strong>è·¨æ¨¡æç¹å¾äº¤äºæºå¶ï¼</strong> æä½³çè·¨æ¨¡æç¹å¾äº¤äºæºå¶å°ä¸æç¡®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼è®ºææåºäºä¸¤é¡¹ä¸»è¦åæ°ï¼
1. <strong>åå±è§è§æ¥å°å­å¹ï¼HVGCï¼æ¡æ¶ï¼</strong> è¯¥æ¡æ¶çæä¸å¯¹è§£è¦çå­å¹ââä¸ä¸ªè§é¢å­å¹ï¼Tvï¼åä¸ä¸ªé³é¢å­å¹ï¼TAï¼ï¼ä»èå¨æ¡ä»¶é¶æ®µæ¶é¤äºæ¨¡æå¹²æ°ãHVGCéè¿ä¸é¶æ®µç®¡éå®ç°ï¼é¦åï¼VLLMçæè¯¦ç»çè§é¢æè¿°ï¼å¶æ¬¡ï¼LLMä»è§é¢æè¿°ä¸­æåç¸å³çå¬è§äºä»¶æ ç­¾ï¼æåï¼LLMå©ç¨è§é¢æè¿°åå¬è§æ ç­¾çæçº¯é³é¢å­å¹ãè¿ç§æ¹æ³ç¡®ä¿äºå­å¹çæ¨¡æçº¯åº¦ååç¡®æ§ã
2. <strong>BridgeDiTæ¨¡åï¼</strong> ä¸ç§æ°é¢çåå¡æ©æ£Transformerï¼éç¨<strong>åéäº¤åæ³¨æåï¼DCAï¼æºå¶</strong>ãDCAåå½ä¸ä¸ªå¼ºå¤§çâæ¡¥æ¢âï¼å®ç°äºè§é¢åé³é¢å¡ä¹é´çå¯¹ç§°ãååä¿¡æ¯äº¤æ¢ï¼ä»èå®ç°äºè¯­ä¹åæ¶é´ä¸çåæ­¥ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæå¨ä¸ä¸ªåºåæ°æ®éä¸è¿è¡äºå¹¿æ³çå®éªï¼å¹¶è¾ä»¥äººå·¥è¯ä¼°ï¼ç»æè¡¨æï¼
* BridgeDiTæ¨¡åå¨å¤§å¤æ°ææ ä¸åä¼äºææåºçº¿æ¹æ³ï¼è¾¾å°äºæåè¿çæ§è½ï¼åæ¬è§é¢è´¨éï¼FVDãKVDï¼ãé³é¢è´¨éï¼FADãKLï¼ãææ¬å¯¹é½ï¼CLIPSIMãCLAPï¼åæ¶é´åæ­¥ï¼AV-Alignï¼ã
* HVGCæ¡æ¶å¨é¶æ ·æ¬åå¨è®­ç»è®¾ç½®ä¸å§ç»è¡¨ç°æä½³ï¼éªè¯äºå¶å¨æ¶é¤æ¨¡æå¹²æ°æ¹é¢çé²æ£æ§ã
* DCAèåæºå¶å¨è®­ç»è¿ç¨ä¸­å§ç»ä¼äºå¶ä»èåæºå¶ï¼è¡¨æå¶å¨å®ç°åè¶çæ¶é´åè¯­ä¹åæ­¥æ¹é¢çä¼è¶æ§ã
è¿äºç»æçæä¹å¨äºï¼è¯¥æ¹æ³ææå°è§£å³äºT2SVä»»å¡ä¸­çæ ¸å¿ææï¼ä¸ºçæé«è´¨éãåæ­¥çææ¬å°æå£°è§é¢æä¾äºæ°çèå¼ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
* <strong>æ°æ®ç¨ç¼ºï¼</strong> T2SVé¢åé¢ä¸´å¤§è§æ¨¡ãé«è´¨éãæ æ³¨è¯å¥½çé³è§é¢æ°æ®ç¨ç¼ºçææã
* <strong>æ¨¡åä¾èµæ§ï¼</strong> æ¨¡åçæ§è½é«åº¦ä¾èµäºæ°æ®è´¨éï¼ä¸ç¨³å®æä½åè¾¨çè§é¢ä¼éä½é¢è®­ç»éª¨å¹²ç½ç»çè½åï¼èåæçé³é¢æç»å¤é³åä¼ä½¿ç²¾ç¡®åæ­¥å¤æåã
* <strong>åè½éå¶ï¼</strong> å½åçæ¬çBridgeDiTä¸»è¦ä¸æ³¨äºçæé³æï¼å°ä¸æ¯æè¯­é³åå¤æçé³ä¹ä¹è°±ã
* <strong>åºç¡æ¨¡åéå¶ï¼</strong> æ¨¡åçæ´ä½æ§è½åéäºæéçåºç¡T2VåT2Aæ¨¡åçæ§è½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
* <strong>æ°æ®éæ©å±ï¼</strong> æ¶éæ´å¤§ãæ´é«è´¨éçé³è§é¢æ°æ®éï¼å¹¶æ¹è¿æ°æ®å¤çç®¡éä»¥è¿è¡æ¸æ´ãè¿æ»¤åå­å¹çæã
* <strong>åè½æ©å±ï¼</strong> å°BridgeDiTæ©å±å°æ¯æè¯­é³åé³ä¹ï¼åæ¬éæä¸é¨çåå½¢åæ­¥æ¨¡ååææé³ä¹è¾å¥èå¥ä¸æç»ªçææ¯ã
* <strong>åè®­ç»ä¼åï¼</strong> æ¢ç´¢åè®­ç»ä¼åææ¯ï¼ä¾å¦åºç¨å¸¦æä¸é¨è®¾è®¡å¥å±çäººç±»åé¦å¼ºåå­¦ä¹ ï¼RLHFï¼ï¼ä»¥è¿ä¸æ­¥æé«æ¨¡åçæ¶ç©ºè¿è´¯æ§ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥HVGCæ¡æ¶åBridgeDiTæ¨¡åï¼ç¹å«æ¯å¶DCAæºå¶ï¼ï¼ä¸ºææ¬å°æå£°è§é¢çæé¢åååºäºéè¦è´¡ç®ï¼ææè§£å³äºæ¨¡æå¹²æ°åè·¨æ¨¡æç¹å¾äº¤äºçé¾é¢ï¼å¹¶åå¾äºæ¾èçæ§è½æåã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization.</li>
<li>Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics.</li>
<li>Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.03117v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.03117v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02994v1'></a></p>
<h2 id="towards-scalable-and-consistent-3d-editing"><a href="https://arxiv.org/abs/2510.02994v1">Towards Scalable and Consistent 3D Editing</a></h2>
<p><strong>Authors:</strong> Ruihao Xia, Yang Tang, Pan Zhou</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ruihao Xia, Yang Tang, Pan Zhouæ°åçè®ºæâTowards Scalable and Consistent 3D Editingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="towards-scalable-and-consistent-3d-editing_1">è®ºæãTowards Scalable and Consistent 3D Editingãæè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³3Dç¼è¾é¢åçæ ¸å¿ææï¼å¦ä½å®ç°å¯¹3Dèµäº§çç²¾ç¡®ãå±é¨åç¼è¾ï¼åæ¶ä¿æè·¨è§å¾ä¸è´æ§ãç»æä¿çåº¦åç²¾ç»æ§å¶ï¼ä¸æ éæå¨åå»º3Dæ©ç ãç°æç2Dç¼è¾å·¥å·å·²éå¸¸ç´è§åæç¨ï¼ä½3Dç¼è¾ä»é¢ä¸´éåº¦æ¢ãæäº§çå ä½å¤±çä»¥åå¯¹ä¸åç¡®3Dæ©ç çä¾èµç­é®é¢ï¼è¿éå¶äºå¶å¨æ²æµ¸å¼åå®¹åä½ãæ°å­å¨±ä¹åAR/VRç­å®éåºç¨ä¸­çå¯æ©å±æ§åå®ç¨æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼è®ºæå¨æ°æ®åæ¨¡åä¸¤æ¹é¢é½è¿è¡äºåæ°ï¼</p>
<ul>
<li><strong>æ°æ®æ¹é¢ï¼3DEditVerseæ°æ®éã</strong> è®ºæå¼å¥äºè¿ä»ä¸ºæ­¢æå¤§çéå¯¹3Dç¼è¾åºåæ°æ®é3DEditVerseï¼åå«116,309ä¸ªé«è´¨éè®­ç»å¯¹å1,500ä¸ªç²¾å¿ç­åçæµè¯å¯¹ãè¯¥æ°æ®ééè¿äºè¡¥çç®¡éæå»ºï¼åæ¬ï¼<ul>
<li><strong>å§¿æé©±å¨çå ä½ç¼è¾ï¼</strong> çææè·å¨ç»è§è²å¤æ ·åå§¿æåå ä½ååçâååâèµäº§ã</li>
<li><strong>åºç¡æ¨¡åå¼å¯¼çå¤è§ç¼è¾ï¼</strong> å©ç¨ææ¬æä»¤åä¸ç³»ååºç¡æ¨¡åï¼å¦DeepSeek-R1ãFluxãQwen-VLãTrellisï¼è¿è¡å¤è§ä¿®æ¹ï¼ç¡®ä¿ç¼è¾çå±é¨æ§ãå¤è§å¾ä¸è´æ§åè¯­ä¹å¯¹é½ã</li>
<li><strong>ä¸è´æ§ä¿çç3Dæåï¼</strong> éç¨æ©ç å¼å¯¼çéç»ç­ç¥ï¼æ¾å¼å®ä½3Dç¼è¾åºåï¼å¹¶èåæºåç®æ æ½å¨è¡¨ç¤ºï¼ä»¥ç¡®ä¿ç¼è¾çä¿çåº¦ã</li>
<li><strong>åç¼è¾ä¸è´æ§è¿æ»¤ï¼</strong> éè¿DINOv2ç¹å¾ç¸ä¼¼æ§è¿æ»¤ï¼è¿ä¸æ­¥ä¿è¯å¨å±ä¸è´æ§ã</li>
</ul>
</li>
<li><strong>æ¨¡åæ¹é¢ï¼3DEditFormerã</strong> è®ºææåºäºä¸ä¸ª3Dç»æä¿æçæ¡ä»¶Transformeræ¨¡å3DEditFormerï¼å®éè¿ä»¥ä¸æºå¶å¢å¼ºäºå¾åå°3Dççæè½åï¼<ul>
<li><strong>åéå¼å¯¼æ³¨æååï¼Dual-Guidance Attention Blockï¼ï¼</strong> å¼å¥ä¸¤ä¸ªå¹¶è¡çäº¤åæ³¨æåè·¯å¾ï¼å¨ä¸åæ©æ£é¶æ®µæ³¨å¥æºèµäº§çå¤é¶æ®µç¹å¾ï¼ä»èå°å¯ç¼è¾åºåä¸ä¿çç»æåç¦»ã</li>
<li><strong>å¤é¶æ®µç¹å¾æåï¼</strong> ä»å»ç»çTrellisæ¨¡åä¸­æåç»ç²åº¦ç»æç¹å¾ï¼å¨æææ©æ£æ­¥éª¤ï¼åè¯­ä¹è½¬æ¢ç¹å¾ï¼å¨æ©ææ©æ£æ­¥éª¤ï¼ï¼ä»¥ææäºè¡¥çç»æä¿¡æ¯ã</li>
<li><strong>æ¶é´èªéåºé¨æ§æºå¶ï¼Time-Adaptive Gatingï¼ï¼</strong> å¨æå¹³è¡¡ç»ç²åº¦ç»æç¹å¾åè¯­ä¹è½¬æ¢ç¹å¾çå½±åï¼å¨æ©æé¶æ®µå¼ºè°è¯­ä¹ç¼è¾ï¼å¨åæé¶æ®µå¼ºè°ç»æä¿çåº¦ã</li>
</ul>
</li>
</ul>
<p>è¿äºåæ°ä½¿å¾3DEditFormeræ éè¾å©3Dæ©ç å³å¯å®ç°ç²¾ç¡®ãä¸è´ä¸ç»æä¿æç3Dç¼è¾ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½è¶è¶SOTAï¼</strong> å¹¿æ³çå®éªè¡¨æï¼3DEditFormerå¨å®éåå®æ§ä¸åä¼äºç°ææåè¿çåºçº¿æ¹æ³ï¼å¦EditP23ãInstant3ditåVoxHammerï¼ã
*   <strong>æ é3Dæ©ç ï¼</strong> 3DEditFormeræ éä»»ä½è¾å©3Dæ©ç ï¼ç®åäºç¼è¾æµç¨ï¼åæ¶å®ç°äºåè¶çç¼è¾è´¨éåä¸è´æ§ãä¸ä¾èµç²¾ç¡®3Dæ©ç çVoxHammerç¸æ¯ï¼3DEditFormerå¨3Dææ ä¸å¹³åæé«äº13%ã
*   <strong>é«ä¿çåº¦ä¸å®ç¨æ§ï¼</strong> è¯¥æ¡æ¶è½å¤å®ç°é«è´¨éçå±é¨ä¿®æ¹ï¼åæ¶ä¿æç»æä¿çåº¦ï¼è§£å³äºç°ææ¹æ³ä¸­å¸¸è§çå ä½å¤±çåä¸ä¸è´æ§é®é¢ã
*   <strong>å»ºç«æ°æ åï¼</strong> è®ºæçå·¥ä½ä¸ºå®ç¨åå¯æ©å±ç3Dç¼è¾è®¾å®äºæ°æ åï¼ä¸ºè¯¥é¢åçç³»ç»æ§è¿å±å¥ å®äºåºç¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææåºï¼3DEditFormerä¾èµäºæ½å¨ç©ºé´ç¼è¾ï¼è½ç¶æçé«ï¼ä½å¨å¤çé«åè¾¨ç3Dèµäº§æ¶å¯è½ä¼å¼å¥ç²¾åº¦æå¤±ãå¨æ½å¨è½¬æ¢è¿ç¨ä¸­ï¼ç»ç²åº¦çå ä½ç»èå¯è½ä¼è¢«éçº§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢ç´æ¥å¨åå§3Dåä¸­è¿è¡æ æç¼è¾ï¼ä»¥æ´å¥½å°ä¿çç»ç²åº¦çç½æ ¼ä¿çåº¦ã</p>
<hr />
<p>è¿ä»½æè¦çªåºäºè®ºæå¨è§£å³3Dç¼è¾æææ¹é¢çåæ°æ§ãææ¯è´¡ç®åéè¦ææï¼åæ¶ä¹æåºäºå¶å½åæ¹æ³çå±éæ§ä»¥åæªæ¥å¯è½çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs.</li>
<li>On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer.</li>
<li>Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02994v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02994v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02987v1'></a></p>
<h2 id="tit-score-evaluating-long-prompt-based-text-to-image-alignment-via-text-to-image-to-text-consistency"><a href="https://arxiv.org/abs/2510.02987v1">TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency</a></h2>
<p><strong>Authors:</strong> Juntong Wang, Huiyu Duan, Jiarui Wang, Ziheng Jia, Guangtao Zhai, Xiongkuo Min</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Juntong Wangç­äººæ°åçè®ºæâTIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistencyâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åææ¬å°å¾åï¼T2Iï¼æ¨¡åå¨å¤çé¿èè¯¦ç»çæç¤ºæ¶ï¼é¾ä»¥ææçè§£åéµå¾ªæä»¤ï¼å¯¼è´çæå¾åä¸é¿æç¤ºä¹é´ä¸è´æ§å·®çé®é¢ãç°æçT2Iå¯¹é½è¯ä¼°ææ å¨é¿æç¤ºå¾åçææ¹é¢ä¸äººç±»åå¥½çä¸è´æ§è¾å·®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
*   <strong>LPG-Benchåºåæµè¯ï¼</strong> è®ºæå¼å¥äºä¸ä¸ªåä¸ºLPG-Benchçç»¼ååºåæµè¯ï¼ä¸é¨ç¨äºè¯ä¼°åºäºé¿æç¤ºçT2Içæãè¯¥åºååå«200ä¸ªç²¾å¿å¶ä½çæç¤ºï¼å¹³åé¿åº¦è¶è¿250å­ï¼å¹¶ä»13ä¸ªæåè¿çæ¨¡åçæäº2600å¼ å¾åï¼è¿è¡äºå¨é¢çäººå·¥æåæ æ³¨ã
*   <strong>TITè¯ä¼°æ¡æ¶ï¼</strong> è®ºææåºäºä¸ç§æ°é¢çé¶æ ·æ¬è¯ä¼°ææ ï¼ç§°ä¸ºTITï¼Text-to-Image-to-Text consistencyï¼ï¼ç¨äºè¯ä¼°é¿æç¤ºçæçå¾åãTITçæ ¸å¿æ¦å¿µæ¯éè¿ç´æ¥æ¯è¾åå§æç¤ºä¸å¤§åå¤æ¨¡ææ¨¡åï¼LMMï¼å¯¹çæå¾åçæè¿°ä¹é´çä¸è´æ§æ¥éåT2Iå¯¹é½ã
*   <strong>TITçä¸¤ç§å®ä¾åï¼</strong> TITæ¡æ¶åæ¬ä¸¤ç§äºè¡¥çå®ä¾åï¼
    *   <strong>TIT-Scoreï¼</strong> ä¸ç§é«æçåºäºåµå¥çå®ä¾åï¼ä½¿ç¨åè¿çææ¬åµå¥æ¨¡åï¼Qwen3-Embeddingï¼å°ææ¬ç¼ç ä¸ºç¹å¾åéï¼å¹¶éè¿è®¡ç®ä½å¼¦ç¸ä¼¼åº¦æ¥è¡¡éä¸è´æ§ã
    *   <strong>TIT-Score-LLMï¼</strong> ä¸ç§åºäºå¤§åè¯­è¨æ¨¡åï¼LLMï¼çå®ä¾åï¼å©ç¨åæ²¿LLMï¼å¦Gemini 2.5 Proï¼ç´æ¥è¯ä¼°ä¸¤ä¸ªææ¬ä¹é´çè¯­ä¹ç¸ä¼¼åº¦ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ç°æææ çå±éæ§ï¼</strong> LPG-Benchä¸çå®éªè¡¨æï¼ç°ææåè¿çT2Iå¯¹é½è¯ä¼°ææ ï¼å¦CLIP-scoreãLMM-scoreç­ï¼å¨é¿æç¤ºå¾åçææ¹é¢ä¸äººç±»åå¥½çä¸è´æ§è¾å·®ã
*   <strong>TITçä¼è¶æ§ï¼</strong> TITæ¡æ¶ï¼ç¹å«æ¯TIT-Score-LLMï¼å¨ä¸äººç±»å¤æ­çå¯¹é½æ¹é¢è¡¨ç°åºåè¶çæ§è½ï¼å¶éå¯¹åç¡®çæ¯æå¼ºçåºçº¿ï¼LMM4LMMï¼ç»å¯¹æé«äº7.31%ï¼è¾¾å°äº66.51%ãæ åçTIT-Scoreä¹è¾¾å°äºæ¥è¿æåè¿çæ§è½ï¼åæ¶å·ææ´é«çæçåå¯è®¿é®æ§ã
*   <strong>è§£è¦è®¾è®¡çæææ§ï¼</strong> æ¶èç ç©¶è¯å®äºå¶è§£è¦è®¾è®¡çæææ§ï¼å³è§è§æç¥ä¸ææ¬è¯­ä¹å¯¹é½çåç¦»ï¼è¿è§£å³äºç«¯å°ç«¯LMMè¯ååºæçä¸ç¨³å®æ§é®é¢ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>LLMè¯åçç¨³å®æ§é®é¢ï¼</strong> è®ºææåºï¼ç´æ¥ä½¿ç¨LMMè¿è¡ç«¯å°ç«¯è¯åå­å¨åºæçç¼ºé·ï¼LMMç¼ºä¹ç¨³å®çãç»è¿æ ¡åçâè¯ååèæ¡æ¶âï¼å¯¼è´ç»æä¸ä¸è´ä¸éç°æ§å·®ã
*   <strong>VLMéª¨å¹²éæ©çå±éæ§ï¼</strong> å°½ç®¡TITæ¡æ¶è¡¨ç°åºè²ï¼ä½åææ¾ç¤ºVLMéª¨å¹²æ¨¡åçåæ°éå¹¶ä¸ä¸å®ä¿è¯æ´å¥½çæ§è½ï¼ä¾å¦Qwen2.5vlç³»åä¸­ï¼7Bæ¨¡åæ¯32Bæ¨¡ååå¾äºæ´é«çåç¡®çï¼è¿è¡¨æåå§æ¨¡åè§æ¨¡å¹¶éVLMæææ§çå¯ä¸å³å®å ç´ ã
*   <strong>æåç¸å³æ§ææ çè§£éï¼</strong> SRCCåKRCCç­æåç¸å³æ§ææ å¨LPG-Benchèæ¯ä¸åºè°¨æè§£éï¼å ä¸ºå¾æåé¡¹ç®æ°éè¾å°ï¼13ä¸ªï¼ä»¥åå°é¢çå®æ°æ®ä¸­å­å¨å¤§éå¹³å±å¯è½ä¼éå¶å¶ç¨³å®æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>T2Iæ¨¡åçåå±ï¼</strong> LPG-BenchåTITæ¹æ³å±åä¸ºT2Iæ¨¡åçåºåæµè¯ååå±æä¾äºæ´æ·±å¥çè§è§ï¼ç¹å«æ¯å¨é¿ææ¬çè§£ååå®¹åä½æ¹é¢ã
*   <strong>VLMååµå¥æ¨¡åçä¼åï¼</strong> è¿ä¸æ­¥ç ç©¶VLMåææ¬åµå¥æ¨¡åçéæ©åä¼åï¼ä»¥å¨æ§è½åè®¡ç®èµæºä¹é´åå¾æ´å¥½çå¹³è¡¡ï¼ä»èæé«TITæ¡æ¶çå®ç¨æ§åå¯è®¿é®æ§ã
*   <strong>æ´å¤æçææ¬çè§£ï¼</strong> è®ºæå¼ºè°äºé«çº§ææ¬çè§£æ¯è®¸å¤T2Iç³»ç»çå³é®ç¶é¢ï¼æªæ¥çç ç©¶å¯ä»¥ä¸æ³¨äºå¼åè½å¤æ´å¥½å°å¤çé¿ãè¯¦ç»ååäºé£æ ¼æä»¤çæ¨¡åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation.</li>
<li>Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations.</li>
<li>Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation.</li>
<li>To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02987v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02987v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02898v1'></a></p>
<h2 id="one-patch-to-caption-them-all-a-unified-zero-shot-captioning-framework"><a href="https://arxiv.org/abs/2510.02898v1">One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</a></h2>
<p><strong>Authors:</strong> Lorenzo Bianchi, Giacomo Pacini, Fabio Carrara, Nicola Messina, Giuseppe Amato, Fabrizio Falchi</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâOne Patch to Caption Them All: A Unified Zero-Shot Captioning Frameworkâè®ºæçå¨é¢æè¦ï¼ç¨ä¸­æåæï¼å¹¶æ¶µçäºæ¨è¦æ±çææè¦ç¹ã</p>
<h3 id="one-patch-to-caption-them-all-a-unified-zero-shot-captioning-framework_1">è®ºææè¦ï¼One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
å½åé¶æ ·æ¬å¾åå­å¹çæå¨ï¼zero-shot captionersï¼ä¸»è¦ä¾èµå¨å±å¾åè¡¨ç¤ºæ¥çææ´ä¸ªå¾åçå­å¹ï¼è¿éå¶äºå®ä»¬å¯¹å¾åä»»æå±é¨åºåè¿è¡ç»ç²åº¦æè¿°çè½åï¼ä¸éè¦åºåçº§å«ççç£æ°æ®ãæ¬ææ¨å¨è§£å³å¦ä½å¨é¶æ ·æ¬è®¾ç½®ä¸ï¼å¯¹å¾åçä»»æåºåï¼ä»åä¸ªå¾ååå°éè¿ç»­åºåä¹è³æ´ä¸ªå¾åï¼çæé«è´¨éãç»ç²åº¦çå­å¹ï¼èæ éåºåçº§å«ççç£ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®</strong>
è®ºææåºäºä¸ä¸ªåä¸º <strong>Patch-ioner</strong> çç»ä¸é¶æ ·æ¬å­å¹çææ¡æ¶ï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>èå¼è½¬åï¼ä»å¾åä¸­å¿å°å¾ååä¸­å¿ï¼patch-centricï¼çå­å¹çæã</strong> ä¼ ç»çå­å¹æ¹æ³ä»¥æ´ä¸ªå¾åä¸ºå¤çååï¼èPatch-ionerå°åä¸ªå¾ååè§ä¸ºåå­å­å¹çæååã
*   <strong>ä»»æåºåçå­å¹çæï¼</strong> éè¿èåéå®å¾ååçç¹å¾ï¼è¯¥æ¡æ¶è½å¤çµæ´»å°ä¸ºä»»æå½¢ç¶åå¤§å°çåºåï¼åæ¬åä¸ªå¾ååãè¾¹çæ¡ãé¼ æ è½¨è¿¹æå®çåºåãéè¿ç»­åºåä»¥åæ´ä¸ªå¾åï¼çæå­å¹ï¼èæ éåºåçº§å«ççç£ã
*   <strong>è§£è¦çé¶æ ·æ¬è§£ç å¨ï¼</strong> æ¡æ¶å°å¾åç¼ç åææ¬è§£ç è§£è¦ãè§è§è¯­è¨æ¨¡åï¼VLMï¼ç¨äºæåè¯­è¨å¯¹é½çå¯éå¾åååµå¥ï¼ç¶åéè¿ä¸ä¸ªåæ°æ å³çå¾ååèåæºå¶çæåºåè¡¨ç¤ºï¼æåç±ä¸ä¸ªä»å¨ææ¬æ°æ®ä¸è®­ç»çé¶æ ·æ¬ææ¬è§£ç å¨çæå­å¹ã
*   <strong>æ¨¡æé´éç¼è§£ç­ç¥ï¼</strong> è®ºæåæå¹¶éç¨äºä¸¤ç§æ¨¡æé´éç¼è§£ç­ç¥ï¼åºäºè®°å¿çæ½å¨æå½±ï¼memory-based latent projectionï¼åè®­ç»æ¶æ³¨å¥åªå£°ï¼noise injectionï¼ï¼ä»¥ä½¿ææ¬è§£ç å¨è½å¤å¤çè§è§åµå¥ã
*   <strong>è§è§éª¨å¹²ç½ç»çéè¦æ§åæï¼</strong> è®ºææ·±å¥ç ç©¶äºä¸åé¢è®­ç»è§è§è¯­è¨éª¨å¹²ç½ç»ï¼ç¹å«æ¯DINOç³»åæ¨¡åï¼å¨çæææä¹çå¯éè§è§ç¹å¾æ¹é¢çä½ç¨ï¼åç°DINOv2-basedæ¨¡åå¨å±é¨è¯­ä¹è¡¨ç¤ºæ¹é¢è¡¨ç°åºè²ã
*   <strong>å¼å¥æ°çä»»å¡ï¼è½¨è¿¹å­å¹çæï¼Trace Captioningï¼ã</strong> ä¸ºäºå±ç¤ºæ¡æ¶ççµæ´»æ§ï¼è®ºæå¼å¥äºæ ¹æ®ç¨æ·é¼ æ è½¨è¿¹çæå­å¹çæ°ä»»å¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>åè¶çæ§è½ï¼</strong> Patch-ionerå¨é¶æ ·æ¬å¯éå­å¹ï¼dense captioningï¼ãåºåéå­å¹ï¼region-set captioningï¼ä»¥åæ°å¼å¥çè½¨è¿¹å­å¹çæä»»å¡ä¸ï¼åæ¾èä¼äºç°æåºçº¿åæåè¿çç«äºæ¨¡åã
*   <strong>å¨å±ä»»å¡çç«äºåï¼</strong> å¨æ´ä¸ªå¾åå­å¹çæä»»å¡ä¸ï¼Patch-ionerä¹è¡¨ç°åºä¸æåè¿çé¶æ ·æ¬å¾åå­å¹çæå¨ç¸å½çç«äºåã
*   <strong>éª¨å¹²ç½ç»çå³é®ä½ç¨ï¼</strong> å®éªè¯æï¼è½å¤çæææä¹ãå¯éçè§è§ç¹å¾çéª¨å¹²ç½ç»ï¼å¦DINOï¼å¯¹äºå¨å¤åºåå­å¹ä»»å¡ä¸­å®ç°æåè¿çæ§è½è³å³éè¦ã
*   <strong>é«ææ§ï¼</strong> è¯¥æ¹æ³åªéå¯¹è§è§éª¨å¹²ç½ç»è¿è¡ä¸æ¬¡ååä¼ æ­å³å¯æåæ´ä¸ªå¾åçå¾ååç¹å¾ï¼è¿äºç¹å¾å¯ä»¥éå¤ç¨äºå¯¹å¤ä¸ªåºåè¿è¡å­å¹çæï¼æé«äºå®éåºç¨çæçã
*   <strong>ç»ä¸æ§ï¼</strong> æ¡æ¶æåå°å¼¥åäºå±é¨åå¨å±çè§£ä¹é´çé¸¿æ²ï¼ä¸ºå¤ç²åº¦å­å¹ä»»å¡æä¾äºä¸ä¸ªç»ä¸çé¶æ ·æ¬è§£å³æ¹æ¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>ä¸å¨çç£æ¹æ³çå·®è·ï¼</strong> å°½ç®¡å¨é¶æ ·æ¬è®¾ç½®ä¸è¡¨ç°å¼ºå²ï¼ä½æ¨¡åæ§è½ä»è½åäºå¨çç£ãä»»å¡ç¹å®çæ¹æ³ã
*   <strong>ä¸ä¸æèå´åºå®ï¼</strong> æ¯ä¸ªå¾ååçä¸ä¸æèå´ç±éª¨å¹²ç½ç»å³å®ï¼æ æ³æ ¹æ®ç¨æ·æå¾è¿è¡è°æ´ã
*   <strong>æ¨¡æè·³è·å¼å¥åªå£°ï¼</strong> æ¨¡æé´éå¯è½å¼å¥åªå£°ï¼å¯¼è´å¹»è§ï¼hallucinationsï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>å¼±çç£çæ´åï¼</strong> æªæ¥å·¥ä½å¯ä»¥èèæ´åå¼±çç£ï¼ä¾å¦å¾åçº§å«çå­å¹æå¤±ï¼ä»¥æ¹è¿å¯¹æ¯å­¦ä¹ è¡¨ç¤ºä¸­çå¾ååçº§å«è¯­ä¹ã
*   <strong>ä¼åå¾ååå°ææ¬çæå½±ï¼</strong> è¿ä¸æ­¥ä¼åå¾ååå°ææ¬çæå½±æºå¶ï¼ä»¥è¿ä¸æ­¥åå°é¶æ ·æ¬è®¾ç½®ä¸çæ¨¡æé´éã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿æåºPatch-ioneræ¡æ¶ï¼æåå°å°é¶æ ·æ¬å­å¹çæçç¦ç¹ä»æ´ä¸ªå¾åè½¬ç§»å°å¾ååï¼ä»èå®ç°äºå¯¹å¾åä»»æåºåè¿è¡ç»ç²åº¦æè¿°çè½åï¼å¹¶å¨å¤ä¸ªåºåçº§å­å¹ä»»å¡ä¸­åå¾äºæ¾èè¿å±ï¼ä¸ºè¯¥é¢åå¼è¾äºæ°çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision.</li>
<li>We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.</li>
<li>Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks.</li>
<li>Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02898v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02898v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02803v1'></a></p>
<h2 id="work-zones-challenge-vlm-trajectory-planning-toward-mitigation-and-robust-autonomous-driving"><a href="https://arxiv.org/abs/2510.02803v1">Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Yifan Liao, Zhen Sun, Xiaoyun Qiu, Zixiao Zhao, Wenbing Tang, Xinlei He, Xinhu Zheng, Tianwei Zhang, Xinyi Huang, Xingshuo Han</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in <script type="math/tex">68.0%</script> of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of <script type="math/tex">8</script> common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around <script type="math/tex">3\times</script> in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time (<script type="math/tex">0.58</script>s) compared with other methods such as fine-tuning
(<script type="math/tex">17.90</script>s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºYifan Liaoç­äººæ°åçè®ºæâWork Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Drivingâçå¨é¢æè¦ï¼ç¨ä¸­æåæï¼</p>
<p><strong>è®ºææè¦ï¼å·¥ä½åºå¯¹VLMè½¨è¿¹è§åçææï¼ç¼è§£ä¸é²æ£èªå¨é©¾é©¶</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è§è§è¯­è¨æ¨¡åï¼VLMsï¼å¨èªå¨é©¾é©¶ä¸­ï¼ç¹å«æ¯å¨å¤æä¸å¨æååçå·¥ä½åºï¼work zonesï¼è¿è¡è½¨è¿¹è§åæ¶æé¢ä¸´çææãå°½ç®¡VLMså¨å¤æ¨¡ææ¨çæ¹é¢è¡¨ç°åºè²ï¼ä½å¶å¨å·¥ä½åºè¿ç§åå«ä¸è§åå¸å±ãä¸´æ¶äº¤éç®¡å¶åå¨æå ä½ç»æçç¯å¢ä¸­çè½¨è¿¹è§åè½åå°æªå¾å°ååæ¢ç´¢ãç ç©¶åç°ï¼ä¸»æµVLMså¨68.0%çæåµä¸æ æ³çææ­£ç¡®çè½¨è¿¹ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäºä¸ä¸ªåä¸º<strong>REACT-Drive</strong>çè½¨è¿¹è§åæ¡æ¶ï¼è¯¥æ¡æ¶å°VLMsä¸æ£ç´¢å¢å¼ºçæï¼RAGï¼ç¸ç»åãå¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>é¦æ¬¡ç³»ç»æ§ç ç©¶ï¼</strong> å¯¹VLMså¨å·¥ä½åºè½¨è¿¹è§åä¸­çè¡¨ç°è¿è¡äºé¦æ¬¡ç³»ç»æ§ç ç©¶ï¼å¹¶æ­ç¤ºäºå¶æ¾èçå¤±è´¥çã
*   <strong>å¤±è´¥æ¨¡å¼åæï¼</strong> éè¿å­å¾ææåèç±»åæï¼è¯å«å¹¶äººå·¥éªè¯äº8ç§å¸¸è§å¤±è´¥æ¨¡å¼ï¼æ·±å¥çè§£äºVLMå¤±è´¥çæ ¹æ¬åå ã
*   <strong>REACT-Driveæ¡æ¶ï¼</strong>
    *   <strong>ç¦»çº¿é¶æ®µï¼</strong> å°åå²å¤±è´¥æ¡ä¾è½¬åä¸ºçº¦æè§ååå¯æ§è¡çè½¨è¿¹ç¼è§£ä»£ç ï¼å¹¶éè¿èªéªè¯æºå¶ç¡®ä¿å¶å¯ç¨æ§ï¼æå»ºäºä¸ä¸ªå¯æç´¢çå¤±è´¥æ¡ä¾ç¼è§£ä»£ç æ°æ®åºã
    *   <strong>å¨çº¿é¶æ®µï¼</strong> å©ç¨RAGä»æ°æ®åºä¸­æ£ç´¢ä¸æ°åºæ¯ç¸ä¼¼çå¤±è´¥æ¨¡å¼ï¼å¹¶æ§è¡ç¸åºçç¼è§£ä»£ç æ¥æå¯¼è½¨è¿¹çæï¼ç¡®ä¿è½¨è¿¹ç¬¦åå®å¨è¦æ±åäº¤éè§åã
*   <strong>æçä¼åï¼</strong> REACT-Driveéè¿éç¨ç¼è§£ä»£ç ï¼æ¾èéä½äºæ¨çæ¶é´ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èéä½è¯¯å·®ï¼</strong> å¨ROADWorkæ°æ®éä¸çå®éªç»æè¡¨æï¼REACT-Driveç¸è¾äºä½¿ç¨Qwen2.5-VLè¯ä¼°çVLMåºçº¿ï¼å¹³åä½ç§»è¯¯å·®ï¼ADEï¼éä½äºçº¦3åã
*   <strong>æ¨çæ¶é´ä¼å¿ï¼</strong> REACT-Driveå®ç°äºæä½çæ¨çæ¶é´ï¼0.58ç§ï¼ï¼è¿ä½äºå¶ä»æ¹æ³ï¼å¦å¾®è°ç17.90ç§ï¼ï¼è¿å¯¹äºå®æ¶èªå¨é©¾é©¶è³å³éè¦ã
*   <strong>ç©çä¸çéªè¯ï¼</strong> å¨15ä¸ªçå®ä¸çå·¥ä½åºåºæ¯ä¸­è¿è¡çå®è½¦å®éªè¿ä¸æ­¥è¯æäºREACT-Driveçå¼ºå¤§å®ç¨æ§åé²æ£æ§ï¼ç¢°æçï¼CRï¼éè³0.0ã
*   <strong>æ¨¡å¼è¦ççéè¦æ§ï¼</strong> å®éªè¡¨æï¼æ¨¡å¼å¤æ ·æ§å¨å®ç°æ³åæ¹é¢èµ·çå³é®ä½ç¨ï¼è¦ççå¤±è´¥æ¨¡å¼è¶å¤ï¼æ¨¡åå¤çæªç¥å·¥ä½åºæ¡ä¾çè½åè¶å¼ºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æéçåºæ¯è¦çï¼</strong> æ¬ç ç©¶æªè½ç³»ç»æ§å°æ¶µçææé¿å°¾åºæ¯ï¼ä¾å¦æç«¯å¤©æ°æå¤é´é©¾é©¶ä¸çå·¥ä½åºã
*   <strong>æéçæ°æ®éï¼</strong> è¯ä¼°ä¸»è¦åºäºROADWorkæ°æ®éåä½èæ¶éçç©çæ°æ®ï¼æªåå«å¶ä»å·¥ä½åºæ°æ®éï¼è¿ä¸»è¦æ¯ç±äºå¯è®¿é®æ°æ®ç¨ç¼ºã
*   <strong>ç¼ºä¹å®è½¦é¨ç½²ï¼</strong> REACT-Driveå°æªå¨çå®çèªå¨é©¾é©¶è½¦è¾ä¸é¨ç½²ï¼å ä¸ºå¨å·¥ä½åºè¿è¡æ­¤ç±»å®éªå­å¨æé«é£é©ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   æå»ºæ´å¤§è§æ¨¡ãæ´å¤æ ·åçå·¥ä½åºåºæ¯æ°æ®éï¼ä»¥è¿ä¸æ­¥æåæ¨¡åçæ³åè½åã
*   æ¢ç´¢å°REACT-Driveé¨ç½²å°çå®èªå¨é©¾é©¶è½¦è¾ä¸çå®å¨å¯è¡æ¹æ¡ï¼ä»¥å¼¥åä»¿çä¸ç°å®ä¸çä¹é´çå·®è·ã
*   ç»§ç»­ç ç©¶VLMså¨é¿å°¾åæç«¯å·¥ä½åºåºæ¯ä¸çè½¨è¿¹è§åè½åã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæé¦æ¬¡ç³»ç»æ§å°æ­ç¤ºäºVLMså¨å·¥ä½åºè½¨è¿¹è§åä¸­çä¸è¶³ï¼å¹¶éè¿æåºREACT-Driveæ¡æ¶ï¼ææå°å©ç¨åå²å¤±è´¥ç»éªåRAGæºå¶ï¼æ¾èæåäºèªå¨é©¾é©¶ç³»ç»å¨å¤æå·¥ä½åºç¯å¢ä¸­çé²æ£æ§åæçï¼ä¸ºæªæ¥èªå¨é©¾é©¶ç³»ç»çå®å¨æ§åå¯é æ§æä¾äºæä»·å¼çè§£å³æ¹æ¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG).</li>
<li>Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02803v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02803v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02790v1'></a></p>
<h2 id="maskcd-mitigating-lvlm-hallucinations-by-image-head-masked-contrastive-decoding"><a href="https://arxiv.org/abs/2510.02790v1">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a></h2>
<p><strong>Authors:</strong> Jingyuan Deng, Yujiu Yang</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jingyuan DengåYujiu Yangæ°åçè®ºæâMaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decodingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="maskcd-lvlm">MaskCD: éè¿å¾åå¤´æ©è½å¯¹æ¯è§£ç ç¼è§£LVLMå¹»è§ç°è±¡</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤§åè§è§-è¯­è¨æ¨¡åï¼LVLMsï¼ä¸­æ®éå­å¨çâå¹»è§âç°è±¡ãå¹»è§æçæ¯LVLMsçæä¸å¶è¾å¥è§è§åææ¬åå®¹ç¸çç¾çä¿¡æ¯ï¼ä¾å¦çæä¸å­å¨çç©ä½ãéè¯¯æè¿°å±æ§æäº§çæ æä¹çå¥å­ãç°æçç¼è§£æ¹æ³ï¼å¦å¯¹æ¯è§£ç ï¼CDï¼åæ³¨æåæä½ï¼å­å¨åèªçå±éæ§ï¼CDæ¹æ³é¾ä»¥æå»ºåéçå¯¹æ¯æ ·æ¬ï¼èæ³¨æåæä½æ¹æ³åé«åº¦ææä¸ç¼ºä¹ç¨³å®æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäº<strong>å¾åå¤´æ©è½å¯¹æ¯è§£ç ï¼MaskCDï¼</strong>æ¹æ³ï¼å¶æ ¸å¿åæ°ç¹å¨äºï¼
*   <strong>è¯å«âå¾åå¤´âï¼</strong> éè¿åæLVLMsä¸­æ³¨æåæºå¶çåé¨å·¥ä½åçï¼ä½èåç°å¹¶è¯å«åºæ¨¡åä¸­é£äºå¾åäºå¯¹å¾åtokenåéé«æ³¨æåçâå¾åå¤´âãè¿äºå¾åå¤´è¢«è®¤ä¸ºæ¯å¤çè§è§ä¿¡æ¯çå³é®é¨åã
*   <strong>æå»ºå¯¹æ¯æ ·æ¬ï¼</strong> MaskCDå©ç¨è¿äºè¯å«åºçâå¾åå¤´âæ¥æå»ºâåæ ·æ¬âãå·ä½æ¥è¯´ï¼å¨çæåæ ·æ¬çæ¨çé¶æ®µï¼éè¿æ©è½ï¼å°æ³¨æåè¾åºè®¾ç½®ä¸ºé¶ï¼è¿äºå¾åå¤´ï¼ä»èé»æ­¢åæ ·æ¬è®¿é®æç¨çè§è§ä¿¡æ¯ãè¿ç§æ¹æ³ç¡®ä¿äºåå»çæ ·æ¬åªåå«éè¦æµæ¶çæ æä¿¡æ¯ã
*   <strong>ç»åå¯¹æ¯è§£ç ï¼</strong> MaskCDå°è¿ç§å¾åå¤´æ©è½æºå¶ä¸å¯¹æ¯è§£ç ç¸ç»åãéè¿ä»åå§æ ·æ¬çè¾åºlogitsä¸­åå»åæ ·æ¬çè¾åºlogitsï¼æ¨¡åè½å¤æ´ç²¾ç¡®å°å©ç¨çæ­£æç¨çè§è§åææ¬ä¿¡æ¯ï¼ä»èç¼è§£å¹»è§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
MaskCDå¨å¤ä¸ªåºåæµè¯ï¼CHAIRãPOPEãAMBERåMMEï¼ä¸å¯¹LLaVA-1.5-7båQwen-VL-7bæ¨¡åè¿è¡äºè¯ä¼°ï¼åå¾äºæ¾èææï¼
*   <strong>ææç¼è§£å¹»è§ï¼</strong> MaskCDå¨CHAIRè¯ä¼°ä¸­æ¾èéä½äºå¹»è§çï¼LLaVA-1.5-7bçCHAIR_såCHAIR_iåå«éä½äº19.12%å29.87%ï¼ï¼ä¼äºVCDãM3IDåOPERAç­ç°ææ¹æ³ã
*   <strong>ä¿æéç¨è½åï¼</strong> å¨POPEåMMEç­è¯ä¼°éç¨è½åçåºåæµè¯ä¸­ï¼MaskCDè¡¨ç°åºä¸OPERAç¸å½ææ´ä¼çæ§è½ï¼åæ¶ä¿çäºLVLMsçéç¨è½åï¼çè³å¨æäºå­éä¸æææåã
*   <strong>ç¨³å®æ§ä¸å®ç¨æ§ï¼</strong> æ¶èå®éªè¡¨æï¼MaskCDå¯¹è¶åæ°ï¼å¦éå¼Tåå¯¹æ¯å¼ºåº¦Î±ï¼å·æè¯å¥½çç¨³å®æ§ï¼å³ä½¿å¨è¾å¤§å¼ä¸ä¹è½ææç¼è§£å¹»è§ï¼è¯æäºå¶ä½ä¸ºCDæ¹æ³çå¯é æ§ã
*   <strong>å¾åå¤´éæ©çåçæ§ï¼</strong> å®éªè¯å®ï¼æ©è½âå¾åå¤´âæ¯éæºæ©è½å¶ä»å¤´æ´ææï¼è¿è¡¨æå¾åå¤´ç¡®å®åå«äºæ´å³é®çè§è§ä¿¡æ¯ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹å¦è¯äºMaskCDçå ä¸ªå±éæ§ï¼
*   <strong>æ¨çåå¾åå¤çï¼</strong> MaskCDéè¦æåä½¿ç¨å¾åè¿è¡æ¨çä»¥è·åå¾åå¤´çæ©ç ï¼è¿ä¼å ç¨ä¸å®çè®¡ç®èµæºã
*   <strong>æ¨¡åä¾èµæ§ï¼</strong> è·åå°çæ©ç ä»éç¨äºç¸åå®¶æçLLMéª¨å¹²ç½ç»ãå¯¹äºæ°çLLMåºç¡æ¨¡åï¼éè¦éæ°è·åç¸åºçæ©ç ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºæé¼å±æªæ¥çç ç©¶æ¢ç´¢ï¼
*   <strong>å¨ææ©ç æå»ºï¼</strong> å¦ä½å¨æ¨¡åæä½è¿ç¨ä¸­å¨ææå»ºæ©ç ï¼ä»¥æè±å¯¹é¢åè·åæ©ç çéå¶ã
*   <strong>æ´å¹¿æ³çéç¨æ§ï¼</strong> å¯»æ¾è½å¤è·¨ä¸åLLMéª¨å¹²ç½ç»éç¨çå¹»è§ç¼è§£ç­ç¥ã</p>
<hr />
<p>æ»èè¨ä¹ï¼MaskCDéè¿åæ°æ§å°è¯å«å¹¶å©ç¨LVLMä¸­çâå¾åå¤´âæ¥æå»ºé«è´¨éçå¯¹æ¯æ ·æ¬ï¼ææç¼è§£äºæ¨¡åçå¹»è§ç°è±¡ï¼åæ¶ä¿æäºå¶éç¨è½åãè¿é¡¹å·¥ä½ä¸ºLVLMå¹»è§ç¼è§£æä¾äºä¸ä¸ªç¨³å®ä¸é«æçæ°è§è§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
work, we propose image head Masked Contrastive Decoding (MaskCD).</li>
<li>Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02790v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02790v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02778v1'></a></p>
<h2 id="adard-key-adaptive-relevance-diversity-keyframe-sampling-for-long-form-video-understanding"><a href="https://arxiv.org/abs/2510.02778v1">AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</a></h2>
<p><strong>Authors:</strong> Xian Zhang, Zexi Wu, Zinuo Li, Hongming Xu, Luqi Gong, Farid Boussaid, Naoufel Werghi, Mohammed Bennamoun</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xian Zhangç­äººæ°åçè®ºæâAdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understandingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="adard-key-adaptive-relevance-diversity-keyframe-sampling-for-long-form-video-understanding_1">è®ºææè¦ï¼AdaRD-Key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³é¿è§é¢çè§£ä¸­çå³é®ææãç°æçè§è§-è¯­è¨æ¨¡åï¼VLMsï¼åå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼å¨å¤çé¿è§é¢æ¶ï¼éå¸¸éç¨ååéæ ·æåºäºåºå®æ¶é´é´éçå³é®å¸§éæ©æ¹æ³ãè¿äºæ¹æ³å¾å¾ä¼å¿½ç¥è§é¢ä¸­çå³é®æ¶å»ï¼å¯¼è´å¯¹æ¥è¯¢çååºä¸åç¡®ï¼æèå¨è¿½æ±è§è§å¤æ ·æ§æ¶å¿½è§äºæ¥è¯¢ç¸å³æ§ãå·ä½æ¥è¯´ï¼ååéæ ·å¯è½éè¿ä¸æ¥è¯¢ç¸å³çéè¦åå®¹ï¼èåºäºåºå®æ¶é´é´éçæ¹æ³è½ç¶åå°äºåä½ï¼ä½å¯è½éæ¼ç­æ¶ãç»ç²åº¦çéè¦äºä»¶ãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¨é¿è§é¢ä¸­é«æãåç¡®å°éæ©æ¢ä¸æ¥è¯¢é«åº¦ç¸å³åå·æè§è§å¤æ ·æ§çå³é®å¸§ï¼ä»¥æåè§é¢çè§£æ¨¡åçæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
AdaRD-Keyæåºäºä¸ä¸ªæ éè®­ç»ãæ¥è¯¢é©±å¨çå³é®å¸§éæ ·æ¨¡åï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>èåç¸å³æ§-å¤æ ·æ§æå¤§åä½ç§¯ï¼RD-MVï¼ç®æ ï¼</strong> AdaRD-Keyå¼å¥äºRD-MVç®æ å½æ°ï¼è¿æ¯é¦ä¸ªèåä¼åæ¥è¯¢ç¸å³æ§ååµå¥ç©ºé´å¤æ ·æ§çå³é®å¸§éæ ·æ¹æ³ãå®ç»åäºæ¥è¯¢æ¡ä»¶ä¸çç¸å³æ§åæ°åå¯¹æ°è¡åå¼å¤æ ·æ§åéï¼ä»¥éæ©æ¢ä¿¡æ¯ä¸°å¯åéåä½çå¸§ãè¿ç§æ¹æ³éè¿æå¤§åæéå¸§ç¹å¾åéçGramç©éµçå¯¹æ°è¡åå¼æ¥å ä½å°è¡¨ç¤ºå¤æ ·æ§ï¼ç¡®ä¿äºæéå¸§å¨è¯­ä¹ä¸å·æåºååº¦ã</li>
<li><strong>åå¼æ§-é¢ç®ç¼©æ¾ï¼VB-Scaleï¼ï¼</strong> ä¸ºäºéåºä¸åè§é¢é¿åº¦ååæ°åå¸çæ¥è¯¢ç¹æ§ï¼AdaRD-Keyå¼å¥äºVB-Scaleæºå¶ãè¯¥æºå¶æ ¹æ®ç¸å³æ§åæ°çåå¼æ§ï¼âå³°å¼âæâå¹³å¦âï¼åå¸§é¢ç®æ¯ï¼å³æ¯éæ©æ§½ä½çåéå¸§æ°éï¼å¨æè°æ´ç¸å³æ§ä¸å¤æ ·æ§ä¹é´çæè¡¡åæ°Î»ãå½ç¸å³æ§åå¸è¾å°éæ¶ï¼æ¨¡åæ´ä¾§éç¸å³æ§ï¼å½åå¸è¾å¹³å¦æåæ£æ¶ï¼å¤æ ·æ§åå¾æ´éè¦ã</li>
<li><strong>è½»éçº§ç¸å³æ§æç¥é¨æ§æºå¶ï¼Lightweight Relevance-Aware Gatingï¼ï¼</strong> ä¸ºå¤çä¸è§é¢å¯¹é½è¾å¼±çå®½æ³æ¥è¯¢ï¼AdaRD-Keyéç¨äºä¸ä¸ªè½»éçº§çç¸å³æ§æç¥é¨æ§æºå¶ãå½ç¸å³æ§åå¸è¡¨æä¸è§é¢å¯¹é½è¾å¼±æ¶ï¼ä¾å¦ï¼æå¤§ç¸å³æ§åæ°ä½äºéå¼ï¼ï¼è¯¥æ¹æ³ä¼æ ç¼åæ¢å°ä»å¤æ ·æ§æ¨¡å¼ï¼ä»èå¨æ éé¢å¤çç£çæåµä¸å¢å¼ºè¦çèå´ï¼é¿åæ¾å¤§åªå£°ã</li>
<li><strong>å³æå³ç¨é¨ç½²ä¸é«ææ§ï¼</strong> æ´ä¸ªæµç¨æ éè®­ç»ï¼è®¡ç®æçé«ï¼å¨åä¸ªGPUä¸å®æ¶è¿è¡ï¼ï¼å¹¶ä¸å¯ä»¥å³æå³ç¨å°å¼å®¹ç°æVLMsï¼æ éå¾®è°ææ¶æä¿®æ¹ï¼ä¾¿äºè·¨æ°æ®éãé¢ååä»»å¡çæ ç¼éæã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
AdaRD-Keyå¨LongVideoBenchåVideo-MMEç­é¿è§é¢çè§£åºåæµè¯ä¸åå¾äºæåè¿çæ§è½ï¼å°¤å¶å¨é¿è§é¢ä¸è¡¨ç°çªåºã</p>
<ul>
<li><strong>LongVideoBenchä¸çæ§è½æåï¼</strong> å¨32å¸§é¢ç®ä¸ï¼AdaRD-Keyå°Qwen2-VLçæ´ä½åç¡®çæåè³60.8%ï¼æ¯M-LL Selectoré«åº3.8ä¸ªç¾åç¹ï¼æ¯AKSé«åº0.3ä¸ªç¾åç¹ãå¨64å¸§é¢ç®ä¸ï¼AdaRD-Keyçåç¡®çè¾¾å°62.9%ï¼æ¯MAXINFOé«åº1.4ä¸ªç¾åç¹ï¼æ¯AKSé«åº0.2ä¸ªç¾åç¹ãå¨3-10åéè§é¢ç±»å«ä¸­ï¼AdaRD-Keyæ¯AKSæåäº1.3%ã</li>
<li><strong>Video-MMEä¸çé²æ£å¢çï¼</strong> å¨32å¸§é¢ç®ä¸ï¼AdaRD-Keyå°Qwen2-VLçæ´ä½åæ°æåè³60.7%ï¼æ¯åºçº¿é«3.1ä¸ªç¾åç¹ï¼æ¯Q-Frameé«2.4ä¸ªç¾åç¹ï¼æ¯AKSé«0.8ä¸ªç¾åç¹ãå¨é¿è§é¢ï¼30-60åéï¼ä¸­ï¼åç¡®çæåè³51.9%ï¼æ¯åºçº¿é«4.5ä¸ªç¾åç¹ï¼æ¯Q-Frameé«3.6ä¸ªç¾åç¹ï¼æ¯AKSé«0.8ä¸ªç¾åç¹ã</li>
<li><strong>è§é¢å­å¹ä»»å¡çæ¹è¿ï¼</strong> å¨VCapsBenchä¸ï¼AdaRD-Keyä¹æ¾èæåäºè§é¢å­å¹çæ§è½ãå¨4å¸§éæ ·ä¸ï¼Qwen-2.5VLç»åAdaRD-Keyåï¼åç¡®çï¼ARï¼ä»44.86æåè³52.41ï¼+7.55ï¼ï¼ä¸ä¸è´çï¼IRï¼éä½9.51ï¼è¦ççï¼CRï¼æå2.25ã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> éæ­¥æ·»å ç¸å³æ§ãå¤æ ·æ§ãè½»éçº§ç¸å³æ§æç¥é¨æ§ååå¼æ§-é¢ç®ç¼©æ¾æ¨¡åï¼åå¸¦æ¥äºæ§è½çæç»­æåï¼å°¤å¶å¨é¿è§é¢ä¸æææ´æ¾èãè¿è¡¨æAdaRD-Keyçåä¸ªç»ä»¶é½å¯¹æåæ§è½ååºäºè´¡ç®ã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼AdaRD-Keyè½å¤ææå°ä»é¿è§é¢ä¸­æååºä¸æ¥è¯¢ç¸å³ä¸å·æå¤æ ·æ§çå³é®ä¿¡æ¯ï¼æ¾èæåäºä¸æ¸¸VLMå¨è§é¢é®ç­åå­å¹ä»»å¡ä¸­çæ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåºAdaRD-Keyæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»å¶è®¾è®¡åä¸ä¸æå¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼</p>
<ul>
<li><strong>BLIP-2ç¹å¾çä¾èµæ§ï¼</strong> AdaRD-Keyä¾èµäºBLIP-2æåå¸§çº§è¯­ä¹ç¹å¾åæ¥è¯¢ç¸å³æ§åæ°ãå¦æåºå±VLMï¼å¦BLIP-2ï¼çç¹å¾æåè½åæéæå­å¨åå·®ï¼å¯è½ä¼å½±åAdaRD-Keyéæ©å³é®å¸§çè´¨éã</li>
<li><strong>è¶åæ°æææ§ï¼</strong> å°½ç®¡VB-Scaleæºå¶æ¨å¨èªéåºå°è°æ´å¤æ ·æ§æéÎ»ï¼ä½AminãAmaxãÎ±åPcapç­è¶åæ°çè®¾ç½®ä»å¯è½å½±åæ§è½ï¼å°¤å¶æ¯å¨ç¹å®ææç«¯è§é¢åºæ¯ä¸ã</li>
<li><strong>âå¼±å¯¹é½âçå®ä¹ï¼</strong> è½»éçº§ç¸å³æ§æç¥é¨æ§æºå¶ä¾èµäºâå¼±å¯¹é½âçå¤æ­ï¼ä¾å¦ï¼åºäºæå¤§ç¸å³æ§åæ°éå¼Ïï¼ãè¿ä¸ªéå¼çè®¾ç½®å¯è½éè¦æ ¹æ®å·ä½åºç¨è¿è¡è°æ´ï¼ä¸å½çè®¾ç½®å¯è½å¯¼è´å¨æäºæåµä¸éè¯¯å°åæ¢å°ä»å¤æ ·æ§æ¨¡å¼ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> å°½ç®¡è®ºæå¼ºè°å¶è®¡ç®æçé«ï¼å®æ¶è¿è¡å¨åä¸ªGPUä¸ï¼ï¼ä½å¯¹äºè¶é¿è§é¢æéè¦å¤çå¤§éè§é¢çåºæ¯ï¼å¸§ç¹å¾çç¼å­åGramç©éµçæ´æ°ä»ç¶å¯è½å¸¦æ¥ä¸å®çåå­åè®¡ç®å¼éã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºè®ºæçè´¡ç®åæ½å¨å±éæ§ï¼æªæ¥ç ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸æ¹åï¼</p>
<ul>
<li><strong>æ´åè¿çç¹å¾æåå¨ï¼</strong> æ¢ç´¢ä½¿ç¨æ´åè¿çãéå¯¹é¿è§é¢ä¼åçè§è§-è¯­è¨æ¨¡åä½ä¸ºç¹å¾æåå¨ï¼ä»¥è·åæ´ä¸°å¯ãæ´é²æ£çå¸§çº§è¯­ä¹è¡¨ç¤ºï¼ä»èè¿ä¸æ­¥æåå³é®å¸§éæ©çåç¡®æ§ã</li>
<li><strong>èªéåºè¶åæ°ä¼åï¼</strong> å¼åæ´æºè½çæºå¶ï¼è½å¤æ ¹æ®è§é¢åå®¹ãæ¥è¯¢ç±»åæç¹å®ä»»å¡ï¼èªå¨ä¼åVB-Scaleä¸­çè¶åæ°ï¼åå°å¯¹äººå·¥ç»éªè®¾ç½®çä¾èµã</li>
<li><strong>å¤æ¨¡æèåï¼</strong> å°½ç®¡è®ºæä¸»è¦å³æ³¨è§è§å¸§ï¼ä½é¿è§é¢éå¸¸åå«é³é¢ãææ¬ï¼å¦å­å¹ï¼ç­å¤æ¨¡æä¿¡æ¯ãæªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢å¦ä½å°AdaRD-Keyæ©å±å°å¤æ¨¡æå³é®ä¿¡æ¯éæ ·ï¼ä»¥æ´å¨é¢å°çè§£è§é¢åå®¹ã</li>
<li><strong>å®æ¶æ§ä¸è¾¹ç¼é¨ç½²ï¼</strong> è¿ä¸æ­¥ä¼åç®æ³çè®¡ç®æçååå­å ç¨ï¼ä½¿å¶è½å¤æ´å¥½å°éåºèµæºåéçè¾¹ç¼è®¾å¤æå¯¹å®æ¶æ§è¦æ±æ´é«çåºç¨åºæ¯ã</li>
<li><strong>ç¨æ·åé¦éæï¼</strong> æ¢ç´¢å°ç¨æ·åé¦ï¼ä¾å¦ï¼ç¨æ·å¯¹æéå³é®å¸§çæ»¡æåº¦ï¼éæå°å³é®å¸§éæ©è¿ç¨ä¸­ï¼ä»¥å®ç°æ´ä¸ªæ§ååç¨æ·é©±å¨çè§é¢çè§£ã</li>
<li><strong>æ´å¤æçæ¥è¯¢ç±»åï¼</strong> éå¯¹æ´å¤æãæ´æ½è±¡çæ¥è¯¢ç±»åï¼ä¾å¦æ¶åæ¨çãé¢æµæææåæçæ¥è¯¢ï¼è¿ä¸æ­¥æåAdaRD-Keyå¨è¿äºåºæ¯ä¸çæ§è½ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding.</li>
<li>Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02778v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02778v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02722v1'></a></p>
<h2 id="mogic-boosting-motion-generation-via-intention-understanding-and-visual-context"><a href="https://arxiv.org/abs/2510.02722v1">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a></h2>
<p><strong>Authors:</strong> Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹Junyu Shiç­äººæ°åçè®ºæâMoGIC: Boosting Motion Generation via Intention Understanding and Visual Contextâè¿è¡äºå¨é¢åæãä»¥ä¸æ¯è¯¥è®ºæçæè¦ï¼</p>
<p><strong>è®ºææè¦ï¼MoGICï¼éè¿æå¾çè§£åè§è§ä¸ä¸ææåè¿å¨çæ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçææ¬é©±å¨è¿å¨çææ¹æ³éå¸¸å°åæè§ä¸ºè¯­è¨åè¿å¨ä¹é´çååæ å°ï¼ä½æªè½ææææå¨ä½æ§è¡çå æé»è¾åé©±å¨è¡ä¸ºçäººç±»æå¾ãæ­¤å¤ï¼ç¼ºä¹è§è§åºç¡éå¶äºçæçç²¾ç¡®æ§åä¸ªæ§åï¼å ä¸ºä»å­è¯­è¨é¾ä»¥æå®ç»ç²åº¦çæ¶ç©ºç»èãè¿å¯¼è´çæçè¿å¨ç¼ºä¹çå®æãå¯æ§æ§å·®ï¼å¹¶ä¸é¾ä»¥æ³åå°æ´å¹¿æ³çä»»å¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>MoGICç»ä¸æ¡æ¶ï¼</strong> è®ºææåºäºMoGICï¼ä¸ä¸ªç»ä¸çæ¡æ¶ï¼å°æå¾å»ºæ¨¡åè§è§åéªæ´åå°å¤æ¨¡æè¿å¨åæä¸­ãéè¿èåä¼åå¤æ¨¡ææ¡ä»¶è¿å¨çæåæå¾é¢æµï¼MoGICè½å¤æ­ç¤ºæ½å¨çäººç±»ç®æ ï¼å©ç¨è§è§åéªå¢å¼ºçæï¼å¹¶å±ç°å¤åè½çå¤æ¨¡æçæè½åã
*   <strong>æå¾é¢æµå¤´ï¼IPHï¼ä¸è¿å¨çæå¤´ï¼MGHï¼è§£è¦ï¼</strong> MoGICéè¿è§£è¦ççæå¤´ï¼æå¾é¢æµå¤´è¾åºç¦»æ£çæå¾æè¿°ï¼è¿å¨çæå¤´çæè¿ç»­è½¨è¿¹ï¼æ¥æç¡®å»ºæ¨¡äººç±»æå¾ï¼é¿åäºè¯­ä¹æ··æ·ã
*   <strong>æ··åæ³¨æåæºå¶ï¼Mixture-of-Attentionï¼ï¼</strong> å¼å¥äºå·æèªéåºèå´çæ··åæ³¨æåæºå¶ï¼ä»¥å®ç°æ¡ä»¶ä»¤çåè¿å¨å­åºåä¹é´ææçå±é¨å¯¹é½ï¼ä»èå¤çé¨åå¯¹åºå³ç³»åæ¶é´éä½é®é¢ã
*   <strong>Mo440Håºåæ°æ®éï¼</strong> è®ºæç­åå¹¶èªå¨æ æ³¨äºMo440Hï¼ä¸ä¸ªåå«440å°æ¶é«è´¨éè¿å¨æ°æ®ï¼æ¥èª21ä¸ªæ°æ®éï¼çå¤§è§æ¨¡åºåï¼æ¶µçåäººæ´»å¨ãäººæºäº¤äºåäºº-ç©äº¤äºï¼æ¯æä¸æ¨¡æå­¦ä¹ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èæåè¿å¨çæè´¨éï¼</strong> å¨HumanML3DåMo440Hæ°æ®éä¸ï¼ç»è¿å¾®è°åï¼MoGICçFIDï¼FrÃ©chet Inception Distanceï¼åå«éä½äº38.6%å34.6%ï¼è¡¨æå¶çæçè¿å¨æ´å·çå®æåå¤æ ·æ§ã
*   <strong>è¶è¶LLMåºçº¿ï¼</strong> å¨è¿å¨æè¿°ä»»å¡ä¸­ï¼MoGICå­åè½»éçº§ææ¬å¤´è¶è¶äºåºäºLLMçæ¹æ³ï¼è¯æäºå¶å¨åæ°è¾å°çæåµä¸ä»è½ä¿æç«äºåã
*   <strong>å®ç°æå¾é¢æµåè§è§æ¡ä»¶çæï¼</strong> MoGICä¸ä»è½çæè¿å¨ï¼è¿è½é¢æµæ½å¨æå¾ï¼å¹¶æ¯æè§è§æ¡ä»¶ä¸çè¿å¨çæï¼å¦å¾åå°è¿å¨åæãè§è§æ¡ä»¶è¿å¨è¡¥å¨ï¼ï¼æå¤§å°æåäºè¿å¨åæçå¯æ§æ§åæå¾çè§£è½åã
*   <strong>æ··åæ³¨æåæºå¶çæææ§ï¼</strong> æ¶èå®éªè¡¨æï¼æ··åæ³¨æåæºå¶æ¾èæåäºæ£ç´¢æ§è½ï¼å¹¶ä½¿æ¨¡åè½å¤çææ´ç²¾ç¡®çå±é¨è¿å¨ååºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåMoGICæ¨¡åçå·ä½å±éæ§ãç¶èï¼ä»å¶å¼ºè°æå¾çè§£åè§è§ä¸ä¸ææ¥è§£å³ç°ææ¹æ³çä¸è¶³ï¼å¦ç¼ºä¹å æé»è¾ãç²¾ç¡®æ§ãä¸ªæ§åï¼æ¥çï¼å¯ä»¥æ¨æ­åºè¿äºæ¯ç°ææ¹æ³çæ®éå±éï¼èMoGICæ¨å¨åæå®ä»¬ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´ç²¾ç¡®ãèªéåºåæå¾æç¥çè¿å¨åæï¼</strong> MoGICä¸ºæªæ¥ç ç©¶å¥ å®äºåºç¡ï¼å¯ä»¥è¿ä¸æ­¥æ¢ç´¢å¦ä½å®ç°æ´ç²¾ç¡®ãèªéåºåæå¾æç¥çè¿å¨åæã
*   <strong>æ©å±å°æ´å¤æçäº¤äºååºæ¯ï¼</strong> é´äºMo440Hæ°æ®éæ¶µçäºäººæºäº¤äºåäºº-ç©äº¤äºï¼æªæ¥å·¥ä½å¯ä»¥è¿ä¸æ­¥æ¢ç´¢MoGICå¨æ´å¤æãå¤ä¸»ä½ãå¤å¯¹è±¡åºæ¯ä¸­çåºç¨ã
*   <strong>å®æ¶æ¨ççä¼åï¼</strong> å°½ç®¡MoGICå¨å°ééæ ·æ­¥æ°ä¸ä»è½ä¿æç«äºåï¼ä½è¿ä¸æ­¥ä¼åå®æ¶æ¨çéåº¦ä»¥æ»¡è¶³æ´ä¸¥æ ¼çåºç¨éæ±ä»æ¯ä¸ä¸ªæ¹åã
*   <strong>æ´æ·±å¥çå æç»æå»ºæ¨¡ï¼</strong> å°½ç®¡MoGICéè¿æå¾é¢æµææäºé¨åå æé»è¾ï¼ä½æªæ¥å¯ä»¥æ¢ç´¢æ´æ·±å±æ¬¡çå æç»æå»ºæ¨¡ï¼ä»¥æ´å¥½å°çè§£åé¢æµäººç±»è¡ä¸ºã</p>
<p>æ»èè¨ä¹ï¼MoGICéè¿æ´åæå¾å»ºæ¨¡åè§è§åéªï¼æ¾èæ¨å¨äºå¤æ¨¡æè¿å¨çæé¢åçåå±ï¼ä¸ºå®ç°æ´æºè½ãæ´å¯æ§çäººç±»è¿å¨åæåçè§£æä¾äºæ°çè§è§åå¼ºå¤§çå·¥å·ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02722v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02722v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.03198v1'></a></p>
<h2 id="memory-forcing-spatio-temporal-memory-for-consistent-scene-generation-on-minecraft"><a href="https://arxiv.org/abs/2510.03198v1">Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</a></h2>
<p><strong>Authors:</strong> Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, Li Jiang</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Junchao Huangç­äººæ°åçè®ºæâMemory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraftâçå¨é¢æè¦ã</p>
<hr />
<h3 id="memory-forcing-spatio-temporal-memory-for-consistent-scene-generation-on-minecraft_1">è®ºææè¦ï¼âMemory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraftâ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³èªåå½è§é¢æ©æ£æ¨¡åå¨Minecraftç­å¼æ¾ä¸çç¯å¢ä¸­è¿è¡åºæ¯çææ¶é¢ä¸´çæ ¸å¿ææãå·ä½æ¥è¯´ï¼æ¨¡åéè¦ï¼
*   å¨æ¢ç´¢æ°åºæ¯æ¶çæèªç¶åå®¹ã
*   å¨éæ°è®¿é®å·²æ¢ç´¢åºåæ¶ä¿æç©ºé´ä¸è´æ§ã
*   å¨æéçè®¡ç®é¢ç®ä¸ï¼ææåç¼©åå©ç¨åå²ä¿¡æ¯ã</p>
<p>ç°ææ¹æ³å­å¨ä¸ä¸ªæè¡¡ï¼ä»ä¾èµæ¶é´è®°å¿çæ¨¡åç¼ºä¹é¿æç©ºé´ä¸è´æ§ï¼èè¿åº¦ä¾èµç©ºé´è®°å¿ï¼å¨ç©ºé´ä¸ä¸æä¸è¶³æ¶ï¼å¯è½æå®³æ°åºæ¯ççæè´¨éãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¨æ¢ç´¢çµæ´»æ§åéè®¿ä¸è´æ§ä¹é´åå¾å¹³è¡¡ï¼å¹¶ææç®¡çæéä¸ä¸æçªå£åçæ¶ç©ºè®°å¿ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäºâMemory Forcingâæ¡æ¶ï¼éè¿ä»¥ä¸åæ°è§£å³äºä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>å ä½ç´¢å¼ç©ºé´è®°å¿ï¼Geometry-indexed Spatial Memoryï¼ï¼</strong> å¼å¥äºä¸ç§æ°çç©ºé´è®°å¿æºå¶ï¼éè¿æµå¼3Déå»ºç»´æ¤æ¾å¼3Dç¼å­ãå®å°å½åå¯è§ç¹æ å°åå¶æºå¸§ï¼ä»èå®ç°é«æçâç¹å°å¸§æ£ç´¢ï¼Point-to-Frame Retrievalï¼âï¼ä»¥éæ©ç´§åä¸ä¸å§¿æç¸å³çåå²è§å¾ãè¿ç§æ¹æ³æ¯åºäºå¤è§çæ£ç´¢æ´é²æ£ï¼å¹¶ä¸æ£ç´¢å¤æåº¦ä¸å¯è§ç©ºé´è¦çèå´èéåºåé¿åº¦ææ¯ä¾ï¼ä»èæé«äºè®¡ç®æçåå­å¨æçã</li>
<li><strong>æ··åè®­ç»ï¼Hybrid Trainingï¼ï¼</strong> è®¾è®¡äºä¸ç§è®­ç»åè®®ï¼éè¿æ¨¡æä¸åçæ¸¸æç©æ³æ¨¡å¼ï¼æ¢ç´¢åéè®¿ï¼ï¼æå¯¼æ¨¡åå¨æ¢ç´¢æ°åºæ¯æ¶ä¾èµæ¶é´è®°å¿ï¼å¨éè®¿æ¶ç»åç©ºé´è®°å¿ä»¥ä¿æä¸è´æ§ã</li>
<li><strong>é¾å¼ååè®­ç»ï¼Chained Forward Trainingï¼ï¼</strong> æ©å±äºèªåå½è®­ç»ï¼å¼å¥äºæ¨¡åæ¨æ¼ãå¨è¿ç§è®­ç»ä¸­ï¼æ¨¡åéæ­¥ç¨èªå·±çé¢æµæ¿æ¢çå®çæ¶é´ä¸ä¸æï¼ä»èäº§çæ´å¤§çå§¿æååï¼é¼å±æ¨¡åä¾èµç©ºé´è®°å¿æ¥ç»´æä¸è´æ§ï¼å¹¶åå°èªåå½æ¨çä¸­å¸¸è§çç´¯ç§¯è¯¯å·®ã</li>
<li><strong>è®°å¿å¢å¼ºæ¶æï¼</strong> å¨Diffusion Transformer (DiT)éª¨å¹²ä¸­éæäºç©ºé´è®°å¿æååè®°å¿äº¤åæ³¨æåæ¨¡åï¼å©ç¨å ä½ç´¢å¼çç©ºé´è®°å¿æä¾é¿æç©ºé´ä¸ä¸æã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
éè¿å¨Minecraftåºåæµè¯ä¸è¿è¡çå¤§éå®éªï¼Memory Forcingå±ç¤ºäºåè¶çæ§è½ï¼</p>
<ul>
<li><strong>é¿æç©ºé´ä¸è´æ§ï¼</strong> å¨éæ°è®¿é®å·²æ¢ç´¢åºåæ¶ï¼æ¨¡åè¡¨ç°åºä¼è¶çé¿æç©ºé´ä¸è´æ§ååºæ¯è¿è´¯æ§ï¼æ¾èä¼äºä»ä¾èµæ¶é´è®°å¿åç°æç©ºé´è®°å¿åºçº¿ã</li>
<li><strong>çæè´¨éï¼</strong> å¨æ°ç¯å¢ä¸­ï¼æ¨¡åå¨çææ§è½æ¹é¢ä¹ä¼äºææåºçº¿ï¼çæåå®¹æ´èªç¶ãæ´å·ååºæ§ï¼å¹¶è½æ´å¥½å°æ³åå°æªè§è¿çå°å½¢ã</li>
<li><strong>è®¡ç®æçï¼</strong> å ä½ç´¢å¼ç©ºé´è®°å¿å¨æ£ç´¢éåº¦ä¸æ¯WorldMemå¿«7.3åï¼åæ¶åå°äº98.2%çåå­å­å¨ï¼è¯æäºå¶å¨å¤çæ©å±åºåæ¶çè®¡ç®æçã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> è¯æäºæ··åè®­ç»åé¾å¼ååè®­ç»ç­ç¥ä»¥å3Då ä½æ£ç´¢æºå¶å¯¹æ¨¡åæ§è½çè´¡ç®ã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼Memory Forcingæåè§£å³äºçæè´¨éåé¿æè®°å¿ä¸è´æ§ä¹é´çæ ¸å¿æè¡¡ï¼å¹¶å¨ä¿æè®¡ç®æççåæ¶ï¼å¨å¤æ ·åç¯å¢ä¸­å®ç°äºåè¶çæ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹æåºäºå½åæ¹æ³çå±éæ§ï¼</p>
<ul>
<li><strong>é¢åç¹å¼æ§ï¼</strong> å½åå®ç°ä¸»è¦å¨Minecraftæ¸¸æåºæ¯ä¸­éªè¯ï¼å¯è½æ æ³ç´æ¥æ³åå°å¶ä»ç¯å¢ï¼éè¦è¿è¡é¢åç¹å®çéåºã</li>
<li><strong>åºå®åè¾¨çï¼</strong> æ¨¡åä»¥åºå®ç384 Ã 224åç´ åè¾¨çè¿è¡ï¼è¿å¯è½éå¶äºå¨éè¦æ´é«ä¿çåº¦çåºç¨ä¸­çè§è§ç»èã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä½èæåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ©å±å°å¤æ ·åæ¸¸æç¯å¢åçå®ä¸çåºæ¯ï¼</strong> å°æ¡æ¶æ©å±å°æ´å¹¿æ³çäº¤äºå¼åºæ¯åæ´é«åè¾¨çã</li>
<li><strong>é¢åéåºææ¯ï¼</strong> æ¢ç´¢ä¿çæ ¸å¿è®°å¿æºå¶åæ¶éåºä¸åè§è§ç¹å¾çé¢åéåºææ¯ã</li>
<li><strong>éæé«çº§å éææ¯ï¼</strong> ç»ååè¿çå éææ¯ï¼è¿ä¸æ­¥æé«å¨å¤æ ·åäº¤äºåºæ¯ä¸­çæçåæ§è½ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºèªåå½è§é¢çææ¨¡åå¨å¤æå¼æ¾ä¸çç¯å¢ä¸­çæ¶ç©ºè®°å¿ç®¡çæä¾äºä¸ä¸ªæ°é¢ä¸é«æçè§£å³æ¹æ¡ï¼éè¿åæ°çè®­ç»åè®®åå ä½ç´¢å¼ç©ºé´è®°å¿ï¼æåå¹³è¡¡äºæ¢ç´¢çµæ´»æ§åéè®¿ä¸è´æ§ï¼ä¸ºæªæ¥çä¸çæ¨¡åç ç©¶å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas.</li>
<li>Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.</li>
<li>We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.03198v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.03198v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-06 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
