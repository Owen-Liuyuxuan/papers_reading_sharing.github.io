<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-06 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-03/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-07/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-06">Arxiv Computer Vision Papers - 2025-10-06</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#monster-a-unified-model-for-motion-scene-text-retrieval" class="nav-link">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#taming-text-to-sounding-video-generation-via-advanced-modality-condition-and-interaction" class="nav-link">Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-scalable-and-consistent-3d-editing" class="nav-link">Towards Scalable and Consistent 3D Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#tit-score-evaluating-long-prompt-based-text-to-image-alignment-via-text-to-image-to-text-consistency" class="nav-link">TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency</a>
                </li>
                <li class="nav-item">
                    <a href="#one-patch-to-caption-them-all-a-unified-zero-shot-captioning-framework" class="nav-link">One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</a>
                </li>
                <li class="nav-item">
                    <a href="#work-zones-challenge-vlm-trajectory-planning-toward-mitigation-and-robust-autonomous-driving" class="nav-link">Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#maskcd-mitigating-lvlm-hallucinations-by-image-head-masked-contrastive-decoding" class="nav-link">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a>
                </li>
                <li class="nav-item">
                    <a href="#adard-key-adaptive-relevance-diversity-keyframe-sampling-for-long-form-video-understanding" class="nav-link">AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#mogic-boosting-motion-generation-via-intention-understanding-and-visual-context" class="nav-link">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a>
                </li>
                <li class="nav-item">
                    <a href="#memory-forcing-spatio-temporal-memory-for-consistent-scene-generation-on-minecraft" class="nav-link">Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-06">Arxiv Computer Vision Papers - 2025-10-06</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月3日Arxiv计算机视觉论文的每日报告执行摘要：</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文执行摘要 (2025-10-03)</strong></p>
<p><strong>概述与主要趋势：</strong>
今天的论文集展示了计算机视觉领域在多模态理解、生成和编辑方面的持续快速发展。核心趋势包括：</p>
<ol>
<li><strong>多模态融合与统一模型：</strong> 显著关注于整合文本、图像、视频和运动等多种模态，以实现更强大的检索、生成和理解能力。</li>
<li><strong>生成模型的高级控制与一致性：</strong> 文本到图像/视频生成模型正在努力解决长提示对齐、时空一致性以及减少幻觉等挑战。</li>
<li><strong>3D内容生成与编辑：</strong> 3D场景和内容的可扩展、一致性编辑和生成是另一个重要方向。</li>
<li><strong>鲁棒性与可靠性：</strong> 针对VLM在复杂场景（如自动驾驶）中的鲁棒性以及大型视觉语言模型（LVLM）幻觉的缓解策略受到关注。</li>
<li><strong>效率与优化：</strong> 视频理解中的关键帧采样以及运动生成中的意图理解旨在提高效率和质量。</li>
</ol>
<p><strong>特别显著或创新的论文：</strong></p>
<ul>
<li><strong>"MonSTeR: a Unified Model for Motion, Scene, Text Retrieval" (Luca Collorone et al.)</strong>：这份论文因其提出一个<strong>统一模型</strong>来处理运动、场景和文本检索而显得尤为突出。这种跨模态的统一方法在信息检索领域具有巨大的潜力，可能为未来的多模态搜索引擎奠定基础。</li>
<li><strong>"TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency" (Juntong Wang et al.)</strong>：在文本到图像生成日益复杂、长提示成为常态的背景下，提出一个<strong>新的评估指标</strong>来衡量长提示对齐度至关重要。TIT-Score通过“文本-图像-文本”的一致性来评估，提供了一种更全面和可靠的度量方法，对生成模型的研究和开发具有直接指导意义。</li>
<li><strong>"MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding" (Jingyuan Deng, Yujiu Yang)</strong>：随着LVLM的广泛应用，幻觉问题是其可靠性的主要障碍。MaskCD提出了一种<strong>新颖的解码策略</strong>来缓解这一问题，这对于提升LVLM在实际应用中的可信度具有重要意义。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>“文本-图像-文本”一致性评估：</strong> TIT-Score的提出表明，对生成模型输出质量的评估正从单一模态或简单对齐转向更复杂的循环一致性验证。</li>
<li><strong>基于意图的运动生成：</strong> MoGIC通过理解意图来提升运动生成，预示着未来生成模型将更深入地理解用户或环境的潜在目标。</li>
<li><strong>时空记忆机制：</strong> Memory Forcing在Minecraft场景生成中引入时空记忆，强调了在长序列或复杂场景生成中保持一致性和连贯性的重要性。</li>
<li><strong>统一的多模态检索架构：</strong> MonSTeR代表了将多种模态（运动、场景、文本）整合到单一检索框架中的趋势，旨在实现更全面的信息访问。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<ol>
<li><strong>"MonSTeR: a Unified Model for Motion, Scene, Text Retrieval"</strong>：对于对多模态信息检索和统一模型架构感兴趣的研究人员，这篇论文提供了潜在的突破性方法。</li>
<li><strong>"TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency"</strong>：任何从事文本到图像生成模型开发或评估的人都应该深入阅读，因为它提供了一个关键的评估工具。</li>
<li><strong>"MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding"</strong>：对于关注大型视觉语言模型可靠性、幻觉问题及其解决方案的研究人员来说，这篇论文是必读的。</li>
<li><strong>"Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction"</strong>：对于视频生成和多模态交互感兴趣的团队，这篇论文展示了如何通过高级条件控制来提升视频生成质量。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速把握今日Arxiv计算机视觉领域的关键进展，并识别出对您研究最有价值的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.03200v1">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a></li>
<li><a href="#2510.03117v1">Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</a></li>
<li><a href="#2510.02994v1">Towards Scalable and Consistent 3D Editing</a></li>
<li><a href="#2510.02987v1">TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency</a></li>
<li><a href="#2510.02898v1">One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</a></li>
<li><a href="#2510.02803v1">Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving</a></li>
<li><a href="#2510.02790v1">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a></li>
<li><a href="#2510.02778v1">AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</a></li>
<li><a href="#2510.02722v1">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a></li>
<li><a href="#2510.03198v1">Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.03200v1'></a></p>
<h2 id="monster-a-unified-model-for-motion-scene-text-retrieval"><a href="https://arxiv.org/abs/2510.03200v1">MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</a></h2>
<p><strong>Authors:</strong> Luca Collorone, Matteo Gioia, Massimiliano Pappa, Paolo Leoni, Giovanni Ficarra, Or Litany, Indro Spinelli, Fabio Galasso</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“MonSTeR: a Unified Model for Motion, Scene, Text Retrieval”论文的全面摘要：</p>
<p><strong>论文摘要：MonSTeR: 运动、场景、文本统一检索模型</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
人类在复杂环境中的运动受意图驱动，但其实现必须依赖周围环境的支持。现有研究缺乏有效工具来评估骨骼运动（运动）、意图（文本）和周围环境（场景）三者之间的一致性。这导致了在人类运动生成和检索中，环境上下文未能被充分利用，以及人类场景交互模型缺乏全局一致性评估。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>首次提出统一的运动-场景-文本检索模型（MonSTeR）：</strong> MonSTeR是第一个能够在一个统一的潜在空间中评估文本、运动和场景三模态之间一致性的模型。
*   <strong>建模高阶关系：</strong> 受拓扑深度学习的启发，MonSTeR通过建模超越成对关系的更高阶交互来捕捉模态间的复杂依赖。它通过对单模态和跨模态表示进行对齐，构建了一个统一的潜在空间。具体来说，它不仅对齐单模态项（t, s, m），还对齐成对的跨模态项（ts, sm, mt），以捕捉三模态交互。
*   <strong>灵活且鲁棒的检索能力：</strong> 这种建模方式使得MonSTeR能够灵活且鲁棒地执行各种检索任务，包括给定一个模态检索另一个模态，或给定两个模态的表示检索一个模态。
*   <strong>人类场景交互模型评估工具：</strong> MonSTeR可作为评估文本条件人类场景交互模型（HSI）的工具，能够评估运动路径的合理性、与场景的符合度，并与人类偏好高度一致。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的检索性能：</strong> MonSTeR在HUMANISE+和TRUMANS+数据集上的检索任务中，显著优于仅依赖单模态表示的三模态模型。在“All”协议的st2m任务中，MonSTeR相对于最佳场景感知模型性能提升了209%。
*   <strong>与人类偏好对齐：</strong> 通过用户研究验证，MonSTeR的检索分数与人类偏好高度一致（66.5%的对齐率），证明了其评估结果的可靠性。
*   <strong>多功能性：</strong> MonSTeR的潜在空间在零样本场景内物体放置（In-Scene Object Placement）和运动描述（Motion Captioning）等下游任务中展现出强大的通用性。在零样本物体放置任务中，平均误差仅为18cm，在运动描述任务中，MonSTeR+GPT2在多项指标上超越了MotionGPT。
*   <strong>场景-运动接地能力：</strong> MonSTeR能够区分与场景和文本意图一致的运动路径与不一致的路径，并能识别出穿透场景物体的运动，表明其潜在空间内化了自然运动不应穿透场景的知识。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>计算成本：</strong> 跨模态编码器仅在对齐的模态对上进行训练，因为在未配对数据上进行训练的计算成本过高。
*   <strong>静态场景：</strong> 当前模型假设场景是静态的，即人类动作不会改变场景布局。这限制了其在动态场景中的应用。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>动态场景编码器：</strong> 探索和开发能够处理动态场景的编码器，以扩展MonSTeR在更复杂、交互性更强的环境中的应用。
*   <strong>更复杂的模态交互：</strong> 进一步研究和建模更复杂的高阶模态交互，以捕捉更细致的依赖关系。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model.</li>
<li>Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations.</li>
<li>We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.03200v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.03200v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.03117v1'></a></p>
<h2 id="taming-text-to-sounding-video-generation-via-advanced-modality-condition-and-interaction"><a href="https://arxiv.org/abs/2510.03117v1">Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</a></h2>
<p><strong>Authors:</strong> Kaisi Guan, Xihua Wang, Zhengfeng Lai, Xin Cheng, Peng Zhang, XiaoJiang Liu, Ruihua Song, Meng Cao</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV, cs.SD</p>
<p><strong>Abstract:</strong></p>
<p>This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文《通过高级模态条件和交互驯服文本到有声视频生成》解决了文本到有声视频（T2SV）生成这一具有挑战性的任务。T2SV的目标是根据文本条件生成具有同步音频的视频，同时确保视频和音频模态都与文本对齐。</p>
<p><strong>1. 主要问题或研究问题：</strong>
尽管在联合音视频训练方面取得了进展，但T2SV生成仍面临两个关键挑战：
1. <strong>模态干扰：</strong> 单一共享文本描述（视频文本等于音频文本）常常导致模态干扰，混淆预训练的骨干网络。
2. <strong>跨模态特征交互机制：</strong> 最佳的跨模态特征交互机制尚不明确。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述挑战，论文提出了两项主要创新：
1. <strong>分层视觉接地字幕（HVGC）框架：</strong> 该框架生成一对解耦的字幕——一个视频字幕（Tv）和一个音频字幕（TA），从而在条件阶段消除了模态干扰。HVGC通过三阶段管道实现：首先，VLLM生成详细的视频描述；其次，LLM从视频描述中提取相关的听觉事件标签；最后，LLM利用视频描述和听觉标签生成纯音频字幕。这种方法确保了字幕的模态纯度和准确性。
2. <strong>BridgeDiT模型：</strong> 一种新颖的双塔扩散Transformer，采用<strong>双重交叉注意力（DCA）机制</strong>。DCA充当一个强大的“桥梁”，实现了视频和音频塔之间的对称、双向信息交换，从而实现了语义和时间上的同步。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文在三个基准数据集上进行了广泛的实验，并辅以人工评估，结果表明：
* BridgeDiT模型在大多数指标上均优于所有基线方法，达到了最先进的性能，包括视频质量（FVD、KVD）、音频质量（FAD、KL）、文本对齐（CLIPSIM、CLAP）和时间同步（AV-Align）。
* HVGC框架在零样本和全训练设置下始终表现最佳，验证了其在消除模态干扰方面的鲁棒性。
* DCA融合机制在训练过程中始终优于其他融合机制，表明其在实现卓越的时间和语义同步方面的优越性。
这些结果的意义在于，该方法有效地解决了T2SV任务中的核心挑战，为生成高质量、同步的文本到有声视频提供了新的范式。</p>
<p><strong>4. 论文中提及的局限性：</strong>
* <strong>数据稀缺：</strong> T2SV领域面临大规模、高质量、标注良好的音视频数据稀缺的挑战。
* <strong>模型依赖性：</strong> 模型的性能高度依赖于数据质量，不稳定或低分辨率视频会降低预训练骨干网络的能力，而嘈杂的音频或画外音则会使精确同步复杂化。
* <strong>功能限制：</strong> 当前版本的BridgeDiT主要专注于生成音效，尚不支持语音和复杂的音乐乐谱。
* <strong>基础模型限制：</strong> 模型的整体性能受限于所选的基础T2V和T2A模型的性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
* <strong>数据集扩展：</strong> 收集更大、更高质量的音视频数据集，并改进数据处理管道以进行清洗、过滤和字幕生成。
* <strong>功能扩展：</strong> 将BridgeDiT扩展到支持语音和音乐，包括集成专门的唇形同步模块和捕捉音乐输入节奏与情绪的技术。
* <strong>后训练优化：</strong> 探索后训练优化技术，例如应用带有专门设计奖励的人类反馈强化学习（RLHF），以进一步提高模型的时空连贯性。</p>
<p>总而言之，这篇论文通过引入HVGC框架和BridgeDiT模型（特别是其DCA机制），为文本到有声视频生成领域做出了重要贡献，有效解决了模态干扰和跨模态特征交互的难题，并取得了显著的性能提升。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization.</li>
<li>Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics.</li>
<li>Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.03117v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.03117v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02994v1'></a></p>
<h2 id="towards-scalable-and-consistent-3d-editing"><a href="https://arxiv.org/abs/2510.02994v1">Towards Scalable and Consistent 3D Editing</a></h2>
<p><strong>Authors:</strong> Ruihao Xia, Yang Tang, Pan Zhou</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ruihao Xia, Yang Tang, Pan Zhou撰写的论文“Towards Scalable and Consistent 3D Editing”的全面摘要。</p>
<hr />
<h3 id="towards-scalable-and-consistent-3d-editing_1">论文《Towards Scalable and Consistent 3D Editing》摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决3D编辑领域的核心挑战：如何实现对3D资产的精确、局部化编辑，同时保持跨视图一致性、结构保真度和精细控制，且无需手动创建3D掩码。现有的2D编辑工具已非常直观和易用，但3D编辑仍面临速度慢、易产生几何失真以及对不准确3D掩码的依赖等问题，这限制了其在沉浸式内容创作、数字娱乐和AR/VR等实际应用中的可扩展性和实用性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述挑战，论文在数据和模型两方面都进行了创新：</p>
<ul>
<li><strong>数据方面：3DEditVerse数据集。</strong> 论文引入了迄今为止最大的配对3D编辑基准数据集3DEditVerse，包含116,309个高质量训练对和1,500个精心策划的测试对。该数据集通过互补的管道构建，包括：<ul>
<li><strong>姿态驱动的几何编辑：</strong> 生成捕获动画角色多样化姿态和几何变化的“前后”资产。</li>
<li><strong>基础模型引导的外观编辑：</strong> 利用文本指令和一系列基础模型（如DeepSeek-R1、Flux、Qwen-VL、Trellis）进行外观修改，确保编辑的局部性、多视图一致性和语义对齐。</li>
<li><strong>一致性保留的3D提升：</strong> 采用掩码引导的重绘策略，显式定位3D编辑区域，并融合源和目标潜在表示，以确保编辑的保真度。</li>
<li><strong>后编辑一致性过滤：</strong> 通过DINOv2特征相似性过滤，进一步保证全局一致性。</li>
</ul>
</li>
<li><strong>模型方面：3DEditFormer。</strong> 论文提出了一个3D结构保持的条件Transformer模型3DEditFormer，它通过以下机制增强了图像到3D的生成能力：<ul>
<li><strong>双重引导注意力块（Dual-Guidance Attention Block）：</strong> 引入两个并行的交叉注意力路径，在不同扩散阶段注入源资产的多阶段特征，从而将可编辑区域与保留结构分离。</li>
<li><strong>多阶段特征提取：</strong> 从冻结的Trellis模型中提取细粒度结构特征（在晚期扩散步骤）和语义转换特征（在早期扩散步骤），以捕捉互补的结构信息。</li>
<li><strong>时间自适应门控机制（Time-Adaptive Gating）：</strong> 动态平衡细粒度结构特征和语义转换特征的影响，在早期阶段强调语义编辑，在后期阶段强调结构保真度。</li>
</ul>
</li>
</ul>
<p>这些创新使得3DEditFormer无需辅助3D掩码即可实现精确、一致且结构保持的3D编辑。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能超越SOTA：</strong> 广泛的实验表明，3DEditFormer在定量和定性上均优于现有最先进的基线方法（如EditP23、Instant3dit和VoxHammer）。
*   <strong>无需3D掩码：</strong> 3DEditFormer无需任何辅助3D掩码，简化了编辑流程，同时实现了卓越的编辑质量和一致性。与依赖精确3D掩码的VoxHammer相比，3DEditFormer在3D指标上平均提高了13%。
*   <strong>高保真度与实用性：</strong> 该框架能够实现高质量的局部修改，同时保持结构保真度，解决了现有方法中常见的几何失真和不一致性问题。
*   <strong>建立新标准：</strong> 论文的工作为实用和可扩展的3D编辑设定了新标准，为该领域的系统性进展奠定了基础。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文指出，3DEditFormer依赖于潜在空间编辑，虽然效率高，但在处理高分辨率3D资产时可能会引入精度损失。在潜在转换过程中，细粒度的几何细节可能会被降级。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的工作可以探索直接在原始3D域中进行无损编辑，以更好地保留细粒度的网格保真度。</p>
<hr />
<p>这份摘要突出了论文在解决3D编辑挑战方面的创新性、技术贡献和重要成果，同时也指出了其当前方法的局限性以及未来可能的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs.</li>
<li>On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer.</li>
<li>Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02994v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02994v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02987v1'></a></p>
<h2 id="tit-score-evaluating-long-prompt-based-text-to-image-alignment-via-text-to-image-to-text-consistency"><a href="https://arxiv.org/abs/2510.02987v1">TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency</a></h2>
<p><strong>Authors:</strong> Juntong Wang, Huiyu Duan, Jiarui Wang, Ziheng Jia, Guangtao Zhai, Xiongkuo Min</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Juntong Wang等人撰写的论文“TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前文本到图像（T2I）模型在处理长而详细的提示时，难以有效理解和遵循指令，导致生成图像与长提示之间一致性差的问题。现有的T2I对齐评估指标在长提示图像生成方面与人类偏好的一致性较差。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
*   <strong>LPG-Bench基准测试：</strong> 论文引入了一个名为LPG-Bench的综合基准测试，专门用于评估基于长提示的T2I生成。该基准包含200个精心制作的提示，平均长度超过250字，并从13个最先进的模型生成了2600张图像，进行了全面的人工排名标注。
*   <strong>TIT评估框架：</strong> 论文提出了一种新颖的零样本评估指标，称为TIT（Text-to-Image-to-Text consistency），用于评估长提示生成的图像。TIT的核心概念是通过直接比较原始提示与大型多模态模型（LMM）对生成图像的描述之间的一致性来量化T2I对齐。
*   <strong>TIT的两种实例化：</strong> TIT框架包括两种互补的实例化：
    *   <strong>TIT-Score：</strong> 一种高效的基于嵌入的实例化，使用先进的文本嵌入模型（Qwen3-Embedding）将文本编码为特征向量，并通过计算余弦相似度来衡量一致性。
    *   <strong>TIT-Score-LLM：</strong> 一种基于大型语言模型（LLM）的实例化，利用前沿LLM（如Gemini 2.5 Pro）直接评估两个文本之间的语义相似度。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>现有指标的局限性：</strong> LPG-Bench上的实验表明，现有最先进的T2I对齐评估指标（如CLIP-score、LMM-score等）在长提示图像生成方面与人类偏好的一致性较差。
*   <strong>TIT的优越性：</strong> TIT框架，特别是TIT-Score-LLM，在与人类判断的对齐方面表现出卓越的性能，其配对准确率比最强的基线（LMM4LMM）绝对提高了7.31%，达到了66.51%。标准的TIT-Score也达到了接近最先进的性能，同时具有更高的效率和可访问性。
*   <strong>解耦设计的有效性：</strong> 消融研究证实了其解耦设计的有效性，即视觉感知与文本语义对齐的分离，这解决了端到端LMM评分固有的不稳定性问题。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>LLM评分的稳定性问题：</strong> 论文指出，直接使用LMM进行端到端评分存在固有的缺陷，LMM缺乏稳定的、经过校准的“评分参考框架”，导致结果不一致且重现性差。
*   <strong>VLM骨干选择的局限性：</strong> 尽管TIT框架表现出色，但分析显示VLM骨干模型的参数量并不一定保证更好的性能，例如Qwen2.5vl系列中，7B模型比32B模型取得了更高的准确率，这表明原始模型规模并非VLM有效性的唯一决定因素。
*   <strong>排名相关性指标的解释：</strong> SRCC和KRCC等排名相关性指标在LPG-Bench背景下应谨慎解释，因为待排名项目数量较少（13个）以及地面真实数据中存在大量平局可能会限制其稳定性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>T2I模型的发展：</strong> LPG-Bench和TIT方法共同为T2I模型的基准测试和发展提供了更深入的视角，特别是在长文本理解和内容创作方面。
*   <strong>VLM和嵌入模型的优化：</strong> 进一步研究VLM和文本嵌入模型的选择和优化，以在性能和计算资源之间取得更好的平衡，从而提高TIT框架的实用性和可访问性。
*   <strong>更复杂的文本理解：</strong> 论文强调了高级文本理解是许多T2I系统的关键瓶颈，未来的研究可以专注于开发能够更好地处理长、详细和叙事风格指令的模型。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation.</li>
<li>Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations.</li>
<li>Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation.</li>
<li>To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02987v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02987v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02898v1'></a></p>
<h2 id="one-patch-to-caption-them-all-a-unified-zero-shot-captioning-framework"><a href="https://arxiv.org/abs/2510.02898v1">One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</a></h2>
<p><strong>Authors:</strong> Lorenzo Bianchi, Giacomo Pacini, Fabio Carrara, Nicola Messina, Giuseppe Amato, Fabrizio Falchi</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework”论文的全面摘要，用中文写成，并涵盖了您要求的所有要点。</p>
<h3 id="one-patch-to-caption-them-all-a-unified-zero-shot-captioning-framework_1">论文摘要：One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</h3>
<p><strong>1. 主要问题或研究问题</strong>
当前零样本图像字幕生成器（zero-shot captioners）主要依赖全局图像表示来生成整个图像的字幕，这限制了它们对图像任意局部区域进行细粒度描述的能力，且需要区域级别的监督数据。本文旨在解决如何在零样本设置下，对图像的任意区域（从单个图像块到非连续区域乃至整个图像）生成高质量、细粒度的字幕，而无需区域级别的监督。</p>
<p><strong>2. 关键创新或方法贡献</strong>
论文提出了一个名为 <strong>Patch-ioner</strong> 的统一零样本字幕生成框架，其核心创新在于：
*   <strong>范式转变：从图像中心到图像块中心（patch-centric）的字幕生成。</strong> 传统的字幕方法以整个图像为处理单元，而Patch-ioner将单个图像块视为原子字幕生成单元。
*   <strong>任意区域的字幕生成：</strong> 通过聚合选定图像块的特征，该框架能够灵活地为任意形状和大小的区域（包括单个图像块、边界框、鼠标轨迹指定的区域、非连续区域以及整个图像）生成字幕，而无需区域级别的监督。
*   <strong>解耦的零样本解码器：</strong> 框架将图像编码和文本解码解耦。视觉语言模型（VLM）用于提取语言对齐的密集图像块嵌入，然后通过一个参数无关的图像块聚合机制生成区域表示，最后由一个仅在文本数据上训练的零样本文本解码器生成字幕。
*   <strong>模态间隙缓解策略：</strong> 论文分析并采用了两种模态间隙缓解策略：基于记忆的潜在投影（memory-based latent projection）和训练时注入噪声（noise injection），以使文本解码器能够处理视觉嵌入。
*   <strong>视觉骨干网络的重要性分析：</strong> 论文深入研究了不同预训练视觉语言骨干网络（特别是DINO系列模型）在生成有意义的密集视觉特征方面的作用，发现DINOv2-based模型在局部语义表示方面表现出色。
*   <strong>引入新的任务：轨迹字幕生成（Trace Captioning）。</strong> 为了展示框架的灵活性，论文引入了根据用户鼠标轨迹生成字幕的新任务。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>卓越的性能：</strong> Patch-ioner在零样本密集字幕（dense captioning）、区域集字幕（region-set captioning）以及新引入的轨迹字幕生成任务上，均显著优于现有基线和最先进的竞争模型。
*   <strong>全局任务的竞争力：</strong> 在整个图像字幕生成任务上，Patch-ioner也表现出与最先进的零样本图像字幕生成器相当的竞争力。
*   <strong>骨干网络的关键作用：</strong> 实验证明，能够生成有意义、密集的视觉特征的骨干网络（如DINO）对于在多区域字幕任务中实现最先进的性能至关重要。
*   <strong>高效性：</strong> 该方法只需对视觉骨干网络进行一次前向传播即可提取整个图像的图像块特征，这些特征可以重复用于对多个区域进行字幕生成，提高了实际应用的效率。
*   <strong>统一性：</strong> 框架成功地弥合了局部和全局理解之间的鸿沟，为多粒度字幕任务提供了一个统一的零样本解决方案。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>与全监督方法的差距：</strong> 尽管在零样本设置下表现强劲，但模型性能仍落后于全监督、任务特定的方法。
*   <strong>上下文范围固定：</strong> 每个图像块的上下文范围由骨干网络决定，无法根据用户意图进行调整。
*   <strong>模态跳跃引入噪声：</strong> 模态间隙可能引入噪声，导致幻觉（hallucinations）。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>弱监督的整合：</strong> 未来工作可以考虑整合弱监督，例如图像级别的字幕损失，以改进对比学习表示中的图像块级别语义。
*   <strong>优化图像块到文本的投影：</strong> 进一步优化图像块到文本的投影机制，以进一步减少零样本设置下的模态间隙。</p>
<p>总而言之，这篇论文通过提出Patch-ioner框架，成功地将零样本字幕生成的焦点从整个图像转移到图像块，从而实现了对图像任意区域进行细粒度描述的能力，并在多个区域级字幕任务中取得了显著进展，为该领域开辟了新的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision.</li>
<li>We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.</li>
<li>Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks.</li>
<li>Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02898v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02898v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02803v1'></a></p>
<h2 id="work-zones-challenge-vlm-trajectory-planning-toward-mitigation-and-robust-autonomous-driving"><a href="https://arxiv.org/abs/2510.02803v1">Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Yifan Liao, Zhen Sun, Xiaoyun Qiu, Zixiao Zhao, Wenbing Tang, Xinlei He, Xinhu Zheng, Tianwei Zhang, Xinyi Huang, Xingshuo Han</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in <script type="math/tex">68.0%</script> of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of <script type="math/tex">8</script> common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around <script type="math/tex">3\times</script> in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time (<script type="math/tex">0.58</script>s) compared with other methods such as fine-tuning
(<script type="math/tex">17.90</script>s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Yifan Liao等人撰写的论文“Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving”的全面摘要，用中文写成：</p>
<p><strong>论文摘要：工作区对VLM轨迹规划的挑战：缓解与鲁棒自动驾驶</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决视觉语言模型（VLMs）在自动驾驶中，特别是在复杂且动态变化的工作区（work zones）进行轨迹规划时所面临的挑战。尽管VLMs在多模态推理方面表现出色，但其在工作区这种包含不规则布局、临时交通管制和动态几何结构的环境中的轨迹规划能力尚未得到充分探索。研究发现，主流VLMs在68.0%的情况下无法生成正确的轨迹。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了一个名为<strong>REACT-Drive</strong>的轨迹规划框架，该框架将VLMs与检索增强生成（RAG）相结合。其主要创新点包括：
*   <strong>首次系统性研究：</strong> 对VLMs在工作区轨迹规划中的表现进行了首次系统性研究，并揭示了其显著的失败率。
*   <strong>失败模式分析：</strong> 通过子图挖掘和聚类分析，识别并人工验证了8种常见失败模式，深入理解了VLM失败的根本原因。
*   <strong>REACT-Drive框架：</strong>
    *   <strong>离线阶段：</strong> 将历史失败案例转化为约束规则和可执行的轨迹缓解代码，并通过自验证机制确保其可用性，构建了一个可搜索的失败案例缓解代码数据库。
    *   <strong>在线阶段：</strong> 利用RAG从数据库中检索与新场景相似的失败模式，并执行相应的缓解代码来指导轨迹生成，确保轨迹符合安全要求和交通规则。
*   <strong>效率优化：</strong> REACT-Drive通过重用缓解代码，显著降低了推理时间。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著降低误差：</strong> 在ROADWork数据集上的实验结果表明，REACT-Drive相较于使用Qwen2.5-VL评估的VLM基线，平均位移误差（ADE）降低了约3倍。
*   <strong>推理时间优势：</strong> REACT-Drive实现了最低的推理时间（0.58秒），远低于其他方法（如微调的17.90秒），这对于实时自动驾驶至关重要。
*   <strong>物理世界验证：</strong> 在15个真实世界工作区场景中进行的实车实验进一步证明了REACT-Drive的强大实用性和鲁棒性，碰撞率（CR）降至0.0。
*   <strong>模式覆盖的重要性：</strong> 实验表明，模式多样性在实现泛化方面起着关键作用，覆盖的失败模式越多，模型处理未知工作区案例的能力越强。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>有限的场景覆盖：</strong> 本研究未能系统性地涵盖所有长尾场景，例如极端天气或夜间驾驶下的工作区。
*   <strong>有限的数据集：</strong> 评估主要基于ROADWork数据集和作者收集的物理数据，未包含其他工作区数据集，这主要是由于可访问数据稀缺。
*   <strong>缺乏实车部署：</strong> REACT-Drive尚未在真实的自动驾驶车辆上部署，因为在工作区进行此类实验存在极高风险。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   构建更大规模、更多样化的工作区场景数据集，以进一步提升模型的泛化能力。
*   探索将REACT-Drive部署到真实自动驾驶车辆上的安全可行方案，以弥合仿真与现实世界之间的差距。
*   继续研究VLMs在长尾和极端工作区场景下的轨迹规划能力。</p>
<p>总而言之，这篇论文首次系统性地揭示了VLMs在工作区轨迹规划中的不足，并通过提出REACT-Drive框架，有效地利用历史失败经验和RAG机制，显著提升了自动驾驶系统在复杂工作区环境中的鲁棒性和效率，为未来自动驾驶系统的安全性和可靠性提供了有价值的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG).</li>
<li>Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02803v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02803v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02790v1'></a></p>
<h2 id="maskcd-mitigating-lvlm-hallucinations-by-image-head-masked-contrastive-decoding"><a href="https://arxiv.org/abs/2510.02790v1">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a></h2>
<p><strong>Authors:</strong> Jingyuan Deng, Yujiu Yang</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jingyuan Deng和Yujiu Yang撰写的论文“MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding”的全面摘要。</p>
<hr />
<h3 id="maskcd-lvlm">MaskCD: 通过图像头掩蔽对比解码缓解LVLM幻觉现象</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决大型视觉-语言模型（LVLMs）中普遍存在的“幻觉”现象。幻觉指的是LVLMs生成与其输入视觉和文本内容相矛盾的信息，例如生成不存在的物体、错误描述属性或产生无意义的句子。现有的缓解方法，如对比解码（CD）和注意力操作，存在各自的局限性：CD方法难以构建合适的对比样本，而注意力操作方法则高度敏感且缺乏稳定性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了<strong>图像头掩蔽对比解码（MaskCD）</strong>方法，其核心创新点在于：
*   <strong>识别“图像头”：</strong> 通过分析LVLMs中注意力机制的内部工作原理，作者发现并识别出模型中那些倾向于对图像token分配高注意力的“图像头”。这些图像头被认为是处理视觉信息的关键部分。
*   <strong>构建对比样本：</strong> MaskCD利用这些识别出的“图像头”来构建“坏样本”。具体来说，在生成坏样本的推理阶段，通过掩蔽（将注意力输出设置为零）这些图像头，从而阻止坏样本访问有用的视觉信息。这种方法确保了减去的样本只包含需要抵消的无效信息。
*   <strong>结合对比解码：</strong> MaskCD将这种图像头掩蔽机制与对比解码相结合。通过从原始样本的输出logits中减去坏样本的输出logits，模型能够更精确地利用真正有用的视觉和文本信息，从而缓解幻觉。</p>
<p><strong>3. 主要结果及其意义：</strong>
MaskCD在多个基准测试（CHAIR、POPE、AMBER和MME）上对LLaVA-1.5-7b和Qwen-VL-7b模型进行了评估，取得了显著成果：
*   <strong>有效缓解幻觉：</strong> MaskCD在CHAIR评估中显著降低了幻觉率（LLaVA-1.5-7b的CHAIR_s和CHAIR_i分别降低了19.12%和29.87%），优于VCD、M3ID和OPERA等现有方法。
*   <strong>保持通用能力：</strong> 在POPE和MME等评估通用能力的基准测试中，MaskCD表现出与OPERA相当或更优的性能，同时保留了LVLMs的通用能力，甚至在某些子集上有所提升。
*   <strong>稳定性与实用性：</strong> 消融实验表明，MaskCD对超参数（如阈值T和对比强度α）具有良好的稳定性，即使在较大值下也能有效缓解幻觉，证明了其作为CD方法的可靠性。
*   <strong>图像头选择的合理性：</strong> 实验证实，掩蔽“图像头”比随机掩蔽其他头更有效，这表明图像头确实包含了更关键的视觉信息。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文也坦诚了MaskCD的几个局限性：
*   <strong>推理前图像处理：</strong> MaskCD需要提前使用图像进行推理以获取图像头的掩码，这会占用一定的计算资源。
*   <strong>模型依赖性：</strong> 获取到的掩码仅适用于相同家族的LLM骨干网络。对于新的LLM基础模型，需要重新获取相应的掩码。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文鼓励未来的研究探索：
*   <strong>动态掩码构建：</strong> 如何在模型操作过程中动态构建掩码，以摆脱对预先获取掩码的限制。
*   <strong>更广泛的适用性：</strong> 寻找能够跨不同LLM骨干网络通用的幻觉缓解策略。</p>
<hr />
<p>总而言之，MaskCD通过创新性地识别并利用LVLM中的“图像头”来构建高质量的对比样本，有效缓解了模型的幻觉现象，同时保持了其通用能力。这项工作为LVLM幻觉缓解提供了一个稳定且高效的新视角。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
work, we propose image head Masked Contrastive Decoding (MaskCD).</li>
<li>Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02790v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02790v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02778v1'></a></p>
<h2 id="adard-key-adaptive-relevance-diversity-keyframe-sampling-for-long-form-video-understanding"><a href="https://arxiv.org/abs/2510.02778v1">AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</a></h2>
<p><strong>Authors:</strong> Xian Zhang, Zexi Wu, Zinuo Li, Hongming Xu, Luqi Gong, Farid Boussaid, Naoufel Werghi, Mohammed Bennamoun</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xian Zhang等人撰写的论文“AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding”的全面摘要。</p>
<hr />
<h3 id="adard-key-adaptive-relevance-diversity-keyframe-sampling-for-long-form-video-understanding_1">论文摘要：AdaRD-Key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决长视频理解中的关键挑战。现有的视觉-语言模型（VLMs）和多模态大语言模型（MLLMs）在处理长视频时，通常采用均匀采样或基于固定时间间隔的关键帧选择方法。这些方法往往会忽略视频中的关键时刻，导致对查询的响应不准确，或者在追求视觉多样性时忽视了查询相关性。具体来说，均匀采样可能错过与查询相关的重要内容，而基于固定时间间隔的方法虽然减少了冗余，但可能遗漏短时、细粒度的重要事件。因此，核心问题是如何在长视频中高效、准确地选择既与查询高度相关又具有视觉多样性的关键帧，以提升视频理解模型的性能。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
AdaRD-Key提出了一个无需训练、查询驱动的关键帧采样模块，其主要创新点包括：</p>
<ul>
<li><strong>联合相关性-多样性最大化体积（RD-MV）目标：</strong> AdaRD-Key引入了RD-MV目标函数，这是首个联合优化查询相关性和嵌入空间多样性的关键帧采样方法。它结合了查询条件下的相关性分数和对数行列式多样性分量，以选择既信息丰富又非冗余的帧。这种方法通过最大化所选帧特征向量的Gram矩阵的对数行列式来几何地表示多样性，确保了所选帧在语义上具有区分度。</li>
<li><strong>变异性-预算缩放（VB-Scale）：</strong> 为了适应不同视频长度和分数分布的查询特性，AdaRD-Key引入了VB-Scale机制。该机制根据相关性分数的变异性（“峰值”或“平坦”）和帧预算比（即每选择槽位的候选帧数量）动态调整相关性与多样性之间的权衡参数λ。当相关性分布较尖锐时，模型更侧重相关性；当分布较平坦或分散时，多样性变得更重要。</li>
<li><strong>轻量级相关性感知门控机制（Lightweight Relevance-Aware Gating）：</strong> 为处理与视频对齐较弱的宽泛查询，AdaRD-Key采用了一个轻量级的相关性感知门控机制。当相关性分布表明与视频对齐较弱时（例如，最大相关性分数低于阈值），该方法会无缝切换到仅多样性模式，从而在无需额外监督的情况下增强覆盖范围，避免放大噪声。</li>
<li><strong>即插即用部署与高效性：</strong> 整个流程无需训练，计算效率高（在单个GPU上实时运行），并且可以即插即用地兼容现有VLMs，无需微调或架构修改，便于跨数据集、领域和任务的无缝集成。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
AdaRD-Key在LongVideoBench和Video-MME等长视频理解基准测试上取得了最先进的性能，尤其在长视频上表现突出。</p>
<ul>
<li><strong>LongVideoBench上的性能提升：</strong> 在32帧预算下，AdaRD-Key将Qwen2-VL的整体准确率提升至60.8%，比M-LL Selector高出3.8个百分点，比AKS高出0.3个百分点。在64帧预算下，AdaRD-Key的准确率达到62.9%，比MAXINFO高出1.4个百分点，比AKS高出0.2个百分点。在3-10分钟视频类别中，AdaRD-Key比AKS提升了1.3%。</li>
<li><strong>Video-MME上的鲁棒增益：</strong> 在32帧预算下，AdaRD-Key将Qwen2-VL的整体分数提升至60.7%，比基线高3.1个百分点，比Q-Frame高2.4个百分点，比AKS高0.8个百分点。在长视频（30-60分钟）中，准确率提升至51.9%，比基线高4.5个百分点，比Q-Frame高3.6个百分点，比AKS高0.8个百分点。</li>
<li><strong>视频字幕任务的改进：</strong> 在VCapsBench上，AdaRD-Key也显著提升了视频字幕的性能。在4帧采样下，Qwen-2.5VL结合AdaRD-Key后，准确率（AR）从44.86提升至52.41（+7.55），不一致率（IR）降低9.51，覆盖率（CR）提升2.25。</li>
<li><strong>消融研究：</strong> 逐步添加相关性、多样性、轻量级相关性感知门控和变异性-预算缩放模块，均带来了性能的持续提升，尤其在长视频上效果更显著。这表明AdaRD-Key的各个组件都对提升性能做出了贡献。</li>
</ul>
<p>这些结果表明，AdaRD-Key能够有效地从长视频中提取出与查询相关且具有多样性的关键信息，显著提升了下游VLM在视频问答和字幕任务中的性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确指出AdaRD-Key方法的具体局限性。然而，从其设计和上下文可以推断出一些潜在的方面：</p>
<ul>
<li><strong>BLIP-2特征的依赖性：</strong> AdaRD-Key依赖于BLIP-2提取帧级语义特征和查询相关性分数。如果底层VLM（如BLIP-2）的特征提取能力有限或存在偏差，可能会影响AdaRD-Key选择关键帧的质量。</li>
<li><strong>超参数敏感性：</strong> 尽管VB-Scale机制旨在自适应地调整多样性权重λ，但Amin、Amax、α和Pcap等超参数的设置仍可能影响性能，尤其是在特定或极端视频场景下。</li>
<li><strong>“弱对齐”的定义：</strong> 轻量级相关性感知门控机制依赖于“弱对齐”的判断（例如，基于最大相关性分数阈值τ）。这个阈值的设置可能需要根据具体应用进行调整，不当的设置可能导致在某些情况下错误地切换到仅多样性模式。</li>
<li><strong>计算成本：</strong> 尽管论文强调其计算效率高（实时运行在单个GPU上），但对于超长视频或需要处理大量视频的场景，帧特征的缓存和Gram矩阵的更新仍然可能带来一定的内存和计算开销。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于论文的贡献和潜在局限性，未来研究可以探索以下方向：</p>
<ul>
<li><strong>更先进的特征提取器：</strong> 探索使用更先进的、针对长视频优化的视觉-语言模型作为特征提取器，以获取更丰富、更鲁棒的帧级语义表示，从而进一步提升关键帧选择的准确性。</li>
<li><strong>自适应超参数优化：</strong> 开发更智能的机制，能够根据视频内容、查询类型或特定任务，自动优化VB-Scale中的超参数，减少对人工经验设置的依赖。</li>
<li><strong>多模态融合：</strong> 尽管论文主要关注视觉帧，但长视频通常包含音频、文本（如字幕）等多模态信息。未来的工作可以探索如何将AdaRD-Key扩展到多模态关键信息采样，以更全面地理解视频内容。</li>
<li><strong>实时性与边缘部署：</strong> 进一步优化算法的计算效率和内存占用，使其能够更好地适应资源受限的边缘设备或对实时性要求更高的应用场景。</li>
<li><strong>用户反馈集成：</strong> 探索将用户反馈（例如，用户对所选关键帧的满意度）集成到关键帧选择过程中，以实现更个性化和用户驱动的视频理解。</li>
<li><strong>更复杂的查询类型：</strong> 针对更复杂、更抽象的查询类型，例如涉及推理、预测或情感分析的查询，进一步提升AdaRD-Key在这些场景下的性能。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding.</li>
<li>Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02778v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02778v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02722v1'></a></p>
<h2 id="mogic-boosting-motion-generation-via-intention-understanding-and-visual-context"><a href="https://arxiv.org/abs/2510.02722v1">MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</a></h2>
<p><strong>Authors:</strong> Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对Junyu Shi等人撰写的论文“MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context”进行了全面分析。以下是该论文的摘要：</p>
<p><strong>论文摘要：MoGIC：通过意图理解和视觉上下文提升运动生成</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
现有的文本驱动运动生成方法通常将合成视为语言和运动之间的双向映射，但未能有效捕捉动作执行的因果逻辑和驱动行为的人类意图。此外，缺乏视觉基础限制了生成的精确性和个性化，因为仅凭语言难以指定细粒度的时空细节。这导致生成的运动缺乏真实感、可控性差，并且难以泛化到更广泛的任务。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>MoGIC统一框架：</strong> 论文提出了MoGIC，一个统一的框架，将意图建模和视觉先验整合到多模态运动合成中。通过联合优化多模态条件运动生成和意图预测，MoGIC能够揭示潜在的人类目标，利用视觉先验增强生成，并展现多功能的多模态生成能力。
*   <strong>意图预测头（IPH）与运动生成头（MGH）解耦：</strong> MoGIC通过解耦的生成头（意图预测头输出离散的意图描述，运动生成头生成连续轨迹）来明确建模人类意图，避免了语义混淆。
*   <strong>混合注意力机制（Mixture-of-Attention）：</strong> 引入了具有自适应范围的混合注意力机制，以实现条件令牌和运动子序列之间有效的局部对齐，从而处理部分对应关系和时间错位问题。
*   <strong>Mo440H基准数据集：</strong> 论文策划并自动标注了Mo440H，一个包含440小时高质量运动数据（来自21个数据集）的大规模基准，涵盖单人活动、人机交互和人-物交互，支持三模态学习。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著提升运动生成质量：</strong> 在HumanML3D和Mo440H数据集上，经过微调后，MoGIC的FID（Fréchet Inception Distance）分别降低了38.6%和34.6%，表明其生成的运动更具真实感和多样性。
*   <strong>超越LLM基线：</strong> 在运动描述任务中，MoGIC凭借轻量级文本头超越了基于LLM的方法，证明了其在参数较少的情况下仍能保持竞争力。
*   <strong>实现意图预测和视觉条件生成：</strong> MoGIC不仅能生成运动，还能预测潜在意图，并支持视觉条件下的运动生成（如图像到运动合成、视觉条件运动补全），极大地提升了运动合成的可控性和意图理解能力。
*   <strong>混合注意力机制的有效性：</strong> 消融实验表明，混合注意力机制显著提升了检索性能，并使模型能够生成更精确的局部运动响应。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及MoGIC模型的具体局限性。然而，从其强调意图理解和视觉上下文来解决现有方法的不足（如缺乏因果逻辑、精确性、个性化）来看，可以推断出这些是现有方法的普遍局限，而MoGIC旨在克服它们。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更精确、自适应和意图感知的运动合成：</strong> MoGIC为未来研究奠定了基础，可以进一步探索如何实现更精确、自适应和意图感知的运动合成。
*   <strong>扩展到更复杂的交互和场景：</strong> 鉴于Mo440H数据集涵盖了人机交互和人-物交互，未来工作可以进一步探索MoGIC在更复杂、多主体、多对象场景中的应用。
*   <strong>实时推理的优化：</strong> 尽管MoGIC在少量采样步数下仍能保持竞争力，但进一步优化实时推理速度以满足更严格的应用需求仍是一个方向。
*   <strong>更深入的因果结构建模：</strong> 尽管MoGIC通过意图预测捕捉了部分因果逻辑，但未来可以探索更深层次的因果结构建模，以更好地理解和预测人类行为。</p>
<p>总而言之，MoGIC通过整合意图建模和视觉先验，显著推动了多模态运动生成领域的发展，为实现更智能、更可控的人类运动合成和理解提供了新的视角和强大的工具。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02722v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02722v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.03198v1'></a></p>
<h2 id="memory-forcing-spatio-temporal-memory-for-consistent-scene-generation-on-minecraft"><a href="https://arxiv.org/abs/2510.03198v1">Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</a></h2>
<p><strong>Authors:</strong> Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, Li Jiang</p>
<p><strong>Published:</strong> 2025-10-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Junchao Huang等人撰写的论文“Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft”的全面摘要。</p>
<hr />
<h3 id="memory-forcing-spatio-temporal-memory-for-consistent-scene-generation-on-minecraft_1">论文摘要：“Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft”</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决自回归视频扩散模型在Minecraft等开放世界环境中进行场景生成时面临的核心挑战。具体来说，模型需要：
*   在探索新场景时生成自然内容。
*   在重新访问已探索区域时保持空间一致性。
*   在有限的计算预算下，有效压缩和利用历史信息。</p>
<p>现有方法存在一个权衡：仅依赖时间记忆的模型缺乏长期空间一致性，而过度依赖空间记忆（在空间上下文不足时）可能损害新场景的生成质量。因此，核心问题是如何在探索灵活性和重访一致性之间取得平衡，并有效管理有限上下文窗口内的时空记忆。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了“Memory Forcing”框架，通过以下创新解决了上述问题：</p>
<ul>
<li><strong>几何索引空间记忆（Geometry-indexed Spatial Memory）：</strong> 引入了一种新的空间记忆机制，通过流式3D重建维护显式3D缓存。它将当前可见点映射回其源帧，从而实现高效的“点到帧检索（Point-to-Frame Retrieval）”，以选择紧凑且与姿态相关的历史视图。这种方法比基于外观的检索更鲁棒，并且检索复杂度与可见空间覆盖范围而非序列长度成比例，从而提高了计算效率和存储效率。</li>
<li><strong>混合训练（Hybrid Training）：</strong> 设计了一种训练协议，通过模拟不同的游戏玩法模式（探索和重访），指导模型在探索新场景时依赖时间记忆，在重访时结合空间记忆以保持一致性。</li>
<li><strong>链式前向训练（Chained Forward Training）：</strong> 扩展了自回归训练，引入了模型推演。在这种训练中，模型逐步用自己的预测替换真实的时间上下文，从而产生更大的姿态变化，鼓励模型依赖空间记忆来维持一致性，并减少自回归推理中常见的累积误差。</li>
<li><strong>记忆增强架构：</strong> 在Diffusion Transformer (DiT)骨干中集成了空间记忆提取和记忆交叉注意力模块，利用几何索引的空间记忆提供长期空间上下文。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
通过在Minecraft基准测试上进行的大量实验，Memory Forcing展示了卓越的性能：</p>
<ul>
<li><strong>长期空间一致性：</strong> 在重新访问已探索区域时，模型表现出优越的长期空间一致性和场景连贯性，显著优于仅依赖时间记忆和现有空间记忆基线。</li>
<li><strong>生成质量：</strong> 在新环境中，模型在生成性能方面也优于所有基线，生成内容更自然、更具响应性，并能更好地泛化到未见过的地形。</li>
<li><strong>计算效率：</strong> 几何索引空间记忆在检索速度上比WorldMem快7.3倍，同时减少了98.2%的内存存储，证明了其在处理扩展序列时的计算效率。</li>
<li><strong>消融研究：</strong> 证明了混合训练和链式前向训练策略以及3D几何检索机制对模型性能的贡献。</li>
</ul>
<p>这些结果表明，Memory Forcing成功解决了生成质量和长期记忆一致性之间的核心权衡，并在保持计算效率的同时，在多样化环境中实现了卓越的性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文也指出了当前方法的局限性：</p>
<ul>
<li><strong>领域特异性：</strong> 当前实现主要在Minecraft游戏场景中验证，可能无法直接泛化到其他环境，需要进行领域特定的适应。</li>
<li><strong>固定分辨率：</strong> 模型以固定的384 × 224像素分辨率运行，这可能限制了在需要更高保真度的应用中的视觉细节。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
作者提出了以下未来研究方向：</p>
<ul>
<li><strong>扩展到多样化游戏环境和真实世界场景：</strong> 将框架扩展到更广泛的交互式场景和更高分辨率。</li>
<li><strong>领域适应技术：</strong> 探索保留核心记忆机制同时适应不同视觉特征的领域适应技术。</li>
<li><strong>集成高级加速技术：</strong> 结合先进的加速技术，进一步提高在多样化交互场景中的效率和性能。</li>
</ul>
<hr />
<p>总而言之，这篇论文为自回归视频生成模型在复杂开放世界环境中的时空记忆管理提供了一个新颖且高效的解决方案，通过创新的训练协议和几何索引空间记忆，成功平衡了探索灵活性和重访一致性，为未来的世界模型研究奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas.</li>
<li>Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.</li>
<li>We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.03198v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.03198v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-06 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
