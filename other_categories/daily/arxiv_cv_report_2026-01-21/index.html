<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-21 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-20/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-22/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-21">Arxiv Computer Vision Papers - 2026-01-21</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#reasoning-or-pattern-matching-probing-large-vision-language-models-with-visual-puzzles" class="nav-link">Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles</a>
                </li>
                <li class="nav-item">
                    <a href="#implicit-neural-representation-facilitates-unified-universal-vision-encoding" class="nav-link">Implicit Neural Representation Facilitates Unified Universal Vision Encoding</a>
                </li>
                <li class="nav-item">
                    <a href="#implicit-neural-representation-facilitates-unified-universal-vision-encoding_1" class="nav-link">论文方法分析与总结：Implicit Neural Representation Facilitates Unified Universal Vision Encoding</a>
                </li>
                <li class="nav-item">
                    <a href="#videomama-mask-guided-video-matting-via-generative-prior" class="nav-link">VideoMaMa: Mask-Guided Video Matting via Generative Prior</a>
                </li>
                <li class="nav-item">
                    <a href="#motion-3-to-4-3d-motion-reconstruction-for-4d-synthesis" class="nav-link">Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#motion-3-to-4-3d-motion-reconstruction-for-4d-synthesis_1" class="nav-link">论文方法分析与总结：《Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis》</a>
                </li>
                <li class="nav-item">
                    <a href="#lightonocr-a-1b-end-to-end-multilingual-vision-language-model-for-state-of-the-art-ocr" class="nav-link">LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</a>
                </li>
                <li class="nav-item">
                    <a href="#omnitransfer-all-in-one-framework-for-spatio-temporal-video-transfer" class="nav-link">OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</a>
                </li>
                <li class="nav-item">
                    <a href="#soft-tail-dropping-for-adaptive-visual-tokenization" class="nav-link">Soft Tail-dropping for Adaptive Visual Tokenization</a>
                </li>
                <li class="nav-item">
                    <a href="#rig-aware-3d-reconstruction-of-vehicle-undercarriages-using-gaussian-splatting" class="nav-link">Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#_1" class="nav-link">论文方法分析与总结</a>
                </li>
                <li class="nav-item">
                    <a href="#copy-trasform-paste-zero-shot-object-object-alignment-guided-by-vision-language-and-geometric-constraints" class="nav-link">Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</a>
                </li>
                <li class="nav-item">
                    <a href="#iir-vlm-in-context-instance-level-recognition-for-large-vision-language-models" class="nav-link">IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#iir-vlm-in-context-instance-level-recognition-for-large-vision-language-models_1" class="nav-link">论文方法分析：IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-21">Arxiv Computer Vision Papers - 2026-01-21</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对您提供的 Arxiv 计算机视觉论文列表的简明执行摘要，旨在帮助忙碌的研究人员快速了解该领域的最新进展。</p>
<hr />
<p><strong>执行摘要：2026年1月20日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>大型视觉语言模型 (VLMs) 的能力探索与提升</strong>，特别是在<strong>多模态理解、生成式建模以及三维视觉任务</strong>方面。我们观察到以下几个关键趋势：</p>
<ul>
<li><strong>深入理解与推理能力：</strong> 研究人员正积极探索 VLMs 的内在推理机制，而非仅仅依赖模式匹配，以应对更复杂的视觉任务。</li>
<li><strong>统一的视觉表示：</strong> 致力于开发更通用、更高效的视觉编码器，能够统一处理不同类型的视觉信息。</li>
<li><strong>高质量的视频处理：</strong> 在视频分割、三维运动重建和视频风格迁移等领域，涌现出更精细、更具创造性的方法。</li>
<li><strong>OCR 的突破性进展：</strong> 大型多语言 VLM 在 OCR 任务上展现出强大的端到端能力，预示着光学字符识别的新时代。</li>
<li><strong>零样本与少样本学习：</strong> 通过结合视觉语言约束和几何信息，实现更灵活的零样本对象对齐和识别。</li>
<li><strong>三维重建的精细化：</strong> 在特定场景（如车辆底盘）的三维重建中，利用先进技术实现高精度和细节还原。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>“Reasoning or Pattern Matching?”</strong> 论文通过设计视觉谜题，对大型 VLMs 的推理能力进行了深入的探究，为理解模型行为提供了重要视角。</li>
<li><strong>“LightOnOCR”</strong> 提出了一个拥有 10 亿参数的多语言端到端 VLM，在 OCR 任务上取得了最先进的性能，标志着 VLM 在文档理解领域的重大突破。</li>
<li><strong>“Copy-Trasform-Paste”</strong> 引入了一种新颖的零样本对象对齐方法，巧妙地结合了视觉语言理解和几何约束，展现了强大的泛化能力。</li>
<li><strong>“Rig-Aware 3D Reconstruction of Vehicle Undercarriages”</strong> 在特定领域的 3D 重建上取得了显著进展，利用 Gaussian Splatting 技术实现了高精度的车辆底盘重建。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>基于隐式神经表示的通用视觉编码：</strong> “Implicit Neural Representation Facilitates Unified Universal Vision Encoding” 预示着隐式神经表示在构建统一视觉编码器方面的潜力。</li>
<li><strong>生成式先验在视频分割中的应用：</strong> “VideoMaMa” 展示了利用生成式先验来提升视频抠图精度的有效性。</li>
<li><strong>自适应视觉标记化：</strong> “Soft Tail-dropping for Adaptive Visual Tokenization” 提出了一种新的自适应 tokenization 方法，可能对 Transformer 类模型有广泛影响。</li>
<li><strong>上下文实例级识别：</strong> “IIR-VLM” 探索了在 VLM 中进行上下文实例级识别的新范式，为提升模型对具体实例的理解能力提供了思路。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其对当前研究热点的影响力和潜在的突破性贡献，以下论文值得深入阅读：</p>
<ol>
<li><strong>“Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles”</strong>: 对于理解当前大型 VLM 的能力边界和未来发展方向至关重要。</li>
<li><strong>“LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR”</strong>: 如果您对 OCR、多语言模型或大型 VLM 的实际应用感兴趣，这篇论文不容错过。</li>
<li><strong>“Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints”</strong>: 对于需要进行零样本对象操作或理解对象间关系的领域，这篇论文提供了创新的解决方案。</li>
<li><strong>“Implicit Neural Representation Facilitates Unified Universal Vision Encoding”</strong>: 如果您关注底层视觉表示的学习和统一，这篇论文可能揭示了新的研究路径。</li>
</ol>
<hr />
<p>希望这份摘要能为您提供有价值的洞察！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.13705v1">Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles</a></li>
<li><a href="#2601.14256v1">Implicit Neural Representation Facilitates Unified Universal Vision Encoding</a></li>
<li><a href="#2601.14255v1">VideoMaMa: Mask-Guided Video Matting via Generative Prior</a></li>
<li><a href="#2601.14253v1">Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis</a></li>
<li><a href="#2601.14251v1">LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</a></li>
<li><a href="#2601.14250v1">OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</a></li>
<li><a href="#2601.14246v1">Soft Tail-dropping for Adaptive Visual Tokenization</a></li>
<li><a href="#2601.14208v1">Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting</a></li>
<li><a href="#2601.14207v1">Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</a></li>
<li><a href="#2601.14188v1">IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.13705v1'></a></p>
<h2 id="reasoning-or-pattern-matching-probing-large-vision-language-models-with-visual-puzzles"><a href="https://arxiv.org/abs/2601.13705v1">Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles</a></h2>
<p><strong>Authors:</strong> Maria Lymperaiou, Vasileios Karampinis, Giorgos Filandrianos, Angelos Vlachos, Chrysoula Zerva, Athanasios Voulodimos</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析您提供的论文方法部分，并严格遵循您提出的分析框架。请提供您希望我分析的论文内容。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution.</li>
<li>By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.13705v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.13705v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14256v1'></a></p>
<h2 id="implicit-neural-representation-facilitates-unified-universal-vision-encoding"><a href="https://arxiv.org/abs/2601.14256v1">Implicit Neural Representation Facilitates Unified Universal Vision Encoding</a></h2>
<p><strong>Authors:</strong> Matthew Gwilliam, Xiao Wang, Xuefeng Hu, Zhenheng Yang</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇关于“隐式神经表示促进统一通用视觉编码”的论文，重点关注其方法创新、设计逻辑、优势与不足，并提供实用的分析框架。</p>
<hr />
<h2 id="implicit-neural-representation-facilitates-unified-universal-vision-encoding_1">论文方法分析与总结：Implicit Neural Representation Facilitates Unified Universal Vision Encoding</h2>
<h3 id="1">1. 摘要翻译</h3>
<p><strong>论文摘要翻译：</strong></p>
<p>图像表示学习模型通常被设计用于识别或生成。各种形式的对比学习帮助模型学习将图像转换为对分类、检测和分割有用的嵌入。另一方面，模型可以通过像素级、感知级和对抗性损失来重构图像，从而学习到对图像生成有用的潜在空间。我们旨在通过一种首创的模型来统一这两个方向，该模型学习同时对识别和生成都有用的表示。我们将我们的模型训练为一个隐式神经表示（INR）的超网络，它学习将图像映射到模型权重，以实现快速、准确的重构。我们进一步整合我们的INR超网络与知识蒸馏，以提高其泛化能力和性能。除了新颖的训练设计，该模型还学习到一个前所未有的压缩嵌入空间，在各种视觉任务中表现出色。完整的模型在图像表示学习方面达到了最先进的水平，同时还通过其高质量的微小嵌入实现了生成能力。代码可在<a href="https://github.com/facebookresearch/dinov3">此链接</a>获取。</p>
<h3 id="2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>任务割裂</strong>：现有的图像表示学习模型通常专注于识别（如分类、检测、分割）或生成（如图像合成），缺乏一个能够同时高效处理这两类任务的统一框架。</li>
<li><strong>效率与性能的权衡</strong>：识别任务需要强大的语义理解，而生成任务则需要精细的像素级信息。将两者融合需要一种能够捕捉不同粒度信息并高效表示的方法。</li>
<li><strong>压缩表示的需求</strong>：在实际应用中，尤其是在处理大规模数据时，压缩表示（如微小嵌入）对于降低存储和计算成本至关重要，但现有方法在压缩后往往会牺牲性能。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>识别模型</strong>：主要依赖对比学习等方法，擅长捕捉高层语义，但对像素级细节的重构能力较弱。</li>
<li><strong>生成模型</strong>：如VAE、GAN等，擅长生成逼真图像，但其潜在空间通常不直接适用于下游的识别任务，需要额外的适配或蒸馏。</li>
<li><strong>后验融合</strong>：现有工作尝试通过后验方法（如PCA）来融合识别和生成模型，但这种方法不是原生的统一，可能存在性能损失。</li>
<li><strong>INR的训练成本</strong>：隐式神经表示（INR）虽然能实现高质量的重构，但其训练成本极高，需要为每个样本单独训练，且不具备泛化性。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>超网络与INR的结合</strong>：通过INR超网络，可以学习一个通用的模型，该模型能够为任意输入图像生成特定于该图像的INR网络权重，从而实现高效的INR训练和泛化。</li>
<li><strong>统一表示的潜力</strong>：一个原生的统一编码器，其特征应该能够同时包含高层（图像分类）、中层（语义分割）、低层（深度估计）和像素级（重构）的信息。</li>
<li><strong>压缩表示的有效性</strong>：通过设计一种能够生成微小（Tiny）但信息丰富的嵌入（TinToks）的机制，可以在保持强大识别和生成能力的同时，实现高效的压缩。</li>
</ul>
</li>
</ul>
<h3 id="3">3. 方法设计详解</h3>
<p><strong>核心思想：</strong> 构建一个名为 HUVR (Hyper-network for Unified Visual Representation) 的模型，它是一个INR超网络，能够学习将图像映射到INR网络权重，从而实现对图像的高效识别和高质量重构。同时，它还能生成一种名为“TinToks”的微小压缩表示，用于下游任务。</p>
<p><strong>方法Pipeline：</strong></p>
<ol>
<li>
<p><strong>INR超网络（Hyper-network for INR）</strong>：</p>
<ul>
<li><strong>输入</strong>：一张图像。</li>
<li><strong>核心组件</strong>：<ul>
<li><strong>Transformer Encoder (E)</strong>：接收图像的patch tokens和全局token（一个可学习的全局token，类似于ViT中的CLS token）。</li>
<li><strong>MLP</strong>：用于处理Transformer Encoder的输出。</li>
<li><strong>INR Hyper-Network</strong>：这是整个模型的核心。它接收图像的表示（来自Transformer Encoder和MLP），并输出一个INR网络的权重（<script type="math/tex">\theta'</script>）。</li>
</ul>
</li>
<li><strong>INR (Implicit Neural Representation)</strong>：INR是一个小型神经网络，它接收坐标作为输入，输出对应坐标的像素值（如RGB）。其权重<script type="math/tex">\theta</script>是预先定义好的“基础”INR权重，而超网络的作用是学习如何根据输入图像来“调制”这些基础权重，生成一个特定于该图像的INR网络权重<script type="math/tex">\theta'</script>。</li>
<li><strong>输出</strong>：<ul>
<li><strong>标准尺寸表示</strong>：Transformer Encoder的输出，用于识别任务。</li>
<li><strong>微小表示 (TinToks)</strong>：通过在Transformer Encoder和INR预测层之间引入可学习的特征下采样和上采样层，生成一种压缩的表示。</li>
<li><strong>INR权重 (<script type="math/tex">\theta'</script>)</strong>：用于后续的图像重构。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>关键创新点 (Key Innovations)</strong>：</p>
<ul>
<li>
<p><strong>Key Innovation #1: Patch Tokens as Weight Tokens</strong></p>
<ul>
<li><strong>动机</strong>：传统的INR超网络会丢弃输出的图像token，仅使用它们来计算损失或进行推理。这使得密集型任务（如语义分割）变得困难，因为图像信息主要存储在权重token中，而这些权重token与空间位置的相关性不强。</li>
<li><strong>设计</strong>：作者将Transformer Encoder的输出的“数据token”（patch tokens）本身用作INR的“权重token”。这意味着，每个patch token都参与到INR权重的生成过程中。</li>
<li><strong>挑战与解决方案</strong>：标准INR超网络中，权重token的数量必须是INR权重Wi的维度（din或dout）的因子。为了解决这个问题，作者将INR的预测从“每图像”改为“每patch”。这意味着超网络输出的token现在是patch tokens，它们直接用于生成每个patch的INR权重。</li>
</ul>
</li>
<li>
<p><strong>Key Innovation #2: Global Tokens to Modulate and Summarize</strong></p>
<ul>
<li><strong>动机</strong>：原始的INR超网络没有CLS token，这不利于识别任务。</li>
<li><strong>设计</strong>：引入一个可学习的全局token（<code>g</code>）。这个全局token可以作为识别任务的CLS token。它与patch tokens（<code>p</code>）结合，通过投影和矩阵乘法（<code>g</code>投影到<code>dout</code>维度，<code>p</code>投影到<code>din</code>维度，然后计算<code>g × p^T</code>）来生成调制矩阵<code>Mi</code>，用于调制INR权重<code>Wi</code>。</li>
<li><strong>优势</strong>：<ul>
<li><strong>统一性</strong>：同时支持INR预测和图像识别。</li>
<li><strong>效率</strong>：避免了为每个patch生成独立的INR权重，而是通过全局token和patch token的交互来生成调制矩阵。</li>
<li><strong>信息整合</strong>：全局token可以汇总全局信息，patch token则提供局部信息，两者结合生成更丰富的INR权重。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Key Innovation #3: Tiny Tokens (TinToks)</strong></p>
<ul>
<li><strong>动机</strong>：需要能够独立设置Transformer Encoder的维度（<code>d_vit</code>）和INR权重Wi的维度（<code>d_in</code>, <code>d_out</code>）。同时，计算受限的应用需要更小的token。</li>
<li><strong>设计</strong>：引入一个中间表示层，称为“TinToks”。在Transformer Encoder的输出和INR预测层之间，插入可学习的特征下采样和上采样层。</li>
<li><strong>流程</strong>：<ol>
<li>Transformer Encoder输出标准尺寸的token。</li>
<li>通过一个线性层将这些token下采样到更小的维度<code>dt</code>（TinToks）。</li>
<li>（可选）使用Transformer Decoder处理TinToks，以允许更好的重构。</li>
<li>通过线性层将TinToks上采样回INR预测所需的维度<code>din</code>和<code>dout</code>。</li>
</ol>
</li>
<li><strong>优势</strong>：<ul>
<li><strong>压缩</strong>：生成了尺寸更小的表示（TinToks），显著降低了存储和计算需求。</li>
<li><strong>灵活性</strong>：允许<code>d_vit</code>和<code>d_in</code>/<code>d_out</code>之间存在差异。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Key Innovation #4: Distillation for Unified Representation</strong></p>
<ul>
<li><strong>动机</strong>：INR超网络本身可能不直接学习到好的高层语义信息，这对于识别任务至关重要。</li>
<li><strong>设计</strong>：使用知识蒸馏，将预训练的视觉编码器（如DINOv3）的特征作为教师信号，来指导HUVR模型的学习。</li>
<li><strong>流程</strong>：计算HUVR模型（特别是最后Encoder和Decoder块的输出）与教师模型特征之间的L2蒸馏损失。蒸馏损失可以应用于全局token和patch token。</li>
<li><strong>优势</strong>：<ul>
<li><strong>语义增强</strong>：将预训练模型的强大语义能力迁移到HUVR中，使其在识别任务上表现更好。</li>
<li><strong>统一性</strong>：通过蒸馏，确保了压缩表示（TinToks）也具备良好的语义信息，从而支持下游的识别任务。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构概览 (Figure 2):</strong></p>
<ul>
<li><strong>输入图像</strong> -&gt; <strong>Transformer Encoder</strong> (处理patch tokens和global token) -&gt; <strong>MLP</strong> -&gt; <strong>INR HyperNetwork</strong></li>
<li>INR HyperNetwork输出：<ul>
<li><strong>Standard Compressed Encoding (d = ViT)</strong>：用于识别任务。</li>
<li><strong>Tiny Tokens (d = Tiny)</strong>：压缩表示，用于识别和生成。</li>
<li><strong>INR Encoding (d = INR)</strong>：用于生成INR权重。</li>
</ul>
</li>
<li><strong>INR Modulation Matrix</strong>：由INR HyperNetwork的输出生成，用于调制<strong>Base Patch INR</strong>的权重，得到<strong>Predicted Patch INR</strong>。</li>
<li><strong>Patch</strong>：通过INR网络（使用调制后的权重）进行重构。</li>
<li><strong>Reconstruction</strong>：将重构的patches拼接起来，形成最终的重构图像。</li>
</ul>
<p><strong>训练目标：</strong>
*   <strong>INR重构损失</strong>：像素级MSE损失，用于评估重构图像与原始图像的相似度。
*   <strong>蒸馏损失</strong>：用于将预训练模型的语义信息迁移到HUVR模型中。
*   <strong>视觉质量损失</strong>：可选，如SSIM, LPIPS等，进一步提升重构质量。</p>
<h3 id="4">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>原生统一 vs. 后验融合</strong>：HUVR是第一个原生设计用于同时处理识别和生成任务的模型，而许多现有工作是后验地融合识别和生成模型。</li>
<li><strong>INR超网络 vs. 传统模型</strong>：HUVR利用INR超网络来生成特定于图像的INR权重，这与直接训练一个大型Transformer模型（如DINOv3）或生成模型（如VAE）在根本上不同。它将图像表示学习与神经渲染（INR）结合起来。</li>
<li><strong>TinToks的压缩能力</strong>：TinToks是一种新颖的压缩表示，旨在同时支持识别和生成，这与传统的PCA压缩（主要用于识别）或仅用于生成任务的潜在表示不同。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>首个统一INR超网络</strong>：将INR超网络与Transformer架构结合，实现了对图像表示学习的统一，能够同时进行识别和重构。</li>
<li><strong>TinToks的提出</strong>：设计了一种高效的压缩表示，在保持强大性能的同时，显著减小了嵌入的尺寸。</li>
<li><strong>全局与局部token的协同调制</strong>：通过全局token和patch token的交互来调制INR权重，实现了更精细的控制和更好的性能。</li>
<li><strong>知识蒸馏的应用</strong>：有效地将预训练模型的语义知识注入到INR超网络中，提升了其在识别任务上的表现。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>通用视觉表示学习</strong>：适用于需要同时进行图像识别（分类、分割）和图像生成（重构）的场景。</li>
<li><strong>资源受限环境</strong>：TinToks的引入使其特别适合部署在计算和存储资源受限的设备上。</li>
<li><strong>需要高质量重构的场景</strong>：INR的特性使其在需要精确像素级重构的任务中表现出色。</li>
</ul>
</li>
</ul>
<h3 id="5">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>识别任务评估</strong>：在ImageNet、ObjectNet、FGVC数据集上进行线性探针分类评估。在ADE20K上进行语义分割评估，在NYUv2上进行深度估计评估。</li>
<li><strong>重构任务评估</strong>：在ImageNet验证集上计算PSNR、SSIM、LPIPS指标。</li>
<li><strong>生成任务评估</strong>：使用HUVR的TinToks训练DiT模型，评估生成质量（FID, IS, Precision, Recall）。</li>
<li><strong>消融实验</strong>：通过移除关键组件（如重构损失、全局token、TinToks压缩等）来验证各部分的重要性。</li>
<li><strong>超网络设计对比</strong>：与现有INR超网络方法进行比较。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>识别性能</strong>：HUVR在ImageNet分类上优于DINOv3 (+0.4%)，在ADE20K分割上优于DINOv3 (+1.2 mIoU)，在重构上优于DINOv3 (+4.84 PSNR)。</li>
<li><strong>TinToks性能</strong>：压缩比为96x的TinToks，在ImageNet分类上比DINOv3 PCA基线高48%，在同等嵌入尺寸下比Stable Diffusion VAE高1.26 PSNR。</li>
<li><strong>重构性能</strong>：HUVR的INR超网络在ImageNette上取得了SOTA的PSNR结果，且训练时间更短。</li>
<li><strong>统一性</strong>：HUVR是第一个能够同时在压缩表示上实现良好识别和重构性能的方法。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>统一表示</strong>：在需要同时进行识别和重构的任务上，HUVR表现出强大的统一能力。</li>
<li><strong>压缩表示</strong>：TinToks在保持高识别性能的同时，实现了极高的压缩率，在资源受限场景下优势明显。</li>
<li><strong>INR重构</strong>：在像素级重构任务上，HUVR的INR超网络表现出色。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>生成质量</strong>：虽然HUVR的TinToks具有生成潜力，但直接使用DiT模型生成的图像质量仍不如专门的生成模型（如SD VAE）。</li>
<li><strong>训练成本</strong>：尽管INR超网络加速了INR的训练，但整个HUVR模型的预训练仍然需要大量的计算资源和时间。</li>
<li><strong>数据依赖</strong>：与SigLIP2等方法相比，HUVR在某些数据集上可能需要更多数据或更长的训练时间才能达到最佳性能。</li>
<li><strong>工程复杂性</strong>：将INR超网络、Transformer和知识蒸馏结合起来，增加了模型的实现和调优的复杂性。</li>
</ul>
</li>
</ul>
<h3 id="6">6. 实用指南</h3>
<ul>
<li>
<p><strong>开源情况</strong>：论文提供了代码链接（<a href="https://github.com/facebookresearch/dinov3">https://github.com/facebookresearch/dinov3</a>），方便研究者复现和使用。</p>
</li>
<li>
<p><strong>实现细节</strong>：</p>
<ul>
<li><strong>预训练数据</strong>：使用DataComp和ImageNet22k混合数据进行预训练。</li>
<li><strong>Transformer架构</strong>：使用ViT-B/16和ViT-L/16。</li>
<li><strong>位置编码</strong>：使用Rotary Positional Embeddings (RoPE)。</li>
<li><strong>知识蒸馏</strong>：使用DINOv3作为教师模型，蒸馏损失应用于Encoder和Decoder的输出。</li>
<li><strong>TinToks维度</strong>：<code>dt</code>是可调参数，实验中使用了32维。</li>
<li><strong>INR架构</strong>：使用带有PixelShuffle的上采样层，而不是纯MLP。</li>
<li><strong>训练设置</strong>：AdamW优化器，余弦退火学习率，梯度裁剪。</li>
</ul>
</li>
<li>
<p><strong>迁移可能</strong>：</p>
<ul>
<li><strong>其他视觉任务</strong>：HUVR的核心思想是学习一种通用的视觉表示，其TinToks和标准表示理论上可以用于各种下游视觉任务，如目标检测、实例分割等，但可能需要针对性地进行微调。</li>
<li><strong>多模态任务</strong>：论文提到未来工作可以探索与Vision Language Models (VLMs)的结合，这需要文本对齐的预训练。</li>
<li><strong>生成任务的改进</strong>：通过更先进的扩散模型或生成器设计，可以进一步提升HUVR在生成任务上的表现。</li>
<li><strong>INR的改进</strong>：将HUVR的INR超网络与更高效的INR架构结合，可能进一步提升重构和生成质量。</li>
</ul>
</li>
</ul>
<h3 id="7">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：INR超网络+TinToks实现统一视觉表示。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li>图像输入Transformer，生成标准token。</li>
<li>通过下采样生成微小token（TinToks）。</li>
<li>利用全局/局部token生成INR权重，用于重构。</li>
<li>通过知识蒸馏增强语义，实现统一识别与生成。</li>
</ol>
</li>
</ul>
<hr />
<p>这篇论文提出了一种非常有前景的统一视觉表示学习框架，通过巧妙地结合INR超网络、Transformer和知识蒸馏，实现了在识别和生成任务上的良好性能，特别是其提出的TinToks在压缩表示方面具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks.</li>
<li>The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14256v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14256v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14255v1'></a></p>
<h2 id="videomama-mask-guided-video-matting-via-generative-prior"><a href="https://arxiv.org/abs/2601.14255v1">VideoMaMa: Mask-Guided Video Matting via Generative Prior</a></h2>
<p><strong>Authors:</strong> Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim, Joon-Young Lee</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域高水平研究生，专注于深入分析论文的方法部分，并按照您提供的框架进行详细解读。请提供您希望我分析的论文。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models.</li>
<li>Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions.</li>
<li>To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14255v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14255v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14253v1'></a></p>
<h2 id="motion-3-to-4-3d-motion-reconstruction-for-4d-synthesis"><a href="https://arxiv.org/abs/2601.14253v1">Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis</a></h2>
<p><strong>Authors:</strong> Hongyuan Chen, Xingyu Chen, Youjia Zhang, Zexiang Xu, Anpei Chen</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="motion-3-to-4-3d-motion-reconstruction-for-4d-synthesis_1">论文方法分析与总结：《Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis》</h2>
<h3 id="1_1">1. 摘要翻译</h3>
<p>本文提出了一种名为 <strong>Motion 3-to-4</strong> 的前馈框架，旨在从单个单目视频和可选的3D参考网格中合成高质量的4D动态对象。尽管2D、视频和3D内容生成领域取得了显著进展，但由于训练数据有限以及从单目视角恢复几何和运动的固有模糊性，4D合成仍然面临挑战。Motion 3-to-4 通过将4D合成分解为静态3D形状生成和运动重建两个任务来解决这些挑战。利用一个规范的参考网格，该模型学习紧凑的运动潜在表示，并预测逐帧的顶点轨迹，以恢复完整、时间连贯的几何体。一个可扩展的逐帧Transformer进一步增强了对不同序列长度的鲁棒性。在标准基准和具有精确地面真实几何的新数据集上的评估表明，Motion 3-to-4 与现有工作相比，在保真度和空间一致性方面表现更优。项目页面可在 <a href="https://motion3-to-4.github.io/">https://motion3-to-4.github.io/</a> 访问。</p>
<h3 id="2_1">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>4D内容创作需求</strong>：虚拟现实、电影制作、机器人和模拟等领域对高保真4D资产（同时捕捉静态形状和动态运动）的需求日益增长。</li>
<li><strong>现有4D合成的挑战</strong>：当前4D合成方法在数据稀缺、单目视角下的几何和运动恢复模糊性以及时间连贯性方面存在显著困难。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>多视角生成依赖</strong>：许多方法依赖于多视角生成，但受限于2D生成模型的视图不一致性。</li>
<li><strong>优化缓慢且易出错</strong>：基于优化的方法（如迭代网格对齐）耗时且容易出现时间伪影。</li>
<li><strong>VAE模型的局限性</strong>：基于VAE的方法虽然高效，但需要大规模、多样化的训练数据来构建良好的潜在分布，在有限的4D数据集上泛化能力差。</li>
<li><strong>3D生成+4D对齐的低效</strong>：先生成3D网格再进行时间对齐的策略（如V2M4）耗时且容易出现拓扑漂移。</li>
<li><strong>渲染监督的不足</strong>：依赖渲染监督的方法（如GVFD）在稀缺的4D数据上训练，可能导致几何和3D结构较弱。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>将4D合成问题分解为 <strong>静态3D形状生成</strong> 和 <strong>动态运动重建</strong> 两个更易处理的子问题，可以有效克服单目4D合成的挑战。</li>
<li>利用一个 <strong>稳定的静态网格作为参考几何体</strong>，并估计相对于该规范状态的逐帧3D运动流，可以实现更鲁棒和精确的4D重建。</li>
<li>一个 <strong>前馈（feed-forward）的、可扩展的Transformer架构</strong> 能够高效地处理不同序列长度，并学习到强大的运动表示。</li>
</ul>
</li>
</ul>
<h3 id="3_1">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：</p>
<p>Motion 3-to-4 的核心思想是将4D合成分解为 <strong>静态3D形状生成</strong> 和 <strong>动态运动重建</strong>。其整体流程如下：</p>
<ol>
<li><strong>输入</strong>：单个单目视频（<code>V</code>）和可选的第一个视频帧的3D参考网格（<code>M</code>）。</li>
<li><strong>静态3D形状编码 (Motion Latent Learning - Geometry)</strong>：<ul>
<li>如果未提供参考网格，则使用预训练的3D生成模型（如Hunyuan3D 2.0 [110]）从视频第一帧生成一个初始网格。</li>
<li>对参考网格 <code>M</code> 进行采样，得到 <code>N</code> 个点 <code>X₀ = {(xi, ni, ci)}</code>，包含3D坐标、法线和颜色。</li>
<li>使用一个 <strong>Shape Encoder</strong>（受3DShape2VecSet [99]启发）将这些点嵌入到一个紧凑的1D潜在表示 <code>Zₓ ∈ R^(K×C)</code> 中。该编码器通过一个可学习的查询集 <code>A</code> 与采样点 <code>X₀</code> 进行 <strong>自注意力（Self-Attention）</strong> 聚合，捕捉网格的几何和语义结构。</li>
</ul>
</li>
<li><strong>视频特征提取与时空信息融合 (Motion Latent Learning - Video)</strong>：<ul>
<li>使用预训练的 <strong>DINOv2 [56] 编码器</strong> 提取视频 <code>V</code> 的逐帧 <strong>Patch-level特征</strong>。</li>
<li>为这些Patch特征注入 <strong>时间嵌入（Temporal Embeddings）</strong>，使其感知帧的顺序。</li>
<li>采用 <strong>Alternating-Attention架构</strong>（VGGT [75] 启发），结合 <strong>全局注意力（Global Attention）</strong> 和 <strong>逐帧注意力（Frame-wise Attention）</strong> 来聚合时空信息。</li>
<li>将静态形状的潜在表示 <code>Zₓ</code> 与视频特征融合，生成每个时间步 <code>t</code> 的 <strong>运动感知潜在表示 <code>Zₜ</code></strong>。该表示同时编码了共享的几何结构和帧特定的运动信息。</li>
</ul>
</li>
<li><strong>运动解码 (Motion Decoding)</strong>：<ul>
<li>使用一个 <strong>Motion Decoder</strong>，它是一个 <strong>交叉注意力（Cross-Attention）</strong> 解码器。</li>
<li>将参考网格 <code>M</code> 的 <code>M</code> 个采样点 <code>P₀ = {(xi, ni, ci)}</code> 作为查询（Queries）。</li>
<li>利用运动感知潜在表示 <code>Zₜ</code> 作为键（Keys）和值（Values）。</li>
<li>解码器预测每个查询点在时间步 <code>t</code> 的 <strong>逐帧3D运动流（Per-frame 3D motion flow）</strong>，即相对于参考网格的顶点轨迹 <code>Xₜ</code>。</li>
<li>最终通过一个共享的 <strong>MLP</strong> 将解码后的点特征映射到最终的3D坐标。</li>
</ul>
</li>
<li><strong>输出</strong>：生成时间连贯的4D动态对象，包含完整的几何体和运动。</li>
</ol>
<p><strong>模型结构</strong>：</p>
<ul>
<li><strong>Shape Encoder</strong>：负责将输入的3D网格（或从视频生成的网格）编码成一个紧凑的、语义丰富的潜在表示 <code>Zₓ</code>。它利用了Point Cloud Encoder和Self-Attention机制。</li>
<li><strong>Video Encoder (DINOv2)</strong>：负责从输入的单目视频中提取具有鲁棒性的Patch-level特征。</li>
<li><strong>Motion Latent Learning Module</strong>：这是核心模块，结合了Shape Encoder和Video Encoder的输出。它通过Alternating-Attention架构（Global-Frame Attention）来融合时空信息，生成每个时间步的运动感知潜在表示 <code>Zₜ</code>。</li>
<li><strong>Motion Decoder</strong>：一个基于Cross-Attention的解码器，将参考网格的点作为查询，利用 <code>Zₜ</code> 来预测每个点的逐帧运动轨迹。</li>
<li><strong>MLP Head</strong>：将解码后的运动特征转换为最终的3D坐标。</li>
</ul>
<p><strong>算法解释</strong>：</p>
<ul>
<li>
<p><strong>Alternating-Attention Architecture</strong>：</p>
<ul>
<li><code>Z⁽ˡ⁾</code> 表示第 <code>l</code> 层注意力后的表示。</li>
<li><strong>Global Update</strong>: <code>Z⁽ˡ⁾ = GlobalAttn(Z⁽ˡ⁻¹⁾)</code>：在全局层面聚合信息，捕捉跨帧的整体运动趋势。</li>
<li><strong>Frame-wise Update</strong>: <code>Z⁽ˡ⁾ = FrameAttn(Z⁽ˡ⁻¹⁾)</code>：在逐帧层面进行注意力计算，捕捉当前帧的细节运动。</li>
<li>这种交替设计旨在高效地处理长序列，同时保持空间和时间依赖性。</li>
</ul>
</li>
<li>
<p><strong>Motion Decoder (Cross-Attention)</strong>：</p>
<ul>
<li><code>Xₜ = MotionDecoder(X₀, Zₜ)</code>：<ul>
<li><code>X₀</code> (sampled points from reference mesh) 作为 <strong>Queries</strong>。</li>
<li><code>Zₜ</code> (motion-aware latent representation) 作为 <strong>Keys</strong> 和 <strong>Values</strong>。</li>
</ul>
</li>
<li>这个交叉注意力机制使得模型能够将参考网格上的每个点“对齐”到视频中的对应像素或区域，从而预测其在当前帧的运动。这种方式确保了表面对应关系的一致性，避免了独立预测每帧几何的拓扑漂移问题。</li>
</ul>
</li>
</ul>
<h3 id="4_1">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>分解策略</strong>：Motion 3-to-4 核心创新在于将4D合成分解为 <strong>静态形状生成</strong> 和 <strong>动态运动重建</strong>，并利用一个 <strong>固定的参考网格</strong> 来驱动运动预测。这与许多方法（如直接生成4D NeRF、多视角生成后优化、或逐帧生成3D模型再对齐）有本质区别。</li>
<li><strong>运动表示</strong>：它不直接学习4D的完整表示，而是学习一个 <strong>紧凑的运动潜在表示</strong>，并将其与静态形状信息结合，通过 <strong>逐帧运动流预测</strong> 来实现4D动态。</li>
<li><strong>参考网格的作用</strong>：参考网格提供了一个稳定的、具有固定拓扑的几何基础，使得运动预测可以被视为一个 <strong>表面点到像素的对齐问题</strong>，大大简化了问题难度并保证了时间一致性。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>新颖的4D合成框架</strong>：提出了一种前馈、端到端的4D合成框架，将4D问题分解为更易处理的子问题。</li>
<li><strong>静态形状与动态运动的解耦</strong>：通过利用参考网格，实现了形状和运动的有效解耦，提高了泛化能力和鲁棒性。</li>
<li><strong>可扩展的Transformer架构</strong>：设计了Alternating-Attention架构，能够处理任意长度的视频序列。</li>
<li><strong>高效的运动表示学习</strong>：通过运动感知潜在表示和交叉注意力解码器，实现了高效且准确的运动预测。</li>
<li><strong>新的Motion-80数据集</strong>：为4D重建任务提供了更具挑战性的基准。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>单目视频输入</strong>：对输入视频的视角要求较低，适用于单目视频。</li>
<li><strong>具有可变形物体</strong>：适用于物体形状会随时间变化的场景。</li>
<li><strong>需要高保真和时间连贯性</strong>：在保真度和时间一致性方面表现优异。</li>
<li><strong>已有3D模型作为先验</strong>：当有第一个视频帧的3D模型时，可以提供更强的先验信息，提升效果（"Ours w/m"）。</li>
</ul>
</li>
</ul>
<h3 id="5_1">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：在两个数据集上进行评估：<ul>
<li><strong>Motion-80</strong>：作者自建的数据集，包含丰富的纹理和多样的运动，有短序列和长序列。</li>
<li><strong>Consistent4D benchmark [29]</strong>：一个已有的基准，用于评估渲染指标。</li>
</ul>
</li>
<li><strong>基线方法</strong>：与多种SOTA方法进行比较，包括：<ul>
<li><strong>前馈方法</strong>：L4GM [60] (3D Gaussians)，GVFD [100] (VAE-based motion generation)。</li>
<li><strong>优化方法</strong>：V2M4 [7] (3D Gen. + 4D Align)。</li>
</ul>
</li>
<li><strong>评估指标</strong>：<ul>
<li><strong>几何指标</strong>：Chamfer Distance (CD)，F-Score。</li>
<li><strong>外观指标</strong>：LPIPS，CLIP，FVD，DreamSim。</li>
</ul>
</li>
<li><strong>消融实验</strong>：通过移除或替换模型中的关键模块（如Frame Attn, Global Attn, Ref Token）来验证各组件的有效性。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>几何性能</strong>：在Motion-80数据集上，Motion 3-to-4 在CD和F-Score上均显著优于所有基线方法。</li>
<li><strong>外观性能</strong>：在CLIP和DreamSim指标上，Motion 3-to-4 也优于基线方法，表明其生成内容更具保真度和一致性。</li>
<li><strong>“Ours w/m”</strong>：当使用地面真实静态网格作为输入时，性能进一步大幅提升，证明了其运动重建能力的强大。</li>
<li><strong>消融实验</strong>：结果表明，Frame Attn, Global Attn, Ref Token 都是提升模型性能的关键组件。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>长序列处理</strong>：在长序列上，Motion 3-to-4 表现出比其他方法更稳定的时间连贯性。</li>
<li><strong>非正交视角</strong>：在Consist4D数据集上，与L4GM等方法在非正交视角下出现严重鬼影伪影不同，Motion 3-to-4 表现出更好的鲁棒性。</li>
<li><strong>从静态网格生成动态4D</strong>：这是Motion 3-to-4 的独特优势，能够将现有的3D模型转化为动态4D内容。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>几何编码器不显式建模拓扑</strong>：当物体不同部分在参考网格中不清晰分离时，可能导致顶点粘连（Vertex sticking artifacts），如图7(A)所示。</li>
<li><strong>依赖初始网格拓扑</strong>：当运动导致物体拓扑发生剧烈变化时，基于第一个视频帧生成的初始网格可能无法适应，导致失败（如图7(B)所示）。</li>
<li><strong>计算开销</strong>：虽然是前馈模型，但处理长序列和高分辨率网格仍需一定的计算资源。</li>
</ul>
</li>
</ul>
<h3 id="6_1">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文已开源，项目页面提供了代码链接。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>Shape Encoder</strong>：使用3DShape2VecSet [99] 架构，输入4096个采样点。</li>
<li><strong>Video Encoder</strong>：使用DINOv2-ViT-B/14 [56]，输入224x224视频。</li>
<li><strong>Motion Latent Learning</strong>：16层Alternating-Attention（8 Global, 8 Frame）。</li>
<li><strong>Motion Decoder</strong>：交叉注意力，使用64个运动Token。</li>
<li><strong>训练</strong>：AdamW优化器，学习率<code>4e-4</code>，余弦退火调度，1000步warm-up，梯度裁剪（norm 1.0），BF16混合精度。</li>
<li><strong>数据</strong>：12帧序列训练，时间数据增强（步长1, 2, 4）。</li>
<li><strong>推理</strong>：对于无网格输入，使用Hunyuan3D 2.0 [110] 生成初始网格。对于长视频，使用滑动窗口（步长255）处理。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他3D生成模型</strong>：Shape Encoder部分可以替换为其他先进的3D生成模型，以获得更好的初始形状。</li>
<li><strong>运动迁移</strong>：论文展示了Motion Transfer的能力（图6），表明该方法可以用于将一个视频的运动迁移到另一个具有不同形状和外观的3D模型上。这为动画制作和虚拟角色驱动提供了新的可能性。</li>
<li><strong>其他任务</strong>：该框架的解耦思想（静态形状+动态运动）可以借鉴到其他需要处理动态几何的任务中，例如动态场景重建、物体交互模拟等。</li>
</ul>
</li>
</ul>
<h3 id="7_1">7. 总结</h3>
<ul>
<li>
<p><strong>核心思想</strong>：<strong>解耦4D合成，用静态网格驱动动态运动。</strong></p>
</li>
<li>
<p><strong>速记版pipeline</strong>：</p>
<ol>
<li><strong>获取基础形状</strong>：从视频第一帧生成或直接使用一个3D模型。</li>
<li><strong>学习运动信号</strong>：用视频信息提取运动特征，并与形状信息融合。</li>
<li><strong>预测运动轨迹</strong>：根据运动信号，预测基础形状上每个点的移动方向和距离。</li>
<li><strong>生成动态4D模型</strong>：将预测的运动应用到基础形状上，形成随时间变化的4D模型。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh.</li>
<li>Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14253v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14253v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14251v1'></a></p>
<h2 id="lightonocr-a-1b-end-to-end-multilingual-vision-language-model-for-state-of-the-art-ocr"><a href="https://arxiv.org/abs/2601.14251v1">LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</a></h2>
<p><strong>Authors:</strong> Said Taghadouini, Adrien Cavaillès, Baptiste Aubertin</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9<script type="math/tex">\times</script> smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域的专业高水平研究生，深入分析您提供的论文方法部分。我将重点关注论文的新颖之处、动机、设计逻辑、流程细节、优势与不足，并提供一个清晰、结构化的分析框架。</p>
<p>请提供您想要我分析的论文内容。我将按照上述框架进行详细解读。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines.</li>
<li>Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9<script type="math/tex">\times</script> smaller and substantially faster than prior best-performing models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14251v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14251v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14250v1'></a></p>
<h2 id="omnitransfer-all-in-one-framework-for-spatio-temporal-video-transfer"><a href="https://arxiv.org/abs/2601.14250v1">OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</a></h2>
<p><strong>Authors:</strong> Pengze Zhang, Yanze Wu, Mengtian Li, Xu Bai, Songtao Zhao, Fulong Ye, Chong Mou, Xinghui Li, Zhuowei Chen, Qian He, Mingyuan Gao</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种名为 OmniTransfer 的统一框架，旨在解决现有视频定制方法在利用视频固有的时空信息方面存在的不足。OmniTransfer 通过多视角信息增强外观一致性，并利用时间线索实现精细的时间控制，从而在外观（ID 和风格）和时间（相机运动和视频效果）转移方面超越现有方法，并能在不依赖姿态信息的情况下实现与姿态引导方法相当的运动转移效果。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>OmniTransfer 的核心创新在于其“All-in-one”的统一框架设计，以及为实现这一目标而引入的三个关键机制：</p>
<ul>
<li><strong>任务感知位置偏差 (Task-aware Positional Bias):</strong> 这是该框架处理不同视频转移任务的关键。它能够自适应地利用参考视频的信息，以改进时间对齐或外观一致性。这意味着模型不是采用一种通用的方法，而是根据具体的任务需求（例如，是需要保持人物身份的风格转移，还是需要复制视频的运动轨迹）来调整其对参考视频信息的利用方式。</li>
<li><strong>参考解耦因果学习 (Reference-decoupled Causal Learning):</strong> 这个设计旨在提高效率和精确性。通过将参考分支和目标分支分离，模型可以更精确地将参考视频的特征（如外观、风格或运动）转移到目标视频上，同时避免了不必要的计算和潜在的混淆。这种解耦可能有助于模型更好地理解和隔离需要转移的信息。</li>
<li><strong>任务自适应多模态对齐 (Task-adaptive Multimodal Alignment):</strong> 为了处理多样化的视频转移任务，该框架引入了多模态语义引导。这使得模型能够动态地识别和区分不同的任务，并采取相应的对齐策略。多模态的引入（可能包括文本描述、音频等）为模型提供了更丰富的上下文信息，使其能够更灵活地适应各种复杂的视频生成场景。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>OmniTransfer 的提出可能对视频生成和定制领域产生深远影响：</p>
<ul>
<li><strong>统一化和通用性:</strong> 它提供了一个通用的解决方案，有望取代目前针对不同任务需要不同模型的局面，大大简化了视频生成的工作流程。</li>
<li><strong>性能提升:</strong> 在外观和时间转移方面超越现有方法，以及在运动转移方面达到与姿态引导方法相当的水平，表明其在生成质量和控制精度上取得了显著进步。</li>
<li><strong>新范式:</strong> 它可能开启一个“灵活、高保真视频生成”的新范式，为研究人员和开发者提供更强大的工具。</li>
<li><strong>降低技术门槛:</strong> 通过提供一个更易于使用的统一框架，可能降低视频生成技术的应用门槛。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作与媒体制作:</strong> 电影、广告、短视频等领域的特效制作、风格化处理、内容再利用。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 生成逼真的虚拟场景、角色动画和交互体验。</li>
<li><strong>游戏开发:</strong> 快速生成游戏中的动态场景和角色动作。</li>
<li><strong>个性化视频生成:</strong> 用户可以根据自己的需求定制视频，例如将自己的形象融入到电影片段中。</li>
<li><strong>教育和培训:</strong> 创建更具吸引力和互动性的教学视频。</li>
<li><strong>数字人技术:</strong> 生成更自然、更具表现力的数字人动画。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了一个强大的框架，但仍有一些潜在的局限性可以推断出来：</p>
<ul>
<li><strong>计算资源需求:</strong> 尽管提到了“效率”，但处理时空信息、多视角信息以及多模态对齐通常需要大量的计算资源（GPU、内存）。</li>
<li><strong>数据依赖性:</strong> 像大多数深度学习模型一样，OmniTransfer 的性能很可能高度依赖于训练数据的质量和数量。</li>
<li><strong>“无姿态”运动转移的解释:</strong> 摘要提到“matching pose-guided methods in motion transfer without using pose”。这可能意味着在某些复杂或精细的运动转移任务上，虽然能达到相似的水平，但其内在的运动理解机制可能与直接使用姿态信息的方法有所不同，或者在某些特定场景下仍有差距。需要进一步的实验来验证其通用性和鲁棒性。</li>
<li><strong>“任务自适应”的粒度:</strong> “Task-adaptive Multimodal Alignment”的有效性取决于其对不同任务的区分能力有多精细。如果任务界限模糊，或者存在混合任务，其表现可能需要进一步考察。</li>
<li><strong>可解释性:</strong> 尽管框架设计精巧，但其内部机制（如“任务感知位置偏差”的具体实现）的可解释性可能是一个挑战。</li>
</ul>
<p>总而言之，OmniTransfer 是一项令人兴奋的研究，它通过创新的统一框架和三个关键机制，有望显著提升视频生成和定制的能力，为该领域带来新的突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer.</li>
<li>Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14250v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14250v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14246v1'></a></p>
<h2 id="soft-tail-dropping-for-adaptive-visual-tokenization"><a href="https://arxiv.org/abs/2601.14246v1">Soft Tail-dropping for Adaptive Visual Tokenization</a></h2>
<p><strong>Authors:</strong> Zeyuan Chen, Kai Zhang, Zhuowen Tu, Yuanjun Xiong</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析您提供的论文，并遵循您提出的分析框架。请提供论文的PDF文件，我将为您进行详细的解读。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14246v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14246v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14208v1'></a></p>
<h2 id="rig-aware-3d-reconstruction-of-vehicle-undercarriages-using-gaussian-splatting"><a href="https://arxiv.org/abs/2601.14208v1">Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting</a></h2>
<p><strong>Authors:</strong> Nitin Kulkarni, Akhil Devarashetti, Charlie Cluss, Livio Forte, Dan Buckmaster, Philip Schneider, Chunming Qiao, Alina Vereshchaka</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV, cs.GR, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇关于“Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting”的论文。我将重点关注其方法部分的创新点、设计逻辑、优势与不足，并提供实用的分析和指导。</p>
<hr />
<h2 id="_1">论文方法分析与总结</h2>
<h3 id="1_2">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> 基于高程感知的3D高斯喷溅的车辆底盘3D重建</p>
<p><strong>摘要：</strong> 检查二手车的底盘是一项劳动密集型任务，需要检查员蹲伏或爬行在车辆下方进行彻底检查。此外，在线买家很少能看到底盘照片。我们提出了一种端到端的流程，利用一个三摄像头阵列，在车辆驶过时捕捉底盘的视频，并生成一个交互式的3D模型。该3D模型使检查员和客户能够旋转、缩放和切割底盘，从而在几秒钟内检测出锈蚀、泄漏或碰撞损坏，从而提高工作场所的安全性和买家信心。我们的主要贡献是一个高程感知的结构从运动（SfM）流程，专门设计用于克服广角镜头畸变和低视差场景的挑战。我们的方法通过集成精确的相机标定、同步的视频流和强大的相机阵列几何先验，克服了广角镜头畸变和低视差场景的挑战。我们使用一种受限的匹配策略，结合学习到的组件，如DISK特征提取器和基于注意力机制的LightGlue匹配器，生成通常在标准SfM流程中无法获得的高质量稀疏点云。这些点云为高斯喷溅过程提供种子，生成可实时渲染的逼真底盘模型。我们的实验和消融研究表明，我们的设计选择对于实现最先进的质量至关重要。</p>
<h3 id="2_2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>提升二手车交易透明度与效率</strong>：当前二手车交易中，底盘检查是关键但效率低下且存在安全隐患的环节。在线买家无法直观了解车辆底盘状况，增加了交易的不确定性。</li>
<li><strong>解决现有技术瓶颈</strong>：传统的底盘检查方法（人工检查）效率低、安全性差；现有的3D重建技术（如NeRF）在生产环境中训练和渲染速度过慢，无法满足实时交互的需求。</li>
<li><strong>应对特定场景挑战</strong>：车辆底盘检查场景具有独特的挑战，如<strong>广角镜头带来的严重畸变</strong>和<strong>低视差（车辆移动距离与相机到物体距离的比例小）</strong>，这使得传统的SfM方法难以获得高质量的稀疏点云。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>人工检查</strong>：劳动强度大、效率低、存在安全风险（如跌落、接触有害物质）、主观性强。</li>
<li><strong>缺乏底盘视图</strong>：在线交易中，买家无法获得底盘的详细信息，导致信息不对称。</li>
<li><strong>传统SfM</strong>：<ul>
<li>对<strong>广角镜头畸变</strong>敏感，难以准确建模和校正，导致特征匹配困难和点云漂移。</li>
<li>在<strong>低视差场景</strong>下，视图间的差异很小，难以进行精确的三角测量，容易产生几何退化和累积误差。</li>
</ul>
</li>
<li><strong>NeRF类方法</strong>：虽然能生成逼真视图，但训练和渲染速度慢，不适合生产环境下的实时交互需求。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>通过<strong>精确的相机标定</strong>和<strong>严格的视频同步</strong>，可以有效缓解广角镜头畸变和低视差带来的问题。</li>
<li>结合<strong>学习型特征提取器（DISK）</strong>和<strong>基于注意力机制的匹配器（LightGlue）</strong>，可以在复杂场景下获得更鲁棒、更密集的特征匹配。</li>
<li>利用<strong>相机阵列的几何先验（Rig-Aware）</strong>，可以约束SfM的优化过程，防止相机位姿漂移，生成更准确的稀疏点云。</li>
<li>将高质量的稀疏点云作为<strong>3D高斯喷溅（3D-GS）</strong>的种子，可以在保证实时渲染的同时，生成逼真、细节丰富的3D模型。</li>
</ul>
</li>
</ul>
<h3 id="3_2">3. 方法设计详解</h3>
<p>该方法是一个<strong>端到端的流水线</strong>，主要包含四个核心步骤：</p>
<p><strong>整体流程图 (Fig. 1):</strong></p>
<pre class="codehilite"><code>Raw undercarriage videos from 3-way camera rig
↓
1. One-Time Initialization (Camera Calibration)
    - ChArUco video sweep
    - Frame curation
    - Estimate camera intrinsics
    - Undistortion validation
↓
2. Video Synchronization (Per Vehicle)
    - Phase correlation (motion)
    - Offset search + trim
    - Aligned frame triplets
↓
3. Structure-from-Motion (Rig-Aware SfM)
    - Left Camera, Central Camera, Right Camera
    - Sharp frame triplets selection
    - Undistort frames
    - DISK feature extraction &amp; matching via LightGlue
    - Generating sparse 3D point cloud
↓
4. Gaussian Splatting
    - Initialize Gaussians: (μ, Σ, α, c)
    - Iterative 2D projection + Blending
    - Interactive 3D undercarriage visualization
</code></pre>

<p><strong>详细步骤解析：</strong></p>
<p><strong>步骤 1: 相机标定 (One-Time Initialization)</strong></p>
<ul>
<li><strong>动机</strong>：广角镜头（160° FOV）是必需的，但会引入严重的径向和切向畸变。不准确的相机模型会严重影响后续的SfM精度。此步骤只需执行一次。</li>
<li><strong>技术细节</strong>：<ul>
<li><strong>ChArUco Board (Fig. 2)</strong>：使用结合了棋盘格和ArUco标记的ChArUco板。这种组合提供了精确的亚像素角点检测和鲁棒的ID识别，相比纯棋盘格或纯ArUco标记，能获得更低的重投影误差。论文中使用的板子尺寸为53x37内方格，边长22mm，ArUco标记16mm宽。</li>
<li><strong>视频扫描</strong>：让ChArUco板在三个广角相机前进行不同角度（俯仰、偏航、滚转）和距离（30-120cm）的扫描，以捕捉不同基线变化下的图像。</li>
<li><strong>帧筛选 (Frame Curation)</strong>：从视频中滑动一个10帧的窗口，通过计算<strong>拉普拉斯算子方差 (Eq. 1)</strong> 来评估图像的锐度。拉普拉斯算子对边缘敏感，其方差越大表示图像边缘越清晰，模糊越少。选择拉普拉斯方差最大的帧作为最锐利的帧，以减少运动模糊。</li>
<li><strong>相机模型与优化</strong>：<ul>
<li>使用<strong>八参数的OpenCV相机模型</strong>（包含径向畸变k1, k2, k3, k4, k5, k6和切向畸变p1, p2，Eq. 3）。相比标准模型，增加了三个径向畸变系数，以更好地拟合极端广角镜头的复杂畸变。</li>
<li>使用<strong>Levenberg-Marquardt优化器</strong>来拟合模型，最小化<strong>均方根（RMS）重投影误差 (Eq. 4)</strong>。RMS误差衡量实际角点位置与模型预测位置之间的欧氏距离。</li>
<li><strong>定性评估</strong>：通过视觉检查未畸变图像中的直线是否仍然是直线来辅助验证（Fig. 4）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出</strong>：精确的相机内参矩阵 K (Eq. 2) 和畸变系数。</li>
</ul>
<p><strong>步骤 2: 视频同步 (Video Synchronization)</strong></p>
<ul>
<li><strong>动机</strong>：尽管使用硬件触发器，但由于信号和编码延迟，三个摄像机的视频流可能存在亚帧级别的时序偏移，导致空间-时间对应不准确，产生鬼影。</li>
<li><strong>技术细节</strong>：<ul>
<li><strong>两阶段同步</strong>：<ul>
<li><strong>全局垂直运动估计</strong>：利用<strong>相位相关（Phase Correlation）</strong>计算连续帧之间的垂直方向像素位移（shift），得到每个视频的shift序列 {St, Sc, Sr}。在计算前，对帧进行高斯模糊、CLAHE（对比度限制自适应直方图均衡化）和拉普拉斯滤波，以突出关键特征。</li>
<li><strong>L1损失最小化偏移搜索</strong>：通过最小化两个视频shift序列之间的<strong>L1损失 (Eq. 5)</strong> 来寻找最佳的<strong>时间偏移量</strong>。论文采用迭代优化策略：先在一个较大的偏移范围内均匀搜索，找到最优偏移点后，再在一个更小的范围内进行精细搜索，直到达到最小误差。</li>
</ul>
</li>
<li><strong>对齐与裁剪</strong>：一旦找到最佳偏移量，就对视频进行对齐，并裁剪多余帧以使所有视频长度一致，得到<strong>同步的帧三元组</strong>。</li>
</ul>
</li>
<li><strong>关键指标</strong>：同步后，相机对之间的垂直运动平均差异从774像素减少到约22像素。</li>
</ul>
<p><strong>步骤 3: 结构从运动 (Rig-Aware Structure-from-Motion)</strong></p>
<ul>
<li><strong>动机</strong>：在广角、低视差场景下，生成高质量的稀疏点云是后续3D高斯喷溅的基础。传统SfM方法在此场景下表现不佳，需要结合学习型特征和几何先验。</li>
<li><strong>技术细节</strong>：<ul>
<li><strong>帧选择与图像去畸变</strong>：<ul>
<li>从同步后的视频中，均匀采样<strong>k=250个最锐利的帧三元组</strong>。锐度通过<strong>拉普拉斯算子方差的平均值 (Eq. 6)</strong> 来评分。</li>
<li>使用步骤1中得到的相机内参和畸变系数，对选取的帧进行<strong>去畸变</strong>，得到几何上正确的图像。</li>
</ul>
</li>
<li><strong>特征提取</strong>：<ul>
<li>使用<strong>DISK</strong>（一种学习型局部特征描述符），它在大量图像上训练，对视角和光照变化具有更好的鲁棒性。</li>
<li>应用<strong>CLAHE</strong>增强图像对比度，提取更多特征，尤其是在暗区。</li>
<li>每个帧最多提取8192个DISK特征。</li>
</ul>
</li>
<li><strong>受限特征匹配 (Constrained Feature Matching)</strong>：<ul>
<li><strong>匹配策略</strong>：不进行穷举匹配，而是利用<strong>相机阵列的已知时空关系</strong>。定义一个<strong>时间窗口W5(i) = {i-5, ..., i+5}</strong>，并考虑三种匹配对：<ul>
<li><strong>类内匹配 (intra-camera)</strong>：同一相机内不同帧的匹配 (L↔L, C↔C, R↔R)。用于跟踪相机自身运动。</li>
<li><strong>跨相机匹配 (cross-camera)</strong>：<ul>
<li>L↔C</li>
<li>C↔R</li>
</ul>
</li>
<li><strong>不匹配 L↔R</strong>：由于L和R相机基线较大，而相机到物体距离较近，导致视角差异过大，直接匹配困难。</li>
</ul>
</li>
<li><strong>匹配器</strong>：使用<strong>LightGlue</strong>（基于注意力机制的图神经网络），它能考虑全局特征上下文，找到更准确的匹配，即使在纹理稀疏或重复的场景下。</li>
<li><strong>几何验证</strong>：将LightGlue的匹配结果输入<strong>COLMAP</strong>，使用<strong>RANSAC</strong>进行最终的几何验证，过滤异常值，确保与相机模型的一致性。</li>
</ul>
</li>
<li><strong>高程感知稀疏点云生成 (Rig-Aware Sparse Point Cloud Generation)</strong>：<ul>
<li>采用<strong>增量式SfM</strong>方法，从一个强图像对开始，逐步添加新视图。</li>
<li><strong>关键创新：相机阵列几何先验集成</strong>：在<strong>捆绑调整（Bundle Adjustment, BA）</strong>优化中，将相机阵列的相对位姿作为先验信息。<ul>
<li><strong>目标函数 (Eq. 7)</strong>：最小化重投影误差。</li>
<li><strong>正则化项</strong>：基于相机阵列的相对位置和方向（例如，提供左相机和右相机相对于中心相机的位姿先验：<script type="math/tex">t_{L-C} = [-0.31,0,0]</script>, <script type="math/tex">t_{C-R} = [+0.31, 0, 0]</script>）。</li>
</ul>
</li>
<li><strong>优势</strong>：这种“高程感知”（Rig-Aware）的BA能够<strong>防止相机位姿漂移</strong>，尤其是在低视差场景下，从而生成更准确、更一致的稀疏点云。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出</strong>：稀疏3D点云和精确的相机位姿。</li>
</ul>
<p><strong>步骤 4: 高斯喷溅 (Gaussian Splatting)</strong></p>
<ul>
<li><strong>动机</strong>：将SfM生成的稀疏点云转化为一个<strong>逼真、可交互的3D模型</strong>，并实现<strong>实时渲染</strong>。</li>
<li><strong>技术细节</strong>：<ul>
<li><strong>初始化</strong>：<ul>
<li>将SfM生成的稀疏点云中的每个点初始化为一个<strong>3D高斯分布 (Eq. 8)</strong>。</li>
<li><strong>均值 μᵢ</strong>：来自稀疏点云的点坐标。</li>
<li><strong>颜色 Cᵢ</strong>：初始化为对应图像像素的颜色。</li>
<li><strong>不透明度 αᵢ</strong>：初始化为1。</li>
<li><strong>协方差 Σᵢ</strong>：初始化为各向同性协方差矩阵（σ²I），并根据局部点密度进行缩放。</li>
</ul>
</li>
<li><strong>优化</strong>：通过<strong>迭代2D投影和Alpha混合</strong>来优化高斯分布的参数（位置、形状、颜色、不透明度），使其能够重建物体表面的外观。</li>
<li><strong>渲染</strong>：使用<strong>可见性感知的前向Alpha混合</strong>来合成最终图像。</li>
</ul>
</li>
<li><strong>优势</strong>：3D-GS相比NeRF，在训练和渲染速度上具有显著优势，非常适合生产环境。</li>
<li><strong>输出</strong>：可交互的、逼真的3D车辆底盘模型。</li>
</ul>
<h3 id="4_2">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>与传统SfM</strong>：本文方法在SfM阶段引入了<strong>学习型特征（DISK+LightGlue）</strong>和<strong>相机阵列几何先验（Rig-Aware BA）</strong>，专门解决了广角、低视差场景下的挑战，而传统SfM通常依赖手工特征且对畸变和低视差敏感。</li>
<li><strong>与NeRF</strong>：本文方法使用<strong>3D高斯喷溅</strong>作为最终表示，而非NeRF的隐式神经表示。3D-GS在<strong>训练速度和实时渲染能力</strong>上远超NeRF，更适合生产应用。同时，3D-GS依赖于SfM生成的稀疏点云，因此高质量的SfM是关键。</li>
<li><strong>与现有3D-GS方法</strong>：大多数3D-GS方法直接使用COLMAP等标准SfM工具生成的点云。本文的创新在于<strong>定制化、高程感知的SfM流程</strong>，为3D-GS提供了更优质的种子点云，从而提升了最终模型的质量。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>高程感知SfM流程</strong>：将相机阵列的几何结构作为先验信息融入SfM的BA优化中，有效解决了低视差场景下的相机位姿漂移问题。</li>
<li><strong>针对性地结合学习型特征</strong>：在SfM阶段，将DISK特征提取器和LightGlue匹配器与受限匹配策略结合，克服了广角镜头畸变和复杂纹理下的特征匹配难题。</li>
<li><strong>端到端的流水线</strong>：将相机标定、视频同步、高程感知SfM和3D高斯喷溅无缝集成，形成一个完整的、可用于生产环境的3D重建系统。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>车辆底盘检查</strong>：这是论文的核心应用场景，特别适合在线二手车交易平台。</li>
<li><strong>其他需要高精度、实时交互3D重建的场景</strong>：例如，工业检测、文物数字化、建筑检查等，只要能构建类似的相机阵列并进行标定，该方法就有潜力。</li>
<li><strong>低视差、广角镜头场景</strong>：该方法的设计初衷就是为了解决这类场景的挑战。</li>
</ul>
</li>
</ul>
<h3 id="5_2">5. 实验分析</h3>
<ul>
<li><strong>验证方法</strong>：<ul>
<li><strong>基线比较</strong>：将提出的方法与以下基线进行比较：<ul>
<li><strong>Vanilla SfM</strong>：未使用本文提出的标定、同步、匹配策略和几何先验的标准SfM。</li>
<li><strong>Rig-Aware SfM (SIFT)</strong>：使用本文提出的高程感知SfM流程，但特征提取和匹配使用经典的SIFT和COLMAP匹配器。</li>
<li><strong>消融实验</strong>：逐步移除本文方法中的关键组件（如相机标定、视频同步、自定义匹配、几何先验），以验证每个组件的贡献。</li>
</ul>
</li>
<li><strong>评估指标</strong>：<ul>
<li><strong>SfM阶段</strong>：Registered Images, Sparse 3D Points, Mean Track Length, Reprojection Error (px)。</li>
<li><strong>3D-GS阶段</strong>：PSNR, SSIM, LPIPS。</li>
</ul>
</li>
</ul>
</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>SfM阶段</strong>：本文提出的“Our SfM (DISK+LG)”在所有指标上均显著优于基线方法。例如，在稀疏3D点数量上，比Vanilla SfM（126,496）高出近3倍（427,299）。重投影误差也最低（0.4909 px）。</li>
<li><strong>3D-GS阶段</strong>：本文方法在PSNR (30.66 dB), SSIM (0.92), LPIPS (0.19) 上均表现最佳，表明生成的模型在视觉质量和感知上更接近真实图像。</li>
<li><strong>消融实验</strong>：移除任何一个关键组件（如相机标定、自定义匹配、几何先验）都会导致SfM结果退化，进而影响最终3D-GS模型的质量（Fig. 7），证明了每个组件的重要性。</li>
</ul>
</li>
<li><strong>优势场景</strong>：<ul>
<li><strong>车辆底盘</strong>：实验结果（Fig. 6, Fig. 8, Fig. 9）显示，该方法能够生成细节丰富、几何准确的底盘模型，捕捉到锈蚀、油渍等关键诊断信息。</li>
<li><strong>实时渲染</strong>：模型渲染速度超过130 FPS，非常适合交互式检查。</li>
</ul>
</li>
<li><strong>局限性</strong>：<ul>
<li><strong>数据采集硬件依赖</strong>：需要一个定制的三摄像头阵列。</li>
<li><strong>计算开销</strong>：虽然3D-GS渲染速度快，但SfM阶段的特征提取和匹配（尤其是LightGlue）仍然需要一定的计算资源。训练时间约为8-10分钟/车（RTX A6000 GPU）。</li>
<li><strong>对光照和遮挡的鲁棒性</strong>：虽然DISK和LightGlue有所提升，但在极端光照变化或严重遮挡的情况下，特征匹配仍可能受到影响。</li>
<li><strong>标定过程</strong>：虽然是一次性标定，但标定的精度直接影响最终结果。</li>
</ul>
</li>
</ul>
<h3 id="6_2">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文中未明确提及是否开源。但其方法是基于现有成熟框架（COLMAP, LightGlue, 3D-GS）的组合与改进，理论上复现难度适中。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>相机标定</strong>：ChArUco板的设计和使用是关键。确保标定视频覆盖足够的视角和距离变化。OpenCV的相机模型和Levenberg-Marquardt优化器是核心。</li>
<li><strong>视频同步</strong>：相位相关和L1损失最小化是核心算法。需要仔细调整L1损失的搜索范围和迭代次数。</li>
<li><strong>SfM</strong>：<ul>
<li><strong>帧选择</strong>：拉普拉斯方差是关键的锐度度量。</li>
<li><strong>特征提取</strong>：DISK的实现和使用。</li>
<li><strong>匹配</strong>：LightGlue的配置和使用，以及与COLMAP的集成。</li>
<li><strong>Rig-Aware BA</strong>：将相机阵列的相对位姿作为先验信息添加到COLMAP的BA中，这是最核心的创新点，需要深入理解COLMAP的BA接口或自行实现。</li>
</ul>
</li>
<li><strong>3D-GS</strong>：使用开源的3D-GS实现，并用SfM生成的点云作为初始化。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他车辆部件</strong>：如果能构建类似的相机阵列，可以用于其他车辆部件（如发动机舱、车身侧面）的3D重建。</li>
<li><strong>非车辆场景</strong>：对于其他需要3D重建的场景，如果存在低视差、广角镜头等挑战，可以借鉴其高程感知SfM和学习型特征匹配的思路。但需要重新设计相机阵列和标定过程。</li>
<li><strong>任务扩展</strong>：生成的3D模型可以用于更高级的任务，如自动损伤检测、3D模型比对等。</li>
</ul>
</li>
</ul>
<h3 id="7_2">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：利用相机阵列几何先验和学习型特征，实现车辆底盘的实时高精度3D重建。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>精确标定</strong>：用ChArUco板校正广角镜头畸变。</li>
<li><strong>视频对齐</strong>：同步三路视频，确保时间一致。</li>
<li><strong>智能重建</strong>：用学习特征和相机阵列信息，生成高质量3D点云。</li>
<li><strong>快速渲染</strong>：用高斯喷溅技术，生成逼真交互式3D模型。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage.</li>
<li>Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes.</li>
<li>Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig.</li>
<li>Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14208v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14208v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14207v1'></a></p>
<h2 id="copy-trasform-paste-zero-shot-object-object-alignment-guided-by-vision-language-and-geometric-constraints"><a href="https://arxiv.org/abs/2601.14207v1">Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</a></h2>
<p><strong>Authors:</strong> Rotem Gatenyo, Ohad Fried</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3句话)</strong></p>
<p>本论文提出了一种新颖的零样本（zero-shot）方法，用于在文本提示的指导下，对两个三维网格模型进行空间对齐。该方法通过直接在测试时优化相对位姿（平移、旋转、缩放），并结合CLIP驱动的梯度、可微分渲染器以及几何约束（如表面附着和避免穿透），实现了无需额外训练的灵活对齐。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>该论文的核心创新在于其<strong>端到端的、无训练的、多模态融合的零样本对齐框架</strong>。具体来说，其关键方法论体现在以下几个方面：</p>
<ul>
<li><strong>直接测试时位姿优化 (Direct Test-Time Pose Optimization):</strong> 与依赖预训练模型或几何算法的传统方法不同，该论文直接在测试阶段通过梯度下降来优化两个物体之间的相对位姿（平移、旋转、缩放）。这使得方法具有极高的灵活性，能够适应各种未见过的物体和关系。</li>
<li><strong>CLIP驱动的语言引导 (CLIP-Driven Language Guidance):</strong> 利用预训练的CLIP模型强大的图文理解能力，将文本提示（描述物体间的空间关系）转化为可用于指导位姿优化的梯度信号。这是实现“零样本”和“语言条件”的关键。</li>
<li><strong>可微分渲染器 (Differentiable Renderer):</strong> 结合可微分渲染器，使得从三维网格到二维图像的渲染过程可以进行梯度反向传播。这意味着可以计算出位姿变化对渲染图像的影响，从而指导优化过程。</li>
<li><strong>几何约束的融合 (Integration of Geometric Constraints):</strong><ul>
<li><strong>软ICP变体 (Soft ICP Variant):</strong> 引入一种类似于迭代最近点（ICP）的损失项，鼓励两个物体表面之间产生“附着”或接触，增强了物理上的合理性。</li>
<li><strong>穿透损失 (Penetration Loss):</strong> 设计了惩罚两个物体相互穿透的损失项，确保了对齐结果的物理可行性。</li>
</ul>
</li>
<li><strong>分阶段优化策略 (Phased Schedule):</strong> 通过分阶段加强接触约束，逐步引导模型收敛到更精确和稳定的对齐状态。</li>
<li><strong>相机控制 (Camera Control):</strong> 优化过程中集中关注物体交互区域，提高了效率和准确性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>这项研究对三维内容创作、场景理解和虚拟现实等领域具有重要的潜在影响：</p>
<ul>
<li><strong>降低内容创作门槛:</strong> 使得用户能够通过简单的自然语言描述，快速、准确地将三维模型放置到指定位置和姿态，极大地简化了三维场景的搭建和编辑过程。</li>
<li><strong>推动零样本三维理解:</strong> 证明了在没有特定训练数据的情况下，通过多模态融合（视觉+语言+几何）可以实现复杂的三维空间关系理解和操作，为零样本三维任务的研究开辟了新方向。</li>
<li><strong>提升三维交互的自然性:</strong> 使得三维交互更加直观和人性化，用户无需掌握复杂的专业工具，即可通过语言指令完成精细的三维操作。</li>
<li><strong>促进可微分渲染和多模态学习的结合:</strong> 进一步展示了可微分渲染在连接高层语义（语言）和低层几何操作（位姿优化）方面的强大能力，鼓励更多研究探索此类结合。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>三维内容创作与编辑:</strong> 游戏开发、影视特效、建筑可视化、产品设计等领域，用于快速组装和调整三维场景。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 在VR/AR环境中，用户可以通过语音指令精确地放置和对齐虚拟物体。</li>
<li><strong>机器人学:</strong> 机器人可以通过语言指令理解并执行物体抓取、放置和组装任务。</li>
<li><strong>三维场景理解与重建:</strong> 辅助理解场景中物体之间的空间关系，为场景重建提供更精确的对齐信息。</li>
<li><strong>数字人与虚拟化身:</strong> 用于精确控制虚拟角色的手部或身体与环境物体的交互。</li>
<li><strong>医学影像处理:</strong> 在医学领域，可能用于对齐不同时间点或不同模态的医学扫描数据。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管该方法表现出色，但从摘要中可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对CLIP模型的依赖性:</strong> 方法的性能在很大程度上依赖于CLIP模型对物体及其关系的理解能力。如果CLIP对某些特定物体或复杂关系理解不足，可能会影响对齐效果。</li>
<li><strong>计算复杂度:</strong> 直接进行测试时优化，尤其是在高分辨率或复杂网格上，可能需要一定的计算资源和时间。虽然摘要提到了相机控制等优化效率的策略，但其绝对计算成本仍需进一步评估。</li>
<li><strong>对初始猜测的敏感性:</strong> 虽然是直接优化，但优化过程可能仍然会对初始的位姿猜测有一定的敏感性，尤其是在存在多个局部最优解的情况下。</li>
<li><strong>几何约束的鲁棒性:</strong> 软ICP和穿透损失的有效性可能依赖于网格的质量和密度。对于非常稀疏或有噪声的网格，这些几何约束的鲁棒性可能受到影响。</li>
<li><strong>“零样本”的定义:</strong> 尽管是零样本，但其“零样本”的定义可能仅限于物体类别和关系，而对于网格本身的几何特性（如拓扑结构、细节程度）可能仍有隐含的假设。</li>
<li><strong>对复杂场景的扩展性:</strong> 摘要主要讨论的是两个网格的对齐。将其扩展到包含大量物体和复杂交互的整个场景的对齐，可能面临更大的挑战。</li>
</ul>
<p>总而言之，这篇论文提出了一种非常令人兴奋的零样本三维对齐方法，它巧妙地融合了视觉语言模型和几何约束，通过直接优化实现了高效且灵活的对齐。其对内容创作和三维交互的潜在影响是巨大的，但同时也存在对预训练模型依赖、计算成本和特定几何场景鲁棒性等方面的潜在挑战。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model.</li>
<li>Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14207v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14207v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.14188v1'></a></p>
<h2 id="iir-vlm-in-context-instance-level-recognition-for-large-vision-language-models"><a href="https://arxiv.org/abs/2601.14188v1">IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Liang Shi, Wei Li, Kevin M Beussman, Lin Chen, Yun Fu</p>
<p><strong>Published:</strong> 2026-01-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="iir-vlm-in-context-instance-level-recognition-for-large-vision-language-models_1">论文方法分析：IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</h2>
<h3 id="1_3">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> IIR-VLM：面向大型视觉语言模型的上下文实例级识别</p>
<p><strong>摘要：</strong> 实例级识别（ILR）旨在区分个体实例，其中人脸识别是一个典型例子。尽管现代大型视觉语言模型（VLMs）拥有强大的视觉感知能力，但我们在ILR任务上发现它们表现不佳，常常远逊于领域特定的ILR模型。这一局限性阻碍了VLMs的许多实际应用，例如在需要识别熟悉人物和物体以实现有效视觉理解的场景。现有解决方案通常采用逐实例训练的方法，这不仅需要大量的数据收集和训练成本，而且在细粒度判别方面也存在困难。</p>
<p>本文提出IIR-VLM，一种增强了上下文实例级识别能力的VLM。我们集成了预训练的ILR专家模型作为辅助视觉编码器，以提供用于学习多样化实例的专业化特征，从而使VLMs能够以单次学习（one-shot）的方式在上下文中学习新实例。此外，IIR-VLM利用这些知识进行实例感知的视觉理解。我们在现有的实例个性化基准测试上验证了IIR-VLM的有效性。最后，我们在一个具有挑战性的新基准上展示了其卓越的ILR性能，该基准评估了ILR能力在不同难度和多样化类别下的表现，涵盖了人物、人脸、宠物和通用物体等实例。</p>
<h3 id="2_3">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力</strong>：作者希望赋予大型视觉语言模型（VLMs）在<strong>上下文（in-context）</strong>中进行<strong>实例级识别（Instance-Level Recognition, ILR）</strong>的能力。</li>
<li><strong>现有方法痛点</strong>：<ol>
<li><strong>现有VLMs在ILR任务上表现不佳</strong>：尽管VLMs在通用视觉理解方面表现出色，但在区分高度相似的个体实例时，其性能远不如专门为ILR任务训练的模型。</li>
<li><strong>传统ILR方法效率低下</strong>：现有的ILR解决方案大多依赖于<strong>逐实例训练（per-instance training）</strong>，这需要为每个新实例收集专门的数据集并进行重复训练，成本高昂且难以扩展。</li>
<li><strong>现有方法泛化性不足</strong>：一些尝试解决此问题的先进方法（如IDA-VLM）在多样化和具有视觉相似实例的任务上表现有限，主要局限于人脸识别等特定领域。</li>
</ol>
</li>
<li><strong>研究假设</strong>：<ol>
<li><strong>通用视觉编码器缺乏细粒度判别能力</strong>：VLMs的通用视觉编码器虽然擅长捕捉语义信息，但缺乏区分高度相似实例所需的细粒度特征。</li>
<li><strong>领域专家模型可以弥补这一不足</strong>：将专门为ILR任务训练的“专家模型”作为辅助编码器集成到VLM中，可以为VLM注入所需的细粒度判别能力。</li>
<li><strong>上下文学习是实现高效ILR的关键</strong>：通过精心设计的训练范式，VLM可以在不进行昂贵逐实例微调的情况下，在上下文中学习并识别新实例。</li>
</ol>
</li>
</ul>
<h3 id="3_3">3. 方法设计详解</h3>
<p><strong>流程总结：</strong></p>
<p>IIR-VLM 的核心思想是通过<strong>集成预训练的ILR专家模型作为辅助视觉编码器</strong>，并采用<strong>两阶段的上下文学习训练范式</strong>，来增强VLM的实例级识别和理解能力。</p>
<p><strong>Pipeline 概述 (图1):</strong></p>
<ol>
<li><strong>输入</strong>：一个查询图像（Query Image）和一个包含多个候选图像的图库（Gallery Images）。</li>
<li><strong>双编码器处理</strong>：<ul>
<li><strong>通用视觉编码器 (General-purpose Visual Encoder)</strong>：这是VLM自带的、预训练的视觉编码器（例如，Qwen2.5-VL-3B 的编码器）。它将输入图像（查询图像和图库图像）编码成多通道、多Token的特征表示 <script type="math/tex">f(c) = [f_1(c), f_2(c), \dots, f_n(c)]</script>。</li>
<li><strong>ILR专家编码器 (Expert Encoder)</strong>：这是一个<strong>预训练的、专门针对特定类别（如人脸、宠物、物体等）进行实例级识别任务训练过的模型</strong>。它将输入图像编码成一个<strong>单维度的、高度判别的身份特征</strong> <script type="math/tex">f_e(c)</script>。</li>
</ul>
</li>
<li><strong>特征融合 (Integration Mechanism)</strong>：<ul>
<li><strong>身份嵌入生成</strong>：专家编码器输出的特征 <script type="math/tex">f_e(c)</script> 通过一个<strong>可训练的MLP</strong>投影得到 <script type="math/tex">f'_e(c)</script>。</li>
<li><strong>注意力机制</strong>：使用 <script type="math/tex">f'_e(c)</script> 作为查询（Query），与通用编码器输出的每个Token特征 <script type="math/tex">f_i(c)</script> 计算<strong>注意力得分 <script type="math/tex">A_i</script></strong>。这个得分衡量了专家特征与通用特征Token的匹配程度。
    <script type="math/tex; mode=display">A_i = \text{Softmax}(\text{Sim}(f_i(c), f'_e(c))) \quad (3)</script>
    其中 <script type="math/tex">\text{Sim}</script> 是相似度函数（如点积）。</li>
<li><strong>特征注入</strong>：将注意力加权的专家特征 <script type="math/tex">A_i f'_e(c)</script>
<strong>加到</strong>原始的通用特征Token <script type="math/tex">f_i(c)</script> 上，生成增强后的特征 <script type="math/tex">F_i(c)</script>。
    <script type="math/tex; mode=display">F_i(c) = f_i(c) + A_i f'_e(c) \quad (4)</script>
</li>
<li><strong>结果</strong>：原始的多Token特征序列 <script type="math/tex">f(c)</script> 被替换为增强后的特征序列 <script type="math/tex">F(c) = [F_1(c), F_2(c), \dots, F_n(c)]</script>。这个过程将专家模型提供的细粒度实例信息注入到VLM的视觉表示中。</li>
</ul>
</li>
<li><strong>上下文学习训练 (Two-stage Fine-tuning)</strong>：<ul>
<li><strong>数据准备</strong>：将现有的ILR数据集（如人脸、宠物、物体Re-ID数据集）转化为<strong>指令微调（instruction tuning）</strong>格式。每个样本包含一个查询图像 <script type="math/tex">c_q</script> 和一个图库 <script type="math/tex">G = \{c_g^1, c_g^2, \dots, c_g^K\}</script>。目标是让VLM在给定图库的情况下，识别出与查询图像匹配的实例。为了增加难度，图库中的负样本（non-matching images）会选择与查询图像具有高视觉相似度的图像（通过相似度阈值 <script type="math/tex">\tau</script> 控制）。</li>
<li><strong>阶段1：实例匹配 (Stage 1: Instance Matching)</strong>：<ul>
<li><strong>目标</strong>：训练VLM具备在上下文中正确识别出图库中与查询图像匹配的实例的能力。</li>
<li><strong>任务形式</strong>：类似于一个<strong>多项选择题</strong>，VLM需要从图库中选出正确的图像。</li>
<li><strong>训练细节</strong>：冻结LLM和通用视觉编码器的参数，<strong>仅训练投影层（projector）</strong>。这一阶段旨在对齐专家特征和VLM的特征空间，实现知识迁移。</li>
</ul>
</li>
<li><strong>阶段2：实例感知理解 (Stage 2: Instance-aware Understanding)</strong>：<ul>
<li><strong>目标</strong>：在识别出实例的基础上，让VLM能够利用该实例信息进行更深层次的视觉理解，例如生成描述性字幕。</li>
<li><strong>任务形式</strong>：VLM需要为查询图像生成一个详细的<strong>字幕</strong>，并且该字幕需要<strong>明确提及并引用识别出的实例</strong>（例如，使用方括号标记，如 <code>[Person 3]</code>）。</li>
<li><strong>训练细节</strong>：同样冻结LLM和通用视觉编码器，<strong>仅训练投影层</strong>。这一阶段旨在让模型将实例识别能力转化为实例感知的生成能力。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出</strong>：<ul>
<li><strong>实例匹配结果</strong>：识别出图库中最匹配查询图像的实例。</li>
<li><strong>实例感知字幕</strong>：生成包含实例信息的描述性文本。</li>
</ul>
</li>
</ol>
<p><strong>模型结构：</strong></p>
<ul>
<li><strong>核心组件</strong>：<ul>
<li><strong>大型视觉语言模型 (VLM)</strong>：作为基础模型，包含一个通用视觉编码器和一个大型语言模型（LLM），通过投影层连接。</li>
<li><strong>ILR专家编码器 (Expert Encoder)</strong>：一个或多个预训练的、专门为特定类别ILR任务设计的模型。这些模型可以是现成的（off-the-shelf），也可以是针对特定任务微调的。</li>
<li><strong>注意力机制与MLP</strong>：用于将专家特征注入到VLM的通用视觉特征中。</li>
</ul>
</li>
<li><strong>协同工作</strong>：通用视觉编码器提供基础的视觉语义信息，而ILR专家编码器提供细粒度的实例判别线索。注意力机制负责将专家提供的关键信息有效地融合到通用特征中，形成更具判别力的表示。两阶段训练则引导VLM学习如何利用这些增强的特征进行实例匹配和实例感知的理解。</li>
</ul>
<p><strong>算法解释：</strong></p>
<ul>
<li><strong>公式 (3) <script type="math/tex">A_i = \text{Softmax}(\text{Sim}(f_i(c), f'_e(c)))</script></strong>：<ul>
<li><strong>意义</strong>：计算专家特征 <script type="math/tex">f'_e(c)</script> 与通用编码器输出的第 <script type="math/tex">i</script> 个Token特征 <script type="math/tex">f_i(c)</script> 之间的<strong>相关性或注意力权重</strong>。</li>
<li><strong>作用</strong>：通过相似度计算，确定专家提供的实例信息在多大程度上应该“激活”或“影响”通用特征的特定部分。Softmax确保这些权重在所有Token上加起来为1，表示一种分布。</li>
</ul>
</li>
<li><strong>公式 (4) <script type="math/tex">F_i(c) = f_i(c) + A_i f'_e(c)</script></strong>：<ul>
<li><strong>意义</strong>：将经过注意力加权的专家特征（即专家提供的、与当前通用特征Token最相关的部分）<strong>加到</strong>原始的通用特征Token上。</li>
<li><strong>作用</strong>：这是<strong>特征注入</strong>的核心。通过加法操作，将专家模型提取的细粒度实例信息“叠加”到VLM的通用视觉表示中。这使得VLM的视觉表示不仅包含通用语义，还融入了专家模型对实例的精确判断。</li>
</ul>
</li>
</ul>
<h3 id="4_3">4. 方法对比分析</h3>
<ul>
<li><strong>本质区别</strong>：<ul>
<li><strong>与现有VLMs</strong>：现有VLMs通常只使用一个通用视觉编码器，缺乏专门的实例判别能力。IIR-VLM引入了<strong>外部的、预训练的ILR专家模型</strong>作为辅助，直接增强了视觉表示的细粒度判别力。</li>
<li><strong>与逐实例训练方法</strong>：逐实例训练方法需要为每个新实例进行昂贵的微调。IIR-VLM采用<strong>上下文学习</strong>，通过一次性（one-shot）或少量样本（few-shot）在上下文中学习新实例，避免了重复训练。</li>
<li><strong>与IDA-VLM等方法</strong>：IDA-VLM等方法虽然也尝试解决ILR问题，但IIR-VLM通过<strong>更通用的专家模型集成机制</strong>和<strong>更全面的两阶段训练</strong>，在多样化类别和高难度任务上展现出更强的泛化能力和性能。</li>
</ul>
</li>
<li><strong>创新贡献</strong>：<ol>
<li><strong>ILR专家模型集成机制</strong>：提出了一种将预训练的ILR专家模型作为辅助视觉编码器，通过注意力机制将细粒度实例特征注入VLM通用视觉表示的有效方法。</li>
<li><strong>两阶段上下文学习训练范式</strong>：设计了一种轻量级的训练流程，先进行实例匹配，再进行实例感知理解，使得VLM能够高效地在上下文中学习和利用新实例信息。</li>
<li><strong>构建了具有挑战性的ILR基准</strong>：创建了一个跨越多个类别（人物、人脸、宠物、通用物体）和不同难度级别的ILR基准，为评估和推动相关研究提供了重要资源。</li>
</ol>
</li>
<li><strong>适用场景</strong>：<ul>
<li>需要VLM在<strong>未知或新实例</strong>上进行识别和理解的场景。</li>
<li><strong>个性化应用</strong>，如智能家居摄像头识别家庭成员、宠物，或个性化推荐系统识别用户偏好的物品。</li>
<li><strong>需要细粒度视觉判别</strong>的场景，尤其是在候选实例之间视觉相似度很高的情况下。</li>
<li><strong>对训练效率和可扩展性有要求</strong>的场景，避免了昂贵的逐实例微调。</li>
</ul>
</li>
</ul>
<h3 id="5_3">5. 实验分析</h3>
<ul>
<li><strong>验证方法</strong>：<ul>
<li><strong>基准测试</strong>：在作者构建的<strong>四类别（人物、人脸、宠物、通用物体）ILR基准</strong>上进行评估，该基准包含不同难度级别和高度相似的负样本。</li>
<li><strong>对比模型</strong>：与现有的通用VLMs（Gemini 2.5 Pro, Qwen2.5-VL-3B/7B）以及专门的ILR方法（IDA-VLM）进行比较。</li>
<li><strong>消融实验</strong>：分析了<strong>专家编码器的作用</strong>、<strong>两阶段训练的影响</strong>以及<strong>不同难度级别下模型性能的变化</strong>。</li>
</ul>
</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>整体性能优越</strong>：IIR-VLM在作者构建的基准上取得了显著的性能提升，尤其是在结合了专家编码器和两阶段训练后，平均准确率达到88.5%，远超其基础模型Qwen2.5-VL-3B（提升33.1%）。</li>
<li><strong>专家编码器至关重要</strong>：消融实验表明，引入ILR专家编码器能带来显著的性能提升，尤其是在高难度（高视觉相似度）的ILR任务中，其优势更为明显（最高可达+3.1%）。</li>
<li><strong>两阶段训练的有效性</strong>：两阶段训练比仅进行第二阶段训练（直接生成字幕）效果更好，第一阶段的实例匹配训练为后续的实例感知理解打下了坚实基础。</li>
<li><strong>在挑战性任务上表现突出</strong>：在具有高视觉相似度负样本的测试用例中，IIR-VLM能够准确识别匹配实例，并生成准确的描述性字幕。</li>
</ul>
</li>
<li><strong>优势场景</strong>：<ul>
<li><strong>高视觉相似度场景</strong>：如图3所示，随着视觉相似度阈值 <script type="math/tex">\tau</script> 的增加（难度增加），IIR-VLM（带专家）的性能优势越发明显。</li>
<li><strong>多样化类别场景</strong>：在人物、人脸、宠物和通用物体等多个类别上都表现出良好的泛化能力。</li>
</ul>
</li>
<li><strong>局限性</strong>：<ul>
<li><strong>仍有错误率</strong>：尽管性能优越，但仍有约10%的实例匹配错误率，尤其是在极端视觉相似度或视觉信息不足的情况下。</li>
<li><strong>继承VLM的局限性</strong>：在某些情况下，模型可能出现“幻觉”（hallucination），生成不存在的描述，或将匹配图像的描述误用于查询图像，这表明模型仍受限于基础VLM的固有弱点。</li>
<li><strong>数据依赖</strong>：虽然避免了逐实例训练，但ILR专家模型本身仍需要大量领域特定的数据进行预训练。</li>
</ul>
</li>
</ul>
<h3 id="6_3">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文中未明确提及是否开源，但通常学术论文会附带代码链接。如果未开源，复现需要根据论文描述的细节自行实现。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>基础VLM选择</strong>：论文使用了Qwen2.5-VL-3B作为基础模型。选择一个强大的多模态模型是基础。</li>
<li><strong>ILR专家模型选择/训练</strong>：<ul>
<li>对于成熟的类别（如人脸、人物），可以使用现成的SOTA模型（如Arcface, PLIP）。</li>
<li>对于数据不那么丰富的类别（如宠物、通用物体），需要<strong>从头训练或微调一个通用的视觉编码器</strong>，并使用<strong>实例分类损失和度量学习损失（如Triplet Loss）</strong>进行训练。</li>
</ul>
</li>
<li><strong>特征融合</strong>：MLP的维度、注意力机制的实现细节（如相似度函数）需要仔细调整。</li>
<li><strong>训练数据格式</strong>：将ILR数据集转化为指令微调格式，包含查询图像和图库，并设计合适的prompt。</li>
<li><strong>两阶段训练</strong>：<ul>
<li><strong>Stage 1</strong>：主要关注实例匹配的准确性，可以看作是多项选择题。</li>
<li><strong>Stage 2</strong>：在Stage 1基础上，训练模型生成包含实例信息的字幕。</li>
</ul>
</li>
<li><strong>超参数</strong>：图库大小 K、相似度阈值 <script type="math/tex">\tau</script> 是影响任务难度和模型表现的关键超参数，需要根据具体任务进行调整。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>迁移到其他ILR任务</strong>：该方法的核心思想——集成ILR专家和上下文学习——可以非常自然地迁移到其他ILR任务，只要能获得或训练相应的ILR专家模型。</li>
<li><strong>迁移到其他VLM任务</strong>：将ILR专家注入到其他需要细粒度视觉理解的VLM任务中，例如视觉问答（VQA）中涉及特定对象识别的问题，或者视觉定位任务。</li>
<li><strong>迁移到其他模态</strong>：理论上，如果存在其他模态的“专家模型”（如音频专家、文本专家），也可以尝试将类似的方法应用于多模态融合模型中。</li>
</ul>
</li>
</ul>
<h3 id="7_3">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：<strong>专家注入+上下文学习，赋能VLM实例识别</strong>。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>找专家</strong>：用专门的ILR模型提取实例特征。</li>
<li><strong>加信息</strong>：将专家特征融入VLM的视觉表示。</li>
<li><strong>学匹配</strong>：让VLM在图库中学会找对实例。</li>
<li><strong>学描述</strong>：让VLM基于找对的实例生成描述。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition.</li>
<li>We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner.</li>
<li>Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.14188v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.14188v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-21 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
