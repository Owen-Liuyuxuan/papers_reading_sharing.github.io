<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-15 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-12/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-16/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-15">Arxiv Computer Vision Papers - 2025-12-15</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#an-anatomy-of-vision-language-action-models-from-modules-to-milestones-and-challenges" class="nav-link">An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</a>
                </li>
                <li class="nav-item">
                    <a href="#moment-based-3d-gaussian-splatting-resolving-volumetric-occlusion-with-order-independent-transmittance" class="nav-link">Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance</a>
                </li>
                <li class="nav-item">
                    <a href="#v-rgbx-video-editing-with-accurate-controls-over-intrinsic-properties" class="nav-link">V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</a>
                </li>
                <li class="nav-item">
                    <a href="#particulate-feed-forward-3d-object-articulation" class="nav-link">Particulate: Feed-Forward 3D Object Articulation</a>
                </li>
                <li class="nav-item">
                    <a href="#anchordream-repurposing-video-diffusion-for-embodiment-aware-robot-data-synthesis" class="nav-link">AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#structure-from-tracking-distilling-structure-preserving-motion-for-video-generation" class="nav-link">Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#matanyone-2-scaling-video-matting-via-a-learned-quality-evaluator" class="nav-link">MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator</a>
                </li>
                <li class="nav-item">
                    <a href="#svg-t2i-scaling-up-text-to-image-latent-diffusion-model-without-variational-autoencoder" class="nav-link">SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</a>
                </li>
                <li class="nav-item">
                    <a href="#reframing-music-driven-2d-dance-pose-generation-as-multi-channel-image-generation" class="nav-link">Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#editmgt-unleashing-potentials-of-masked-generative-transformers-in-image-editing" class="nav-link">EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-15">Arxiv Computer Vision Papers - 2025-12-15</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月12日Arxiv计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月12日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期Arxiv论文集中体现了计算机视觉领域在<strong>多模态融合、三维场景表示与生成、视频内容理解与编辑</strong>等方面的显著进展。特别是，<strong>视觉-语言-动作（Vision-Language-Action, VLA）模型</strong>的探索、<strong>基于高斯泼溅（Gaussian Splatting）的三维重建</strong>的优化、以及<strong>视频生成与编辑</strong>的精细化控制成为突出亮点。同时，<strong>数据合成与机器人应用</strong>的结合也展现出新的研究方向。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>VLA模型全面分析：</strong> "An Anatomy of Vision-Language-Action Models" 提供了对当前VLA模型模块、里程碑及挑战的系统性梳理，为该快速发展领域提供了宝贵的理论框架和未来方向指引。</li>
<li><strong>三维高斯泼溅的突破：</strong> "Moment-Based 3D Gaussian Splatting" 解决了体积遮挡问题，通过引入基于矩的方法和独立于顺序的透射率，显著提升了三维场景重建的准确性和鲁棒性。</li>
<li><strong>视频编辑的精细化控制：</strong> "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties" 提出了一种能够精确控制视频内在属性（如光照、材质）的编辑方法，为高质量视频内容创作提供了强大工具。</li>
<li><strong>机器人数据合成的新范式：</strong> "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis" 巧妙地将视频扩散模型应用于机器人数据合成，为训练具身智能体提供了更高效、更具现实意义的数据来源。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>多模态理解与生成：</strong> VLA模型的深入研究预示着更强大的跨模态理解和生成能力，能够处理更复杂的任务，如机器人控制和交互。</li>
<li><strong>高效三维场景表示与渲染：</strong> 基于高斯泼溅的技术持续演进，朝着更高效、更逼真、更能处理复杂场景（如遮挡）的方向发展。</li>
<li><strong>视频内容生成与编辑的精细化：</strong> 从结构保持的运动提取到基于扩散模型的视频编辑，研究正朝着更可控、更具创造性的方向迈进。</li>
<li><strong>扩散模型在特定领域的应用拓展：</strong> 扩散模型不仅在图像生成领域表现出色，还被成功应用于机器人数据合成等新兴领域。</li>
<li><strong>Transformer在图像编辑中的潜力挖掘：</strong> "EditMGT" 展示了Masked Generative Transformers在图像编辑任务中的强大能力，预示着Transformer在视觉内容编辑领域的进一步应用。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其对领域发展的指导意义和技术上的创新性，以下论文建议优先阅读全文：</p>
<ol>
<li><strong>"An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges"</strong> - 为理解和推进VLA模型研究提供了全面的视角。</li>
<li><strong>"Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance"</strong> - 在三维重建领域具有重要的技术突破，解决了关键的遮挡问题。</li>
<li><strong>"V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties"</strong> - 对于视频内容创作和编辑领域的研究者具有直接的应用价值和启发。</li>
<li><strong>"AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis"</strong> - 展示了前沿生成模型在机器人领域的创新应用，是跨学科研究的重要参考。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速了解近期Arxiv计算机视觉领域的重要进展，并为您的进一步研究提供方向。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.11362v1">An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</a></li>
<li><a href="#2512.11800v1">Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance</a></li>
<li><a href="#2512.11799v1">V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</a></li>
<li><a href="#2512.11798v1">Particulate: Feed-Forward 3D Object Articulation</a></li>
<li><a href="#2512.11797v1">AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis</a></li>
<li><a href="#2512.11792v1">Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</a></li>
<li><a href="#2512.11782v1">MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator</a></li>
<li><a href="#2512.11749v1">SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</a></li>
<li><a href="#2512.11720v1">Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation</a></li>
<li><a href="#2512.11715v1">EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.11362v1'></a></p>
<h2 id="an-anatomy-of-vision-language-action-models-from-modules-to-milestones-and-challenges"><a href="https://arxiv.org/abs/2512.11362v1">An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</a></h2>
<p><strong>Authors:</strong> Chao Xu, Suyu Zhang, Yang Liu, Baigui Sun, Weihong Chen, Bo Xu, Qi Liu, Juncheng Wang, Shujun Wang, Shan Luo, Jan Peters, Athanasios V. Vasilakos, Stefanos Zafeiriou, Jiankang Deng</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \href{https://suyuz1.github.io/Survery/}{project page}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Anatomy of Vision-Language-Action Models</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>这篇论文是一份关于视觉-语言-动作（VLA）模型的全面综述，旨在为研究人员提供一个清晰、结构化的学习路径。它通过分解VLA模型的组成模块、梳理关键发展里程碑，并深入探讨当前研究面临的五大核心挑战（表示、执行、泛化、安全、数据集与评估），来系统性地梳理该领域的研究现状。其核心贡献在于提供了一个结构化的框架，既能帮助新手快速入门，也能为资深研究者提供战略性指导，从而加速该领域的进步。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>这篇论文的关键创新在于其<strong>结构化的组织方式和对VLA模型发展路径的模拟</strong>。它没有简单地罗列模型，而是将VLA模型的学习和发展过程比作一个“通才智能体”的成长路线图：
*   <strong>模块化分解 (Modules):</strong> 从构成VLA模型的基本组件入手，为理解复杂模型打下基础。
*   <strong>历史演进 (Milestones):</strong> 追溯关键的里程碑式进展，帮助理解领域是如何发展到今天的。
*   <strong>挑战驱动 (Challenges):</strong> 聚焦于当前最前沿的五大核心挑战，这五大挑战本身就是对领域内关键技术瓶颈的提炼和归纳。这种挑战驱动的分析方式，能够直接指向研究的薄弱环节和未来的研究方向。
*   <strong>类比“通才智能体”的成长：</strong> 将VLA模型的演进与一个通用智能体的发展过程相对应，即从基础的感知-动作循环，到跨越不同载体和环境的扩展能力，再到最终的可信赖部署，这种类比提供了一个更具象化和战略性的视角来理解VLA模型的整体发展蓝图。</p>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>加速新研究者的入门：</strong> 其结构化的方法和清晰的路线图将极大地降低新进入VLA领域的研究者的学习门槛，帮助他们快速掌握核心概念和研究方向。</li>
<li><strong>为资深研究者提供战略指导：</strong> 通过对挑战的深入分析和对未来机遇的展望，论文能够帮助资深研究者识别新的研究热点和潜在的突破点，从而更有效地规划研究方向。</li>
<li><strong>促进领域内的标准化和共识：</strong> 论文对模块、里程碑和挑战的系统性梳理，有助于在领域内形成更统一的语言和评价标准，促进研究成果的可比性和可复现性。</li>
<li><strong>推动VLA模型向更通用、更可靠的方向发展：</strong> 聚焦于泛化、安全等挑战，将直接引导研究者关注如何构建更强大、更值得信赖的VLA系统。</li>
<li><strong>提供一个动态更新的知识库：</strong> 论文的“活版本”在线维护，意味着它将成为一个持续更新的、权威的VLA领域知识库，对于保持研究的同步性至关重要。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>机器人学 (Robotics):</strong> 这是VLA模型最直接的应用领域，包括但不限于：<ul>
<li><strong>人机交互 (Human-Robot Interaction):</strong> 让机器人能够理解自然语言指令，并执行相应的物理动作。</li>
<li><strong>家庭服务机器人 (Home Service Robots):</strong> 如清洁、烹饪、辅助老年人等。</li>
<li><strong>工业自动化 (Industrial Automation):</strong> 机器人能够根据指令进行更复杂的装配、搬运等任务。</li>
<li><strong>自动驾驶 (Autonomous Driving):</strong> 虽然侧重于驾驶，但VLA模型可以帮助车辆理解更复杂的交通指令和环境信息。</li>
</ul>
</li>
<li><strong>虚拟现实/增强现实 (VR/AR):</strong> VLA模型可以用于创建更具交互性的虚拟环境，让用户能够通过语言与虚拟对象进行互动。</li>
<li><strong>智能助手 (Intelligent Assistants):</strong> 能够理解更复杂的指令，并执行与物理世界相关的任务（例如，通过连接智能家居设备）。</li>
<li><strong>教育和培训 (Education and Training):</strong> 用于创建交互式学习环境，例如模拟实验或技能培训。</li>
<li><strong>游戏开发 (Game Development):</strong> 创造更智能、更具响应性的游戏角色和环境。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>综述的固有局限性：</strong> 作为一篇综述，它本身不提出新的模型或算法，而是对现有研究进行梳理和总结。其价值在于其组织、分析和指导能力，而非原创性贡献。</li>
<li><strong>“活版本”的维护挑战：</strong> 虽然“活版本”是一个优点，但其质量和及时性高度依赖于维护团队的持续投入。如果维护不善，可能会很快过时。</li>
<li><strong>对“通才智能体”的类比可能存在简化：</strong> 将VLA模型发展比作“通才智能体”的成长是一个有用的框架，但现实中的智能体发展可能比这个类比更复杂和非线性。</li>
<li><strong>对五大挑战的侧重：</strong> 论文聚焦于五大挑战，这可能意味着其他一些次要但仍重要的挑战可能不会得到同等程度的关注。</li>
<li><strong>技术深度限制：</strong> 摘要提供了高层次的概述，具体的模型细节、算法实现和实验结果需要在论文正文中才能找到。摘要本身无法评估论文的技术深度和严谨性。</li>
<li><strong>发表日期（2025-12-12）：</strong> 虽然摘要是关于2025年的论文，但实际的论文内容可能是在此日期之前完成的。摘要本身可能无法完全反映该领域在2025年底的最新进展。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文的摘要表明，它将成为VLA领域一个非常重要的参考资料。其结构化的方法、对核心挑战的深入分析以及对未来研究方向的指引，使其在计算机视觉和机器人学领域具有极高的潜在价值。它有望成为该领域研究者必读的文献，并对推动VLA技术的发展起到关键作用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with.</li>
<li>Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation.</li>
<li>We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11362v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11362v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11800v1'></a></p>
<h2 id="moment-based-3d-gaussian-splatting-resolving-volumetric-occlusion-with-order-independent-transmittance"><a href="https://arxiv.org/abs/2512.11800v1">Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance</a></h2>
<p><strong>Authors:</strong> Jan U. Müller, Robin Tim Landsgesell, Leif Van Holland, Patrick Stotko, Reinhard Klein</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>这篇论文提出了一种名为“基于矩的3D高斯泼溅”（Moment-Based 3D Gaussian Splatting）的新方法，旨在解决现有3D高斯泼溅（3DGS）在渲染半透明物体时存在的体积遮挡问题。通过引入一种新颖的、基于统计矩的连续透射率计算方法，该方法能够在不依赖光线追踪或像素排序的情况下，实现高保真度的体积渲染，从而显著提升重建和渲染质量。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>该论文的核心创新在于其<strong>基于统计矩的连续透射率计算方法</strong>。具体来说：</p>
<ul>
<li><strong>紧凑且连续的密度表示：</strong> 作者提出用一组统计矩（如均值、方差等）来紧凑地表示相机光线上所有贡献的3D高斯分布的密度分布。这种表示方式是连续的，避免了离散采样带来的误差。</li>
<li><strong>解析推导和计算：</strong> 论文中解析地推导并计算了每个像素的矩，这些矩是从所有参与渲染的3D高斯中聚合而来。</li>
<li><strong>连续透射率函数重建：</strong> 利用计算出的矩，可以重建出一条相机光线上的连续透射率函数。</li>
<li><strong>独立采样：</strong> 这种连续透射率函数允许在每个高斯内部进行独立采样，从而精确地模拟光线在半透明介质中的衰减，而无需考虑高斯之间的渲染顺序。</li>
</ul>
<p>这种方法巧妙地绕过了传统alpha混合的顺序依赖性，以及光线追踪的计算成本，实现了高效且准确的体积渲染。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>这篇论文的潜在影响是深远的，主要体现在：</p>
<ul>
<li><strong>提升3DGS的渲染质量和适用范围：</strong> 解决了3DGS 在处理复杂半透明场景（如烟雾、水、玻璃、毛发等）时的固有局限性，使其能够生成更逼真、更具物理准确性的渲染结果。</li>
<li><strong>加速体积渲染的研究：</strong> 通过将体积渲染的准确性与光栅化的高效性相结合，为未来体积渲染的研究开辟了新的方向，可能催生更高效、更逼真的渲染技术。</li>
<li><strong>推动新一代新视角合成技术：</strong> 能够生成更高质量的新视角图像，尤其是在包含复杂透明元素的场景中，这将对虚拟现实、增强现实、电影制作等领域产生积极影响。</li>
<li><strong>降低对光线追踪的依赖：</strong> 在保持高渲染质量的同时，避免了光线追踪的计算开销，使得实时或近实时的高质量体积渲染成为可能。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>新视角合成 (Novel View Synthesis)：</strong> 这是最直接的应用，能够生成更逼真、更具沉浸感的新视角图像。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 在构建逼真的虚拟环境和叠加虚拟物体时，能够更准确地模拟光线与场景的交互，提升用户体验。</li>
<li><strong>3D内容创作和可视化：</strong> 艺术家和设计师可以更轻松地创建和渲染包含复杂透明材质的3D场景，如电影特效、游戏资产等。</li>
<li><strong>医学影像可视化：</strong> 对于需要渲染人体组织、器官等半透明结构的医学影像，该技术可以提供更清晰、更准确的3D可视化。</li>
<li><strong>机器人和自动驾驶：</strong> 在模拟真实世界环境时，能够更准确地模拟光线在雨、雾等天气条件下的传播，提高模拟的真实性。</li>
<li><strong>科学可视化：</strong> 用于可视化流体动力学、粒子模拟等涉及半透明介质的科学数据。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了该方法的强大之处，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算复杂度：</strong> 虽然避免了光线追踪，但计算和聚合每个像素的统计矩可能仍然具有一定的计算开销，尤其是在处理非常密集的3D高斯场景时。论文中提到“解析推导和计算”，这暗示了推导过程可能涉及复杂的数学运算。</li>
<li><strong>对3D高斯表示的依赖：</strong> 该方法是建立在3D高斯表示的基础上的。如果原始3D高斯表示本身存在不足（例如，无法精确捕捉某些复杂的几何形状或材质），那么该方法的表现也会受到限制。</li>
<li><strong>内存开销：</strong> 存储和处理统计矩可能需要额外的内存开销，尤其是在高分辨率图像和大规模场景下。</li>
<li><strong>参数调优：</strong> 尽管方法是连续的，但可能仍然需要对某些参数进行调优，以达到最佳的渲染效果，这可能需要一定的专业知识。</li>
<li><strong>对“粗糙近似”的改进程度：</strong> 摘要提到“coarse approximations of the density integral”，虽然该方法解决了这个问题，但其“物理准确性”的程度可能仍然受到一些因素的影响，例如高斯核函数的形状选择等。</li>
</ul>
<p>总而言之，这篇论文提出的“基于矩的3D高斯泼溅”方法，通过巧妙地利用统计矩来解决体积遮挡问题，有望在3D高斯泼溅领域带来一次重要的技术飞跃，尤其是在处理半透明物体方面，具有巨大的潜力和广泛的应用前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields.</li>
<li>In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting.</li>
<li>As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11800v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11800v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11799v1'></a></p>
<h2 id="v-rgbx-video-editing-with-accurate-controls-over-intrinsic-properties"><a href="https://arxiv.org/abs/2512.11799v1">V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</a></h2>
<p><strong>Authors:</strong> Ye Fang, Tong Wu, Valentin Deschaintre, Duygu Ceylan, Iliyan Georgiev, Chun-Hao Paul Huang, Yiwei Hu, Xuelin Chen, Tuanfeng Yang Wang</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了 V-RGBX，一个首个端到端的视频编辑框架，能够精确控制视频的内在属性。V-RGBX 实现了视频的逆渲染到内在通道（如反照率、法线、材质、辐照度），并基于这些内在表示进行照片级逼真视频合成，同时支持基于关键帧的内在通道编辑。其核心在于一种交错的条件机制，允许用户通过关键帧直观且物理上合理地编辑视频的任何内在属性，并能将编辑效果物理一致地传播到整个视频序列。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>V-RGBX 的核心创新在于其<strong>“内在感知视频编辑”</strong>的端到端框架，以及实现这一目标的<strong>“交错条件机制”（interleaved conditioning mechanism）</strong>。</p>
<ul>
<li><strong>内在感知视频编辑：</strong> 传统视频编辑往往直接操作像素或 RGB 图像，难以实现物理上一致的修改。V-RGBX 的突破在于它首先将视频分解为物理意义明确的内在属性（albedo, normal, material, irradiance），然后在此基础上进行编辑和合成。这使得编辑操作更具物理基础和可控性。</li>
<li><strong>交错条件机制：</strong> 这是实现上述内在感知编辑的关键技术。它能够将用户在关键帧上对特定内在属性的编辑指令，有效地“注入”到视频合成过程中。这种机制使得编辑能够跨越时间维度，并确保编辑结果在物理上是连贯和合理的。它允许用户灵活地选择和操纵任何一种内在属性，例如改变物体的颜色（反照率）、改变表面的光照响应（材质）、甚至改变场景的整体光照（辐照度）。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>V-RGBX 的出现可能对视频生成和编辑领域产生深远影响：</p>
<ul>
<li><strong>提升视频编辑的可控性和真实感：</strong> 允许用户以物理为基础的方式修改视频，而非仅仅进行像素级的“魔法”。这将极大地提升视频编辑的精度和真实感，尤其是在需要改变物体外观、材质或光照等场景下。</li>
<li><strong>推动视频内容创作的民主化：</strong> 使得非专业人士也能通过更直观、更物理化的方式对视频进行精细化编辑，降低了高质量视频制作的门槛。</li>
<li><strong>促进视频理解和生成模型的融合：</strong> V-RGBX 证明了将视频的“理解”（逆渲染到内在属性）与“生成”（基于内在属性合成视频）以及“编辑”（修改内在属性）紧密结合的可行性，为未来更强大的视频模型提供了新的范式。</li>
<li><strong>为新一代视频编辑工具奠定基础：</strong> V-RGBX 的框架可以被视为未来视频编辑软件的核心技术，能够实现更高级、更智能的编辑功能。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>电影和电视制作：</strong> 用于特效制作、场景重构、角色外观修改、光照调整等，可以显著提高制作效率和艺术表现力。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 在构建沉浸式体验时，能够动态地修改虚拟场景或叠加的虚拟物体，使其与真实环境的光照和材质更加匹配。</li>
<li><strong>游戏开发：</strong> 用于动态改变游戏场景的视觉风格、材质效果，或实现更逼真的光照模拟。</li>
<li><strong>产品展示和广告：</strong> 允许在不重新拍摄的情况下，灵活地修改产品颜色、材质或展示环境的光照，以满足不同的营销需求。</li>
<li><strong>数字人和虚拟形象：</strong> 精细控制虚拟角色的外观和在不同光照下的表现。</li>
<li><strong>科学可视化：</strong> 在模拟和可视化复杂物理过程时，能够更灵活地调整和展示不同物理属性的影响。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了 V-RGBX 的强大能力，但仍有一些潜在的局限性可以推断：</p>
<ul>
<li><strong>计算复杂度：</strong> 视频逆渲染、合成和编辑是一个计算密集型的过程。端到端的框架可能需要大量的计算资源和时间，尤其是在处理高分辨率或长视频时。</li>
<li><strong>对训练数据的依赖：</strong> 像大多数深度学习模型一样，V-RGBX 的性能很可能高度依赖于训练数据的质量和数量。如果训练数据在某些方面存在偏差，模型在处理未见过的数据时可能会遇到困难。</li>
<li><strong>内在属性的准确性：</strong> 逆渲染得到内在属性的准确性直接影响后续的合成和编辑效果。如果逆渲染过程本身存在误差，这些误差可能会被放大到最终的视频输出中。</li>
<li><strong>编辑的“物理合理性”的边界：</strong> 尽管论文声称支持“物理上合理”的编辑，但“物理合理性”的定义和边界可能是一个挑战。模型在处理极端或非物理的编辑请求时，其表现如何仍需进一步验证。</li>
<li><strong>对特定场景的泛化能力：</strong> 摘要提到“多样化的应用”，但模型在处理极其复杂或与训练数据分布差异很大的场景时，其泛化能力可能受到限制。</li>
<li><strong>用户界面的复杂性：</strong> 虽然支持“直观”的编辑，但要充分利用所有内在属性的编辑能力，可能仍然需要用户具备一定的专业知识来理解和操作这些属性。</li>
</ul>
<p>总而言之，V-RGBX 是一项令人兴奋的研究，它通过引入内在属性作为视频编辑的核心，为视频内容创作和处理带来了新的可能性。其端到端的框架和创新的条件机制是该研究的亮点，有望在多个领域产生重要影响。然而，计算效率、数据依赖性和内在属性的准确性等问题，是未来研究和实际应用中需要关注的方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing.</li>
<li>Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner.</li>
<li>We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11799v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11799v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11798v1'></a></p>
<h2 id="particulate-feed-forward-3d-object-articulation"><a href="https://arxiv.org/abs/2512.11798v1">Particulate: Feed-Forward 3D Object Articulation</a></h2>
<p><strong>Authors:</strong> Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby, Shangzhe Wu, Andrea Vedaldi</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Particulate: Feed-Forward 3D Object Articulation”的全面摘要，重点关注其核心贡献、方法、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> PARTICULATE: Feed-Forward 3D Object Articulation</p>
<p><strong>作者：</strong> Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby, Shangzhe Wu, Andrea Vedaldi</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心问题：</strong>
该论文旨在解决从单个静态 3D 网格中直接推断日常物品的完整<strong>三维（3D）可动结构</strong>的问题。这包括识别构成物品的各个<strong>3D 部件</strong>、它们之间的<strong>运动学结构（如层级关系）</strong>以及它们的<strong>运动约束（如运动类型、轴向和范围）</strong>。现有方法要么速度慢（需要逐个优化），要么依赖于先验知识或部件检索，限制了其准确性和泛化能力。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>PARTICULATE 模型：</strong> 提出了一种名为 PARTICULATE 的<strong>前馈（feed-forward）</strong>方法，能够一次性推断出所有可动结构属性。
*   <strong>Part Articulation Transformer (PAT)：</strong> 模型的核心是一个<strong>Transformer 网络</strong>，它处理输入网格的点云表示。该网络具有<strong>灵活且可扩展的架构</strong>，能够原生支持<strong>多关节（multi-joint）</strong>的推断。
*   <strong>端到端训练：</strong> 模型在<strong>多样化的可动 3D 资产数据集</strong>上进行端到端训练，使其能够学习到广泛的关节结构和运动模式。
*   <strong>快速推理：</strong> 与需要逐个对象优化的传统方法相比，PARTICULATE 的前馈推理速度极快，<strong>能在几秒钟内</strong>生成完整的可动 3D 模型。
*   <strong>泛化能力：</strong> 该模型能够<strong>准确推断 AI 生成的 3D 资产</strong>的可动结构，这使得结合图像到 3D 生成器，可以从单个图像（或文本提示）中实现完整的可动 3D 对象提取。
*   <strong>新基准和评估协议：</strong> 论文引入了一个<strong>新的、具有挑战性的 3D 可动性估计基准数据集</strong>，该数据集包含高质量的 3D 资产和精确的可动性标注。同时，还<strong>重新设计了评估协议</strong>，使其更能反映人类对可动性评估的偏好。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能优越：</strong> 在多个数据集和评估指标上，PARTICULATE <strong>显著优于</strong>现有的最先进方法，尤其是在部件分割和运动约束预测方面。
*   <strong>高效性：</strong> 实现了<strong>极快的推理速度</strong>，将可动 3D 模型生成时间从数小时缩短到数秒，极大地提高了效率。
*   <strong>泛化到 AI 生成内容：</strong> 成功地处理了 AI 生成的 3D 模型，这对于将生成式模型与可动性理解相结合具有重要意义。
*   <strong>可用于物理模拟：</strong> 生成的可动 3D 模型可以<strong>无缝导入物理引擎</strong>进行模拟，为机器人学、游戏和虚拟现实等领域提供了强大的工具。
*   <strong>新基准的价值：</strong> 新的基准数据集和评估协议为该领域的研究提供了更可靠的评估标准。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>对训练数据分布的依赖：</strong> 虽然模型能泛化到未见过实例，但对于<strong>与训练数据中运动学结构差异很大的对象</strong>，其恢复能力会受到限制。这主要是因为可用训练数据的规模与通用数据集（如 ImageNet）相比仍然较小。
*   <strong>AI 生成资产的潜在问题：</strong> 当与 3D 生成器结合使用时，生成的资产有时会出现<strong>部件穿透</strong>，这可能是由于生成器本身的伪影或不精确的运动预测。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提高对新颖运动学结构的鲁棒性：</strong> 通过增加训练数据的多样性或开发更具泛化能力的模型架构来解决。
*   <strong>增强物理真实性：</strong> 通过<strong>后处理技术</strong>（如物理模拟反馈）来改善生成的可动资产的物理合理性，减少部件穿透。
*   <strong>大规模模拟到现实（Sim-to-Real）训练：</strong> 利用生成的可动资产进行大规模的模拟到现实训练，以推动机器人学等领域的发展。
*   <strong>处理不规则或内部部件：</strong> 进一步提升模型处理复杂几何形状、隐藏部件或内部运动的能力。</p>
<p>总而言之，PARTICULATE 是一项重要的工作，它通过一个高效、端到端的前馈 Transformer 模型，显著推进了从单个静态 3D 网格中自动推断复杂可动结构的能力，为生成和理解可动 3D 对象开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints.</li>
<li>We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences.</li>
<li>Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11798v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11798v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11797v1'></a></p>
<h2 id="anchordream-repurposing-video-diffusion-for-embodiment-aware-robot-data-synthesis"><a href="https://arxiv.org/abs/2512.11797v1">AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis</a></h2>
<p><strong>Authors:</strong> Junjie Ye, Rong Xue, Basile Van Hoorick, Pavel Tokmakov, Muhammad Zubair Irshad, Yue Wang, Vitor Guizilini</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis”的全面中文摘要：</p>
<p><strong>论文题目：</strong> AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis (AnchorDream：为具身感知机器人数据合成重新利用视频扩散模型)</p>
<p><strong>作者：</strong> Junjie Ye, Rong Xue, Basile Van Hoorick, Pavel Tokmakov, Muhammad Zubair Irshad, Yue Wang, Vitor Guizilini</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决机器人模仿学习（Imitation Learning）中<strong>大规模、多样化机器人演示数据收集的瓶颈问题</strong>。现实世界数据的采集成本高昂，而模拟器则存在多样性不足、保真度低以及显著的“现实到模拟”（sim-to-real）差距。现有的生成模型虽然能改变视觉外观，但往往无法生成新的行为，或者存在“具身不一致”（embodiment inconsistencies）的问题，导致生成不切实际的运动。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>AnchorDream 提出了一种<strong>具身感知的世界模型</strong>，其核心创新在于<strong>重新利用预训练的视频扩散模型（video diffusion models）来合成机器人数据</strong>。其关键方法论贡献包括：</p>
<ul>
<li><strong>具身锚定（Embodiment Grounding）：</strong> AnchorDream 将扩散过程<strong>条件化在机器人运动的渲染视频上</strong>。通过这种方式，模型将机器人的运动（具身）作为生成过程的“锚点”，从而防止生成不切实际的机器人姿态或运动，并确保生成的物体和环境与机器人的运动学（kinematics）保持一致。</li>
<li><strong>解耦轨迹与环境生成（Decoupled Trajectory and Environment Synthesis）：</strong> 该方法首先通过程序化方法（如扰动关键状态和重组运动片段）<strong>扩展和渲染机器人运动轨迹</strong>，生成仅包含机器人本身的运动视频。然后，将这些运动视频作为条件输入给视频扩散模型，以<strong>合成具有逼真视觉效果和与运动学一致的环境和物体</strong>。这种解耦避免了对显式环境建模或模拟器执行的需求。</li>
<li><strong>利用预训练视频扩散模型：</strong> 论文利用了在海量互联网数据上训练的视频扩散模型所蕴含的丰富世界先验知识（如物体外观、场景布局和时间一致性），将其应用于机器人数据合成。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>AnchorDream 在模拟器和真实机器人实验中都取得了显著的成果：</p>
<ul>
<li><strong>数据规模扩展：</strong> 该方法能够将<strong>少量人类遥操作演示数据扩展到大规模、多样化、高质量的数据集</strong>。</li>
<li><strong>性能提升：</strong><ul>
<li>在<strong>模拟器基准测试</strong>中，生成的合成数据带来了<strong>36.4%的相对性能提升</strong>。</li>
<li>在<strong>真实世界研究</strong>中，性能<strong>几乎翻倍</strong>。</li>
</ul>
</li>
<li><strong>意义：</strong> 这些结果表明，将生成式世界模型<strong>锚定在机器人运动上</strong>，为扩展模仿学习数据提供了一条<strong>切实可行且高效的路径</strong>，无需昂贵的数据收集或复杂的环境建模。它有效地缩小了合成数据与真实世界数据的差距。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>全局轨迹条件化的重要性：</strong> 论文指出，仅依赖局部上下文进行生成有时会导致场景布局与机器人未来运动不一致（如图3所示）。虽然全局轨迹条件化有所改善，但仍可能存在挑战。</li>
<li><strong>推理窗口长度：</strong> 较短的推理窗口会影响生成结果的性能，表明长序列的生成需要更长的推理窗口来维持时间一致性。</li>
<li><strong>对预训练模型的依赖：</strong> AnchorDream 依赖于预训练的视频扩散模型，其性能上限可能受到预训练模型自身能力的影响。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的机器人应用：</strong> 论文提到，虽然研究集中在桌面操作任务上，但将 AnchorDream 扩展到<strong>移动机器人或长时序操作（long-horizon manipulation）</strong>等更广泛的领域是一个令人兴奋的未来研究方向。</li>
<li><strong>更精细的具身控制：</strong> 进一步探索如何更精细地控制机器人的具身特性，以生成更复杂、更具挑战性的行为。</li>
<li><strong>与更先进生成模型的结合：</strong> 探索将 AnchorDream 的具身锚定思想与未来更强大的生成模型相结合的可能性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>AnchorDream 是一项重要的研究工作，它通过一种创新的“具身锚定”机制，成功地将强大的视频扩散模型应用于机器人数据合成。通过将机器人运动作为生成过程的约束，该方法能够生成逼真且在运动学上一致的演示数据，有效解决了模仿学习中的数据瓶颈问题。其在模拟器和真实机器人上的出色表现，为实现更强大、更通用的机器人策略提供了新的途径，尤其是在数据获取受限的情况下。该研究为具身感知生成模型在机器人领域的应用开辟了新的前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions.</li>
<li>To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis.</li>
<li>Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11797v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11797v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11792v1'></a></p>
<h2 id="structure-from-tracking-distilling-structure-preserving-motion-for-video-generation"><a href="https://arxiv.org/abs/2512.11792v1">Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</a></h2>
<p><strong>Authors:</strong> Yang Fei, George Stoica, Jingyuan Liu, Qifeng Chen, Ranjay Krishna, Xiaojuan Wang, Benlin Liu</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文分析：Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话总结）</strong></p>
<p>该论文提出了一种新颖的“Structure From Tracking”方法，通过从一个强大的自回归视频跟踪模型（SAM2）中提炼出结构保持的运动先验，并将其注入到一个双向视频扩散模型（CogVideoX）中，从而显著提升了视频生成在保持物体结构完整性方面的能力。这项工作解决了现有扩散模型在生成具有复杂运动（如关节和形变物体）的视频时容易出现物理不合理过渡的问题，并在多个评估指标上取得了显著的性能提升。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>该论文的核心创新在于其“蒸馏”（distilling）结构保持运动先验的方法，具体体现在两个关键技术点上：</p>
<ul>
<li><strong>双向特征融合模块 (bidirectional feature fusion module):</strong> 这个模块能够从一个循环模型（如SAM2）中提取全局的、结构保持的运动先验。这意味着它不仅仅关注局部的像素运动，而是能够理解和捕捉到物体整体的运动模式和结构约束。</li>
<li><strong>局部语法流损失 (Local Gram Flow loss):</strong> 这个损失函数旨在对齐局部特征的运动方式。它鼓励模型在生成视频时，局部区域的特征能够以一种符合语法（即结构保持）的方式协同运动，从而避免了局部运动的割裂和不连贯，进一步增强了结构的稳定性。</li>
</ul>
<p>通过将SAM2的强大跟踪能力和结构理解能力“蒸馏”到CogVideoX的生成能力中，论文有效地弥合了现有视频生成模型在结构保真度方面的不足。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>这项研究对视频生成领域具有重要的潜在影响，主要体现在：</p>
<ul>
<li><strong>提升视频生成质量和真实感:</strong> 尤其是在生成涉及人类、动物等复杂形变物体的视频时，能够生成更具物理合理性和视觉一致性的内容，减少“幻觉”和不自然的形变。</li>
<li><strong>推动更高级的视频编辑和创作工具:</strong> 能够生成更可控、更符合用户意图的视频，为视频编辑、特效制作、虚拟现实内容生成等应用提供更强大的基础。</li>
<li><strong>促进跨模型知识迁移的研究:</strong> 论文展示了如何有效地将一个模型的强大能力（如跟踪和结构理解）迁移到另一个模型（如生成模型）中，为未来研究不同类型模型之间的知识融合提供了新的思路。</li>
<li><strong>为理解和模拟复杂动态场景提供新视角:</strong> 通过强制模型学习和保持结构，有助于我们更深入地理解现实世界中物体运动的内在规律。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>电影和动画制作:</strong> 生成更逼真、更具表现力的角色动画和场景。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 创建更具沉浸感和交互性的虚拟环境和数字角色。</li>
<li><strong>游戏开发:</strong> 生成更流畅、更自然的NPC行为和游戏场景。</li>
<li><strong>机器人学:</strong> 模拟和预测复杂物体的运动，用于训练和评估机器人控制策略。</li>
<li><strong>医学影像分析:</strong> 生成模拟病变发展过程的视频，辅助诊断和治疗规划。</li>
<li><strong>内容创作平台:</strong> 为用户提供更强大的视频生成和编辑工具，降低创作门槛。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了显著的性能提升，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对SAM2的依赖性:</strong> 该方法的核心是“蒸馏”SAM2的运动先验。如果SAM2本身存在某些固有的局限性（例如，在某些极端情况下跟踪失败），这些局限性可能会在一定程度上影响SAM2VideoX的性能。</li>
<li><strong>计算成本:</strong> 引入双向特征融合模块和局部语法流损失可能会增加模型的训练和推理成本。虽然摘要没有直接提及，但更复杂的模块通常意味着更高的计算需求。</li>
<li><strong>通用性:</strong> 摘要强调了对“关节和可变形物体”的改进。虽然这表明了其在复杂场景下的优势，但对于完全刚性或非常简单的场景，其带来的增益可能不如在复杂场景下显著。</li>
<li><strong>“语法流”的定义和鲁棒性:</strong> “Local Gram Flow loss”的具体实现和其对不同类型“语法”的鲁棒性需要进一步的实验验证。摘要中提到“aligns how local features move together”，这暗示了其对局部特征的协同运动有要求，但具体如何定义和衡量这种“协同”以及其在各种复杂情况下的表现，仍需深入研究。</li>
<li><strong>数据需求:</strong> 虽然论文提到“Scaling training data alone, so far, has failed to resolve physically implausible transitions”，但该方法本身可能仍然需要大量高质量的训练数据来有效地学习和蒸馏运动先验。</li>
</ul>
<p>总而言之，这篇论文提出了一种非常有前景的方法，通过巧妙地结合跟踪模型的结构理解能力和扩散模型的生成能力，显著提升了视频生成在结构保真度方面的表现。其提出的双向特征融合和局部语法流损失是关键的技术亮点，有望推动视频生成技术向更真实、更可控的方向发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX).</li>
<li>With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together.</li>
<li>Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11792v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11792v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11782v1'></a></p>
<h2 id="matanyone-2-scaling-video-matting-via-a-learned-quality-evaluator"><a href="https://arxiv.org/abs/2512.11782v1">MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator</a></h2>
<p><strong>Authors:</strong> Peiqing Yang, Shangchen Zhou, Kai Hao, Qingyi Tao</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator”的全面中文摘要：</p>
<p><strong>论文题目：</strong> MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator
<strong>作者：</strong> Peiqing Yang, Shangchen Zhou, Kai Hao, Qingyi Tao</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
视频抠图（Video Matting, VM）领域长期受到现有数据集规模小、真实性不足的限制。尽管利用分割数据可以增强语义稳定性，但缺乏有效的边界监督导致抠图结果常呈现“分割感”，缺乏精细的细节。这阻碍了视频抠图模型在复杂场景下的性能提升和泛化能力。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决上述问题，本文提出了<strong>MatAnyone 2</strong>，其核心创新在于引入了一个<strong>学习型抠图质量评估器（Matting Quality Evaluator, MQE）</strong>。MQE能够<strong>在没有真实抠图标签的情况下</strong>，评估预测抠图（alpha matte）的语义和边界质量，并生成一个像素级的评估图，区分可靠和错误区域。</p>
<p>MQE的贡献体现在两个方面，从而实现了视频抠图的规模化：
*   <strong>在线抠图质量引导（Online Matting-quality Guidance）：</strong> MQE作为训练过程中的在线反馈信号，能够抑制错误区域，提供全面的监督。它通过一个损失函数（<script type="math/tex">L_{eval}</script>）来惩罚错误区域，从而引导模型学习更准确的抠图。
*   <strong>离线数据筛选模块（Offline Selection Module）：</strong> MQE可用于数据整理，通过结合领先的视频和图像抠图模型的优势，提高标注数据的质量。</p>
<p>基于MQE，作者构建了一个<strong>大规模、真实世界的视频抠图数据集VMReal</strong>，包含28K个视频片段和2.4M帧。</p>
<p>此外，为了处理长视频中主体外观的大尺度变化，论文引入了<strong>参考帧训练策略（Reference-frame Training Strategy）</strong>，该策略利用了局部训练窗口之外的长距离帧，以有效处理外观变化，而无需显著增加内存开销。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> MatAnyone 2 在合成和真实世界基准测试中均取得了<strong>最先进的性能</strong>，在所有指标上均超越了现有方法。
*   <strong>数据集构建：</strong> 成功构建了<strong>VMReal数据集</strong>，这是迄今为止最大规模、最真实的视频抠图数据集之一，为视频抠图研究提供了重要资源。
*   <strong>方法有效性：</strong> MQE的引入显著提升了模型在语义准确性和边界细节方面的表现。参考帧策略有效解决了长视频中的外观变化问题。
*   <strong>通用性：</strong> 实验表明，VMReal数据集能够有效提升包括RVM在内的其他视频抠图模型的性能，证明了其作为通用训练资源的价值。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>数据标注的局限性：</strong> 尽管MQE和双分支标注流水线提高了数据标注的自动化程度，但其性能仍受限于所使用的预训练模型（如SAM 2和MattePro）的性能。
*   <strong>迭代优化潜力：</strong> 论文提到，可以考虑将标注流水线升级为迭代精炼过程，让改进后的抠图模型逐步精炼alpha标注，从而进一步提升数据集质量和模型性能。但这需要大量的工程投入和计算资源。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>迭代精炼的标注流水线：</strong> 将数据标注过程与模型训练过程结合，形成一个“闭环”的“数据-模型精炼飞轮”，以持续提升数据集质量和模型性能。
*   <strong>更广泛的应用：</strong> 探索MQE在其他需要像素级质量评估的计算机视觉任务中的应用。
*   <strong>处理更复杂的场景：</strong> 进一步研究如何处理更具挑战性的场景，例如极端的遮挡、快速的运动以及非常精细的透明或半透明物体。</p>
<p>总而言之，这篇论文通过引入创新的MQE和参考帧策略，有效解决了视频抠图领域的数据集规模和模型泛化能力问题，构建了高质量的VMReal数据集，并显著提升了视频抠图的性能，为该领域的研究和应用开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth.</li>
<li>To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training.</li>
<li>Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11782v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11782v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11749v1'></a></p>
<h2 id="svg-t2i-scaling-up-text-to-image-latent-diffusion-model-without-variational-autoencoder"><a href="https://arxiv.org/abs/2512.11749v1">SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</a></h2>
<p><strong>Authors:</strong> Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng, Ziyang Yuan, Xiaoshi Wu, Yuanxing Zhang, Huan Yang, Xintao Wang, Pengfei Wan, Kun Gai, Jie Zhou, Jiwen Lu</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder”的全面中文摘要：</p>
<p><strong>论文摘要：SVG-T2I：在无需变分自编码器的情况下，扩展文本到图像的潜在扩散模型</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决在视觉基础模型（VFM）的表示空间内训练大规模文本到图像（T2I）扩散模型这一新兴但尚未充分探索的领域。核心研究问题在于：
* <strong>能否在一个统一的特征空间中，在不牺牲性能的前提下，同时支持视觉重建、感知、高保真生成和语义理解？</strong>
* <strong>VFM表示是否天然兼容大规模、高分辨率的T2I扩散模型训练，这是实现实际应用的关键？</strong></p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<ul>
<li><strong>SVG-T2I框架的提出：</strong> 作者扩展了现有的SVG（Self-supervised representations for Visual Generation）框架，提出了SVG-T2I，一个能够直接在VFM特征域中进行高质量T2I合成的模型。</li>
<li><strong>直接在VFM特征空间训练：</strong> 与以往依赖VAE等模型将图像映射到低维潜在空间再进行扩散模型训练不同，SVG-T2I直接利用高维VFM（如DINOv3）特征进行扩散模型训练。这利用了VFM本身强大的视觉理解能力。</li>
<li><strong>统一的架构设计：</strong> 采用Unified Next-DiT架构作为骨干，该架构能够自然地处理文本和图像（VFM特征）作为联合序列，实现了跨模态的交互。</li>
<li><strong>可选的残差编码器：</strong> 提供了两种自编码器配置：纯DINOv3特征（autoencoder-P）和带有可选残差分支（autoencoder-R）以补偿高频细节。研究表明，对于高分辨率生成，纯VFM特征已足够，残差编码器并非必需。</li>
<li><strong>大规模训练和多阶段策略：</strong> 论文进行了大规模的T2I训练，并采用了多阶段的渐进式训练策略，从低分辨率到高分辨率，逐步优化模型。</li>
<li><strong>开源：</strong> 作者完全开源了模型（包括自编码器和生成模型）、训练、推理和评估流程以及预训练权重，以促进该领域的研究。</li>
</ul>
<p><strong>3. 主要结果及意义：</strong></p>
<ul>
<li><strong>竞争力表现：</strong> SVG-T2I在GenEval上取得了0.75的得分，在DPG-Bench上取得了85.78的得分，与当前最先进的模型（如SD3-Medium, FLUX.1等）相当，甚至在某些方面超越了SDXL和DALL-E 2。</li>
<li><strong>验证VFM的生成能力：</strong> 实验结果有力地证明了VFM本身具有强大的生成能力，可以直接用于高质量的T2I合成，而无需依赖传统的VAE作为潜在空间映射器。</li>
<li><strong>统一表示的潜力：</strong> 该工作展示了VFM特征空间作为统一表示的巨大潜力，可以整合视觉理解、感知和生成任务，为构建更通用的视觉模型铺平道路。</li>
<li><strong>高分辨率生成能力：</strong> 论文成功地将SVG框架扩展到了高分辨率T2I生成，并证明了VFM特征在高分辨率下依然能保持细节信息。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对特定细节的挑战：</strong> 模型在生成高度细节化的人脸（如眼睛、眉毛）和精确的手指结构时偶尔会遇到困难，这在生成模型中是常见挑战。</li>
<li><strong>文本渲染的局限性：</strong> SVG-T2I在文本渲染方面表现出有限的可靠性。</li>
<li><strong>VFM特征的尺度不稳定性：</strong> 论文指出，当前的VFM编码器（如DINOv2和DINOv3）在不同输入分辨率下可能存在内部不一致性，这会影响模型在不同尺寸图像上的泛化能力和生成质量。这表明未来的研究需要关注尺度不变性。</li>
<li><strong>对专业数据集的需求：</strong> 解决上述细节和文本渲染问题可能需要更专业的训练数据集和更多的计算资源。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提高尺度不变性：</strong> 未来研究需要专注于提升VFM编码器的尺度不变性，以确保模型在不同分辨率下都能保持稳定的生成质量。</li>
<li><strong>改进细节生成和文本渲染：</strong> 通过更精细的训练策略、更丰富的数据集或更先进的模型架构来解决人脸、手指和文本渲染等方面的不足。</li>
<li><strong>构建更通用的统一视觉模型：</strong> 利用SVG-T2I的成功经验，进一步探索如何将VFM表示空间应用于更广泛的视觉任务，实现真正的统一表示。</li>
<li><strong>探索其他VFM的潜力：</strong> 研究不同类型的VFM在T2I生成任务中的表现，以及如何更好地利用它们的优势。</li>
</ul>
<p>总而言之，SVG-T2I论文在T2I生成领域取得了重要进展，它成功地展示了直接在VFM特征空间进行大规模扩散模型训练的可行性和有效性，为构建更强大、更通用的视觉生成模型提供了新的方向，并积极地通过开源贡献来推动社区发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This performance validates the intrinsic representational power of VFMs for generative tasks.</li>
<li>We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11749v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11749v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11720v1'></a></p>
<h2 id="reframing-music-driven-2d-dance-pose-generation-as-multi-channel-image-generation"><a href="https://arxiv.org/abs/2512.11720v1">Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation</a></h2>
<p><strong>Authors:</strong> Yan Zhang, Han Zou, Lincong Feng, Cong Xie, Ruiqi Yu, Zhenpeng Zhan</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation (将音乐驱动的2D舞蹈姿态生成重构为多通道图像生成)</p>
<p><strong>作者：</strong> Yan Zhang, Han Zou, Lincong Feng, Cong Xie, Ruiqi Yu, Zhenpeng Zhan</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
该论文旨在解决音乐驱动的2D舞蹈姿态生成的核心挑战：如何在复杂、高方差的“in-the-wild”（真实世界）数据分布下，生成与音乐在时间上连贯且节奏对齐的2D舞蹈姿态序列。尽管现有的姿态到视频模型能够将2D姿态序列转化为逼真的舞蹈视频，但生成高质量、节奏准确的2D姿态序列仍然是关键瓶颈。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
作者提出了一种新颖的视角，将音乐驱动的2D舞蹈姿态生成重构为一个<strong>音乐-token条件下的多通道图像合成问题</strong>。其核心方法包括：</p>
<ul>
<li><strong>一键式（One-Hot）姿态表示：</strong> 将2D姿态序列编码为一键式图像，这种稀疏表示能够更好地捕捉高方差的2D姿态分布，并借鉴了现代文本到图像模型（如DiT）的成功经验。</li>
<li><strong>预训练图像VAE压缩：</strong> 使用预训练的图像变分自编码器（VAE）将一键式姿态图像压缩成更紧凑的潜在表示。</li>
<li><strong>DiT风格骨干网络：</strong> 采用类似DiT（Diffusion Transformer）的骨干网络来建模潜在表示，从而继承了先进的文本到图像生成模型的架构和训练优势。</li>
<li><strong>时间共享的临时索引方案：</strong> 引入一种显式同步音乐token和姿态潜在表示的时间索引机制，以促进节奏对齐。</li>
<li><strong>参考姿态条件化策略：</strong> 提出一种参考姿态条件化方法，该方法能够保留主体特定的身体比例和屏幕尺度，并支持长时序的“分段-拼接”生成，以提高生成序列的整体连贯性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在大型“in-the-wild”2D舞蹈数据集和经过校准的AIST++2D基准上的实验，该模型在姿态空间和视频空间度量以及人类偏好方面，均取得了显著优于代表性音乐到舞蹈方法的改进。</p>
<ul>
<li><strong>定量结果：</strong> 在FID、DIV、BAS等指标上均有提升，尤其是在FID（真实性）和BAS（节奏对齐）方面表现突出。</li>
<li><strong>人类评估：</strong> 用户研究表明，该模型生成的舞蹈在音乐结构响应、节奏对齐、动作合理性、真实性和多样性等方面获得了压倒性优势。</li>
<li><strong>泛化能力：</strong> 模型在“in-the-wild”数据集上训练，并在未见过的数据集上进行测试，展现了良好的跨音乐流派和跨风格的泛化能力。</li>
<li><strong>意义：</strong> 该工作成功地将2D姿态生成问题框架化为图像生成问题，有效利用了现有先进的图像生成技术，并提出了针对舞蹈生成特性的关键改进，为生成高质量、节奏准确的2D舞蹈姿态提供了新的有效途径。</li>
</ul>
<p><strong>4. 局限性：</strong>
论文中提到了一些局限性：</p>
<ul>
<li><strong>手部姿态质量：</strong> “in-the-wild”舞蹈视频中的手部姿态数据可能存在运动模糊和自遮挡，导致手部关键点标注质量不高，这会影响手部姿态的生成质量，可能导致手指交换或抖动。</li>
<li><strong>多人物交互：</strong> 模型目前仅支持单人舞蹈生成，尚未扩展到多人物交互，如接触、镜像和编队等。</li>
</ul>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，论文提出了潜在的未来研究方向：</p>
<ul>
<li><strong>改进手部姿态生成：</strong> 通过更高帧率的手部裁剪、更鲁棒的手部检测器或轻量级的手部精炼头来提高手部姿态的质量。</li>
<li><strong>多人物舞蹈生成：</strong> 探索如何将模型扩展到支持多人物交互，例如利用碰撞和空间先验来处理多人物舞蹈。</li>
</ul>
<p>总而言之，这篇论文通过将音乐驱动的2D舞蹈姿态生成重新定义为多通道图像合成问题，并引入一键式姿态表示、时间共享索引和参考姿态条件化等创新技术，显著提升了生成舞蹈姿态的质量、节奏准确性和整体连贯性，为该领域的研究提供了重要的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation.</li>
<li>Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11720v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11720v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.11715v1'></a></p>
<h2 id="editmgt-unleashing-potentials-of-masked-generative-transformers-in-image-editing"><a href="https://arxiv.org/abs/2512.11715v1">EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing</a></h2>
<p><strong>Authors:</strong> Wei Chow, Linfeng Li, Lingdong Kong, Zefeng Li, Qi Xu, Hang Song, Tian Ye, Xian Wang, Jinbin Bai, Shilin Xu, Xiangtai Li, Junting Pan, Shaoteng Liu, Ran Zhou, Tianshu Yang, Songhua Liu</p>
<p><strong>Published:</strong> 2025-12-12</p>
<p><strong>Categories:</strong> cs.CV, cs.MM, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本论文提出了一种新颖的基于掩码生成Transformer（MGT）的图像编辑框架EditMGT，旨在解决现有扩散模型（DMs）在图像编辑中存在的全局性修改问题。EditMGT通过利用MGT的局部解码特性，结合精细化的注意力图定位和区域保持采样策略，实现了对编辑区域的精确控制，有效保留了非目标区域的完整性，并显著提升了编辑效率。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>EditMGT的核心创新在于其对MGT在图像编辑任务中的潜力挖掘，具体体现在以下几个方面：</p>
<ul>
<li><strong>从扩散模型转向掩码生成Transformer (MGT)：</strong> 这是最根本的范式转变。论文明确指出，扩散模型固有的全局去噪过程容易导致非目标区域的意外修改。而MGT通过预测掩码Token的策略，天然具备局部解码的能力，能够更好地隔离编辑目标与全局上下文。</li>
<li><strong>多层注意力图整合与精细化定位：</strong> EditMGT利用MGT的交叉注意力图来识别与编辑相关的区域。为了实现更精确的定位，论文提出了一种“多层注意力整合”机制，通过融合不同层的注意力信息来提炼出更精细、更准确的编辑区域信号。</li>
<li><strong>区域保持采样 (Region-Hold Sampling)：</strong> 这是EditMGT实现局部编辑的关键技术。该策略的核心思想是限制Token的翻转（即修改）仅发生在低注意力区域（即被识别为编辑目标区域）。这样可以有效抑制对非目标区域的“ spurious edits”（虚假编辑），从而确保编辑的局部性和对周围区域的保护。</li>
<li><strong>基于预训练MGT的轻量级迁移学习：</strong> 论文展示了如何通过“注意力注入”（attention injection）的方式，将一个预训练的文本到图像MGT模型有效地适配到图像编辑任务中，而无需引入额外的参数。这使得模型在保持性能的同时，具有更高的效率和更小的模型规模。</li>
<li><strong>CrispEdit-2M数据集的构建：</strong> 为了训练和评估EditMGT，论文构建了一个高分辨率、包含七种不同编辑类别的“CrispEdit-2M”数据集。高质量、多样化的数据集对于训练鲁棒的图像编辑模型至关重要。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>EditMGT的提出可能对图像编辑领域产生深远影响：</p>
<ul>
<li><strong>新的主流编辑范式：</strong> 如果EditMGT的性能和效率优势得到广泛验证，它可能成为继扩散模型之后，图像编辑领域的一种新的主流方法论。这可能会促使研究者们更多地探索Transformer在图像生成和编辑任务中的应用。</li>
<li><strong>提升编辑的精细度和可控性：</strong> EditMGT在局部编辑和区域保护方面的优势，将极大地提升用户对图像编辑过程的控制能力，使得用户能够更精确地修改图像的特定部分，而不用担心全局的连锁反应。</li>
<li><strong>加速图像编辑的普及：</strong> 6倍的编辑速度提升意味着更快的迭代和更流畅的用户体验，这对于实际应用和普通用户来说具有巨大的吸引力，有望加速高质量图像编辑技术的普及。</li>
<li><strong>推动模型效率的研究：</strong> 在参数量小于1B的情况下实现与现有模型相当的性能，并大幅提升速度，这为在资源受限环境下进行高性能图像编辑提供了新的思路，将推动对模型效率和轻量化研究的关注。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作与设计：</strong> 广告、营销、平面设计、插画等领域，需要快速、精确地对图像进行局部修改和风格调整。</li>
<li><strong>数字艺术与虚拟现实：</strong> 艺术家和创作者可以利用EditMGT进行更精细的数字艺术创作，在虚拟环境中进行场景编辑和资产修改。</li>
<li><strong>图像修复与增强：</strong> 对于老照片修复、瑕疵去除等任务，EditMGT的局部控制能力可以避免对背景的破坏。</li>
<li><strong>人脸编辑：</strong> 精确的面部特征编辑（如表情、发型、妆容）将受益于其局部修改能力。</li>
<li><strong>医学影像分析：</strong> 在某些需要对特定病灶或区域进行标记、修改或增强的医学影像应用中，EditMGT的精确控制能力可能有所帮助。</li>
<li><strong>自动驾驶与机器人视觉：</strong> 在需要对感知到的场景进行局部修改以进行模拟或训练的场景中，EditMGT的效率和可控性可能发挥作用。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了EditMGT的诸多优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对MGT模型的依赖：</strong> EditMGT的性能高度依赖于其底层MGT模型的质量和预训练效果。如果预训练的MGT模型本身存在不足，可能会影响编辑效果。</li>
<li><strong>注意力机制的鲁棒性：</strong> 尽管论文提出了注意力图整合和区域保持采样，但注意力机制的鲁棒性仍然是一个挑战。在复杂场景或模糊的编辑意图下，注意力图的准确性可能会受到影响，从而导致定位不准或编辑错误。</li>
<li><strong>数据集的覆盖范围：</strong> CrispEdit-2M数据集虽然多样，但其覆盖的七种编辑类别是否能完全代表所有图像编辑场景仍需验证。对于未包含在数据集中的新颖编辑任务，模型的泛化能力可能需要进一步评估。</li>
<li><strong>“相似性能”的定义：</strong> 摘要中提到“相似性能”，这可能意味着在某些指标上，EditMGT可能与最先进的扩散模型仍有差距，尽管在其他方面（如速度）有显著优势。具体“相似”到何种程度，需要查阅详细的实验结果。</li>
<li><strong>对“全局性修改”的定义：</strong> 论文将扩散模型的缺点归结为“全局性修改”，但这种“全局性”的程度和影响范围在不同任务中可能有所不同。EditMGT是否能完全避免所有形式的全局影响，或者在某些极端情况下是否也会出现类似问题，需要进一步研究。</li>
<li><strong>模型的可解释性：</strong> 虽然MGT的注意力图提供了定位信号，但整个编辑过程的深层机制和决策过程的可解释性可能不如一些更直观的模型。</li>
</ul>
<p>总而言之，EditMGT是一篇非常有前景的论文，它通过引入MGT范式和创新的局部编辑技术，为图像编辑领域带来了新的视角和解决方案，尤其是在提升编辑效率和局部控制方面展现出巨大潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT.</li>
<li>On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.11715v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.11715v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-15 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
