<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-18 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-17/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-19/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-18">Arxiv Computer Vision Papers - 2025-11-18</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#from-perception-to-reasoning-deep-thinking-empowers-multimodal-large-language-models" class="nav-link">From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#back-to-basics-let-denoising-generative-models-denoise" class="nav-link">Back to Basics: Let Denoising Generative Models Denoise</a>
                </li>
                <li class="nav-item">
                    <a href="#scaling-spatial-intelligence-with-multimodal-foundation-models" class="nav-link">Scaling Spatial Intelligence with Multimodal Foundation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#segment-anything-across-shots-a-method-and-benchmark" class="nav-link">Segment Anything Across Shots: A Method and Benchmark</a>
                </li>
                <li class="nav-item">
                    <a href="#part-x-mllm-part-aware-3d-multimodal-large-language-model" class="nav-link">Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</a>
                </li>
                <li class="nav-item">
                    <a href="#attention-grounded-enhancement-for-visual-document-retrieval" class="nav-link">Attention Grounded Enhancement for Visual Document Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#descriptor-distance-annotated-traffic-perception-question-answering-dtpqa" class="nav-link">Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-metric-aware-multi-person-mesh-recovery-by-jointly-optimizing-human-crowd-in-camera-space" class="nav-link">Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space</a>
                </li>
                <li class="nav-item">
                    <a href="#sf-recon-simplification-free-lightweight-building-reconstruction-via-3d-gaussian-splatting" class="nav-link">SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#is-your-vlm-sky-ready-a-comprehensive-spatial-intelligence-benchmark-for-uav-navigation" class="nav-link">Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-18">Arxiv Computer Vision Papers - 2025-11-18</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年11月17日ArXiv计算机视觉论文的每日执行摘要，旨在帮助您快速了解关键发展。</p>
<hr />
<p><strong>每日ArXiv计算机视觉论文执行摘要 (2025-11-17)</strong></p>
<p><strong>概述：</strong>
今日ArXiv论文主要围绕<strong>多模态大语言模型 (MLLMs)</strong> 在视觉理解和推理方面的应用展开，特别关注其在<strong>空间智能</strong>和<strong>3D感知</strong>领域的扩展。同时，<strong>基础模型</strong>的优化和特定任务的<strong>基准测试</strong>构建也是重要主题。</p>
<p><strong>主要主题和趋势：</strong></p>
<ol>
<li><strong>多模态大语言模型 (MLLMs) 的深化与扩展：</strong> 多篇论文探讨了MLLMs在视觉推理、3D感知和特定应用（如文档检索、交通感知）中的能力。趋势是让MLLMs不仅仅停留在感知层面，而是深入到更复杂的推理和决策。</li>
<li><strong>空间智能与3D感知：</strong> 这是一个突出主题，多篇论文直接或间接关注模型对空间关系、3D结构和场景理解的能力。这包括3D MLLMs、无人机导航的空间智能基准，以及3D重建技术。</li>
<li><strong>基础模型与生成模型：</strong> 有论文回归基础，探讨生成模型（特别是去噪模型）的本质优化，以及如何利用多模态基础模型扩展空间智能。</li>
<li><strong>基准测试与评估：</strong> 为了更好地评估新模型和新方法，多篇论文提出了新的基准测试，涵盖了跨镜头分割、交通感知问答、无人机导航空间智能等。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>1. "From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models" by Wenxin Zhu et al.</strong><ul>
<li><strong>重要性：</strong> 这篇论文代表了MLLMs发展的一个关键方向——从简单的感知任务转向更深层次的“深度思考”和复杂推理。它可能为未来MLLMs的设计和评估提供新的范式。</li>
</ul>
</li>
<li><strong>2. "Back to Basics: Let Denoising Generative Models Denoise" by Tianhong Li, Kaiming He</strong><ul>
<li><strong>重要性：</strong> Kaiming He团队的这篇论文通常意味着对基础原理的深刻洞察和潜在的范式转变。它可能重新审视去噪生成模型的本质，并提出更有效或更简洁的训练和应用方法，对整个生成模型领域有广泛影响。</li>
</ul>
</li>
<li><strong>3. "Scaling Spatial Intelligence with Multimodal Foundation Models" by Zhongang Cai et al.</strong><ul>
<li><strong>重要性：</strong> 这篇论文将多模态基础模型与空间智能相结合，预示着未来AI系统在理解和操作物理世界方面将取得重大进展。它可能为机器人、自动驾驶等领域提供强大的通用能力。</li>
</ul>
</li>
<li><strong>5. "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model" by Chunshi Wang et al.</strong><ul>
<li><strong>重要性：</strong> 这是MLLMs向3D领域深度扩展的一个重要例子，特别强调了“部分感知”能力，这对于精细的3D理解和交互至关重要。</li>
</ul>
</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>深度思考/推理型MLLMs：</strong> MLLMs不再仅仅是“看图说话”，而是被赋予更强的逻辑推理和问题解决能力。</li>
<li><strong>3D MLLMs：</strong> 将MLLMs的能力从2D图像扩展到3D点云、网格等数据，实现对三维世界的语义理解和交互。</li>
<li><strong>特定领域空间智能基准：</strong> 针对无人机导航、交通感知等实际应用场景，开发更具挑战性和实用性的空间智能评估标准。</li>
<li><strong>3D Gaussian Splatting在重建中的应用：</strong> SF-Recon利用3D Gaussian Splatting进行轻量级建筑重建，表明这种新兴技术在实际应用中的潜力。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<ol>
<li><strong>"From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models"</strong> (Wenxin Zhu et al.) - 如果您关注MLLMs的未来发展和推理能力，这篇是必读。</li>
<li><strong>"Back to Basics: Let Denoising Generative Models Denoise"</strong> (Tianhong Li, Kaiming He) - 如果您对生成模型的基础理论和潜在的效率提升感兴趣，这篇值得深入研究。</li>
<li><strong>"Scaling Spatial Intelligence with Multimodal Foundation Models"</strong> (Zhongang Cai et al.) - 如果您对通用AI在物理世界的应用（如机器人、自动驾驶）感兴趣，这篇提供了宏观视角。</li>
<li><strong>"Part-X-MLLM: Part-aware 3D Multimodal Large Language Model"</strong> (Chunshi Wang et al.) - 如果您专注于3D视觉理解和MLLMs在3D领域的应用，这篇提供了具体的技术方向。</li>
<li><strong>"Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation"</strong> (Lingfeng Zhang et al.) - 如果您从事无人机、机器人导航或对VLM在实际复杂环境中的鲁棒性感兴趣，这个基准提供了宝贵的评估工具。</li>
</ol>
<hr />
<p>这份摘要希望能帮助您高效地筛选和理解今日ArXiv计算机视觉领域的重要进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.12861v1">From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</a></li>
<li><a href="#2511.13720v1">Back to Basics: Let Denoising Generative Models Denoise</a></li>
<li><a href="#2511.13719v1">Scaling Spatial Intelligence with Multimodal Foundation Models</a></li>
<li><a href="#2511.13715v1">Segment Anything Across Shots: A Method and Benchmark</a></li>
<li><a href="#2511.13647v1">Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</a></li>
<li><a href="#2511.13415v1">Attention Grounded Enhancement for Visual Document Retrieval</a></li>
<li><a href="#2511.13397v1">Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)</a></li>
<li><a href="#2511.13282v1">Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space</a></li>
<li><a href="#2511.13278v1">SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</a></li>
<li><a href="#2511.13269v1">Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.12861v1'></a></p>
<h2 id="from-perception-to-reasoning-deep-thinking-empowers-multimodal-large-language-models"><a href="https://arxiv.org/abs/2511.12861v1">From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</a></h2>
<p><strong>Authors:</strong> Wenxin Zhu, Andong Chen, Yuchen Song, Kehai Chen, Conghui Zhu, Ziyan Chen, Tiejun Zhao</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“从感知到推理：深度思考赋能多模态大语言模型”论文的摘要，内容涵盖了您要求的所有要点：</p>
<p><strong>论文摘要：从感知到推理：深度思考赋能多模态大语言模型</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决多模态大语言模型（MLLMs）在复杂推理任务中存在的显著局限性。尽管MLLMs在感知任务（如图像理解和生成）上取得了巨大成功，但它们在处理需要逻辑推导、因果推理和多步推理的复杂任务时，仍面临推理路径不透明和泛化能力不足的挑战。核心研究问题是：如何通过引入“多模态思维链”（MCoT）机制，增强MLLMs的复杂推理能力、透明度和可解释性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的核心贡献在于对“多模态思维链”（MCoT）进行了系统性综述和深入分析，并从以下几个方面阐述了其创新性：
*   <strong>MCoT范式分析：</strong> 论文详细介绍了MCoT的背景和理论动机，将其视为解决MLLMs推理瓶颈的有效方案。MCoT通过将复杂问题分解为一系列明确的中间推理步骤，模拟人类的认知过程，从而提高模型的逻辑严谨性和可解释性。
*   <strong>训练阶段策略：</strong> 论文探讨了MCoT在后训练阶段的关键实现策略，包括监督微调（SFT）和强化学习（RL）。这些方法旨在引导模型学习CoT风格的推理模式和输出格式，并优化多步推理过程，将模型从依赖模式匹配的“黑箱”转变为能够进行显式逻辑推理的“白箱”。
*   <strong>推理阶段策略：</strong> 论文总结了推理阶段的多种策略，如CoT提示、搜索策略（如Beam Search、MCTS）、自我完善和知识增强，以及Agent辅助技术。这些策略无需更新模型参数，通过巧妙的引导激活模型潜在的推理能力，使其遵循最优计算路径。
*   <strong>理论机制分析：</strong> 论文深入分析了MCoT增强推理能力的内在机制，包括信息表示（通过生成中间文本弥合模态间隙）、结构化推理（将复杂任务分解为可控子步骤）和过程监督（在训练和推理过程中对中间推理步骤进行监督）。</p>
<p><strong>3. 主要结果及其意义：</strong>
该论文通过系统回顾和分析，揭示了MCoT在以下方面的显著意义：
*   <strong>增强推理能力：</strong> MCoT通过引入显式推理链，显著提升了MLLMs在数学、逻辑、时空、多图像和多模态集成推理等复杂任务上的性能。
*   <strong>提高可解释性与透明度：</strong> MCoT使模型的推理过程变得透明和可解释，有助于理解模型决策的依据，减少“黑箱”预测问题。
*   <strong>广泛的应用潜力：</strong> MCoT在具身智能、自动驾驶、医疗保健、多模态生成、机器翻译和情感计算等领域展现出巨大的应用潜力，推动了这些领域智能系统向更高层次认知推理发展。
*   <strong>促进研究发展：</strong> 论文为CoT-MLLMs这一新兴研究方向提供了结构化的参考和理论基础，有助于促进该领域的持续发展。</p>
<p><strong>4. 局限性：</strong>
论文也坦诚地指出了当前MCoT面临的挑战：
*   <strong>幻觉问题：</strong> 尽管CoT在一定程度上缓解了幻觉，但仍可能引入“过度思考”等副作用，且模态间对齐不足可能引发幻觉。
*   <strong>推理链长度控制：</strong> 推理链过短可能导致思考不足，过长则可能导致信息遗忘和过度思考，影响推理鲁棒性。
*   <strong>安全与伦理问题：</strong> MLLMs的跨模态对齐和显式推理特性使其更容易受到恶意攻击和生成有害信息。
*   <strong>模态覆盖有限：</strong> 当前MCoT主要集中于文本-图像等双模态组合，对更复杂、更通用的全模态推理场景覆盖有限。
*   <strong>效率问题：</strong> 尽管System 2推理能提高性能，但计算成本高昂，如何在性能和计算资源之间取得平衡是一个关键挑战。
*   <strong>数据集和基准建设不足：</strong> 高质量多模态推理数据集和全面的评估基准仍然稀缺，现有研究缺乏统一的标准化评估框架。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文展望了MCoT未来的研究方向，包括：
*   <strong>鲁棒推理：</strong> 探索更细粒度的优化方法，例如在步骤级别进行幻觉缓解和错误检测，以及开发更有效的推理链长度控制机制。
*   <strong>安全推理：</strong> 构建多层次、协调的防御系统，整合数据、模型和推理，以全面提升模型的安全性和鲁棒性。
*   <strong>全模态推理：</strong> 扩展MCoT的统一处理能力至更多模态（如视频、音频、点云等），实现跨模态信息融合和推理支持。
*   <strong>高效推理：</strong> 进一步研究推理经济性，平衡模型性能和计算资源，开发更高效的推理策略和评估基准。
*   <strong>数据集和基准建设：</strong> 建设高质量、多维度、多层次的多模态推理数据集，并建立统一的标准化评估框架，以更全面地评估模型能力。
*   <strong>跨模态对齐机制：</strong> 深入研究如何构建更有效的跨模态对齐机制，以减少视觉干扰导致的推理偏差。</p>
<p>这份摘要全面概括了论文的核心内容，突出了其在多模态推理领域的重要贡献和未来发展方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT.</li>
<li>Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.12861v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.12861v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13720v1'></a></p>
<h2 id="back-to-basics-let-denoising-generative-models-denoise"><a href="https://arxiv.org/abs/2511.13720v1">Back to Basics: Let Denoising Generative Models Denoise</a></h2>
<p><strong>Authors:</strong> Tianhong Li, Kaiming He</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "<script type="math/tex">\textbf{Just image Transformers}</script>", or <script type="math/tex">\textbf{JiT}</script>, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<h3 id="back-to-basics-let-denoising-generative-models-denoise_1">论文摘要分析：Back to Basics: Let Denoising Generative Models Denoise</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于挑战了当前去噪扩散模型中预测噪声或噪声量的主流范式，并提出了一种回归“经典”去噪方法的新范式：直接预测干净数据。作者认为，基于流形假设，直接预测干净数据能让网络在处理高维数据时更有效，并展示了仅使用大型补丁（large-patch）的像素级Transformer（JiT）就能在没有分词器、预训练或额外损失的情况下，实现具有竞争力的生成性能。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>范式转变：</strong> 从预测噪声或噪声量转向直接预测干净数据。这是对当前扩散模型设计哲学的一个根本性改变。</li>
<li><strong>流形假设的应用：</strong> 强调自然数据位于低维流形上，而噪声量则不然。这一理论基础支撑了直接预测干净数据能让“容量不足”的网络在高维空间中有效运行的观点。</li>
<li><strong>“Just image Transformers” (JiT)：</strong> 提出了一种极简的Transformer架构。它直接在像素上操作，使用大型补丁（例如16x16或32x32），无需分词器（tokenizer）、预训练（pre-training）或额外的损失函数（extra loss）。这简化了模型设计，并强调了Transformer在原始数据上的强大能力。</li>
<li><strong>在高维空间中的鲁棒性：</strong> 论文指出，在256x256和512x512分辨率的ImageNet上，当预测高维噪声量可能“灾难性失败”时，JiT仍能报告具有竞争力的结果，这突出了其方法的鲁棒性。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>简化扩散模型设计：</strong> 如果JiT的性能和效率得到进一步验证，它可能会极大地简化未来扩散模型的架构和训练流程，减少对复杂组件（如分词器、预训练模型）的依赖。</li>
<li><strong>重新审视基础理论：</strong> 论文重新强调了流形假设在生成模型中的重要性，可能会促使研究人员重新思考生成模型与数据内在结构之间的关系。</li>
<li><strong>推动Transformer在原始数据上的应用：</strong> 证明了纯粹的像素级Transformer在生成任务上的强大潜力，可能会鼓励更多研究探索Transformer在其他原始数据（如音频、视频）上的直接应用。</li>
<li><strong>提高高分辨率生成效率：</strong> 解决了在高分辨率下预测噪声量可能失败的问题，为高分辨率图像生成提供了一条更有效和鲁棒的路径。</li>
<li><strong>“自包含”范式的潜力：</strong> 追求一种自包含的Transformer扩散范式，可能为未来的生成模型研究提供一个新的方向，即在不依赖外部知识或复杂预处理的情况下，从原始数据中学习。</li>
</ul>
<p><strong>4. 相关领域或应用可能受益于这项研究</strong></p>
<ul>
<li><strong>高分辨率图像生成：</strong> 电影制作、游戏开发、医学影像等需要生成高质量、高分辨率图像的领域。</li>
<li><strong>图像修复与超分辨率：</strong> 直接预测干净数据的方法可能在这些任务中表现出更强的鲁棒性和准确性。</li>
<li><strong>无监督/自监督学习：</strong> 简化模型和训练过程的特性，使其成为探索更高效无监督或自监督生成模型的基础。</li>
<li><strong>计算资源受限环境：</strong> 如果JiT确实能以更少的复杂性达到竞争性性能，那么在计算资源有限的设备上部署生成模型将变得更加可行。</li>
<li><strong>科学数据生成：</strong> 在物理、化学、生物等领域，生成模拟数据或补充实验数据。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>“容量不足”的定义：</strong> 摘要中提到“apparently under-capacity networks”，但没有明确定义这种“容量不足”的程度，以及它与传统扩散模型中使用的网络相比的具体差异。</li>
<li><strong>性能的绝对水平：</strong> 摘要中提到“competitive results”，但没有给出具体的量化指标（如FID、IS等）来与SOTA模型进行直接比较。虽然在高分辨率下表现出色，但其在其他指标上是否能全面超越现有复杂模型仍需验证。</li>
<li><strong>训练效率和资源：</strong> 尽管模型设计简化，但大型补丁Transformer在像素级别操作，其训练时间和计算资源消耗（尤其是对于非常大的图像和模型）仍是一个需要关注的问题。</li>
<li><strong>泛化能力：</strong> 结果主要在ImageNet上报告，其在更复杂、多样化的数据集（如LAION-5B）上的泛化能力和稳定性有待进一步验证。</li>
<li><strong>理论深度：</strong> 摘要强调了流形假设，但没有深入阐述如何通过直接预测干净数据来“利用”流形结构，以及这种利用在数学上如何优于预测噪声。</li>
<li><strong>“无额外损失”的含义：</strong> 尽管声称没有额外损失，但扩散模型本身通常包含一个重建损失（例如L1或L2），这里可能指的是没有额外的感知损失、对抗损失等。这需要通过阅读全文来澄清。</li>
</ul>
<hr />
<p>总的来说，这篇论文提出了一种大胆且具有启发性的方法，挑战了当前扩散模型的主流范式。如果其主张得到充分验证，它可能会对生成模型领域产生深远的影响，推动研究回归更简洁、更基础的原理。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss.</li>
<li>Our approach is conceptually nothing more than "<script type="math/tex">\textbf{Just image Transformers}</script>", or <script type="math/tex">\textbf{JiT}</script>, as we call it.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13720v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13720v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13719v1'></a></p>
<h2 id="scaling-spatial-intelligence-with-multimodal-foundation-models"><a href="https://arxiv.org/abs/2511.13719v1">Scaling Spatial Intelligence with Multimodal Foundation Models</a></h2>
<p><strong>Authors:</strong> Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei, Zhiqian Lin, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Xiangyu Fan, Hanming Deng, Lewei Lu, Liang Pan, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.MM, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Zhongang Cai等人撰写的论文“Scaling Spatial Intelligence with Multimodal Foundation Models”的全面摘要：</p>
<p><strong>论文题目：</strong> Scaling Spatial Intelligence with Multimodal Foundation Models
<strong>作者：</strong> Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei, Zhiqian Lin, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Xiangyu Fan, Hanming Deng, Lewei Lu, Liang Pan, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
尽管多模态基础模型取得了显著进展，但在空间智能方面仍存在不足。本研究旨在探索如何通过扩展多模态基础模型来培养空间智能，以解决现有模型在理解、推理和操作三维空间方面的局限性，这对于具身通用人工智能（AGI）至关重要。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>SenseNova-SI 系列模型：</strong> 论文引入了SenseNova-SI系列多模态基础模型，该模型建立在Qwen3-VL、InternVL3等视觉理解模型和Bagel等统一理解与生成模型之上。
*   <strong>SenseNova-SI-8M 数据集：</strong> 采用系统性方法，根据严格的空间能力分类，精心策划了包含八百万（8M）多样化数据样本的SenseNova-SI-8M数据集，以构建高性能且鲁棒的空间智能模型。该数据集特别关注了以往被忽视的透视（Perspective-taking）任务。
*   <strong>数据驱动的扩展策略：</strong> 论文强调数据扩展和训练策略在提升空间理解能力中的核心作用，而非仅仅依赖模型架构的改变。通过对InternVL3、Qwen3-VL和Bagel等基础模型进行持续训练，验证了数据扩展对空间智能的积极影响。
*   <strong>多模态基础模型的公开：</strong> 所有新训练的多模态基础模型都已公开，以促进该方向的进一步研究。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的空间智能性能：</strong> SenseNova-SI在多项空间智能基准测试中展现出前所未有的性能，包括VSI-Bench（68.7%）、MMSI（43.3%）、MindCube（85.6%）、ViewSpatial（54.6%）和SITE（50.1%）。
*   <strong>保持通用多模态理解能力：</strong> 在提升空间智能的同时，SenseNova-SI在通用多模态理解基准MMBench-En上仍保持了强大的性能（84.9%），表明大规模空间智能训练不会损害模型的通用能力。
*   <strong>数据扩展的影响：</strong> 研究分析了数据扩展对空间能力的影响，发现性能增益随着训练数据量的增加而逐渐减小，但仍持续提升。
*   <strong>涌现的泛化能力：</strong> 观察到早期涌现的泛化能力迹象，模型在训练数据多样性的支持下，能够将所学知识迁移到看似不相关的任务，并能外推到训练分布之外的更长空间上下文。
*   <strong>鲁棒性分析：</strong> 通过受控实验和循环测试设计，验证了SenseNova-SI真正获得了空间能力，而非利用记忆、标注偏差或训练数据中的意外捷径。在VSI-Debiased和MindCube上的测试表明，模型较少依赖文本启发式和语言捷径，更多地依赖空间接地理解。
*   <strong>下游应用潜力：</strong> SenseNova-SI在具身机器人操作任务（EmbodiedBench）中无需微调即可实现显著性能提升，展示了其作为具身AI基础模型的潜力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>链式思考（CoT）的局限性：</strong> 初步研究发现，文本链式思考范式（CoT）对空间推理的提升有限，不足以弥补其计算开销，这表明基于文本的推理可能不是空间智能最有效或最高效的范式，可能需要根本不同的推理机制。
*   <strong>数据饱和趋势：</strong> 性能增益随着训练数据量的增加而逐渐减小，可能预示着数据扩展本身难以达到人类水平的空间智能，未来的进步可能需要范式转变。
*   <strong>涌现能力的初步性：</strong> 论文中讨论的涌现泛化能力迹象仍处于早期阶段，需要进一步深入研究。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>算法创新：</strong> 鉴于数据饱和趋势，未来的研究应侧重于算法创新，以突破SenseNova-SI所建立的强大空间能力基础。
*   <strong>空间链式思考的范式转变：</strong> 探索超越传统文本链式思考的新范式，以更有效、高效地实现空间智能推理。
*   <strong>具身AI应用：</strong> 进一步探索SenseNova-SI在具身机器人操作等下游任务中的应用，并将其作为具身AI的强大基础。
*   <strong>持续更新与研究：</strong> SenseNova-SI是一个正在进行的项目，报告将持续更新，鼓励社区在此方向上进行进一步研究。</p>
<p>总而言之，这篇论文通过大规模、多样化的数据扩展策略，成功构建了SenseNova-SI系列多模态基础模型，显著提升了模型在多项空间智能基准上的性能，并验证了其在通用能力保持、泛化能力涌现以及下游应用方面的潜力。同时，论文也指出了当前空间智能研究的局限性，为未来的研究方向提供了宝贵的见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13719v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13719v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13715v1'></a></p>
<h2 id="segment-anything-across-shots-a-method-and-benchmark"><a href="https://arxiv.org/abs/2511.13715v1">Segment Anything Across Shots: A Method and Benchmark</a></h2>
<p><strong>Authors:</strong> Hengrui Hu, Kaining Ying, Henghui Ding</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Hengrui Hu, Kaining Ying, Henghui Ding撰写的论文“Segment Anything Across Shots: A Method and Benchmark”的全面摘要。</p>
<hr />
<h3 id="segment-anything-across-shots-a-method-and-benchmark_1">论文摘要：Segment Anything Across Shots: A Method and Benchmark</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文主要关注<strong>多镜头半监督视频目标分割 (MVOS)</strong> 任务。现有的视频目标分割 (VOS) 方法主要针对单镜头视频，在处理视频中的镜头不连续性时表现不佳，这严重限制了它们在真实世界应用中的实用性。MVOS任务的挑战在于，如何在视频中跨越多个镜头，持续准确地分割由初始掩码指示的目标对象，尤其是在目标外观、空间位置和背景发生显著变化时。此外，MVOS任务缺乏高质量的标注数据集。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，论文提出了以下关键创新：</p>
<ul>
<li><strong>过渡模仿数据增强 (Transition Mimicking Data Augmentation, TMA) 策略：</strong> 针对MVOS任务中多镜头标注数据稀缺的问题，TMA通过模拟多样化的镜头过渡（如强变换、不同视频片段间的切换、随机复制和渐进平移等），利用现有的单镜头数据集合成高质量的多镜头训练样本。这使得模型能够在不依赖原生多镜头标注的情况下进行有效训练，显著缓解了数据稀缺性。</li>
<li><strong>Segment Anything Across Shots (SAAS) 模型：</strong> SAAS是首个专门针对多镜头视频的半监督VOS方法。它包含：<ul>
<li><strong>过渡检测模块 (Transition Detection Module, TDM)：</strong> 轻量级模块，用于有效检测视频序列中的镜头过渡。</li>
<li><strong>过渡理解模块 (Transition Comprehension Module, TCH)：</strong> 在检测到过渡时，TCH进一步理解过渡，生成压缩的过渡状态表示，并利用辅助训练目标（存在预测和边界框回归）进行指导，以细化先前的记忆。</li>
<li><strong>局部记忆库 (Local Memory Bank, Blocal)：</strong> 一种无需训练的记忆细化机制，用于存储细粒度的对象特征，通过构建最小生成树（MST）将目标对象无监督地划分为语义连贯的子区域，以增强跨过渡的分割质量。</li>
</ul>
</li>
<li><strong>Cut-VOS 基准数据集：</strong> 为了公平评估跨镜头分割性能并更好地反映真实世界多镜头视频的复杂性，论文引入了一个新的MVOS基准数据集。Cut-VOS包含10.2K实例掩码，涵盖100个视频中的174个独特对象，具有比现有YouMVOS数据集高1.6倍的镜头过渡频率和3倍多的对象类别，并对过渡类型进行了手动筛选，以确保多样性和难度。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> 在YouMVOS和Cut-VOS基准测试上进行的广泛实验表明，所提出的SAAS方法在J&amp;F（区域相似性）和It（跨镜头分割性能）指标上均实现了最先进的性能，显著优于现有的VOS方法（如XMem, DEVA, Cutie, SAM2）。例如，SAAS-B+在YouMVOS上达到73.5% J&amp;F和68.9% It，在Cut-VOS上达到60.7% J&amp;F和53.1% It。
*   <strong>TMA策略的有效性：</strong> TMA策略显著提升了模型在多镜头场景下的泛化能力，证明了通过模拟过渡进行数据增强的有效性。
*   <strong>SAAS模块的鲁棒性：</strong> SAAS模型通过其过渡检测和理解模块，能够有效地处理复杂的镜头过渡，并在拥挤场景和目标外观剧烈变化时保持一致的分割性能。
*   <strong>Cut-VOS的挑战性：</strong> Cut-VOS数据集的引入揭示了现有VOS方法在处理复杂多镜头视频时的显著性能下降，特别是对于“延迟切入”、“特写视图”和“场景变化”等过渡类型，现有模型表现不佳，凸显了Cut-VOS的挑战性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>极端外观变化：</strong> SAAS方法在处理目标对象外观发生极端变化（例如，同一个人穿着不同服装、发型不同）的场景时仍然存在困难。TMA策略无法有效模拟此类场景，且局部视觉线索可能不足以提供帮助。
*   <strong>对视觉特征匹配的依赖：</strong> SAAS仍然依赖于视觉特征匹配，缺乏鲁棒的长期推理能力，这导致在目标外观完全改变的过渡中出现分割错误。
*   <strong>数据分布偏差：</strong> TMA策略在复制过程中直接将对象放置在顶层，缺乏针对遮挡场景的专门设计，且训练数据集中遮挡实例的代表性不足，可能导致F（轮廓质量）指标下降。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>增强长期推理能力：</strong> 针对极端外观变化和相似干扰物区分的挑战，未来的研究需要减少对纯视觉特征匹配的依赖，并要求更强的推理能力。
*   <strong>改进TMA策略：</strong> 探索更有效的TMA策略，以模拟更复杂的场景，例如目标外观的极端变化，从而进一步提升模型的泛化能力。
*   <strong>解决遮挡问题：</strong> 改进数据增强策略和模型设计，以更好地处理遮挡场景，减少数据分布偏差。
*   <strong>多模态融合：</strong> 结合更多模态信息（如音频、文本）来辅助理解复杂的视频内容和过渡，可能有助于提升模型在更具挑战性场景下的性能。</p>
<hr />
<p>这篇论文在MVOS领域做出了重要贡献，不仅提出了创新的数据增强策略和模型架构，还发布了高质量的基准数据集，为未来该领域的研究奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively.</li>
<li>To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions.</li>
<li>Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13715v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13715v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13647v1'></a></p>
<h2 id="part-x-mllm-part-aware-3d-multimodal-large-language-model"><a href="https://arxiv.org/abs/2511.13647v1">Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</a></h2>
<p><strong>Authors:</strong> Chunshi Wang, Junliang Ye, Yunhan Yang, Yang Li, Zizhuo Lin, Jun Zhu, Zhuo Chen, Yawei Luo, Chunchao Guo</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&amp;A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/</p>
<p><strong>Analysis:</strong></p>
<p>以下是Chunshi Wang等人撰写的论文“Part-X-MLLM: Part-aware 3D Multimodal Large Language Model”的全面摘要：</p>
<p><strong>1. 解决的主要问题或研究问题</strong></p>
<p>该论文旨在解决当前3D多模态大语言模型（MLLMs）在处理3D资产时存在的“结构不透明性”问题。现有的模型通常将3D形状视为静态、整体的形式，缺乏对细粒度语义理解、组合编辑和程序化动画所需的部件结构进行推理和操作的能力。具体来说，Part-X-MLLM致力于开发一个能够理解和命名部件、将引用与持久性边界框（BBox）关联、并编译可执行的添加/删除/修改程序，同时将几何合成任务委托给强大的几何引擎的语言原生工具。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>Part-X-MLLM引入了以下关键创新和方法论贡献：</p>
<ul>
<li><strong>Part-X-MLLM框架：</strong> 提出了一种原生的3D部件感知多模态大语言模型，将多样化的3D任务（生成、编辑、推理）统一为一种结构化、可执行的部件语法程序。模型接收RGB点云和自然语言提示，自动生成包含部件级边界框、语义描述和编辑命令的连贯token序列。</li>
<li><strong>双编码器架构：</strong> 提出了一种双编码器架构，将结构（XYZ+法线）与外观（RGB）解耦。这避免了在单一编码器中处理这两种信息时可能出现的表示冲突，并在边界框列表、多部件接地和部件问答等任务上取得了持续的性能提升。</li>
<li><strong>结构化规划语言和自回归解码器：</strong> 模型采用一个从预训练LLM初始化的解码器，将融合后的结构、语义和文本token作为输入，自回归地生成遵循结构化规划语言的程序化输出。这种语言定义了用于部件表示（如<code>&lt;boxs&gt;...&lt;boxe&gt;</code>）和编辑操作（如<code>&lt;adds&gt;</code>、<code>&lt;dels&gt;</code>、<code>&lt;mods&gt;</code>）的特殊token。</li>
<li><strong>语义粒度控制：</strong> 通过使用文本语义（CLIP嵌入）对部件边界框进行聚类，模型能够动态控制语义粒度，实现从粗略组件到细粒度部件的无缝过渡，无需手动干预。</li>
<li><strong>UniPart-Bench基准：</strong> 建立了包含3万个条目的部件中心基准数据集，涵盖11个任务家族，并采用几何和语言指标，用于严格评估模型生成的规划质量和下游性能。</li>
<li><strong>多阶段指令微调：</strong> 采用两阶段训练课程，首先预训练一个结构感知编码器以实现鲁棒的几何理解，然后进行全面的指令微调，整合语义编码器并将强大的LLM与专门的任务语法对齐。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>实验结果表明，Part-X-MLLM在多项任务上取得了最先进的性能：</p>
<ul>
<li><strong>边界框生成：</strong> 在边界框生成任务上，Part-X-MLLM在Voxel Recall、Voxel IoU和Bbox IoU指标上均优于基线模型（如PartField和OmniPart），显著提升了几何准确性。</li>
<li><strong>部件理解问答（Part QA）：</strong> 在UniPart-Bench上，Part-X-MLLM在SBERT、SimCSE、BLEU-1、ROUGE-L和METEOR等所有指标上均取得了显著提升，表明其强大的部件级接地和推理能力。</li>
<li><strong>整体3D对象描述：</strong> 在整体对象描述任务上，模型也超越了现有最佳分数，在所有指标上均有显著改进，生成了更准确和详细的描述。</li>
<li><strong>定性结果：</strong> 定性结果展示了模型在部件感知编辑（如替换、添加、删除特定部件）和语义粒度控制方面的卓越能力，能够生成高质量、结构化的规划，并保持原始对象的完整性。</li>
<li><strong>消融研究：</strong> 双编码器架构在所有评估任务上均持续优于单一编码器基线，验证了结构和语义解耦的有效性。</li>
</ul>
<p>这些结果的意义在于，Part-X-MLLM提供了一个统一的、语言原生的接口，能够对3D资产进行语义精确的部件感知生成和编辑，极大地推动了3D多模态理解和生成领域的发展。</p>
<p><strong>4. 论文中提到的局限性</strong></p>
<p>论文也提到了以下局限性：</p>
<ul>
<li><strong>推理速度：</strong> 较长的token序列会减慢推理速度。论文建议通过简单的压缩和分层分组来缓解延迟问题。</li>
<li><strong>分割质量：</strong> 基于置信度的边界框分割仍然相对较浅，整合更强大的特征可以提高分割质量。</li>
<li><strong>LLM通用语言能力：</strong> 在3D任务上进行微调可能会降低基础LLM的通用语言能力。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>虽然论文没有明确列出未来的研究方向，但从其局限性和贡献中可以推断出一些潜在方向：</p>
<ul>
<li><strong>优化推理速度：</strong> 探索更高效的token序列压缩和分层分组方法，以提高模型在处理长序列时的推理速度。</li>
<li><strong>提升分割质量：</strong> 整合更先进的特征和分割技术，以提高基于置信度的边界框分割的精度和鲁棒性。</li>
<li><strong>平衡通用性与专业性：</strong> 研究如何更好地平衡模型在3D任务上的专业能力与基础LLM的通用语言能力，例如通过更精细的微调策略或模块化设计。</li>
<li><strong>更复杂的3D交互：</strong> 扩展模型的程序化语法，以支持更复杂、更精细的3D交互和操作，例如物理模拟、动画控制等。</li>
<li><strong>多模态数据融合：</strong> 探索除了RGB点云和文本之外的其他3D多模态数据（如纹理、材质、运动数据）的有效融合方法。</li>
<li><strong>用户反馈与迭代：</strong> 结合用户反馈机制，使模型能够进行迭代式优化，以更好地满足用户在3D生成和编辑中的需求。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar.</li>
<li>By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend.</li>
<li>Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&amp;A, compositional generation, and localized editing through one unified interface.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13647v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13415v1'></a></p>
<h2 id="attention-grounded-enhancement-for-visual-document-retrieval"><a href="https://arxiv.org/abs/2511.13415v1">Attention Grounded Enhancement for Visual Document Retrieval</a></h2>
<p><strong>Authors:</strong> Wanqing Cui, Wei Huang, Yazhi Guo, Yibo Hu, Meiguang Jin, Junfeng Ma, Keping Bi</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.IR, cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文“Attention Grounded Enhancement for Visual Document Retrieval”的全面摘要。</p>
<hr />
<h3 id="attention-grounded-enhancement-for-visual-document-retrieval_1">论文摘要：Attention Grounded Enhancement for Visual Document Retrieval</h3>
<p><strong>作者：</strong> Wanqing Cui, Wei Huang, Yazhi Guo, Yibo Hu, Meiguang Jin, Junfeng Ma, Keping Bi</p>
<p><strong>1. 主要问题或研究问题：</strong>
视觉文档检索（Visual Document Retrieval, VDR）旨在根据文本查询找到语义相关的文档页面。现有方法虽然通过基于截图的文档编码和细粒度晚期交互显著提高了检索性能，但仍面临一个核心问题：检索器通常仅使用粗粒度的全局相关性标签进行训练，这导致它们无法识别文档中哪些具体区域支持匹配。因此，检索器倾向于依赖表面线索，难以捕捉隐式语义连接，尤其在处理非抽取式查询时表现不佳。这限制了检索的准确性和可解释性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了<strong>A</strong>ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) 框架，其核心创新点包括：</p>
<ul>
<li><strong>利用多模态大型语言模型（MLLM）注意力作为代理局部监督：</strong> AGREE利用预训练MLLM的跨模态注意力机制，生成查询条件下的补丁级显著性注意力分数，作为细粒度的局部监督信号。这使得模型能够识别文档中与查询相关的具体区域。</li>
<li><strong>空间保留注意力下采样：</strong> 为了弥合MLLM高分辨率注意力与下游检索器补丁网格之间的分辨率不匹配，AGREE采用自适应最大池化（adaptive max pooling）对注意力图进行下采样。这确保了关键区域的峰值注意力值在下采样后仍能保持突出，从而实现可扩展的细粒度知识迁移。</li>
<li><strong>注意力引导的检索器训练：</strong> 检索器采用双重目标进行联合优化：<ul>
<li><strong>全局对比学习（Lglobal）：</strong> 优化查询与整个文档页面之间的全局相关性。</li>
<li><strong>局部对齐损失（Llocal）：</strong> 将检索器的补丁相似性分数与MLLM注意力模式对齐。值得注意的是，局部监督仅应用于正向查询-页面对，因为只有在文档相关时，MLLM的注意力模式才具有语义意义。论文比较了KL散度、Top-K显著性对比和余弦相似度三种局部对齐损失，发现余弦相似度表现最佳，因为它更侧重于显著区域的方向性一致性。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的性能提升：</strong> 在具有挑战性的ViDoRe V2基准测试上，AGREE显著优于仅使用全局监督的基线模型。例如，AGREEQwen2.5在平均nDCG@1上取得了+7.03%的绝对增益，在平均nDCG@5上取得了+2.95%的绝对增益，这表明其在处理需要语义推理的非抽取式检索任务方面的强大能力。
*   <strong>更深层次的对齐和可解释性：</strong> 定量和定性分析表明，AGREE促进了查询词与文档区域之间更深层次的对齐，超越了表面级匹配，实现了更准确和可解释的检索。检索器能够捕捉隐式语义对应关系，并突出支持匹配的特定内容。
*   <strong>注意力质量与性能的相关性：</strong> 实验证明，MLLM注意力与人类判断的对齐程度越高，注意力引导训练的效果越好。使用“query-token attention”策略的7B模型与人类标注的对齐度最高，并在V1和V2上取得了最大的性能提升。
*   <strong>选择性注意力监督的有效性：</strong> 即使在训练数据子集上应用注意力监督，AGREE也能取得显著改进，尤其是在“不匹配优先”采样策略下，通过关注检索器与MLLM注意力模式最不一致的样本，可以在减少标注成本的同时保持性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>MLLM注意力信号的噪声：</strong> MLLM的注意力信号不可避免地包含噪声，过强的注意力目标可能导致模型对噪声过拟合。
*   <strong>“不匹配优先”采样的局限性：</strong> 尽管“不匹配优先”采样可以减少标注成本，但它需要为所有样本计算注意力以识别困难实例。此外，在不完整的标注训练集下，基于检索性能的困难实例挖掘可能不可靠，因为低性能实例可能包含大量未标注的正例，引入噪声而非有用的监督。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更精确的接地形式：</strong> 未来研究可以探索更精确的接地形式，例如目标检测风格的标注，以进一步改善细粒度对齐，从而构建更可解释的视觉文档检索系统。
*   <strong>利用更准确的标注数据：</strong> 随着未来方法能够生成更准确的代理标签，AGREE可以直接利用它们实现进一步的性能提升。
*   <strong>改进困难实例挖掘：</strong> 探索更可靠的基于检索性能的困难实例挖掘方法，以实现真正的注意力标注成本降低。</p>
<hr />
<p>总而言之，AGREE框架通过将MLLM的细粒度注意力作为局部监督信号，成功地解决了视觉文档检索中现有方法对粗粒度全局标签的过度依赖问题。它不仅提高了检索性能，特别是在处理非抽取式查询时，还使得检索结果更具可解释性，能够揭示文档中驱动相关性的具体区域。这为未来构建更智能、更透明的文档理解系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework.</li>
<li>Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13415v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13415v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13397v1'></a></p>
<h2 id="descriptor-distance-annotated-traffic-perception-question-answering-dtpqa"><a href="https://arxiv.org/abs/2511.13397v1">Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)</a></h2>
<p><strong>Authors:</strong> Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Nikos Theodoridis等人撰写的论文“Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)”的全面摘要，使用中文撰写：</p>
<p><strong>论文摘要：Distance-Annotated Traffic Perception Question Answering (DTPQA)</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决自动驾驶领域中视觉-语言模型（VLMs）感知能力评估的不足。现有基准通常无法在复杂的交通场景中，特别是在不同距离下，独立且鲁棒地评估VLMs的感知能力，而这对于自动驾驶的安全至关重要。具体来说，研究问题是如何创建一个能够评估VLM在交通场景中对关键物体和代理的感知能力，并能分析其性能随距离增加而下降的基准，同时排除推理或高级世界知识等其他技能的干扰。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要创新是引入了<strong>Distance-Annotated Traffic Perception Question Answering (DTPQA)</strong>，这是一个专门用于评估VLM在交通场景中感知系统的视觉问答（VQA）基准。其关键贡献包括：
*   <strong>专注于感知能力评估：</strong> DTPQA设计了琐碎但对驾驶决策至关重要的感知型问题，避免了需要复杂推理或高级世界知识的问题，从而孤立地评估VLMs的感知能力。
*   <strong>距离标注：</strong> 每个DTPQA样本都包含被提问对象与摄像头的距离标注。这使得研究人员能够分析VLM的性能如何随着对象距离的增加而下降，这是自动驾驶中一个关键的挑战。
*   <strong>双重基准设计：</strong> DTPQA包含两部分：
    *   <strong>DTP-Synthetic：</strong> 使用CARLA模拟器创建的合成基准，允许精确控制交通场景、对象距离和环境条件，从而生成高度可控和可重复的数据。
    *   <strong>DTP-Real：</strong> 基于nuScenes数据集的真实世界图像构建，通过添加距离和特定感知问题标注来扩展现有数据。
*   <strong>平衡的样本分布：</strong> 基准确保每个距离和每个答案类别的样本数量平衡，以防止模型因语言偏差而获得优势。
*   <strong>多类别感知任务：</strong> 涵盖了多种与驾驶相关的感知任务，例如识别行人存在、行人方向、行人数量、车辆转向灯状态、交通灯颜色和交通标志识别。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   DTPQA已成功用于评估最先进（SOTA）的小型VLM在交通场景中的感知能力。
*   评估结果（如论文图4所示）表明，SOTA小型VLM在DTPQA任务上的表现显著低于人类水平，尤其是在需要空间感知的任务上（例如，Cat.2-Synth、Cat.4-Synth、Cat.2-Real）。
*   这种显著的性能差距证明了DTPQA能够有效挑战现有模型，并作为评估VLM在交通场景中感知能力的宝贵基准。
*   数据集的可用性（包括Python脚本）使得研究人员能够生成更多类似数据，进一步推动该领域的研究。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>DTP-Synthetic的模拟器限制：</strong> 尽管CARLA是优秀的模拟器，但模拟数据仍可能存在与真实世界不符的误差，需要手动审查和清理。
*   <strong>DTP-Real对nuScenes的依赖：</strong> DTP-Real的质量继承自nuScenes数据集，虽然nuScenes是高质量数据集，但并非完美无瑕。
*   <strong>距离精确度：</strong> 在DTP-Real中，由于真实世界数据的性质，无法像合成数据那样精确控制对象距离，而是使用距离区间进行分类。
*   <strong>特定场景限制：</strong> 例如，在DTP-Synthetic的交通灯和交通标志类别中，由于对象位置固定和摄像头视野限制，某些距离（如5米）的样本难以捕获。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>VLM性能与问题措辞/提示结构的关系：</strong> DTPQA可用于研究VLM在简单视觉任务上的性能如何受问题措辞或提示结构微小变化的影响。
*   <strong>利用额外标注进行研究：</strong>
    *   <strong>天气依赖性研究：</strong> DTP-Synth中的天气标注可用于研究VLM感知能力对天气条件的依赖性。
    *   <strong>计数能力研究：</strong> Cat.3-Real中包含的场景中多人距离方差的标注，可用于研究VLM在恒定距离下计数能力的方差依赖性。
    *   <strong>城镇标注：</strong> DTP-Synth中的城镇标注也可用于其他分析。
*   <strong>VLM的微调和训练：</strong> DTPQA数据集可用于微调VLM，或作为更大训练数据集的一部分，以提升其在自动驾驶感知任务中的表现。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance.</li>
<li>In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13397v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13397v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13282v1'></a></p>
<h2 id="towards-metric-aware-multi-person-mesh-recovery-by-jointly-optimizing-human-crowd-in-camera-space"><a href="https://arxiv.org/abs/2511.13282v1">Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space</a></h2>
<p><strong>Authors:</strong> Kaiwen Wang, Kaili Zheng, Yiming Shi, Chenyi Guo, Ji Wu</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Kaiwen Wang等人撰写的论文“Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space”的全面摘要，重点突出其主要贡献、结果和局限性。</p>
<p><strong>论文摘要：Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决从单张图像中进行多人人体网格恢复（HMR）的挑战。现有方法通常以单人中心的方式处理，独立优化每个人体，导致场景缺乏一致性，例如深度冲突和尺度不一致。这限制了端到端HMR模型在学习复杂人际和人-环境关系方面的潜力。核心问题是如何在相机空间中实现多人场景的度量一致性重建。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<ul>
<li><strong>深度条件平移优化（DTO）框架：</strong> 论文引入了一种新颖的、基于优化的DTO方法，用于联合优化人群中所有个体在相机空间中的平移。DTO利用人体身高的人体测量学先验和单目深度估计器提供的深度线索，在一个最大后验（MAP）框架内，解决所有主体在场景中一致的放置问题。这确保了场景级别的连贯性，解决了现有方法中深度和尺度不一致的问题。</li>
<li><strong>DTO-Humans数据集：</strong> 通过将DTO应用于4D-Humans数据集，作者构建了一个新的大规模伪真值（pGT）数据集，名为DTO-Humans。该数据集包含0.56M高质量、场景一致的多人图像，具有平均每张图像4.8人的密集人群，为训练更鲁棒的多人模型提供了关键的、物理上合理的监督。</li>
<li><strong>度量感知HMR（MA-HMR）网络：</strong> 论文提出了一个端到端网络MA-HMR，可以直接估计度量尺度下的人体网格和相机参数。该网络包含一个专门的相机分支，用于预测相机的视场（FoV），并引入了一种新颖的相对度量损失，明确惩罚个体之间不合理的真实世界尺寸关系，从而实现真正的度量尺度网格恢复。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<ul>
<li><strong>DTO的有效性：</strong> DTO框架显著提高了伪真值数据的质量。在Relative Human数据集上，DTO将基线CHMR模型的PCDR0.2（all）分数从60.43提高到74.16，超越了所有微调的SOTA方法。在MuPoTS数据集上，DTO实现了最低的MPJPE，证明了其在不同人群密度场景中的一致性和度量准确性。</li>
<li><strong>MA-HMR的性能：</strong> MA-HMR在Relative Human数据集上取得了75.35的PCDR0.2（all）新SOTA，证明了端到端微调在处理复杂深度线索和年龄分布方面的优势。在3DPW、CMU Panoptic和Hi4D等经典基准测试上，MA-HMR也取得了最先进的性能，例如在3DPW上实现了58.5毫米的MPJPE和36.3毫米的PA-MPJPE，并在CMU Panoptic上实现了最低的平均误差76.3毫米。</li>
<li><strong>度量准确性：</strong> MA-HMR通过相对度量损失有效地学习了合理的人体尺度，显著降低了身高误差，进一步验证了其在度量尺度恢复方面的能力。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong></p>
<ul>
<li><strong>对上游深度估计模型的依赖：</strong> DTO框架的局限性在于其对上游相对深度估计模型的准确性。深度图中存在的错误或模糊性可能会传播，导致不合理的场景重建。例如，严重的遮挡可能导致深度模型将前景深度值分配给背景人物，从而导致DTO错误地缩小其尺度。</li>
<li><strong>模糊深度线索：</strong> 在深度线索模糊的场景中（例如运动员跳跃），深度模型可能会依赖有缺陷的启发式方法，导致人物在场景中放置错误。</li>
<li><strong>部分可见性：</strong> 在没有清晰地平面且部分可见的场景中，深度模型可能会混淆，导致其错误地扁平化多个人的相对深度，DTO随后继承了这些错误。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>改进单目深度估计：</strong> 论文明确指出，未来单目深度估计的进步将直接提高DTO框架的准确性和鲁棒性，从而带来更好的3D人体场景理解。</li>
<li><strong>更复杂的场景理解：</strong> 解决深度估计模型在遮挡、模糊深度线索和部分可见性等挑战性场景中的局限性，将是未来研究的关键方向。</li>
<li><strong>扩展到动态场景：</strong> 尽管论文主要关注单张图像，但将度量一致性扩展到视频或动态场景中的多人重建，将是重要的未来工作。</li>
</ul>
<p>总而言之，这篇论文通过引入DTO框架和DTO-Humans数据集，以及创新的MA-HMR网络，在多人人体网格恢复领域取得了显著进展，有效地弥合了相对场景理解和绝对度量重建之间的鸿沟。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd.</li>
<li>Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image.</li>
<li>Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale.</li>
<li>This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales.</li>
<li>Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13282v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13282v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13278v1'></a></p>
<h2 id="sf-recon-simplification-free-lightweight-building-reconstruction-via-3d-gaussian-splatting"><a href="https://arxiv.org/abs/2511.13278v1">SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting</a></h2>
<p><strong>Authors:</strong> Zihan Li, Tengfei Wang, Wentian Gan, Hao Zhan, Xin Wang, Zongqian Zhan</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zihan Li等人撰写的论文“SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting”的全面摘要。</p>
<hr />
<h3 id="sf-recon-3d">SF-Recon: 基于3D高斯泼溅的免简化轻量级建筑重建</h3>
<p><strong>1. 主要问题或研究问题</strong></p>
<p>该论文旨在解决传统多视角几何（multi-view geometry）流水线在生成轻量级建筑表面模型时所面临的挑战。传统方法通常涉及密集的重建、网格化和随后的简化步骤，这些步骤不仅繁琐、计算成本高昂，而且其质量高度依赖于中间高复杂度网格的保真度，导致鲁棒性和可复现性差。具体来说，研究问题是如何直接从多视角图像重建轻量级、结构忠实的建筑表面模型，而无需耗时的后处理网格简化步骤。</p>
<p><strong>2. 关键创新或方法贡献</strong></p>
<p>SF-Recon提出了一种新颖的框架，通过以下关键创新直接从多视角图像重建轻量级建筑表面模型：</p>
<ul>
<li><strong>直接重建，免除简化：</strong> SF-Recon是首个利用3D高斯泼溅（3DGS）直接从多视角图像重建轻量级建筑表面网格的方法，消除了传统流水线中复杂的后处理网格简化需求。</li>
<li><strong>法线梯度引导的高斯优化（Normal-Gradient-Guided Gaussian Optimization）：</strong> 为了确保高斯场准确捕捉建筑框架，该方法引入了法线梯度引导的优化。它通过在训练过程中估计表面法线并计算图像梯度来提取边缘掩码，然后将这些掩码整合到损失函数中，引导高斯基元集中在结构边界（如屋脊和墙壁边界），同时抑制非边缘区域的占用。</li>
<li><strong>多视角边缘一致性剪枝策略（Multi-View Edge-Consistency Pruning Strategy）：</strong> 在后续训练迭代中，该策略系统地移除对结构边界支持很少或没有支持的高斯基元。通过计算每个高斯基元在不同视角下投影到建筑边缘的一致性得分，低得分基元被剪枝，从而产生一个更稀疏、边界对齐的高斯场，提高结构清晰度并减少冗余。</li>
<li><strong>多视角深度约束的Delaunay三角剖分重建（Multi-View Depth-Constrained Delaunay Triangulation Reconstruction）：</strong> 该方法利用训练期间渲染的深度图，建立可靠的3D点云到2D图像的可见性对应关系。随后，通过可见性驱动的Delaunay图割（graph cut）提取表面，生成一个轻量级、结构忠实的建筑网格。这确保了即使在采样密度变化和遮挡情况下也能保持几何细节和表面平滑度。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<ul>
<li><strong>性能优越：</strong> 在作者提出的SF数据集上（包含10个手动重建的建筑模型），SF-Recon在保持计算效率的同时，能够直接从多视角图像重建轻量级建筑模型，显著减少了面数和顶点数。</li>
<li><strong>结构保真度高：</strong> 与PGSR、2DGS和Metashape等基线方法相比，SF-Recon生成的网格在保持轻量化的同时，更好地保留了建筑的关键结构特征（如屋脊和墙壁边界），避免了过度平滑和结构完整性丧失。</li>
<li><strong>鲁棒性强：</strong> 实验结果表明，SF-Recon对输入图像分辨率的变化不敏感，在不同分辨率下均表现出一致的良好性能，优于传统方法（如Metashape）在低分辨率下的性能下降问题。</li>
<li><strong>效率提升：</strong> 通过消除后处理网格简化步骤，SF-Recon简化了整个重建流水线，提高了效率和可复现性。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong></p>
<ul>
<li><strong>顶点和面数：</strong> 尽管SF-Recon能够生成高质量的轻量级模型，但与传统劳动密集型简化流水线相比，它在边缘处仍保留了过多的顶点，导致面数冗余和网格简化不足。</li>
<li><strong>纹理稀疏区域的性能：</strong> 当多视角图像覆盖范围广且纹理丰富时，从法线中提取边界掩码是有效的。但在纹理稀疏的区域，其性能会下降。</li>
<li><strong>计算效率：</strong> 尽管论文强调了计算效率，但仍有进一步改进的空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>提高效率和鲁棒性：</strong> 未来的工作将专注于进一步提高SF-Recon的计算效率和在纹理稀疏区域的鲁棒性。</li>
<li><strong>优化顶点和面数：</strong> 探索新的策略以进一步减少边缘处的冗余顶点和面数，从而实现更极致的轻量化。</li>
<li><strong>结合语义信息：</strong> 进一步整合语义信息，以更好地引导高斯基元的分布和剪枝，从而在复杂场景中实现更精确和结构化的重建。</li>
</ul>
<hr />
<p>这份摘要旨在全面概括SF-Recon论文的核心内容，突出其在计算机视觉和3D重建领域的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency.</li>
<li>Website:https://lzh282140127-cell.github.io/SF-Recon-project/</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13278v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13278v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.13269v1'></a></p>
<h2 id="is-your-vlm-sky-ready-a-comprehensive-spatial-intelligence-benchmark-for-uav-navigation"><a href="https://arxiv.org/abs/2511.13269v1">Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation</a></h2>
<p><strong>Authors:</strong> Lingfeng Zhang, Yuchen Zhang, Hongsheng Li, Haoxiang Fu, Yingbo Tang, Hangjun Ye, Long Chen, Xiaojun Liang, Xiaoshuai Hao, Wenbo Ding</p>
<p><strong>Published:</strong> 2025-11-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Lingfeng Zhang等人撰写的论文“Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation”的摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决现有视觉-语言模型（VLMs）在无人机（UAV）导航场景中空间智能能力未被充分探索的问题。当前的VLM评估基准主要关注人类视角，而非无人机视角，导致在复杂、动态的无人机环境中，VLM在导航和环境解释方面的有效性存在不确定性。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>SpatialSky-Bench基准的引入：</strong> 论文提出了一个全面的基准SpatialSky-Bench，专门用于评估VLM在无人机导航中的空间智能能力。该基准分为两大类（环境感知和场景理解）和13个细分子类别，涵盖了从边界框识别、颜色、距离、高度到着陆安全分析等任务。
*   <strong>SpatialSky-Dataset数据集的构建：</strong> 为了支持基准评估和模型训练，论文构建了一个包含100万个样本的大规模数据集SpatialSky-Dataset，其中包含多样化的标注（包括RGB图像、语义掩码、LiDAR深度数据、姿态信息和边界框标注），并通过自动化流程生成了问答对。
*   <strong>Sky-VLM模型的开发：</strong> 论文引入了Sky-VLM，这是一个专门为无人机空间推理设计的VLM，采用两阶段训练方法：首先在SpatialSky-Dataset上进行监督微调（SFT）以获取无人机特定的空间推理能力；然后通过基于群相对策略优化（GRPO）的强化微调（RFT）进一步提升模型在关键空间推理任务上的决策准确性。</p>
<p><strong>3. 主要结果及其重要性</strong>
*   对主流开源和闭源VLM的广泛评估显示，它们在复杂的无人机导航场景中表现不佳，凸显了其空间能力的显著不足。
*   Sky-VLM在所有基准任务上均实现了最先进的性能（SOTA），平均得分达到53.30，比最佳基线模型（GPT-5）提高了139.6%。这表明Sky-VLM在无人机场景中的空间智能能力显著优于现有模型。
*   消融研究证实，两阶段训练方法，特别是强化微调，对提升模型在空间推理任务上的性能至关重要，尤其是在需要像素级准确度的任务中。
*   数据规模扩展研究表明，随着训练数据量的增加，模型性能持续提升，且强化学习阶段能有效增强空间推理能力。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文主要强调了现有VLM在处理无人机视角下的空间智能方面的显著局限性，例如：物体尺度变化、俯视遮挡、深度信息缺乏以及复杂的地面理解要求。这些是SpatialSky-Bench和SpatialSky-Dataset旨在解决的挑战，但论文并未明确提及Sky-VLM自身的具体局限性。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文为开发适用于无人机场景的空间感知VLM铺平了道路。未来的研究可以进一步探索：
*   优化Sky-VLM模型以适应更广泛的无人机导航任务和更复杂的动态环境。
*   将Sky-VLM的空间智能能力与其他无人机系统集成，以实现更高级别的自主导航和决策。
*   探索更高效的数据生成方法和训练策略，以进一步提升VLM在无人机场景中的鲁棒性和泛化能力。
*   研究如何将VLM的空间智能与其他感知模态（如雷达、惯性测量单元等）更紧密地融合，以应对更具挑战性的无人机导航场景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation.</li>
<li>To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios.</li>
<li>Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts.</li>
<li>Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.13269v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.13269v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-18 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
