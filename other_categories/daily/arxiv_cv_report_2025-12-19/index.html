<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-19 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-18/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-22/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-19">Arxiv Computer Vision Papers - 2025-12-19</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-rewardbench-2-evaluating-omni-reward-models-for-interleaved-text-and-image" class="nav-link">Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</a>
                </li>
                <li class="nav-item">
                    <a href="#kling-omni-technical-report" class="nav-link">Kling-Omni Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-language-action-models-for-autonomous-driving-past-present-and-future" class="nav-link">Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</a>
                </li>
                <li class="nav-item">
                    <a href="#the-world-is-your-canvas-painting-promptable-events-with-reference-images-trajectories-and-text" class="nav-link">The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</a>
                </li>
                <li class="nav-item">
                    <a href="#next-embedding-prediction-makes-strong-vision-learners" class="nav-link">Next-Embedding Prediction Makes Strong Vision Learners</a>
                </li>
                <li class="nav-item">
                    <a href="#easyv2v-a-high-quality-instruction-based-video-editing-framework" class="nav-link">EasyV2V: A High-quality Instruction-based Video Editing Framework</a>
                </li>
                <li class="nav-item">
                    <a href="#dvgt-driving-visual-geometry-transformer" class="nav-link">DVGT: Driving Visual Geometry Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#differences-that-matter-auditing-models-for-capability-gap-discovery-and-rectification" class="nav-link">Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</a>
                </li>
                <li class="nav-item">
                    <a href="#adatooler-v-adaptive-tool-use-for-images-and-videos" class="nav-link">AdaTooler-V: Adaptive Tool-Use for Images and Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#depth-any-panoramas-a-foundation-model-for-panoramic-depth-estimation" class="nav-link">Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-19">Arxiv Computer Vision Papers - 2025-12-19</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月18日 Arxiv 计算机视觉领域论文的简明执行摘要。这份摘要旨在帮助忙碌的研究人员快速了解该领域最重要的发展。</p>
<hr />
<p><strong>执行摘要：2025年12月18日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集呈现出几个显著的趋势：</p>
<ul>
<li><strong>多模态融合的深化与泛化：</strong> 多个研究聚焦于整合文本、图像、视频甚至动作信息，以构建更强大、更通用的模型。这包括评估多模态奖励模型、开发用于自动驾驶的视觉-语言-动作模型，以及利用参考图像、轨迹和文本进行事件生成。</li>
<li><strong>基础模型的构建与应用：</strong> 论文中出现了构建通用基础模型的努力，例如用于全景深度估计的“Depth Any Panoramas”，以及旨在提升视觉学习能力的“Next-Embedding Prediction”。</li>
<li><strong>模型的可解释性、审计与鲁棒性：</strong> 研究开始关注模型的内部机制和能力差异，例如通过“Differences That Matter”来审计模型并发现和纠正能力差距。</li>
<li><strong>视觉内容生成与编辑的进步：</strong> 论文展示了在图像和视频生成与编辑方面的创新，如“The World is Your Canvas”和“EasyV2V”。</li>
<li><strong>自适应与工具使用：</strong> “AdaTooler-V”展示了模型在处理图像和视频时自适应工具使用的能力，预示着模型将更加灵活和高效。</li>
</ul>
<p><strong>特别值得关注的论文：</strong></p>
<ul>
<li><strong>"Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image"</strong>：这篇论文在多模态奖励模型评估方面迈出了重要一步，对于衡量和改进跨文本和图像的通用奖励模型至关重要。</li>
<li><strong>"The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text"</strong>：该研究在事件生成方面展现了令人兴奋的潜力，通过多模态输入实现更具创造性和可控性的内容生成。</li>
<li><strong>"Next-Embedding Prediction Makes Strong Vision Learners"</strong>：这项工作提出了一种简单但有效的方法来提升视觉学习器的性能，可能对未来的视觉模型预训练产生广泛影响。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>全能奖励模型（Omni Reward Models）：</strong> 评估和开发能够处理多种模态输入的奖励模型是当前的热点。</li>
<li><strong>视觉-语言-动作（VLA）模型在特定领域的应用：</strong> 尤其是在自动驾驶等复杂场景中，VLA模型的整合和发展是关键。</li>
<li><strong>基于参考的生成模型：</strong> 利用参考图像、轨迹和文本来指导内容生成，实现更精细化的控制。</li>
<li><strong>自适应工具使用：</strong> 模型能够根据任务和数据动态选择和使用工具，是提升模型泛化能力和效率的重要方向。</li>
<li><strong>模型能力审计与对齐：</strong> 深入理解模型的内在能力，并识别和弥合能力差距，对于构建可靠和负责任的AI至关重要。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其对多模态评估、基础模型构建以及内容生成领域的潜在影响，以下论文值得深入阅读：</p>
<ol>
<li><strong>"Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image"</strong> (对于理解多模态模型评估的最新进展至关重要)</li>
<li><strong>"The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text"</strong> (对于对生成模型和多模态内容创作感兴趣的研究人员)</li>
<li><strong>"Next-Embedding Prediction Makes Strong Vision Learners"</strong> (对于希望提升视觉模型性能和理解预训练技术的研究人员)</li>
<li><strong>"Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation"</strong> (对于3D视觉和基础模型研究人员)</li>
</ol>
<hr />
<p>希望这份摘要能帮助您快速掌握近期 Arxiv 计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.16899v1">Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</a></li>
<li><a href="#2512.16776v1">Kling-Omni Technical Report</a></li>
<li><a href="#2512.16760v1">Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</a></li>
<li><a href="#2512.16924v1">The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</a></li>
<li><a href="#2512.16922v1">Next-Embedding Prediction Makes Strong Vision Learners</a></li>
<li><a href="#2512.16920v1">EasyV2V: A High-quality Instruction-based Video Editing Framework</a></li>
<li><a href="#2512.16919v1">DVGT: Driving Visual Geometry Transformer</a></li>
<li><a href="#2512.16921v1">Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</a></li>
<li><a href="#2512.16918v1">AdaTooler-V: Adaptive Tool-Use for Images and Videos</a></li>
<li><a href="#2512.16913v1">Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.16899v1'></a></p>
<h2 id="multimodal-rewardbench-2-evaluating-omni-reward-models-for-interleaved-text-and-image"><a href="https://arxiv.org/abs/2512.16899v1">Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</a></h2>
<p><strong>Authors:</strong> Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to &gt;90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image
<strong>Authors:</strong> Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad
<strong>Categories:</strong> cs.CL, cs.CV
<strong>Published Date:</strong> 2025-12-18</p>
<p><strong>Abstract:</strong>
Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to &gt;90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.</p>
<hr />
<p><strong>中文分析：</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong>
本论文提出了 Multimodal RewardBench 2 (MMRB2)，这是首个针对处理交织文本和图像的“全能型”（omni）模型而设计的、全面的多模态奖励模型（RM）评估基准。MMRB2 涵盖了文本到图像生成、图像编辑、交织生成和多模态推理等四个关键任务，通过高质量、专家标注的偏好数据，为评估和改进多模态奖励模型提供了重要的资源和研究方向。</p>
<p><strong>2. 关键创新或方法论：</strong>
*   <strong>首个全面的多模态奖励模型基准 (MMRB2)：</strong> 论文的核心创新在于构建了一个专门针对处理交织文本和图像的多模态奖励模型的基准。这填补了现有研究中对这类模型评估的空白，因为以往的奖励模型研究主要集中在纯文本领域。
*   <strong>任务多样性与深度：</strong> MMRB2 不仅包含基础的生成任务（文本到图像、图像编辑、交织生成），还引入了更具挑战性的“图像思维”（thinking-with-images）多模态推理任务，这要求模型能够理解和整合视觉信息进行逻辑推理。
*   <strong>高质量、专家标注的数据集：</strong> 论文强调了其数据集的质量，包括：
    *   <strong>实用且具挑战性的提示 (prompts)：</strong> 确保评估的真实性和难度。
    *   <strong>来自 SOTA 模型和代理的响应：</strong> 提供了当前最先进模型生成的输出，用于比较。
    *   <strong>强人类专家共识的偏好对：</strong> 通过“集成过滤策略”（ensemble filtering strategy）精心筛选，确保了标注数据的可靠性和一致性，这对于训练有效的奖励模型至关重要。
*   <strong>评估方法的多样性：</strong> 论文不仅评估了传统的基于人类偏好的奖励模型，还考察了“多模态 LLM-as-a-judge”等新兴的自动化评估方法，并对比了不同模型（包括 Gemini 3 Pro, GPT-5, Gemini 2.5 Pro, GPT-4o, Qwen3-VL-32B, Gemini 2.5 Flash）在 MMRB2 上的表现。</p>
<p><strong>3. 对该领域的潜在影响：</strong>
*   <strong>推动多模态奖励模型的发展：</strong> MMRB2 的发布将极大地促进多模态奖励模型的研究和开发。研究人员将有了一个标准化的平台来评估和比较他们的模型，从而加速算法的迭代和优化。
*   <strong>提升多模态生成和理解能力：</strong> 通过更有效的奖励模型，可以训练出在多模态任务上表现更优的 LLM，从而提升模型在图像生成、编辑、多模态对话和推理等方面的能力。
*   <strong>为模型评估提供新视角：</strong> “LLM-as-a-judge”在多模态领域的应用和评估，为自动化模型评估提供了新的思路和实践，有助于降低人工评估的成本和时间。
*   <strong>促进跨模态对齐和理解：</strong> 评估奖励模型在处理交织文本和图像时的表现，间接推动了模型对视觉和语言信息之间深层对齐和理解的研究。</p>
<p><strong>4. 可能受益的相关领域或应用：</strong>
*   <strong>多模态对话系统：</strong> 能够理解和生成包含图像的对话，例如智能助手、虚拟客服等。
*   <strong>内容创作工具：</strong> 辅助用户进行图像生成、编辑，以及根据文本描述创作包含图像的叙事内容。
*   <strong>教育和培训：</strong> 开发能够理解和解释图文并茂内容的智能教育平台。
*   <strong>辅助技术：</strong> 为视障人士提供更丰富的图像描述和多模态信息交互。
*   <strong>机器人和自动驾驶：</strong> 提升机器人对复杂环境（包含图像和指令）的理解和决策能力。
*   <strong>多模态搜索和推荐：</strong> 结合图像和文本信息进行更精准的搜索和个性化推荐。</p>
<p><strong>5. 从摘要中可以推断出的局限性：</strong>
*   <strong>评估的局限性：</strong> 尽管 MMRB2 旨在全面，但任何基准都可能无法完全覆盖所有潜在的多模态场景和挑战。摘要中提到“21个源任务”，这表明其覆盖范围是有限的。
*   <strong>“LLM-as-a-judge”的局限性：</strong> 尽管论文评估了 LLM 作为评判者，但 LLM 本身也可能存在偏见、幻觉或对某些细微差别的理解不足，这可能影响评估的准确性。摘要中也指出了 LLM 评判者与人类专家之间仍有差距（&gt;90% vs 66-80%）。
*   <strong>数据标注的成本和主观性：</strong> 尽管强调了专家共识，但偏好标注本身仍然可能存在一定程度的主观性，尤其是在评估创造性或开放式生成任务时。
*   <strong>模型性能的相对性：</strong> 摘要中给出的模型性能是相对于 MMRB2 基准而言的，并不代表模型在所有实际应用中的绝对表现。
*   <strong>未来研究方向的提示：</strong> 摘要中提到“key areas to improve the reward models going forward”，这暗示了当前奖励模型在某些方面仍存在不足，需要进一步的研究和改进。</p>
<p><strong>对计算机视觉领域的趣味性或重要性：</strong></p>
<p>这篇论文对于计算机视觉领域具有重要的意义，主要体现在以下几个方面：</p>
<ul>
<li><strong>视觉与语言的深度融合：</strong> MMRB2 关注的是“交织”的文本和图像，这意味着模型需要处理的不仅仅是独立的图像或文本，而是它们之间相互嵌入、相互依赖的复杂关系。这直接推动了计算机视觉模型在理解和生成具有上下文关联的视觉内容方面的能力。</li>
<li><strong>从“理解”到“生成”的桥梁：</strong> 论文涵盖了文本到图像生成、图像编辑和交织生成等任务，这表明其研究成果直接服务于生成式视觉模型的发展。通过更优的奖励模型，可以指导生成模型产生更符合人类偏好、更具创造性或更符合指令的图像。</li>
<li><strong>“图像思维”的挑战：</strong> “thinking-with-images”这一任务尤其令人兴奋。它要求模型不仅能识别图像中的物体，还能基于图像内容进行推理、规划或解决问题。这需要模型具备更高级的视觉理解能力，能够从像素层面提取抽象概念，并将其与逻辑推理结合。这对于需要视觉感知和决策的机器人、自动驾驶等领域至关重要。</li>
<li><strong>评估方法的创新：</strong> 引入“LLM-as-a-judge”来评估多模态模型，为计算机视觉领域提供了一种新的、可扩展的评估范式。这可以加速对视觉模型（尤其是生成模型）的评估过程，并可能发现人类评估者容易忽略的细微问题。</li>
<li><strong>推动多模态数据集的建设：</strong> MMRB2 的成功将激励更多高质量、多模态数据集的创建，这些数据集将成为未来多模态研究的重要基石。</li>
</ul>
<p>总而言之，这篇论文通过构建一个创新的、高质量的基准，直接解决了当前多模态模型（尤其是处理交织文本和图像的模型）在训练和评估中的关键瓶颈，为计算机视觉领域在理解、生成和推理方面的发展提供了重要的推动力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation.</li>
<li>MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16899v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16899v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16776v1'></a></p>
<h2 id="kling-omni-technical-report"><a href="https://arxiv.org/abs/2512.16776v1">Kling-Omni Technical Report</a></h2>
<p><strong>Authors:</strong>  Kling Team, Jialu Chen, Yuanzheng Ci, Xiangyu Du, Zipeng Feng, Kun Gai, Sainan Guo, Feng Han, Jingbin He, Kang He, Xiao Hu, Xiaohua Hu, Boyuan Jiang, Fangyuan Kong, Hang Li, Jie Li, Qingyu Li, Shen Li, Xiaohan Li, Yan Li, Jiajun Liang, Borui Liao, Yiqiao Liao, Weihong Lin, Quande Liu, Xiaokun Liu, Yilun Liu, Yuliang Liu, Shun Lu, Hangyu Mao, Yunyao Mao, Haodong Ouyang, Wenyu Qin, Wanqi Shi, Xiaoyu Shi, Lianghao Su, Haozhi Sun, Peiqin Sun, Pengfei Wan, Chao Wang, Chenyu Wang, Meng Wang, Qiulin Wang, Runqi Wang, Xintao Wang, Xuebo Wang, Zekun Wang, Min Wei, Tiancheng Wen, Guohao Wu, Xiaoshi Wu, Zhenhua Wu, Da Xie, Yingtong Xiong, Yulong Xu, Sile Yang, Zikang Yang, Weicai Ye, Ziyang Yuan, Shenglong Zhang, Shuaiyu Zhang, Yuanxing Zhang, Yufan Zhang, Wenzheng Zhao, Ruiliang Zhou, Yan Zhou, Guosheng Zhu, Yongjie Zhu</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Kling-Omni Technical Report”的全面摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> Kling-Omni Technical Report</p>
<p><strong>作者：</strong> Kling Team, Kuaishou Technology (Jialu Chen, Yuanzheng Ci, Xiangyu Du, Zipeng Feng, Kun Gai, Sainan Guo, Feng Han, Jingbin He, Kang He, Xiao Hu, Xiaohua Hu, Boyuan Jiang, Fangyuan Kong, Hang Li, Jie Li, Qingyu Li, Shen Li, Xiaohan Li, Yan Li, Jiajun Liang, Borui Liao, Yiqiao Liao, Weihong Lin, Quande Liu, Xiaokun Liu, Yilun Liu, Yuliang Liu, Shun Lu, Hangyu Mao, Yunyao Mao, Haodong Ouyang, Wenyu Qin, Wanqi Shi, Xiaoyu Shi, Lianghao Su, Haozhi Sun, Peiqin Sun, Pengfei Wan, Chao Wang, Chenyu Wang, Meng Wang, Qiulin Wang, Runqi Wang, Xintao Wang, Xuebo Wang, Zekun Wang, Min Wei, Tiancheng Wen, Guohao Wu, Xiaoshi Wu, Zhenhua Wu, Da Xie, Yingtong Xiong, Yulong Xu, Sile Yang, Zikang Yang, Weicai Ye, Ziyang Yuan, Shenglong Zhang, Shuaiyu Zhang, Yuanxing Zhang, Yufan Zhang, Wenzheng Zhao, Ruiliang Zhou, Yan Zhou, Guosheng Zhu, Yongjie Zhu)</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
当前视频生成领域存在功能分离、任务碎片化的问题，例如视频生成、编辑和智能推理被割裂开来。现有的模型在处理多样化的多模态输入（文本、图像、视频）并生成高质量、智能化的视频内容方面存在挑战。特别是，自然语言提示难以捕捉视觉细节和复杂的用户意图，导致模型难以实现精细的控制和深度的理解。此外，现有模型在语义推理和理解场景的物理逻辑方面能力有限，更像是被动的生成器而非智能的代理。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
Kling-Omni 提出了一个<strong>通用生成框架</strong>，旨在解决上述问题，实现视频生成、编辑和智能推理任务的统一。其核心创新包括：</p>
<ul>
<li><strong>多模态视觉语言 (MVL) 作为新的交互范式：</strong> Kling-Omni 引入了 MVL 作为一种新的交互方式，将自然语言作为语义骨架，并结合多模态描述，构建统一的输入表示。这增强了模型对文本和视觉信号的理解与控制能力，使其能够更深入地理解和推断用户意图。</li>
<li><strong>端到端统一的框架：</strong> Kling-Omni 将视频生成、编辑和推理整合到一个<strong>整体系统</strong>中，打破了传统上分离的流水线方法。它能够直接从 MVL 输入合成高保真度的视频。</li>
<li><strong>Prompt Enhancer (PE) 模块：</strong> 该模块利用多模态大语言模型 (MLLM) 来理解复杂的用户输入，并将其映射到与模型训练数据分布一致的表示。PE 能够推断创作者的具体意图，并进行提示重构，从而提高生成质量，尤其是在身份保持、空间一致性和颜色保真度方面。</li>
<li><strong>多模态视觉语言 (MVL) 信号的整合：</strong> Kling-Omni 能够处理文本指令、参考图像和视频上下文等多种输入，并将它们统一处理，生成电影级质量的视频内容。</li>
<li><strong>多阶段训练策略：</strong> 框架采用了从指令预训练、监督微调到强化学习 (RL) 的多阶段训练策略，并结合了 DPO (Direct Preference Optimization) 来优化模型输出以符合人类审美偏好。</li>
<li><strong>高效的训练和推理优化：</strong> 论文详细介绍了其在数据处理、模型加速（如两阶段蒸馏）、训练优化（如多模态数据管道和负载均衡、微批次弹性 ulysses 并行）以及推理优化（如模型并行、张量并行、计算-通信重叠、混合量化和缓存机制）方面的技术细节，以支持大规模训练和高效推理。</li>
<li><strong>全面的数据系统：</strong> 构建了一个包含大规模真实世界数据采集和任务导向的合成数据构建的综合数据系统，并设计了三层数据处理流程（基础过滤、时间质量评估、视频-文本和图像-视频对齐）来确保数据质量。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
Kling-Omni 在各项评估中展现出卓越的能力，包括：</p>
<ul>
<li><strong>卓越的多模态指令遵循能力：</strong> 模型能够准确理解并执行复杂的文本、图像和视频组合指令。</li>
<li><strong>强大的推理和编辑能力：</strong> 在推理驱动的编辑（如添加、移除、替换、背景替换、风格化）和多模态参考（如图像/元素库参考、新视角生成、运动迁移）方面表现出色。</li>
<li><strong>与 SOTA 模型相比的优越性：</strong> 通过与 Veo 3.1 和 Runway-Aleph 等领先模型的对比评估，Kling-Omni 在动态质量、视觉质量、提示遵循和身份一致性等多个维度上均取得了显著的优势，尤其是在图像参考和视频编辑任务上。</li>
<li><strong>推动多模态世界模拟器的发展：</strong> Kling-Omni 被认为是迈向能够感知、推理、生成和与动态复杂世界交互的多模态世界模拟器的重要一步。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
论文中并未明确列出局限性，但从其对未来研究方向的展望中可以推测，一些高级功能（如“Features described in this section are not yet supported in the online version.”）可能尚未完全实现或集成到在线版本中。此外，虽然模型在推理方面取得了进展，但“更高级的交互式和推理增强型生成任务”仍有进一步探索的空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文展望了 Kling-Omni 在构建多模态世界模拟器方面的潜力，这暗示了未来的研究方向可能包括：</p>
<ul>
<li><strong>更强的感知、推理和交互能力：</strong> 进一步提升模型在理解和模拟复杂现实世界动态方面的能力。</li>
<li><strong>更精细的控制和创造力：</strong> 探索更广泛的用户交互方式，例如通过视觉信号（如绘图、标注）进行更精细的视频控制，以及增强模型的自主创造力。</li>
<li><strong>更广泛的应用场景：</strong> 将 Kling-Omni 应用于更广泛的领域，如电影制作、游戏开发、虚拟现实等。</li>
<li><strong>实时性和交互性：</strong> 进一步优化模型的推理速度，以支持更实时的交互式视频生成和编辑。</li>
<li><strong>伦理和社会影响：</strong> 随着模型能力的增强，对生成内容的真实性、偏见和滥用等伦理问题进行深入研究和应对。</li>
</ul>
<p><strong>总结：</strong>
Kling-Omni 技术报告介绍了一个开创性的通用视频生成框架，通过引入多模态视觉语言 (MVL) 范式，将视频生成、编辑和智能推理任务统一在一个端到端的系统中。该框架通过创新的 Prompt Enhancer 模块、多阶段训练策略以及高效的优化技术，实现了对复杂多模态输入的深刻理解和高保真度视频的生成。其在各项评估中展现出的优越性能，以及在推理和交互式编辑方面的强大能力，标志着其在迈向更智能、更通用的视频内容创作和多模态世界模拟器方面迈出了坚实的一步。这篇论文为未来视频生成和多模态人工智能的研究提供了重要的理论和技术基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16776v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16776v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16760v1'></a></p>
<h2 id="vision-language-action-models-for-autonomous-driving-past-present-and-future"><a href="https://arxiv.org/abs/2512.16760v1">Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</a></h2>
<p><strong>Authors:</strong> Tianshuai Hu, Xiaolu Liu, Song Wang, Yiyao Zhu, Ao Liang, Lingdong Kong, Guoyang Zhao, Zeying Gong, Jun Cen, Zhiyu Huang, Xiaoshuai Hao, Linfeng Li, Hang Song, Xiangtai Li, Jun Ma, Shaojie Shen, Jianke Zhu, Dacheng Tao, Ziwei Liu, Junwei Liang</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</p>
<p><strong>作者：</strong> Tianshuai Hu, Xiaolu Liu, Song Wang, Yiyao Zhu, Ao Liang, Lingdong Kong, Guoyang Zhao, Zeying Gong, Jun Cen, Zhiyu Huang, Xiaoshuai Hao, Linfeng Li, Hang Song, Xiangtai Li, Jun Ma, Shaojie Shen, Jianke Zhu, Dacheng Tao, Ziwei Liu, Junwei Liang</p>
<p><strong>摘要：</strong></p>
<p>这篇论文全面回顾了自动驾驶领域中从传统的“感知-决策-行动”（Perception-Decision-Action, PDA）模块化方法，到视觉-动作（Vision-Action, VA）模型，再到当前新兴的视觉-语言-动作（Vision-Language-Action, VLA）框架的演进历程。论文旨在为 VLA 模型在自动驾驶领域的快速发展提供一个结构化的视角，梳理其概念基础、架构趋势以及未来的研究方向。</p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>传统的 PDA 自动驾驶系统在复杂、长尾场景下表现不佳，其模块化设计易导致误差累积。VA 模型虽然能直接从视觉输入映射到动作，但缺乏可解释性、泛化能力弱且难以进行结构化推理。因此，研究如何构建更智能、可解释、泛化能力强且能理解人类指令的自动驾驶系统是核心问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>结构化梳理与分类：</strong> 论文首次对自动驾驶领域的 VLA 模型进行了系统性的梳理和分类。<ul>
<li><strong>VLA 架构分类：</strong> 将 VLA 模型分为两大范式：<ul>
<li><strong>端到端 VLA (End-to-End VLA)：</strong> 将感知、推理和规划集成在单个模型中。</li>
<li><strong>双系统 VLA (Dual-System VLA)：</strong> 将 VLM 的慢速推理与专用驾驶模块的快速执行分离。</li>
</ul>
</li>
<li><strong>子类划分：</strong> 在两大范式下，进一步区分了文本式与数值式动作生成器，以及显式与隐式引导机制。</li>
</ul>
</li>
<li><strong>演进历程追踪：</strong> 详细追溯了从早期 VA 模型到现代 VLA 框架的发展脉络，阐述了 VLA 模型出现的动机。</li>
<li><strong>数据集与基准总结：</strong> 系统性地总结了用于评估 VLA 驾驶系统的代表性数据集和基准，为模型评估提供了参考。</li>
<li><strong>挑战与未来方向展望：</strong> 深入分析了 VLA 模型在实际部署中面临的关键挑战，并提出了未来研究方向。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>VLA 模型的优势：</strong> VLA 模型通过整合视觉理解、语言推理和可执行动作，提供了一种更具可解释性、泛化性和人类对齐性的自动驾驶策略。它们能够处理更复杂的场景，并理解人类的高级指令。</li>
<li><strong>架构范式的重要性：</strong> 论文提出的端到端 VLA 和双系统 VLA 的分类，为理解不同 VLA 模型的设计理念和权衡提供了清晰的框架。</li>
<li><strong>推动领域发展：</strong> 通过对现有工作的全面梳理和分类，论文为该领域的研究人员提供了一个结构化的路线图，有助于加速 VLA 模型在自动驾驶领域的进步。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>实时处理与延迟：</strong> VLA 模型继承了大型视觉语言模型（VLM）的计算密集性，高分辨率、高帧率的输入会产生大量的视觉 token，多视图融合会加剧内存和延迟问题。实现自动驾驶所需的亚 50ms 推理仍然是一个挑战。</li>
<li><strong>领域特定基础模型缺失：</strong> 通用 VLM 虽然提供了强大的先验知识，但并未针对自动驾驶的特定感知、物理或多传感器融合进行优化。自动驾驶需要精确的空间推理、遵守交通规则以及理解罕见的高风险场景，这些能力通用模型尚未完全捕捉。</li>
<li><strong>数据成本高昂：</strong> VLA 模型依赖于多样化的高质量多模态数据集，但收集这些数据集成本高昂。合成环境可以提供帮助，但模拟到现实的差距（如噪声特性、光照和行为差异）仍然存在。</li>
<li><strong>可解释性与幻觉：</strong> VLA 模型生成的自然语言解释（通过 CoT 等方式）可能是人为产物，而非真实因果推理的忠实反映。语言幻觉（hallucination）风险增加，模型可能用看似合理的叙述来解释错误的决策。确保感知、动作和解释之间的一致性是一个开放性挑战。</li>
<li><strong>长时序连贯性：</strong> 当前基于 Transformer 的 VLA 模型受限于有限的上下文窗口和短时序条件，难以在长时序内保持情境感知和多阶段交互的连贯性，尤其是在多智能体或动态交通场景下，可能导致决策不一致。</li>
<li><strong>安全与可靠性：</strong> 尽管 VLA 模型有望提高可解释性，但其推理失败、指令遵循错误或跨模态不一致等风险仍需深入研究和评估。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>统一的视觉-语言-世界模型：</strong> 将 VLA 与预测性世界模型相结合，模拟未来场景演变，实现主动规划和更可靠的决策。</li>
<li><strong>更丰富的多模态融合：</strong> 整合更多传感器（如 LiDAR、雷达、事件相机、高精地图）的早期、紧密融合，并利用语言增强语义理解，同时确保 3D 几何的精确性。</li>
<li><strong>社会化和知识驱动的驾驶：</strong> 培养模型更深层次的常识推理能力，理解意图、惯例和因果关系，并利用外部知识库支持社会化和预见性的驾驶行为。</li>
<li><strong>持续学习与在线学习：</strong> 使模型能够适应不断变化的道路基础设施和区域驾驶习惯，实现安全、增量的日常驾驶学习，避免灾难性遗忘，并解决长尾泛化问题。</li>
<li><strong>标准化评估与安全保障：</strong> 开发更全面的基准，评估多步指令执行、模糊语言处理和抗幻觉能力，并探索形式化验证工具以提供安全保证。</li>
<li><strong>以人为本的交互与个性化：</strong> 实现更丰富的车内交互，允许用户指定驾驶偏好，并使模型能够适应不同用户的驾驶风格，同时平衡个性化与安全性。</li>
</ul>
<p>总而言之，这篇论文为理解和推进自动驾驶领域的 VLA 模型研究提供了一个全面且深入的视角，指出了当前的研究现状、关键挑战以及未来的发展方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity.</li>
<li>Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16760v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16760v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16924v1'></a></p>
<h2 id="the-world-is-your-canvas-painting-promptable-events-with-reference-images-trajectories-and-text"><a href="https://arxiv.org/abs/2512.16924v1">The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</a></h2>
<p><strong>Authors:</strong> Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text”的全面中文摘要：</p>
<p><strong>论文题目：</strong> The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</p>
<p><strong>作者：</strong> Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前的世界模型（world models）在生成可控、语义丰富的事件方面存在局限性。现有的“可提示事件”（promptable events）方法主要依赖文本提示，这在处理复杂的时空动态、多主体交互以及精确的对象身份控制时显得不足。文本本身难以完全捕捉事件的“何时”、“何地”、“谁”以及“什么”等关键信息，尤其是在需要精细控制对象运动、出现/消失以及外观时。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了 <strong>WorldCanvas</strong>，一个创新的框架，用于生成可提示的世界事件。其核心贡献在于引入了一种<strong>多模态提示范式</strong>，该范式结合了三种互补的模态：</p>
<ul>
<li><strong>轨迹（Trajectories）：</strong> 编码了事件的“何时”和“何地”，通过点序列定义了对象的运动路径、速度（点间距）和可见性（是否出现/消失）。</li>
<li><strong>参考图像（Reference Images）：</strong> 提供了“谁”的视觉基础，用于精确地指定对象的外观和身份，实现参考图像引导的生成。</li>
<li><strong>文本（Text）：</strong> 描述了事件的“什么”，提供了高层语义意图、交互和因果关系。</li>
</ul>
<p>WorldCanvas 的关键技术创新包括：
*   <strong>数据策展流水线：</strong> 构建了一个包含轨迹-视频-文本三元组的数据集，其中文本（动作描述）与轨迹紧密对齐，并提取了用于视觉基础的参考图像。
*   <strong>轨迹注入（Trajectory Injection）：</strong> 将轨迹信息通过高斯热力图和点 VAE 映射等方式注入到生成模型中，使模型能够遵循用户指定的运动路径。
*   <strong>空间感知加权交叉注意力（Spatial-Aware Weighted Cross-Attention）：</strong> 提出了一种新的注意力机制，用于在多主体场景中将文本描述与对应的轨迹进行精确对齐，解决了主体相似或动态出现时文本-轨迹匹配的挑战。
*   <strong>直观的用户界面：</strong> 设计了一个用户友好的接口，允许用户方便地输入轨迹、参考图像和文本，实现对事件的精细化控制。</p>
<p><strong>3. 主要结果与意义：</strong>
WorldCanvas 在生成可控、连贯且语义丰富的世界事件方面取得了显著成果。
*   <strong>生成能力：</strong> 能够生成包含多主体交互、对象进入/退出场景、参考图像引导的外观以及反直觉事件的视频。
*   <strong>一致性：</strong> 生成的视频不仅在时间上连贯，而且在对象身份和场景方面表现出涌现式的一致性，即使对象暂时消失后重新出现也能保持其身份和外观。
*   <strong>控制精度：</strong> 通过多模态提示，用户可以精确控制事件的“何时”、“何地”、“谁”和“什么”，实现了比纯文本方法更精细的控制。
*   <strong>意义：</strong> WorldCanvas 将世界模型从被动的预测者转变为<strong>交互式的、用户塑造的模拟器</strong>，为构建更高级、更具创造性的世界模型奠定了基础。其在物理合理性、因果推理和未来预测方面的能力也得到了初步验证。</p>
<p><strong>4. 提及的局限性：</strong>
论文中提到，尽管 WorldCanvas 在许多方面表现出色，但在处理<strong>极端复杂的空间变换或逻辑推理</strong>时，有时仍会失败。例如，在相机进行大幅度旋转时，可能会出现模糊和不一致；在涉及复杂逻辑推理（如相机移开后物体状态的持续变化）的场景中，模型可能无法完全捕捉预期的物理行为。</p>
<p><strong>5. 未来研究方向：</strong>
论文指出了未来研究的几个方向：
*   <strong>复杂运动下的持续性：</strong> 如何在剧烈和复杂的运动中保持场景和对象的一致性。
*   <strong>视线外内容的逻辑推理：</strong> 如何让模型在内容暂时不可见时，仍能进行正确的逻辑推理，并预测其后续状态。
*   <strong>更高级的世界模型：</strong> WorldCanvas 作为一种生成可控、语义丰富事件的框架，可以作为构建更先进、能够进行连贯、持久场景模拟的世界模型的基石。</p>
<p><strong>总体而言，</strong> WorldCanvas 是一项重要的研究成果，它通过创新的多模态提示范式，极大地提升了生成可控、语义丰富的世界事件的能力，为实现真正交互式的、用户驱动的世界模拟开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16924v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16924v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16922v1'></a></p>
<h2 id="next-embedding-prediction-makes-strong-vision-learners"><a href="https://arxiv.org/abs/2512.16922v1">Next-Embedding Prediction Makes Strong Vision Learners</a></h2>
<p><strong>Authors:</strong> Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Next-Embedding Prediction Makes Strong Vision Learners</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结):</strong></p>
<p>本研究提出了一种名为“Next-Embedding Predictive Autoregression (NEPA)”的新型自监督学习范式，将生成式预训练的成功经验迁移至计算机视觉领域。与传统的学习表示（representation learning）不同，NEPA直接训练模型生成嵌入（embeddings）以执行预测任务，具体而言，模型学习根据过去的图像块嵌入预测未来的图像块嵌入。这种方法在不依赖像素重建、离散化token、对比损失或特定任务头部的情况下，仅通过预测未来嵌入，就能够训练出强大的视觉学习器，并在ImageNet等基准测试中取得了优异的性能。</p>
<p><strong>2. 关键创新或方法论:</strong></p>
<ul>
<li><strong>核心创新：从学习表示到学习模型 (Learning Representations to Learning Models):</strong> 这是本研究最核心的理念转变。传统的自监督学习（如SimCLR, MoCo, DINO等）侧重于学习能够捕捉图像语义信息的“表示”，这些表示随后用于下游任务。而NEPA则直接将模型训练目标设定为“生成嵌入以执行预测任务”，即模型本身就是一个预测器，其输出的嵌入具有直接的可预测性。</li>
<li><strong>Next-Embedding Predictive Autoregression (NEPA):</strong><ul>
<li><strong>预测未来嵌入 (Predicting Future Embeddings):</strong> 模型被训练来预测序列中下一个图像块的嵌入，给定前面一系列图像块的嵌入。这类似于自然语言处理中的自回归模型（如GPT系列），但作用于视觉嵌入序列。</li>
<li><strong>因果掩码 (Causal Masking):</strong> 在预测时，模型只能访问过去的嵌入信息，而不能看到未来的信息，这确保了预测的“因果性”。</li>
<li><strong>停止梯度 (Stop Gradient):</strong> 摘要中提到“stop gradient”，这通常用于在训练过程中阻止梯度流向某些部分，以避免信息泄露或优化问题。在NEPA中，这可能意味着在计算损失时，目标嵌入（即未来的真实嵌入）的梯度不会反向传播到生成这些目标嵌入的模型部分，或者用于防止模型过度拟合于生成目标嵌入本身。</li>
<li><strong>仅使用嵌入进行预测:</strong> NEPA不依赖于像素级别的重建（如MAE），也不依赖于将图像块量化为离散token（如VQ-VAE, BEiT），更不使用对比学习的损失函数。这极大地简化了模型设计和训练过程。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响:</strong></p>
<ul>
<li><strong>简化自监督学习范式:</strong> NEPA提供了一种极其简洁的自监督学习框架，消除了许多现有方法中常见的复杂组件（如负样本对、大批量数据、多头结构等）。这使得模型更容易实现和扩展。</li>
<li><strong>提升模型泛化能力:</strong> 通过直接学习预测嵌入的能力，模型可能能够学习到更具鲁棒性和泛化性的视觉特征，从而在各种下游任务中表现出色。</li>
<li><strong>推动生成式预训练在视觉领域的应用:</strong> 成功将自然语言处理中生成式预训练的强大能力引入计算机视觉，为未来视觉模型的设计提供了新的思路和方向。</li>
<li><strong>探索新的模型架构和训练目标:</strong> NEPA的成功可能会激发研究人员探索更多基于生成式预测的自监督学习方法，以及更轻量级、更高效的视觉模型。</li>
<li><strong>潜在的模态无关性:</strong> 摘要中提到“potentially modality-agnostic”，暗示这种基于嵌入预测的生成式预训练方法可能不仅限于视觉，也可能适用于其他模态（如音频、文本），甚至多模态融合。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用:</strong></p>
<ul>
<li><strong>基础视觉模型预训练:</strong> NEPA可以作为构建强大基础视觉模型（Foundation Models）的新方法，这些模型可以为各种下游任务提供强大的起点。</li>
<li><strong>图像识别与分类:</strong> 如摘要所示，在ImageNet等数据集上微调后表现优异。</li>
<li><strong>语义分割、目标检测等密集预测任务:</strong> 摘要提到在ADE20K上的有效迁移，表明其在像素级理解任务上的潜力。</li>
<li><strong>视频理解:</strong> 将NEPA扩展到视频序列，预测未来帧的嵌入，可能对视频预测、动作识别等任务有益。</li>
<li><strong>3D视觉:</strong> 预测点云或体素的嵌入，可能有助于3D场景理解和生成。</li>
<li><strong>多模态学习:</strong> 如果其模态无关性得到证实，将极大地促进跨模态理解和生成。</li>
<li><strong>低资源场景下的模型训练:</strong> 简洁的训练范式和对复杂组件的依赖减少，可能使其在数据量有限的情况下更具优势。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性:</strong></p>
<ul>
<li><strong>“未来”的定义和粒度:</strong> 摘要中提到“future patch embeddings”。“未来”具体指多远的未来？“patch embeddings”的粒度如何？这些细节会影响模型的学习内容和性能。例如，预测非常近的未来嵌入可能更侧重于局部纹理，而预测更远的未来嵌入则可能需要更强的全局语义理解。</li>
<li><strong>“停止梯度”的具体实现和影响:</strong> 摘要中仅提及“stop gradient”，但其具体实现方式（例如，是应用于目标嵌入还是其他部分）以及它对训练动态和最终性能的影响，需要进一步的实验验证。</li>
<li><strong>计算效率和内存需求:</strong> 虽然摘要强调了“scalability”，但对于非常大的模型和高分辨率图像，预测大量嵌入的计算量和内存需求可能仍然是一个挑战。</li>
<li><strong>对特定任务的适应性:</strong> 尽管摘要声称迁移效果好，但对于某些与“预测未来嵌入”目标差异较大的下游任务，其适应性仍需进一步评估。例如，一些需要精细局部特征的任务，可能需要额外的微调策略。</li>
<li><strong>与现有方法的直接比较:</strong> 摘要提供了在ImageNet上的性能数据，但并未直接与当前最先进的自监督学习方法（如MAE, DINOv2等）进行详细的横向比较，例如在训练时间和计算资源消耗上的对比。</li>
<li><strong>理论解释的深度:</strong> 摘要主要侧重于方法和实验结果，对于“为什么”这种方法能够如此有效，其背后的理论解释可能还需要更深入的研究。</li>
</ul>
<p>总而言之，这篇论文提出的NEPA方法是一个非常令人兴奋的进展，它通过一种新颖的生成式预测范式，极大地简化了自监督视觉学习的流程，并取得了令人印象深刻的性能。其潜在的模态无关性和普适性，预示着它可能成为未来视觉模型预训练的重要方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16922v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16922v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16920v1'></a></p>
<h2 id="easyv2v-a-high-quality-instruction-based-video-editing-framework"><a href="https://arxiv.org/abs/2512.16920v1">EasyV2V: A High-quality Instruction-based Video Editing Framework</a></h2>
<p><strong>Authors:</strong> Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：EasyV2V: A High-quality Instruction-based Video Editing Framework</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话总结)</strong></p>
<p>EasyV2V 提出了一种新颖且高效的框架，用于实现基于文本指令的视频编辑。该框架通过创新的数据构建策略、简化的模型设计以及统一的时空控制机制，显著提升了视频编辑的质量、一致性和可控性，并在多项评估中超越了现有技术和商业系统。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>EasyV2V 的核心创新在于其对视频编辑设计空间的系统性研究和整合，具体体现在以下几个方面：</p>
<ul>
<li>
<p><strong>数据构建策略 (Data Design):</strong></p>
<ul>
<li><strong>利用现有专家模型及其快速逆向操作 (Composing existing experts with fast inverses):</strong> 这是一种巧妙的数据增强方法，通过组合已有的图像编辑模型（如风格迁移、对象替换等）及其逆向操作，生成大量高质量的视频编辑对。这解决了直接获取大规模、多样化的视频编辑数据困难的问题。</li>
<li><strong>单帧监督和伪对 (Single-frame supervision and pseudo pairs):</strong> 将图像编辑对提升到视频编辑，通过单帧的监督信息，并利用共享的仿射运动来生成伪视频对，这是一种高效利用现有图像编辑能力的方法。</li>
<li><strong>挖掘带密集字幕的视频片段 (Mining dense-captioned clips):</strong> 从现有数据集中寻找带有详细描述的视频片段，用于构建视频编辑对，增加了数据的丰富性和指令的准确性。</li>
<li><strong>添加过渡监督 (Transition supervision):</strong> 专门训练模型理解编辑是如何在时间上展开的，这对于生成平滑自然的视频编辑至关重要，解决了视频编辑中常见的突兀感问题。</li>
</ul>
</li>
<li>
<p><strong>模型设计 (Architecture Design):</strong></p>
<ul>
<li><strong>利用预训练文本到视频模型的编辑能力 (Pretrained text-to-video models possess editing capability):</strong> 作者发现，现有的文本到视频生成模型本身就蕴含了强大的编辑潜力，无需从头设计复杂的模型。</li>
<li><strong>简化的条件输入和轻量级微调 (Simple sequence concatenation for conditioning with light LoRA fine-tuning):</strong> 采用简单的序列拼接方式来整合文本指令，并利用 LoRA (Low-Rank Adaptation) 等轻量级微调技术，即可训练出高性能的模型。这大大降低了训练成本和模型复杂度，使得模型更易于部署和使用。</li>
</ul>
</li>
<li>
<p><strong>控制机制 (Control Mechanism):</strong></p>
<ul>
<li><strong>统一的时空掩码机制 (Unify spatiotemporal control via a single mask mechanism):</strong> 提出了一种灵活的掩码机制，能够同时控制编辑的空间区域和时间范围，实现了精细化的编辑控制。</li>
<li><strong>支持可选的参考图像 (Optional reference images):</strong> 允许用户提供参考图像，以指导编辑的风格或内容，进一步增强了编辑的灵活性和用户的主动性。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>EasyV2V 的研究对视频编辑领域具有重要的潜在影响：</p>
<ul>
<li><strong>降低视频编辑的门槛:</strong> 通过简化的模型和灵活的输入方式，使得非专业用户也能更容易地进行高质量的视频编辑，推动了视频创作的民主化。</li>
<li><strong>推动指令驱动的视频内容生成:</strong> 这种基于文本指令的编辑方式是未来视频内容生成的重要方向，EasyV2V 的成功将激励更多研究者探索更强大的指令理解和执行能力。</li>
<li><strong>促进视频编辑技术的商业化应用:</strong> 其在性能上的突破和易用性将加速相关技术在短视频平台、内容创作工具、电影后期制作等领域的商业化落地。</li>
<li><strong>为未来视频生成模型的设计提供新思路:</strong> 其对预训练模型编辑能力的挖掘以及轻量级微调的有效性，为后续视频生成模型的研发提供了宝贵的经验和方向。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作与社交媒体:</strong> 极大地赋能短视频创作者，使其能够快速、高效地对视频内容进行个性化编辑和风格化处理。</li>
<li><strong>电影与电视后期制作:</strong> 为专业后期制作人员提供更强大、更灵活的工具，加速特效制作和画面调整流程。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR) 内容生成:</strong> 能够更便捷地编辑和生成沉浸式视频内容。</li>
<li><strong>教育与培训:</strong> 制作更具吸引力和互动性的教学视频。</li>
<li><strong>个性化视频广告:</strong> 快速生成符合特定用户需求的广告视频。</li>
<li><strong>数字人与虚拟形象:</strong> 为虚拟形象的动画和表情编辑提供更精细的控制。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要中强调了 EasyV2V 的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对数据质量的依赖:</strong> 尽管作者提出了多种数据构建策略，但最终编辑效果仍可能受到原始视频质量、指令清晰度以及生成数据质量的影响。</li>
<li><strong>复杂场景下的挑战:</strong> 对于包含大量动态物体、复杂光照变化或快速运动的场景，模型的编辑效果和一致性可能仍会面临挑战。</li>
<li><strong>指令的理解深度:</strong> 虽然支持文本指令，但对于非常抽象、模糊或需要高度语义理解的编辑指令，模型可能仍难以完美执行。</li>
<li><strong>计算资源需求:</strong> 尽管模型设计简化，但视频处理本身通常需要较高的计算资源，尤其是在处理高分辨率或长视频时。</li>
<li><strong>对特定编辑类型的泛化能力:</strong> 摘要提到“state-of-the-art video editing results”，但具体在哪些类型的编辑任务上表现优异，以及对全新、未见过编辑类型的泛化能力，需要进一步的实验验证。例如，对于需要深度语义理解的“让视频中的猫变成狗”这类任务，其效果如何仍是未知数。</li>
<li><strong>“Fast inverses” 的局限性:</strong> 尽管利用了“fast inverses”，但这些逆向操作的质量和效率可能并非完美，可能会引入伪影或限制编辑的自由度。</li>
</ul>
<p>总而言之，EasyV2V 是一项令人兴奋的研究，它通过系统性的方法解决了视频编辑中的关键挑战，并有望对该领域产生深远影响。其创新的数据构建和简化的模型设计是其成功的关键。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16920v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16920v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16919v1'></a></p>
<h2 id="dvgt-driving-visual-geometry-transformer"><a href="https://arxiv.org/abs/2512.16919v1">DVGT: Driving Visual Geometry Transformer</a></h2>
<p><strong>Authors:</strong> Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“DVGT: Driving Visual Geometry Transformer”论文的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> DVGT: Driving Visual Geometry Transformer</p>
<p><strong>作者：</strong> Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>自动驾驶的核心任务之一是准确感知和重建三维场景几何。然而，现有方法在以下方面存在不足：
*   <strong>缺乏针对自动驾驶的通用模型：</strong> 大多数模型难以适应不同的驾驶场景和多变的相机配置。
*   <strong>对精确相机参数的依赖：</strong> 传统方法通常需要精确的相机内外参，这限制了其灵活性和可扩展性。
*   <strong>几何信息不完整或精度不足：</strong> 一些方法只能预测2.5D几何或存在量化误差，难以实现精细的场景理解。
*   <strong>缺乏端到端的度量尺度几何预测：</strong> 许多模型需要额外的后处理（如与LiDAR对齐）才能获得度量尺度的几何信息。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>本文提出了<strong>Driving Visual Geometry Transformer (DVGT)</strong>，一个专为自动驾驶设计的、端到端的视觉几何Transformer模型，旨在解决上述挑战。其核心创新点包括：</p>
<ul>
<li><strong>全局密集3D点图重建：</strong> DVGT能够从一系列无序的多视图图像中直接预测一个全局、密集且度量尺度的3D点图，提供连续且高保真的场景几何表示。</li>
<li><strong>3D先验自由（Prior-Free）设计：</strong> 模型完全摆脱了对显式3D几何先验（如精确相机参数）的依赖，通过纯粹的数据驱动方式学习几何信息，使其能够灵活适应各种相机配置。</li>
<li><strong>高效的空间-时间注意力机制：</strong> DVGT采用了一种分解式的注意力机制，包括<strong>视图内局部注意力（Intra-View Local Attention）</strong>、<strong>跨视图空间注意力（Cross-View Spatial Attention）</strong>和<strong>跨帧时间注意力（Cross-Frame Temporal Attention）</strong>。这种设计在保持有效几何信息融合的同时，显著提高了计算效率和推理速度，克服了传统全局注意力机制的计算瓶颈。</li>
<li><strong>统一的Ego-centric坐标系：</strong> 模型将3D点图预测在<strong>参考帧的Ego-centric坐标系</strong>下，并将预测的位姿也转换为相对于参考帧的Ego位姿。这种设计使得几何表示对相机焦距、位姿和视图数量具有不变性，是实现通用自动驾驶感知模型的关键。</li>
<li><strong>多任务联合预测：</strong> DVGT不仅预测3D点图，还<strong>联合预测每帧的Ego位姿</strong>，使其成为一个全面的视觉几何模型。</li>
<li><strong>大规模、多样化的训练数据集构建：</strong> 为了训练一个具有强大泛化能力的模型，作者构建了一个包含nuScenes、OpenScene、Waymo、KITTI和DDAD等多个公开数据集的混合数据集，并开发了一个鲁棒的流程来生成高质量的密集3D点图伪标签。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>卓越的3D几何重建性能：</strong> DVGT在多个公开数据集上，包括KITTI、nuScenes、Waymo和OpenScene，均显著优于现有的通用视觉几何模型和专门的驾驶几何模型。在度量尺度3D点图重建和射线深度估计方面，DVGT取得了最先进的性能。</li>
<li><strong>准确的Ego位姿估计：</strong> DVGT在Ego位姿估计任务上也表现出色，在OpenScene和DDAD数据集上取得了领先结果，并在nuScenes和Waymo上与现有模型相当，证明了其作为综合视觉几何模型的有效性。</li>
<li><strong>强大的泛化能力：</strong> 由于其3D先验自由设计和在多样化数据集上的训练，DVGT展现出对不同相机配置和驾驶场景的强大适应性，克服了传统方法的局限性。</li>
<li><strong>端到端、无需后处理：</strong> DVGT能够直接从图像序列预测度量尺度的几何信息，无需依赖外部传感器（如LiDAR）进行后对齐，大大简化了部署流程。</li>
<li><strong>高效性：</strong> 分解式注意力机制使得模型在保持高性能的同时，实现了更快的推理速度。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>Waymo数据集性能相对较低：</strong> 作者提到在Waymo数据集上的性能不如其他数据集，这归因于训练时对该数据集的采样权重设置，可能未能充分体现其数据量和多样性。</li>
<li><strong>KITTI数据集位姿估计略低：</strong> 在KITTI数据集上，DVGT的位姿估计性能略低于其他数据集，作者认为这可能与KITTI数据集的双目相机设置限制了3D和Ego运动的理解有关。</li>
<li><strong>尺度缩放的挑战：</strong> 在处理远距离场景时，直接回归大数值的3D坐标可能导致训练不稳定。虽然通过线性缩放（10x）解决了这个问题，但过大的线性缩放（100x）或非线性缩放（arcsinh）可能导致精度下降或几何结构失真。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>优化数据集采样策略：</strong> 针对Waymo等数据量大但可能存在采样不均的数据集，进一步优化训练时的采样权重，以提升在这些数据集上的性能。</li>
<li><strong>探索更精细的尺度处理：</strong> 研究更鲁棒的尺度处理方法，以应对自动驾驶场景中极端动态范围的几何信息，同时避免精度损失。</li>
<li><strong>进一步提升Ego位姿估计精度：</strong> 针对特定场景（如KITTI）或更具挑战性的位姿估计任务，探索更先进的位姿估计模块或训练策略。</li>
<li><strong>扩展到更广泛的下游任务：</strong> 将DVGT预测的密集、度量尺度的3D几何信息应用于更广泛的自动驾驶下游任务，如路径规划、目标检测和跟踪等，并评估其带来的提升。</li>
<li><strong>实时性优化：</strong> 尽管DVGT已经实现了高效性，但对于对实时性要求极高的自动驾驶系统，可以进一步探索模型压缩、量化或更高效的网络结构来提升推理速度。</li>
</ul>
<p><strong>总结：</strong></p>
<p>DVGT通过引入创新的空间-时间注意力机制和3D先验自由设计，成功构建了一个能够从无序多视图图像中端到端预测度量尺度全局3D点图和Ego位姿的Transformer模型。该模型在自动驾驶场景下展现出卓越的几何重建和位姿估计能力，并克服了传统方法对相机参数的依赖，为构建更鲁棒、更通用的自动驾驶感知系统奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs.</li>
<li>Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16919v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16919v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16921v1'></a></p>
<h2 id="differences-that-matter-auditing-models-for-capability-gap-discovery-and-rectification"><a href="https://arxiv.org/abs/2512.16921v1">Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</a></h2>
<p><strong>Authors:</strong> Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification
<strong>作者：</strong> Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>AuditDM</strong> 的自动化框架，旨在解决当前多模态大语言模型（MLLMs）评估方法在解释性不足和难以充分揭示模型能力差距方面的问题。AuditDM 通过“审计”模型间的差异来主动发现和纠正 MLLMs 的失效模式。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<ul>
<li><strong>现有评估方法的局限性：</strong> 传统的 MLLMs 评估方法（如基准测试）通常是封闭集、固定知识范围的，难以揭示模型在长尾、细微能力上的差异，并且缺乏解释性，使得理解模型为何失败变得困难。</li>
<li><strong>模型能力差距的诊断与修复：</strong> 如何系统性地发现 MLLMs 的细微能力差距和失效模式，并利用这些发现来改进模型，是实际应用中的关键挑战。</li>
</ul>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>模型审计（Model Auditing）范式：</strong> 论文引入了“模型审计”这一新范式，旨在系统性地发现模型能力差距并诊断其弱点。</li>
<li><strong>AuditDM 框架：</strong><ul>
<li><strong>审计模型（Auditor Model）：</strong> 论文提出训练一个 MLLM 作为“审计模型”，通过强化学习（GRPO）进行微调。</li>
<li><strong>生成挑战性样本：</strong> 该审计模型能够生成具有挑战性的问题-图像对（包括反事实图像），这些样本旨在最大化目标模型与参考模型（或模型集合）之间的响应差异，从而暴露目标模型的弱点。</li>
<li><strong>无标注数据生成：</strong> AuditDM 生成的失效示例是无标注的，可以直接用于模型的纠正和改进。</li>
<li><strong>解释性与诊断性：</strong> AuditDM 生成的失效模式示例具有多样性和可解释性，能够提供对模型弱点的深入理解。</li>
</ul>
</li>
<li><strong>两种纠正策略：</strong><ul>
<li><strong>增强标注数据：</strong> 将审计生成的样本添加到原始训练数据中。</li>
<li><strong>自举无标注数据：</strong> 利用审计模型生成伪标签数据，进行迭代式训练和改进。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>高效的失效模式发现：</strong> AuditDM 在发现模型弱点方面比基线方法（仅依赖提示工程）效率高得多，成功率显著提升。</li>
<li><strong>揭示细微能力差距：</strong> 在 PaliGemma2 模型上，AuditDM 发现了超过 20 种不同的失效类型，并揭示了 28B 模型在某些任务上（如幻觉规避、计数、颜色识别）反而不如 3B 模型。</li>
<li><strong>显著的模型性能提升：</strong><ul>
<li>通过 AuditDM 生成的数据进行微调，在 16 个基准测试上，所有模型都获得了持续的性能提升。</li>
<li>一个 3B 模型在经过 AuditDM 改进后，甚至超越了其 28B 的对应模型。</li>
<li>在 Gemma3-4B 模型上，AuditDM 在多个基准测试上带来了显著的性能提升，缩小了与更大模型的差距，并在某些任务上超越了 12B 模型。</li>
</ul>
</li>
<li><strong>意义：</strong> 论文表明，随着数据规模化效应递减，有针对性的模型审计是诊断和改进 MLLMs 的有效途径，为模型持续学习和改进提供了新的方向。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>图像生成限制：</strong> AuditDM 在生成用于接地（grounding）和分割（segmentation）任务的探针问题时，需要具有密集标注的图像。此外，对于文本/图表导向的 OCR 任务，合成具有密集文本和复杂图表的图像存在困难。</li>
<li><strong>计算复杂度：</strong> 整个 AuditDM 流程（包括审计模型训练、数据生成和目标模型微调）需要大量的计算资源，例如生成 Gemma3-4B 的数据集需要数天时间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>改进图像生成：</strong> 通过自举伪标签和使用更强的视觉标注器，以及开发文本/图表专用生成器来克服图像生成方面的限制。</li>
<li><strong>优化计算效率：</strong> 探索更高效的数据生成和训练策略，以降低计算成本。</li>
<li><strong>更广泛的应用：</strong> 将 AuditDM 应用于更多类型的多模态模型和任务，探索其在不同场景下的有效性。</li>
<li><strong>更强的审计模型：</strong> 研究如何构建更强大的审计模型，以发现更复杂、更隐蔽的失效模式。</li>
</ul>
<p><strong>总结：</strong></p>
<p>AuditDM 框架通过引入“模型审计”这一创新范式，利用强化学习训练的 MLLM 作为审计者，能够高效、系统地发现和解释 MLLMs 的能力差距和失效模式。其生成的无标注、针对性训练数据能够显著提升模型的性能，并为模型提供更可靠的评估。这项工作为理解和改进日益复杂的多模态大模型提供了一条有前景的路径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16921v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16921v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16918v1'></a></p>
<h2 id="adatooler-v-adaptive-tool-use-for-images-and-videos"><a href="https://arxiv.org/abs/2512.16918v1">AdaTooler-V: Adaptive Tool-Use for Images and Videos</a></h2>
<p><strong>Authors:</strong> Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AdaTooler-V: Adaptive Tool-Use for Images and Videos”的全面中文摘要：</p>
<p><strong>论文题目：</strong> AdaTooler-V: Adaptive Tool-Use for Images and Videos</p>
<p><strong>作者：</strong> Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文主要解决了当前多模态大语言模型（MLLMs）在处理图像和视频任务时，存在“盲目使用工具”的问题。现有模型倾向于不加区分地调用视觉工具，即使在问题本身不需要工具的情况下，这不仅增加了推理开销，还可能降低模型性能。核心研究问题是如何让 MLLMs 能够自适应地决定何时真正需要使用视觉工具，从而在保证性能的同时优化推理效率。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>AdaTooler-V 模型：</strong> 提出了一种名为 AdaTooler-V 的 MLLM，其核心能力是实现自适应的工具使用。模型能够判断视觉问题是否真正需要工具，并据此选择文本链式思考（CoT）或多模态交错 CoT（包含视觉工具）。
*   <strong>AT-GRPO 算法：</strong> 引入了一种名为 AT-GRPO（Adaptive Tool-use GRPO）的强化学习算法。该算法通过计算每个样本的“工具效益得分”（Tool Benefit Score, AS），动态调整奖励函数。只有当工具使用能带来实际性能提升时，模型才会被奖励，否则会受到惩罚，从而鼓励模型仅在必要时使用工具。
*   <strong>新数据集：</strong> 构建了两个大规模数据集：AdaTooler-V-CoT-100k 用于监督微调（SFT）的冷启动，以及 AdaTooler-V-300k 用于强化学习（RL）训练。这些数据集涵盖了单图像、多图像和视频等多种模态，以及数学、逻辑推理、空间理解等多样化的视觉推理任务。
*   <strong>两阶段训练框架：</strong> 采用 SFT 和 RLVR（Reinforcement Learning with Verifiable Rewards）的两阶段训练策略。SFT 阶段通过多轮工具交互轨迹建立丰富的推理模式和行为先验，RLVR 阶段则利用 AT-GRPO 算法进一步优化模型的推理策略，使其能够自主学习更有效的工具使用方式。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在十二个基准测试上的实验表明，AdaTooler-V 在各种视觉推理任务中展现出强大的推理能力，显著优于现有方法。
*   <strong>SOTA 表现：</strong> AdaTooler-V-7B 模型在 V* 高分辨率基准测试上取得了 89.8% 的准确率，超越了商业模型 GPT-4o 和 Gemini 1.5 Pro。
*   <strong>效率优化：</strong> 通过自适应工具使用，模型能够减少不必要的工具调用，从而降低推理开销，同时保持甚至提升性能。
*   <strong>跨领域泛化：</strong> AdaTooler-V 在 MME、MathVista、InfoVQA 等通用推理基准上也表现出色，显示出良好的跨领域泛化能力。
*   <strong>对视觉理解的重要性：</strong> 实验结果（如 Tab. 5）验证了视觉工具使用对于准确的多模态理解至关重要，尤其是在需要精细视觉细节或空间对应关系的任务中。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>工具效益评估的单一参考模型：</strong> 当前的工具效益（AS）评估依赖于单一参考模型，这可能导致对工具是否真正有益的评估存在偏差。
*   <strong>奖励设计侧重于可验证任务：</strong> 奖励设计主要针对多项选择和数值问答等可验证任务，对于开放式生成任务的适应性较差。
*   <strong>数据集来源：</strong> AdaTooler-V-300k 数据集主要来自公开基准，对真实世界中的长尾案例、噪声条件和跨领域场景的覆盖有限。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更鲁棒的工具效益评估：</strong> 开发学习型效益估计器或利用模型集成来获得更准确的 AS 预测。
*   <strong>支持开放式生成任务：</strong> 引入学习型奖励模型、多模态判别器或对比学习信号，以更好地支持开放式生成任务。
*   <strong>扩展数据集：</strong> 增加真实世界样本、合成困难案例或采用领域自适应技术，以增强模型的泛化能力。</p>
<p><strong>总结：</strong>
AdaTooler-V 论文提出了一种创新的 MLLM，通过引入自适应工具使用机制，解决了现有模型盲目调用视觉工具的问题。其核心贡献在于 AT-GRPO 算法和配套的数据集，使得模型能够智能地决定何时使用工具，从而在提高推理效率的同时，在多模态视觉推理任务上取得了显著的性能提升，甚至超越了顶尖的商业模型。该研究为开发更高效、更智能的多模态模型提供了重要方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools.</li>
<li>First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16918v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16918v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.16913v1'></a></p>
<h2 id="depth-any-panoramas-a-foundation-model-for-panoramic-depth-estimation"><a href="https://arxiv.org/abs/2512.16913v1">Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</a></h2>
<p><strong>Authors:</strong> Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi</p>
<p><strong>Published:</strong> 2025-12-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP_website/}</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation (全景深度任意化：一个全景深度估计的基础模型)</p>
<p><strong>作者：</strong> Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>DAP (Depth Any Panoramas)</strong> 的全景深度估计基础模型，旨在实现跨越不同场景距离的泛化能力。研究的核心在于解决全景深度估计领域中数据规模不足、领域鸿沟（室内/室外、合成/真实）以及模型泛化能力有限等关键挑战。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<ul>
<li><strong>全景深度估计的泛化性不足：</strong> 现有的全景深度估计方法，无论是基于相对/尺度不变性还是统一的度量深度方法，在泛化到多样化的真实世界场景（尤其是室外）时都存在困难。</li>
<li><strong>数据规模和质量的限制：</strong> 收集和标注大规模、高质量的全景深度数据成本高昂，是制约模型性能的关键因素。</li>
<li><strong>领域鸿沟：</strong> 合成数据与真实数据、室内场景与室外场景之间存在显著的领域差异，影响模型的鲁棒性。</li>
</ul>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>大规模数据引擎：</strong> 论文构建了一个包含超过200万个全景图像的大规模数据集，整合了公共数据集（如Structured3D）、高质量的UE5模拟器生成数据（DAP-2M-Labeled）以及从网络收集的真实全景图像（DAP-2M-Unlabeled）。</li>
<li><strong>三阶段伪标签精炼流水线：</strong> 为了有效利用海量无标签数据并弥合领域鸿沟，论文设计了一个创新的三阶段伪标签生成和精炼流程：<ul>
<li><strong>阶段1：场景不变性标注器训练 (Scene-Invariant Labeler Training)：</strong> 在高质量的合成数据上训练一个标注器，使其能够跨越室内外场景学习到物理上一致的深度线索，为后续的伪标签生成提供良好初始化。</li>
<li><strong>阶段2：真实性不变性标注器训练 (Realism-Invariant Labeler Training)：</strong> 利用一个PatchGAN判别器来评估深度预测的质量，并选择高置信度的伪标签样本。然后，在包含合成数据和精炼后的伪标签数据的扩展数据集上训练一个真实性不变性标注器，使其能够适应真实世界的视觉变化。</li>
<li><strong>阶段3：DAP模型训练：</strong> 在所有标记数据和精炼后的伪标签数据上进行最终的DAP模型训练，实现大规模半监督学习。</li>
</ul>
</li>
<li><strong>模型设计：</strong><ul>
<li><strong>DINOv3-Large骨干网络：</strong> 利用强大的预训练视觉模型DINOv3-Large作为特征提取器，以获得优越的泛化能力。</li>
<li><strong>即插即用范围掩码头 (Plug-and-play Range Mask Head)：</strong> 引入一个能够根据不同距离阈值（10m, 20m, 50m, 100m）生成有效深度区域掩码的模块，以适应不同尺度的场景，并提高预测的鲁棒性和稳定性。</li>
<li><strong>多损失优化：</strong> 结合多种损失函数来提升模型性能，包括：<ul>
<li><strong>尺度不变性损失 (LSILog)：</strong> 标准的尺度不变性损失。</li>
<li><strong>密集保真度损失 (LDF)：</strong> 通过将深度图分解为12个透视视图来增强局部细节和结构一致性。</li>
<li><strong>梯度锐化损失 (Lgrad)：</strong> 聚焦于边缘区域，以保留物体边界的清晰度。</li>
<li><strong>法线损失 (Lnormal)：</strong> 增强几何一致性。</li>
<li><strong>点云损失 (Lpts)：</strong> 进一步保证几何一致性。</li>
<li><strong>掩码损失 (Lmask)：</strong> 用于训练范围掩码头。</li>
</ul>
</li>
<li><strong>失真图 (Distortion Map)：</strong> 用于补偿全景图像（ERP）投影中的像素几何畸变，确保梯度贡献在整个球形域内均衡。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>卓越的零样本泛化能力：</strong> 在Stanford2D3D、Matterport3D和Deep360等多个室内外基准测试中，DAP模型在零样本设置下（无需在测试集上进行微调）取得了最先进的性能，显著优于现有方法。</li>
<li><strong>鲁棒且度量一致的深度预测：</strong> DAP模型能够生成锐利的对象边界、平滑的全局几何，并在远距离和天空区域表现出优越的鲁棒性，尤其是在复杂的真实世界场景中。</li>
<li><strong>强大的尺度感知能力：</strong> 模型能够准确地预测场景的绝对尺度，并且在不同尺度下保持一致性。</li>
<li><strong>有效弥合领域鸿沟：</strong> 三阶段伪标签精炼流水线成功地减少了合成与真实、室内与室外数据之间的领域差异。</li>
<li><strong>基础模型潜力：</strong> DAP模型作为一个基础模型，为全景深度估计领域提供了一个统一且强大的框架，为未来的研究奠定了基础。</li>
</ul>
<p><strong>4. 局限性：</strong></p>
<ul>
<li>论文中未明确提及明显的局限性，但可以推测，尽管模型在多种场景下表现出色，但在极端复杂或非常规的场景下，其性能仍可能受到影响。</li>
<li>虽然模型在零样本设置下表现优异，但对于特定领域的数据，通过微调可能仍能进一步提升性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的场景覆盖：</strong> 探索更多样化的真实世界场景，包括极端天气、低光照、动态场景等，以进一步提升模型的鲁棒性。</li>
<li><strong>实时性提升：</strong> 针对需要实时应用的场景，研究如何优化模型结构和推理速度。</li>
<li><strong>多模态融合：</strong> 结合其他传感器信息（如RGB-D、LiDAR）或更丰富的语义信息，以进一步提高深度估计的准确性和鲁棒性。</li>
<li><strong>可解释性研究：</strong> 深入分析模型在处理不同场景和几何结构时的决策过程，以增强其可解释性。</li>
<li><strong>动态场景下的全景深度估计：</strong> 扩展到处理包含运动物体的动态全景场景。</li>
</ul>
<p>总而言之，这篇论文通过构建大规模数据集和创新的伪标签精炼策略，并结合先进的模型设计，成功地开发了一个强大的全景深度估计基础模型DAP。该模型在泛化能力、度量准确性和鲁棒性方面取得了显著的突破，为全景深度估计领域的研究和应用开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances.</li>
<li>To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.16913v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.16913v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-19 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
