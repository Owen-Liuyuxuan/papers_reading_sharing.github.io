<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-29 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-26/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-30/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-29">Arxiv Computer Vision Papers - 2025-09-29</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-26" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-26)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#category-discovery-an-open-world-perspective" class="nav-link">Category Discovery: An Open-World Perspective</a>
                </li>
                <li class="nav-item">
                    <a href="#pixel-motion-diffusion-is-what-we-need-for-robot-control" class="nav-link">Pixel Motion Diffusion is What We Need for Robot Control</a>
                </li>
                <li class="nav-item">
                    <a href="#see-point-fly-a-learning-free-vlm-framework-for-universal-unmanned-aerial-navigation" class="nav-link">See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</a>
                </li>
                <li class="nav-item">
                    <a href="#caprl-stimulating-dense-image-caption-capabilities-via-reinforcement-learning" class="nav-link">CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#wow-towards-a-world-omniscient-world-model-through-embodied-interaction" class="nav-link">WoW: Towards a World omniscient World model Through Embodied Interaction</a>
                </li>
                <li class="nav-item">
                    <a href="#labeling-copilot-a-deep-research-agent-for-automated-data-curation-in-computer-vision" class="nav-link">LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#ccnext-an-effective-self-supervised-stereo-depth-estimation-approach" class="nav-link">CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach</a>
                </li>
                <li class="nav-item">
                    <a href="#where-mllms-attend-and-what-they-rely-on-explaining-autoregressive-token-generation" class="nav-link">Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#-quant-towards-learnable-quantization-for-low-bit-pattern-recognition" class="nav-link">Î³-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition</a>
                </li>
                <li class="nav-item">
                    <a href="#nifty-a-non-local-image-flow-matching-for-texture-synthesis" class="nav-link">NIFTY: a Non-Local Image Flow Matching for Texture Synthesis</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-29">Arxiv Computer Vision Papers - 2025-09-29</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-26">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-26)</h2>
<p><strong>æ¦è¿°ä¸è¶å¿ï¼</strong></p>
<p>ä»æ¥ Arxiv è®¡ç®æºè§è§è®ºæåç°åºå¤æ¨¡æå­¦ä¹ ãå·èº«æºè½ä¸æºå¨äººæ§å¶ãä»¥åæ°æ®æçä¸æ¨¡åä¼åç­å¤ä¸ªäº¤åé¢åçç­ç¹ãå·èº«æºè½åæºå¨äººå¯¼èªæ¯æ¾èçè¶å¿ï¼å¤ç¯è®ºææ¢ç´¢äºå¦ä½å©ç¨è§è§è¯­è¨æ¨¡å (VLM) æåç´ çº§è¿å¨æ©æ£æ¨¡åå®ç°æ´æºè½çæºå¨äººè¡ä¸ºãæ­¤å¤ï¼æ°æ®æ æ³¨ä¸ç­å±çèªå¨åãä»¥åæ¨¡åéåä¸çº¹çåæç­åºç¡ç ç©¶ä¹æç»­åå°å³æ³¨ã</p>
<p><strong>éè¦ä¸åæ°è®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"Pixel Motion Diffusion is What We Need for Robot Control" (E-Ro Nguyen et al.)</strong>ï¼è¿ç¯è®ºææåºäºä¸ç§æ°é¢çåç´ è¿å¨æ©æ£æ¨¡åï¼å¯è½ä¸ºæºå¨äººæ§å¶æä¾äºä¸ç§æ´ç´è§ãæ´ææçèå¼ï¼ææç®åå¤æçæºå¨äººä»»å¡è§åãå¶åæ°æ§å¨äºç´æ¥å¨åç´ å±é¢è¿è¡è¿å¨æ©æ£ï¼å¯è½æ¯ä¼ ç»çç¶æç©ºé´æ¹æ³æ´å·æ³åæ§ã</li>
<li><strong>"See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation" (Chih Yao Hu et al.)</strong>ï¼è¯¥å·¥ä½å±ç¤ºäºä¸ä¸ªæ éå­¦ä¹ ç VLM æ¡æ¶ï¼ç¨äºéç¨æ äººæºå¯¼èªãå¶âå­¦ä¹ -èªç±âçç¹æ§éå¸¸å¼äººæ³¨ç®ï¼è¡¨æ VLM å¨ç¹å®ä»»å¡ä¸­å¯è½å·å¤å¼ºå¤§çé¶æ ·æ¬æå°æ ·æ¬æ³åè½åï¼æå¤§å°éä½äºé¨ç½²ææ¬åæ°æ®ä¾èµã</li>
<li><strong>"LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision" (Debargha Ganguly et al.)</strong>ï¼è¿ç¯è®ºææåºäºä¸ç§ç¨äºèªå¨åæ°æ®ç­å±çæ·±åº¦ç ç©¶ä»£çãå¨æ°æ®é©±å¨çè®¡ç®æºè§è§é¢åï¼èªå¨åæ°æ®æ æ³¨åç­å±æ¯æé«æçåè´¨éçå³é®ç¶é¢ï¼è¯¥å·¥ä½æææ¾èå éç ç©¶åå¼åå¨æã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åä¸ææ¯ï¼</strong></p>
<ol>
<li><strong>å·èº«æºè½ä¸æºå¨äººæ§å¶çèåï¼</strong> å¤ç¯è®ºæï¼å¦ "Pixel Motion Diffusion" å "See, Point, Fly"ï¼å¼ºè°äºå°è®¡ç®æºè§è§ä¸æºå¨äººæ§å¶æ·±åº¦ç»åï¼å®ç°æ´èªä¸»ãæ´æºè½çå·èº«æºè½ä½ã</li>
<li><strong>è§è§è¯­è¨æ¨¡å (VLM) çæ³åä¸è§£éæ§ï¼</strong> VLM ä¸ä»è¢«ç¨äºå¯¼èªï¼"See, Point, Fly"ï¼ï¼å¶åé¨æºå¶åæ³¨æåæ¨¡å¼ä¹åå°äºå³æ³¨ï¼"Where MLLMs Attend and What They Rely On"ï¼ï¼è¿å¯¹äºçè§£åæ¹è¿ VLM è³å³éè¦ã</li>
<li><strong>æ°æ®æçä¸èªå¨åï¼</strong> "LABELING COPILOT" å¼ºè°äºèªå¨åæ°æ®ç­å±çéè¦æ§ï¼è "CapRL" åéè¿å¼ºåå­¦ä¹ åºæ¿å¯éå¾åå­å¹è½åï¼é½æ¨å¨æé«æ°æ®å©ç¨æçåæ¨¡åæ§è½ã</li>
<li><strong>ä¸çæ¨¡å (World Model) çæå»ºï¼</strong> "WoW: Towards a World omniscient World model Through Embodied Interaction" æ¨å¨éè¿å·èº«äº¤äºæå»ºä¸ä¸ªâå¨ç¥âçä¸çæ¨¡åï¼è¿æ¯è¿åéç¨äººå·¥æºè½çéè¦ä¸æ­¥ã</li>
</ol>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå¿ç¢çç ç©¶äººåï¼ä»¥ä¸è®ºæå¯è½æå¼å¾æ·±å¥éè¯»ï¼</p>
<ul>
<li><strong>"Pixel Motion Diffusion is What We Need for Robot Control" (E-Ro Nguyen et al.)</strong>ï¼å¯¹äºä»äºæºå¨äººæ§å¶æå·èº«æºè½çç ç©¶äººåï¼è¿ç¯è®ºæå¯è½æä¾äºä¸ç§å¨æ°çè§è§åæ¹æ³ã</li>
<li><strong>"See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation" (Chih Yao Hu et al.)</strong>ï¼å¯¹äºå³æ³¨ VLM æ³åè½åãé¶æ ·æ¬å­¦ä¹ ææ äººæºåºç¨çç ç©¶äººåï¼è¿ç¯è®ºæå·æå¾é«çåèä»·å¼ã</li>
<li><strong>"LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision" (Debargha Ganguly et al.)</strong>ï¼å¯¹äºä»»ä½å¤çå¤§è§æ¨¡æ°æ®éãå¯»æ±æé«æ°æ®æ æ³¨åç­å±æççç ç©¶äººåï¼è¿ç¯è®ºææä¾äºå®ç¨çè§£å³æ¹æ¡ã</li>
<li><strong>"WoW: Towards a World omniscient World model Through Embodied Interaction" (Xiaowei Chi et al.)</strong>ï¼å¯¹äºå¯¹éç¨äººå·¥æºè½ãä¸çæ¨¡åæå»ºåå·èº«å­¦ä¹ æå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºææç»äºä¸ä¸ªå®å¤§çç ç©¶æ¹åã</li>
</ul>
<p>ä»å¤©çæ¥åæ­ç¤ºäºè®¡ç®æºè§è§é¢åå¨å¤æ¨¡æãå·èº«æºè½åæ°æ®æçæ¹é¢çæç»­è¿æ­¥ï¼é¢ç¤ºçæªæ¥æºè½ç³»ç»å°æ´å èªä¸»ãé«æåéç¨ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.22542v1">Category Discovery: An Open-World Perspective</a></li>
<li><a href="#2509.22652v1">Pixel Motion Diffusion is What We Need for Robot Control</a></li>
<li><a href="#2509.22653v1">See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</a></li>
<li><a href="#2509.22647v1">CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</a></li>
<li><a href="#2509.22642v1">WoW: Towards a World omniscient World model Through Embodied Interaction</a></li>
<li><a href="#2509.22631v1">LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</a></li>
<li><a href="#2509.22627v1">CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach</a></li>
<li><a href="#2509.22496v1">Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation</a></li>
<li><a href="#2509.22448v1"><script type="math/tex">Î³</script>-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition</a></li>
<li><a href="#2509.22318v1">NIFTY: a Non-Local Image Flow Matching for Texture Synthesis</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.22542v1'></a></p>
<h2 id="category-discovery-an-open-world-perspective"><a href="https://arxiv.org/abs/2509.22542v1">Category Discovery: An Open-World Perspective</a></h2>
<p><strong>Authors:</strong> Zhenqi He, Yuanpei Liu, Kai Han</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Category discovery (CD) is an emerging open-world learning task, which aims
at automatically categorizing unlabelled data containing instances from unseen
classes, given some labelled data from seen classes. This task has attracted
significant attention over the years and leads to a rich body of literature
trying to address the problem from different perspectives. In this survey, we
provide a comprehensive review of the literature, and offer detailed analysis
and in-depth discussion on different methods. Firstly, we introduce a taxonomy
for the literature by considering two base settings, namely novel category
discovery (NCD) and generalized category discovery (GCD), and several derived
settings that are designed to address the extra challenges in different
real-world application scenarios, including continual category discovery,
skewed data distribution, federated category discovery, etc. Secondly, for each
setting, we offer a detailed analysis of the methods encompassing three
fundamental components, representation learning, label assignment, and
estimation of class number. Thirdly, we benchmark all the methods and distill
key insights showing that large-scale pretrained backbones, hierarchical and
auxiliary cues, and curriculum-style training are all beneficial for category
discovery, while challenges remain in the design of label assignment, the
estimation of class numbers, and scaling to complex multi-object
scenarios.Finally, we discuss the key insights from the literature so far and
point out promising future research directions. We compile a living survey of
the category discovery literature at
\href{https://github.com/Visual-AI/Category-Discovery}{https://github.com/Visual-AI/Category-Discovery}.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Zhenqi He, Yuanpei Liu, Kai Hanæ°åçè®ºæâCategory Discovery: An Open-World Perspectiveâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºæä¸»è¦å³æ³¨âç±»å«åç°âï¼Category Discovery, CDï¼è¿ä¸æ°å´çå¼æ¾ä¸çå­¦ä¹ ä»»å¡ãCDæ¨å¨èªå¨å¯¹åå«æ¥èªæªè§ç±»å«çæªæ è®°æ°æ®è¿è¡åç±»ï¼åæ¶å©ç¨æ¥èªå·²ç¥ç±»å«çå°éæ è®°æ°æ®ãè¿è§£å³äºä¼ ç»æ·±åº¦å­¦ä¹ æ¨¡åå¨å°é­ä¸çåè®¾ä¸ï¼å³ææç±»å«å¨è®­ç»æ¶é½å·²ç¥ï¼çå±éæ§ï¼å¹¶ä½¿å¶è½å¤éåºç°å®ä¸çä¸­ä¸æ­åºç°æ°ç±»å«çåºæ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥ç»¼è¿°è®ºææ¬èº«çä¸»è¦è´¡ç®å¨äºå¯¹CDé¢åè¿è¡äºç³»ç»æ§çåé¡¾ååæï¼èéæåºæ°çCDæ¹æ³ãå¶å³é®åæ°åè´¡ç®ä½ç°å¨ï¼</p>
<ul>
<li><strong>å¨é¢çåç±»æ³ï¼</strong> è®ºæé¦åæåºäºä¸ä¸ªå¨é¢çCDåç±»æ³ï¼å°å¶åä¸ºä¸¤ç§åºæ¬è®¾ç½®ââæ°é¢ç±»å«åç°ï¼Novel Category Discovery, NCDï¼åå¹¿ä¹ç±»å«åç°ï¼Generalized Category Discovery, GCDï¼ï¼ä»¥åä¸ç§æ´¾çè®¾ç½®ï¼åæ¬æç»­ç±»å«åç°ï¼Continual Category Discovery, CCDï¼ãé¢åæ¼ç§»ä¸çç±»å«åç°ï¼CD with Domain Shiftï¼ãèé¦ç±»å«åç°ï¼Federated Category Discovery, FCDï¼ç­ï¼ä»¥åºå¯¹æ´å¤æçç°å®ä¸çææã</li>
<li><strong>è¯¦ç»çæ¹æ³åæï¼</strong> å¯¹æ¯ç§è®¾ç½®ä¸çæ¹æ³è¿è¡äºæ·±å¥åæï¼éç¹å³æ³¨å¶ä¸ä¸ªæ ¸å¿ç»æé¨åï¼è¡¨ç¤ºå­¦ä¹ ï¼representation learningï¼ãæ ç­¾åéï¼label assignmentï¼åç±»å«æ°éä¼°è®¡ï¼estimation of class numberï¼ã</li>
<li><strong>åºåæµè¯åå³é®æ´å¯ï¼</strong> è®ºæå¯¹ç°ææ¹æ³è¿è¡äºå¨é¢çåºåæµè¯ï¼å¹¶æç¼åºå³é®æ´å¯ï¼æåºå¤§è§æ¨¡é¢è®­ç»éª¨å¹²ç½ç»ãåå±åè¾å©çº¿ç´¢ä»¥åè¯¾ç¨å¼è®­ç»å¯¹ç±»å«åç°ççå¤ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥è®ºæéè¿å¯¹ç°ææç®çç»¼ååæååºåæµè¯ï¼å¾åºäºä»¥ä¸ä¸»è¦ç»æåéè¦æä¹ï¼</p>
<ul>
<li><strong>é¢è®­ç»éª¨å¹²ç½ç»çéè¦æ§ï¼</strong> å¤§è§æ¨¡èªçç£é¢è®­ç»è§è§éª¨å¹²ç½ç»ï¼å¦DINOv2ï¼å¨ç±»å«åç°ä»»å¡ä¸­è¡¨ç°åºæ¾èä¼å¿ï¼æ¾èæåäºç¹å¾è´¨éãé²æ£æ§åå¯æ©å±æ§ã</li>
<li><strong>åå±ä¿¡æ¯åè¾å©çº¿ç´¢çä»·å¼ï¼</strong> åå±ä¿¡æ¯è¢«è¯ææ¯ç±»å«åç°çéè¦å½çº³åç½®ï¼è½å¤æææåæ§è½ãæ­¤å¤ï¼é¤äºåå§å¾åç¹å¾ä¹å¤çè¾å©ä¿¡æ¯ï¼å¦å¯¹è±¡çº§æææ¬ä¿¡æ¯ï¼ä¹æå¤§å°ä¿è¿äºè¯­ä¹çè§£åç±»å«åç°ã</li>
<li><strong>ææä¸å±éï¼</strong> å°½ç®¡åå¾äºè¿å±ï¼ä½å¨æ ç­¾åéç­ç¥è®¾è®¡ãç±»å«æ°éçåç¡®ä¼°è®¡ä»¥åæ©å±å°å¤æå¤å¯¹è±¡åºæ¯æ¹é¢ä»å­å¨ææãä¾å¦ï¼NCDæ¹æ³éå¸¸åè®¾æªæ è®°æ°æ®åªåå«æ°é¢ç±»å«ï¼è¿å¨ç°å®ä¸çä¸­å¹¶ä¸æ»æ¯æç«ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è¯¥ç»¼è¿°æåºäºCDé¢åç°æç ç©¶çå ä¸ªå±éæ§ï¼</p>
<ul>
<li><strong>æ ç­¾åéç­ç¥çæ®éæ§ï¼</strong> ç¼ºä¹æ®éæä¼çæ ç­¾åéç­ç¥ï¼å¶æææ§é«åº¦ä¾èµäºè¡¨ç¤ºå­¦ä¹ èå¼åæ°æ®éç¹æ§ï¼å¦ç±»å«ç²åº¦ãåå¸åæï¼ã</li>
<li><strong>ç±»å«æ°éä¼°è®¡çæçåç¨³å®æ§ï¼</strong> ç°ææ¹æ³éå¸¸å°ç±»å«æ°éä¼°è®¡è§ä¸ºåå¤çæ­¥éª¤ï¼å¨å¤æåºæ¯ä¸ä»é¢ä¸´æçãå¯æ©å±æ§åç¨³å®æ§é®é¢ã</li>
<li><strong>è·¨æ¨¡æä¿¡æ¯çå©ç¨ä¸è¶³ï¼</strong> å¤§å¤æ°CDç ç©¶ä¸»è¦éä¸­å¨è§è§ç®¡éï¼å¯¹è·¨æ¨¡æçº¿ç´¢ï¼ç¹å«æ¯ææ¬ï¼çå©ç¨ç¸å¯¹ä¸è¶³ï¼å°½ç®¡VLMsï¼å¦CLIPï¼æ¾ç¤ºåºå·¨å¤§æ½åï¼ä½å¶å¼æ¾ä¸çæ§è½å¯è½æºäºè®°å¿èéæ³åï¼å­å¨æ°æ®æ³é²é£é©ã</li>
<li><strong>å¤æå¤å¯¹è±¡åºæ¯çéç¨æ§ï¼</strong> å¤§å¤æ°CDç ç©¶å¨åå¯¹è±¡æ°æ®éä¸å¼ååè¯ä¼°ï¼è¿ä¸ç°å®ä¸çä¸­åå«å¤ä¸ªäº¤äºå¯¹è±¡ãæä¹±èæ¯åé®æ¡çå¤æåºæ¯ä¸ç¬¦ã</li>
<li><strong>ç°å®ä¸çè®¾ç½®çæ´åï¼</strong> ç°ææ¹æ³å¾å°è½åæ¶å¤çæç»­å­¦ä¹ ãè·¨é¢åæ³åãç±»å«ä¸å¹³è¡¡åé¢åæ¼ç§»ç­ç°å®ä¸çææã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°åæï¼è®ºææåºäºä»¥ä¸æåæ¯çæªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>èªéåºææ··åæ ç­¾åéç­ç¥ï¼</strong> å¼åè½å¤æ´å¥½å°ä¸å­¦ä¹ å°çè¡¨ç¤ºåæ°æ®éç¹æ§å¯¹é½çèªéåºææ··åæ ç­¾åéç­ç¥ã</li>
<li><strong>è®­ç»ä¸­ç±»å«æ°éçèªéåºæ¨æ­ï¼</strong> ç ç©¶è½å¤å¨è®­ç»è¿ç¨ä¸­ææãé«æå°èªéåºæ¨æ­ç±»å«æ°éçæ¡æ¶ã</li>
<li><strong>è¯­ä¹åéªçæ¾å¼æ´åï¼</strong> å¨ä¸è¿åº¦ä¾èµVLMéª¨å¹²ç½ç»çæåµä¸ï¼æ¾å¼æ´åä»è¯­è¨ä¸­æåçè¯­ä¹åéªï¼å¦ææ¬æè¿°æå±æ§åç§°ï¼ï¼ä»¥é¿åè®°å¿æé¢ååç½®çæ½å¨é·é±ã</li>
<li><strong>å¤å¯¹è±¡åºæ¯çç±»å«åç°ï¼</strong> å°CDæ©å±å°å¤æå¤å¯¹è±¡åºæ¯ï¼è§£å³å®ä¾é´ç¹å¾è§£è¦ãå¯¹è±¡åç°ï¼å¯è½æ ¹æ®ç¨æ·å´è¶£ï¼ä»¥åå°å®ä¾çº§è¯æ®ä¸æ°å´ç±»å«å³èçé®é¢ã</li>
<li><strong>ç»ä¸çç°å®ä¸çæ¡æ¶ï¼</strong> å¼åè½å¤æ´åæç»­å­¦ä¹ ãè·¨é¢åæ³åãç±»å«ä¸å¹³è¡¡åé¢åæ¼ç§»ç­å¤ç§ç°å®ä¸çææçç»ä¸æ¡æ¶ã</li>
<li><strong>æ©å±å°å¶ä»æ¨¡æåä»»å¡ï¼</strong> å°ç±»å«åç°æ©å±å°å®ä¾çº§æåç´ çº§åç°ï¼ä»¥å3Dæ·±åº¦ä¼°è®¡ãè§é¢åæåå¨ä½è¯å«ç­ç¸å³ä»»å¡ã</li>
</ul>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºç±»å«åç°é¢åæä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»æ»ç»äºç°æææ¯ï¼è¿æç¡®æåºäºæªæ¥çç ç©¶æ¹åï¼å¯¹äºæ¨å¨å¼æ¾ä¸çå­¦ä¹ åäººå·¥æºè½å¨ç°å®ä¸çä¸­çåºç¨å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Firstly, we introduce a taxonomy
for the literature by considering two base settings, namely novel category
discovery (NCD) and generalized category discovery (GCD), and several derived
settings that are designed to address the extra challenges in different
real-world application scenarios, including continual category discovery,
skewed data distribution, federated category discovery, etc.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22542v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22542v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22652v1'></a></p>
<h2 id="pixel-motion-diffusion-is-what-we-need-for-robot-control"><a href="https://arxiv.org/abs/2509.22652v1">Pixel Motion Diffusion is What We Need for Robot Control</a></h2>
<p><strong>Authors:</strong> E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾E-Ro Nguyenç­äººæ°åçè®ºæâPixel Motion Diffusion is What We Need for Robot Controlâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºæé¢ç®ï¼</strong> Pixel Motion Diffusion is What We Need for Robot Control</p>
<p><strong>ä½èï¼</strong> E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo</p>
<h3 id="_1">è®ºææè¦</h3>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸ºDAWNï¼Diffusion is All We Need for robot controlï¼çç»ä¸æ©æ£æ¨¡åæ¡æ¶ï¼ç¨äºè¯­è¨æ¡ä»¶ä¸çæºå¨äººæä½ãDAWNéè¿ç»æåçåç´ è¿å¨è¡¨ç¤ºï¼å°é«å±è¿å¨æå¾ä¸ä½å±æºå¨äººå¨ä½è¿æ¥èµ·æ¥ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçåºäºåç´ è¿å¨çä¸¤é¶æ®µæºå¨äººæ§å¶æ¡æ¶ï¼å¨å¤çé«å±è¿å¨çæåä½å±æ§å¶å¨æ¹é¢æªè½ååå©ç¨è§è§çææ¨¡ååæ©æ£ç­ç¥çææ°è¿å±ï¼å¯¼è´æ§è½å­å¨å·®è·ãè®ºææ¨å¨è§£å³å¦ä½æå»ºä¸ä¸ªå¯è§£éãæ¨¡ååä¸æ°æ®é«æçæºå¨äººæ§å¶ç³»ç»ï¼è¯¥ç³»ç»è½å¤æææ¡¥æ¥é«å±è¯­è¨æä»¤åä½å±æºå¨äººå¨ä½ï¼å°¤å¶æ¯å¨å¤æçå¤ä»»å¡åçå®ä¸çåºæ¯ä¸­ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ä¸¤é¶æ®µæ©æ£æ¨¡åæ¡æ¶ (DAWN)ï¼</strong> è®ºææ ¸å¿åæ°å¨äºæåºäºä¸ä¸ªä¸¤é¶æ®µçæ©æ£æ¨¡åæ¡æ¶ãé«å±æ§å¶å¨ï¼Motion Directorï¼æ¯ä¸ä¸ªæ½å¨æ©æ£æ¨¡åï¼è´è´£ä»å½åè§è§è§æµåè¯­è¨æä»¤çææéçå¯éåç´ è¿å¨è¡¨ç¤ºãä½å±æ§å¶å¨ï¼Action Expertï¼æ¯ä¸ä¸ªæ©æ£Transformerï¼å°çæçåç´ è¿å¨ä¸é¢å¤è¾å¥ï¼è§è§ãæºå¨äººç¶æãè¯­è¨æä»¤ï¼ç»åï¼çææç»çæºå¨äººå¨ä½åºåã
*   <strong>ç»æååç´ è¿å¨è¡¨ç¤ºï¼</strong> DAWNä½¿ç¨æ¾å¼å¯éåç´ è¿å¨ä½ä¸ºé«å±è¿å¨æå¾åä½å±æºå¨äººå¨ä½ä¹é´çç»æåä¸­é´è¡¨ç¤ºãè¿ç§è¡¨ç¤ºæ¹å¼ä¸ä»å¯è§£éï¼èä¸è½å¤æææ¡¥æ¥ä¸åæ¨¡æçä¿¡æ¯ã
*   <strong>å©ç¨é¢è®­ç»æ¨¡åï¼</strong> Motion Directorå©ç¨é¢è®­ç»çæ½å¨æ©æ£æ¨¡åè¿è¡RGBå¾åçæï¼å¹¶å°å¶éåºäºåç´ è¿å¨çæãAction Expertåå©ç¨é¢è®­ç»çè§è§åè¯­è¨æ¨¡åï¼å¦ConvNeXt-S DINOv3åT5-smallï¼æ¥å¢å¼ºå¶è¡¨ç¤ºè½åã
*   <strong>ç«¯å°ç«¯å¯è®­ç»ç³»ç»ï¼</strong> æ´ä¸ªDAWNç³»ç»æ¯å®å¨å¯è®­ç»çï¼åæ¶ä¿æäºä¸­é´è¿å¨æ½è±¡çå¯è§£éæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> DAWNå¨CALVINåºåæµè¯ä¸åå¾äºæåè¿çç»æï¼å±ç¤ºäºå¼ºå¤§çå¤ä»»å¡æ§è½ãå¨MetaWorldåºåæµè¯ä¸ä¹éªè¯äºå¶æææ§ï¼å¹¶æ¾èä¼äºç°ææ¹æ³ï¼å°¤å¶æ¯å¨è¯­ä¹ç¸ä¼¼ä½ä»»å¡ä¸åçåºæ¯ï¼å¦âå¼é¨âä¸âå³é¨âï¼ã
*   <strong>æ°æ®é«æåçå®ä¸çè¿ç§»ï¼</strong> å°½ç®¡å¨æ¨¡æåç°å®ä¹é´å­å¨æ¾èçé¢åå·®è·ï¼ä¸çå®ä¸çæ°æ®æéï¼DAWNéè¿æå°çå¾®è°å®ç°äºå¯é ççå®ä¸çè¿ç§»ãè¿è¡¨ææ©æ£æ¨¡åç»åè¿å¨ä¸­å¿è¡¨ç¤ºå¨å¯æ©å±åé²æ£çæºå¨äººå­¦ä¹ ä¸­å·æå®éå¯è¡æ§ã
*   <strong>å¯è§£éæ§åæ¨¡ååï¼</strong> åç´ è¿å¨ä½ä¸ºä¸­é´è¡¨ç¤ºï¼ä½¿å¾ç³»ç»å·æè¯å¥½çå¯è§£éæ§ãæ¨¡ååè®¾è®¡åè®¸ä¸¤ä¸ªæ©æ£æ¨¡åå¹¶è¡è®­ç»ï¼å¹¶å¯ç¬ç«åçº§ï¼ä¾¿äºæªæ¥è§è§ææ§å¶é¢åçæ°è¿å±éæã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   å°½ç®¡DAWNå¨æ°æ®æéçæåµä¸è¡¨ç°åºè²ï¼ä½ä¸æäºéè¦å¤§éé¢è®­ç»æ°æ®çSOTAæ¹æ³ï¼å¦VPPï¼ç¸æ¯ï¼å¶é¢è®­ç»è§æ¨¡ä»ææåç©ºé´ãè®ºææå°ï¼å¨ä¸VPPè¿è¡å¬å¹³æ¯è¾æ¶ï¼ä»ä»¬éç¨äºVPPçé¢è®­ç»æ£æ¥ç¹ï¼å¹¶å¯¹DAWNè¿è¡äºå¾®è°ï¼è¿æç¤ºäºå¨å®å¨ä»å¤´å¼å§è®­ç»æ¶ï¼æ°æ®éå¯è½ä»æ¯ä¸ä¸ªèèå ç´ ã
*   å¨æäºçå®ä¸çä»»å¡ä¸­ï¼å°½ç®¡DAWNè¡¨ç°ä¼å¼ï¼ä½ä»ææ¹è¿ç©ºé´ï¼ä¾å¦å¨æäºç¹å®ä»»å¡ï¼å¦MetaWorldçâhammer assemblyâåâfaucet-closeâï¼ä¸ï¼æåçå¹¶é100%ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¹¿æ³çé¢è®­ç»ï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¨æ´å¤§è§æ¨¡çæºå¨äººæ¼ç¤ºæ°æ®éä¸è¿è¡é¢è®­ç»ï¼ä»¥è¿ä¸æ­¥æåDAWNçæ³åè½ååæ§è½ã
*   <strong>æ´å¤æçè¿å¨æ½è±¡ï¼</strong> æ¢ç´¢é¤äºåç´ è¿å¨ä¹å¤ï¼æ´ä¸°å¯ãæ´å¤æçä¸­é´è¿å¨æ½è±¡ï¼ä»¥å¤çæ´ç²¾ç»ææ´æ½è±¡çæºå¨äººæ§å¶ä»»å¡ã
*   <strong>å¤æ¨¡æèåï¼</strong> è¿ä¸æ­¥ä¼åä¸åæ¨¡æï¼è§è§ãè¯­è¨ãæºå¨äººç¶æãåç´ è¿å¨ï¼çèåç­ç¥ï¼ä»¥å®ç°æ´é²æ£åæºè½çæºå¨äººè¡ä¸ºã
*   <strong>èªéåºæ©æ£æ­¥æ°ï¼</strong> è®ºææå°å¢å æ©æ£æ­¥æ°å¯ä»¥æé«æ§è½ï¼ä½è¾¾å°ä¸å®éå¼åæ¶çéåãæªæ¥å¯ä»¥ç ç©¶èªéåºå°ç¡®å®æ©æ£æ­¥æ°ï¼ä»¥å¹³è¡¡æ§è½åè®¡ç®æçã
*   <strong>å¤èåè°ï¼</strong> è®ºæå¨åèæä½ä¸­å±ç¤ºäºDAWNçæææ§ï¼æªæ¥å¯ä»¥è¿ä¸æ­¥æ¢ç´¢å¶å¨æ´å¤æçå¤èåè°ä»»å¡ä¸­çåºç¨ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥DAWNæ¡æ¶ï¼æåå°å°æ©æ£æ¨¡åä¸ç»æååç´ è¿å¨è¡¨ç¤ºç¸ç»åï¼ä¸ºè¯­è¨æ¡ä»¶ä¸çæºå¨äººæä½æä¾äºä¸ä¸ªå¼ºå¤§ãå¯è§£éä¸æ°æ®é«æçè§£å³æ¹æ¡ãå¶å¨å¤ä¸ªåºåæµè¯ä¸çä¼å¼è¡¨ç°ï¼ä»¥åå¨çå®ä¸çä¸­çå¯é è¿ç§»è½åï¼ä¸ºæºå¨äººå­¦ä¹ é¢åå¼è¾äºæ°çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation.</li>
<li>DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld.</li>
<li>Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control.</li>
<li>Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22652v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22652v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22653v1'></a></p>
<h2 id="see-point-fly-a-learning-free-vlm-framework-for-universal-unmanned-aerial-navigation"><a href="https://arxiv.org/abs/2509.22653v1">See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</a></h2>
<p><strong>Authors:</strong> Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee, Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen, Tsung-Wei Ke, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CL, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºChih Yao Huç­äººæ°åçè®ºæâSee, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigationâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼See, Point, Fly: ä¸ç§ç¨äºéç¨æ äººæºå¯¼èªçåè®­ç»VLMæ¡æ¶</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æ äººæºï¼UAVï¼èªä¸»å¯¼èªä¸­çä¸ä¸ªæ ¸å¿ææï¼å¦ä½å¨åç§å¤æåå¨æç¯å¢ä¸­ï¼ä½¿æ äººæºè½å¤æ ¹æ®ä»»æå½¢å¼çèªç¶è¯­è¨æä»¤ï¼æ éç¹å®ä»»å¡è®­ç»å°å¯¼èªå°ä»»ä½ç®æ ãç°æçåºäºè§è§-è¯­è¨æ¨¡åï¼VLMï¼çæ¹æ³éå¸¸å°å¨ä½é¢æµè§ä¸ºææ¬çæä»»å¡ï¼è¿éå¶äºå¶ç²¾åº¦åæ³åè½åï¼å°¤å¶æ¯å¨éè¦ç²¾ç»å¨ä½æ§å¶åå¤çæªè§ç¯å¢æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å°å¨ä½é¢æµéæ°å®ä¹ä¸º2Dç©ºé´å®ä½ä»»å¡ï¼</strong> ä¸å°å¨ä½é¢æµè§ä¸ºææ¬çæä¸åï¼SPFå°æ äººæºå¯¼èªçå¨ä½é¢æµè§ä¸º2Dç©ºé´å®ä½ä»»å¡ãVLMè¢«ç¨äºå°æ¨¡ç³çè¯­è¨æä»¤åè§£ä¸ºè¾å¥å¾åä¸ç2Dèªç¹è¿­ä»£æ æ³¨ã
*   <strong>2Dèªç¹å°3Dä½ç§»åéçè½¬æ¢ï¼</strong> SPFå°é¢æµç2Dèªç¹ä¸é¢æµçè¡è¿è·ç¦»ç»åï¼å°å¶è½¬æ¢ä¸º3Dä½ç§»åéä½ä¸ºæ äººæºçå¨ä½æä»¤ï¼ä»èå®ç°ç²¾ç¡®ç3Då¨ä½æ§å¶ã
*   <strong>èªéåºè¡è¿è·ç¦»è°æ´ï¼</strong> SPFè½å¤èªéåºå°è°æ´è¡è¿è·ç¦»ï¼ä»¥å®ç°æ´é«æçå¯¼èªï¼å¨å¼æ¾åºåéåæ´å¤§æ­¥é¿ï¼å¨éç¢ç©éè¿åæ´è°¨æã
*   <strong>é­ç¯æ§å¶ä¸å¨æç®æ è·è¸ªï¼</strong> SPFä»¥é­ç¯æ§å¶æ¹å¼æ§è¡å¯¼èªï¼ä½¿æ äººæºè½å¤å¨å¨æç¯å¢ä¸­è·è¸ªç§»å¨ç®æ ã
*   <strong>åè®­ç»æ¡æ¶ï¼</strong> SPFæ¯ä¸ä¸ªå®å¨åè®­ç»çæ¡æ¶ï¼ç´æ¥å©ç¨é¢è®­ç»çVLMè¿è¡é«å±ç©ºé´æ¨çï¼æ éé¢å¤çç¥ç»ç½ç»è®­ç»ãæè½åºãå¤é¨æ·±åº¦ä¼ æå¨æç­ç¥ä¼åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>DRLæ¨¡æåºåçæ°SOTAï¼</strong> å¨DRLæ¨¡æåºåæµè¯ä¸­ï¼SPFçæ§è½è¶è¶äºä¹åæä½³æ¹æ³ï¼æåçç»å¯¹æé«äº63%ã
*   <strong>çå®ä¸çè¯ä¼°çåè¶è¡¨ç°ï¼</strong> å¨å¹¿æ³ççå®ä¸çè¯ä¼°ä¸­ï¼SPFä¹ä»¥æ¾èä¼å¿è¶è¶äºå¼ºå¤§çåºçº¿æ¹æ³ã
*   <strong>åºè²çæ³åè½åï¼</strong> SPFå¯¹ä¸åçVLMï¼å¦Gemini 2.5 Pro, GPT-4.1, Claude 3.7 Sonnet, Llama 4 Maverickç­ï¼è¡¨ç°åºæ¾èçæ³åè½åï¼è¯æäºå¶è®¾è®¡çæææ§ã
*   <strong>æçæåï¼</strong> èªéåºæ­¥é¿æ§å¶å¨æ¾èç¼©ç­äºä»»å¡å®ææ¶é´ï¼åæ¶ä¿æææé«äºæåçã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>VLMä¸åç¡®æ§ï¼</strong> VLMå¯è½å­å¨å¹»è§åè¯¯è§£ï¼å¯¹å°åæè¿è·ç¦»ç®æ çå®ä½ç²¾åº¦å¯è½ä¸éã
*   <strong>èªéåºæ­¥é¿å¯åå¼æ¹æ³çå±éæ§ï¼</strong> èªéåºæ­¥é¿å¯åå¼æ¹æ³æä¾äºéå¼æ·±åº¦ï¼ä½å¯è½ä¸ç²¾ç¡®ã
*   <strong>å¯¹æç¤ºè¯æªè¾çæææ§ï¼</strong> æ§è½å¯è½å¯¹æç¤ºè¯çæªè¾ææã
*   <strong>å¯¹å¨æéç¢ç©çååºæ§ï¼</strong> ç±äºVLMæ¨çå»¶è¿ï¼çº¦1-3ç§ï¼ï¼å¯¹é«åº¦å¨æéç¢ç©çååºæ§æéã
*   <strong>æç´¢æ¨¡å¼çéæä¼æ§ï¼</strong> VLMçæçæç´¢æ¨¡å¼ä¸ä¿è¯æ¯æä¼çã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   æé«æç¥é²æ£æ§ã
*   æ¹è¿å®ä½æºå¶ã
*   éä½ç³»ç»å»¶è¿ä»¥æé«ååºæ§ã
*   æ¢ç´¢VLMçå¾®è°ã
*   å¼åæ´å¤æçæ¢ç´¢ç­ç¥ã</p>
<p>æ»èè¨ä¹ï¼SPFæåºäºä¸ç§æ°é¢çãåè®­ç»çVLMæ¡æ¶ï¼éè¿å°å¨ä½é¢æµéæ°å®ä¹ä¸º2Dç©ºé´å®ä½ä»»å¡ï¼å¹¶ç»åèªéåºè·ç¦»è°æ´åé­ç¯æ§å¶ï¼æ¾èæåäºæ äººæºå¨å¤æãå¨æç¯å¢ä¸­çå¯¼èªè½åãå¶å¨æ¨¡æåçå®ä¸çä¸­çåè¶æ§è½ä»¥åå¯¹ä¸åVLMçæ³åè½åï¼ä½¿å¶æä¸ºéç¨æ äººæºå¯¼èªé¢åçä¸ä¸ªéè¦è¿å±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs).</li>
<li>SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%.</li>
<li>In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22653v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22653v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22647v1'></a></p>
<h2 id="caprl-stimulating-dense-image-caption-capabilities-via-reinforcement-learning"><a href="https://arxiv.org/abs/2509.22647v1">CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Image captioning is a fundamental task that bridges the visual and linguistic
domains, playing a critical role in pre-training Large Vision-Language Models
(LVLMs). Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models. This approach often leads to
models that memorize specific ground-truth answers, limiting their generality
and ability to generate diverse, creative descriptions. To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.
A primary challenge, however, is designing an objective reward function for the
inherently subjective nature of what constitutes a "good" caption. We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM
generates a caption, and the objective reward is derived from the accuracy of a
separate, vision-free LLM answering Multiple-Choice Questions based solely on
that caption. As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B
results in substantial gains across 12 benchmarks. Moreover, within the Prism
Framework for caption quality evaluation, CapRL achieves performance comparable
to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.
Code is available here: https://github.com/InternLM/CapRL.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Long Xingç­äººæ°åçè®ºæâCapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learningâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼CapRLï¼éè¿å¼ºåå­¦ä¹ æ¿åå¯éå¾åæè¿°è½å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¾åæè¿°æ¯è¿æ¥è§è§åè¯­è¨é¢åçå³é®ä»»å¡ï¼å¨å¤§åè§è§-è¯­è¨æ¨¡åï¼LVLMsï¼çé¢è®­ç»ä¸­æ®æ¼éè¦è§è²ãç¶èï¼å½åæåè¿çå¾åæè¿°æ¨¡åéå¸¸éç¨çç£å¾®è°ï¼SFTï¼è¿è¡è®­ç»ï¼è¿ä¾èµäºæè´µä¸é¾ä»¥æ©å±çäººå·¥æ æ³¨æ°æ®ãSFTæ¹æ³å¾å¾å¯¼è´æ¨¡åè®°å¿ç¹å®ççå®ç­æ¡ï¼éå¶äºå¶æ³åè½ååçæå¤æ ·åãåé æ§æè¿°çè½åãæ ¸å¿ææå¨äºï¼å¦ä½ä¸ºå¾åæè¿°è¿ä¸å¼æ¾å¼ãæ¬è´¨ä¸ä¸»è§çä»»å¡è®¾è®¡ä¸ä¸ªå®¢è§çå¥å±å½æ°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºåæSFTçå±éæ§ï¼è®ºææåºäº<strong>CapRLï¼Captioning Reinforcement Learningï¼</strong>ï¼ä¸ä¸ªæ°é¢çè®­ç»æ¡æ¶ï¼å°âå¥½çâæè¿°è´¨ééæ°å®ä¹ä¸ºå¶âæç¨âï¼ä¸ä¸ªé«è´¨éçæè¿°åºè¯¥è½å¤è®©ä¸ä¸ªéè§è§è¯­è¨æ¨¡åï¼LLMï¼åç¡®åç­å³äºå¯¹åºå¾åçé®é¢ãCapRLå¼å¥äºä¸ä¸ªè§£è¦çä¸¤é¶æ®µæµæ°´çº¿ï¼
*   <strong>ç¬¬ä¸é¶æ®µï¼</strong> LVLMçæå¾åæè¿°ã
*   <strong>ç¬¬äºé¶æ®µï¼</strong> ä¸ä¸ªç¬ç«çãæ è§è§çLLMä»åºäºçæçæè¿°æ¥åç­å¤é¡¹éæ©é¢ï¼MCQsï¼ï¼å¶åç­çåç¡®æ§ä½ä¸ºLVLMçå®¢è§å¥å±ä¿¡å·ï¼ç¨äºå¼ºåå­¦ä¹ è®­ç»ã
*   <strong>å¥å±å½æ°è®¾è®¡ï¼</strong> éè¿å°æè¿°è´¨éä¸LLMåç­MCQsçåç¡®æ§æé©ï¼CapRLå°ä¸»è§çæè¿°è¯ä¼°è½¬åä¸ºå®¢è§ãå¯éªè¯çå¥å±ä¿¡å·ï¼ææé¿åäºä¼ ç»å¥å±æ¨¡åä¸­å¸¸è§çåä½æç®æ´åå¥½ç­å¥å±æ¬ºéªé®é¢ã
*   <strong>QAæ°æ®ç­å±ï¼</strong> è®ºæè¿å¼åäºä¸ä¸ªç¹å®çQAç­å±æµæ°´çº¿ï¼ä»¥ç¡®ä¿MCQsæ°æ®çè´¨éï¼å¹¶ä¿è¯é®é¢åªè½éè¿åæå¾ååå®¹æ¬èº«æ¥åç­ï¼é¿åä¿¡æ¯æ³é²ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æ¾èæåï¼</strong> ä½ä¸ºé¦æ¬¡å°å¯éªè¯å¥å±å¼ºåå­¦ä¹ ï¼RLVRï¼åºç¨äºä¸»è§å¾åæè¿°ä»»å¡çç ç©¶ï¼CapRLå¨å¤ä¸ªè®¾ç½®ä¸æ¾èæåäºæ§è½ã
*   <strong>å¤§è§æ¨¡æ°æ®éé¢è®­ç»ï¼</strong> å¨ç±CapRL-3Bæ æ³¨çCapRL-5Mæè¿°æ°æ®éä¸è¿è¡é¢è®­ç»ï¼æ¨¡åå¨12ä¸ªåºåæµè¯ä¸­åå¾äºæ¾èæåã
*   <strong>ä¸åè¿æ¨¡ååª²ç¾ï¼</strong> å¨Prismæ¡æ¶ä¸è¿è¡æè¿°è´¨éè¯ä¼°æ¶ï¼CapRLå®ç°äºä¸Qwen2.5-VL-72Bæ¨¡åç¸å½çæ§è½ï¼å¹¶å¹³åè¶è¶åºçº¿8.4%ã
*   <strong>æ³åè½åååç¡®æ§ï¼</strong> ç»æéªè¯äºCapRLè½å¤ææè®­ç»æ¨¡åçææ´éç¨ãæ´åç¡®çå¾åæè¿°ï¼è¶è¶äºä¼ ç»SFTæ¨¡åå¨å¤æ ·æ§ååé æ§æ¹é¢çå±éæ§ã
*   <strong>æ°æ®æçï¼</strong> CapRLéè¿RLVRå®ç°äºåè¶çæ°æ®æçï¼å³ä½¿æ¯ç¨ççQAçç£ä¹è¶³ä»¥å¸¦æ¥æ¾èçæè¿°è½åæåã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸»è¦å³æ³¨äºSFTçå±éæ§ï¼å³æ¨¡åå¾åäºè®°å¿ç¹å®ç­æ¡ï¼å¯¼è´æ³åè½ååçæå¤æ ·åæè¿°çè½ååéãæ­¤å¤ï¼æ©æåºäºLVLMä½ä¸ºè¯å¤è/å¥å±æ¨¡åçå°è¯å­å¨å¥å±æ¬ºéªï¼å¦åå¥½åä½æç®æ´æè¿°ï¼åè®­ç»æ²çº¿ä¸ç¨³å®çé®é¢ãè½ç¶CapRLéè¿å¶è§£è¦çVQAå¥å±æºå¶è§£å³äºè¿äºé®é¢ï¼ä½è®ºæå¹¶æªæç¡®æåºCapRLèªèº«å¨ç¹å®åºæ¯ä¸çå±éæ§ï¼ä¾å¦å¨æç«¯å¤ææé«åº¦æ½è±¡çå¾åæè¿°ä»»å¡ä¸­å¯è½é¢ä¸´çææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæå¼ºè°äºCapRLæ¡æ¶å¨å¤æ¨¡æé¢è®­ç»ä¸­çå®ç¨ä»·å¼ï¼å ä¸ºå®è½å¤ä»¥éå¸¸ä½çæ æ³¨ææ¬æå»ºé«è´¨éãå¯æ©å±çæ°æ®éãè¿æç¤ºäºæªæ¥ç ç©¶å¯ä»¥è¿ä¸æ­¥æ¢ç´¢ï¼
*   <strong>æ´å¤§è§æ¨¡çåºç¨ï¼</strong> å°CapRLåºç¨äºæ´å¤§è§æ¨¡çå¾å-ææ¬å¯¹æ°æ®éï¼ä»¥è¿ä¸æ­¥æåLVLMçæ§è½ã
*   <strong>æ´å¤æçå¥å±è®¾è®¡ï¼</strong> æ¢ç´¢æ´ç²¾ç»çå¥å±å½æ°è®¾è®¡ï¼ä»¥åºå¯¹å¾åæè¿°ä¸­æ´ç»å¾®çä¸»è§æ§ææã
*   <strong>å¤æ¨¡ææºè½çèåï¼</strong> ç»åCapRLçä¼å¿ï¼æ¨å¨ä»éææç¥å°äº¤äºå¼ãç«¯å°ç«¯å¤æ¨¡ææºè½çåå±ï¼åæ¬é¿ä¸ä¸æå¤æ¨¡æãæºè½ä½è¡ä¸ºä»¥åç»ä¸çé¢è®­ç»ç®æ ã
*   <strong>é«æéåºæ§ï¼</strong> ç ç©¶è½»éçº§å¾®è°åæ£ç´¢ç­é«æéåºæ§æ¹æ³ï¼ä»¥å®å¨å°å°CapRLç³»ç»é¨ç½²å°æ´å¹¿æ³çé¢ååè®¾å¤ä¸­ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models.</li>
<li>To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.</li>
<li>We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image.</li>
<li>As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22647v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22642v1'></a></p>
<h2 id="wow-towards-a-world-omniscient-world-model-through-embodied-interaction"><a href="https://arxiv.org/abs/2509.22642v1">WoW: Towards a World omniscient World model Through Embodied Interaction</a></h2>
<p><strong>Authors:</strong> Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai, Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu, Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan, Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.RO, cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºXiaowei Chiç­äººæ°åçè®ºæâWoW: Towards a World-Omniscient World-Model Through Embodied Interactionâçæè¦ï¼éç¹çªåºå¶å¨è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçè´¡ç®ï¼</p>
<p><strong>è®ºææè¦ï¼WoWï¼éè¿å·èº«äº¤äºè¿åä¸çå¨ç¥ä¸çæ¨¡å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åè§é¢çææ¨¡åï¼å¦Soraï¼ä¸»è¦ä¾èµè¢«å¨è§å¯ï¼é¾ä»¥çæ­£çè§£ç©çå æå³ç³»ãè¿å¯¼è´å®ä»¬å¨å¤çéè¦çå®ç©çæ¨ççåºæ¯æ¶è¡¨ç°åºèå¼±æ§åç©çå¹»è§ãè®ºæçæ ¸å¿ç ç©¶é®é¢æ¯ï¼å¦ä½éè¿å¤§è§æ¨¡ãå æä¸°å¯ççå®ä¸çäº¤äºï¼ä½¿å·èº«ä¸çæ¨¡ååå±åºçå®çç©çç´è§ï¼ä»èåæç°ææ¨¡åçå±éæ§ï¼å®ç°å¯¹ç©çä¸ççæ·±å»çè§£åé¢æµï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å·èº«ä¸çæ¨¡åWoWï¼</strong> æåºå¹¶å®ä¾åäºä¸ä¸ª140äº¿åæ°ççæå¼ä¸çæ¨¡åWoWï¼è¯¥æ¨¡åéè¿å¤§è§æ¨¡ï¼200ä¸æ¡ï¼æºå¨äººäº¤äºè½¨è¿¹è¿è¡è®­ç»ï¼æ¨å¨ç´æ¥åæåç´ çº§çæªæ¥é¢æµï¼å¹¶éè¿çææ¬èº«è¿è¡æ³è±¡åæ¨çã
*   <strong>SOPHIAæ¡æ¶ï¼</strong> å¼å¥äºä¸ç§æ°é¢çæ¶æèå¼SOPHIAï¼å°è§è§è¯­è¨æ¨¡åï¼VLMï¼çæ¨çè½åä¸æ©æ£Transformerï¼DiTï¼ççæè½åç¸ç»åãSOPHIAéè¿âé¢æµ-æ¹è¯-ç²¾ç¼âçè¿­ä»£å¾ªç¯ï¼å°æ³è±¡åæ¨çç»ä¸ä¸ºå·èº«æºè½çåºæ¬ç»æé¨åï¼ä¸»å¨å°æ¨¡åççæè½åçº¦æå°ç©çç°å®ã
*   <strong>Flow-Maskéå¨åå­¦æ¨¡åï¼FM-IDMï¼ï¼</strong> è®¾è®¡äºä¸ä¸ªè§é¢å°å¨ä½çæ¨¡åï¼å°é¢æµçè§é¢å¸§è½¬åä¸ºå¯æ§è¡çæºå¨äººå¨ä½ï¼ä»èé­åäºâæ³è±¡å°è¡å¨âçå¾ªç¯ãFM-IDMéè¿åæå½åç¶æåæ³è±¡çä¸ä¸ç¶æä¹é´çåæµååºæ¯ä¸ä¸æï¼æ¨æ­åºæ§è¡è½¬æ¢æéç7èªç±åº¦æ«ç«¯æ§è¡å¨å¨ä½ã
*   <strong>WoWBenchåºåï¼</strong> å»ºç«äºä¸ä¸ªæ°çåºåï¼ä¸æ³¨äºè§é¢ä¸­çç©çä¸è´æ§åå ææ¨çï¼åå«4ä¸ªæ ¸å¿è½åå20ä¸ªå­ä»»å¡ï¼ç¨äºå¨é¢è¯ä¼°å·èº«ä¸çæ¨¡åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ç©çç´è§çæåï¼</strong> WoWæ¨¡åå¨ç©ççè§£æ¹é¢è¡¨ç°åºæ¾èè½åï¼å°¤å¶æ¯å¨ç©çå æå³ç³»ãç¢°æå¨åå­¦åç©ä½æ°¸ææ§æ¹é¢ï¼å¨WoWBenchåºåæµè¯ä¸­åå¾äºæåè¿çæ§è½ï¼äººç±»åèªä¸»è¯ä¼°åè¡¨ç°ä¼å¼ï¼ï¼æä»¤çè§£åç¡®çè¾¾å°96.53%ï¼ç©çå®å¾éµå¾ªåç¡®çè¾¾å°80.16%ãè¿æåå°æ¯æäºè®ºæçæ ¸å¿åè®¾ï¼å³å¤§è§æ¨¡çå®ä¸çäº¤äºæ¯AIåå±ç©çç´è§çåºç³ã
*   <strong>åæç©çå¹»è§ï¼</strong> SOPHIAæ¡æ¶éè¿VLMä»£çå¯¹DiTçæçè¾åºè¿è¡è¯ä¼°ï¼å¹¶éè¿è¿­ä»£æ¼åè¯­è¨æä»¤æ¥æå¯¼å¶ç²¾ç¼ï¼ææè§£å³äºæ¨¡åçè§£ç©ççæ¦çæ§åå¸å¯¼è´çéæºä¸ç¨³å®æ§åç©çå¹»è§é®é¢ã
*   <strong>æ³è±¡å°è¡å¨çé­ç¯ï¼</strong> éå¨åå­¦æ¨¡åå°ç²¾ç¼åçè®¡åè½¬åä¸ºå¯æ§è¡çæºå¨äººå¨ä½ï¼æåå®ç°äºä»æ³è±¡å°ç©çè¡å¨çé­ç¯ï¼ä½¿æ¨¡åè½å¤æå¯¼ç©çæºå¨äººæ§è¡ä»»å¡ã
*   <strong>æ³åè½åï¼</strong> WoWå±ç¤ºäºå¯¹æ°é¢æºå¨äººå·èº«ãæä½ä»»å¡åè§è§é¢åçå¼ºå¤§æ³åè½åï¼è¯æå¶å­¦ä¹ çæ¯äº¤äºçåºå±ç©çåçï¼èéä»ä»æ¯è®­ç»æ°æ®çä¸ä¸æã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   å°½ç®¡WoWå¨ç©çæ¨çæ¹é¢åå¾äºæ¾èè¿å±ï¼ä½æ¨¡åå¯¹å¤æãå°é¾çç©çæ¨çä»»å¡çææ¡ä»ç¶å·ææææ§ï¼éè¦è¿ä¸æ­¥çæ©å±ï¼scalingï¼ã
*   æ¨¡åæ§è½çæåéçåæ°æ°éçå¢å èæ¾èåéï¼å¨æ§è½åæçä¹é´å­å¨å³é®çæè¡¡ã
*   å½åè§é¢ä¸çæ¨¡åå¨3Dä¸è´æ§ãç©çè¿è´¯æ§åæ¶é´æ¨çæ¹é¢ä»å­å¨å±éæ§ï¼è¿äºæ¯å®ç°å¿ å®ç¯å¢æ¨¡ææå¿éçã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   ç»§ç»­æ©å±æ¨¡åè§æ¨¡åè®­ç»æ°æ®éï¼ä»¥è¿ä¸æ­¥æåæ¨¡åå¨å¤æç©çæ¨çä»»å¡ä¸çæ§è½ã
*   æ¢ç´¢æ´ææçæ¶æè®¾è®¡åè®­ç»ç­ç¥ï¼ä»¥ä¼åæ§è½ä¸æ¨çæçä¹é´çå¹³è¡¡ã
*   è¿ä¸æ­¥ç ç©¶å¦ä½å¢å¼ºä¸çæ¨¡åç3Dä¸è´æ§ãç©çè¿è´¯æ§åæ¶é´æ¨çè½åï¼ä»¥å®ç°æ´å¿ å®çç¯å¢æ¨¡æã
*   å°WoWä½ä¸ºè®¤ç¥æ²çï¼è¿ä¸æ­¥æ¢ç´¢å¶å¨VLMä»»å¡è§åä¸­çåºç¨ï¼éè¿æ¨¡æåé¦å¸®å©VLMè°è¯é»è¾è°¬è¯¯ï¼æåé¿ç¨è§åè½åã
*   å°æ¨¡åãæ°æ®ååºåå¼æºï¼ä¸ºå·èº«ä¸çæ¨¡åé¢åçæªæ¥ç ç©¶æä¾åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories.</li>
<li>Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions.</li>
<li>We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22642v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22642v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22631v1'></a></p>
<h2 id="labeling-copilot-a-deep-research-agent-for-automated-data-curation-in-computer-vision"><a href="https://arxiv.org/abs/2509.22631v1">LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</a></h2>
<p><strong>Authors:</strong> Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen, Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru, Vipin Chaudhary</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Curating high-quality, domain-specific datasets is a major bottleneck for
deploying robust vision systems, requiring complex trade-offs between data
quality, diversity, and cost when researching vast, unlabeled data lakes. We
introduce Labeling Copilot, the first data curation deep research agent for
computer vision. A central orchestrator agent, powered by a large multimodal
language model, uses multi-step reasoning to execute specialized tools across
three core capabilities: (1) Calibrated Discovery sources relevant,
in-distribution data from large repositories; (2) Controllable Synthesis
generates novel data for rare scenarios with robust filtering; and (3)
Consensus Annotation produces accurate labels by orchestrating multiple
foundation models via a novel consensus mechanism incorporating non-maximum
suppression and voting. Our large-scale validation proves the effectiveness of
Labeling Copilot's components. The Consensus Annotation module excels at object
discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per
image-nearly double the 7.4 ground-truth objects-achieving a final annotation
mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class
imbalance to discover 903 new bounding box categories, expanding its capability
to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a
10-million sample scale, features an active learning strategy that is up to 40x
more computationally efficient than alternatives with equivalent sample
efficiency. These experiments validate that an agentic workflow with optimized,
scalable tools provides a robust foundation for curating industrial-scale
datasets.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Debargha Gangulyç­äººæ°åçè®ºæâLABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Visionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="labeling-copilot">è®ºææè¦ï¼LABELING COPILOT: è®¡ç®æºè§è§èªå¨åæ°æ®ç®¡çæ·±åº¦ç ç©¶ä»£ç</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®¡ç®æºè§è§é¢åä¸­ä¸ä¸ªå³é®çç¶é¢ï¼é«è´¨éãé¢åç¹å®æ°æ®éçç­å±ãå¨å¤çåºå¤§ãæªæ è®°çæ°æ®æ¹æ¶ï¼æ°æ®è´¨éãå¤æ ·æ§åææ¬ä¹é´éè¦è¿è¡å¤æçæè¡¡ï¼è¿ä¸¥éé»ç¢äºé²æ£è§è§ç³»ç»çé¨ç½²åå¯æ©å±æ§ãä¼ ç»çç®¡éå¼æ¹æ³æ æ³ææåºå¯¹è¿ç§å¨æä¸éè¦æºè½å³ç­çææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
Labeling Copilotå¼å¥äºé¦ä¸ªç¨äºè®¡ç®æºè§è§æ°æ®ç­å±çæ·±åº¦ç ç©¶ä»£çï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>Agenticæ¡æ¶ï¼</strong> è®ºææåºå¹¶å®ç°äºä¸ä¸ªæ°é¢çAgenticç³»ç»ï¼å°æ°æ®æ£ç´¢ãåæåæ æ³¨ç»ä¸å°ä¸ä¸ªè¿è´¯çãç®æ é©±å¨çå·¥ä½æµä¸­ãä¸ä¸ªç±å¤§åå¤æ¨¡æè¯­è¨æ¨¡åé©±å¨çä¸­å¤®åè°ä»£çï¼éè¿å¤æ­¥æ¨çæ¥æ§è¡ä¸ä¸å·¥å·ã
*   <strong>æ ¡ååç°å·¥å·ï¼Calibrated Discoveryï¼ï¼</strong> è¯¥å·¥å·ç»åäºä¸»å¨å­¦ä¹ ç­ç¥ååå¸å¤ï¼OODï¼æ£æµï¼è½å¤ä»å¤§åæ°æ®å­å¨åºä¸­é«æå°åç°ç¸å³ãåå¸åçæ°æ®ãå®éè¿å°ç»å¸ä¸»å¨å­¦ä¹ ç®æ³éæä¸ºåºäºFAISSçè¿ä¼¼æè¿é»æ¡æ¶ï¼å®ç°äºå¨åä¸çº§æ ·æ¬è§æ¨¡ä¸çå¯æ©å±æ§ã
*   <strong>å¯æ§åæå·¥å·ï¼Controllable Synthesisï¼ï¼</strong> ä»£çå©ç¨æä»¤éµå¾ªæ©æ£æ¨¡ååå¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼çæéå¯¹ç¨æåºæ¯çæ°é¢æ°æ®ï¼å¹¶è¿è¡é²æ£è¿æ»¤ï¼ä»¥è§£å³æ°æ®å¤æ ·æ§ä¸è¶³çé®é¢ã
*   <strong>å±è¯æ æ³¨ç­ç¥ï¼Consensus Annotationï¼ï¼</strong> ä»£çéè¿åè°å¤ä¸ªåºç¡æ¨¡åï¼å¦DETICãGroundingDINOï¼ä½ä¸ºç¬ç«âä¸å®¶âå·¥å·ï¼å¹¶éç¨æ°é¢çå±è¯æºå¶ï¼ç»åéæå¤§å¼æå¶åæç¥¨ï¼ï¼ä»åæçå¼±æ ç­¾ä¸­çæåç¡®ãé«è´¨éçä¼ªæ ç­¾ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
Labeling Copilotçç»ä»¶ç»è¿å¤§è§æ¨¡éªè¯ï¼è¯æäºå¶æææ§ï¼
*   <strong>å±è¯æ æ³¨æ¨¡åå¨ç®æ åç°æ¹é¢è¡¨ç°åºè²ï¼</strong> å¨å¯éçCOCOæ°æ®éä¸ï¼å®å¹³åæ¯å¾åçæ14.2ä¸ªåéææ¡ï¼å ä¹æ¯7.4ä¸ªçå®å¯¹è±¡çä¸¤åï¼ï¼æç»æ æ³¨mAPè¾¾å°37.1%ã
*   <strong>å¤çæç«¯ç±»å«ä¸å¹³è¡¡ï¼</strong> å¨ç½ç»è§æ¨¡çOpen Imagesæ°æ®éä¸ï¼è¯¥æ¨¡åæååç°äº903ä¸ªæ°çè¾¹çæ¡ç±»å«ï¼ä½¿å¶æ»è½åæ©å±å°1500å¤ä¸ªã
*   <strong>æ ¡ååç°å·¥å·çè®¡ç®æçï¼</strong> å¨åä¸çº§æ ·æ¬è§æ¨¡ä¸æµè¯æ¶ï¼è¯¥å·¥å·çä¸»å¨å­¦ä¹ ç­ç¥æ¯å·æåç­æ ·æ¬æççæ¿ä»£æ¹æ¡è®¡ç®æçé«åº40åã
*   <strong>Agenticå·¥ä½æµçé²æ£æ§ï¼</strong> è¿äºå®éªéªè¯äºAgenticå·¥ä½æµä¸ä¼åãå¯æ©å±å·¥å·ç¸ç»åï¼ä¸ºç­å±å·¥ä¸çº§æ°æ®éæä¾äºåå®çåºç¡ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æ°æ®ç­å±çå¤ææ§ï¼</strong> è®ºææåºï¼æ°æ®ç­å±æ¯ä¸ä¸ªå¨æææï¼éè¦æºè½å³ç­ï¼èéçº¿æ§ç®¡éãè¿æ¬èº«å°±æ¯Agenticæ¡æ¶è¯ççåå ï¼ä½åæ¶ä¹æå³çè¯¥è¿ç¨åºæçå¤ææ§ã
*   <strong>ç²¾åº¦-å¬åçæè¡¡ï¼</strong> å¨å¤§è§æ¨¡å¤ç±»å«ç¯å¢ä¸­ï¼ç®¡çç²¾åº¦-å¬åççæè¡¡æ¯ä¸ä¸ªæ ¹æ¬æ§ææï¼å°¤å¶æ¯å¨Open Imagesæ°æ®éä¸ï¼å°½ç®¡å¬åçé«ï¼ä½ç²¾åº¦è¾ä½ï¼é¢æµå¯¹è±¡æ°éè¿é«äºæ­£ç¡®æ£æµæ°éã
*   <strong>è¯ä¼°ææï¼</strong> è¯ä¼°ä¸ä¸ªé¿æè¿è¡ãèªä¸»çä»£çï¼å¦Labeling Copilotï¼æåºäºéå¹³å¡çåºåææï¼å ä¸ºå¶ç®æ æ¯çææ°æ®éï¼è¿æ¯ä¸ä¸ªå·æå¹¿éåå¼æ¾å¼è¡å¨ç©ºé´çä»»å¡ãä¼ ç»çç«¯å°ç«¯è¯ä¼°å¯è½æ æ³ææä»£çå¨ä¸åé¢åä¸­çéç¨æç¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å·¥å·æ©å±æ§ï¼</strong> è®ºææå°å¯ä»¥è½»æ¾æ·»å æ°çâåå§å·¥å·âï¼ä¾å¦âæ°æ®ä¿®å¤âå·¥å·ç¨äºæ¥æ¾åä¿®å¤æ æ³¨éè¯¯ï¼æâæ°æ®éç§âå·¥å·ç¨äºèªå¨æ¨¡ç³ææä¿¡æ¯ã
*   <strong>è§£è¦æºè½ï¼</strong> ä»£ççæ¨çï¼ä½æ¶åææ´å¤æ°æ®ï¼ä¸å·¥å·çæ§è¡æ¯åç¦»çï¼è¿ä½¿å¾ä»£ççæç¥æºè½å¯ä»¥ç¬ç«äºå·¥å·çè½åè¿è¡æ¹è¿ãæªæ¥çç ç©¶å¯ä»¥è¿ä¸æ­¥æ¢ç´¢å¦ä½ä¼åè¿ç§è§£è¦ã
*   <strong>è¿­ä»£éåºï¼</strong> ä»£çéè¿ç»åå·¥å·è¾åºåäººå·¥åé¦ï¼å­¦ä¹ åéåºï¼å¨è¿­ä»£å¨æä¸­éæ­¥æ¹è¿æ°æ®éãæªæ¥çå·¥ä½å¯ä»¥è¿ä¸æ­¥ä¼åè¿ç§äººæºåä½åè¿­ä»£å­¦ä¹ æºå¶ã
*   <strong>æ´å¹¿æ³çCVä»»å¡åºç¨ï¼</strong> ä»£çåå¶æ ¸å¿å·¥å·åå¯å¹¿æ³åºç¨äºä»ç®æ æ£æµå°å¨æ¯åå²ç­åç§CVä»»å¡ï¼æªæ¥çç ç©¶å¯ä»¥æ¢ç´¢å¶å¨æ´å¤ç¹å®é¢åï¼å¦å»å­¦å½±åãå·¥ä¸æ£æµï¼ä¸­çå®å¶ååºç¨ã</p>
<hr />
<p>æ»èè¨ä¹ï¼Labeling Copilotéè¿å¼å¥ä¸ä¸ªåæ°çAgenticæ¡æ¶ï¼å°æ°æ®åç°ãåæåæ æ³¨æ´åå°ä¸ä¸ªç»ä¸çãæºè½é©±å¨çå·¥ä½æµä¸­ï¼æ¾èæåäºè®¡ç®æºè§è§æ°æ®éç­å±çæçãå¯æ©å±æ§åè´¨éãå¶æ¨¡ååè®¾è®¡åå¯¹å¤§è§æ¨¡æ°æ®çå¤çè½åï¼ä½¿å¶æä¸ºè§£å³å·¥ä¸çº§æ°æ®ç­å±ææçå¼ºå¤§å·¥å·ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>A central orchestrator agent, powered by a large multimodal
language model, uses multi-step reasoning to execute specialized tools across
three core capabilities: (1) Calibrated Discovery sources relevant,
in-distribution data from large repositories; (2) Controllable Synthesis
generates novel data for rare scenarios with robust filtering; and (3)
Consensus Annotation produces accurate labels by orchestrating multiple
foundation models via a novel consensus mechanism incorporating non-maximum
suppression and voting.</li>
<li>On the web-scale Open Images dataset, it navigated extreme class
imbalance to discover 903 new bounding box categories, expanding its capability
to over 1500 total.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22631v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22631v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22627v1'></a></p>
<h2 id="ccnext-an-effective-self-supervised-stereo-depth-estimation-approach"><a href="https://arxiv.org/abs/2509.22627v1">CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach</a></h2>
<p><strong>Authors:</strong> Alexandre Lopes, Roberto Souza, Helio Pedrini</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Depth Estimation plays a crucial role in recent applications in robotics,
autonomous vehicles, and augmented reality. These scenarios commonly operate
under constraints imposed by computational power. Stereo image pairs offer an
effective solution for depth estimation since it only needs to estimate the
disparity of pixels in image pairs to determine the depth in a known rectified
system. Due to the difficulty in acquiring reliable ground-truth depth data
across diverse scenarios, self-supervised techniques emerge as a solution,
particularly when large unlabeled datasets are available. We propose a novel
self-supervised convolutional approach that outperforms existing
state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) while balancing computational cost. The proposed CCNeXt architecture
employs a modern CNN feature extractor with a novel windowed epipolar
cross-attention module in the encoder, complemented by a comprehensive redesign
of the depth estimation decoder. Our experiments demonstrate that CCNeXt
achieves competitive metrics on the KITTI Eigen Split test data while being
10.18<script type="math/tex">\times</script> faster than the current best model and achieves state-of-the-art
results in all metrics in the KITTI Eigen Split Improved Ground Truth and
Driving Stereo datasets when compared to recently proposed techniques. To
ensure complete reproducibility, our project is accessible at
\href{https://github.com/alelopes/CCNext}{\texttt{https://github.com/alelopes/CCNext}}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Alexandre Lopes, Roberto Souza, Helio Pedriniæ°åçè®ºæâCCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approachâçå¨é¢æè¦ã</p>
<hr />
<h3 id="ccnext">è®ºææè¦ï¼CCNeXt: ä¸ç§ææçèªçç£ç«ä½æ·±åº¦ä¼°è®¡æ¹æ³</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³èªçç£ç«ä½æ·±åº¦ä¼°è®¡é¢åä¸­å­å¨çå³é®ææï¼å¦ä½å¨å®ç°æåè¿çæ·±åº¦ä¼°è®¡åç¡®æ§çåæ¶ï¼æ¾èéä½è®¡ç®ææ¬åè¿è¡æ¶ï¼ä»¥æ»¡è¶³æºå¨äººãèªå¨é©¾é©¶åå¢å¼ºç°å®ç­åºç¨å¯¹å®æ¶æ§åè®¡ç®æççéæ±ãç°æçæ¹æ³å¾å¾å¨Transformer-basedæ¨¡åçé«ç²¾åº¦ä¸CNN-basedæ¨¡åçä½è®¡ç®ææ¬ä¹é´å­å¨æè¡¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
CCNeXtå¼å¥äºä»¥ä¸å³é®åæ°æ¥è§£å³ä¸è¿°é®é¢ï¼
*   <strong>æ°åç¼ç å¨-è§£ç å¨æ¶æï¼</strong> æåºäºä¸ç§åºäºConvNeXtçèªçç£ç«ä½æ·±åº¦ä¼°è®¡ç¼ç å¨-è§£ç å¨æ¶æãConvNeXtä½ä¸ºéª¨å¹²ç½ç»ï¼ç¸æ¯ResNetåTransformer-basedæ¨¡åï¼å¨æ§è½åæçä¹é´åå¾äºæ´å¥½çå¹³è¡¡ã
*   <strong>çªå£åæçº¿äº¤åæ³¨æåæ¨¡åï¼Windowed Epipolar Cross-Attentionï¼ï¼</strong> å¨ç¼ç å¨ä¸­å¼å¥äºä¸ç§æ°é¢ççªå£åæçº¿äº¤åæ³¨æåæºå¶ãè¯¥æºå¶å©ç¨ç«ä½å ä½ç¹æ§ï¼å°æ³¨æåéå¶å¨æææçº¿åéèå´åï¼ä»èæ¾èåå°è®¡ç®å¤ææ§ï¼å¹¶é²æ­¢å ä½ä¸ä¸åççéè¯¯å¹éã
*   <strong>æ·±åº¦ä¼°è®¡è§£ç å¨çå¨é¢éæ°è®¾è®¡ï¼ICEPï¼ï¼</strong> æåºäºä¸ç§åä¸ºâIndividual Contextual Expansive Path (ICEP)âçè½»éçº§è§£ç å¨ï¼éè¿æ¹è¿çå·ç§¯å±ä½ç½®åè·³è·è¿æ¥ï¼å¢å¼ºäºé«åè¾¨çè¾åºçè´¨éååº¦éï¼åæ¶éä½äºæ§è¡æ¶é´ãå®éè¿å»¶è¿æé«å±è·³è·è¿æ¥çèåï¼ç¡®ä¿ç²¾ç»å°ºåº¦ç¹å¾æ´å¥½å°ç¨äºæç»é¢æµï¼å¹¶ä½¿ç¨åé¨è·³è·åä¿çæ¢¯åº¦æµã
*   <strong>è®¡ç®æçä¸åç¡®æ§å¹³è¡¡ï¼</strong> CCNeXtæç¡®è®¾è®¡ç¨äºå¹³è¡¡åº¦éæ§è½åè®¡ç®æçï¼æ¨å¨è¶è¶ç°æCNNåViTæ¶æï¼åæ¶æ¾èæé«è¿è¡éåº¦ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>KITTI Eigen Splitæ°æ®éï¼</strong> CCNeXtå¨KITTI Eigen Splitæµè¯æ°æ®ä¸åå¾äºå·æç«äºåçææ ï¼å¹¶ä¸æ¯å½åæä½³æ¨¡åå¿«10.18åã
*   <strong>KITTI Eigen Split Improved Ground TruthåDriving Stereoæ°æ®éï¼</strong> å¨KITTI Eigen Split Improved Ground TruthåDriving Stereoæ°æ®éçææææ ä¸ï¼CCNeXtåè¾¾å°äºæåè¿çæ§è½ã
*   <strong>è®¡ç®æçï¼</strong> éè¿FLOPsåæï¼CCNeXtçå®æ´æ¶æFLOPsè¿ä½äºChiTransformerï¼65.942 GFLOPs vs. 665 GFLOPsï¼ï¼å¶çªå£åäº¤åæ³¨æåæ¨¡åç¸æ¯æ åå¨è¡äº¤åæ³¨æåï¼è®¡ç®ææ¬éä½äº70.6%ã
*   <strong>ç»è®¡å­¦æä¹ï¼</strong> å¯¹KITTI Eigen Splitçç»è®¡åæè¡¨æï¼CCNeXtå¨AbsRelãSqRelãRMSEåÎ´ &lt; 1.25ç­ææ ä¸è¡¨ç°åºç»è®¡å­¦ä¸çæ¾èä¼è¶æ§ã
*   <strong>å¤©æ°æ¡ä»¶ä¸çé²æ£æ§ï¼</strong> å¨DrivingStereoæ°æ®éçå¤©æ°å­éï¼å¤äºãå¤é¾ãæ´å¤©ï¼ä¸ï¼CCNeXtè¡¨ç°åºä¸è´çä¼å¼æ§è½ï¼ä»å¨é¨å¤©æ¡ä»¶ä¸æ§è½ç¥æä¸éï¼çªæ¾äºå¶å¨ä¸åç¯å¢ä¸çé²æ£æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ¨çå¯¹æªè§æ°æ®éçä¾èµï¼</strong> æ¨¡åå¨æ¨çæ¶ä¾èµäºç¸æºç³»ç»çåé¨åæ°åå¤åï¼è¿ä½¿å¾å¨æªè§æ°æ®éä¸è¿è¡æ¨çæ¶å¯è½åºç°é®é¢ï¼å°¤å¶æ¯å¨ä½¿ç¨é¢æµçéæ°ç¼©æ¾è§å·®æ¶ã
*   <strong>å¤æ°æ®éè®­ç»çææï¼</strong> ç±»ä¼¼å°ï¼å¨å¤ä¸ªæ°æ®éä¸è¿è¡è®­ç»æ¶ï¼éæ°ç¼©æ¾è§å·®çéå¶ä¹å­å¨ã
*   <strong>å¯¹æ ¡æ­£å¾åçä¾èµï¼</strong> æ¨¡åéè¦ä¸å¯¹æ ¡æ­£è¿çå¾åæ¥è¿è¡æ·±åº¦åè§å·®å¹éï¼ä¸éç¨äºæªæ ¡æ­£çå¾åå¯¹ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´åå¯¹æ¯å­¦ä¹ æèªè¸é¦ç®æ ï¼</strong> å°å ä½çº¦ææ³¨æåä¸å¯¹æ¯å­¦ä¹ æèªè¸é¦ç®æ ç¸ç»åï¼ä»¥è¿ä¸æ­¥æé«è¡¨å¾è´¨éï¼èæ éå¢å çç£è¦æ±ã
*   <strong>æ´è½»éçº§çæ¶æï¼</strong> æ¢ç´¢æ´è½»éçº§çæ¶æï¼ä»¥å®ç°å¨åµå¥å¼å¹³å°ä¸é¨ç½²ï¼æ»¡è¶³ä¸¥æ ¼çå»¶è¿ååèé¢ç®ã
*   <strong>æ¶å£ç¯å¢ä¸çé²æ£æ§ï¼</strong> è§£å³å¨æç«¯å¤©æ°ãå¤é´æä¼ æå¨åªå£°ç­ä¸å©ç¯å¢å ç´ ä¸çé²æ£æ§ææï¼è¿éè¦æ´å¤§è§æ¨¡çæ°æ®éåèªéåºå­¦ä¹ ç­ç¥ã
*   <strong>å¨çº¿æ ¡æ­£æ¹æ³ï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¨çº¿æ ¡æ­£æ¹æ³ï¼ä»¥å¤çæªæ ¡æ­£çç«ä½å¾åå¯¹ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥åæ°çConvNeXtéª¨å¹²ç½ç»ãçªå£åæçº¿äº¤åæ³¨æåæºå¶åéæ°è®¾è®¡çè§£ç å¨ï¼æåå°å¨èªçç£ç«ä½æ·±åº¦ä¼°è®¡é¢åå®ç°äºæ§è½åæççæ¾èæåï¼ä¸ºå®æ¶åºç¨æä¾äºæ´å¯è¡çè§£å³æ¹æ¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a novel
self-supervised convolutional approach that outperforms existing
state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) while balancing computational cost.</li>
<li>The proposed CCNeXt architecture
employs a modern CNN feature extractor with a novel windowed epipolar
cross-attention module in the encoder, complemented by a comprehensive redesign
of the depth estimation decoder.</li>
<li>Our experiments demonstrate that CCNeXt
achieves competitive metrics on the KITTI Eigen Split test data while being
10.18<script type="math/tex">\times</script> faster than the current best model and achieves state-of-the-art
results in all metrics in the KITTI Eigen Split Improved Ground Truth and
Driving Stereo datasets when compared to recently proposed techniques.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22627v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22627v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22496v1'></a></p>
<h2 id="where-mllms-attend-and-what-they-rely-on-explaining-autoregressive-token-generation"><a href="https://arxiv.org/abs/2509.22496v1">Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation</a></h2>
<p><strong>Authors:</strong> Ruoyu Chen, Xiaoqing Guo, Kangwei Liu, Siyuan Liang, Shiming Liu, Qunli Zhang, Hua Zhang, Xiaochun Cao</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in aligning visual inputs with natural language outputs. Yet, the
extent to which generated tokens depend on visual modalities remains poorly
understood, limiting interpretability and reliability. In this work, we present
EAGLE, a lightweight black-box framework for explaining autoregressive token
generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual
regions while quantifying the relative influence of language priors and
perceptual evidence. The framework introduces an objective function that
unifies sufficiency (insight score) and indispensability (necessity score),
optimized via greedy search over sparsified image regions for faithful and
efficient attribution. Beyond spatial attribution, EAGLE performs
modality-aware analysis that disentangles what tokens rely on, providing
fine-grained interpretability of model decisions. Extensive experiments across
open-source MLLMs show that EAGLE consistently outperforms existing methods in
faithfulness, localization, and hallucination diagnosis, while requiring
substantially less GPU memory. These results highlight its effectiveness and
practicality for advancing the interpretability of MLLMs. The code is available
at https://github.com/RuoyuChen10/EAGLE.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Ruoyu Chenç­äººæ°åçè®ºæâWhere MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generationâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨å°è§è§è¾å¥ä¸èªç¶è¯­è¨è¾åºå¯¹é½æ¹é¢è¡¨ç°åºè²ï¼ä½å¶çæçææ¬å¨å¤å¤§ç¨åº¦ä¸ä¾èµäºè§è§æ¨¡æä»ä¸æ¸æ¥ï¼è¿éå¶äºæ¨¡åçå¯è§£éæ§åå¯é æ§ãæ­¤å¤ï¼MLLMså®¹æäº§çå¹»è§ï¼è¿å¨å®å¨å³é®é¢åï¼å¦å»çä¿å¥åèªå¨é©¾é©¶ï¼ä¸­ä¼æå®³ä¿¡ä»»ãå æ­¤ï¼è¯¥ç ç©¶æ¨å¨å¼åä¸ç§é«æä¸å¿ å®çå½å æ¹æ³ï¼ä»¥æé«å³ç­éæåº¦ï¼è¯æ­éè¯¯ï¼å¹¶å¢å¼ºMLLMsçå®å¨æ§ä¸å¯ä¿¡åº¦ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªåä¸ºEAGLEï¼Explaining Autoregressive Generation by Language priors or Evidenceï¼çè½»éçº§é»çå½å æ¡æ¶ï¼ç¨äºè§£éMLLMsä¸­çèªåå½ä»¤ççæãå¶ä¸»è¦åæ°åæ¬ï¼
*   <strong>ç»ä¸çå®¢è§å½æ°ï¼</strong> EAGLEå¼å¥äºä¸ä¸ªç»åäºâæ´å¯åå¾åâï¼sufficiencyï¼åâå¿è¦æ§å¾åâï¼indispensabilityï¼çå®¢è§å½æ°ãæ´å¯åå¾åè¡¡éæå°è¾å¥åºåéå¨æå¤§åç®æ æ ç­¾çææ¦çæ¹é¢çååæ§ï¼èå¿è¦æ§å¾ååè¯å«ç§»é¤åä¼æ¾èéä½çææ¦ççå³é®åºåã
*   <strong>è´ªå©ªæç´¢ä¼åï¼</strong> éè¿å¯¹ç¨çå¾ååºåè¿è¡è´ªå©ªæç´¢æ¥ä¼åå®¢è§å½æ°ï¼ä»¥å®ç°å¿ å®é«æçå½å ï¼ä»èæå»ºä¸ä¸ªæåºæåï¼è§£éåªäºæç¥åºåä¿è¿äºMLLMsççæã
*   <strong>æ¨¡ææç¥åæï¼</strong> é¤äºç©ºé´å½å ï¼EAGLEè¿è¿è¡æ¨¡ææç¥åæï¼éåæ¯ä¸ªçæçä»¤çæ¯æ´å¤å°ä¾èµè¯­è¨åéªè¿æ¯æç¥è¯æ®ï¼ä»èæä¾æ´ç»ç²åº¦çæ¨¡åå³ç­å¯è§£éæ§ã
*   <strong>å¹»è§è¯æ­ä¸ç¼è§£ï¼</strong> è¯¥æ¹æ³è½å¤åç¡®è¯å«å¯¼è´å¹»è§çè§è§åç´ ï¼å¹¶éè¿ç§»é¤æå°éçå¹²æ°åºåæ¥ç¼è§£å¹»è§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
å¨LLaVA-1.5ãQwen2.5-VLåInternVL3.5ç­å¼æºMLLMsä¸è¿è¡çå¹¿æ³å®éªè¡¨æï¼EAGLEå¨ä»¥ä¸æ¹é¢æç»­ä¼äºç°ææ¹æ³ï¼
*   <strong>å¿ å®æ§ï¼Faithfulnessï¼ï¼</strong> å¨å¾åå­å¹åVQAä»»å¡ä¸­ï¼EAGLEå¨æå¥åå é¤ææ ä¸æ¾èä¼äºç°ææ¹æ³ï¼ä¾å¦å¨å¾åå­å¹ä»»å¡ä¸­ï¼æå¥åå é¤ææ å¹³ååå«æé«äº20.0%å13.4%ã
*   <strong>å®ä½ï¼Localizationï¼ï¼</strong> å¨Pointing Gameä»»å¡ä¸­ï¼EAGLEå¨ç®±çº§åæ©ç çº§æ æ³¨ä¸ååå¾äºæä½³ç»æï¼è¯æå¶è½å¤å°é¢æµç»æåç¡®å°å®ä½å°ç¹å®å¯¹è±¡ã
*   <strong>å¹»è§è¯æ­ï¼Hallucination Diagnosisï¼ï¼</strong> å¨RePOPEåºåæµè¯ä¸­ï¼EAGLEå¨å¹³åæå°æ ¡æ­£åºåï¼AMCRï¼åé¢ç®ä¸æ ¡æ­£æåçï¼CSR@10%ï¼æ¹é¢æ¾èä¼äºç°ææ¹æ³ï¼è¡¨æå®è½ææè¯å«å¯¼è´å¹»è§çè¾å¥åºåï¼å¹¶éè¿ç§»é¤å°éåºåæ¥çº æ­£å¹»è§ã
*   <strong>GPUåå­æçï¼</strong> EAGLEæéçGPUåå­æ¾èå°äºç°ææ¹æ³ï¼ä¾å¦å¨Qwen2.5-VL 7Bä¸ä»é17.68 GBï¼èIGOS++éè¦96.90 GBï¼è¿å¸æ¾äºå¶å¨ç°ä»£MLLMsä¸­çå®ç¨æ§ã</p>
<p>è¿äºç»æå¼ºè°äºEAGLEå¨æé«MLLMså¯è§£éæ§æ¹é¢çæææ§åå®ç¨æ§ï¼è½å¤æä¾æ´å¿ å®ãèµæºæçæ´é«ä¸äººç±»å¯çè§£çè§£éã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è¯¥ç ç©¶æå°äºä¸¤ä¸ªä¸»è¦å±éæ§ï¼
*   <strong>å¯æ©å±æ§éå¶ï¼</strong> è¿­ä»£å­ééæ©åè´ªå©ªæç´¢ç­ç¥éå¶äºEAGLEä¸è½»éçº§å¯è§åæ¹æ³ç¸æ¯çå¯æ©å±æ§ã
*   <strong>ä¾§éäºè§£éåé¨åç¼è§£ï¼</strong> è¯¥æ¡æ¶ä¸»è¦å³æ³¨å¹»è§çè§£éåé¨åç¼è§£ï¼å°æªæ¢ç´¢ä¸»å¨é¢é²å¹»è§çæ¹æ³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çç ç©¶å°æ¢ç´¢ä»¥ä¸æ¹åï¼
*   <strong>æ´å¿«çæç´¢ç­ç¥ï¼</strong> å¼åæ´å¿«çæç´¢ç­ç¥ä»¥æé«å¯æ©å±æ§ã
*   <strong>è§£éå¼å¯¼çå»åï¼</strong> æ¢ç´¢åºäºè§£éçå»åæ¹æ³ï¼ç¨äºè®­ç»MLLMsï¼ä»¥ä¸»å¨é¢é²å¹»è§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present
EAGLE, a lightweight black-box framework for explaining autoregressive token
generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual
regions while quantifying the relative influence of language priors and
perceptual evidence.</li>
<li>Extensive experiments across
open-source MLLMs show that EAGLE consistently outperforms existing methods in
faithfulness, localization, and hallucination diagnosis, while requiring
substantially less GPU memory.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22496v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22496v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22448v1'></a></p>
<h2 id="-quant-towards-learnable-quantization-for-low-bit-pattern-recognition"><a href="https://arxiv.org/abs/2509.22448v1"><script type="math/tex">Î³</script>-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition</a></h2>
<p><strong>Authors:</strong> Mishal Fatima, Shashank Agnihotri, Marius Bock, Kanchana Vaishnavi Gandikota, Kristof Van Laerhoven, Michael Moeller, Margret Keuper</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Most pattern recognition models are developed on pre-proce-ssed data. In
computer vision, for instance, RGB images processed through image signal
processing (ISP) pipelines designed to cater to human perception are the most
frequent input to image analysis networks. However, many modern vision tasks
operate without a human in the loop, raising the question of whether such
pre-processing is optimal for automated analysis. Similarly, human activity
recognition (HAR) on body-worn sensor data commonly takes normalized
floating-point data arising from a high-bit analog-to-digital converter (ADC)
as an input, despite such an approach being highly inefficient in terms of data
transmission, significantly affecting the battery life of wearable devices. In
this work, we target low-bandwidth and energy-constrained settings where
sensors are limited to low-bit-depth capture. We propose <script type="math/tex">\gamma</script>-Quant,
i.e.~the task-specific learning of a non-linear quantization for pattern
recognition. We exemplify our approach on raw-image object detection as well as
HAR of wearable data, and demonstrate that raw data with a learnable
quantization using as few as 4-bits can perform on par with the use of raw
12-bit data. All code to reproduce our experiments is publicly available via
https://github.com/Mishalfatima/Gamma-Quant</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Mishal Fatimaç­äººæ°åçè®ºæâ<script type="math/tex">\gamma</script>-Quant: Towards Learnable Quantization for Low-bit Pattern Recognitionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="gamma-quant-towards-learnable-quantization-for-low-bit-pattern-recognition">è®ºææè¦ï¼<script type="math/tex">\gamma</script>-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åå¤§å¤æ°æ¨¡å¼è¯å«æ¨¡åä¾èµäºé¢å¤çæ°æ®ï¼ä¾å¦è®¡ç®æºè§è§ä¸­çRGBå¾åç»è¿å¾åä¿¡å·å¤çï¼ISPï¼ç®¡éå¤çä»¥éåºäººç±»æç¥ï¼ä»¥åäººä½æ´»å¨è¯å«ï¼HARï¼ä¸­æ¥èªé«ä½æ¨¡æ°è½¬æ¢å¨ï¼ADCï¼çå½ä¸åæµ®ç¹æ°æ®ãç¶èï¼è¿ç§é¢å¤çå¯¹äºèªå¨ååææ¯å¦æä¼ï¼ä»¥åå¶å¨ä½å¸¦å®½ãä½è½èåºæ¯ï¼å¦å¯ç©¿æ´è®¾å¤ï¼ä¸­çæçä½ä¸ï¼å¯¼è´æ°æ®ä¼ è¾éå¤§ãçµæ± å¯¿å½åå½±åï¼æ¯ä¸ä¸ªå³é®é®é¢ãè®ºææ¨å¨è§£å³ä¼ æå¨åéäºä½ä½æ·±åº¦æè·çåºæ¯ï¼æ¢ç©¶å¦ä½ä¸ºæ¨¡å¼è¯å«ä»»å¡å­¦ä¹ ä¸ç§æä¼çéçº¿æ§éåæ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäº<strong><script type="math/tex">\gamma</script>-Quant</strong>ï¼ä¸ç§éå¯¹ç¹å®ä»»å¡å­¦ä¹ éçº¿æ§éåçæ¹æ³ãå¶æ ¸å¿ææ³æ¯ï¼ä¸æ¯ä½¿ç¨åºå®ççº¿æ§æå¯¹æ°éåï¼èæ¯éè¿ä¸ä¸ªå¯å­¦ä¹ çåæ°<script type="math/tex">\gamma</script>ï¼ç±»ä¼¼äºä¼½é©¬æ ¡æ­£ï¼åä¸ä¸ªåç§»é<script type="math/tex">\mu</script>æ¥åæ°åéåå½æ°ï¼
<script type="math/tex">Q(Î§, Î³, Î¼) = Q_N(\text{sign}(X â Î¼) \cdot |X â Î¼|^Î³)</script>
è¿ä¸ªå¯å­¦ä¹ çéçº¿æ§éåå¨ä¸ç¥ç»ç½ç»ä¸èµ·è¿è¡ä¼åï¼ä»¥éåºç¹å®ä»»å¡ãéè¿ç´éä¼°è®¡å¨ï¼straight-through estimatorï¼å¤çéåæä½çä¸å¯å¾®æ§ï¼ä»èå®ç°åºäºæ¢¯åº¦çä¼åãè¿ç§æ¹æ³åè®¸å¨æ¨¡æä¿¡å·è¢«æ°å­åä¹åï¼å¨ADCå±é¢è¿è¡ä»»å¡ç¹å®çéåå­¦ä¹ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æåï¼</strong> <script type="math/tex">\gamma</script>-Quantå¨åå§å¾åç®æ æ£æµåå¯ç©¿æ´æ°æ®HARä»»å¡ä¸­åè¡¨ç°åºç³»ç»æ§æ¹è¿ã
*   <strong>ä½ä½æ·±åº¦ä¸çé«è¡¨ç°ï¼</strong> è®ºæå±ç¤ºäºä½¿ç¨ä½è³4æ¯ç¹çåå§æ°æ®ï¼éè¿å¯å­¦ä¹ éåï¼å¨ç®æ æ£æµä»»å¡ä¸­å¯ä»¥è¾¾å°ä¸ä½¿ç¨12æ¯ç¹åå§æ°æ®ç¸å½çæ§è½ãå¨HARä»»å¡ä¸­ï¼çè³2æ¯ç¹æ°æ®ä¹è½åå¾ä¸é«æ¯ç¹æ°æ®ç¸è¿çæ§è½ã
*   <strong>è½èåå¸¦å®½ä¼åï¼</strong> è¿ç§æ¹æ³æ¾èåå°äºä¼ æå¨çæ°æ®ä¼ è¾éåè½èï¼å¯¹å¯ç©¿æ´è®¾å¤ç­èµæºåéçåºç¨å·æéè¦æä¹ã
*   <strong>ä¼äºçº¿æ§éåï¼</strong> <script type="math/tex">\gamma</script>-Quantå§ç»ä¼äºåºäºçº¿æ§éåçæ¨¡åï¼å°¤å¶æ¯å¨ä½äº®åº¦åºåçç»èå¤çä¸ï¼è¿å¯¹äºç®æ æ£æµè³å³éè¦ã
*   <strong>å¯¹æ°ä¸<script type="math/tex">\gamma</script>-Quantï¼</strong> è®ºææåºï¼å¨æäºæåµä¸ï¼å¯¹æ°éåï¼éè¿ç²¾å¿éæ©ç<script type="math/tex">\epsilon</script>å¼ï¼çæ§è½ä¸<script type="math/tex">\gamma</script>-Quantç¸å½ï¼ä½<script type="math/tex">\gamma</script>-Quantçä¼å¿å¨äºå¶åæ°æ¯èªå¨å­¦ä¹ çï¼æ éæå¨è°æ´è¶åæ°ï¼ä½¿å¶æ´å·éç¨æ§åéåºæ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æ¨¡æä¿¡å·çä»£çï¼</strong> è®ºææ¿è®¤ï¼ç±äºæ æ³ç´æ¥è·åçæ­£çæ¨¡æä¿¡å·ï¼å¶å·¥ä½ä¸­ä½¿ç¨é«ä½æ·±åº¦RAWå¾åä½ä¸ºæ¨¡æä¿¡å·çä»£çãå°½ç®¡è¿å¨ä¿¡å·å¤ççè®ºä¸­æ¯ææçåè®¾ï¼ä½çæ­£çæ¨¡æä¿¡å·ä¸é«ä½æ·±åº¦æ°å­è¾å¥ä¹é´ä»å­å¨ä¿¡æ¯æå¤±ã
*   <strong>çæ³æµè¯ç¯å¢ï¼</strong> çæ³æåµä¸ï¼ä½èå¸æè½å¨ä¼ æå¨ä¸ç´æ¥æµè¯<script type="math/tex">\gamma</script>-Quantï¼å¤çæ¨¡æä¿¡å·ï¼ä»¥æè·å¾è¿ä¸æ­¥çç²¾åº¦æåã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´çµæ´»çéååæ°åï¼</strong> æ¢ç´¢æ´çµæ´»çéåå½æ°åæ°åï¼ä¾å¦å¨å¯¹æ°éåä¸­å­¦ä¹ <script type="math/tex">\epsilon</script>å¼ã
*   <strong>ç»åç½ç»æ¶æä¼åï¼</strong> å°<script type="math/tex">\gamma</script>-Quantæ¡æ¶ä¸é«æçç½ç»æ¶æåç½ç»éåç¸ç»åï¼ä»¥è¿ä¸æ­¥æé«è®¡ç®æçåæ¨çæ§è½ã
*   <strong>ç´æ¥å¨æ¨¡æä¿¡å·ä¸æµè¯ï¼</strong> å¨æªæ¥ï¼å¦æææ¯åè®¸ï¼ç´æ¥å¨ä¼ æå¨ä¸å¯¹æ¨¡æä¿¡å·åºç¨<script type="math/tex">\gamma</script>-Quantï¼ä»¥éªè¯å¶å¨çå®ä¸çåºæ¯ä¸­çæ½åã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºå¨èµæºåéç¯å¢ä¸è¿è¡æ¨¡å¼è¯å«æä¾äºä¸ç§æ°é¢ä¸é«æçè§£å³æ¹æ¡ãéè¿å¼å¥ä»»å¡ç¹å®çå¯å­¦ä¹ éçº¿æ§éåï¼<script type="math/tex">\gamma</script>-Quantå¨ä¿æçè³è¶è¶é«ä½æ·±åº¦æ°æ®æ§è½çåæ¶ï¼æ¾èéä½äºæ°æ®ä¼ è¾åè½èï¼ä¸ºæå»ºæ´é«æãå¯æç»­çæºå¨å­¦ä¹ å·¥ä½æµå¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose <script type="math/tex">\gamma</script>-Quant,
i.e.~the task-specific learning of a non-linear quantization for pattern
recognition.</li>
<li>We exemplify our approach on raw-image object detection as well as
HAR of wearable data, and demonstrate that raw data with a learnable
quantization using as few as 4-bits can perform on par with the use of raw
12-bit data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22448v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22448v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.22318v1'></a></p>
<h2 id="nifty-a-non-local-image-flow-matching-for-texture-synthesis"><a href="https://arxiv.org/abs/2509.22318v1">NIFTY: a Non-Local Image Flow Matching for Texture Synthesis</a></h2>
<p><strong>Authors:</strong> Pierrick Chatillon, Julien Rabin, David TschumperlÃ©</p>
<p><strong>Published:</strong> 2025-09-26</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>This paper addresses the problem of exemplar-based texture synthesis. We
introduce NIFTY, a hybrid framework that combines recent insights on diffusion
models trained with convolutional neural networks, and classical patch-based
texture optimization techniques. NIFTY is a non-parametric flow-matching model
built on non-local patch matching, which avoids the need for neural network
training while alleviating common shortcomings of patch-based methods, such as
poor initialization or visual artifacts. Experimental results demonstrate the
effectiveness of the proposed approach compared to representative methods from
the literature. Code is available at https://github.com/PierrickCh/Nifty.git</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹Pierrick Chatillon, Julien Rabin, David TschumperlÃ©æ°åçè®ºæâNIFTY: a Non-Local Image Flow Matching for Texture Synthesisâè¿è¡äºå¨é¢åæï¼å¹¶æä¾ä»¥ä¸æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼NIFTY: ä¸ç§ç¨äºçº¹çåæçéå±é¨å¾åæµå¹éæ¹æ³</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åºäºæ ·æ¬ççº¹çåæé®é¢ãä¼ ç»çåºäºè¡¥ä¸ççº¹çä¼åæ¹æ³ï¼å¦Kwatraç­äººçå·¥ä½ï¼è½ç¶ææï¼ä½å¸¸é¢ä¸´åå§åææãè§è§ä¼ªå½±ä»¥åé¾ä»¥ææé¿è·ç¦»ç¸å³æ§ç­é®é¢ãåæ¶ï¼è½ç¶æ©æ£æ¨¡åå¨å¾åçææ¹é¢è¡¨ç°åºè²ï¼ä½å®ä»¬éå¸¸éè¦å¤§éçç¥ç»ç½ç»è®­ç»ï¼ä¸æ¨çè¿ç¨å¯è½è¾æ¢ãNIFTYçç®æ æ¯å¼åä¸ç§æ éç¥ç»ç½ç»è®­ç»ï¼åæ¶åæä¼ ç»åºäºè¡¥ä¸æ¹æ³ç¼ºé·ççº¹çåææ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
NIFTYï¼Non-local Image Flow-matching Texture sYnthesisï¼æ¯ä¸ä¸ªæ··åæ¡æ¶ï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>éåæ°æµå¹éæ¨¡åï¼</strong> NIFTYå°çº¹çä¼åé®é¢éæ°æå»ºä¸ºæµå¹éï¼Flow Matching, FMï¼æ¡æ¶ä¸çä¸ä¸ªæ¶é´ç§¯åé®é¢ï¼ä½é¿åäºå¯¹ç¥ç»ç½ç»çè®­ç»ãå®éè¿æ¾å¼è®¡ç®è¡¥ä¸åå¸ä¸çéåº¦åºæ¥å®ç°æµå¹éï¼èä¸æ¯éè¿CNNè¿ä¼¼ã
*   <strong>éå±é¨è¡¥ä¸å¹éï¼</strong> è¯¥æ¹æ³å©ç¨éå±é¨è¡¥ä¸å¹éæ¥è®¡ç®éåº¦åºï¼è¿ä¸éå±é¨åå¼ï¼Non-Local Meansï¼çææ³ç¸ä¼¼ï¼éè¿èåç¸ä¼¼è¡¥ä¸æ¥çææ°ççº¹çã
*   <strong>æçä¼åç­ç¥ï¼</strong> ä¸ºäºéä½è®¡ç®ææ¬ï¼NIFTYå¼å¥äºå¤å°ºåº¦åæãtop-kæè¿é»ï¼NNï¼éæ ·ä»¥åè®°å¿æºå¶ãtop-k NNéæ ·éå¶äºéåº¦åºè®¡ç®ä¸­èèçè¡¥ä¸æ°éï¼èè®°å¿æºå¶åä¿çäºè¿­ä»£è¿ç¨ä¸­è¡¨ç°æ´å¥½çk-NNç´¢å¼ï¼ä»¥åå°åç»­è®¡ç®éå¹¶æé«ç¨³å®æ§ã
*   <strong>ä¸ä¼ ç»TOçèç³»ï¼</strong> è®ºææåºï¼NIFTYå¨ç¹å®æ¡ä»¶ä¸ï¼ä¾å¦k=1ï¼T=1ï¼ä¸ä¼ ç»ççº¹çä¼åï¼TOï¼ç®æ³æç¸ä¼¼ä¹å¤ï¼ä½éè¿æµçèåèéç´æ¥æè¿é»æ¿æ¢ï¼NIFTYè¡¨ç°åºæ´ç¨³å®çè¡ä¸ºã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>é²æ£æ§åå¾åè´¨éï¼</strong> å®éªç»æè¡¨æï¼NIFTYå¨åå§åé²æ£æ§ãå¾åè´¨éåéåº¦æ¹é¢ä¼äºç°ææ¹æ³ãä¸ä¼ ç»ççº¹çä¼åç®æ³ç¸æ¯ï¼NIFTYè½å¤é¿åå¤å¶åºåä¹é´çå¼ºä¼ªå½±ï¼å¹¶çææ´é¼çççº¹çã
*   <strong>åé æ§åæï¼</strong> NIFTYè½å¤éè¿æ¼æ¥åå§çº¹ççå±é¨å¯æ¬ï¼çææªæ¾è§è¿ä½åççæ°ç»æï¼å±ç¤ºäºå¶å¨åæä¸­çâåé æ§âã
*   <strong>æçæåï¼</strong> éè¿top-kéæ ·åè®°å¿æºå¶ï¼NIFTYå¨ä¿æé«è´¨éåæçåæ¶ï¼æ¾èåå°äºè®¡ç®ææ¬åæéçæ­¥éª¤æ°éãå®éåæï¼å¦W2è·ç¦»ãSIFIDãèªç¸å³æ§ï¼è¯å®äºNIFTYå¨è´¨éåéåº¦ä¸ä¼äºTOåå°åU-Netæ¨¡åã
*   <strong>æå¼è½åï¼</strong> è®ºæè¿å±ç¤ºäºNIFTYå¨çº¹çæ··åæ¹é¢çåºç¨ï¼åæ¬åå¸çº§æ··åãåç´ çº§æ··ååç©ºé´æå¼ï¼è¯æäºå¶å¨çæä¸åçº¹çä¹é´å¹³æ»è¿æ¸¡çè½åã
*   <strong>æ½å¨ç©ºé´åºç¨ï¼</strong> NIFTYå¯ä»¥ä¸é¢è®­ç»çèªç¼ç å¨ç»åï¼å¨æ½å¨ç©ºé´ä¸­è¿è¡çº¹çåæï¼è¿è¡¨æäºå¶æ¹æ³çéç¨æ§åæ©å±æ½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®ååºNIFTYæ¹æ³çå·ä½å±éæ§ï¼ä½å¯ä»¥ä»å¶è®¾è®¡åæ¯è¾ä¸­æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼
*   <strong>è®¡ç®ææ¬ï¼</strong> å°½ç®¡NIFTYéè¿top-k NNåè®°å¿æºå¶éä½äºè®¡ç®ææ¬ï¼ä½å¯¹äºéå¸¸å¤§çå¾åæéè¦æé«ç²¾åº¦çåºæ¯ï¼éå±é¨è¡¥ä¸å¹éçè®¡ç®éä»ç¶å¯è½æ¯ä¸ä¸ªææã
*   <strong>è¶åæ°æææ§ï¼</strong> å°½ç®¡NIFTYæ¯ä¼ ç»TOå¯¹åå§åä¸é£ä¹ææï¼ä½å¶æ§è½å¯è½ä»ç¶ä¾èµäºè¡¥ä¸å¤§å°ãæ­¥é¿ãkå¼ãæ¶é´æ­¥æ°Tç­è¶åæ°çéæ©ã
*   <strong>é¿è·ç¦»ç¸å³æ§ï¼</strong> å°½ç®¡NIFTYéè¿éå±é¨å¹éè¯å¾ææé¿è·ç¦»ç¸å³æ§ï¼ä½å¯¹äºå·æéå¸¸å¤ææé«åº¦ç»æåå¨å±æ¨¡å¼ççº¹çï¼å¶ææè½åå¯è½ä»ææåç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ½å¨ç©ºé´ä¸­çè¡¥ä¸æµå¹éï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¨åéçæ½å¨ç©ºé´ä¸­è¿è¡è¡¥ä¸æµå¹éï¼ä»¥å©ç¨æ´æ½è±¡çç¹å¾è¡¨ç¤ºã
*   <strong>éææ´å¤æçå½çº³åç½®ï¼</strong> æªæ¥çå·¥ä½å¯ä»¥èèéææ´å¤æçå½çº³åç½®ï¼ä¾å¦éè¿éå±é¨è¡¥ä¸èåå»ºæ¨¡æ³¨æåæ¨¡åï¼ä»¥è¿ä¸æ­¥æåæ¨¡åçæ§è½åæ³åè½åã
*   <strong>å®æ¶ææ´å¿«éçåæï¼</strong> æ¢ç´¢è¿ä¸æ­¥ä¼åç®æ³ï¼ä»¥å®ç°æ´å¿«çåæéåº¦ï¼çè³å®æ¶çº¹ççæã
*   <strong>å¤çéå¹³ç¨³çº¹çï¼</strong> å°½ç®¡è®ºæä¸»è¦å³æ³¨å¹³ç¨³çº¹çï¼ä½NIFTYçéå±é¨ç¹æ§å¯è½ä½¿å¶ææ½åæ©å±å°éå¹³ç¨³çº¹ççåæã</p>
<p>æ»èè¨ä¹ï¼NIFTYæåºäºä¸ç§æ°é¢çéåæ°æµå¹éæ¹æ³ï¼éè¿å°æ©æ£æ¨¡ååä¼ ç»è¡¥ä¸ä¼åææ¯ç¸ç»åï¼å¨æ éç¥ç»ç½ç»è®­ç»çæåµä¸ï¼ææå°è§£å³äºåºäºæ ·æ¬ççº¹çåæé®é¢ï¼å¹¶åå¾äºä»¤äººä¿¡æçå®éªç»æã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Experimental results demonstrate the
effectiveness of the proposed approach compared to representative methods from
the literature.</li>
<li>Code is available at https://github.com/PierrickCh/Nifty.git</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.22318v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.22318v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-29 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
