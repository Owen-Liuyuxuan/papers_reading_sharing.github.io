<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-14 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-13/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-14">Arxiv Computer Vision Papers - 2025-11-14</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#right-looks-wrong-reasons-compositional-fidelity-in-text-to-image-generation" class="nav-link">Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#pan-a-world-model-for-general-interactable-and-long-horizon-world-simulation" class="nav-link">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</a>
                </li>
                <li class="nav-item">
                    <a href="#depth-anything-3-recovering-the-visual-space-from-any-views" class="nav-link">Depth Anything 3: Recovering the Visual Space from Any Views</a>
                </li>
                <li class="nav-item">
                    <a href="#semanticvla-semantic-aligned-sparsification-and-enhancement-for-efficient-robotic-manipulation" class="nav-link">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#spot-sparsification-with-attention-dynamics-via-token-relevance-in-vision-transformers" class="nav-link">SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#nuplan-r-a-closed-loop-planning-benchmark-for-autonomous-driving-via-reactive-multi-agent-simulation" class="nav-link">nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</a>
                </li>
                <li class="nav-item">
                    <a href="#groundiff-diffusion-based-ground-surface-generation-from-digital-surface-models" class="nav-link">GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</a>
                </li>
                <li class="nav-item">
                    <a href="#msgnav-unleashing-the-power-of-multi-modal-3d-scene-graph-for-zero-shot-embodied-navigation" class="nav-link">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a>
                </li>
                <li class="nav-item">
                    <a href="#propa-toward-process-level-optimization-in-visual-reasoning-via-reinforcement-learning" class="nav-link">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#robobenchmart-benchmarking-robots-in-retail-environment" class="nav-link">RoboBenchMart: Benchmarking Robots in Retail Environment</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-14">Arxiv Computer Vision Papers - 2025-11-14</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份为忙碌的研究人员准备的 Arxiv 计算机视觉领域每日报告执行摘要，涵盖了 2025 年 11 月 13 日发布的 10 篇论文：</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025-11-13)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文主要围绕以下几个核心主题展开：</p>
<ul>
<li><strong>世界模型与通用模拟 (World Models &amp; General Simulation):</strong> 多篇论文致力于构建更通用、交互性更强、时间跨度更长的世界模型，以支持更复杂的模拟和具身智能。</li>
<li><strong>高效与鲁棒的感知 (Efficient &amp; Robust Perception):</strong> 深度估计、语义理解和视觉 Transformer 优化是关键，旨在提高模型在各种复杂环境下的感知能力和效率。</li>
<li><strong>具身智能与机器人操作 (Embodied AI &amp; Robotic Manipulation):</strong> 机器人导航、操作和基准测试是重要方向，强调将视觉感知与实际物理交互相结合。</li>
<li><strong>生成模型与数据合成 (Generative Models &amp; Data Synthesis):</strong> 文本到图像生成和地面表面生成等领域继续探索如何生成高质量、高保真度的数据。</li>
<li><strong>自动驾驶与多智能体模拟 (Autonomous Driving &amp; Multi-Agent Simulation):</strong> 自动驾驶的闭环规划和多智能体交互模拟是该领域的热点。</li>
</ul>
<p><strong>2. 特别重要或创新的论文亮点：</strong></p>
<ul>
<li><strong>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation (PAN Team et al.)</strong>: 这篇论文标题直接点明了其雄心壮志，旨在构建一个<strong>通用、可交互且长时程的世界模型</strong>。如果能有效实现，这将是具身智能和模拟领域的一个重大突破，可能为训练更智能的AI代理提供一个强大的平台。</li>
<li><strong>Depth Anything 3: Recovering the Visual Space from Any Views (Haotong Lin et al.)</strong>: 作为“Depth Anything”系列的最新迭代，这篇论文承诺从“任何视角”恢复视觉空间，暗示了其在<strong>通用深度估计</strong>方面的强大能力和鲁棒性，可能在各种应用中提供更可靠的3D感知。</li>
<li><strong>MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation (Xun Huang et al.)</strong>: 该论文利用<strong>多模态3D场景图</strong>实现<strong>零样本具身导航</strong>，这是一个非常前沿且具有挑战性的方向。通过语义丰富的场景表示，有望大幅提升机器人在未知环境中的泛化导航能力。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>通用世界模型 (General World Models):</strong> "PAN" 论文的出现表明，构建能够模拟复杂物理世界和交互的通用模型正成为一个核心目标。</li>
<li><strong>多模态3D场景图 (Multi-modal 3D Scene Graphs):</strong> 结合视觉、语言和其他模态来构建语义丰富的3D场景表示，以支持更高级的推理和决策，尤其在具身智能中潜力巨大。</li>
<li><strong>基于注意力机制的稀疏化 (Sparsification with Attention Dynamics):</strong> "SPOT" 论文利用注意力动态来优化 Vision Transformers 的效率，这表明在保持性能的同时，对模型效率的追求仍在持续深化。</li>
<li><strong>流程级优化 (Process-level Optimization) 在视觉推理中：</strong> "PROPA" 论文通过强化学习在流程层面优化视觉推理，这超越了简单的端到端学习，旨在提高模型推理的透明度和效率。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于希望深入了解最新进展的研究人员，我强烈建议阅读以下论文：</p>
<ul>
<li><strong>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation (PAN Team et al.)</strong>: 如果您对具身智能、模拟和通用AI感兴趣，这篇论文可能定义了未来的研究方向。</li>
<li><strong>Depth Anything 3: Recovering the Visual Space from Any Views (Haotong Lin et al.)</strong>: 对于任何涉及3D感知、机器人或自动驾驶的研究人员，其在通用深度估计方面的进步可能具有直接的应用价值。</li>
<li><strong>MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation (Xun Huang et al.)</strong>: 对于具身智能、机器人导航和语义理解领域的研究者，这篇论文提供了零样本泛化导航的新思路。</li>
<li><strong>Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation (Mayank Vatsa et al.)</strong>: 如果您从事生成模型或多模态AI研究，这篇论文深入探讨了文本到图像生成中的一个关键挑战——组合性保真度，对于理解和改进生成质量至关重要。</li>
</ul>
<hr />
<p>希望这份摘要能帮助您快速把握今日 Arxiv 计算机视觉领域的关键进展！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.10136v1">Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</a></li>
<li><a href="#2511.09057v2">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</a></li>
<li><a href="#2511.10647v1">Depth Anything 3: Recovering the Visual Space from Any Views</a></li>
<li><a href="#2511.10518v1">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a></li>
<li><a href="#2511.10488v1">SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</a></li>
<li><a href="#2511.10403v1">nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</a></li>
<li><a href="#2511.10391v1">GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</a></li>
<li><a href="#2511.10376v1">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a></li>
<li><a href="#2511.10279v1">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a></li>
<li><a href="#2511.10276v1">RoboBenchMart: Benchmarking Robots in Retail Environment</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.10136v1'></a></p>
<h2 id="right-looks-wrong-reasons-compositional-fidelity-in-text-to-image-generation"><a href="https://arxiv.org/abs/2511.10136v1">Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Mayank Vatsa, Aparna Bharati, Richa Singh</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于揭示了当前领先的文本到图像模型在处理“逻辑组合性”方面的根本性缺陷。通过对否定、计数和空间关系这三个基本原语的系统性调查，作者发现当这些原语组合时，模型的性能会急剧下降，暴露出严重的干扰问题，并指出现有架构和简单扩展无法弥补这一差距。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>这篇论文的创新点主要体现在其<strong>诊断性分析框架和对失败根源的归因</strong>。它不是提出一个新的模型，而是通过深入分析现有模型在处理特定逻辑组合（否定、计数、空间关系）时的表现，揭示了其内在的局限性。其方法论可以概括为：
*   <strong>系统性分解与测试：</strong> 将逻辑组合性分解为否定、计数和空间关系等基本原语，并测试模型在单一原语和组合原语上的表现。
*   <strong>归因分析：</strong> 将性能崩溃归因于三个关键因素：训练数据中缺乏显式否定、连续注意力架构不适合离散逻辑，以及评估指标偏重视觉合理性而非约束满足。
*   <strong>批判性评估：</strong> 批判性地指出当前解决方案和简单扩展无法解决根本问题，强调需要基础性的进展。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<p>这篇论文对计算机视觉领域具有深远的潜在影响：</p>
<ul>
<li><strong>范式转变的呼吁：</strong> 它挑战了当前文本到图像生成领域“越大越好”或“更多数据更好”的普遍观念，明确指出需要从根本上重新思考模型架构和表示学习。</li>
<li><strong>指导未来研究方向：</strong> 为未来的研究指明了关键方向，即关注如何实现真正的组合性，而非仅仅提升视觉逼真度。这将促使研究人员探索新的表示学习方法、推理机制和更适合离散逻辑的架构。</li>
<li><strong>更鲁棒和可控的生成：</strong> 如果能解决这些组合性问题，未来的文本到图像模型将能生成更精确、更符合用户意图的图像，尤其是在需要精确控制图像内容（如特定数量的物体、物体间的精确关系、或明确排除某些元素）的应用场景。</li>
<li><strong>改进评估指标：</strong> 论文强调了现有评估指标的不足，这将推动社区开发更侧重于“约束满足”和“逻辑准确性”的评估方法，从而更全面地衡量模型的性能。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>高精度图像生成：</strong> 任何需要精确控制生成图像内容的领域，例如产品设计、建筑可视化、科学插图等。</li>
<li><strong>交互式图像编辑：</strong> 用户希望通过自然语言指令精确修改图像，例如“移除背景中的红色汽车”、“增加三只鸟在树上”。</li>
<li><strong>具身智能/机器人：</strong> 机器人需要理解复杂的指令并将其转化为视觉场景，例如“把桌子上除了杯子以外的所有东西都移开”。</li>
<li><strong>教育和辅助技术：</strong> 生成特定场景以帮助学习或理解复杂概念。</li>
<li><strong>多模态理解：</strong> 文本到图像模型的缺陷反映了其对语言深层语义和逻辑理解的不足，解决这些问题也将促进更深层次的多模态理解。</li>
<li><strong>可解释AI：</strong> 理解模型为何在组合性任务上失败，有助于我们更好地理解其内部工作机制。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>缺乏具体解决方案：</strong> 摘要明确指出“实现真正的组合性将需要表示和推理方面的根本性进展，而不是对现有架构的增量调整”，但并未提出具体的解决方案或新模型。它更多地是一篇诊断性、批判性的调查。</li>
<li><strong>评估范围：</strong> 尽管提到了否定、计数和空间关系，但“逻辑组合性”是一个非常广泛的概念。摘要中未提及其他复杂的逻辑关系（如因果关系、时间关系、属性组合等），可能这些也是现有模型的弱点。</li>
<li><strong>“根本性缺陷”的定义：</strong> 摘要将“根本性缺陷”归因于训练数据、架构和评估指标。虽然这些是重要的因素，但更深层次的认知科学或符号推理的缺失也可能是其根本原因，摘要中未深入探讨。</li>
<li><strong>“简单扩展”的界限：</strong> 摘要声称“简单扩展无法弥补这一差距”，但并未详细定义“简单扩展”的范围。例如，更大规模的预训练、更复杂的微调策略是否也属于“简单扩展”？</li>
<li><strong>未来研究的挑战：</strong> 论文提出了一个重大挑战，但如何实现“表示和推理方面的根本性进展”本身就是一个巨大的开放性问题，可能需要跨学科的努力。</li>
</ul>
<hr />
<p>总而言之，这篇论文虽然没有提出新的模型，但其对当前文本到图像生成模型核心缺陷的深刻洞察和系统性分析，使其在计算机视觉领域具有重要的理论和实践指导意义。它为该领域未来的发展设定了一个清晰而富有挑战性的研究议程。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10136v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10136v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.09057v2'></a></p>
<h2 id="pan-a-world-model-for-general-interactable-and-long-horizon-world-simulation"><a href="https://arxiv.org/abs/2511.09057v2">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</a></h2>
<p><strong>Authors:</strong>  PAN Team, Jiannan Xiang, Yi Gu, Zihan Liu, Zeyu Feng, Qiyue Gao, Yiyan Hu, Benhao Huang, Guangyi Liu, Yichi Yang, Kun Zhou, Davit Abrahamyan, Arif Ahmad, Ganesh Bannur, Junrong Chen, Kimi Chen, Mingkai Deng, Ruobing Han, Xinqi Huang, Haoqiang Kang, Zheqi Li, Enze Ma, Hector Ren, Yashowardhan Shinde, Rohan Shingre, Ramsundar Tanikella, Kaiming Tao, Dequan Yang, Xinle Yu, Cong Zeng, Binglin Zhou, Zhengzhong Liu, Zhiting Hu, Eric P. Xing</p>
<p><strong>Published:</strong> 2025-11-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<h3 id="pan-a-world-model-for-general-interactable-and-long-horizon-world-simulation_1">论文摘要分析：PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>PAN 引入了一个通用、可交互且支持长时序的世界模型，它能够根据历史信息和自然语言动作预测未来的世界状态，并以高质量视频模拟的形式呈现。该模型通过结合基于大型语言模型（LLM）的自回归潜在动力学骨干和视频扩散解码器，实现了在广泛领域内对动作条件下的世界进行连贯、长期的预测性模拟。这标志着在构建能够进行推理和行动的通用世界模型方面迈出了重要一步。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>PAN 的核心创新在于其 <strong>Generative Latent Prediction (GLP) 架构</strong>，它巧妙地融合了两种强大的范式：</p>
<ul>
<li><strong>基于大型语言模型 (LLM) 的自回归潜在动力学骨干：</strong> 这是关键所在。LLM 的强大之处在于其对文本知识的广泛理解和推理能力。通过将世界动力学建模为潜在空间中的自回归过程，并利用 LLM 作为骨干，PAN 能够：<ul>
<li><strong>将模拟建立在丰富的文本知识之上：</strong> 这使得模型能够理解和处理更抽象、更复杂的动作和世界概念，超越了纯视觉或物理规则的限制。</li>
<li><strong>支持自然语言指定动作的条件化：</strong> 智能体可以通过自然语言指令来控制模拟，极大地提升了交互性和泛化能力。</li>
<li><strong>实现潜在空间推理（想象）：</strong> LLM 在潜在空间中进行推理，模拟世界如何演变，这对应于智能体的“想象”能力。</li>
</ul>
</li>
<li><strong>视频扩散解码器：</strong> 负责将潜在空间中的推理结果解码为感知上细节丰富且时间上连贯的视觉观测（高质量视频）。这弥合了抽象的潜在推理与可感知的现实世界动态之间的鸿沟。</li>
</ul>
<p>这种结合实现了 <strong>潜在空间推理（想象）与可实现的世界动力学（现实）的统一</strong>，使得模型既能进行高层次的语义理解和规划，又能生成逼真的视觉输出。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动通用人工智能 (AGI) 的发展：</strong> 能够进行预测性模拟和推理的通用世界模型是实现 AGI 的关键组成部分。PAN 在泛化性、交互性和长时序一致性方面的进步，使其成为通向 AGI 的重要里程碑。</li>
<li><strong>革新智能体规划和决策：</strong> 智能体将能够通过“想象”不同动作序列的后果来规划和制定策略，从而在复杂、不确定的环境中做出更明智的决策。</li>
<li><strong>促进机器人学和具身智能的发展：</strong> 机器人可以利用 PAN 来模拟其动作对环境的影响，进行任务规划、技能学习和故障排除，而无需在真实世界中进行昂贵且耗时的试错。</li>
<li><strong>提升内容生成和虚拟现实体验：</strong> 能够根据自然语言指令生成连贯、长期的动态视频，将极大地丰富虚拟世界、游戏和电影制作的交互性和真实感。</li>
<li><strong>为科学发现和工程设计提供新工具：</strong> 研究人员和工程师可以利用世界模型来模拟复杂系统（如气候、生物过程、材料行为）的演变，加速发现和创新。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>机器人学 (Robotics)：</strong> 任务规划、强化学习、技能获取、安全探索。</li>
<li><strong>具身智能 (Embodied AI)：</strong> 智能体在虚拟或真实环境中的导航、操作和交互。</li>
<li><strong>强化学习 (Reinforcement Learning)：</strong> 模型基强化学习 (Model-Based RL) 将获得更强大、更通用的世界模型。</li>
<li><strong>自然语言处理 (Natural Language Processing)：</strong> 结合视觉和语言理解，实现更深层次的语义推理和指令遵循。</li>
<li><strong>计算机图形学 (Computer Graphics) 和虚拟现实 (Virtual Reality)：</strong> 动态场景生成、交互式叙事、虚拟训练环境。</li>
<li><strong>自动驾驶 (Autonomous Driving)：</strong> 预测其他车辆和行人的行为，进行路径规划和风险评估。</li>
<li><strong>科学模拟 (Scientific Simulation)：</strong> 物理、生物、社会系统的预测建模。</li>
</ul>
<p><strong>5. 从摘要中推断出的局限性</strong></p>
<ul>
<li><strong>训练数据规模和多样性：</strong> 摘要提到“在大型视频-动作对数据集上训练，涵盖不同领域”。虽然这听起来很强大，但“大型”和“多样”的程度仍是关键。构建真正涵盖“开放领域”的视频-动作对数据集本身就是一项巨大的挑战，且数据的质量（标注的准确性、动作的粒度）会直接影响模型的性能。</li>
<li><strong>计算资源需求：</strong> 结合 LLM 和视频扩散模型，尤其是在“大型”数据集上训练，意味着巨大的计算资源需求，包括 GPU、存储和训练时间。这可能会限制其广泛应用和复现。</li>
<li><strong>“开放领域”的真正泛化能力：</strong> 尽管目标是“开放领域”，但模型在面对训练数据中未曾见过的新颖场景、物体或交互模式时，其泛化能力仍需通过严格的测试来验证。LLM 的知识虽然广泛，但其对物理世界和因果关系的理解仍可能存在局限。</li>
<li><strong>因果推理的深度和鲁棒性：</strong> 摘要强调“因果控制”和“推理”，但世界模型的因果推理能力是一个复杂的问题。模型是否能真正理解深层次的因果机制，而不仅仅是学习表面的相关性，这对于处理反事实情景和复杂规划至关重要。</li>
<li><strong>潜在空间表示的解释性：</strong> LLM 在潜在空间中进行推理，这个潜在空间的表示可能非常复杂且难以解释。这可能使得调试模型行为、理解其决策过程变得困难。</li>
<li><strong>“长时序一致性”的挑战：</strong> 尽管声称支持长时序，但生成长时间、高保真且完全一致的视频序列仍然是视频生成领域的巨大挑战。随着模拟时间的增长，误差累积和细节漂移的可能性会增加。</li>
</ul>
<hr />
<p>总而言之，PAN 论文提出了一种令人兴奋且具有前瞻性的方法，通过将 LLM 的强大语言理解和推理能力与视频扩散模型的视觉生成能力相结合，旨在构建一个更通用、更具交互性的世界模型。这对于计算机视觉、机器学习乃至通用人工智能领域都具有深远的潜在影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.09057v2.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.09057v2">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10647v1'></a></p>
<h2 id="depth-anything-3-recovering-the-visual-space-from-any-views"><a href="https://arxiv.org/abs/2511.10647v1">Depth Anything 3: Recovering the Visual Space from Any Views</a></h2>
<p><strong>Authors:</strong> Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<p><strong>论文摘要分析：Depth Anything 3: Recovering the Visual Space from Any Views</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>Depth Anything 3 (DA3) 提出了一种新颖且极简的模型，能够从任意数量的视觉输入中预测空间一致的几何信息，无论是否已知相机姿态。通过利用单个普通 Transformer 作为骨干网络和单一的深度射线预测目标，DA3 在保持高细节和泛化能力的同时，显著简化了模型架构和训练范式。它在新的视觉几何基准上取得了最先进的性能，并在多项任务上超越了现有技术。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>DA3 的核心创新在于其<strong>极简主义建模（minimal modeling）</strong>理念，体现在以下两点：</p>
<ul>
<li><strong>单一普通 Transformer 作为骨干网络：</strong> 论文指出，一个标准的、未经特殊架构定制的 Transformer（例如，vanilla DINO encoder）足以作为骨干网络。这挑战了传统上认为需要为特定几何任务设计复杂或专门架构的观念，表明通用视觉 Transformer 具有更强的几何理解能力。</li>
<li><strong>单一深度射线预测目标：</strong> DA3 摒弃了复杂的多任务学习，而是采用单一的“深度射线（depth-ray）”预测目标。这简化了训练过程，并可能有助于模型更专注于核心的几何恢复任务，避免不同任务之间的冲突或权重平衡问题。</li>
</ul>
<p>此外，论文还提到了一个<strong>教师-学生训练范式（teacher-student training paradigm）</strong>，这通常用于知识蒸馏，以将一个更复杂或性能更好的教师模型的知识转移到一个更简单或更高效的学生模型中，从而使DA3在保持模型简洁性的同时，达到与DA2相当的细节和泛化水平。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>简化几何建模范式：</strong> DA3 的成功表明，在几何恢复任务中，可能不需要高度定制的复杂架构。这可能引导未来的研究更多地关注通用视觉模型（如Transformer）的潜力，并探索如何通过更简洁的训练目标和范式来解决复杂问题。</li>
<li><strong>推动通用视觉理解：</strong> 如果一个普通 Transformer 能够有效处理多视图几何，这进一步证明了这类模型在理解视觉空间和三维结构方面的强大能力，为构建更通用的视觉智能体奠定了基础。</li>
<li><strong>新的基准和评估标准：</strong> 论文建立了一个新的视觉几何基准，涵盖相机姿态估计、任意视图几何和视觉渲染。这将为该领域的研究提供一个统一且更全面的评估平台，促进更公平和有意义的比较。</li>
<li><strong>提升多视图几何的实用性：</strong> 能够在未知相机姿态下恢复几何信息，极大地扩展了多视图几何技术的应用范围，使其在更多现实世界场景中变得可行。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>三维重建和建模：</strong> 能够从任意视图恢复空间一致的几何信息，将极大地简化三维模型的创建过程，尤其是在没有精确相机校准信息的情况下。</li>
<li><strong>机器人学和自主导航：</strong> 机器人需要理解其周围环境的三维结构和自身姿态。DA3 的技术可以帮助机器人在未知环境中进行更鲁棒的定位、建图和避障。</li>
<li><strong>增强现实 (AR) 和虚拟现实 (VR)：</strong> 精确的深度和几何信息对于AR/VR应用中的场景理解、物体放置和真实感渲染至关重要。DA3 可以帮助在各种环境下实现更沉浸式的体验。</li>
<li><strong>计算机图形学和视觉效果：</strong> 艺术家和开发者可以利用DA3从普通视频或图像中提取几何信息，用于场景重建、光照估计和特效制作。</li>
<li><strong>自动驾驶：</strong> 车辆需要实时感知周围环境的深度和三维结构，以进行路径规划和障碍物检测。DA3 的方法可能提供更鲁棒和高效的解决方案。</li>
<li><strong>遥感和测绘：</strong> 从航空或卫星图像中提取地形和建筑物的三维信息。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>“教师-学生训练范式”的依赖性：</strong> 尽管摘要强调了DA3的极简主义，但其性能达到DA2水平是通过教师-学生范式实现的。这意味着DA3的训练可能仍然依赖于一个更复杂的“教师”模型（可能是DA2或类似模型）的知识，而不是完全从头开始的极简训练。这可能影响其独立训练的效率或所需的计算资源。</li>
<li><strong>“公共学术数据集”的范围：</strong> 论文提到所有模型都“完全在公共学术数据集上训练”。虽然这保证了可复现性，但这些数据集的规模、多样性和真实世界复杂性可能与某些商业应用场景仍有差距。模型在高度复杂、光照变化剧烈或包含罕见物体的真实世界数据上的泛化能力仍需进一步验证。</li>
<li><strong>“深度射线”预测的精确定义和挑战：</strong> 摘要中没有详细说明“深度射线”的具体定义和如何从其恢复完整的几何信息。虽然它简化了目标，但其在处理遮挡、纹理缺失或高度反射表面时的鲁棒性可能是一个潜在的挑战。</li>
<li><strong>计算效率和推理速度：</strong> 尽管模型架构简化，但Transformer模型通常计算成本较高。摘要中未提及DA3的推理速度或计算效率，这对于实时应用（如机器人或自动驾驶）至关重要。</li>
<li><strong>几何一致性的具体表现：</strong> 摘要强调“空间一致的几何信息”，但具体在哪些复杂场景下（例如，动态场景、透明物体、重复纹理）能保持这种一致性，以及其鲁棒性如何，仍需通过实验细节来评估。</li>
</ul>
<hr />
<p>总而言之，Depth Anything 3 是一项令人兴奋的研究，它通过极简主义的方法在多视图几何领域取得了显著进展。其核心思想——用通用Transformer和单一目标解决复杂几何问题——具有颠覆性的潜力，并有望推动该领域向更通用、更高效的方向发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses.</li>
<li>We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering.</li>
<li>On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy.</li>
<li>Moreover, it outperforms DA2 in monocular depth estimation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10647v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10518v1'></a></p>
<h2 id="semanticvla-semantic-aligned-sparsification-and-enhancement-for-efficient-robotic-manipulation"><a href="https://arxiv.org/abs/2511.10518v1">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Wei Li, Renshan Zhang, Rui Shao, Zhijian Fang, Kaiwen Zhou, Zhuotao Tian, Liqiang Nie</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>SemanticVLA 提出了一种新颖的 VLA 框架，通过语义对齐的稀疏化和增强来解决机器人操作中感知冗余和指令-视觉对齐不足的问题。它引入了语义引导的双重视觉剪枝器 (SD-Pruner) 来高效处理视觉输入，以及语义互补的分层融合器 (SH-Fuser) 来整合多模态特征，并最终通过语义条件动作耦合器 (SA-Coupler) 提升从感知到动作的转换效率和可解释性。该方法在性能和效率上均超越了现有 SOTA 模型，显著降低了训练成本和推理延迟。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>SemanticVLA 的核心创新在于其<strong>语义对齐的稀疏化和增强</strong>策略，具体体现在以下三个相互关联的组件：</p>
<ul>
<li><strong>语义引导的双重视觉剪枝器 (SD-Pruner)</strong>：这是解决感知冗余的关键。它包含两个子模块：<ul>
<li><strong>指令驱动剪枝器 (ID-Pruner)</strong>：利用 SigLIP 模型，根据指令提取全局动作线索和局部语义锚点，实现语义层面的稀疏化。</li>
<li><strong>空间聚合剪枝器 (SA-Pruner)</strong>：利用 DINOv2 模型，将几何丰富的特征压缩成任务自适应的 token，实现空间层面的稀疏化。这种双重剪枝确保了在减少冗余的同时，保留了对任务至关重要的语义和空间信息。</li>
</ul>
</li>
<li><strong>语义互补的分层融合器 (SH-Fuser)</strong>：在稀疏化之后，如何有效利用这些特征是关键。SH-Fuser 负责融合来自 SigLIP 的密集补丁和 DINOv2 的稀疏 token，以构建连贯的表示。这解决了传统方法中指令-视觉对齐不足的问题，通过互补融合将语义与空间几何信息深度整合。</li>
<li><strong>语义条件动作耦合器 (SA-Coupler)</strong>：这是从感知到动作转换的创新。它取代了传统的“观察到自由度 (observation-to-DoF)”方法，通过语义条件化来生成动作。这意味着动作的生成不再仅仅依赖于原始观测，而是由经过语义理解和稀疏化后的特征驱动，从而实现更高效和可解释的行为建模。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升机器人操作的实用性</strong>：通过显著提高效率（训练成本和推理延迟降低 2.7-3.0 倍）和性能（成功率提升 21.1%），SemanticVLA 有望加速 VLA 模型在实际机器人部署中的应用，使其在资源受限的环境下也能有效工作。</li>
<li><strong>推动高效 VLA 模型设计</strong>：该论文提出的语义对齐稀疏化和多模态特征融合策略，为未来设计更高效、更鲁棒的 VLA 模型提供了新的范式和思路。</li>
<li><strong>增强模型可解释性</strong>：语义条件动作耦合器通过引入语义信息来指导动作生成，可能使得机器人行为的决策过程更具可解释性，这对于安全关键型应用至关重要。</li>
<li><strong>促进跨模态学习的融合</strong>：该工作有效结合了视觉（DINOv2）、语言（SigLIP）和动作，展示了如何通过精巧的设计实现不同预训练模型之间的协同作用，为多模态大模型在具身智能领域的应用提供了宝贵经验。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>具身智能 (Embodied AI)</strong>：所有涉及机器人与环境交互、需要理解指令并执行复杂任务的场景，如服务机器人、工业自动化、家庭助手等。</li>
<li><strong>高效深度学习</strong>：对于需要在边缘设备或计算资源有限的平台上运行的深度学习模型，其稀疏化和效率提升的方法具有普适性。</li>
<li><strong>多模态学习</strong>：如何有效融合来自不同模态（视觉、语言）的信息，并将其应用于下游任务，是多模态学习的核心挑战，SemanticVLA 提供了成功的案例。</li>
<li><strong>机器人学习 (Robot Learning)</strong>：特别是模仿学习、强化学习等领域，SemanticVLA 的高效感知和动作生成机制可以作为基础模块。</li>
<li><strong>人机交互 (Human-Robot Interaction)</strong>：更准确、更高效地理解人类指令，并将其转化为机器人动作，将极大地改善人机交互体验。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>泛化性挑战</strong>：尽管在 LIBERO 基准测试上表现出色，但 VLA 模型在面对全新、未见过的物体、环境或任务时，其泛化能力仍是一个普遍挑战。摘要中未提及对极端泛化能力的评估。</li>
<li><strong>语义锚点的鲁棒性</strong>：ID-Pruner 依赖于 SigLIP 提取全局动作线索和局部语义锚点。这些锚点的质量和鲁棒性，尤其是在复杂、模糊或低光照场景下，可能会影响整体性能。</li>
<li><strong>计算开销的绝对值</strong>：虽然相对 OpenVLA 降低了训练成本和推理延迟，但 VLA 模型本身的绝对计算开销可能仍然较高，尤其是在部署到非常轻量级的硬件上时。</li>
<li><strong>指令复杂性</strong>：摘要中未详细说明所处理指令的复杂程度。对于高度抽象、多步骤或需要常识推理的指令，模型的理解和执行能力可能仍有提升空间。</li>
<li><strong>对预训练模型的依赖</strong>：SemanticVLA 依赖于 SigLIP 和 DINOv2 等强大的预训练模型。这些基础模型的限制（如数据偏差、特定领域知识的缺乏）可能会间接影响 SemanticVLA 的性能。</li>
<li><strong>动作空间和自由度</strong>：摘要中提到“observation-to-DoF”方法，但未具体说明所处理的机器人自由度数量和动作空间的复杂性。对于高自由度、连续动作空间的任务，模型的控制精度和稳定性可能面临挑战。</li>
</ul>
<hr />
<p>总而言之，SemanticVLA 在解决机器人操作中 VLA 模型的效率和语义对齐问题上取得了显著进展，其创新性的稀疏化和融合策略为具身智能领域带来了新的突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation.</li>
<li>Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10518v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10518v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10488v1'></a></p>
<h2 id="spot-sparsification-with-attention-dynamics-via-token-relevance-in-vision-transformers"><a href="https://arxiv.org/abs/2511.10488v1">SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</a></h2>
<p><strong>Authors:</strong> Oded Schlesinger, Amirhossein Farzam, J. Matias Di Martino, Guillermo Sapiro</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为 SPOT 的框架，旨在解决 Vision Transformers (ViT) 中计算成本高昂的问题。SPOT 通过在注意力计算之前，利用 token 嵌入、交互和跨层注意力动态来识别并消除不重要的 token，从而实现计算效率的显著提升，同时保持甚至提高模型性能。其核心在于提供了一种上下文感知且可解释的 token 相关性检测机制，以指导 token 稀疏化。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>该论文的关键创新在于其独特的 token 相关性检测机制，即 <strong>SParsification with attentiOn dynamics via Token relevance (SPOT)</strong>。具体方法学亮点包括：</p>
<ul>
<li><strong>早期检测与稀疏化：</strong> SPOT 在注意力计算之前就进行不重要 token 的检测和移除，这与许多后处理或事后分析的方法不同，从而最大化了计算效率的提升。</li>
<li><strong>多维度 token 重要性推断：</strong> 它不仅仅依赖于单一的特征（如 token 嵌入），而是综合利用了 token 嵌入、它们之间的交互以及跨层（layers）的注意力动态来推断 token 的重要性。这种多维度的方法使得相关性检测更加“上下文感知”和“可解释”。</li>
<li><strong>轻量级预测器：</strong> SPOT 采用计算开销很小的预测器，这些预测器可以灵活地集成到各种 ViT 架构中，并学习针对特定输入（input-specific）的有效 token 优先级策略。</li>
<li><strong>自适应性能：</strong> 其通用设计支持根据不同的资源限制调整性能水平，这意味着它可以灵活地在效率和精度之间进行权衡。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>这篇论文对计算机视觉领域具有显著的潜在影响：</p>
<ul>
<li><strong>ViT 部署的普及：</strong> 显著降低 ViT 的计算成本将使其在资源受限的环境（如移动设备、边缘计算）中更易于部署和应用，从而加速 ViT 在更广泛场景中的普及。</li>
<li><strong>可持续AI：</strong> 减少模型运行所需的计算资源，有助于降低AI模型的碳足迹，符合当前可持续AI发展的趋势。</li>
<li><strong>模型可解释性提升：</strong> 通过推断 token 的“重要性”，SPOT 提供了一种更具可解释性的方式来理解 ViT 内部的工作机制，有助于研究人员更好地理解模型关注的区域。</li>
<li><strong>新研究方向的启发：</strong> 这种基于动态注意力稀疏化的思想可能会启发更多关于高效 ViT 设计、自适应计算和模型压缩的研究。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>实时计算机视觉系统：</strong> 例如自动驾驶、机器人视觉、视频监控等，这些应用对延迟和计算资源有严格要求。</li>
<li><strong>移动和边缘AI：</strong> 在智能手机、物联网设备等计算能力有限的平台上部署高性能ViT模型。</li>
<li><strong>大规模图像/视频分析：</strong> 处理海量数据时，效率的提升可以显著降低成本和时间。</li>
<li><strong>医学图像分析：</strong> 在需要高精度但同时对计算资源敏感的医疗诊断系统中。</li>
<li><strong>任何使用ViT作为骨干网络的任务：</strong> 包括图像分类、目标检测、语义分割、图像生成等。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>“轻量级预测器”的额外开销：</strong> 尽管摘要强调预测器是“计算开销很小”的，但它们仍然引入了额外的计算和参数。在极端资源受限的场景下，这部分开销是否仍然可以忽略不计，需要进一步评估。</li>
<li><strong>“学习”过程的复杂性：</strong> 预测器需要“学习”如何推导有效的 token 优先级。这个学习过程可能需要特定的训练策略、数据量或超参数调优，这可能会增加模型的训练复杂性。</li>
<li><strong>通用性与特定架构的适配：</strong> 摘要提到可以“插入到各种 ViT 架构中”，但具体在不同 ViT 变体（如 Swin Transformer, DeiT, MAE 等）上的适配效果和性能表现可能有所不同，需要针对性验证。</li>
<li><strong>“保持或甚至提高准确性”的边界：</strong> 摘要指出可以“保持或甚至提高准确性”，这通常意味着在某些情况下可能会有轻微的性能下降，或者性能提升是特定于某些数据集或任务的。其鲁棒性在各种复杂场景下的表现如何，仍需详细实验数据支持。</li>
<li><strong>“可解释性”的量化：</strong> 摘要提到“更上下文感知和可解释的相关性检测过程”，但“可解释性”往往是一个主观概念。如何量化和评估这种可解释性的提升，以及它在实际应用中的价值，是值得探讨的问题。</li>
</ul>
<hr />
<p>总的来说，SPOT 提出了一种非常有前景的方法来解决 ViT 的计算效率瓶颈，其多维度、上下文感知的 token 稀疏化策略是其核心亮点。如果其在各种 ViT 架构和任务上都能展现出摘要中所述的显著效率提升和性能保持，那么它无疑将对 ViT 的实际应用和未来发展产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10488v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10488v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10403v1'></a></p>
<h2 id="nuplan-r-a-closed-loop-planning-benchmark-for-autonomous-driving-via-reactive-multi-agent-simulation"><a href="https://arxiv.org/abs/2511.10403v1">nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</a></h2>
<p><strong>Authors:</strong> Mingxing Peng, Ruoyu Yao, Xusen Guo, Jun Ma</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了nuPlan-R，一个用于自动驾驶的全新闭环规划基准。它通过将基于学习的反应式多智能体模拟集成到nuPlan框架中，解决了现有基准中基于规则的反应式智能体（如IDM）缺乏行为多样性和真实人类交互的问题。nuPlan-R利用去噪扩散模型生成更真实、多样且类人的交通行为，从而提供了一个更能反映真实世界交互式驾驶的评估环境。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>用学习型反应式智能体取代规则型智能体：</strong> 这是核心创新。nuPlan-R用“去噪扩散模型（noise-decoupled diffusion-based reactive agents）”取代了传统的基于规则的IDM智能体。扩散模型在生成高质量、多样化数据方面表现出色，这里被用于生成更真实、多样且类人的交通行为。</li>
<li><strong>交互感知智能体选择机制：</strong> 引入此机制以确保模拟的真实性（realism）和计算效率（computational efficiency）。这表明系统能够智能地选择哪些智能体需要更复杂的行为模拟，从而在保持真实性的同时优化资源使用。</li>
<li><strong>扩展的评估指标：</strong> 增加了两个额外的指标，以实现对规划性能更全面的评估。这表明作者认为现有指标不足以捕捉复杂交互场景下的规划器性能。</li>
<li><strong>在nuPlan框架内的集成：</strong> 将这些创新集成到现有的nuPlan框架中，表明其旨在成为一个可扩展和兼容的解决方案。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>更真实的规划器评估：</strong> nuPlan-R将成为自动驾驶规划器评估的新标准，因为它能更好地反映真实世界的复杂交互场景。这将有助于识别在传统基准中可能被忽视的规划器弱点。</li>
<li><strong>加速学习型规划器的发展：</strong> 通过提供一个更具挑战性和真实性的评估环境，nuPlan-R将更好地突出学习型规划器在处理复杂动态场景中的优势，从而激励和加速该领域的研究和发展。</li>
<li><strong>缩小模拟与现实之间的差距：</strong> 更真实的模拟环境有助于减少将规划器从模拟部署到真实世界时遇到的“模拟-现实差距”（sim-to-real gap），从而提高自动驾驶系统的安全性和可靠性。</li>
<li><strong>促进多智能体交互研究：</strong> 引入基于扩散模型的反应式多智能体模拟，将推动对复杂交通场景中多智能体行为预测和交互建模的研究。</li>
</ul>
<p><strong>4. 相关领域或应用可能受益于这项研究</strong></p>
<ul>
<li><strong>自动驾驶规划与控制：</strong> 这是最直接的受益者，所有从事自动驾驶路径规划、行为预测和决策的团队都将从中受益。</li>
<li><strong>交通流模拟与管理：</strong> 更真实的交通行为模拟可以用于城市交通规划、交通拥堵预测和智能交通信号灯控制等领域。</li>
<li><strong>机器人学与多智能体系统：</strong> 论文中关于多智能体交互和行为建模的方法，可以推广到其他需要复杂多智能体协作和避障的机器人应用中。</li>
<li><strong>计算机图形学与虚拟现实：</strong> 生成更真实、多样的人类驾驶行为，对于创建高保真度的虚拟交通环境和训练模拟器具有重要价值。</li>
<li><strong>行为预测与意图识别：</strong> 扩散模型在生成多样化行为方面的能力，可以启发在其他领域（如人机交互、体育分析）中进行更精细的行为预测和意图识别。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 虽然摘要提到了“交互感知智能体选择机制”以确保计算效率，但基于扩散模型的学习型反应式智能体通常比简单的规则型模型具有更高的计算复杂度。在大规模、长时间的模拟中，其计算资源需求可能仍然是一个挑战。</li>
<li><strong>数据依赖性：</strong> 基于学习的智能体需要大量的真实世界数据进行训练，以确保其行为的真实性和多样性。摘要中没有提及训练数据的来源和规模，这可能是一个潜在的瓶颈。</li>
<li><strong>模型泛化能力：</strong> 扩散模型在训练数据分布之外的场景中，其生成行为的真实性和多样性可能受到限制。例如，在极端天气、罕见事故或从未见过的交通模式下，模型的泛化能力可能需要进一步验证。</li>
<li><strong>可解释性：</strong> 相比于规则型智能体，基于深度学习的智能体（如扩散模型）通常具有较低的可解释性。理解为什么智能体在特定情况下做出某种行为可能更困难，这对于安全关键的自动驾驶应用来说是一个挑战。</li>
<li><strong>“真实性”的定义和度量：</strong> 尽管论文声称其模型产生“更真实、多样且类人的交通行为”，但“真实性”的客观度量和验证方法在自动驾驶领域仍然是一个持续的挑战。摘要中提到的“广泛实验”将需要详细说明这些验证方法。</li>
</ul>
<hr />
<p>总而言之，nuPlan-R代表了自动驾驶闭环规划基准的一个重要进步，它通过引入先进的机器学习技术来模拟更真实的人类驾驶行为，从而为评估和开发下一代自动驾驶系统提供了更坚实的基础。其对计算机视觉和机器学习领域的潜在趣味性在于，它将扩散模型这一强大的生成模型引入到多智能体行为模拟中，为理解和预测复杂动态环境中的交互行为开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework.</li>
<li>These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation.</li>
<li>We will open-source the code for the new benchmark.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10403v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10403v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10391v1'></a></p>
<h2 id="groundiff-diffusion-based-ground-surface-generation-from-digital-surface-models"><a href="https://arxiv.org/abs/2511.10391v1">GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</a></h2>
<p><strong>Authors:</strong> Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Digital Terrain Models (DTMs) represent the bare-earth elevation and are important in numerous geospatial applications. Such data models cannot be directly measured by sensors and are typically generated from Digital Surface Models (DSMs) derived from LiDAR or photogrammetry. Traditional filtering approaches rely on manually tuned parameters, while learning-based methods require well-designed architectures, often combined with post-processing. To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task. We incorporate a gated design with confidence-guided generation that enables selective filtering. To increase scalability, we further propose Prior-Guided Stitching (PrioStitch), which employs a downsampled global prior automatically generated using GrounDiff to guide local high-resolution predictions. We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks. In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization. Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods. The project page is available at https://deepscenario.github.io/GrounDiff/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="groundiff-diffusion-based-ground-surface-generation-from-digital-surface-models_1">论文摘要分析：GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文引入了GrounDiff，一个开创性的基于扩散模型（diffusion-based）的框架，用于从数字表面模型（DSM）生成数字地形模型（DTM）。它将非地面结构移除问题重新定义为去噪任务，并结合了置信度引导的生成和一种名为PrioStitch的尺度扩展机制，显著提升了DTM生成的精度和效率，超越了现有最先进的方法。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>扩散模型应用于DSM-to-DTM转换：</strong> 这是核心创新。论文首次将扩散模型引入到DTM生成任务中，将去除非地面结构（如建筑物、植被）的问题建模为一个迭代的去噪过程。这与传统的基于过滤或判别式学习的方法截然不同。</li>
<li><strong>置信度引导的生成（Confidence-Guided Generation）与门控设计（Gated Design）：</strong> 这种机制允许模型选择性地过滤，即在生成过程中根据置信度信息决定哪些区域需要更强的去噪或保留，从而实现更精细和准确的地面提取。</li>
<li><strong>Prior-Guided Stitching (PrioStitch) 用于可扩展性：</strong> 为了解决高分辨率数据处理的计算挑战，PrioStitch提出了一种创新的分层方法。它首先使用GrounDiff生成一个下采样的全局先验（global prior），然后利用这个先验来指导局部高分辨率区域的预测，从而在保持高精度的同时提高处理大规模数据的效率。</li>
<li><strong>无需任务特定优化的通用性：</strong> 论文强调GrounDiff在道路重建等需要高精度和平滑度的任务中，仅使用DSM输入，无需任务特定的优化，就能达到甚至超越专门技术的效果，这体现了其方法的通用性和鲁棒性。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>DTM生成范式转变：</strong> 将扩散模型引入DTM生成，可能开创该领域的新研究方向，促使更多研究者探索生成模型在地球空间数据处理中的应用。</li>
<li><strong>提升DTM生成精度和效率：</strong> 显著降低RMSE（高达93%和47%）表明GrounDiff在精度上取得了突破性进展，这将直接影响依赖DTM的下游应用。PrioStitch机制也解决了大规模数据处理的效率瓶颈。</li>
<li><strong>推动地球空间AI发展：</strong> 作为一个通用的、高性能的DTM生成工具，GrounDiff可以作为许多地球空间分析和应用的基础，加速该领域AI技术的发展。</li>
<li><strong>启发其他地球空间去噪/重建任务：</strong> 扩散模型在图像生成领域取得了巨大成功，GrounDiff的成功应用可能会启发研究者将扩散模型应用于其他地球空间数据（如点云、遥感图像）的去噪、补全、重建等任务。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>地理信息系统 (GIS) 和测绘：</strong> DTM是GIS的核心数据，GrounDiff的改进将直接提升地图制作、地形分析、水文建模、地质勘探的精度。</li>
<li><strong>城市规划与管理：</strong> 准确的DTM对于城市洪水模拟、基础设施规划、建筑高度限制、景观分析至关重要。</li>
<li><strong>灾害管理与应急响应：</strong> 在洪水、滑坡等自然灾害的风险评估和模拟中，高精度DTM是不可或缺的。</li>
<li><strong>自动驾驶与机器人导航：</strong> 道路重建和精确地形信息对于自动驾驶车辆在复杂环境中的路径规划和感知至关重要。GrounDiff在道路重建上的表现尤为突出。</li>
<li><strong>环境科学与生态学：</strong> 植被去除后的裸地模型对于森林砍伐监测、土壤侵蚀研究、生物多样性评估等有重要意义。</li>
<li><strong>军事与国防：</strong> 精确的地形数据对于军事行动规划、模拟和导航至关重要。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 尽管PrioStitch提高了可扩展性，但扩散模型通常在训练和推理时都具有较高的计算成本。摘要中未详细说明其具体的计算效率与传统方法的对比，尤其是在超大规模数据集上的表现。</li>
<li><strong>模型泛化能力：</strong> 摘要提到在“多样数据集”上进行了评估，但未具体说明这些数据集的多样性程度（例如，不同地形类型、植被密度、建筑风格等）。模型在全新、未见过的高度复杂或异常地形上的泛化能力仍需进一步验证。</li>
<li><strong>“非地面结构”的定义：</strong> 扩散模型通过“去噪”来移除非地面结构。这可能意味着它依赖于训练数据中对“地面”和“非地面”的隐式或显式定义。对于一些模糊的边界情况（例如，非常低矮的灌木丛、半埋的物体），模型的判断可能存在挑战。</li>
<li><strong>参数调优的复杂性：</strong> 尽管论文声称传统方法依赖手动调优参数，而GrounDiff避免了这一点，但扩散模型本身也可能涉及超参数的选择（如扩散步数、学习率等），这些参数的选择是否对性能有显著影响，摘要中未提及。</li>
<li><strong>“置信度引导”的具体实现：</strong> 摘要中提到了置信度引导的生成，但没有详细说明如何计算或利用这种置信度。这可能是一个关键的技术细节，其有效性依赖于其具体实现。</li>
</ul>
<hr />
<p>总而言之，GrounDiff代表了DTM生成领域的一个重要进步，它巧妙地将扩散模型的强大能力引入到地球空间数据处理中，解决了长期存在的精度和效率挑战。其在多个基准测试上的卓越表现，以及在道路重建等特定应用中的出色性能，预示着它将在未来的地球空间AI应用中发挥关键作用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task.</li>
<li>We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks.</li>
<li>In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization.</li>
<li>Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10391v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10391v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10376v1'></a></p>
<h2 id="msgnav-unleashing-the-power-of-multi-modal-3d-scene-graph-for-zero-shot-embodied-navigation"><a href="https://arxiv.org/abs/2511.10376v1">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a></h2>
<p><strong>Authors:</strong> Xun Huang, Shijia Zhao, Yunxiang Wang, Xin Lu, Wanfa Zhang, Rongsheng Qu, Weixin Li, Yunhong Wang, Chenglu Wen</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="msgnav-unleashing-the-power-of-multi-modal-3d-scene-graph-for-zero-shot-embodied-navigation_1">论文摘要分析：MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为 <strong>M3DSG (Multi-modal 3D Scene Graph)</strong> 的新型多模态3D场景图，通过用动态分配的图像替换传统的文本关系边，解决了现有零样本具身导航方法中视觉信息丢失和词汇受限的问题。基于M3DSG，作者开发了 <strong>MSGNav</strong> 系统，一个零样本导航框架，它包含高效推理、开放词汇支持和精确探索推理的模块，并特别解决了零样本导航中的“最后一英里问题”，实现了最先进的性能。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>核心创新在于 <strong>M3DSG (Multi-modal 3D Scene Graph)</strong> 的设计。
*   <strong>多模态关系表示：</strong> 现有方法通常将丰富的视觉观察压缩成文本关系，导致信息丢失和词汇限制。M3DSG通过用<strong>动态分配的图像</strong>替换文本关系边，直接保留了视觉线索，避免了这种不可逆的视觉证据损失。这使得场景图能够更丰富、更准确地表示环境。
*   <strong>MSGNav 系统架构：</strong>
    *   <strong>Key Subgraph Selection module：</strong> 用于高效推理，可能通过关注与当前任务最相关的场景图子集。
    *   <strong>Adaptive Vocabulary Update module：</strong> 支持开放词汇，允许系统处理未见过的物体或概念。
    *   <strong>Closed-Loop Reasoning module：</strong> 用于精确的探索推理，可能涉及对导航过程中的不确定性进行建模和修正。
    *   <strong>Visibility-based Viewpoint Decision module：</strong> 明确解决了零样本导航中的“最后一英里问题”，即确定可行的目标位置和合适的最终视角，这在实际部署中至关重要。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升零样本具身导航性能：</strong> 通过更丰富的场景表示和专门设计的推理模块，MSGNav有望显著提高机器人在未知环境中进行零样本导航的成功率和效率。</li>
<li><strong>推动多模态场景理解：</strong> M3DSG的提出为如何有效地将视觉信息融入结构化场景表示提供了新的范式，可能启发更多结合多模态数据的场景理解和推理方法。</li>
<li><strong>降低机器人部署成本：</strong> 零样本能力意味着机器人无需针对每个新环境进行大量的任务特定强化学习训练，大大降低了部署的复杂性和成本，加速了具身智能体的实际应用。</li>
<li><strong>解决“最后一英里问题”：</strong> 明确提出并解决这一关键问题，对于提升导航系统的实用性和用户体验具有重要意义。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>具身智能体 (Embodied AI)：</strong> 机器人导航、物体抓取、人机交互等需要理解复杂环境并执行任务的具身智能体。</li>
<li><strong>服务机器人：</strong> 在家庭、医院、仓库等未知或半结构化环境中执行任务的服务机器人。</li>
<li><strong>自动驾驶：</strong> 虽然具身导航更侧重于室内或局部环境，但其场景理解和零样本泛化能力可能对自动驾驶中的复杂场景理解和决策提供借鉴。</li>
<li><strong>虚拟现实/增强现实 (VR/AR)：</strong> 需要构建和理解虚拟环境，并支持用户在其中进行自然交互的应用。</li>
<li><strong>通用人工智能 (AGI)：</strong> 提升机器人的环境感知和自主决策能力，是通向AGI的重要一步。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>M3DSG的构建成本：</strong> 摘要提到现有方法“高构建成本”，但M3DSG用动态分配的图像替换文本关系边，这可能意味着在图像特征提取、匹配和动态分配上会有新的计算开销。虽然避免了文本压缩的损失，但构建和维护这种多模态图的复杂性和效率仍需关注。</li>
<li><strong>“动态分配的图像”的具体机制：</strong> 摘要没有详细说明这些图像是如何“动态分配”到关系边上的。这可能涉及复杂的视觉特征匹配、语义关联或时序关联，其鲁棒性和泛化能力是关键。</li>
<li><strong>实时性要求：</strong> 对于实际的机器人导航，实时性至关重要。M3DSG的构建、Key Subgraph Selection以及Closed-Loop Reasoning等模块的计算效率是否能满足实时导航的需求，是需要进一步验证的。</li>
<li><strong>数据依赖性：</strong> 尽管是零样本方法，但M3DSG的构建和MSGNav的训练（如果存在预训练阶段）可能仍然依赖于高质量的多模态3D场景数据。其对不同场景、光照、纹理变化的鲁棒性如何？</li>
<li><strong>“最后一英里问题”的普适性：</strong> 尽管提出了Visibility-based Viewpoint Decision module，但其在极端复杂或高度遮挡环境下的表现如何，以及是否能完全解决所有“最后一英里”的挑战，仍有待观察。</li>
</ul>
<hr />
<p>总的来说，这篇论文在零样本具身导航领域提出了一个非常有前景的方向，通过引入多模态3D场景图，解决了现有方法的关键痛点。其创新性在于对场景图表示的根本性改进，以及为解决实际导航问题而设计的系统模块。如果其性能和效率能够得到充分验证，将对具身智能体的实际部署产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images.</li>
<li>Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning.</li>
<li>Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10376v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10376v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10279v1'></a></p>
<h2 id="propa-toward-process-level-optimization-in-visual-reasoning-via-reinforcement-learning"><a href="https://arxiv.org/abs/2511.10279v1">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="propa-toward-process-level-optimization-in-visual-reasoning-via-reinforcement-learning_1">论文摘要分析：PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为PROPA的新型框架，旨在通过结合蒙特卡洛树搜索（MCTS）和可验证奖励强化学习（GRPO）来解决视觉语言模型（VLMs）在复杂视觉推理中多步骤依赖导致的错误级联问题。PROPA通过生成密集的、过程级别的奖励，在无需人工标注的情况下优化推理的每个中间步骤，并结合SFT和PRM来克服冷启动问题并指导推理时搜索，从而显著提升了VLMs的推理和泛化能力。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>PROPA的核心创新在于其<strong>过程级优化</strong>策略，具体体现在以下几个方面：</p>
<ul>
<li><strong>MCTS与GRPO的集成以生成密集过程级奖励：</strong> 这是最关键的创新点。传统的RLVR方法（如GRPO）只提供稀疏的、结果层面的反馈，难以稳定优化多步骤推理。PROPA通过将MCTS引入GRPO框架，能够探索推理路径并评估中间步骤的质量，从而生成更密集的、过程级别的奖励信号，指导模型在每个中间步骤进行优化，而无需昂贵的人工步骤级标注。</li>
<li><strong>SFT与GRPO的交错更新：</strong> 为了解决强化学习常见的“冷启动”问题，PROPA巧妙地将GRPO的更新与监督微调（SFT）交错进行。这意味着模型可以从成功的推理轨迹中学习（通过SFT），同时也能通过GRPO从探索和试错中学习，从而加速训练并提高稳定性。</li>
<li><strong>过程奖励模型（PRM）的引入：</strong> PRM被训练来在推理时指导搜索，确保测试时的搜索策略与训练时学到的优化信号保持一致。这有助于将训练阶段获得的优势有效地迁移到实际推理中。</li>
<li><strong>无需人工步骤级标注：</strong> 这是一个重要的实际优势，因为它大大降低了训练复杂视觉推理模型的成本和数据准备难度。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>PROPA的提出对计算机视觉和机器学习领域，特别是视觉推理和VLM研究，具有以下潜在影响：</p>
<ul>
<li><strong>推动复杂视觉推理能力：</strong> 通过解决多步骤推理中的错误级联问题，PROPA有望显著提升VLMs处理更复杂、更需要逻辑推理的视觉任务的能力，例如视觉问答、指令遵循、场景理解等。</li>
<li><strong>降低训练成本和数据依赖：</strong> 无需昂贵的步骤级人工标注，使得开发和部署高性能视觉推理模型变得更加可行，尤其是在数据标注资源有限的场景。</li>
<li><strong>启发新的RL在VLM中的应用：</strong> PROPA展示了如何巧妙地结合MCTS和RL来生成更丰富的奖励信号，这可能会启发研究人员探索更多将高级搜索算法与强化学习结合，以优化复杂序列决策任务的方法。</li>
<li><strong>提升模型泛化能力：</strong> 摘要中提到在域外任务上取得了显著提升，这表明PROPA不仅能提高特定任务的性能，还能增强模型的泛化能力，使其在面对新颖或未见过的情境时表现更好。</li>
</ul>
<p><strong>4. 相关领域或应用可能受益于这项研究</strong></p>
<ul>
<li><strong>视觉问答（VQA）和视觉常识推理：</strong> 这些任务通常需要多步骤的视觉和语言理解。</li>
<li><strong>具身智能/机器人学：</strong> 机器人需要进行多步骤的规划和决策，PROPA的方法可以帮助它们在视觉感知的基础上进行更鲁棒的推理和行动。</li>
<li><strong>自动驾驶：</strong> 理解复杂的交通场景和预测多步事件需要强大的视觉推理能力。</li>
<li><strong>医疗影像分析：</strong> 诊断和治疗规划可能涉及对多模态数据的复杂推理。</li>
<li><strong>人机交互：</strong> 提升VLM的推理能力可以使聊天机器人或虚拟助手更好地理解用户的复杂视觉指令和意图。</li>
<li><strong>内容生成与编辑：</strong> 例如，根据复杂指令生成图像或视频，或进行智能编辑。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<p>尽管摘要展示了PROPA的强大潜力，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算成本：</strong> MCTS本身是计算密集型的，尤其是在搜索空间较大时。虽然它带来了密集奖励，但训练和推理时的计算开销可能比纯SFT或简单RLVR方法更高。</li>
<li><strong>PRM的鲁棒性：</strong> 过程奖励模型（PRM）的性能对整个框架至关重要。如果PRM训练不充分或存在偏差，可能会误导推理时的搜索，影响最终性能。</li>
<li><strong>超参数调优的复杂性：</strong> 结合了SFT、GRPO、MCTS和PRM，PROPA可能涉及更多的超参数，其调优过程可能相对复杂和耗时。</li>
<li><strong>奖励函数的定义：</strong> 尽管MCTS生成了“密集”奖励，但这些奖励的内在质量和设计（例如，如何量化中间步骤的“好坏”）仍然是关键。摘要中未详细说明奖励的具体形式，这可能是一个需要仔细考量的地方。</li>
<li><strong>对VLM骨干的依赖：</strong> 尽管PROPA在多种VLM骨干上表现良好，但其最终性能仍可能受限于底层VLM骨干模型的表达能力和预训练质量。</li>
<li><strong>“冷启动”问题的完全解决程度：</strong> 尽管交错更新有助于缓解冷启动，但对于极其复杂的推理任务，模型在早期阶段仍可能面临探索效率低下的问题。</li>
</ul>
<hr />
<p>总而言之，PROPA代表了视觉推理领域的一个重要进展，通过创新性地结合MCTS和RL，实现了无需人工标注的过程级优化，为构建更智能、更具泛化能力的视觉语言模型开辟了新途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations.</li>
<li>Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines.</li>
<li>It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10279v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10279v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10276v1'></a></p>
<h2 id="robobenchmart-benchmarking-robots-in-retail-environment"><a href="https://arxiv.org/abs/2511.10276v1">RoboBenchMart: Benchmarking Robots in Retail Environment</a></h2>
<p><strong>Authors:</strong> Konstantin Soshin, Alexander Krapukhin, Andrei Spiridonov, Denis Shepelev, Gregorii Bukhtuev, Andrey Kuznetsov, Vlad Shakhuro</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Most existing robotic manipulation benchmarks focus on simplified tabletop scenarios, typically involving a stationary robotic arm interacting with various objects on a flat surface. To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items. This setting presents significant challenges, including dense object clutter and varied spatial configurations -- with items positioned at different heights, depths, and in close proximity. By targeting the retail domain, our benchmark addresses a setting with strong potential for near-term automation impact. We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks. To support further research, we release the RoboBenchMart suite, which includes a procedural store layout generator, a trajectory generation pipeline, evaluation tools and fine-tuned baseline models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<p><strong>论文摘要分析：RoboBenchMart: Benchmarking Robots in Retail Environment</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献是引入了RoboBenchMart，一个针对零售“暗店”（dark store）环境设计的、更具挑战性和真实感的机器人操作基准。它旨在解决现有基准过于简化、仅限于桌面场景的局限性，通过模拟杂乱、多样的商品摆放和复杂的空间配置，推动机器人操作在零售自动化领域的进步。作者还发布了完整的基准套件，包括生成器、轨迹生成工具、评估工具和基线模型。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>关键创新在于<strong>将机器人操作基准从简化的桌面场景扩展到高度复杂和真实的零售“暗店”环境</strong>。这不仅仅是场景的改变，更是对任务复杂度的本质提升。具体的方法论创新体现在：</p>
<ul>
<li><strong>场景复杂性提升：</strong> 引入了“暗店”环境，其特点是<strong>密集的物体杂乱（dense object clutter）</strong>和<strong>多样的空间配置（varied spatial configurations）</strong>，包括不同高度、深度和紧密相邻的物品。这远超传统基准的平面、稀疏物体设置。</li>
<li><strong>任务真实性提升：</strong> 聚焦于“杂货商品”的复杂操作任务，这些任务在零售领域具有直接的自动化潜力。</li>
<li><strong>完整工具链的发布：</strong> 提供了<strong>程序化商店布局生成器（procedural store layout generator）</strong>、<strong>轨迹生成管道（trajectory generation pipeline）</strong>、<strong>评估工具（evaluation tools）</strong>和<strong>微调的基线模型（fine-tuned baseline models）</strong>。这使得研究人员能够方便地复现、扩展和比较不同的机器人操作算法。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动机器人操作研究的范式转变：</strong> RoboBenchMart将迫使研究人员从理想化的实验室环境转向更具挑战性的真实世界场景，从而加速开发出更鲁棒、更通用的机器人操作策略。</li>
<li><strong>加速零售自动化进程：</strong> 通过提供一个标准化的、高难度的基准，它将直接激励和评估在零售物流、仓储和拣选等领域具有实际应用价值的机器人技术。</li>
<li><strong>揭示当前SOTA模型的局限性：</strong> 论文明确指出“当前最先进的通用模型难以解决即使是常见的零售任务”，这为未来的研究指明了方向，即需要开发新的算法来应对杂乱、多样性和复杂空间配置带来的挑战。</li>
<li><strong>促进多模态感知与操作的融合：</strong> 应对零售环境的挑战，需要机器人具备更强的视觉感知能力（识别杂乱中的物体、估计深度和姿态）、更精细的抓取规划能力以及更智能的路径规划能力。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>机器人操作与抓取（Robotic Manipulation &amp; Grasping）：</strong> 这是最直接受益的领域，需要开发新的算法来处理高密度杂乱、部分遮挡和多样化的物体形状。</li>
<li><strong>计算机视觉（Computer Vision）：</strong> 特别是物体检测、实例分割、3D重建、姿态估计等领域，需要更鲁棒的算法来应对复杂背景和光照条件下的杂货商品。</li>
<li><strong>强化学习（Reinforcement Learning）：</strong> 机器人学习在复杂、高维状态空间中进行决策和规划，以完成操作任务。</li>
<li><strong>具身智能（Embodied AI）：</strong> 机器人需要在物理世界中感知、理解和行动，RoboBenchMart提供了一个极佳的测试平台。</li>
<li><strong>物流与仓储自动化（Logistics &amp; Warehouse Automation）：</strong> 零售“暗店”是典型的物流场景，该研究直接服务于这一领域的自动化需求。</li>
<li><strong>服务机器人（Service Robotics）：</strong> 虽然聚焦零售，但其处理杂乱环境和多样物品的能力，对其他服务机器人（如家庭助手、医疗辅助机器人）也有借鉴意义。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>仅限于“暗店”环境：</strong> 尽管比桌面场景更真实，但“暗店”通常是受控环境，可能不完全涵盖所有零售场景（例如，有顾客在场的商店、更复杂的商品包装）。</li>
<li><strong>数据生成与真实世界的差距：</strong> 摘要提到“程序化商店布局生成器”，这意味着数据可能主要来自模拟环境。虽然模拟是必要的，但模拟与真实世界之间的“域间隙”（domain gap）仍然是一个挑战，可能需要额外的真实世界数据或域适应技术来弥补。</li>
<li><strong>任务复杂度的具体范围：</strong> 摘要提到“复杂操作任务”和“多样杂货商品”，但具体任务类型（例如，单件拣选、多件拣选、堆叠、整理）和商品多样性（例如，软包装、硬包装、易碎品、不规则形状）的详细程度尚不清楚。这些细节会影响基准的全面性。</li>
<li><strong>基线模型的性能：</strong> 摘要指出SOTA模型“挣扎”，这表明当前模型在这一新基准上表现不佳。虽然这是基准的初衷，但也意味着解决这些任务可能需要大量的计算资源和时间。</li>
<li><strong>未提及硬件平台：</strong> 摘要没有说明基准是针对特定类型的机器人硬件（例如，协作臂、移动操作臂）还是更通用的。硬件的选择会影响任务的执行方式和挑战。</li>
</ul>
<hr />
<p>总而言之，RoboBenchMart是一个非常及时和重要的贡献，它将机器人操作研究推向了更具挑战性和实际应用价值的领域。它为计算机视觉、机器学习和机器人学交叉领域的研究人员提供了一个急需的、标准化的平台，以开发下一代能够应对真实世界复杂性的智能机器人。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items.</li>
<li>We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10276v1.pdf">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10276v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-14 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
