<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-14 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-13/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-17/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-14">Arxiv Computer Vision Papers - 2025-11-14</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#right-looks-wrong-reasons-compositional-fidelity-in-text-to-image-generation" class="nav-link">Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#pan-a-world-model-for-general-interactable-and-long-horizon-world-simulation" class="nav-link">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</a>
                </li>
                <li class="nav-item">
                    <a href="#depth-anything-3-recovering-the-visual-space-from-any-views" class="nav-link">Depth Anything 3: Recovering the Visual Space from Any Views</a>
                </li>
                <li class="nav-item">
                    <a href="#semanticvla-semantic-aligned-sparsification-and-enhancement-for-efficient-robotic-manipulation" class="nav-link">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#spot-sparsification-with-attention-dynamics-via-token-relevance-in-vision-transformers" class="nav-link">SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#nuplan-r-a-closed-loop-planning-benchmark-for-autonomous-driving-via-reactive-multi-agent-simulation" class="nav-link">nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</a>
                </li>
                <li class="nav-item">
                    <a href="#groundiff-diffusion-based-ground-surface-generation-from-digital-surface-models" class="nav-link">GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</a>
                </li>
                <li class="nav-item">
                    <a href="#msgnav-unleashing-the-power-of-multi-modal-3d-scene-graph-for-zero-shot-embodied-navigation" class="nav-link">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a>
                </li>
                <li class="nav-item">
                    <a href="#propa-toward-process-level-optimization-in-visual-reasoning-via-reinforcement-learning" class="nav-link">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#robobenchmart-benchmarking-robots-in-retail-environment" class="nav-link">RoboBenchMart: Benchmarking Robots in Retail Environment</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-14">Arxiv Computer Vision Papers - 2025-11-14</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份为忙碌的研究人员准备的 Arxiv 计算机视觉领域日报执行摘要，涵盖了 2025 年 11 月 13 日发布的 10 篇论文：</p>
<hr />
<p><strong>Arxiv 计算机视觉日报执行摘要 (2025-11-13)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文集展示了计算机视觉和机器学习领域几个关键且相互关联的趋势：</p>
<ul>
<li><strong>世界模型与通用模拟 (World Models &amp; General Simulation):</strong> 多个工作致力于构建更通用、交互性更强、时间跨度更长的世界模型，以支持更复杂的模拟和具身智能。</li>
<li><strong>高效与稀疏化 (Efficiency &amp; Sparsification):</strong> 在大型模型和复杂任务中，如何提高效率、减少计算量是持续关注的焦点，尤其体现在视觉Transformer的稀疏化和机器人操作中的语义对齐稀疏化。</li>
<li><strong>具身智能与机器人 (Embodied AI &amp; Robotics):</strong> 具身导航、机器人操作、零售环境中的机器人基准测试以及自动驾驶的闭环规划等，都强调了将视觉智能应用于物理世界交互的需求。</li>
<li><strong>生成模型与场景理解 (Generative Models &amp; Scene Understanding):</strong> 文本到图像生成中的组合性保真度、基于扩散模型的地面生成以及多模态3D场景图在导航中的应用，都体现了对复杂场景生成和理解能力的追求。</li>
<li><strong>视觉推理与优化 (Visual Reasoning &amp; Optimization):</strong> 强化学习被用于优化视觉推理过程，以期实现更高级别的认知能力。</li>
</ul>
<p><strong>2. 特别重要或创新的论文亮点：</strong></p>
<ul>
<li><strong>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation (PAN Team et al.):</strong> 这篇论文似乎代表了在构建通用世界模型方面的一个重大飞跃，其目标是实现可交互和长时序的模拟。如果成功，这将对具身智能、机器人和模拟训练产生深远影响。</li>
<li><strong>Depth Anything 3: Recovering the Visual Space from Any Views (Haotong Lin et al.):</strong> 作为“Depth Anything”系列的最新迭代，这篇论文承诺从任意视角恢复视觉空间，这可能意味着在通用深度估计方面取得了显著进展，对于3D重建、AR/VR和机器人感知至关重要。</li>
<li><strong>MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation (Xun Huang et al.):</strong> 利用多模态3D场景图实现零样本具身导航是一个非常创新的方向。它结合了高级语义理解和具身决策，有望显著提升机器人在未知环境中的适应性。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>通用世界模型 (General World Models):</strong> PAN论文的出现预示着对构建能够模拟复杂物理和交互的通用世界模型的研究将进一步升温。</li>
<li><strong>基于语义的稀疏化与增强 (Semantic-aligned Sparsification &amp; Enhancement):</strong> 不仅仅是简单的稀疏化，而是结合语义信息进行稀疏化和增强，这在提高效率的同时能更好地保持关键信息，尤其在机器人和大型视觉模型中具有潜力。</li>
<li><strong>多模态3D场景图在具身智能中的应用 (Multi-modal 3D Scene Graphs for Embodied AI):</strong> 将视觉、语言和其他模态信息整合到3D场景图中，并用于具身导航和推理，是实现更智能、更具认知能力的具身智能体的关键。</li>
<li><strong>强化学习优化视觉推理过程 (RL for Process-level Optimization in Visual Reasoning):</strong> 将强化学习应用于优化视觉推理的“过程”而非仅仅是最终结果，这可能开启更灵活、更适应性强的视觉认知系统。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>为了深入了解当前最前沿的进展和潜在的突破，我建议优先阅读以下论文：</p>
<ul>
<li><strong>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation (PAN Team et al.):</strong> 如果您对通用AI、模拟和具身智能感兴趣，这篇论文是必读的。</li>
<li><strong>Depth Anything 3: Recovering the Visual Space from Any Views (Haotong Lin et al.):</strong> 对于任何涉及3D感知、重建或机器人导航的研究人员，了解最新的通用深度估计技术至关重要。</li>
<li><strong>MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation (Xun Huang et al.):</strong> 对于具身智能、机器人导航和高级场景理解的研究者，这篇论文提供了令人兴奋的新范式。</li>
<li><strong>Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation (Mayank Vatsa et al.):</strong> 如果您从事生成模型或多模态AI研究，理解文本到图像生成中的组合性保真度问题及其解决方案至关重要。</li>
</ul>
<hr />
<p>这份摘要旨在帮助您快速把握今日Arxiv计算机视觉领域的脉搏。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.10136v1">Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</a></li>
<li><a href="#2511.09057v2">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</a></li>
<li><a href="#2511.10647v1">Depth Anything 3: Recovering the Visual Space from Any Views</a></li>
<li><a href="#2511.10518v1">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a></li>
<li><a href="#2511.10488v1">SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</a></li>
<li><a href="#2511.10403v1">nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</a></li>
<li><a href="#2511.10391v1">GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</a></li>
<li><a href="#2511.10376v1">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a></li>
<li><a href="#2511.10279v1">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a></li>
<li><a href="#2511.10276v1">RoboBenchMart: Benchmarking Robots in Retail Environment</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.10136v1'></a></p>
<h2 id="right-looks-wrong-reasons-compositional-fidelity-in-text-to-image-generation"><a href="https://arxiv.org/abs/2511.10136v1">Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Mayank Vatsa, Aparna Bharati, Richa Singh</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Mayank Vatsa, Aparna Bharati, Richa Singh撰写的论文“Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation”的全面摘要：</p>
<p><strong>论文摘要：文本到图像生成中的组合忠实度问题</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文探讨了当前领先的文本到图像（T2I）生成模型（如DALL-E 3、Stable Diffusion等）的一个根本性缺陷：它们在处理逻辑组合方面的能力不足，即无法同时满足多个约束条件（如计数、属性绑定、空间关系和否定）。尽管这些模型在视觉真实感方面表现出色，但在处理复杂提示时，其性能会急剧下降，导致生成图像在语义上不准确。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要贡献在于：
*   <strong>形式化分析交集性失败：</strong> 论文提供了一个形式化的解释，说明为什么在单个组合原语上表现良好的模型，在这些原语组合时会突然失效，并将其与约束优化和组合硬度联系起来。
*   <strong>方法论综合：</strong> 论文综合了解决这些问题的现有方法，包括用于否定的数据增强和对比训练；用于计数的架构策略和专家混合；用于空间关系布局和结构控制；以及用于联合组合的混合神经符号管道。
*   <strong>基准评估：</strong> 论文回顾了15个基准测试，展示了从人工研究到自动化和对抗性评估的转变，并讨论了现有基准的优点、偏见和不足。
*   <strong>识别失败根源：</strong> 论文将组合性失败归因于三个关键因素：训练数据中缺乏明确的否定表达；连续注意力架构不适合离散逻辑；以及评估指标偏重视觉合理性而非约束满足。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能急剧下降：</strong> 分析表明，当否定、计数和空间关系等基本原语组合时，模型的性能会急剧下降，表现出严重的干扰（即次乘性性能下降，<script type="math/tex">\rho(y) < 1</script>）。
*   <strong>数据-架构不匹配：</strong> 训练数据中组合原语（特别是否定和高计数场景）的稀疏性，以及连续注意力机制倾向于近似多数模式而非离散逻辑，是导致模型生成看似合理但语义不忠实图像的关键原因。
*   <strong>组合性中的涌现复杂性：</strong> 论文指出，当原语组合时，约束满足问题会变成NP-hard，导致模型在处理复杂提示时出现约束权衡、局部满足但全局不一致等特征性失败。
*   <strong>现有解决方案的局限性：</strong> 论文指出，当前的解决方案和简单的规模扩展无法弥补这一差距，强调需要表示和推理方面的根本性进展，而非对现有架构的增量调整。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>训练数据稀疏性：</strong> 训练数据中明确的否定、高计数场景和复杂空间关系的样本极度稀疏，导致模型难以学习这些模式。
*   <strong>架构限制：</strong> 连续注意力机制本质上不适合离散逻辑和精确计数，导致模型在处理这些任务时出现固有缺陷。
*   <strong>评估指标偏差：</strong> 现有评估指标（如视觉合理性）未能充分捕捉组合忠实度，导致模型优化方向偏离语义准确性。人类评估存在不一致性和可扩展性问题，而自动化指标可能引入系统性偏差。
*   <strong>理论基础不足：</strong> 组合生成领域的计算下限和可学习模式仍不明确，缺乏形式化的理论框架来指导架构创新。
*   <strong>现有方法的局限：</strong> 尽管有数据增强、架构修改和混合方法，但它们往往在复杂否定、高计数或多原语组合场景中表现不佳，且可能面临灾难性遗忘和分布偏移问题。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文提出了以下未来研究方向：
*   <strong>理论基础：</strong> 建立关于忠实组合的计算下限和可学习模式的理论框架，并探索与经典约束满足（如SAT求解）和神经符号方法的联系。
*   <strong>架构创新：</strong> 开发能够平衡连续和离散推理的架构，例如模块化架构、记忆增强网络和分层模型，以明确处理“什么”、“哪里”和“多少”等概念。
*   <strong>训练范式：</strong> 设计优先考虑约束满足而非感知质量的训练目标，并利用课程学习和组合数据增强来提高泛化能力，同时解决数据效率和奖励规范问题。
*   <strong>评估方法：</strong> 开发能够衡量组合泛化而非记忆的基准，并更好地表征视觉吸引力与组合忠实度之间的权衡。
*   <strong>扩展到更抽象概念：</strong> 将在否定、计数和空间关系方面开发的技术扩展到时间推理、因果关系和抽象概念，为可控、可靠和真正智能的图像生成铺平道路。</p>
<p>总而言之，这篇论文深刻剖析了当前T2I模型在组合性方面的核心缺陷，强调了实现真正组合性需要跨理论、架构、训练和评估等多个层面的根本性突破，而非仅仅是增量改进。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10136v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10136v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.09057v2'></a></p>
<h2 id="pan-a-world-model-for-general-interactable-and-long-horizon-world-simulation"><a href="https://arxiv.org/abs/2511.09057v2">PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</a></h2>
<p><strong>Authors:</strong>  PAN Team, Jiannan Xiang, Yi Gu, Zihan Liu, Zeyu Feng, Qiyue Gao, Yiyan Hu, Benhao Huang, Guangyi Liu, Yichi Yang, Kun Zhou, Davit Abrahamyan, Arif Ahmad, Ganesh Bannur, Junrong Chen, Kimi Chen, Mingkai Deng, Ruobing Han, Xinqi Huang, Haoqiang Kang, Zheqi Li, Enze Ma, Hector Ren, Yashowardhan Shinde, Rohan Shingre, Ramsundar Tanikella, Kaiming Tao, Dequan Yang, Xinle Yu, Cong Zeng, Binglin Zhou, Zhengzhong Liu, Zhiting Hu, Eric P. Xing</p>
<p><strong>Published:</strong> 2025-11-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>PAN 引入了一个通用、可交互且支持长时序的世界模型，它能够根据历史信息和自然语言动作，通过高质量的视频模拟来预测未来的世界状态。该模型通过结合基于大型语言模型（LLM）的自回归潜在动力学骨干和视频扩散解码器，实现了潜在空间推理与可实现世界动力学的统一，从而支持开放域、动作条件下的连贯长时序模拟。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>PAN 的核心创新在于其 <strong>Generative Latent Prediction (GLP) 架构</strong>，它巧妙地结合了两种强大的模型范式：</p>
<ul>
<li><strong>基于大型语言模型 (LLM) 的自回归潜在动力学骨干：</strong> 这是关键所在。LLM 的引入使得模型能够：<ul>
<li><strong>将模拟建立在广泛的文本知识之上：</strong> 利用 LLM 强大的世界知识和推理能力，使得潜在空间中的“想象”更加符合现实世界的逻辑和常识。</li>
<li><strong>支持自然语言指定的动作条件：</strong> 允许用户或智能体通过自然语言指令来控制模拟，极大地提高了交互性和通用性。</li>
<li><strong>实现潜在空间推理 (imagination)：</strong> LLM 在潜在空间中处理和预测世界状态的演变，这对于长时序的连贯性至关重要。</li>
</ul>
</li>
<li><strong>视频扩散解码器：</strong> 负责将潜在空间中的预测结果解码为感知上细节丰富且时间上连贯的视觉观测（即高质量视频）。这弥合了抽象推理与具象视觉输出之间的鸿沟。</li>
</ul>
<p>这种“LLM驱动的潜在动力学 + 扩散模型视觉生成”的组合，是其实现“通用、可交互、长时序”世界模拟的关键。它解决了现有视频生成模型缺乏因果控制和长时序一致性，以及现有世界模型在通用性和可控性上的局限。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<p>PAN 对计算机视觉和更广泛的AI领域具有深远影响：</p>
<ul>
<li><strong>推动通用世界模型的发展：</strong> 克服了现有世界模型在特定领域和有限深度上的限制，向构建能够理解和模拟复杂、开放世界动态的通用AI迈出了重要一步。</li>
<li><strong>赋能更强大的智能体：</strong> 智能体将能够进行更高级的“想象、预测和推理”，从而制定更有效的规划和策略，尤其是在需要长时序考量的任务中。</li>
<li><strong>提升人机交互：</strong> 通过自然语言动作控制模拟的能力，将使得用户能够更直观、更灵活地与模拟环境进行交互，为虚拟现实、增强现实、游戏和教育等领域带来新的可能性。</li>
<li><strong>促进具身智能和机器人学：</strong> 为机器人提供一个强大的模拟沙盒，用于学习和测试复杂的行为策略，减少对真实世界昂贵且耗时的数据采集和实验的依赖。</li>
<li><strong>模糊了“想象”与“现实”的界限：</strong> 实现了潜在空间推理与可实现世界动力学的统一，这对于理解智能如何从抽象概念映射到具象感知具有重要的理论意义。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>具身智能与机器人学：</strong> 机器人可以通过模拟环境学习复杂的任务、规划路径、测试控制策略，而无需在物理世界中进行大量试错。</li>
<li><strong>强化学习：</strong> 提供一个高质量、可控的模拟环境，用于训练和评估强化学习智能体，尤其是在需要长时序规划和探索的任务中。</li>
<li><strong>虚拟现实 (VR) / 增强现实 (AR)：</strong> 创建更具沉浸感、交互性和动态性的虚拟世界，用户可以通过自然语言与环境互动。</li>
<li><strong>游戏开发：</strong> 生成更智能、更具动态性的游戏世界和非玩家角色 (NPC) 行为，提升游戏体验。</li>
<li><strong>内容创作：</strong> 辅助电影、动画、广告等领域的视频内容生成，通过语言指令控制场景和角色行为。</li>
<li><strong>科学模拟与预测：</strong> 在气候建模、城市规划、灾害预测等领域，通过模拟不同干预措施的效果。</li>
<li><strong>教育与培训：</strong> 提供交互式模拟环境，用于技能培训、概念学习等。</li>
</ul>
<p><strong>5. 从摘要中推断出的潜在局限性</strong></p>
<ul>
<li><strong>训练数据规模和多样性：</strong> 摘要提到“Trained on large-scale video-action pairs spanning diverse domains”，但“large-scale”和“diverse”的程度仍是关键。如果数据不够全面，模型在某些特定或罕见场景下的泛化能力可能受限。</li>
<li><strong>计算资源需求：</strong> 结合了LLM和视频扩散模型，这两种模型都以其巨大的计算需求而闻名。训练和部署这样的模型可能需要非常庞大的计算资源。</li>
<li><strong>“真实性”与“可信度”的平衡：</strong> 尽管强调“高品质视频模拟”和“连贯、长时序动力学”，但在极端或复杂交互下，模拟的物理真实性、因果链的准确性以及与现实世界的偏差仍需严格评估。LLM的“幻觉”问题也可能在潜在空间推理中体现，导致模拟结果与现实不符。</li>
<li><strong>动作粒度和复杂性：</strong> “自然语言动作”的粒度和复杂性可能存在限制。模型可能擅长理解高层语义动作（如“拿起杯子”），但在处理非常精细、多步骤或需要复杂物理交互的动作时，其表现可能下降。</li>
<li><strong>评估指标的挑战：</strong> 评估“通用、可交互、长时序”世界模型的性能本身就是一项挑战。除了传统的视频质量指标，如何量化模拟的因果准确性、规划能力和推理能力，需要更复杂的评估框架。</li>
<li><strong>“开放域”的边界：</strong> 尽管声称支持“开放域”，但任何模型都有其知识边界。当遇到训练数据中从未出现过的概念、物体或交互时，模型的表现可能会下降。</li>
</ul>
<hr />
<p>总而言之，PAN 代表了世界模型研究领域的一个重大进步，它通过巧妙地融合了LLM和扩散模型的优势，为构建更通用、更智能的AI系统铺平了道路。其在计算机视觉领域的意义在于，它不仅仅是生成逼真的视频，更重要的是，它能够以一种可控、可推理的方式生成视频，从而实现对未来世界状态的预测和模拟，这对于智能体的感知、决策和行动至关重要。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.09057v2">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.09057v2">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10647v1'></a></p>
<h2 id="depth-anything-3-recovering-the-visual-space-from-any-views"><a href="https://arxiv.org/abs/2511.10647v1">Depth Anything 3: Recovering the Visual Space from Any Views</a></h2>
<p><strong>Authors:</strong> Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Haotong Lin等人撰写的论文“Depth Anything 3: Recovering the Visual Space from Any Views”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Depth Anything 3: Recovering the Visual Space from Any Views</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决从任意数量的视觉输入（无论是否已知相机姿态）中恢复空间一致的3D几何结构这一核心问题。现有的3D视觉任务（如单目深度估计、运动结构、多视角立体视觉、同步定位与建图）通常依赖高度专业化的模型，这些模型在处理不同输入类型或任务时存在局限性，并且难以有效利用大规模预训练模型。作者寻求一种最小化的建模策略，以实现对视觉空间的通用3D结构恢复。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
Depth Anything 3 (DA3) 提出了以下关键创新：
*   <strong>统一的单平面Transformer骨干网络：</strong> DA3证明，一个单一的、普通的Transformer（例如，香草DINO编码器）作为骨干网络就足够了，无需专门的架构修改，即可处理任意视角的输入。这使得模型能够继承大规模预训练模型的强大特征提取能力。
*   <strong>单一深度-射线预测目标：</strong> 模型采用单一的深度-射线（depth-ray）预测目标，避免了复杂的多任务学习。深度-射线表示法能够捕获场景结构和相机运动，并通过元素级操作实现一致的点云生成。
*   <strong>教师-学生训练范式：</strong> 采用教师-学生训练范式，利用合成数据训练强大的教师模型生成高质量的伪标签，并将其与真实世界数据中的稀疏或噪声深度对齐，从而在不牺牲几何准确性的前提下增强标签细节和完整性。
*   <strong>输入自适应的跨视角自注意力机制：</strong> 引入了输入自适应的跨视角自注意力机制，通过在选定层中动态重排token，实现所有视角之间的高效信息交换，从而处理任意数量的输入视图。
*   <strong>双DPT头部：</strong> 设计了一个新颖的双DPT头部，用于联合预测密集的深度图和射线值，确保两个预测任务之间的强交互，同时避免冗余的中间表示。
*   <strong>新的视觉几何基准：</strong> 建立了一个全面的视觉几何基准，涵盖相机姿态估计、任意视角几何和视觉渲染，以更好地评估模型并跟踪该领域的进展。</p>
<p><strong>3. 主要结果及其意义</strong>
DA3在所建立的视觉几何基准上取得了显著的SOTA（State-of-the-Art）性能：
*   <strong>相机姿态准确性：</strong> 在相机姿态准确性方面，DA3平均超越了先前的SOTA VGGT模型44.3%。
*   <strong>几何准确性：</strong> 在几何准确性方面，DA3平均超越了VGGT模型25.1%。
*   <strong>单目深度估计：</strong> 在单目深度估计任务中，DA3也优于Depth Anything 2 (DA2)。
*   <strong>视觉渲染：</strong> 结合3D高斯飞溅（3DGS）的下游任务中，DA3作为几何骨干网络，显著提高了渲染质量，尤其在处理薄结构和宽基线户外环境等挑战性区域表现出色。
*   <strong>效率和泛化性：</strong> 即使是参数量更小的DA3-Large模型，其性能也超越了先前的SOTA模型，展现了卓越的效率。所有模型均仅使用公开学术数据集进行训练，证明了其强大的泛化能力。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>推理计算成本：</strong> 从射线图恢复相机姿态在推理时计算成本较高，尽管通过添加轻量级相机头部Dc来预测视场、旋转和位移，可以减轻这一问题。
*   <strong>特定场景的性能饱和：</strong> 在某些数据集（如7Scenes）上，由于有限的视频设置，姿态条件带来的性能提升不如其他数据集明显，表明在这些情况下性能可能已达到饱和。
*   <strong>教师模型对合成数据的依赖：</strong> 教师模型主要在合成数据上训练，以提供高质量的伪标签。尽管通过RANSAC对齐，但合成数据与真实世界数据之间的差异仍可能影响模型的泛化性。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>动态场景推理：</strong> 将DA3的推理能力扩展到动态场景。
*   <strong>语言和交互线索整合：</strong> 将语言和交互线索整合到模型中，以实现更全面的世界理解。
*   <strong>更大规模的预训练：</strong> 探索更大规模的预训练，以弥合几何理解与可操作世界模型之间的差距，从而推动通用3D感知的研究。
*   <strong>其他3D视觉任务：</strong> 期望DA3强大的几何理解能力能惠及其他3D视觉任务。</p>
<hr />
<p>这篇论文通过其最小化的建模策略、创新的深度-射线表示和教师-学生训练范式，为通用3D几何恢复设定了新的标准，并为未来3D感知领域的研究开辟了广阔前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses.</li>
<li>We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering.</li>
<li>On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy.</li>
<li>Moreover, it outperforms DA2 in monocular depth estimation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10647v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10518v1'></a></p>
<h2 id="semanticvla-semantic-aligned-sparsification-and-enhancement-for-efficient-robotic-manipulation"><a href="https://arxiv.org/abs/2511.10518v1">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Wei Li, Renshan Zhang, Rui Shao, Zhijian Fang, Kaiwen Zhou, Zhuotao Tian, Liqiang Nie</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Wei Li等人撰写的论文“SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation”的全面摘要。</p>
<hr />
<h3 id="semanticvla">SemanticVLA: 面向高效机器人操作的语义对齐稀疏化与增强</h3>
<p><strong>1. 主要问题或研究问题</strong></p>
<p>当前视觉-语言-动作（VLA）模型在机器人操作方面取得了显著进展，但其实际部署仍面临两大关键限制：
1. <strong>感知冗余（Perceptual Redundancy）</strong>：模型低效处理不相关的视觉输入，导致计算资源浪费。
2. <strong>指令-视觉对齐肤浅（Superficial Instruction-Vision Alignment）</strong>：模型难以将语义信息与动作进行深度关联，阻碍了动作的语义接地。</p>
<p>这些限制导致在动态和杂乱环境中，现有VLA模型在计算效率、任务接地和可解释性方面表现不佳。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>为了解决上述挑战，本文提出了<strong>SemanticVLA</strong>，一个新颖的VLA框架，通过语义对齐的稀疏化和增强来实现高效且可解释的机器人操作。其核心创新包括三个集成模块：</p>
<ol>
<li>
<p><strong>语义引导双重视觉剪枝器（Semantic-guided Dual Visual Pruner, SD-Pruner）</strong>：</p>
<ul>
<li><strong>指令驱动剪枝器（Instruction-driven Pruner, ID-Pruner）</strong>：利用SigLIP提取全局动作线索和局部语义锚点，通过指令-图像跨模态相似性进行剪枝，保留最相关的视觉信息。</li>
<li><strong>空间聚合剪枝器（Spatial-aggregation Pruner, SA-Pruner）</strong>：利用DINOv2将几何丰富的特征压缩为任务适应性令牌，并通过FiLM层进行指令调制，以补充SigLIP的语义信息。</li>
<li><strong>创新点</strong>：SD-Pruner通过指令感知令牌过滤和几何感知聚合，显著稀疏化冗余感知，同时保持语义对齐。</li>
</ul>
</li>
<li>
<p><strong>语义互补分层融合器（Semantic-complementary Hierarchical Fuser, SH-Fuser）</strong>：</p>
<ul>
<li><strong>密集融合器（Dense-Fuser）</strong>：在SigLIP和DINOv2的多个Transformer块之间插入，进行跨模态信息交换，确保语义线索与空间几何先验在不同阶段得到增强。</li>
<li><strong>稀疏融合器（Sparse-Fuser）</strong>：在最终阶段，融合ID-Pruner和SA-Pruner产生的显著令牌，形成紧凑的表示。</li>
<li><strong>创新点</strong>：SH-Fuser通过双流融合机制，整合密集补丁特征和稀疏语义令牌，增强指令语义和空间结构对齐，将视觉令牌数量减少8-16倍，同时保留判别性表示。</li>
</ul>
</li>
<li>
<p><strong>语义条件动作耦合器（Semantic-conditioned Action Coupler, SA-Coupler）</strong>：</p>
<ul>
<li>取代了传统的观察到自由度（DoF）方法，将7-DoF动作（3-DoF平移、3-DoF旋转、1-DoF抓取）表示为单一令牌，实现动作类型的统一语义建模。</li>
<li>设计了专门的预测头，直接回归连续运动参数。</li>
<li><strong>创新点</strong>：SA-Coupler实现了从稀疏感知到语义动作类型的更直观高效映射，提高了动作解码的效率和可解释性。</li>
</ul>
</li>
</ol>
<p><strong>3. 主要结果及其意义</strong></p>
<p>SemanticVLA在模拟和真实世界任务中均取得了显著的性能和效率提升：</p>
<ul>
<li><strong>性能</strong>：在LIBERO基准测试中，SemanticVLA的成功率超越OpenVLA <strong>21.1%</strong>，达到<strong>97.7%</strong>（排名第一）。在真实世界任务中，其成功率也显著优于现有SOTA方法，例如在长时序任务中比OpenVLA-OFT高出22.2%。</li>
<li><strong>效率</strong>：<ul>
<li>训练成本降低 <strong>3.0倍</strong>。</li>
<li>推理延迟降低 <strong>2.7倍</strong>。</li>
<li>视觉输入令牌和动作表示分别减少了1/16或1/8和3/7。</li>
</ul>
</li>
<li><strong>语义接地与泛化</strong>：定性分析表明，SemanticVLA能够准确识别关键几何锚点、区分视觉相似物体、执行多步操作，并在复杂长时序任务中展现出强大的指令遵循能力和泛化性。</li>
</ul>
<p>这些结果表明SemanticVLA在机器人操作领域设立了新的SOTA，通过语义对齐的稀疏化和增强，显著提升了模型的实用性和可部署性。</p>
<p><strong>4. 论文中提到的局限性</strong></p>
<p>尽管SemanticVLA在模拟和真实世界机器人操作任务中表现出色，但仍存在一些局限性：</p>
<ul>
<li><strong>缺乏主动感知和记忆机制</strong>：当前框架尚未整合主动感知或记忆机制，这对于长时序和部分可观察场景中的有效任务执行至关重要。</li>
<li><strong>语言理解能力</strong>：系统处理更具组合性、抽象性或对话驱动指令的语言理解能力仍是一个重大挑战。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>未来的工作将集中于：</p>
<ul>
<li><strong>自适应动作预测策略</strong>：整合强化学习或元学习，以实现更具适应性的动作预测策略。</li>
<li><strong>视觉记忆和时间推理模块</strong>：增强SemanticVLA的视觉记忆和时间推理模块，以支持持久的长时序执行。</li>
<li><strong>交互式语言接地</strong>：整合基于对话或纠正反馈机制的交互式语言接地，以提高系统在开放世界环境中的可用性。</li>
</ul>
<hr />
<p>这篇论文通过其创新的稀疏化、融合和动作耦合机制，为机器人操作中的VLA模型提供了一个高效且可解释的解决方案，在计算机视觉和机器人领域具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation.</li>
<li>Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10518v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10518v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10488v1'></a></p>
<h2 id="spot-sparsification-with-attention-dynamics-via-token-relevance-in-vision-transformers"><a href="https://arxiv.org/abs/2511.10488v1">SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</a></h2>
<p><strong>Authors:</strong> Oded Schlesinger, Amirhossein Farzam, J. Matias Di Martino, Guillermo Sapiro</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Oded Schlesinger等人撰写的论文“SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers”的全面摘要。</p>
<hr />
<p><strong>论文摘要：SPOT: 基于Token相关性的Vision Transformer注意力动态稀疏化</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
Vision Transformer (ViT) 在各种计算机视觉任务中表现出色，但其计算成本高昂，特别是注意力机制的计算复杂度与处理的token数量呈二次方关系。这导致ViT在资源受限环境下的可扩展性受限。论文旨在解决ViT中冗余token的早期检测和消除问题，以提高计算效率，同时保持或提升模型性能。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
论文提出了 <strong>SParsification with attentiOn dynamics via Token relevance (SPOT)</strong> 框架，其核心创新在于：
*   <strong>多层注意力动态集成：</strong> SPOT通过整合来自多个ViT块的token嵌入、token交互以及注意力动态的紧凑表示来推断token的重要性。这使得token重要性检测过程更具上下文感知性和可解释性，能够捕捉token在处理管道中相对重要性的变化。
*   <strong>轻量级预测器：</strong> SPOT采用计算开销很小的预测器，可以即插即用到各种ViT架构中，并学习在不同层之间推导出有效的、输入特定的token优先级。
*   <strong>统计矩的利用：</strong> 为了解决注意力图中可能存在的“虚假高注意力值”和噪声波动问题，SPOT通过计算注意力分数分布的统计矩（均值和方差）来聚合注意力信息，从而提供更鲁棒的token显著性估计。
*   <strong>分层兼容的稀疏化：</strong> 框架支持分层token稀疏化，一旦token在某个迭代中被遮蔽，它在后续迭代中将保持被遮蔽，从而逐步消除不相关的token，集中计算资源于更显著的token子集。
*   <strong>混合策略：</strong> SPOT结合了token嵌入和注意力图的丰富信息，能够分析token间的动态变化，并高效识别对任务贡献最小的token。</p>
<p><strong>3. 主要结果及其重要性</strong>
*   <strong>显著的效率提升：</strong> 经验评估表明，SPOT在保持甚至提高准确性的同时，实现了高达40%的显著效率提升（例如，在DeiT-S模型上，GFLOPS从4.6降低到2.8）。
*   <strong>性能保持或提升：</strong> 在ImageNet-1K数据集上，SPOT在DeiT-T、DeiT-S、LV-ViT-T和LV-ViT-S等多种ViT模型上，相比基线和其他硬稀疏化方法，在相似计算预算下实现了相当或更高的分类准确率。
*   <strong>鲁棒性和泛化能力：</strong> SPOT在各种图像扰动（如ImageNet-C数据集）下表现出一致的鲁棒性，并且在跨不同视觉域（如CIFAR-100、Food-101、DTD、EuroSAT）的评估中，其性能优于基线模型，表明其具有良好的泛化能力和适应性。
*   <strong>可解释性：</strong> 可视化结果显示，SPOT识别出的更具信息量的token与图像的语义对象和视觉特征高度对齐，这增强了其可解释性。
*   <strong>低开销：</strong> 提出的预测器引入的内部开销极小（约占模型GFLOPS和参数数量的4%），远低于通过token稀疏化实现的计算节省。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中没有明确指出SPOT框架本身的具体局限性，但提到了：
*   <strong>现有方法的局限：</strong> 现有稀疏化方法通常依赖于单一模型状态（如当前层token嵌入或注意力图）进行决策，这可能导致次优预测或无法捕捉跨层动态。SPOT正是为了克服这些局限而设计的。
*   <strong>软稀疏化方法的饱和性能：</strong> 在与软稀疏化方法集成时，SPOT的性能提升可能在绝对值上显得“适度”，因为现有方法已经达到了饱和性能，留给改进的空间很小。这并非SPOT的局限，而是其在饱和场景下仍能提供增益的证明。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文没有明确提出未来的研究方向，但从其贡献和讨论中可以推断出以下潜在方向：
*   <strong>更复杂的注意力动态建模：</strong> 虽然SPOT已经整合了多层注意力动态，但可以探索更复杂的时序或图神经网络来捕捉更细粒度的token演化模式。
*   <strong>自适应稀疏化策略：</strong> 目前的稀疏化率p是预设的，未来可以研究更智能、更自适应的策略，根据输入内容或任务动态调整稀疏化率。
*   <strong>与其他高效ViT方法的结合：</strong> SPOT的模块化设计使其可以与现有的其他高效ViT技术（如低秩近似、局部敏感哈希、token压缩/融合等）进一步结合，以实现更大的计算效率提升。
*   <strong>在更多下游任务中的应用：</strong> 论文主要关注图像分类任务，未来可将SPOT扩展到其他计算密集型ViT应用，如目标检测、语义分割或视频处理，以验证其在更复杂场景下的有效性。
*   <strong>理论分析的深化：</strong> 论文从统计学角度解释了SPOT的有效性，未来可以进一步深化理论分析，例如，量化不同信息源对token重要性估计方差和偏差的具体贡献。</p>
<hr />
<p>这篇论文通过引入SPOT框架，为解决Vision Transformer的计算效率问题提供了一个创新且全面的解决方案。它不仅在实践中取得了显著的效率提升和性能保持，而且通过整合多层注意力动态和统计矩，增强了token重要性检测的上下文感知性和可解释性，为ViT的实际部署提供了有力的支持。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10488v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10488v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10403v1'></a></p>
<h2 id="nuplan-r-a-closed-loop-planning-benchmark-for-autonomous-driving-via-reactive-multi-agent-simulation"><a href="https://arxiv.org/abs/2511.10403v1">nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</a></h2>
<p><strong>Authors:</strong> Mingxing Peng, Ruoyu Yao, Xusen Guo, Jun Ma</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Mingxing Peng、Ruoyu Yao、Xusen Guo和Jun Ma撰写的论文“nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation”的全面摘要。</p>
<hr />
<h3 id="nuplan-r">nuPlan-R: 自动驾驶反应式多智能体仿真闭环规划基准测试</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的自动驾驶闭环规划基准测试（如nuPlan和Waymax）在评估自动驾驶系统时，主要依赖于基于规则的反应式智能体（如智能驾驶员模型IDM）。这些智能体缺乏行为多样性，无法捕捉真实的人类交互，导致交通动态过于简化，无法准确评估规划器在复杂、动态和交互式场景中的性能。论文旨在解决这一限制，提供一个更真实、更公平的自动驾驶规划评估环境。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>引入学习型反应式多智能体仿真：</strong> nuPlan-R将基于学习的反应式多智能体仿真无缝集成到nuPlan框架中，取代了传统的基于规则的IDM智能体。具体而言，它采用了基于Nexus架构的去噪扩散模型，生成更真实、多样化和类人行为的交通流。
*   <strong>交互感知智能体选择机制：</strong> 为了提高仿真效率和缓解生成与真实交通分布之间的协变量偏移，论文引入了一种交互感知智能体选择机制。该机制通过综合考虑相对距离、航向差异和速度差异来计算交互强度分数，只选择与自车最相关的Top-k智能体作为反应式智能体进行扩散模型更新，其余智能体则遵循日志回放轨迹。
*   <strong>扩展评估指标：</strong> 在nuPlan原有的闭环分数（CLS）基础上，nuPlan-R引入了两个补充指标：
    *   <strong>成功率（Success Rate, SR）：</strong> 衡量规划器在闭环仿真中保持安全可行轨迹的鲁棒性，反映规划器避免严重碰撞、偏离道路等重大故障的能力。
    *   <strong>全核心通过率（All-Core Pass Rate, PR）：</strong> 评估规划器在所有核心子指标（如安全性、舒适性和效率）上是否一致地达到标准化分数，突出规划器在多样化驾驶场景中的整体平衡性和合规性。
*   <strong>重新实现和评估多种规划方法：</strong> 论文在nuPlan-R基准测试中重新实现了多种基于规则、基于学习和混合的规划方法，以提供更清晰的规划器性能对比。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>反应式智能体行为的真实性、多样性和合理性：</strong> 实验证明，nuPlan-R中基于学习的反应式智能体模型在交通时间（TTC）分布、聚类轨迹FID和香农熵等指标上，均优于基于规则的IDM和扩散基线模型。它能生成更真实、多样化和类人的交通行为，更好地反映现实世界的交互式驾驶。
*   <strong>规划器性能评估的改进：</strong> 在nuPlan-R基准测试中，基于规则的规划器（如IDM和PDM-Closed）的CLS性能显著下降，而基于学习的规划器（如PlanTF和Diffusion-Planner）的性能则有明显提升。这表明nuPlan-R能更公平、有效地评估规划器在复杂交互场景中的决策和交互能力，并更好地凸显基于学习的规划器在处理复杂动态场景中的优势。
*   <strong>新指标的补充价值：</strong> SR和PR指标提供了对规划器鲁棒性和整体平衡性的补充洞察。SR揭示了CLS可能掩盖的规划器脆弱性，而PR则评估了规划器在所有维度上的一致性表现，而非仅仅在少数方面表现突出。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文中没有明确提及当前工作的具体局限性。然而，从其未来工作方向可以推断，当前模型在多智能体交通仿真中的多样性、可控性和可解释性方面仍有提升空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的研究计划将整合大型语言模型或视觉语言模型，以进一步增强反应式多智能体交通仿真的多样性、可控性和可解释性。</p>
<hr />
<p>这份摘要涵盖了论文的核心内容，突出了其在自动驾驶闭环规划基准测试领域的重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework.</li>
<li>These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation.</li>
<li>We will open-source the code for the new benchmark.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10403v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10403v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10391v1'></a></p>
<h2 id="groundiff-diffusion-based-ground-surface-generation-from-digital-surface-models"><a href="https://arxiv.org/abs/2511.10391v1">GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</a></h2>
<p><strong>Authors:</strong> Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Digital Terrain Models (DTMs) represent the bare-earth elevation and are important in numerous geospatial applications. Such data models cannot be directly measured by sensors and are typically generated from Digital Surface Models (DSMs) derived from LiDAR or photogrammetry. Traditional filtering approaches rely on manually tuned parameters, while learning-based methods require well-designed architectures, often combined with post-processing. To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task. We incorporate a gated design with confidence-guided generation that enables selective filtering. To increase scalability, we further propose Prior-Guided Stitching (PrioStitch), which employs a downsampled global prior automatically generated using GrounDiff to guide local high-resolution predictions. We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks. In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization. Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods. The project page is available at https://deepscenario.github.io/GrounDiff/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="groundiff-diffusion-based-ground-surface-generation-from-digital-surface-models_1">论文摘要分析：GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文引入了GrounDiff，一个开创性的基于扩散模型（diffusion-based）的框架，用于从数字表面模型（DSM）生成数字地形模型（DTM）。它通过将非地面结构移除问题建模为去噪任务，并结合了置信度引导的生成和门控设计，实现了选择性过滤。GrounDiff在多个基准测试中显著优于现有的深度学习方法，并在道路重建等应用中展现出卓越的性能。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<ul>
<li><strong>首次将扩散模型应用于DSM到DTM的转换：</strong> 这是最核心的创新点。传统的DSM到DTM转换方法要么是基于手动调参的滤波，要么是需要精心设计的神经网络架构。GrounDiff将这个问题重新定义为一个迭代去噪任务，利用扩散模型的强大生成能力来逐步移除非地面物体（如建筑物、植被），从而得到裸地高程。</li>
<li><strong>置信度引导的生成与门控设计：</strong> 这种机制允许模型在生成过程中进行选择性过滤。它可能意味着模型能够识别哪些区域是“地面”或“非地面”的置信度，并据此调整去噪过程，避免过度平滑或错误地移除地面特征。</li>
<li><strong>Prior-Guided Stitching (PrioStitch) 提高可扩展性：</strong> 为了处理大规模高分辨率数据，PrioStitch利用GrounDiff自动生成的下采样全局先验来指导局部高分辨率预测。这是一种非常实用的策略，解决了扩散模型在处理超高分辨率数据时常见的计算成本问题，同时保持了局部细节的准确性。</li>
<li><strong>任务无关的卓越性能：</strong> 论文强调GrounDiff在DSM到DTM转换和道路重建这两个不同任务上都表现出色，尤其是在道路重建中，它在没有任务特定优化的情况下，仅使用DSM输入就达到了与专业技术相当甚至更好的精度和光滑度。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>范式转变：</strong> 将扩散模型引入DSM到DTM转换领域，可能会引发该领域的研究范式转变。研究人员可能会开始探索扩散模型在其他地球空间数据处理任务中的应用。</li>
<li><strong>提高DTM生成质量和效率：</strong> 显著降低RMSE（高达93%和47%）表明GrounDiff能够生成更准确的DTM。这将直接提升依赖DTM的各种应用（如洪水模拟、城市规划、基础设施建设）的可靠性。</li>
<li><strong>减少人工干预：</strong> 相较于传统依赖手动调参的滤波方法，GrounDiff的自动化特性将大大减少人工干预，提高工作效率。</li>
<li><strong>推动地球空间AI发展：</strong> 证明了扩散模型在处理复杂地球空间数据中的潜力，可能会激励更多AI研究者关注这一领域。</li>
<li><strong>为特定应用提供通用解决方案：</strong> 在道路重建任务中的出色表现，表明GrounDiff可能成为一个通用的基础模型，通过少量微调或无需微调即可适应多种下游任务。</li>
</ul>
<p><strong>4. 相关领域或应用可能受益</strong></p>
<ul>
<li><strong>地理信息系统 (GIS) 和遥感：</strong> DTM是GIS和遥感的核心数据，GrounDiff的改进将直接惠及这些领域。</li>
<li><strong>城市规划与管理：</strong> 准确的DTM对于城市建模、基础设施规划、建筑高度限制、排水系统设计至关重要。</li>
<li><strong>灾害管理：</strong> 洪水模拟、滑坡风险评估等需要精确的地面高程数据。</li>
<li><strong>环境科学：</strong> 植被覆盖分析、水文模型、土壤侵蚀研究。</li>
<li><strong>自动驾驶与机器人：</strong> 道路重建和精确地形信息对于高精度地图构建和环境感知至关重要。GrounDiff+在道路重建中的表现尤其值得关注。</li>
<li><strong>林业与农业：</strong> 裸地模型可以帮助评估森林生物量、农田地形分析等。</li>
<li><strong>考古学：</strong> 发现被植被覆盖的古代遗迹。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 尽管PrioStitch旨在提高可扩展性，但扩散模型通常在训练和推理时对计算资源（尤其是GPU内存和计算时间）有较高要求。摘要中没有具体说明其计算效率与现有SOTA方法的对比。</li>
<li><strong>训练数据依赖：</strong> 作为一种学习型方法，GrounDiff的性能可能高度依赖于训练数据的质量和多样性。如果遇到与训练数据分布差异很大的新地形或传感器数据，其泛化能力可能受到影响。</li>
<li><strong>“置信度引导”和“门控设计”的具体实现细节：</strong> 摘要中没有详细说明这些机制是如何具体实现的，这可能影响其鲁棒性和可解释性。</li>
<li><strong>对特定非地面结构的鲁棒性：</strong> 摘要提到“移除非地面结构”，但没有具体说明对各种复杂非地面结构（如高层建筑、茂密森林、桥梁、电力线等）的处理能力。在极端复杂场景下，其性能可能需要进一步验证。</li>
<li><strong>“GrounDiff+”的额外优化：</strong> 摘要提到GrounDiff+是为了产生更光滑的表面，但没有说明这种优化是否会牺牲其他方面的性能（例如，在某些情况下可能会过度平滑导致细节丢失）。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过将扩散模型引入DSM到DTM的转换任务，提出了一个新颖且高效的解决方案。其在多个基准测试中的卓越性能，以及在道路重建等特定应用中的强大潜力，使其成为计算机视觉和地球空间AI领域一个非常有趣且重要的贡献。它不仅解决了现有方法的痛点，还为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task.</li>
<li>We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks.</li>
<li>In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization.</li>
<li>Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10391v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10391v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10376v1'></a></p>
<h2 id="msgnav-unleashing-the-power-of-multi-modal-3d-scene-graph-for-zero-shot-embodied-navigation"><a href="https://arxiv.org/abs/2511.10376v1">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a></h2>
<p><strong>Authors:</strong> Xun Huang, Shijia Zhao, Yunxiang Wang, Xin Lu, Wanfa Zhang, Rongsheng Qu, Weixin Li, Yunhong Wang, Chenglu Wen</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Xun Huang等人撰写的论文“MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation”的全面摘要。</p>
<hr />
<h3 id="msgnav-3d">MSGNav: 释放多模态3D场景图在零样本具身导航中的力量</h3>
<p><strong>1. 主要问题或研究问题</strong></p>
<p>该论文主要解决具身导航领域中零样本（zero-shot）方法的局限性。现有的零样本具身导航方法通常依赖于显式构建3D场景图，但这些场景图往往将丰富的视觉观测压缩为纯文本关系，导致以下问题：
* <strong>高昂的构建成本：</strong> 文本关系推理需要频繁的MLLM（多语言大模型）查询。
* <strong>视觉信息丢失：</strong> 将视觉信息转换为纯文本关系会丢失重要的视觉证据，降低对感知错误的鲁棒性。
* <strong>受限的词汇量：</strong> 预设词汇量限制了对新颖类别的表示能力，阻碍了开放词汇泛化。
* <strong>“最后一英里”问题：</strong> 即使目标位置被正确识别，也难以确定一个具有良好视野的最终导航视点，导致导航失败。</p>
<p><strong>2. 关键创新或方法贡献</strong></p>
<p>为了解决上述问题，作者提出了以下关键创新：</p>
<ul>
<li><strong>多模态3D场景图（M3DSG）：</strong> 这是论文的核心贡献。M3DSG通过用动态分配的图像替换传统的纯文本关系边，保留了视觉线索。这使得场景图的构建更高效（无需频繁MLLM查询），提供了视觉上下文以增强鲁棒性，并支持动态的开放词汇泛化。</li>
<li><strong>MSGNav导航系统：</strong> 基于M3DSG，作者提出了一个零样本导航系统MSGNav，包含以下模块：<ul>
<li><strong>关键子图选择（Key Subgraph Selection, KSS）：</strong> 用于从复杂的M3DSG中高效提取与目标相关的子图，显著减少了推理所需的token和时间成本。</li>
<li><strong>自适应词汇更新（Adaptive Vocabulary Update, AVU）：</strong> 利用M3DSG中保留的视觉信息动态更新词汇表，支持开放词汇泛化。</li>
<li><strong>闭环推理（Closed-Loop Reasoning, CLR）：</strong> 引入决策记忆和反馈推理，以实现更准确的探索推理。</li>
<li><strong>基于可见性的视点决策（Visibility-based Viewpoint Decision, VVD）模块：</strong> 明确解决了“最后一英里”问题。该模块通过在目标周围采样候选视点，并根据视点与目标点云之间的遮挡情况计算可见性分数，选择具有最高可见性分数的视点作为最终导航目标。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>论文通过在GOAT-Bench和HM3D-OVON数据集上进行全面的实验，证明了MSGNav的有效性：</p>
<ul>
<li><strong>卓越的性能：</strong> MSGNav在GOAT-Bench和HM3D-OVON数据集上均取得了最先进的性能（state-of-the-art），在成功率（SR）和成功路径长度（SPL）方面显著优于现有方法。例如，在HM3D-OVON上，MSGNav的SR达到48.3%，SPL达到27.0%，远超MTU3D等现有最佳方法。在GOAT-Bench上，MSGNav的SR和SPL也分别达到了52.0%和29.6%。</li>
<li><strong>M3DSG的优越性：</strong> 实验结果表明，M3DSG显著优于传统的纯文本3D场景图，尤其是在处理语言和图像目标时，其SR和SPL有显著提升，证明了多模态关系边的有效性。</li>
<li><strong>各模块的贡献：</strong> 消融实验验证了KSS、AVU、CLR和VVD模块的有效性。特别是，VVD模块将成功率提高了12.5%，显著缓解了“最后一英里”问题。AVU和CLR的协同作用也带来了最佳性能。</li>
<li><strong>开放词汇泛化能力：</strong> MSGNav通过保留视觉上下文和自适应词汇更新，展示了对未见场景和目标的强大泛化能力。</li>
</ul>
<p>这些结果表明，MSGNav通过其多模态3D场景图和创新的导航系统设计，成功克服了零样本具身导航中的关键挑战，实现了更高效、鲁棒且具有开放词汇能力的导航。</p>
<p><strong>4. 论文中提及的局限性</strong></p>
<p>尽管MSGNav取得了显著进展，论文也指出了以下局限性：</p>
<ul>
<li><strong>推理效率：</strong> 基于场景图的方法仍然面临较低的推理效率，这主要是由于VFMs（视觉基础模型）和VLMs的延迟。这限制了实时部署的可能性。</li>
<li><strong>“最后一英里”问题未完全解决：</strong> 尽管VVD模块显著缓解了“最后一英里”问题，但并未完全消除。在某些情况下，即使目标被正确识别，代理仍可能因视点未完全对齐而失败。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>更快的图构建和推理：</strong> 探索如何加速场景图的构建和推理过程，以实现实时部署。</li>
<li><strong>主动目标识别：</strong> 探索强化学习方法，例如主动目标识别，以进一步解决“最后一英里”问题。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images.</li>
<li>Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning.</li>
<li>Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10376v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10376v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10279v1'></a></p>
<h2 id="propa-toward-process-level-optimization-in-visual-reasoning-via-reinforcement-learning"><a href="https://arxiv.org/abs/2511.10279v1">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning”论文的全面摘要，由Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau撰写。</p>
<hr />
<h3 id="propa">PROPA: 通过强化学习实现视觉推理中的过程级优化</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
尽管视觉-语言模型（VLMs）取得了显著进展，但在复杂的视觉推理任务中，它们仍然面临挑战。这些任务通常涉及多步骤依赖，早期步骤的错误很容易在推理链中级联传播。现有的后训练范式存在局限性：监督微调（SFT）依赖于昂贵的步骤级标注，而可验证奖励强化学习（RLVR）方法（如GRPO）仅提供稀疏的、结果层面的反馈，阻碍了稳定的优化。因此，核心问题是如何在没有人工标注的情况下，为多步骤视觉推理提供密集、过程级的奖励信号，并有效优化推理过程。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
PROPA（Process-level Reasoning Optimization with interleaved Policy Alignment）框架引入了以下关键创新：</p>
<ul>
<li><strong>MCTS-Guided GRPO框架：</strong> PROPA将蒙特卡洛树搜索（MCTS）与GRPO相结合，自动生成并利用过程级奖励进行视觉推理，从而避免了对密集、手动步骤级标注的需求。MCTS的探索能力和奖励反向传播机制自然地提供了密集的、过程级奖励信号。</li>
<li><strong>局部化GRPO与树节点过滤：</strong> 在MCTS生成的树中，PROPA在每个中间推理步骤应用局部化GRPO更新，而不仅仅是最终答案，以鼓励模型生成更可能导致正确最终答案的中间轨迹。通过组级过滤和非线性转换（对Q(s)值进行对数变换），增强了奖励信号的对比度，使GRPO能够更好地识别高质量的推理路径。</li>
<li><strong>交错式GRPO和SFT训练方案：</strong> 为了解决GRPO的冷启动问题，PROPA提出了一种交错式训练方案。它根据MCTS结果动态划分训练数据：对于MCTS成功找到至少一个正确终端节点的案例，使用MCTS引导的GRPO进行优化；对于所有模拟都失败的案例，则使用SFT来保留基本能力并防止灾难性遗忘。这种方案使模型能够从成功和失败的推理轨迹中进行鲁棒学习。</li>
<li><strong>过程奖励模型（PRM）：</strong> 训练了一个过程奖励模型（PRM），以近似训练期间发现的过程奖励，并在推理时作为启发式方法指导MCTS探索。这确保了测试时搜索与训练信号对齐，显著提高了最终答案的准确性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
PROPA在七个基准测试和四个VLM骨干模型上进行了评估，并取得了显著的性能提升：</p>
<ul>
<li><strong>卓越的性能：</strong> PROPA始终优于SFT和RLVR基线方法。在域内任务上实现了高达17.0%的性能提升，在域外任务上实现了高达21.0%的性能提升，超越了现有最先进水平。</li>
<li><strong>强大的推理和泛化能力：</strong> 这些结果表明PROPA在视觉推理任务中建立了强大的推理和泛化能力，尤其是在处理多步骤依赖和避免错误传播方面表现出色。</li>
<li><strong>训练稳定性和收敛性：</strong> 实验结果显示，PROPA框架在训练过程中表现出改进的收敛性和稳定性，尤其是在SFT激活阶段之后，性能有明显提升。</li>
<li><strong>减少感知和逻辑错误：</strong> 定性分析和人工标注研究表明，PROPA显著降低了感知错误和逻辑错误的数量，这反映了其更准确和可靠的中间推理过程。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>与大型闭源模型的差距：</strong> 尽管PROPA表现出色，但与GPT-4.1等大型闭源模型相比，仍存在一定差距。这可能归因于这些模型更大的参数规模或潜在的数据污染。
*   <strong>Trance数据集上的搜索方法局限性：</strong> 在Trance数据集上，MCTS+PRM方法有时不如贪婪搜索有效。这可能与描述对象类型、数量和位置的词汇和语义相似性较高有关，使得细粒度区分更具挑战性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出未来的研究方向，但从其贡献和局限性可以推断出以下几点：</p>
<ul>
<li><strong>扩展到更复杂的推理任务：</strong> 进一步探索PROPA在更复杂、需要更深层次抽象和多模态交互的推理任务中的应用。</li>
<li><strong>优化PRM的泛化能力：</strong> 改进PRM在词汇和语义相似度高的数据集上的表现，使其在各种场景下都能提供更鲁棒的搜索指导。</li>
<li><strong>结合更强大的基础模型：</strong> 探索将PROPA框架与更大、更先进的VLM基础模型结合，以进一步缩小与闭源模型之间的性能差距。</li>
<li><strong>减少计算开销：</strong> 尽管MCTS-guided GRPO优化了GRPO阶段的效率，但MCTS探索树的预生成仍会增加整体训练时间。未来可以研究更高效的MCTS策略或剪枝技术，以减少计算开销。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations.</li>
<li>Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines.</li>
<li>It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10279v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10279v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.10276v1'></a></p>
<h2 id="robobenchmart-benchmarking-robots-in-retail-environment"><a href="https://arxiv.org/abs/2511.10276v1">RoboBenchMart: Benchmarking Robots in Retail Environment</a></h2>
<p><strong>Authors:</strong> Konstantin Soshin, Alexander Krapukhin, Andrei Spiridonov, Denis Shepelev, Gregorii Bukhtuev, Andrey Kuznetsov, Vlad Shakhuro</p>
<p><strong>Published:</strong> 2025-11-13</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Most existing robotic manipulation benchmarks focus on simplified tabletop scenarios, typically involving a stationary robotic arm interacting with various objects on a flat surface. To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items. This setting presents significant challenges, including dense object clutter and varied spatial configurations -- with items positioned at different heights, depths, and in close proximity. By targeting the retail domain, our benchmark addresses a setting with strong potential for near-term automation impact. We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks. To support further research, we release the RoboBenchMart suite, which includes a procedural store layout generator, a trajectory generation pipeline, evaluation tools and fine-tuned baseline models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<p><strong>论文摘要分析：RoboBenchMart: Benchmarking Robots in Retail Environment</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献是引入了RoboBenchMart，一个针对零售“暗店”（dark store）环境设计的、更具挑战性和真实性的机器人操作基准。它旨在解决现有基准主要关注简化桌面场景的局限性，通过模拟杂乱、多样的商品摆放和复杂的空间配置，推动机器人操作在零售自动化领域的进步。作者指出，即使是当前的SOTA通用模型也难以应对这些零售任务，并发布了包含工具和基线模型的RoboBenchMart套件以支持后续研究。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>关键创新在于<strong>将机器人操作基准从简化的桌面场景提升到复杂的零售“暗店”环境</strong>。这不仅仅是场景的改变，更是对任务复杂度的本质提升。具体的方法论创新体现在：</p>
<ul>
<li><strong>真实世界复杂性的引入：</strong> 模拟了“暗店”中常见的密集物体杂乱（dense object clutter）和多样的空间配置（varied spatial configurations），包括不同高度、深度和紧密相邻的物品。这比传统的平面桌面场景更接近实际应用。</li>
<li><strong>领域特异性挑战：</strong> 专注于零售领域，这意味着机器人需要处理各种形状、大小、材质的杂货商品，这本身就是对感知和抓取鲁棒性的巨大考验。</li>
<li><strong>全面的基准套件：</strong> 提供了支持研究的工具，包括：<ul>
<li><strong>程序化商店布局生成器 (procedural store layout generator)：</strong> 这是一个重要的创新，允许研究人员生成无限多样的测试场景，避免了手动创建场景的繁琐和局限性。</li>
<li><strong>轨迹生成管道 (trajectory generation pipeline)：</strong> 可能用于生成参考轨迹或辅助机器人规划。</li>
<li><strong>评估工具 (evaluation tools)：</strong> 标准化了性能衡量方式。</li>
<li><strong>微调的基线模型 (fine-tuned baseline models)：</strong> 为研究人员提供了起点，可以直观地看到当前SOTA模型在这些任务上的表现，并作为未来改进的参照。</li>
</ul>
</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动机器人操作研究的范式转变：</strong> 将研究焦点从实验室简化场景转向更具挑战性和实际应用价值的真实世界环境，促使研究人员开发更鲁棒、更通用的操作策略。</li>
<li><strong>加速零售自动化进程：</strong> 通过提供一个标准化的、具有挑战性的基准，RoboBenchMart将激励研究人员和企业开发出能够有效应对零售环境中复杂操作任务的机器人系统，从而加速“暗店”和仓库自动化。</li>
<li><strong>揭示现有模型的局限性：</strong> 明确指出当前SOTA通用模型在零售任务上的不足，这为未来的研究指明了方向，例如需要更强的感知能力（处理遮挡、光照变化）、更智能的规划（处理碰撞、序列操作）和更灵活的抓取策略（处理多样化物品）。</li>
<li><strong>促进多模态感知与操作的融合：</strong> 零售环境的复杂性可能需要机器人整合视觉、触觉甚至力觉等多种感知信息，以实现可靠的操作。该基准将推动这方面的研究。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>机器人操作与抓取 (Robotic Manipulation and Grasping)：</strong> 直接受益，需要开发更先进的感知、规划和控制算法。</li>
<li><strong>计算机视觉 (Computer Vision)：</strong><ul>
<li><strong>物体检测与识别：</strong> 在密集杂乱和部分遮挡的环境中准确识别和定位各种商品。</li>
<li><strong>3D场景理解：</strong> 从2D图像或深度图中重建和理解复杂的三维空间布局和物体姿态。</li>
<li><strong>姿态估计：</strong> 准确估计非刚性或形状不规则物体的姿态。</li>
<li><strong>语义分割：</strong> 在复杂背景下精确分割出目标物体。</li>
</ul>
</li>
<li><strong>强化学习 (Reinforcement Learning)：</strong> 复杂的操作任务通常需要RL来学习最优策略，尤其是在处理不确定性和多样性方面。</li>
<li><strong>具身智能 (Embodied AI)：</strong> 机器人需要在物理世界中感知、推理和行动，该基准是具身智能研究的理想测试平台。</li>
<li><strong>物流与仓储自动化 (Logistics and Warehousing Automation)：</strong> 除了零售“暗店”，其他需要机器人处理多样化物品、进行拣选和放置的物流场景也将从中受益。</li>
<li><strong>服务机器人 (Service Robotics)：</strong> 任何需要在非结构化环境中与多样化物品交互的服务机器人应用都可能从中获得启发。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>模拟环境的真实性差距：</strong> 尽管摘要强调了“更真实”，但作为一个模拟基准，它仍然可能无法完全捕捉真实世界中所有的物理复杂性（如摩擦力、物体变形、光照变化、传感器噪声的真实分布等）。</li>
<li><strong>任务范围的限制：</strong> 摘要中提到“复杂操作任务”和“常见零售任务”，但具体任务的种类和复杂程度（例如，是否涉及包装、堆叠、液体处理、易碎品处理等）并未详细说明。</li>
<li><strong>模型泛化能力：</strong> 摘要指出SOTA通用模型表现不佳，这可能意味着当前模型在从简化场景到复杂场景的泛化能力上存在根本性问题，而不仅仅是需要微调。基准本身无法解决这个问题，但能揭示它。</li>
<li><strong>数据量和多样性：</strong> 尽管有程序化生成器，但生成的数据是否能完全覆盖真实世界中所有可能的商品种类、摆放方式和环境条件，仍是一个需要关注的问题。</li>
<li><strong>硬件平台依赖性：</strong> 摘要未提及基准是否与特定机器人硬件平台绑定，或者是否提供了通用的接口。如果基准过于依赖特定硬件，可能会限制其广泛应用。</li>
<li><strong>评估指标的全面性：</strong> 摘要提到了“评估工具”，但未详细说明评估指标是否足够全面，能够衡量操作的成功率、效率、鲁棒性、安全性等多个维度。</li>
</ul>
<hr />
<p>总而言之，RoboBenchMart是一个非常及时和重要的工作，它将机器人操作研究推向了更具挑战性和实际意义的领域。它不仅提供了一个新的测试平台，更重要的是，它揭示了当前SOTA模型在真实世界复杂性面前的不足，为未来的研究指明了清晰的方向。对于计算机视觉领域而言，这意味着需要开发更强大的感知算法，以应对零售环境中物体识别、姿态估计和场景理解的巨大挑战。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items.</li>
<li>We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.10276v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.10276v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-14 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
