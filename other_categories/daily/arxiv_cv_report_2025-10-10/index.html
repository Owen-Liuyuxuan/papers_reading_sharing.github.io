<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-10 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-09/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-13/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-10">Arxiv Computer Vision Papers - 2025-10-10</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-08" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-08)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#quantum-enhanced-computer-vision-going-beyond-classical-algorithms" class="nav-link">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-language-action-models-for-robotics-a-review-towards-real-world-applications" class="nav-link">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a>
                </li>
                <li class="nav-item">
                    <a href="#resplat-learning-recurrent-gaussian-splats" class="nav-link">ReSplat: Learning Recurrent Gaussian Splats</a>
                </li>
                <li class="nav-item">
                    <a href="#novaflow-zero-shot-manipulation-via-actionable-flow-from-generated-videos" class="nav-link">NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#matrix-multimodal-agent-tuning-for-robust-tool-use-reasoning" class="nav-link">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#d2gs-depth-and-density-guided-gaussian-splatting-for-stable-and-accurate-sparse-view-reconstruction" class="nav-link">D^2GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#how-to-teach-large-multimodal-models-new-skills" class="nav-link">How to Teach Large Multimodal Models New Skills</a>
                </li>
                <li class="nav-item">
                    <a href="#resad-normalized-residual-trajectory-modeling-for-end-to-end-autonomous-driving" class="nav-link">ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#videocanvas-unified-video-completion-from-arbitrary-spatiotemporal-patches-via-in-context-conditioning" class="nav-link">VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</a>
                </li>
                <li class="nav-item">
                    <a href="#artdeco-towards-efficient-and-high-fidelity-on-the-fly-3d-reconstruction-with-structured-scene-representation" class="nav-link">ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-10">Arxiv Computer Vision Papers - 2025-10-10</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-08">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-08)</h2>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæå±ç¤ºäºè®¡ç®æºè§è§é¢åæç»­çå¿«éåå±ï¼ä¸»è¦éä¸­å¨ä»¥ä¸å ä¸ªå³é®é¢åï¼</p>
<ul>
<li><strong>å¤æ¨¡æä¸å·èº«æºè½ï¼</strong> æ¾èçè¶å¿æ¯è§è§-è¯­è¨-å¨ä½æ¨¡åå¨æºå¨äººåå·èº«æºè½ä¸­çåºç¨ï¼æ¨å¨å®ç°æ´éç¨ãæ´å¼ºå¤§çæºè½ä½ã</li>
<li><strong>3D éå»ºä¸æ°è§å¾åæï¼</strong> Gaussian Splatting (GS) åå¶åä½ä»ç¶æ¯ 3D åºæ¯è¡¨ç¤ºåæ°è§å¾åæçç­ç¹ï¼ç ç©¶éç¹å¨äºæé«æçãç¨³å®æ§ååç¡®æ§ã</li>
<li><strong>å¤§æ¨¡åè½åæ©å±ä¸åºç¨ï¼</strong> å¦ä½ææå°ææå¤§åå¤æ¨¡ææ¨¡åæ°æè½ï¼ä»¥åå©ç¨çææ¨¡åè¿è¡é¶æ ·æ¬æä½æ¯éè¦çç ç©¶æ¹åã</li>
<li><strong>èªä¸»é©¾é©¶ï¼</strong> æç»­å³æ³¨ç«¯å°ç«¯èªä¸»é©¾é©¶çé²æ£æ§åè½¨è¿¹å»ºæ¨¡ã</li>
<li><strong>è§é¢å¤çï¼</strong> è§é¢è¡¥å¨åçæè§é¢çåºç¨ä¹å æ®ä¸å¸­ä¹å°ã</li>
<li><strong>æ°å´è®¡ç®èå¼ï¼</strong> éå­è®¡ç®å¨è®¡ç®æºè§è§ä¸­çæ½å¨åºç¨å¼å§æµ®ç°ã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms" (Natacha Kuete Meli et al.)ï¼</strong> è¿ç¯è®ºæå·æåç»æ§ï¼é¢ç¤ºäºéå­è®¡ç®å¨è®¡ç®æºè§è§é¢åçæ½å¨é¢ è¦æ§å½±åï¼å°½ç®¡ç®åå¯è½ä»å¤äºæ©æé¶æ®µï¼ä½å¶æ¦å¿µæ§åæ°å¼å¾å³æ³¨ã</li>
<li><strong>"Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications" (Kento Kawaharazuka et al.)ï¼</strong> ä½ä¸ºä¸ç¯ç»¼è¿°ï¼å®ç³»ç»å°æ¢³çäºè§è§-è¯­è¨-å¨ä½æ¨¡åå¨æºå¨äººé¢åçè¿å±ï¼å¯¹äºçè§£è¯¥é¢åçç°ç¶åæªæ¥ææå·æéè¦æå¯¼æä¹ã</li>
<li><strong>"NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos" (Hongyu Li et al.)ï¼</strong> è¿ç¯è®ºæå±ç¤ºäºå©ç¨çæè§é¢ä¸­çå¯æä½æµå®ç°é¶æ ·æ¬æä½çåæ°æ¹æ³ï¼ä¸ºå·èº«æºè½åæºå¨äººæä½æä¾äºæ°çæè·¯ã</li>
<li><strong>"How to Teach Large Multimodal Models New Skills" (Zhen Zhu et al.)ï¼</strong> é´äºå¤§åå¤æ¨¡ææ¨¡åæ¥çå¢é¿çéè¦æ§ï¼è¿ç¯è®ºææ¢è®¨äºå¦ä½ææå°æ©å±å¶è½åï¼å¯¹äº LMM çå®éåºç¨åæªæ¥åå±è³å³éè¦ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>éå­è®¡ç®æºè§è§ï¼</strong> è½ç¶ä»å¤äºèè½é¶æ®µï¼ä½éå­è®¡ç®ä¸è®¡ç®æºè§è§çç»åå¯è½å¨æªæ¥å¸¦æ¥èå¼è½¬åã</li>
<li><strong>å·èº«æºè½çé¶æ ·æ¬æä½ï¼</strong> å©ç¨çææ¨¡ååå¯æä½æµå®ç°é¶æ ·æ¬æºå¨äººæä½ï¼æ¯å·èº«æºè½é¢åä¸ä¸ªåæ»¡åæ¯çæ¹åã</li>
<li><strong>Gaussian Splatting çæç»­ä¼åï¼</strong> D<script type="math/tex">^2</script>GS å ReSplat ç­å·¥ä½è¡¨æï¼GS ä»å¨ä¸æ­æ¼è¿ï¼ä»¥è§£å³ç¨çè§å¾éå»ºãå¨æåºæ¯åæçç­ææã</li>
<li><strong>å¤æ¨¡ææºè½ä½çå·¥å·ä½¿ç¨æ¨çï¼</strong> MATRIX å¼ºè°äºå¤æ¨¡ææºè½ä½å¨å¤æå·¥å·ä½¿ç¨åºæ¯ä¸çé²æ£æ¨çè½åï¼è¿æ¯è¿åæ´éç¨ AI çå³é®ä¸æ­¥ã</li>
<li><strong>ç»ä¸çè§é¢è¡¥å¨æ¡æ¶ï¼</strong> VideoCanvas æåºçç»ä¸è§é¢è¡¥å¨æ¹æ³ï¼éè¿ä¸ä¸ææ¡ä»¶åå¤çä»»ææ¶ç©ºè¡¥ä¸ï¼å±ç¤ºäºè§é¢åå®¹çæåç¼è¾çè¿æ­¥ã</li>
</ul>
<p><strong>4. å»ºè®®å®æ´éè¯»çè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºå³æ³¨å·èº«æºè½åæºå¨äººï¼</strong><ul>
<li><strong>"Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"</strong> (Kento Kawaharazuka et al.) - æä¾å¨é¢çèæ¯åæªæ¥æ¹åã</li>
<li><strong>"NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos"</strong> (Hongyu Li et al.) - åæ°æ§å°å©ç¨çæè§é¢å®ç°é¶æ ·æ¬æä½ã</li>
<li><strong>"MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning"</strong> (Tajamul Ashraf et al.) - å³æ³¨å¤æ¨¡ææºè½ä½çå¤ææ¨çè½åã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨ 3D éå»ºåæ°è§å¾åæï¼</strong><ul>
<li><strong>"ReSplat: Learning Recurrent Gaussian Splats"</strong> (Haofei Xu et al.) - æ¢ç´¢å¨æåºæ¯ä¸ç GS åºç¨ã</li>
<li><strong>"D<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction"</strong> (Meixi Song et al.) - è§£å³ç¨çè§å¾éå»ºçææã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨å¤§æ¨¡ååå¶è½åæ©å±ï¼</strong><ul>
<li><strong>"How to Teach Large Multimodal Models New Skills"</strong> (Zhen Zhu et al.) - æ¢è®¨ LMM çå³é®è½åæ©å±é®é¢ã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨æªæ¥è®¡ç®èå¼ï¼</strong><ul>
<li><strong>"Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms"</strong> (Natacha Kuete Meli et al.) - äºè§£éå­è®¡ç®å¨ CV é¢åçæ½å¨åºç¨ã</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨èªä¸»é©¾é©¶ï¼</strong><ul>
<li><strong>"ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving"</strong> (Zhiyu Zheng et al.) - æ·±å¥äºè§£ç«¯å°ç«¯èªä¸»é©¾é©¶çææ°è¿å±ã</li>
</ul>
</li>
</ul>
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥ Arxiv è®¡ç®æºè§è§è®ºæçæ ¸å¿åå®¹åæ½å¨ä»·å¼ï¼ä»¥ä¾¿æ¨è½æ´é«æå°è¿è¡æ·±å¥ç ç©¶ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></li>
<li><a href="#2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></li>
<li><a href="#2510.08575v1">ReSplat: Learning Recurrent Gaussian Splats</a></li>
<li><a href="#2510.08568v1">NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos</a></li>
<li><a href="#2510.08567v1">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></li>
<li><a href="#2510.08566v1">D<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction</a></li>
<li><a href="#2510.08564v1">How to Teach Large Multimodal Models New Skills</a></li>
<li><a href="#2510.08562v1">ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</a></li>
<li><a href="#2510.08555v1">VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</a></li>
<li><a href="#2510.08551v1">ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.07317v1'></a></p>
<h2 id="quantum-enhanced-computer-vision-going-beyond-classical-algorithms"><a href="https://arxiv.org/abs/2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></h2>
<p><strong>Authors:</strong> Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææ¯å¯¹éå­å¢å¼ºè®¡ç®æºè§è§ï¼QeCVï¼é¢åçä¸é¡¹å¨é¢ç»¼è¿°ï¼æ¨å¨ä¸ºè®¡ç®æºè§è§ç¤¾åºæä¾ä¸ä¸ªéå­è®¡ç®çåèæåãå®ç³»ç»å°ä»ç»äºQeCVçåºæ¬æ¦å¿µãæ¹æ³è®ºä»¥åå¦ä½å°è®¡ç®æºè§è§é®é¢ä¸éå­ç¡¬ä»¶å¼å®¹ï¼å¹¶æ¢è®¨äºè¯¥é¢åçç°æå·¥å·ãææåæªæ¥æ¹åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>è¿ç¯è®ºææ¬èº«æ¯ä¸ç¯ç»¼è¿°ï¼å¶å³é®âåæ°âå¨äºå¶<strong>å¨é¢æ§åæ´åæ§</strong>ãå®ä¸æ¯æåºä¸ä¸ªæ°çç®æ³ï¼èæ¯ï¼
*   <strong>ç³»ç»æ§å°å®ä¹åæå»ºäºQeCVé¢åï¼</strong> å°è®¡ç®æºè§è§ãä¼åçè®ºãæºå¨å­¦ä¹ åéå­è®¡ç®äº¤åèåï¼æç¡®äºQeCVçèç´ã
*   <strong>åèå¼æ¹æ³è®ºï¼</strong> è¯¦ç»éè¿°äºå¦ä½å©ç¨ä¸¤ç§ä¸»è¦çéå­è®¡ç®èå¼ï¼åºäºé¨çéå­è®¡ç®åéå­éç«ï¼æ¥è§£å³è®¡ç®æºè§è§é®é¢ã
*   <strong>å¼å®¹æ§ä¸å®è·µæ§ï¼</strong> å¼ºè°äºå¼åä¸éå­ç¡¬ä»¶å¼å®¹çä¸é¨ç®æ³çéè¦æ§ï¼å¹¶æä¾äºå³äºå¦ä½è®¿é®ãç¼ç¨åæ¨¡æéå­è®¡ç®æºçå®ç¨ä¿¡æ¯ã
*   <strong>é¢åCVç¤¾åºçæ¡¥æ¢ï¼</strong> æ¨å¨å¼¥åè®¡ç®æºè§è§ä¸å®¶ä¸éå­è®¡ç®ä¹é´çç¥è¯é¸¿æ²ï¼ä¸ºCVç ç©¶äººåæä¾è¿å¥QeCVé¢åçå¥é¨æåã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>è¿ç¯ç»¼è¿°å¯¹è®¡ç®æºè§è§é¢åå·ææ·±è¿çæ½å¨å½±åï¼
*   <strong>å éQeCVç ç©¶ï¼</strong> ä½ä¸ºä¸ç¯å¨é¢çåèæåï¼å®å°æå¤§å°éä½è®¡ç®æºè§è§ç ç©¶äººåè¿å¥éå­è®¡ç®é¢åçé¨æ§ï¼ä»èå éQeCVæ°ç®æ³ååºç¨çå¼åã
*   <strong>èå¼è½¬åçå¬ååï¼</strong> å¼ºè°äºéå­è®¡ç®å¨å¤çç»å¸æ¹æ³é¾ä»¥è§£å³æèæ¶è¿é¿çé®é¢ä¸çæ½å¨ä¼å¿ï¼å¯è½ä¿ä½¿è®¡ç®æºè§è§é¢åä»æ ¹æ¬ä¸éæ°æèæäºé®é¢çè§£å³æ¹å¼ã
*   <strong>æ¨å¨éå­ç¥ç»ç½ç»åå±ï¼</strong> æåºåæ°åéå­çµè·¯å¯è½æä¸ºç»å¸ç¥ç»ç½ç»çé¿ææ¿ä»£æ¹æ¡ï¼è¿é¢ç¤ºçæªæ¥è®¡ç®æºè§è§æ¨¡åæ¶æçéå¤§åé©ã
*   <strong>æ ååä¸åä½ï¼</strong> è®¨è®ºäºQeCVè®ºæçåè¡¨åè¯å®¡æ¹é¢ï¼æå©äºå»ºç«è¯¥é¢åçæä½³å®è·µåä¿è¿è·¨å­¦ç§åä½ã</p>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>è®¡ç®æå½±ä¸å¾åå¤çï¼</strong> å¾åå»åªãè¶åè¾¨çãå¾åéå»ºç­ï¼å¶ä¸­æäºä¼åé®é¢å¨ç»å¸è®¡ç®ä¸­å¯è½éå¸¸èæ¶ã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å¤æçå¾ååå²ãç¾çè¯æ­ãè¯ç©åç°ä¸­çåå­æ¨¡æï¼éè¦å¤çå¤§éé«ç»´æ°æ®åå¤æçæ¨¡å¼è¯å«ã</li>
<li><strong>æºå¨äººä¸èªä¸»ç³»ç»ï¼</strong> å®æ¶æç¥ãè·¯å¾è§åãå³ç­å¶å®ï¼å°¤å¶æ¯å¨éè¦å¿«éè§£å³å¤æä¼åé®é¢çåºæ¯ã</li>
<li><strong>å¤§è§æ¨¡æ°æ®åæä¸æ¨¡å¼è¯å«ï¼</strong> ä»»ä½æ¶åå¤çæµ·éè§è§æ°æ®å¹¶ä»ä¸­æåå¤ææ¨¡å¼çåºç¨ï¼å¦é¥æãå¤©æå­¦å¾ååæã</li>
<li><strong>ä¼åé®é¢ï¼</strong> è®¡ç®æºè§è§ä¸­çè®¸å¤ä»»å¡æ¬è´¨ä¸æ¯ä¼åé®é¢ï¼å¦ç¹å¾å¹éãç»æåéå»ºãå¤è§å¾å ä½ï¼ï¼éå­éç«ç­ææ¯å¯è½æä¾æ´ä¼çè§£å³æ¹æ¡ã</li>
<li><strong>æºå¨å­¦ä¹ åºç¡ç ç©¶ï¼</strong> æ¢ç´¢éå­æºå¨å­¦ä¹ æ¨¡åï¼å¦éå­ç¥ç»ç½ç»ï¼å¨è§è§ä»»å¡ä¸­çè¡¨ç°åçè®ºæéã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<ul>
<li><strong>ææ¯æçåº¦ï¼</strong> æè¦æç¡®æåºQeCVæ¯ä¸ä¸ªâæ°ç ç©¶é¢åâï¼ä¸âä¸é¨åæ ¹æ¬æ§çæ°ç®æ³å¿é¡»è¢«å¼åâï¼è¿æç¤ºäºè¯¥é¢åä»å¤äºæ©æé¶æ®µï¼è·ç¦»å®éå¹¿æ³åºç¨å°è¿ã</li>
<li><strong>ç¡¬ä»¶éå¶ï¼</strong> å°½ç®¡æå°äºéå­ç¡¬ä»¶ï¼ä½å½åçéå­è®¡ç®æºä»å­å¨åªå£°å¤§ãçº éè½åå¼±ãéå­æ¯ç¹æ°éæéç­é®é¢ï¼è¿äºé½å¯è½éå¶QeCVç®æ³çå®éæ§è½åå¯æ©å±æ§ã</li>
<li><strong>ç®æ³å¼åé¾åº¦ï¼</strong> âä¸é¨åæ ¹æ¬æ§çæ°ç®æ³âçå¼åæ¬èº«å°±æ¯ä¸é¡¹å·¨å¤§çææï¼éè¦æ·±åçéå­ç©çåè®¡ç®æºç§å­¦ç¥è¯ãå°ç»å¸CVé®é¢è½¬åä¸ºéå­å¼å®¹çå½¢å¼å¹¶éæäºã</li>
<li><strong>çè®ºä¸å®è·µçå·®è·ï¼</strong> æè¦å¼ºè°äºâé«æ½åâåâé¿æâæ¿ä»£ç»å¸ç¥ç»ç½ç»ï¼è¿è¡¨æç®åQeCVççè®ºä¼å¿å¯è½å°æªå¨å®éåºç¨ä¸­å¾å°ååéªè¯æè¶è¶ç»å¸æ¹æ³ã</li>
<li><strong>å¯è®¿é®æ§ä¸å­¦ä¹ æ²çº¿ï¼</strong> å°½ç®¡æ¨å¨ä¸ºCVç¤¾åºæä¾åèï¼ä½éå­è®¡ç®æ¬èº«å·æè¾é«çå­¦ä¹ æ²çº¿ï¼å³ä½¿æç»¼è¿°ï¼éä¸ä¸äººå£«ä¹å¯è½é¢ä¸´çè§£åå®è·µçå°é¾ã</li>
<li><strong>ç»¼è¿°æ§è´¨ï¼</strong> ä½ä¸ºä¸ç¯ç»¼è¿°ï¼å®æ¬èº«ä¸æä¾æ°çå®éªç»ææç®æ³ï¼å¶ä»·å¼å¨äºç¥è¯çæ´ååæ¹åçæå¼ï¼èéç´æ¥çææ¯çªç ´ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°è®ºæå¨è®¡ç®æºè§è§é¢åå·æéè¦çæç¥æä¹ï¼å®ä¸ºæ°å´çéå­å¢å¼ºè®¡ç®æºè§è§é¢åå¥ å®äºåºç¡ï¼å¹¶ä¸ºæªæ¥çç ç©¶ææäºæ¹åãå®é¢ç¤ºçè®¡ç®æºè§è§å¯è½è¿æ¥ä¸åºç±éå­è®¡ç®é©±å¨çèå¼åé©ï¼å°½ç®¡è¿æ¡éè·¯åæ»¡ææï¼ä½å¶æ½å¨çåæ¥æ¯å·¨å¤§çã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing.</li>
<li>However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07317v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07317v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07077v1'></a></p>
<h2 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications"><a href="https://arxiv.org/abs/2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></h2>
<p><strong>Authors:</strong> Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Kento Kawaharazukaç­äººæ°åçè®ºæâVision-Language-Action Models for Robotics: A Review Towards Real-World Applicationsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications_1">è®ºææè¦ï¼Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººé¢åä¸­ä¸ä¸ªæ ¸å¿ææï¼å¦ä½å¼åè½å¤è·¨ä¸åä»»å¡ãç©ä½ãæºå¨äººå½¢æåç¯å¢è¿è¡æ³åçæºå¨äººç­ç¥ï¼ä»èå®ç°æ´çµæ´»ãå¯æ©å±ççå®ä¸çé¨ç½²ãå·ä½æ¥è¯´ï¼å®å³æ³¨äºè§è§-è¯­è¨-å¨ä½ï¼VLAï¼æ¨¡åï¼è¿äºæ¨¡åéè¿ç»ä¸è§è§ãè¯­è¨åå¨ä½æ°æ®æ¥å­¦ä¹ éç¨ç­ç¥ï¼ä»¥åæä¼ ç»æºå¨äººç³»ç»å¨æ³åè½ååæ°æ®æçæ¹é¢çå±éæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçä¸»è¦è´¡ç®å¨äºæä¾äºä¸ä¸ªå¨é¢ãå¨æ çVLAç³»ç»ç»¼è¿°ï¼æ´åäºè½¯ä»¶åç¡¬ä»¶ç»ä»¶ï¼è¿ä¸ä»¥å¾ä»å³æ³¨å¨ä½è¡¨ç¤ºæé«çº§æ¨¡åæ¶æçè°æ¥ä¸åãå·ä½çæ¹æ³è®ºè´¡ç®ä½ç°å¨ä»¥ä¸å ä¸ªæ¹é¢ï¼</p>
<ul>
<li><strong>VLAè®¾è®¡ç­ç¥åæ¼åï¼Section IIIï¼ï¼</strong> è®ºæç³»ç»å°åé¡¾äºVLAæ¨¡åä»æ©æåºäºCNNçç«¯å°ç«¯æ¶æï¼å¦CLIPortï¼å°åºäºTransformerçåºåæ¨¡åï¼å¦GatoãVIMAï¼ï¼åå°å©ç¨é¢è®­ç»VLMéª¨å¹²çç»ä¸çå®ä¸çç­ç¥ï¼å¦RTç³»åãOpenVLAï¼ï¼ä»¥åéææ©æ£åæµå¹éææ¯ï¼å¦OctoãRDT-1Bã<script type="math/tex">\pi_0</script>ï¼çæ¨¡åçåå²æ¼åãææ°è¿å±åæ¬æ½å¨å¨ä½å­¦ä¹ ï¼å¦LAPAï¼ååå±æ§å¶æ¡æ¶ï¼å¦GROOT N1ã<script type="math/tex">\pi_{0.5}</script>ï¼ã</li>
<li><strong>æ¶æåæå»ºæ¨¡åï¼Section IVï¼ï¼</strong> è¯¦ç»åç±»äºVLAæ¨¡åçæ ¸å¿æ¶æç±»åï¼åæ¬æç¥è¿å¨æ¨¡åï¼Sensorimotor Modelï¼ãä¸çæ¨¡åï¼World Modelï¼ååºäºå¯ä¾æ§æ¨¡åï¼Affordance-Based Modelï¼ãæç¥è¿å¨æ¨¡åè¿ä¸æ­¥ç»åä¸ºä¸ç§åä½ï¼æ¶µçäºTransformerä¸ç¦»æ£å¨ä½ä»¤çãæ©æ£å¨ä½å¤´ãæ©æ£TransformerãVLMä¸ç¦»æ£å¨ä½ä»¤çãVLMä¸æ©æ£/æµå¹éå¨ä½å¤´ä»¥åVLMä¸æ©æ£Transformerçç»åãä¸çæ¨¡ååéè¿é¢æµæªæ¥è§å¯æå­¦ä¹ æ½å¨å¨ä½æ¥æå¯¼å¨ä½çæã</li>
<li><strong>æ¨¡æç¹å®å¤çææ¯ï¼Section IV-Dï¼ï¼</strong> è¯¦ç»éè¿°äºVLAæ¨¡åå¦ä½å¤çå¤ç§è¾å¥æ¨¡æï¼åæ¬è§è§ï¼ä½¿ç¨ResNetãViTãCLIPãSigLIPç­ï¼ãè¯­è¨ï¼ä½¿ç¨T5ãLLaMAç­åè¯å¨åç¼ç å¨ï¼åå¨ä½ï¼ç¦»æ£åãè¿ç»­å¨ä½å»ºæ¨¡ãæ½å¨å¨ä½å­¦ä¹ ãè·¨å½¢æå¨ä½è¡¨ç¤ºï¼ãæ­¤å¤ï¼è¿è®¨è®ºäºé³é¢ãè§¦è§å3Dä¿¡æ¯ï¼æ·±åº¦å¾åãå¤è§å¾å¾åãä½ç´ è¡¨ç¤ºãç¹äºï¼ç­è¾å©æ¨¡æçæ´åã</li>
<li><strong>è®­ç»ç­ç¥åå®ç°ï¼Section Vï¼ï¼</strong> æ»ç»äºVLAæ¨¡åçè®­ç»æ¹æ³ï¼åæ¬çç£å­¦ä¹ ãèªçç£å­¦ä¹ åå¼ºåå­¦ä¹ ãå¼ºè°äºé¢è®­ç»ååè®­ç»é¶æ®µçéè¦æ§ï¼ä»¥åæ°æ®è§æ¨¡ãVLMéª¨å¹²ãæ¢¯åº¦éç¦»ãåæ°é«æéåºæ¹æ³ï¼å¦LoRAï¼åå¤ä»»å¡å­¦ä¹ å¨æé«æ³åè½ååè®­ç»æçæ¹é¢çä½ç¨ã</li>
<li><strong>æ°æ®æ¶éãæ°æ®éåæ°æ®å¢å¼ºï¼Section VIï¼ï¼</strong> ç»¼è¿°äºçå®ä¸çæºå¨äººæ°æ®æ¶éæ¹æ³ï¼è¿ç¨æä½ãä»£çè®¾å¤ãäººç±»æ°æ®æ¶éï¼ï¼å¹¶åä¸¾äºç¨äºé¢è®­ç»çå¬å¼æ°æ®éï¼äººç±»è§é¢æ°æ®éãä»¿çæ°æ®éãçå®æºå¨äººæ°æ®éï¼ãæ­¤å¤ï¼è¿è®¨è®ºäºè§è§ãè¯­è¨åå¨ä½æ°æ®å¢å¼ºææ¯ï¼ä»¥åºå¯¹æ°æ®ç¨ç¼ºé®é¢ã</li>
<li><strong>æºå¨äººè¯ä¼°ååºç¨ï¼Section VIIï¼ï¼</strong> ä»ç»äºVLAç ç©¶ä¸­å¸¸ç¨çæºå¨äººå¹³å°ï¼æºæ¢°èãæ/å¤¹æå¨ãç§»å¨æºå¨äººãåè¶³æºå¨äººãäººå½¢æºå¨äººï¼ä»¥åè¯ä¼°åºåï¼å¦robosuiteãManiSkillãLIBEROãCALVINï¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥è®ºæçç»¼è¿°æ­ç¤ºäºVLAæ¨¡åå¨æºå¨äººé¢ååå¾çæ¾èè¿å±åæ½åï¼</p>
<ul>
<li><strong>æ³åè½åçæåï¼</strong> éè¿å¤§è§æ¨¡æ°æ®éåé¢è®­ç»åºç¡æ¨¡åï¼ç¹å«æ¯VLMï¼çå©ç¨ï¼VLAæ¨¡åå¨è·¨ä»»å¡ãç©ä½åç¯å¢æ¹é¢è¡¨ç°åºæ´å¼ºçæ³åè½åã</li>
<li><strong>æ¶æçæ¼è¿ï¼</strong> ä»ç®åçCNNå°å¤æçTransformerãæ©æ£æ¨¡åååå±æ¶æï¼VLAæ¨¡åçè®¾è®¡åå¾è¶æ¥è¶ç²¾å·§ï¼è½å¤æ´å¥½å°å¤çå¤æ¨¡æè¾å¥å¹¶çæå¹³æ»ãç²¾ç¡®çå¨ä½ã</li>
<li><strong>å¤æ¨¡æèåï¼</strong> é¤äºè§è§åè¯­è¨ï¼è§¦è§ãé³é¢å3Dä¿¡æ¯ç­è¾å©æ¨¡æçæ´åï¼è¿ä¸æ­¥å¢å¼ºäºæºå¨äººçæç¥åäº¤äºè½åï¼å°¤å¶æ¯å¨æ¥è§¦ä¸°å¯çä»»å¡ä¸­ã</li>
<li><strong>çå®ä¸çé¨ç½²çæ½åï¼</strong> éçæ°æ®æ¶éç­ç¥çæ¹è¿ãå¬å¼æ°æ®éçä¸°å¯ä»¥åæ°æ®å¢å¼ºææ¯çåºç¨ï¼VLAæ¨¡åæ­£éæ­¥åæçå®ä¸çé¨ç½²çææï¼ä¸ºæ´çµæ´»ãå¯æ©å±çæºå¨äººç³»ç»å¥ å®åºç¡ã</li>
<li><strong>åå±æ¨çåè§åï¼</strong> åå±æ¶æåæç»´é¾ï¼CoTï¼æ¨ççå´èµ·ï¼ä½¿å¾VLAæ¨¡åè½å¤è¿è¡æ´é²æ£çè§åãä»»å¡åè§£åä¸ä¸ææç¥å¨ä½çæï¼å°¤å¶éç¨äºé¿å¨æãå¤æ­¥éª¤ä»»å¡ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
å°½ç®¡VLAæ¨¡ååå¾äºæ¾èè¿å±ï¼è®ºæä¹æåºäºä»¥ä¸å±éæ§ï¼</p>
<ul>
<li><strong>æ°æ®ç¨ç¼ºåå¤æ ·æ§ä¸è¶³ï¼</strong> å°½ç®¡æå¤§è§æ¨¡æ°æ®éï¼ä½åæ¶æ»¡è¶³è§è§ãè¯­è¨åå¨ä½ä¸ç§æ¨¡æçæ°æ®éå¨è§æ¨¡åå¤æ ·æ§ä¸ä»ç¶æéï¼ç¹å«æ¯é«è´¨éçæºå¨äººæ¼ç¤ºæ°æ®æ¶éææ¬é«æã</li>
<li><strong>æºå¨äººå½¢æè½¬ç§»ææï¼</strong> æºå¨äººå½¢æçå¤æ ·æ§ï¼å³èéç½®ãä¼ æå¨ç±»åãè¿å¨ç©ºé´ç­ï¼ä½¿å¾è·¨å½¢æç­ç¥è½¬ç§»æä¸ºä¸ä¸ªä¸»è¦ææãå°äººç±»è¿å¨æ°æ®æ å°å°æºå¨äººå¯æ§è¡å¨ä½ä¹éæäºã</li>
<li><strong>è®¡ç®åè®­ç»ææ¬ï¼</strong> VLAæ¨¡åçé«ç»´åå¤æ¨¡æè¾å¥å¯¼è´å·¨å¤§çè®¡ç®éæ±ï¼å°¤å¶æ¯å¨å¤çé¿æ¶åºåºåãé«åè¾¨çå¾åæé¢å¤æ¨¡ææ¶ã</li>
<li><strong>å®æ¶æ§åå¹³æ»æ§ï¼</strong> ç¦»æ£å¨ä½ä»¤çææ¶ç¼ºä¹å®æ¶ååºæ§åå¹³æ»æ§ï¼èè¿ç»­å¨ä½çææ¹æ³ï¼å¦æ©æ£åæµå¹éï¼æ­£å¨è§£å³è¿ä¸é®é¢ã</li>
<li><strong>è¯ä¼°çæ ååä¸è¶³ï¼</strong> çå®ä¸çç¯å¢ä¸­VLAæ¨¡åçè¯ä¼°ææ ä»ç¶å®ä¹ä¸æ¸ï¼æ³åè¯ä¼°é¢ä¸´æºå¨äººå½¢æå·®å¼ãå®å¨é®é¢åå¯å¤ç°æ§ç­ææã</li>
<li><strong>æç»­å­¦ä¹ åéåºæ§ï¼</strong> å½åVLAæ¨¡åéå¸¸æ æ³å¨å¶åå§è®­ç»é¶æ®µä¹åç»§ç»­å­¦ä¹ åéåºæ°æåµï¼ä½¿å¶å¨é¢å¯¹æ°é¢æåå¸å¤åºæ¯æ¶å®¹æå¤±æã</li>
<li><strong>å®å¨æ§åæéæ£æµï¼</strong> å¨éç»æåç¯å¢ä¸­é¨ç½²VLAæ¨¡åå­å¨å®å¨é£é©ï¼ç¼ºä¹æ£æµåé¿åæå¤äººç±»å­å¨ãä»¥åæéæ£æµåæ¢å¤æºå¶ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸å ä¸ªæªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>å¤æ¨¡ææ°æ®æ ååï¼</strong> ç»ä¸ä¼ æå¨éç½®å¯¹äºå®ç°å¯æ©å±çå¤æ¨¡æVLAç³»ç»è³å³éè¦ï¼å°¤å¶æ¯å¨è§¦è§ä¼ æç­é¢åã</li>
<li><strong>æ¨çè½åå¢å¼ºï¼</strong> æé«VLAç³»ç»å¨é¿å¨æä»»å¡ä¸­çæ¨çè½åï¼åæ¬è®°å¿ä¿æãéæ©æ§å³æ³¨å³é®ä¿¡æ¯ä»¥åæ¶é´æ½è±¡ï¼ä»¥æ¯ææ´ææçè§ååå³ç­ã</li>
<li><strong>æç»­å­¦ä¹ åå¨çº¿éåºï¼</strong> å¼åè½å¤æç»­å­¦ä¹ åéåºæ°ç¯å¢çVLAæ¨¡åï¼å¯è½éè¿å¼ºåå­¦ä¹ ä¸äººç±»åé¦ï¼RLHFï¼æè®¤ç¥åå±å¯åçä¸»å¨å­¦ä¹ æ¥å®ç°ã</li>
<li><strong>ä¸çæ¨¡åä¸çå®ä¸çé¨ç½²ï¼</strong> å©ç¨å­¦ä¹ å°çä¸çæ¨¡åè¿è¡æ´å®å¨ãæ´æ ·æ¬é«æçRLå¾®è°ï¼å¹¶ç»åsim-to-realææ¯ï¼ä»¥åæçå®ä¸çæ¢ç´¢çé£é©ã</li>
<li><strong>å®å¨æ§åæéæ¢å¤ï¼</strong> æ´åVLAæ¨¡åä¸åºäºæ¨¡åçæ§å¶æ¹æ³ï¼ä»¥å®ç°é¢æµæ§æ¨çï¼æé«å®å¨å³é®æåµä¸çå¯é æ§ãå¼åæéæ£æµåèªéåºéè§åç­ç¥ï¼ä»¥åºå¯¹çå®ä¸çç¯å¢ä¸­çæå¤æéã</li>
<li><strong>æ ååè¯ä¼°ï¼</strong> å»ºç«æ´ä¸¥æ ¼ãå¯æ§çè¯ä¼°æ¡ä»¶åç»è®¡åææ¹æ³ï¼ä»¥ç¡®ä¿VLAæ¨¡åæ§è½æ¯è¾çæææ§åå¯é æ§ã</li>
<li><strong>å®éåºç¨ï¼</strong> å°½ç®¡VLAç³»ç»å¨å»çä¿å¥ãè¾å©ææ¯ãå·¥ä¸èªå¨ååèªå¨é©¾é©¶ç­é¢åå·æå·¨å¤§æ½åï¼ä½ä»éæé«å¶å¨é²æ£æ§åéåºæ§æ¹é¢è¾¾å°äººç±»æ°´å¹³çæ§è½ï¼ä»¥å®ç°å®éé¨ç½²ã</li>
</ul>
<hr />
<p>è¿ç¯è®ºæä¸ºæºå¨äººé¢åçVLAç ç©¶æä¾äºä¸ä¸ªå¨é¢çè·¯çº¿å¾ï¼ä¸ä»æ»ç»äºç°æææ¯ï¼è¿æç¡®æåºäºæªæ¥çææåæºéï¼å¯¹äºå¸æå°VLAæ¨¡ååºç¨äºçå®ä¸çæºå¨äººç³»ç»çç ç©¶äººååå·¥ç¨å¸å·æéè¦çæå¯¼æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07077v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07077v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08575v1'></a></p>
<h2 id="resplat-learning-recurrent-gaussian-splats"><a href="https://arxiv.org/abs/2510.08575v1">ReSplat: Learning Recurrent Gaussian Splats</a></h2>
<p><strong>Authors:</strong> Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While feed-forward Gaussian splatting models provide computational efficiency
and effectively handle sparse input settings, their performance is
fundamentally limited by the reliance on a single forward pass during
inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients. Our key insight is that the Gaussian splatting rendering error
serves as a rich feedback signal, guiding the recurrent network to learn
effective Gaussian updates. This feedback signal naturally adapts to unseen
data distributions at test time, enabling robust generalization. To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a <script type="math/tex">16 \times</script> subsampled space, producing <script type="math/tex">16 \times</script> fewer
Gaussians than previous per-pixel Gaussian models. This substantially reduces
computational overhead and allows for efficient Gaussian updates. Extensive
experiments across varying of input views (2, 8, 16), resolutions (<script type="math/tex">256 \times
256</script> to <script type="math/tex">540 \times 960</script>), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed. Our project
page is at https://haofeixu.github.io/resplat/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Haofei Xuç­äººæ°åçè®ºæâReSplat: Learning Recurrent Gaussian Splatsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="resplat">ReSplat: å­¦ä¹ å¾ªç¯é«æ¯æºå°çå¨é¢æè¦</h3>
<p><strong>1. è®ºæè§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>å½åçåé¦é«æ¯æºå°ï¼Gaussian Splattingï¼æ¨¡åè½ç¶å¨è®¡ç®æçåå¤çç¨çè¾å¥è®¾ç½®æ¹é¢è¡¨ç°åºè²ï¼ä½å¶æ§è½åéäºåæ¬¡ååæ¨çãè¿æå³çå®ä»¬é¾ä»¥å¨å¤æåºæ¯æé¢å¯¹æªè§æ°æ®åå¸æ¶ä¿æé«æ§è½åæ³åè½åãä¼ ç»çåºäºä¼åçæ¹æ³ï¼å¦3DGSï¼è½ç¶è½è¾¾å°é«è´¨éç»æï¼ä½è®¡ç®ææ¬é«æä¸èæ¶ãå æ­¤ï¼è®ºææ¨å¨è§£å³å¦ä½å¨ä¿æåé¦æ¨¡åæççåæ¶ï¼éè¿è¿­ä»£ä¼åæåé«æ¯æºå°æ¨¡åçéå»ºè´¨éåæ³åè½åï¼å°¤å¶æ¯å¨ç¨çè§å¾åä¸ååè¾¨çè®¾ç½®ä¸ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<p>ReSplatæ¨¡åå¼å¥äºä»¥ä¸å³é®åæ°ï¼</p>
<ul>
<li><strong>å¾ªç¯é«æ¯æºå°æ¨¡åï¼Recurrent Gaussian Splattingï¼</strong>ï¼ReSplatæåºäºä¸ç§åé¦å¾ªç¯ç½ç»ï¼éè¿è¿­ä»£å°ç»å3Dé«æ¯åæ°ï¼èæ éæ¾å¼è®¡ç®æ¢¯åº¦ãå¶æ ¸å¿ææ³æ¯å©ç¨é«æ¯æºå°çæ¸²æè¯¯å·®ä½ä¸ºä¸°å¯çåé¦ä¿¡å·ï¼æå¯¼å¾ªç¯ç½ç»å­¦ä¹ ææçé«æ¯æ´æ°ãè¿ç§åé¦æºå¶ä½¿å¶è½å¤èªç¶å°éåºæµè¯æ¶æªè§çæ°æ®åå¸ï¼ä»èå®ç°é²æ£çæ³åã</li>
<li><strong>ç´§åçåå§åéå»ºæ¨¡å</strong>ï¼ä¸ºäºå¯å¨å¾ªç¯è¿ç¨å¹¶éä½è®¡ç®å¼éï¼ReSplatå¼å¥äºä¸ä¸ªå¨<script type="math/tex">16 \times</script>ä¸éæ ·ç©ºé´ä¸­æä½çç´§åéå»ºæ¨¡åãè¯¥æ¨¡åçæçé«æ¯æ°éæ¯ä»¥å¾çæ¯åç´ é«æ¯æ¨¡åå°<script type="math/tex">16 \times</script>ï¼æ¾èåå°äºè®¡ç®å¼éï¼å¹¶åè®¸è¿è¡é«æçé«æ¯æ´æ°ã</li>
<li><strong>æ¸²æè¯¯å·®ä½ä¸ºåé¦ä¿¡å·</strong>ï¼ä¸ä¾èµç¹å¾ç¸å³æ§ææ¾å¼æ¢¯åº¦è®¡ç®çä¼ ç»å¾ªç¯ä¼åæ¹æ³ä¸åï¼ReSplatå©ç¨è¾å¥è§å¾çæ¸²æè¯¯å·®ï¼å¨ç¹å¾ç©ºé´ä¸­è®¡ç®ï¼ä½ä¸ºæå¯¼ä¿¡å·ãéè¿å¨å±æ³¨æåæºå¶å°æ¸²æè¯¯å·®ä¼ æ­å°3Dé«æ¯ï¼ä½¿å¾æ¯ä¸ªé«æ¯é½è½æ¥æ¶å°æ¥èªæææ¸²æè¯¯å·®çä¿¡æ¯ï¼ä»èå®ç°æ´ææçæ´æ°ã</li>
<li><strong>åé¶æ®µè®­ç»ç­ç¥</strong>ï¼æ¨¡åè®­ç»åä¸ºä¸¤ä¸ªé¶æ®µï¼é¦åè®­ç»ä¸ä¸ªåå§é«æ¯éå»ºæ¨¡åä»¥æä¾ç´§åçåå§åï¼ç¶åå»ç»è¯¥æ¨¡åå¹¶ç«¯å°ç«¯å°è®­ç»å¾ªç¯æ¨¡åï¼ä½¿ç¨æ¸²ææå¤±åææ°éå¢çæéè¿è¡çç£ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<p>ReSplatå¨å¤ä¸ªå®éªè®¾ç½®ä¸ååå¾äºæ¾èçææï¼</p>
<ul>
<li><strong>æ§è½æåä¸æçå¼é¡¾</strong>ï¼å¨DL3DVæ°æ®éä¸ï¼ä½¿ç¨8ä¸ªè¾å¥è§å¾ï¼512 Ã 960åè¾¨çï¼ï¼ReSplatçPSNRæé«äº+2.7 dBï¼åæ¶ä»ä½¿ç¨<script type="math/tex">1/16</script>çé«æ¯æ°éï¼æ¸²æéåº¦å¿«äº<script type="math/tex">4 \times</script>ãä¸åºäºä¼åç3DGSç¸æ¯ï¼ReSplatéåº¦å¿«äº100åã</li>
<li><strong>é²æ£çæ³åè½å</strong>ï¼ReSplatå¨æªè§æ°æ®éï¼å¦RealEstate10Kï¼åä¸åå¾ååè¾¨çï¼ä»<script type="math/tex">256 \times 256</script>å°<script type="math/tex">540 \times 960</script>ï¼ä¸è¡¨ç°åºå¼ºå¤§çæ³åè½åï¼ä¼äºä»¥å¾çåæ­¥åé¦æ¨¡åãä¾å¦ï¼å¨ä»<script type="math/tex">512 \times 960</script>æ³åå°<script type="math/tex">320 \times 640</script>æ¶ï¼PSNRæé«äº4dBã</li>
<li><strong>é«æ¯æ°éåæ¸²æéåº¦ä¼å</strong>ï¼éè¿<script type="math/tex">16 \times</script>ä¸éæ ·ç©ºé´ä¸­çé«æ¯éå»ºï¼ReSplatæ¾èåå°äºé«æ¯æ°éï¼å¹¶æé«äºæ¸²æéåº¦ï¼åæ¶ä¿æäºæåè¿çæ§è½ã</li>
<li><strong>å¿«éæ¶æ</strong>ï¼å¾ªç¯æ¨¡åå¨3æ¬¡è¿­ä»£åå³å¯æ¶æï¼è¿å¾çäºå¶å¿«éæ¶æç¹æ§ï¼ä½¿å¶å¨å®éåºç¨ä¸­æ´é«æã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼ReSplatæåå°å¹³è¡¡äºåé¦æ¹æ³çæçåè¿­ä»£ä¼åçéåºæ§ï¼ä¸ºç¨çè§å¾ä¸çé«è´¨é3Déå»ºåæ°é¢è§å¾åææä¾äºä¸ç§æ°é¢ä¸é«æçè§£å³æ¹æ¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<p>è®ºæä¹å¦è¯å°æåºäºReSplatçå±éæ§ï¼</p>
<ul>
<li><strong>kNNæ³¨æåè®¡ç®ææ¬</strong>ï¼å½åæ¨¡åä¾èµäºåºäºkNNçç¹æ³¨æåæºå¶ï¼å½é«æ¯æ°ééå¸¸å¤§ï¼ä¾å¦è¶è¿500Kï¼æ¶ï¼ä¼äº§çé«æçè®¡ç®ææ¬ã</li>
<li><strong>è¿­ä»£æ¬¡æ°é¥±å</strong>ï¼æ¨¡åå¨3æ¬¡è¿­ä»£åæ§è½è¶äºé¥±åãä½èæ¨æµï¼å¨å¾ªç¯è¿ç¨ä¸­åºå®é«æ¯æ°éå¯è½æ¯æ½å¨åå ã</li>
<li><strong>èªéåºæ´æ°ç­ç¥</strong>ï¼ç®åæ¨¡åä½¿ç¨åºå®çé«æ¯æ°éè¿è¡æ´æ°ï¼ç¼ºä¹æ´èªéåºçæ´æ°ç­ç¥ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<p>åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ´é«æçç¹æ³¨æåæºå¶</strong>ï¼æ¢ç´¢æ´é«æçåºäºç¹çæ³¨æåæºå¶ï¼å¦Wuç­äºº2022, 2024ï¼åç¨çç»æï¼Renç­äºº2024ï¼ï¼ä»¥è¿ä¸æ­¥æé«æ¨¡åçå¯æ©å±æ§åæçã</li>
<li><strong>èªéåºé«æ¯æ´æ°ç­ç¥</strong>ï¼è®¾è®¡æ´èªéåºçæ´æ°ç­ç¥ï¼ä¾å¦å¨å¾ªç¯è¿ç¨ä¸­å¨æè°æ´é«æ¯æ°éï¼ä»¥åæå½åæ¨¡åå¨åºå®è¿­ä»£æ¬¡æ°åæ§è½é¥±åçé®é¢ã</li>
<li><strong>æ©å±æµè¯æ¶è®¡ç®</strong>ï¼æ¢ç´¢å¦ä½è¿ä¸æ­¥æ©å±æµè¯æ¶è®¡ç®ï¼ä»¥åºå¯¹æ´å¤æææ´å¤§è§æ¨¡çåºæ¯ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼ReSplatéè¿å¼å¥ä¸ä¸ªå©ç¨æ¸²æè¯¯å·®ä½ä¸ºåé¦ä¿¡å·çå¾ªç¯åé¦ç½ç»ï¼å¹¶ç»åç´§åçåå§åæ¨¡åï¼æåå°å¨ä¿æé«æççåæ¶ï¼æ¾èæåäº3Dé«æ¯æºå°æ¨¡åçéå»ºè´¨éåæ³åè½åï¼ä¸ºç¨çè§å¾ä¸çæ°é¢è§å¾åæå¼è¾äºæ°çéå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients.</li>
<li>To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a <script type="math/tex">16 \times</script> subsampled space, producing <script type="math/tex">16 \times</script> fewer
Gaussians than previous per-pixel Gaussian models.</li>
<li>Extensive
experiments across varying of input views (2, 8, 16), resolutions (<script type="math/tex">256 \times
256</script> to <script type="math/tex">540 \times 960</script>), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08575v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08575v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08568v1'></a></p>
<h2 id="novaflow-zero-shot-manipulation-via-actionable-flow-from-generated-videos"><a href="https://arxiv.org/abs/2510.08568v1">NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos</a></h2>
<p><strong>Authors:</strong> Hongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, Jiahui Fu</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºHongyu Liç­äººæ°åçè®ºæâNovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videosâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼NovaFlowï¼éè¿çæè§é¢çå¯æä½æµå®ç°é¶æ ·æ¬æä½</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººé¶æ ·æ¬æ§è¡æ°é¢æä½ä»»å¡çæ ¸å¿ææãç°ææ¹æ³éå¸¸åè®¾ä»»å¡å¨åå¸åæä¾èµäºä¸æºå¨äººæ¬ä½å¹éçæ°æ®è¿è¡å¾®è°ï¼è¿éå¶äºè·¨å¹³å°çè¿ç§»è½åãNovaFlowè´åäºå¼åä¸ä¸ªæ éä»»ä½æ¼ç¤ºå³å¯å°ä»»å¡æè¿°è½¬æ¢ä¸ºç®æ æºå¨äººå¯æä½è®¡åçèªä¸»æä½æ¡æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
NovaFlowæ¡æ¶çæ ¸å¿åæ°å¨äºå¶æ¨¡ååè®¾è®¡åå¯¹â3Då¯æä½å¯¹è±¡æµâè¿ä¸ä¸­é´è¡¨ç¤ºçå©ç¨ãå·ä½è´¡ç®åæ¬ï¼
*   <strong>è§£è¦ä»»å¡çè§£ä¸ä½çº§æ§å¶ï¼</strong> NovaFlowå°ä»»å¡çè§£ï¼éè¿è§é¢çææ¨¡åï¼ä¸æºå¨äººä½çº§æ§å¶ï¼éè¿æç¥æ¨¡ååè½¨è¿¹ä¼åï¼åç¦»ï¼ä»èå®ç°äºè·¨æºå¨äººæ¬ä½çèªç¶è¿ç§»ã
*   <strong>å¯æä½3Då¯¹è±¡æµçè¸é¦ï¼</strong> æ¡æ¶é¦åå©ç¨è§é¢çææ¨¡åï¼å¦WanæVeoï¼æ ¹æ®ä»»å¡æè¿°åæä¸ä¸ª plausible çä»»å¡è§£å³è§é¢ãç¶åï¼éè¿ä¸ç³»åé¢è®­ç»çæç¥æ¨¡åï¼åæ¬åç®æ·±åº¦ä¼°è®¡ã3Dç¹è·è¸ªåå¯¹è±¡æ¥å°ï¼ï¼å°çæç2Dè§é¢è¸é¦æ3Då¯æä½å¯¹è±¡æµã
*   <strong>å¤çä¸åå¯¹è±¡ç±»åï¼</strong>
    *   <strong>åæ§å¯¹è±¡ï¼</strong> ä»å¯¹è±¡æµä¸­è®¡ç®ç¸å¯¹å§¿æï¼å¹¶éè¿æåæè®®åè½¨è¿¹ä¼åå°å¶è½¬åä¸ºæºå¨äººå¨ä½ã
    *   <strong>å¯åå½¢å¯¹è±¡ï¼</strong> å¯¹è±¡æµä½ä¸ºåºäºç²å­å¨åå­¦æ¨¡åçæ¨¡åé¢æµæ§å¶ï¼MPCï¼çè·è¸ªç®æ ï¼å®ç°å¯¹å¯åå½¢å¯¹è±¡çè§åã
*   <strong>é¶æ ·æ¬åæ æ¼ç¤ºï¼</strong> æ´ä¸ªæµç¨æ éä»»ä½æºå¨äººç¹å®æ°æ®æä»»å¡ç¹å®è®­ç»ï¼å®ç°äºçæ­£çé¶æ ·æ¬æä½ã
*   <strong>æç»éæ ·ï¼</strong> ä¸ºäºè¿æ»¤æè§é¢çææ¨¡åå¯è½å¼å¥çå¹»è§åä¸åçè¿å¨ï¼NovaFlowéç¨æç»éæ ·æ­¥éª¤ï¼å©ç¨VLMè¯ä¼°å¹¶éæ©æåçççææµã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
NovaFlowå¨åç§çå®ä¸çæä½ä»»å¡ä¸­åå¾äºæ¾èçæåï¼åæ¬åæ§ãå³èå¼åå¯åå½¢å¯¹è±¡çæçºµï¼ä½¿ç¨äºæ¡é¢Frankaæºæ¢°èåSpotåè¶³ç§»å¨æºå¨äººã
*   <strong>ä¼äºåºçº¿ï¼</strong> NovaFlowå¨é¶æ ·æ¬æ¹æ³ä¸­å®ç°äºæé«çæåçï¼å¹¶ä¸è¶è¶äºéè¦10-30æ¬¡æ¼ç¤ºè®­ç»çæ°æ®ä¾èµåæ¨¡ä»¿å­¦ä¹ ç­ç¥ï¼å¦Diffusion PolicyåInverse Dynamics Modelï¼ã
*   <strong>è·¨æ¬ä½æ³åï¼</strong> å®éªè¯æäºNovaFlowå¨ä¸åæºå¨äººæ¬ä½åå¯¹è±¡ç±»åä¸çæ³åè½åï¼æ éç¹å®äºæ¬ä½çè®­ç»ã
*   <strong>3Då¯¹è±¡æµçéè¦æ§ï¼</strong> è®ºæå¼ºè°äºå¯æä½3Då¯¹è±¡æµä½ä¸ºä¸­é´è¡¨ç¤ºçå³é®ä½ç¨ï¼å®ä½¿å¾æ¡æ¶è½å¤çè§£åæ§è¡å¤æçå¯¹è±¡è¿å¨ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç©çæ§è¡ç¶é¢ï¼</strong> å¤±è´¥åææ­ç¤ºï¼ä¸»è¦çç¶é¢å¨äºç©çæ§è¡é¶æ®µï¼å°¤å¶æ¯å¨æååå¤çæå¤å¨åå­¦æ¹é¢ãè¿è¡¨æå¼æ¾å¾ªç¯è®¡åä¸çå®ä¸çäº¤äºçå¤ææ§ä¹é´å­å¨å·®è·ã
*   <strong>è§é¢çææ¨¡åçå±éæ§ï¼</strong> è§é¢çææ¨¡åææ¶ä¼äº§çä¸ç¬¦åç©çè§å¾ãç¼ºä¹3Dä¸è´æ§æè¿åç¨æ·æä»¤çåå®¹ï¼å°½ç®¡æç»éæ ·ææç¼è§£ï¼ä½æªè½å®å¨æ¶é¤ã
*   <strong>è·è¸ªå¤±è´¥ï¼</strong> 3Dç¹è·è¸ªçç²¾åº¦ä¸è¶³ï¼éå¸¸ç±æ çº¹çè¡¨é¢ãä¸¥éé®æ¡æè§é¢æ¨¡åç»§æ¿çç´¯ç§¯ä¸ä¸è´æ§å¼èµ·ã
*   <strong>æåå¤±è´¥ï¼</strong> æºå¨äººæªè½æ­£ç¡®æåå¯¹è±¡ï¼ä¾å¦ï¼æ¹æ³ä¸å½ãæåå¤±è´¥åæ»ç§»ï¼ã
*   <strong>æ§è¡å¤±è´¥ï¼</strong> è½¨è¿¹æ§è¡è¿ç¨ä¸­åºç°çéè¯¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>é­ç¯åé¦ç³»ç»ï¼</strong> æªæ¥çå·¥ä½å¯ä»¥éä¸­äºæ´åä¸ä¸ªé­ç¯åé¦ç³»ç»ï¼å©ç¨ç¯å¢çå®æ¶åé¦æ¥æ¹è¿æéæ°è§åçæçæµï¼ä»èä½¿ç³»ç»æ´å·éåºæ§åé²æ£æ§ï¼ä»¥åºå¯¹ä¸å¯é¢è§çææã
*   <strong>æ¹è¿è§é¢çæå3Dæåæ¨¡åï¼</strong> è®ºææåºè§é¢çæå3Dæåæ¨¡åæ¯èæ¶æå¤çé¨åï¼æç¤ºæªæ¥å¯ä»¥éè¿ä½¿ç¨æ´å¿«çæ¨¡åæ¥æé«æçã
*   <strong>å¢å¼ºVLMçæ¨çè½åï¼</strong> è¿ä¸æ­¥å©ç¨VLMå¨æç»éæ ·ä¸­å¯¹è¿å¨è¿è¡æ´æ·±å¥çæ¨çï¼ä»¥æ´å¥½å°è¿æ»¤ä¸åçççæè§é¢ã</p>
<p>æ»èè¨ä¹ï¼NovaFlowæåºäºä¸ç§æ°é¢ä¸é«æçé¶æ ·æ¬æºå¨äººæä½èå¼ï¼éè¿å°å¤§è§æ¨¡é¢è®­ç»è§é¢çææ¨¡åçå¸¸è¯æ§ä»»å¡çè§£è½åè½¬åä¸ºå¯æä½ç3Då¯¹è±¡æµï¼æååæäºä¼ ç»æºå¨äººå­¦ä¹ ä¸­çæ°æ®ç¶é¢åæ³åéå¶ãå°½ç®¡å­å¨ä¸äºç©çæ§è¡æ¹é¢çææï¼ä½å¶æ¨¡åååé¶æ ·æ¬è½åä¸ºéç¨æºå¨äººæä½çæªæ¥åå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics.</li>
<li>We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08568v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08568v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08567v1'></a></p>
<h2 id="matrix-multimodal-agent-tuning-for-robust-tool-use-reasoning"><a href="https://arxiv.org/abs/2510.08567v1">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></h2>
<p><strong>Authors:</strong> Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Vision language models (VLMs) are increasingly deployed as controllers with
access to external tools for complex reasoning and decision-making, yet their
effectiveness remains limited by the scarcity of high-quality multimodal
trajectories and the cost of manual annotation. We address this challenge with
a vision-centric agent tuning framework that automatically synthesizes
multimodal trajectories, generates step-wise preference pairs, and trains a VLM
controller for robust tool-use reasoning. Our pipeline first constructs
M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified
trajectories, enabling imitation-based trajectory tuning. Building on this, we
develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool
reasoning. To achieve finer alignment, we further introduce Pref-X, a set of
11K automatically generated preference pairs, and optimize MATRIX on it via
step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use. Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Tajamul Ashrafç­äººæ°åçè®ºæâMATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoningâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³è§è§è¯­è¨æ¨¡åï¼VLMsï¼å¨ä½ä¸ºæ§å¶å¨è®¿é®å¤é¨å·¥å·è¿è¡å¤ææ¨çåå³ç­æ¶æé¢ä¸´çææãå·ä½æ¥è¯´ï¼ç°æVLMsçæææ§åéäºé«è´¨éå¤æ¨¡æè½¨è¿¹çç¨ç¼ºæ§ä»¥åæå¨æ æ³¨çé«æææ¬ï¼è¿éå¶äºå®ä»¬å¨å¼æ¾å¼å¤æ¨¡æä»»å¡ä¸­çé²æ£æ§åæ³åè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäºä¸ä¸ªåä¸º <strong>MATRIX</strong> çä¸¤é¶æ®µè§è§ä¸­å¿ä»£çå¾®è°æ¡æ¶ï¼ç¨äºå®ç°é²æ£çå·¥å·ä½¿ç¨æ¨çï¼
*   <strong>M-TRACE æ°æ®éï¼</strong> é¦åæå»ºäºä¸ä¸ªå¤§è§æ¨¡ç28.5Kå¤æ¨¡æä»»å¡æ°æ®éï¼åå«177Kæ¡ç»è¿éªè¯çè½¨è¿¹ãè¿äºè½¨è¿¹æ¯éè¿èªå¨ååæåéªè¯çæçï¼ç¨äºåºäºæ¨¡ä»¿çè½¨è¿¹å¾®è°ã
*   <strong>MATRIX Agentï¼</strong> å¨M-TRACEä¸å¯¹ä¸ä¸ªVLMæ§å¶å¨è¿è¡å¾®è°ï¼ä»¥å®ç°åæ­¥å·¥å·æ¨çã
*   <strong>Pref-X æ°æ®éï¼</strong> ä¸ºäºå®ç°æ´ç²¾ç»çå¯¹é½ï¼è¿ä¸æ­¥å¼å¥äº11Kç»èªå¨çæçåå¥½å¯¹ã
*   <strong>åæ­¥åå¥½å­¦ä¹ ï¼</strong> éè¿åæ­¥åå¥½å­¦ä¹ ï¼Direct Preference Optimization, DPOï¼å¨Pref-Xä¸ä¼åMATRIXï¼ä»¥æåå³ç­è´¨éåå·¥å·ä½¿ç¨å¯¹é½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
MATRIXå¨ä¸ä¸ªåºåæµè¯ï¼Agent-XãGTAåGAIAï¼ä¸åæç»­è¶è¶äºå¼æºåé­æºçVLMsãè¿è¡¨æMATRIXè½å¤å®ç°å¯æ©å±ä¸ææçå¤æ¨¡æå·¥å·ä½¿ç¨ãå·ä½æ¥è¯´ï¼å¨Agent-Xä¸ï¼MATRIXå¨å·¥å·åç¡®æ§ãå¿ å®åº¦åè¯­ä¹åç¡®æ§æ¹é¢åå¾äºæé«åï¼ç¸å¯¹äºQwen2-VL-7Bææ¾èæåãå¨GTAåGAIAä¸ï¼MATRIXä¹è¡¨ç°åºä¼è¶çæ§è½ï¼éªè¯äºå¶åæ­¥åå¥½ä¼åå¨å¤æ¨¡æå·¥å·ä½¿ç¨ä¸­çæææ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡MATRIXæ¯ææçï¼ä½ä»å­å¨ä¸äºå±éæ§ï¼
*   ç®åï¼å®ä»å¨æ¥è¯¢/ä»»å¡çº§å«ä¸å¯¹å¤æ¨¡æä¿¡å·è¿è¡æ¥å°ã
*   ä¾èµäºåºäºæç¤ºçéªè¯å¨ï¼å¨åå¸åç§»ä¸å¯è½ä¼åºç°é®é¢ã
*   å¨æ²¡æè½¨è¿¹çº§å«ä¿¡ç¨åéçæåµä¸ä¼ååæ­¥åå¥½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çå·¥ä½å°éè¿æ¢ç´¢èªéåºéªè¯å¨ãè¿ç»­å¤æ¨¡ææ¥å°ååå±åå¥½å»ºæ¨¡æ¥è§£å³ä¸è¿°å±éæ§ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥ä¸ä¸ªç»åè½¨è¿¹çç£ååæ­¥åå¥½ä¼åçåæ°æ¡æ¶ï¼æ¾èæåäºVLMså¨å¤æå¤æ¨¡æå·¥å·ä½¿ç¨æ¨çæ¹é¢çè½åï¼ä¸ºæå»ºæ´é²æ£ãå¯æ©å±çå¤æ¨¡æä»£çæä¾äºæ°çéå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use.</li>
<li>Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08567v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08567v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08566v1'></a></p>
<h2 id="d2gs-depth-and-density-guided-gaussian-splatting-for-stable-and-accurate-sparse-view-reconstruction"><a href="https://arxiv.org/abs/2510.08566v1">D<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction</a></h2>
<p><strong>Authors:</strong> Meixi Song, Xin Lin, Dizhe Zhang, Haodong Li, Xiangtai Li, Bo Du, Lu Qi</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in 3D Gaussian Splatting (3DGS) enable real-time,
high-fidelity novel view synthesis (NVS) with explicit 3D representations.
However, performance degradation and instability remain significant under
sparse-view conditions. In this work, we identify two key failure modes under
sparse-view conditions: overfitting in regions with excessive Gaussian density
near the camera, and underfitting in distant areas with insufficient Gaussian
coverage. To address these challenges, we propose a unified framework D<script type="math/tex">^2</script>GS,
comprising two key components: a Depth-and-Density Guided Dropout strategy that
suppresses overfitting by adaptively masking redundant Gaussians based on
density and depth, and a Distance-Aware Fidelity Enhancement module that
improves reconstruction quality in under-fitted far-field areas through
targeted supervision. Moreover, we introduce a new evaluation metric to
quantify the stability of learned Gaussian distributions, providing insights
into the robustness of the sparse-view 3DGS. Extensive experiments on multiple
datasets demonstrate that our method significantly improves both visual quality
and robustness under sparse view conditions. The project page can be found at:
https://insta360-research-team.github.io/DDGS-website/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Meixi Songç­äººæ°åçè®ºæâD<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstructionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼D<script type="math/tex">^2</script>GS: ç¨äºç¨³å®ååç¡®ç¨çè§å¾éå»ºçæ·±åº¦ä¸å¯åº¦å¼å¯¼é«æ¯æ³¼æº</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³3Dé«æ¯æ³¼æºï¼3DGSï¼å¨ç¨çè§å¾æ¡ä»¶ä¸è¿è¡æ°é¢è§å¾åæï¼NVSï¼æ¶é¢ä¸´çæ§è½ä¸éåä¸ç¨³å®æ§é®é¢ãå·ä½æ¥è¯´ï¼ä½èè¯å«åºä¸¤ä¸ªå³é®çå¤±è´¥æ¨¡å¼ï¼ä¸æ¯ç¸æºéè¿é«æ¯å¯åº¦è¿é«å¯¼è´çè¿æåï¼äºæ¯è¿åºåºåé«æ¯è¦çä¸è¶³å¯¼è´çæ¬ æåãç°æçç»ä¸dropoutç­ç¥æ æ³ææè§£å³è¿äºé®é¢ï¼çè³å¯è½æå®³éå»ºè´¨éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼è®ºææåºäºä¸ä¸ªç»ä¸çæ¡æ¶D<script type="math/tex">^2</script>GSï¼åå«ä¸¤ä¸ªæ ¸å¿ç»ä»¶ï¼
*   <strong>æ·±åº¦ä¸å¯åº¦å¼å¯¼Dropout (Depth-and-Density Guided Dropout, DD-Drop) ç­ç¥ï¼</strong> è¯¥ç­ç¥éè¿èªéåºå°æ ¹æ®é«æ¯ç¹çå¯åº¦åæ·±åº¦æ¥é®è½åä½é«æ¯ç¹ï¼ä»èæå¶è¿æåãDD-Dropéç¨å±é¨è¿ç»­åå¨å±ç¦»æ£æºå¶ï¼ä¸ºæ¯ä¸ªé«æ¯ç¹åéä¸ä¸ªåºäºå¶æ·±åº¦ï¼å°ç¸æºçæ¬§æ°è·ç¦»ï¼åå±é¨å¯åº¦ï¼éè¿kè¿é»ä¼°è®¡ï¼çdropoutåæ°ãæ­¤å¤ï¼å®è¿å¼å¥äºåºäºæ·±åº¦çåå±ç­ç¥ï¼å¯¹è¿åºãä¸­åºåè¿åºåºååºç¨ä¸åçè¡°åå å­ï¼ä»¥å®ç°æ´ç²¾ç»çæ§å¶ãè¿ç§æ¦çæ§åæ¸è¿å¼çdropoutæºå¶é¿åäºä¼ ç»ç¡¬æ§dropoutçå¼ç«¯ã
*   <strong>è·ç¦»æç¥ä¿çåº¦å¢å¼º (Distance-Aware Fidelity Enhancement, DAFE) æ¨¡åï¼</strong> è¯¥æ¨¡åéè¿æéå¯¹æ§ççç£ï¼æé«æ¬ æåè¿åºåºåçéå»ºè´¨éãDAFEå©ç¨åç®æ·±åº¦ä¼°è®¡æ¨¡åçææ·±åº¦å¾ï¼å¹¶æå»ºäºå¼æ©ç æ¥åç¦»è¿åºåè¿åºåºåãç¶åï¼è¯¥æ©ç è¢«ç¨äºè°å¶è®­ç»ç®æ ï¼æ¾å¤§è¿åºåºåççç£ä¿¡å·ï¼ä¿ä½¿æ¨¡åå¨è¯¥åºåçææ´å¯éçé«æ¯ç¹ï¼ä»èæææ´ç²¾ç»çç»èã
*   <strong>æ°é¢çè¯ä¼°ææ ââæ¨¡åé´é²æ£æ§ (Inter-Model Robustness, IMR)ï¼</strong> ä¸ºäºéåå­¦ä¹ å°çé«æ¯åå¸çç¨³å®æ§ï¼è®ºæå¼å¥äºIMRææ ãè¯¥ææ åºäº2-Wassersteinè·ç¦»åæä¼ä¼ è¾çè®ºï¼è¡¡éç¬ç«è®­ç»æ¨¡åä¹é´é«æ¯åå¸çä¸è´æ§ï¼ä»èè¯ä¼°æ¨¡åå¯¹åå§ååè®­ç»åªå£°çé²æ£æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
è®ºæå¨LLFFåMip-NeRF360ç­å¤ä¸ªæ°æ®éä¸è¿è¡äºå¹¿æ³çå®éªï¼ç»æè¡¨æD<script type="math/tex">^2</script>GSå¨ç¨çè§å¾æ¡ä»¶ä¸æ¾èæåäºè§è§è´¨éåé²æ£æ§ã
*   <strong>è§è§è´¨éæåï¼</strong> D<script type="math/tex">^2</script>GSå¨PSNRãSSIMãLPIPSåAVGEç­ææ ä¸ååå¾äºæåè¿çæ§è½ï¼å°¤å¶æ¯å¨1/8å1/4åè¾¨ççLLFFæ°æ®éä»¥å24è§å¾çMipNeRF360æ°æ®éä¸ãå®æ§ç»æä¹æ¾ç¤ºï¼D<script type="math/tex">^2</script>GSè½çææ´æ¸æ°çç»èï¼åå°ä¼ªå½±ï¼å¹¶ä¿çæ´å¤é«é¢ç»æã
*   <strong>é²æ£æ§å¢å¼ºï¼</strong> IMRææ çè¯ä¼°ç»ææ¾ç¤ºï¼D<script type="math/tex">^2</script>GSå¨3è§å¾å6è§å¾è®¾ç½®ä¸åå®ç°äºæä½çIMRå¼ï¼è¡¨æå¶å¨ä¸åè¿è¡ä¸­è½äº§çæ´ç¨³å®åä¸è´çé«æ¯éå»ºã
*   <strong>æ¶èç ç©¶ï¼</strong> æ¶èå®éªéªè¯äºDD-DropåDAFEæ¨¡åä¸­åä¸ªç»ä»¶çæææ§ï¼åæ¬æ·±åº¦åå¯åº¦åæ°ãæ·±åº¦åå±ä»¥åDAFEæå¤±çæéï¼è¯å®äºææç»ä»¶é½å¯¹æ´ä½æ§è½æäºè¡¥è´¡ç®ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡D<script type="math/tex">^2</script>GSå¨ç¨çè§å¾è®¾ç½®ä¸è¡¨ç°åºè²ï¼ä½ä»å­å¨æ¹è¿ç©ºé´ï¼
*   DD-Dropç­ç¥ä¾èµäºæå·¥è®¾å®çæ·±åº¦éå¼ååºå®æéç³»æ°ï¼å¯è½æ æ³å®å¨ææå¤æçåºæ¯ç¹å®åéªã
*   IMRé²æ£æ§ææ ä¾§éäºæ¨¡åé´ä¸è´æ§ï¼ä½å°æªèèå¨æè§å¾åæä¸çæç¥ç¨³å®æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   æ¢ç´¢èªéåºdropoutè°åº¦åå¯å­¦ä¹ ççç£æ©ç ï¼ä»¥æ´å¥½å°éåºä¸ååºæ¯ã
*   å¼åæ¶é´æç¥çé²æ£æ§ææ ï¼ä»¥è¯ä¼°å¨æè§å¾åæä¸çç¨³å®æ§ã</p>
<hr />
<p>æ»èè¨ä¹ï¼D<script type="math/tex">^2</script>GSéè¿åæ°çæ·±åº¦ä¸å¯åº¦å¼å¯¼dropoutåè·ç¦»æç¥ä¿çåº¦å¢å¼ºæºå¶ï¼ææè§£å³äºç¨çè§å¾3DGSä¸­çè¿æååæ¬ æåé®é¢ãåæ¶ï¼å¼å¥çæ¨¡åé´é²æ£æ§ææ ä¸ºè¯ä¼°ç¨çè§å¾3DGSçç¨³å®æ§æä¾äºæ°çè§è§ãè¿é¡¹å·¥ä½ä¸ºç¨çè§å¾æ°é¢è§å¾åæçç¨³å®æ§ååç¡®æ§æ ç«äºæ°çåºåã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Recent advances in 3D Gaussian Splatting (3DGS) enable real-time,
high-fidelity novel view synthesis (NVS) with explicit 3D representations.</li>
<li>To address these challenges, we propose a unified framework D<script type="math/tex">^2</script>GS,
comprising two key components: a Depth-and-Density Guided Dropout strategy that
suppresses overfitting by adaptively masking redundant Gaussians based on
density and depth, and a Distance-Aware Fidelity Enhancement module that
improves reconstruction quality in under-fitted far-field areas through
targeted supervision.</li>
<li>Moreover, we introduce a new evaluation metric to
quantify the stability of learned Gaussian distributions, providing insights
into the robustness of the sparse-view 3DGS.</li>
<li>Extensive experiments on multiple
datasets demonstrate that our method significantly improves both visual quality
and robustness under sparse view conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08566v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08566v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08564v1'></a></p>
<h2 id="how-to-teach-large-multimodal-models-new-skills"><a href="https://arxiv.org/abs/2510.08564v1">How to Teach Large Multimodal Models New Skills</a></h2>
<p><strong>Authors:</strong> Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&amp;Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhen Zhuç­äººæ°åçè®ºæâHow to Teach Large Multimodal Models New Skillsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºæé¢ç®ï¼</strong> å¦ä½ææå¤§åå¤æ¨¡ææ¨¡åæ°æè½ï¼
<strong>ä½èï¼</strong> Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu &amp; Derek Hoiem
<strong>æºæï¼</strong> ä¼å©è¯ºä¼å¤§å­¦åå·´çº³-é¦æ§åæ ¡</p>
<h3 id="_1">å¨é¢æè¦</h3>
<p>è¿ç¯è®ºææ·±å¥æ¢è®¨äºå¤§åå¤æ¨¡ææ¨¡åï¼LMMsï¼å¨å­¦ä¹ æ°æè½æ¶å¦ä½é¿åéå¿åæè½åçå³é®é®é¢ãç ç©¶åç°ï¼éè¿å¯¹æ¨¡åç¹å®ç»ä»¶è¿è¡éæ©æ§å¾®è°ï¼å¯ä»¥å¨è·å¾å¼ºå¤§æ°æè½çåæ¶ï¼ææéå¶å¯¹ç°æè½åçæå®³ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³å¤§åå¤æ¨¡ææ¨¡åï¼LMMsï¼å¨å­¦ä¹ æ°æè½æ¶å¦ä½é¿åâç¾é¾æ§éå¿âï¼catastrophic forgettingï¼çé®é¢ï¼å³å¨ç­çªçå¾®è°åï¼æ¨¡åå¨åæä»»å¡ä¸çæ§è½æ¾èä¸éãæ ¸å¿é®é¢æ¯ï¼å¦ä½å¨ä¸æ¹é¤LMMsåæè½åçåæä¸ï¼ææå®ä»¬æ°æè½ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç³»ç»æ§éå¿è¡ä¸ºåæï¼</strong> è®ºæéè¿å¨äºä¸ªç®æ æè½ä¸è¿è¡é¡ºåºå¾®è°ï¼å¹¶çæµå«ä¸ªéç¨åºåä»»å¡ä¸çæ§è½ï¼ç³»ç»å°ç ç©¶äºLMMsçéå¿è¡ä¸ºãç ç©¶åç°ï¼è¡¨è§éå¿å¨åæé¶æ®µå¯ä»¥é¨åæ¢å¤ã
*   <strong>è¾åºtokenåå¸æ¼ç§»çè¯å«ï¼</strong> è®ºæå°éå¿è¡ä¸ºè¿½æº¯å°è¾åºtokenåå¸çå¯æµéæ¼ç§»ãéè¿ä¸ä¸ªç®åçâè®¡æ°åå·®æ¢æµå¨âï¼counting-bias probeï¼ï¼åç°è¿ç§æ¼ç§»ä¸éå¿ç¨åº¦åæ­£ç¸å³ã
*   <strong>é²æ£çå¾®è°ç­ç¥ï¼</strong> åºäºå¯¹è¾åºåå¸æ¼ç§»ççè§£ï¼è®ºææåºäºä¸¤ç§ç®åèé²æ£çå¾®è°ç­ç¥ï¼è½å¤å¨éå¶æ¼ç§»çåæ¶å®ç°å¼ºå¤§çå­¦ä¹ ï¼
    *   <strong>ä»æ´æ°èªæ³¨æåæå½±å±ï¼SA Proj.ï¼ï¼</strong> è¿ç§æ¹æ³å¨è¯­è¨æ¨¡åä¸­ä»è°æ´èªæ³¨æåæå½±å±ï¼å®ç°äºæ¾èçå­¦ä¹ ææï¼åæ¶éå¿æå°ã
    *   <strong>ä»æ´æ°MLPçGate&amp;Upå±å¹¶å»ç»Downæå½±ï¼</strong> è¿ç§æ¹æ³å¨ä¿æå¼ºå¤§ç®æ å­¦ä¹ è½åçåæ¶ï¼ææéå¶äºéå¿ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å¨æ¨¡åå¾®è°çå±éæ§ï¼</strong> ç»ææ¾ç¤ºï¼å¨æ¨¡åå¾®è°è½ç¶è½å¸¦æ¥æå¤§çç®æ ä»»å¡å­¦ä¹ å¢çï¼ä½ä¹ä¼å¯¼è´æä¸¥éçéå¿ã
*   <strong>è§è§ä¾§æ´æ°çå¼±ææ§ï¼</strong> ä»æ´æ°è§è§ç¼ç å¨ææå½±å¨å¸¦æ¥çå­¦ä¹ å¢çå¾å°ï¼å¹¶ä¸å¯è½æå®³æ¨¡åçéç¨è½åã
*   <strong>è¯­è¨æ¨¡åå¾®è°çéè¦æ§ä¸ç¨³å®æ§ï¼</strong> è¯­è¨æ¨¡åï¼LLMï¼çå¾®è°å¯¹äºå­¦ä¹ æ°ä»»å¡è³å³éè¦ãå¨LLMåé¨ï¼SA Proj.åMLP (Gate&amp;Up) çéæ©æ§å¾®è°è¡¨ç°åºæä½³çå­¦ä¹ -ç¨³å®æ§æè¡¡ï¼å¨ä¸åæ¨¡åå®¶æåä»»å¡ä¸­åè½ä¿æå¼ºå¤§çç®æ å¢çï¼åæ¶æ¾èéå¶äºéå¿ã
*   <strong>éå¿ä¸è¾åºåå¸æ¼ç§»çå³èï¼</strong> è®ºæéè¿è®¡æ°åå·®æ¢æµå¨è¯å®ï¼éå¿å¾å¤§ç¨åº¦ä¸æ¯è¾åºåå¸æ¼ç§»çè¡¨ç°ãéå¶è¿ç§æ¼ç§»çæ¹æ³ï¼å¦ç¥è¯è¸é¦æå»ç»MLPçDownæå½±ï¼è½ææç¼è§£éå¿ã
*   <strong>éå¿æ¢å¤ç°è±¡ï¼</strong> è®ºæè§å¯å°ï¼å³ä½¿å¨ç­çªçå¾®è°åï¼æ¨¡åå¨åæä»»å¡ä¸çæ§è½ä¸éï¼ä½å¨å­¦ä¹ åç»­ä¸ä¸ä»»å¡æ¶ï¼è¿äºâéå¿âçç¥è¯å¯ä»¥é¨åæ¢å¤ï¼è¡¨æä¿¡æ¯å¹¶éæ°¸ä¹ä¸¢å¤±ï¼èæ¯ææ¶ä¸å¯è®¿é®ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>èµæºéå¶ï¼</strong> ç±äºèµæºæéï¼è®ºææªè½æ¢ç´¢æ¿ä»£æ¶æãæ´é¿çä»»å¡åºåãæ´å¤§è§æ¨¡çæ¨¡åä»¥åå¶ä»æ¨¡æï¼å¦é³é¢ï¼ã
*   <strong>æªæ·±å¥æ¢è®¨çæ´å¹¿æ³é®é¢ï¼</strong> éç§æ³é²ãå®å¨æ§åç¤¾ä¼å½±åç­æ´å¹¿æ³çé®é¢æå¾æªæ¥è¿ä¸æ­¥ç ç©¶ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   æ¢ç´¢æ´å¤æçæ¶æåæ´é¿çä»»å¡åºåï¼ä»¥è¿ä¸æ­¥éªè¯åä¼åææåºçå¾®è°ç­ç¥ã
*   å°ç ç©¶æ©å±å°æ´å¤§è§æ¨¡çLMMsåæ´å¤æ¨¡æï¼å¦é³é¢ï¼ï¼ä»¥æµè¯è¿äºæ¹æ³çéç¨æ§åå¯æ©å±æ§ã
*   æ·±å¥ç ç©¶LMMså¨æç»­å­¦ä¹ ä¸­çéç§ãå®å¨åç¤¾ä¼å½±åç­ä¼¦çé®é¢ã
*   è¿ä¸æ­¥æ¢ç´¢éå¿æ¢å¤çæºå¶ï¼ä»¥åå¦ä½å©ç¨è¿ç§ç°è±¡æ¥è®¾è®¡æ´ææçæç»­å­¦ä¹ ç­ç¥ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¯¹LMMså¾®è°æºå¶çæ·±å¥åæï¼ä¸ºå¦ä½å¨ä¸çºç²åæè½åçåæä¸æææ¨¡åæ°æè½æä¾äºå®ç¨çæå¯¼åçè®ºè§è§£ãå¶æåºçéæ©æ§å¾®è°ç­ç¥ï¼ç¹å«æ¯éå¯¹èªæ³¨æåæå½±å±åMLP Gate&amp;Upå±çæ´æ°ï¼ä¸ºLMMsçæç»­å­¦ä¹ åéåºæ§åå±å¼è¾äºæ°çéå¾ï¼æå©äºéä½æ¨¡åéåºçç¯å¢åç»æµææ¬ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities?</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08564v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08564v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08562v1'></a></p>
<h2 id="resad-normalized-residual-trajectory-modeling-for-end-to-end-autonomous-driving"><a href="https://arxiv.org/abs/2510.08562v1">ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Zhiyu Zheng, Shaoyu Chen, Haoran Yin, Xinbang Zhang, Jialv Zou, Xinggang Wang, Qian Zhang, Lefei Zhang</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>End-to-end autonomous driving (E2EAD) systems, which learn to predict future
trajectories directly from sensor data, are fundamentally challenged by the
inherent spatio-temporal imbalance of trajectory data. This imbalance creates a
significant optimization burden, causing models to learn spurious correlations
instead of causal inference, while also prioritizing uncertain, distant
predictions, thereby compromising immediate safety. To address these issues, we
propose ResAD, a novel Normalized Residual Trajectory Modeling framework.
Instead of predicting the future trajectory directly, our approach reframes the
learning task to predict the residual deviation from a deterministic inertial
reference. The inertial reference serves as a counterfactual, forcing the model
to move beyond simple pattern recognition and instead identify the underlying
causal factors (e.g., traffic rules, obstacles) that necessitate deviations
from a default, inertially-guided path. To deal with the optimization imbalance
caused by uncertain, long-term horizons, ResAD further incorporates Point-wise
Normalization of the predicted residual. It re-weights the optimization
objective, preventing large-magnitude errors associated with distant, uncertain
waypoints from dominating the learning signal. Extensive experiments validate
the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a
state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two
denoising steps, demonstrating that our approach significantly simplifies the
learning task and improves model performance. The code will be released to
facilitate further research.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhiyu Zhengç­äººæ°åçè®ºæâResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Drivingâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç«¯å°ç«¯èªå¨é©¾é©¶ï¼E2EADï¼ç³»ç»å¨ç´æ¥ä»ä¼ æå¨æ°æ®é¢æµæªæ¥è½¨è¿¹æ¶é¢ä¸´æ ¸å¿ææï¼è½¨è¿¹æ°æ®åºæçæ¶ç©ºä¸å¹³è¡¡ãè¿ç§ä¸å¹³è¡¡å¯¼è´æ¨¡åå¾åäºå­¦ä¹ èåå³èèéå ææ¨çï¼å¹¶ä¼åå¤çä¸ç¡®å®ãè¿è·ç¦»çé¢æµï¼ä»èæå®³å³æ¶å®å¨æ§ãå·ä½æ¥è¯´ï¼è¿è¡¨ç°ä¸ºâå ææ··æ·âåâè§åè§éå°å¢âï¼ä½¿å¾æ¨¡åé¾ä»¥çè§£é©¾é©¶è¡ä¸ºçæ ¹æ¬åå ï¼å¹¶è¢«è¿æãä¸ç¡®å®çè¯¯å·®ä¸»å¯¼ä¼åè¿ç¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºè§£å³ä¸è¿°é®é¢ï¼è®ºææåºäºResADï¼ä¸ä¸ªæ°é¢ç<strong>å½ä¸åæ®å·®è½¨è¿¹å»ºæ¨¡æ¡æ¶</strong>ãå¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>æ®å·®è½¨è¿¹å»ºæ¨¡ï¼Trajectory Residual Modelingï¼ï¼</strong> ResADä¸ç´æ¥é¢æµæªæ¥è½¨è¿¹ï¼èæ¯å°å­¦ä¹ ä»»å¡éæ°å®ä¹ä¸ºé¢æµç¸å¯¹äºç¡®å®æ§æ¯æ§åèçæ®å·®åå·®ãæ¯æ§åèï¼éè¿è½¦è¾å½åç¶æçæå®éåº¦æ¨¡åå¤æ¨å¾å°ï¼ä½ä¸ºåäºå®åºçº¿ï¼è¿«ä½¿æ¨¡åè¶è¶ç®åçæ¨¡å¼è¯å«ï¼è½¬èè¯å«å¯¼è´åç¦»é»è®¤æ¯æ§è·¯å¾çæ½å¨å æå ç´ ï¼å¦äº¤éè§åãéç¢ç©ï¼ãè¿ä½¿å¾æ¨¡åè½å¤å­¦ä¹ âä¸ºä»ä¹å¿é¡»æ¹åè½¨è¿¹âï¼èéâæªæ¥è½¨è¿¹æ¯ä»ä¹âã</li>
<li><strong>ç¹å¼æ®å·®å½ä¸åï¼Point-wise Residual Normalization, PRNormï¼ï¼</strong> ä¸ºè§£å³ä¸ç¡®å®ãé¿æè§éå¯¼è´çä¼åä¸å¹³è¡¡é®é¢ï¼ResADè¿ä¸æ­¥å¼å¥äºå¯¹é¢æµæ®å·®çç¹å¼å½ä¸åãè¿éè¿éæ°å æä¼åç®æ ï¼é²æ­¢ä¸è¿è·ç¦»ãä¸ç¡®å®è·¯ç¹ç¸å³çå¤§å¹åº¦è¯¯å·®ä¸»å¯¼å­¦ä¹ ä¿¡å·ï¼ç¡®ä¿æ°å¼ä¸è½å°ä½å³é®çè¿åºè°æ´ä¹è½è¢«æææè·ã</li>
<li><strong>æ¯æ§åèæ°å¨ï¼Inertia Reference Perturbationï¼ï¼</strong> éè¿å¯¹åå§éåº¦è¿è¡éæºæ°å¨ï¼çæä¸ç»ä¸åçæ¯æ§åèãè¿ä¸ä»å¢å¼ºäºæ¨¡åå¯¹ä¼ æå¨åªå£°çé²æ£æ§ï¼è¿éè¿çæä¸ç³»åæå¾åè®¾ï¼å®ç°äºå¤æ¨¡æè½¨è¿¹é¢æµï¼ä»èäº§çä¸ä¸ä¸æç¸å³çå¤æ ·åè·¯å¾ï¼é¿åäºä¼ ç»æ¹æ³ä¸­åºå®è¯æ±è¡¨çä½æåéå¶ã</li>
<li><strong>å¤æ¨¡æè½¨è¿¹æåºå¨ï¼Multimodal Trajectory Rankerï¼ï¼</strong> åé´ç°æå·¥ä½ï¼ResADå¼åäºä¸ä¸ªè½¨è¿¹æåºå¨ï¼ç¨äºä»å¤ä¸ªæ¨¡æä¸­éæ©æä¼è½¨è¿¹ï¼éè¿Transformerä¸æç¥è¡¨ç¤ºäº¤äºï¼å¹¶é¢æµåé¡¹ææ å¾åï¼ä»¥è¸é¦è§åå¨åçå¼è·¯ç¹çç¥è¯ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éªéªè¯äºResADæ¡æ¶çæææ§ï¼</p>
<ul>
<li><strong>æåè¿æ§è½ï¼</strong> å¨NAVSIMåºåæµè¯ä¸ï¼ResADä½¿ç¨ä»ä¸¤æ­¥å»åªçé¦èæ©æ£ç­ç¥ï¼å¨NAVSIM v1ä¸å®ç°äº88.6çPDMSï¼Planning Driving Metric Scoreï¼æåè¿æ§è½ï¼å¨æ´å·æææ§çNAVSIM v2ä¸å®ç°äº85.5çEPDMSï¼Extended PDMSï¼ï¼è¶è¶äºç°ææ¹æ³ã</li>
<li><strong>ç®åå­¦ä¹ ä»»å¡åæåæ§è½ï¼</strong> ç»æè¡¨æï¼ResADæ¾èç®åäºå­¦ä¹ ä»»å¡ï¼å¹¶æé«äºæ¨¡åæ§è½ãç¹å«æ¯å¨DACï¼Drivable Area Complianceï¼åEPï¼Ego Progressï¼ç­ææ ä¸è¡¨ç°åºè²ï¼è¡¨ææ¨¡åè½æ´å¥½å°éµå®è½¦éè¾¹çãå¯è¡é©¶åºåï¼å¹¶æ´ææå°å®æè·¯çº¿ã</li>
<li><strong>æ³åè½åï¼</strong> å¨Transfuserï¼åºäºMLPï¼åTransfuserDPï¼åºäºæ©æ£ï¼ç­å¼æè§åæ¨¡åä¸çå®éªè¡¨æï¼ResADçå½ä¸åæ®å·®è½¨è¿¹å»ºæ¨¡æ¹æ³å·æè¯å¥½çæ³åè½åï¼è½æ¾èæåè½¨è¿¹è´¨éï¼æé«E2EADç³»ç»çå®å¨æ§åå¯é æ§ã</li>
<li><strong>è®­ç»æçï¼</strong> PRNormä¸ä»æåäºæç»æ§è½ï¼è¿éè¿å éæ¨¡åæ¶æï¼æ¾èæé«äºè®­ç»æçã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æªæç¡®æåå½åResADæ¡æ¶çå±éæ§ãç¶èï¼ä½ä¸ºä¸ç§åºäºæ©æ£æ¨¡åçæ¹æ³ï¼å¶è®¡ç®ææ¬åæ¨çéåº¦ï¼å°½ç®¡è®ºæä¸­æå°ä»ä¸¤æ­¥å»åªï¼å¯è½ä»æ¯å®éé¨ç½²ä¸­éè¦èèçå ç´ ãæ­¤å¤ï¼è½ç¶æ¯æ§åèæ°å¨å®ç°äºå¤æ¨¡æï¼ä½å¶çæè½¨è¿¹çâå¤æ ·æ§âåâè¦çèå´âæ¯å¦è½å®å¨æ¶µçæææç«¯æç½è§é©¾é©¶åºæ¯ï¼ä»æå¾è¿ä¸æ­¥æ¢è®¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææç¡®æåºï¼ä»£ç å°åå¸ä»¥ä¿è¿è¿ä¸æ­¥ç ç©¶ï¼è¿æ¬èº«å°±æç¤ºäºç¤¾åºå¯ä»¥åºäºæ­¤è¿è¡æ©å±ãæ½å¨çæªæ¥ç ç©¶æ¹åå¯è½åæ¬ï¼</p>
<ul>
<li><strong>æ´å¤æçæ¯æ§åèæ¨¡åï¼</strong> æ¢ç´¢é¤äºæå®éåº¦æ¨¡åä¹å¤ï¼æ´å¤æçç©çæ¨¡åæé¢æµæ¨¡åä½ä¸ºæ¯æ§åèï¼ä»¥æä¾æ´ç²¾ç¡®çåºçº¿ã</li>
<li><strong>èªéåºæ°å¨ç­ç¥ï¼</strong> ç ç©¶æ´æºè½ãä¸ä¸ææç¥çæ¯æ§åèæ°å¨ç­ç¥ï¼ä»¥çææ´å·ç¸å³æ§åå¤æ ·æ§çå¤æ¨¡æè½¨è¿¹ã</li>
<li><strong>ä¸å¼ºåå­¦ä¹ çç»åï¼</strong> å°æ®å·®å»ºæ¨¡ä¸å¼ºåå­¦ä¹ ç»åï¼ä½¿æ¨¡åè½å¤éè¿ä¸ç¯å¢çäº¤äºï¼èªä¸»å­¦ä¹ æ´ä¼çæ®å·®é¢æµç­ç¥ã</li>
<li><strong>å®æ¶æ§è½ä¼åï¼</strong> è¿ä¸æ­¥ä¼åæ©æ£æ¨¡åçæ¨çæçï¼ä½¿å¶æ´éç¨äºå¯¹å»¶è¿ææçå®æ¶èªå¨é©¾é©¶ç³»ç»ã</li>
<li><strong>å¯è§£éæ§å¢å¼ºï¼</strong> æ·±å¥ç ç©¶æ®å·®å»ºæ¨¡å¦ä½æåæ¨¡åçå¯è§£éæ§ï¼å¹¶å¼åæ°çå¯è§åå·¥å·æ¥å±ç¤ºæ¨¡åå­¦ä¹ å°çå æå ç´ ã</li>
<li><strong>æç«¯åºæ¯å¤çï¼</strong> è¯ä¼°åæ¹è¿ResADå¨æç«¯æç½è§é©¾é©¶åºæ¯ä¸çæ§è½ï¼ç¡®ä¿å¶å¨æææ¡ä»¶ä¸é½è½å®å¨å¯é å°è¿è¡ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we
propose ResAD, a novel Normalized Residual Trajectory Modeling framework.</li>
<li>Instead of predicting the future trajectory directly, our approach reframes the
learning task to predict the residual deviation from a deterministic inertial
reference.</li>
<li>On the NAVSIM benchmark, ResAD achieves a
state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two
denoising steps, demonstrating that our approach significantly simplifies the
learning task and improves model performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08562v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08562v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08555v1'></a></p>
<h2 id="videocanvas-unified-video-completion-from-arbitrary-spatiotemporal-patches-via-in-context-conditioning"><a href="https://arxiv.org/abs/2510.08555v1">VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</a></h2>
<p><strong>Authors:</strong> Minghong Cai, Qiulin Wang, Zongli Ye, Wenze Liu, Quande Liu, Weicai Ye, Xintao Wang, Pengfei Wan, Kun Gai, Xiangyu Yue</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce the task of arbitrary spatio-temporal video completion, where a
video is generated from arbitrary, user-specified patches placed at any spatial
location and timestamp, akin to painting on a video canvas. This flexible
formulation naturally unifies many existing controllable video generation
tasks--including first-frame image-to-video, inpainting, extension, and
interpolation--under a single, cohesive paradigm. Realizing this vision,
however, faces a fundamental obstacle in modern latent video diffusion models:
the temporal ambiguity introduced by causal VAEs, where multiple pixel frames
are compressed into a single latent representation, making precise frame-level
conditioning structurally difficult. We address this challenge with
VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)
paradigm to this fine-grained control task with zero new parameters. We propose
a hybrid conditioning strategy that decouples spatial and temporal control:
spatial placement is handled via zero-padding, while temporal alignment is
achieved through Temporal RoPE Interpolation, which assigns each condition a
continuous fractional position within the latent sequence. This resolves the
VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen
backbone. To evaluate this new capability, we develop VideoCanvasBench, the
first benchmark for arbitrary spatio-temporal video completion, covering both
intra-scene fidelity and inter-scene creativity. Experiments demonstrate that
VideoCanvas significantly outperforms existing conditioning paradigms,
establishing a new state of the art in flexible and unified video generation.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Minghong Caiç­äººæ°åçè®ºæâVideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioningâçå¨é¢æè¦ã</p>
<hr />
<h3 id="videocanvas-unified-video-completion-from-arbitrary-spatiotemporal-patches-via-in-context-conditioning_1">è®ºææè¦ï¼VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³âä»»ææ¶ç©ºè§é¢è¡¥å¨âè¿ä¸æ ¸å¿é®é¢ãä¼ ç»çè§é¢çæä»»å¡ï¼å¦é¦å¸§å¾åå°è§é¢ãè§é¢ä¿®å¤ãæ©å±åæå¼ï¼éå¸¸æ¯å­¤ç«çãä»»å¡ç¹å®çï¼ç¼ºä¹ç»ä¸çæ¡æ¶ãç°ä»£æ½å¨è§é¢æ©æ£æ¨¡åé¢ä¸´ä¸ä¸ªæ ¹æ¬æ§éç¢ï¼å å æååèªç¼ç å¨ï¼VAEsï¼å°å¤ä¸ªåç´ å¸§åç¼©æåä¸æ½å¨è¡¨ç¤ºï¼å¯¼è´æ¶é´æ¨¡ç³æ§ï¼ä½¿å¾ç²¾ç¡®çå¸§çº§æ¡ä»¶æ§å¶åå¾ç»ææ§å°é¾ãä½èå¸æå®ç°ä¸ä¸ªçµæ´»çè§é¢çæèå¼ï¼åè®¸ç¨æ·å¨è§é¢ç»å¸ä¸çä»»ææ¶ç©ºä½ç½®æ¾ç½®ä»»ææå®è¡¥ä¸ï¼å¹¶çæè¿è´¯ãé«è´¨éçè§é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç»ä¸çä»»ææ¶ç©ºè§é¢è¡¥å¨ä»»å¡ï¼</strong> è®ºæé¦æ¬¡å½¢å¼åå¹¶å¼å¥äºâä»»ææ¶ç©ºè§é¢è¡¥å¨âä»»å¡ï¼å°å¤ç§ç°æåæ°å´çè§é¢çæåºæ¯ï¼å¦Any-Timestep-Patch/Image-to-VideoãIn/OutpaintingãCamera ControlåCross-scene Video Transitionsï¼ç»ä¸å°ä¸ä¸ªè¿è´¯çèå¼ä¸ã
*   <strong>VideoCanvasæ¡æ¶ï¼</strong> æåºäºä¸ä¸ªæ°é¢çæ¡æ¶VideoCanvasï¼é¦æ¬¡å°âä¸ä¸ææ¡ä»¶ï¼In-Context Conditioning, ICCï¼âèå¼åºç¨äºä»»ææ¶ç©ºè§é¢è¡¥å¨ä»»å¡ï¼ä¸æ éå¼å¥æ°çåæ°ã
*   <strong>æ··åæ¡ä»¶ç­ç¥ï¼</strong> ä¸ºäºè§£å³å æVAEsçæ¶é´æ¨¡ç³æ§åç©ºé´ä¸è§åæ§ï¼æåºäºä¸ç§æ··åæ¡ä»¶ç­ç¥ï¼è§£è¦äºç©ºé´åæ¶é´æ§å¶ï¼
    *   <strong>ç©ºé´æ¾ç½®ï¼</strong> éè¿é¶å¡«åï¼zero-paddingï¼å¤çï¼å°æ¡ä»¶è¡¥ä¸æ¾ç½®å¨å®æ´å¸§ç»å¸ä¸ï¼ç¶åç¬ç«ç¼ç ã
    *   <strong>æ¶é´å¯¹é½ï¼</strong> éè¿âæ¶é´RoPEæå¼ï¼Temporal RoPE Interpolationï¼âå®ç°ï¼ä¸ºæ¯ä¸ªæ¡ä»¶åéæ½å¨åºåä¸­çè¿ç»­åæ°ä½ç½®ï¼ä»èè§£å³VAEsçæ¶é´æ¨¡ç³æ§ï¼å¹¶å¨å»ç»çéª¨å¹²ç½ç»ä¸å®ç°åç´ å¸§çº§çç²¾ç¡®æ§å¶ã
*   <strong>VideoCanvasBenchåºåï¼</strong> å¼åäºé¦ä¸ªä¸é¨ç¨äºä»»ææ¶ç©ºè§é¢è¡¥å¨çç»¼ååºåï¼è¯ä¼°æ¨¡åå¨åºæ¯åä¿çåº¦ï¼intra-scene fidelityï¼ååºæ¯é´åé åï¼inter-scene creativityï¼ä¸¤æ¹é¢çæ§è½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>æ¾èä¼äºç°æèå¼ï¼</strong> å®éªè¯æï¼VideoCanvaså¨VideoCanvasBenchä¸æ¾èä¼äºç°æçæ¡ä»¶èå¼ï¼å¦æ½å¨æ¿æ¢ãééæ¼æ¥ï¼ï¼å¨åç§è§é¢è¡¥å¨ä»»å¡ä¸­å»ºç«äºæ°çææ¯æ°´å¹³ã
*   <strong>è§£å³æ¶é´æ¨¡ç³æ§ï¼</strong> æ¶é´RoPEæå¼ç­ç¥æåè§£å³äºå æVAEsçæ¶é´æ¨¡ç³æ§ï¼å®ç°äºç²¾ç¡®çåç´ å¸§å¯¹é½ï¼å¹¶å¨ç®æ å¸§å¤è¾¾å°PSNRå³°å¼ï¼åæ¶ä¿æäºé«ä¿çåº¦ã
*   <strong>é«è´¨éåè¿è´¯æ§ï¼</strong> å®æ§ç»ææ¾ç¤ºï¼VideoCanvasè½å¤çæå¹³æ»ãé«è´¨éçè§é¢ï¼ä¿æç©ä½èº«ä»½åç»æä¸è´æ§ï¼é¿åäºåºçº¿æ¹æ³ä¸­å¸¸è§çéæéå¤ãä¸èªç¶è¿æ¸¡æè¯­ä¹èè´¥é®é¢ã
*   <strong>é¶å¡«åçé²æ£æ§ï¼</strong> å®éªè¡¨æï¼æ··åå æè§é¢VAEså¯¹ç©ºé´é¶å¡«åå·æè¯å¥½çé²æ£æ§ï¼èå¯¹æ¶é´é¶å¡«ååéå¸¸èå¼±ï¼è¿éªè¯äºä½èè§£è¦ç©ºé´åæ¶é´å¤ççå¿è¦æ§ã
*   <strong>å¤åè½åºç¨ï¼</strong> VideoCanvaså±ç°äºå¼ºå¤§çæ°å´è½åï¼åæ¬çµæ´»çæ¶é´æ§å¶ï¼AnyI2Vï¼ãä»»ææ¶ç©ºæ§å¶ï¼AnyP2Vï¼ãåæè§é¢è¿æ¸¡ãé¿æ¶è§é¢æ©å±åå¾ªç¯ãç»ä¸è§é¢ç»ç»åç¸æºæ§å¶ç­ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>å¯éè¾å¥è®¡ç®ææ¬ï¼</strong> å°½ç®¡ç¬ç«å¸§ç¼ç å¯¹äºç¨çæ¡ä»¶éå¸¸ææï¼ä½å¯¹äºå¯éè¾å¥ï¼å³æ¡ä»¶å¸§æ°éè¾å¤ï¼ä¼å¸¦æ¥è®¡ç®ä¸çæè¡¡ï¼å¯¼è´æ¨çæ¶é´éæ¡ä»¶å¸§æ°éçå¢å èç¥å¾®å¢å ã
*   <strong>é¢è®­ç»VAEçå¼å®¹æ§ï¼</strong> å¤§å¤æ°é¢åçè§é¢åºç¡æ¨¡åä½¿ç¨çå æVAEså¹¶æªå¨é¶å¡«åçæ¶é´æ°æ®ä¸è¿è¡é¢è®­ç»ï¼è¿ä½¿å¾å®ä»¬ä¸æ´ç´ çé¶å¡«åæ¹æ³ä¸å¼å®¹ï¼ä¼å¯¼è´åå¸åç§»ï¼éè¦æè´µçVAEåDiTéª¨å¹²ç½ç»éæ°è®­ç»ãVideoCanvaséè¿å¶æ··åç­ç¥è§é¿äºè¿ä¸é®é¢ï¼ä½æªæ¥åºç¡æ¨¡åè¥è½é¢è®­ç»å¨é¶å¡«åæ°æ®ä¸ï¼å°æ¯æ°æ®é©±å¨èå¼çè¡¥åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ··åæºå¶æ¢ç´¢ï¼</strong> æªæ¥å·¥ä½å¯ä»¥æ¢ç´¢ç»åVideoCanvasçç²¾ç»å¯¹é½ä¸æ´é«æçtokenåªæç­ç¥çæ··åæºå¶ï¼ä»¥åºå¯¹å¯éæ¡ä»¶åºåçè®¡ç®ææ¬é®é¢ã
*   <strong>æ°æ®é©±å¨èå¼ï¼</strong> é¼å±æªæ¥åºç¡æ¨¡åå¨é¢è®­ç»æ¶çº³å¥é¶å¡«åæ°æ®ï¼ä»¥ä½¿æ°æ®é©±å¨èå¼ä¸VideoCanvasçæ¨¡åä¸­å¿åæ¡æ¶äºè¡¥ï¼è¿ä¸æ­¥æåçµæ´»åç»ä¸è§é¢åæçè½åã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥ä¸ä¸ªç»ä¸çæ¡æ¶ååæ°çæ··åæ¡ä»¶ç­ç¥ï¼ä¸ºè§é¢çæé¢åå¸¦æ¥äºæ¾èçè¿æ­¥ï¼ç¹å«æ¯å¨è§£å³å æVAEsçæ¶é´æ¨¡ç³æ§æ¹é¢ï¼ä¸ºå®ç°æ´çµæ´»ãæ´ç²¾ç»çè§é¢åå®¹åä½å¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce the task of arbitrary spatio-temporal video completion, where a
video is generated from arbitrary, user-specified patches placed at any spatial
location and timestamp, akin to painting on a video canvas.</li>
<li>We address this challenge with
VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)
paradigm to this fine-grained control task with zero new parameters.</li>
<li>We propose
a hybrid conditioning strategy that decouples spatial and temporal control:
spatial placement is handled via zero-padding, while temporal alignment is
achieved through Temporal RoPE Interpolation, which assigns each condition a
continuous fractional position within the latent sequence.</li>
<li>To evaluate this new capability, we develop VideoCanvasBench, the
first benchmark for arbitrary spatio-temporal video completion, covering both
intra-scene fidelity and inter-scene creativity.</li>
<li>Experiments demonstrate that
VideoCanvas significantly outperforms existing conditioning paradigms,
establishing a new state of the art in flexible and unified video generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08555v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08555v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08551v1'></a></p>
<h2 id="artdeco-towards-efficient-and-high-fidelity-on-the-fly-3d-reconstruction-with-structured-scene-representation"><a href="https://arxiv.org/abs/2510.08551v1">ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</a></h2>
<p><strong>Authors:</strong> Guanghao Li, Kerui Ren, Linning Xu, Zhewen Zheng, Changjian Jiang, Xin Gao, Bo Dai, Jian Pu, Mulin Yu, Jiangmiao Pang</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>On-the-fly 3D reconstruction from monocular image sequences is a
long-standing challenge in computer vision, critical for applications such as
real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:
per-scene optimization yields high fidelity but is computationally expensive,
whereas feed-forward foundation models enable real-time inference but struggle
with accuracy and robustness. In this work, we propose ARTDECO, a unified
framework that combines the efficiency of feed-forward models with the
reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose
estimation and point prediction, coupled with a Gaussian decoder that
transforms multi-scale features into structured 3D Gaussians. To sustain both
fidelity and efficiency at scale, we design a hierarchical Gaussian
representation with a LoD-aware rendering strategy, which improves rendering
fidelity while reducing redundancy. Experiments on eight diverse indoor and
outdoor benchmarks show that ARTDECO delivers interactive performance
comparable to SLAM, robustness similar to feed-forward systems, and
reconstruction quality close to per-scene optimization, providing a practical
path toward on-the-fly digitization of real-world environments with both
accurate geometry and high visual fidelity. Explore more demos on our project
page: https://city-super.github.io/artdeco/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Guanghao Liç­äººæ°åçè®ºæâARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼ARTDECOï¼é¢åç»æååºæ¯è¡¨ç¤ºçé«æé«ä¿çå®æ¶3Déå»º</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åç®å¾ååºåå®æ¶3Déå»ºé¢åé¿æå­å¨çææãç°ææ¹æ³é¢ä¸´ä¸ä¸ªä¸»è¦æè¡¡ï¼åºäºåºæ¯ä¼åçæ¹æ³è½æä¾é«ä¿çåº¦ä½è®¡ç®ææ¬é«æï¼èåé¦åºç¡æ¨¡åè½ç¶è½å®ç°å®æ¶æ¨çï¼ä½å¨åç¡®æ§åé²æ£æ§æ¹é¢è¡¨ç°ä¸è¶³ãå æ­¤ï¼ç ç©¶é®é¢æ¯å¦ä½å¼åä¸ä¸ªç»ä¸çæ¡æ¶ï¼è½å¤ç»ååé¦æ¨¡åçæçååºäºSLAMï¼åæ­¥å®ä½ä¸å»ºå¾ï¼ç®¡çº¿çå¯é æ§ï¼å®ç°é«ä¿çãé«æçä¸é²æ£çå®æ¶3Déå»ºï¼å°¤å¶æ¯å¨å¤§è§æ¨¡åå¤æ ·ååºæ¯ä¸­ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ARTDECOï¼Accurate localization, Robust reconstruction, and Decoder-based renderingï¼æåºäºä»¥ä¸å³é®åæ°ï¼
*   <strong>ç»ä¸æ¡æ¶è®¾è®¡ï¼</strong> ARTDECOæ¯ä¸ä¸ªéå®ä½ãéå»ºåæ¸²æäºä¸ä½çéæç³»ç»ï¼æ¨å¨å¨åç§ç¯å¢ä¸­é²æ£å°è¿è¡ãå®ç»åäºåé¦æ¨¡åçæçåSLAMç®¡çº¿çå¯é æ§ã
*   <strong>èå3Dåºç¡æ¨¡åï¼</strong> è®ºæå°3Dåºç¡æ¨¡åä½ä¸ºæ¨¡ååç»ä»¶éæå°å§¿æä¼°è®¡ãåç¯æ£æµåç¨ å¯ç¹é¢æµä¸­ãè¿ç§éææ¾èæé«äºå®ä½ç²¾åº¦åå»ºå¾ç¨³å®æ§ï¼åæ¶ä¿æäºæçã
*   <strong>åå±åéå¼é«æ¯è¡¨ç¤ºï¼</strong> ARTDECOå¼å¥äºä¸ç§åå±é«æ¯è¡¨ç¤ºï¼å¹¶ç»åäºLoDï¼ç»èå±æ¬¡ï¼æç¥çæ¸²æç­ç¥ãè¿ç§è®¾è®¡éè¿å°å¤å°ºåº¦ç¹å¾è½¬æ¢ä¸ºç»æå3Dé«æ¯ï¼æé«äºæ¸²æä¿çåº¦ï¼åæ¶åå°äºåä½ï¼å¯¹äºå¤§è§æ¨¡ãå¯å¯¼èªç¯å¢è³å³éè¦ã
*   <strong>æ··ååç«¯-åç«¯æ¶æï¼</strong> åç«¯è´è´£ä¼°è®¡ç¸å¯¹å§¿æå¹¶å¯¹å¸§è¿è¡åç±»ï¼æ®éå¸§ãæ å°å¸§ãå³é®å¸§ï¼ï¼åç«¯éè¿åç¯æ£æµåå¨å±æè°æ´ï¼BAï¼ç²¾ç¼å³é®å¸§å§¿æï¼æ å°æ¨¡ååå¢éä¼å3Dé«æ¯ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
ARTDECOå¨å«ä¸ªå¤æ ·åçå®¤åå¤åºåæµè¯ä¸è¿è¡äºå¹¿æ³å®éªï¼ç»ææ¾ç¤ºï¼
*   <strong>äº¤äºå¼æ§è½ï¼</strong> ARTDECOå®ç°äºä¸SLAMç³»ç»ç¸å½çäº¤äºå¼æ§è½ã
*   <strong>é²æ£æ§ï¼</strong> å¶é²æ£æ§ä¸åé¦ç³»ç»ç¸ä¼¼ï¼è½å¤åºå¯¹å¤æåå¤æ ·åçç¯å¢ï¼åæ¬è¿å¨æ¨¡ç³ååªå£°ã
*   <strong>éå»ºè´¨éï¼</strong> éå»ºè´¨éæ¥è¿äºåºäºåºæ¯ä¼åçæ¹æ³ï¼å¨PSNRãSSIMåLPIPSç­ææ ä¸è¡¨ç°åºè²ï¼å°¤å¶æ¯å¨ScanNet++ãTUMåVR-NeRFç­æææ§æ°æ®éä¸ã
*   <strong>å®ä½ç²¾åº¦ï¼</strong> éè¿åç¯æ£æµååæ¹å·®ç©éµæ»¤æ³¢ï¼ARTDECOå¨å¤å°ºåº¦å®¤åå¤æ°æ®éä¸å®ç°äºæ¾èæ´é«çå®ä½ç²¾åº¦ï¼ä¼äºå¶ä»3DGS-based SLAMæ¹æ³åé3DGS SLAMæ¹æ³ã
*   <strong>æçï¼</strong> ARTDECOçè¿è¡éåº¦å¿«äºé¤OnTheFly-NVSä¹å¤çææ3DGS-basedæ¹æ³ï¼å¶é¢å¤çå§¿æä¼°è®¡æ¶é´ææ¬è¢«å¶åè¶çå§¿æç²¾åº¦ææµæ¶ã</p>
<p>è¿äºç»æè¡¨æï¼ARTDECOä¸ºå®æ¶æ°å­åçå®ä¸çç¯å¢æä¾äºä¸æ¡å®ç¨éå¾ï¼å¼å·åç¡®çå ä½å½¢ç¶åé«è§è§ä¿çåº¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¯¹åé¦3Dåºç¡æ¨¡åçä¾èµï¼</strong> ARTDECOé¨åä¾èµäºåé¦3Dåºç¡æ¨¡åè¿è¡å¯¹åºåå ä½é¢æµãå°½ç®¡è¿äºæ¨¡åè½å®ç°å¿«éåå¯æ©å±çæ¨çï¼ä½å¨åªå£°ãæ¨¡ç³æåç§ååä¸ï¼ä»¥åå½è¾å¥è¶åºè®­ç»åå¸æ¶ï¼å¶é²æ£æ§ä¼éä½ã
*   <strong>ç¯å¢åè®¾ï¼</strong> ç³»ç»åè®¾åç§ä¸è´ä¸è§å·®åè¶³ãè¿åè¿äºåè®¾ï¼å¦ä½çº¹çè¡¨é¢ãéå¤ç»ææè¿ä¹éåçè½¨è¿¹ï¼å¯è½å¯¼è´æ¼ç§»æä¼ªå½±ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>ä¸ç¡®å®æ§ä¼°è®¡ï¼</strong> æ´åä¸ç¡®å®æ§ä¼°è®¡ï¼ä»¥æé«ç³»ç»å¨ç°å®ä¸çç¯å¢ä¸­çæ³åè½ååå¯é æ§ã
*   <strong>èªéåºæ¨¡åéæ©ï¼</strong> å¼å¥èªéåºæ¨¡åéæ©æºå¶ï¼ä»¥æ´å¥½å°å¤çä¸ååºæ¯åè¾å¥æ¡ä»¶ã
*   <strong>æ´å¼ºçåéªï¼</strong> æ¢ç´¢æ´å¼ºçåéªç¥è¯ï¼ä»¥è¿ä¸æ­¥æé«ç³»ç»çé²æ£æ§ååç¡®æ§ï¼å°¤å¶æ¯å¨é¢ä¸´ä¸è¿°å±éæ§æ¶ã</p>
<hr />
<p>æ»èè¨ä¹ï¼ARTDECOéè¿å·§å¦å°ç»ååé¦3Dåºç¡æ¨¡åçæçåSLAMç®¡çº¿çå¯é æ§ï¼å¹¶å¼å¥åæ°çåå±é«æ¯è¡¨ç¤ºï¼å¨å®æ¶3Déå»ºé¢ååå¾äºæ¾èè¿å±ãå®å¨ä¿æé«ä¿çåº¦çåæ¶ï¼å®ç°äºäº¤äºå¼æ§è½åå¼ºå¤§çé²æ£æ§ï¼ä¸ºAR/VRãæºå¨äººåæ°å­å­ªçç­åºç¨æä¾äºæåæ¯çè§£å³æ¹æ¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose ARTDECO, a unified
framework that combines the efficiency of feed-forward models with the
reliability of SLAM-based pipelines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08551v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08551v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-10 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
