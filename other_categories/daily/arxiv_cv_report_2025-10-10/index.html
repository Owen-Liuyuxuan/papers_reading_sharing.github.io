<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-10 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-09/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-13/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-10">Arxiv Computer Vision Papers - 2025-10-10</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-08" class="nav-link">Arxiv 计算机视觉每日报告执行摘要 (2025-10-08)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#quantum-enhanced-computer-vision-going-beyond-classical-algorithms" class="nav-link">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-language-action-models-for-robotics-a-review-towards-real-world-applications" class="nav-link">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a>
                </li>
                <li class="nav-item">
                    <a href="#resplat-learning-recurrent-gaussian-splats" class="nav-link">ReSplat: Learning Recurrent Gaussian Splats</a>
                </li>
                <li class="nav-item">
                    <a href="#novaflow-zero-shot-manipulation-via-actionable-flow-from-generated-videos" class="nav-link">NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#matrix-multimodal-agent-tuning-for-robust-tool-use-reasoning" class="nav-link">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#d2gs-depth-and-density-guided-gaussian-splatting-for-stable-and-accurate-sparse-view-reconstruction" class="nav-link">D^2GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#how-to-teach-large-multimodal-models-new-skills" class="nav-link">How to Teach Large Multimodal Models New Skills</a>
                </li>
                <li class="nav-item">
                    <a href="#resad-normalized-residual-trajectory-modeling-for-end-to-end-autonomous-driving" class="nav-link">ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#videocanvas-unified-video-completion-from-arbitrary-spatiotemporal-patches-via-in-context-conditioning" class="nav-link">VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</a>
                </li>
                <li class="nav-item">
                    <a href="#artdeco-towards-efficient-and-high-fidelity-on-the-fly-3d-reconstruction-with-structured-scene-representation" class="nav-link">ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-10">Arxiv Computer Vision Papers - 2025-10-10</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-08">Arxiv 计算机视觉每日报告执行摘要 (2025-10-08)</h2>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文展示了计算机视觉领域持续的快速发展，主要集中在以下几个关键领域：</p>
<ul>
<li><strong>多模态与具身智能：</strong> 显著的趋势是视觉-语言-动作模型在机器人和具身智能中的应用，旨在实现更通用、更强大的智能体。</li>
<li><strong>3D 重建与新视图合成：</strong> Gaussian Splatting (GS) 及其变体仍然是 3D 场景表示和新视图合成的热点，研究重点在于提高效率、稳定性和准确性。</li>
<li><strong>大模型能力扩展与应用：</strong> 如何有效地教授大型多模态模型新技能，以及利用生成模型进行零样本操作是重要的研究方向。</li>
<li><strong>自主驾驶：</strong> 持续关注端到端自主驾驶的鲁棒性和轨迹建模。</li>
<li><strong>视频处理：</strong> 视频补全和生成视频的应用也占据一席之地。</li>
<li><strong>新兴计算范式：</strong> 量子计算在计算机视觉中的潜在应用开始浮现。</li>
</ul>
<p><strong>2. 特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms" (Natacha Kuete Meli et al.)：</strong> 这篇论文具有前瞻性，预示了量子计算在计算机视觉领域的潜在颠覆性影响，尽管目前可能仍处于早期阶段，但其概念性创新值得关注。</li>
<li><strong>"Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications" (Kento Kawaharazuka et al.)：</strong> 作为一篇综述，它系统地梳理了视觉-语言-动作模型在机器人领域的进展，对于理解该领域的现状和未来挑战具有重要指导意义。</li>
<li><strong>"NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos" (Hongyu Li et al.)：</strong> 这篇论文展示了利用生成视频中的可操作流实现零样本操作的创新方法，为具身智能和机器人操作提供了新的思路。</li>
<li><strong>"How to Teach Large Multimodal Models New Skills" (Zhen Zhu et al.)：</strong> 鉴于大型多模态模型日益增长的重要性，这篇论文探讨了如何有效地扩展其能力，对于 LMM 的实际应用和未来发展至关重要。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>量子计算机视觉：</strong> 虽然仍处于萌芽阶段，但量子计算与计算机视觉的结合可能在未来带来范式转变。</li>
<li><strong>具身智能的零样本操作：</strong> 利用生成模型和可操作流实现零样本机器人操作，是具身智能领域一个充满前景的方向。</li>
<li><strong>Gaussian Splatting 的持续优化：</strong> D<script type="math/tex">^2</script>GS 和 ReSplat 等工作表明，GS 仍在不断演进，以解决稀疏视图重建、动态场景和效率等挑战。</li>
<li><strong>多模态智能体的工具使用推理：</strong> MATRIX 强调了多模态智能体在复杂工具使用场景下的鲁棒推理能力，这是迈向更通用 AI 的关键一步。</li>
<li><strong>统一的视频补全框架：</strong> VideoCanvas 提出的统一视频补全方法，通过上下文条件化处理任意时空补丁，展示了视频内容生成和编辑的进步。</li>
</ul>
<p><strong>4. 建议完整阅读的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对于关注具身智能和机器人：</strong><ul>
<li><strong>"Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications"</strong> (Kento Kawaharazuka et al.) - 提供全面的背景和未来方向。</li>
<li><strong>"NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos"</strong> (Hongyu Li et al.) - 创新性地利用生成视频实现零样本操作。</li>
<li><strong>"MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning"</strong> (Tajamul Ashraf et al.) - 关注多模态智能体的复杂推理能力。</li>
</ul>
</li>
<li><strong>对于关注 3D 重建和新视图合成：</strong><ul>
<li><strong>"ReSplat: Learning Recurrent Gaussian Splats"</strong> (Haofei Xu et al.) - 探索动态场景下的 GS 应用。</li>
<li><strong>"D<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction"</strong> (Meixi Song et al.) - 解决稀疏视图重建的挑战。</li>
</ul>
</li>
<li><strong>对于关注大模型和其能力扩展：</strong><ul>
<li><strong>"How to Teach Large Multimodal Models New Skills"</strong> (Zhen Zhu et al.) - 探讨 LMM 的关键能力扩展问题。</li>
</ul>
</li>
<li><strong>对于关注未来计算范式：</strong><ul>
<li><strong>"Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms"</strong> (Natacha Kuete Meli et al.) - 了解量子计算在 CV 领域的潜在应用。</li>
</ul>
</li>
<li><strong>对于关注自主驾驶：</strong><ul>
<li><strong>"ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving"</strong> (Zhiyu Zheng et al.) - 深入了解端到端自主驾驶的最新进展。</li>
</ul>
</li>
</ul>
<p>这份摘要旨在帮助您快速把握今日 Arxiv 计算机视觉论文的核心内容和潜在价值，以便您能更高效地进行深入研究。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></li>
<li><a href="#2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></li>
<li><a href="#2510.08575v1">ReSplat: Learning Recurrent Gaussian Splats</a></li>
<li><a href="#2510.08568v1">NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos</a></li>
<li><a href="#2510.08567v1">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></li>
<li><a href="#2510.08566v1">D<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction</a></li>
<li><a href="#2510.08564v1">How to Teach Large Multimodal Models New Skills</a></li>
<li><a href="#2510.08562v1">ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</a></li>
<li><a href="#2510.08555v1">VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</a></li>
<li><a href="#2510.08551v1">ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.07317v1'></a></p>
<h2 id="quantum-enhanced-computer-vision-going-beyond-classical-algorithms"><a href="https://arxiv.org/abs/2510.07317v1">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a></h2>
<p><strong>Authors:</strong> Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文是对量子增强计算机视觉（QeCV）领域的一项全面综述，旨在为计算机视觉社区提供一个量子计算的参考指南。它系统地介绍了QeCV的基本概念、方法论以及如何将计算机视觉问题与量子硬件兼容，并探讨了该领域的现有工具、挑战和未来方向。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>这篇论文本身是一篇综述，其关键“创新”在于其<strong>全面性和整合性</strong>。它不是提出一个新的算法，而是：
*   <strong>系统性地定义和构建了QeCV领域：</strong> 将计算机视觉、优化理论、机器学习和量子计算交叉融合，明确了QeCV的范畴。
*   <strong>双范式方法论：</strong> 详细阐述了如何利用两种主要的量子计算范式（基于门的量子计算和量子退火）来解决计算机视觉问题。
*   <strong>兼容性与实践性：</strong> 强调了开发与量子硬件兼容的专门算法的重要性，并提供了关于如何访问、编程和模拟量子计算机的实用信息。
*   <strong>面向CV社区的桥梁：</strong> 旨在弥合计算机视觉专家与量子计算之间的知识鸿沟，为CV研究人员提供进入QeCV领域的入门指南。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<p>这篇综述对计算机视觉领域具有深远的潜在影响：
*   <strong>加速QeCV研究：</strong> 作为一篇全面的参考指南，它将极大地降低计算机视觉研究人员进入量子计算领域的门槛，从而加速QeCV新算法和应用的开发。
*   <strong>范式转变的催化剂：</strong> 强调了量子计算在处理经典方法难以解决或耗时过长的问题上的潜在优势，可能促使计算机视觉领域从根本上重新思考某些问题的解决方式。
*   <strong>推动量子神经网络发展：</strong> 提出参数化量子电路可能成为经典神经网络的长期替代方案，这预示着未来计算机视觉模型架构的重大变革。
*   <strong>标准化与协作：</strong> 讨论了QeCV论文的发表和评审方面，有助于建立该领域的最佳实践和促进跨学科协作。</p>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>计算摄影与图像处理：</strong> 图像去噪、超分辨率、图像重建等，其中某些优化问题在经典计算中可能非常耗时。</li>
<li><strong>医学影像分析：</strong> 复杂的图像分割、疾病诊断、药物发现中的分子模拟，需要处理大量高维数据和复杂的模式识别。</li>
<li><strong>机器人与自主系统：</strong> 实时感知、路径规划、决策制定，尤其是在需要快速解决复杂优化问题的场景。</li>
<li><strong>大规模数据分析与模式识别：</strong> 任何涉及处理海量视觉数据并从中提取复杂模式的应用，如遥感、天文学图像分析。</li>
<li><strong>优化问题：</strong> 计算机视觉中的许多任务本质上是优化问题（如特征匹配、结构光重建、多视图几何），量子退火等技术可能提供更优的解决方案。</li>
<li><strong>机器学习基础研究：</strong> 探索量子机器学习模型（如量子神经网络）在视觉任务中的表现和理论极限。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>技术成熟度：</strong> 摘要明确指出QeCV是一个“新研究领域”，且“专门和根本性的新算法必须被开发”，这暗示了该领域仍处于早期阶段，距离实际广泛应用尚远。</li>
<li><strong>硬件限制：</strong> 尽管提到了量子硬件，但当前的量子计算机仍存在噪声大、纠错能力弱、量子比特数量有限等问题，这些都可能限制QeCV算法的实际性能和可扩展性。</li>
<li><strong>算法开发难度：</strong> “专门和根本性的新算法”的开发本身就是一项巨大的挑战，需要深厚的量子物理和计算机科学知识。将经典CV问题转化为量子兼容的形式并非易事。</li>
<li><strong>理论与实践的差距：</strong> 摘要强调了“高潜力”和“长期”替代经典神经网络，这表明目前QeCV的理论优势可能尚未在实际应用中得到充分验证或超越经典方法。</li>
<li><strong>可访问性与学习曲线：</strong> 尽管旨在为CV社区提供参考，但量子计算本身具有较高的学习曲线，即使有综述，非专业人士也可能面临理解和实践的困难。</li>
<li><strong>综述性质：</strong> 作为一篇综述，它本身不提供新的实验结果或算法，其价值在于知识的整合和方向的指引，而非直接的技术突破。</li>
</ul>
<hr />
<p>总而言之，这篇综述论文在计算机视觉领域具有重要的战略意义，它为新兴的量子增强计算机视觉领域奠定了基础，并为未来的研究指明了方向。它预示着计算机视觉可能迎来一场由量子计算驱动的范式变革，尽管这条道路充满挑战，但其潜在的回报是巨大的。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing.</li>
<li>However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07317v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07317v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.07077v1'></a></p>
<h2 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications"><a href="https://arxiv.org/abs/2510.07077v1">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a></h2>
<p><strong>Authors:</strong> Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu</p>
<p><strong>Published:</strong> 2025-10-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Kento Kawaharazuka等人撰写的论文“Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications”的全面摘要。</p>
<hr />
<h3 id="vision-language-action-models-for-robotics-a-review-towards-real-world-applications_1">论文摘要：Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决机器人领域中一个核心挑战：如何开发能够跨不同任务、物体、机器人形态和环境进行泛化的机器人策略，从而实现更灵活、可扩展的真实世界部署。具体来说，它关注于视觉-语言-动作（VLA）模型，这些模型通过统一视觉、语言和动作数据来学习通用策略，以克服传统机器人系统在泛化能力和数据效率方面的局限性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要贡献在于提供了一个全面、全栈的VLA系统综述，整合了软件和硬件组件，这与以往仅关注动作表示或高级模型架构的调查不同。具体的方法论贡献体现在以下几个方面：</p>
<ul>
<li><strong>VLA设计策略和演变（Section III）：</strong> 论文系统地回顾了VLA模型从早期基于CNN的端到端架构（如CLIPort）到基于Transformer的序列模型（如Gato、VIMA），再到利用预训练VLM骨干的统一真实世界策略（如RT系列、OpenVLA），以及集成扩散和流匹配技术（如Octo、RDT-1B、<script type="math/tex">\pi_0</script>）的模型的历史演变。最新进展包括潜在动作学习（如LAPA）和分层控制框架（如GROOT N1、<script type="math/tex">\pi_{0.5}</script>）。</li>
<li><strong>架构和构建模块（Section IV）：</strong> 详细分类了VLA模型的核心架构类型，包括感知运动模型（Sensorimotor Model）、世界模型（World Model）和基于可供性模型（Affordance-Based Model）。感知运动模型进一步细分为七种变体，涵盖了Transformer与离散动作令牌、扩散动作头、扩散Transformer、VLM与离散动作令牌、VLM与扩散/流匹配动作头以及VLM与扩散Transformer的组合。世界模型则通过预测未来观察或学习潜在动作来指导动作生成。</li>
<li><strong>模态特定处理技术（Section IV-D）：</strong> 详细阐述了VLA模型如何处理多种输入模态，包括视觉（使用ResNet、ViT、CLIP、SigLIP等）、语言（使用T5、LLaMA等分词器和编码器）和动作（离散化、连续动作建模、潜在动作学习、跨形态动作表示）。此外，还讨论了音频、触觉和3D信息（深度图像、多视图图像、体素表示、点云）等辅助模态的整合。</li>
<li><strong>训练策略和实现（Section V）：</strong> 总结了VLA模型的训练方法，包括监督学习、自监督学习和强化学习。强调了预训练和后训练阶段的重要性，以及数据规模、VLM骨干、梯度隔离、参数高效适应方法（如LoRA）和多任务学习在提高泛化能力和训练效率方面的作用。</li>
<li><strong>数据收集、数据集和数据增强（Section VI）：</strong> 综述了真实世界机器人数据收集方法（远程操作、代理设备、人类数据收集），并列举了用于预训练的公开数据集（人类视频数据集、仿真数据集、真实机器人数据集）。此外，还讨论了视觉、语言和动作数据增强技术，以应对数据稀缺问题。</li>
<li><strong>机器人评估和应用（Section VII）：</strong> 介绍了VLA研究中常用的机器人平台（机械臂、手/夹持器、移动机器人、四足机器人、人形机器人）以及评估基准（如robosuite、ManiSkill、LIBERO、CALVIN）。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该论文的综述揭示了VLA模型在机器人领域取得的显著进展和潜力：</p>
<ul>
<li><strong>泛化能力的提升：</strong> 通过大规模数据集和预训练基础模型（特别是VLM）的利用，VLA模型在跨任务、物体和环境方面表现出更强的泛化能力。</li>
<li><strong>架构的演进：</strong> 从简单的CNN到复杂的Transformer、扩散模型和分层架构，VLA模型的设计变得越来越精巧，能够更好地处理多模态输入并生成平滑、精确的动作。</li>
<li><strong>多模态融合：</strong> 除了视觉和语言，触觉、音频和3D信息等辅助模态的整合，进一步增强了机器人的感知和交互能力，尤其是在接触丰富的任务中。</li>
<li><strong>真实世界部署的潜力：</strong> 随着数据收集策略的改进、公开数据集的丰富以及数据增强技术的应用，VLA模型正逐步克服真实世界部署的挑战，为更灵活、可扩展的机器人系统奠定基础。</li>
<li><strong>分层推理和规划：</strong> 分层架构和思维链（CoT）推理的兴起，使得VLA模型能够进行更鲁棒的规划、任务分解和上下文感知动作生成，尤其适用于长周期、多步骤任务。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
尽管VLA模型取得了显著进展，论文也指出了以下局限性：</p>
<ul>
<li><strong>数据稀缺和多样性不足：</strong> 尽管有大规模数据集，但同时满足视觉、语言和动作三种模态的数据集在规模和多样性上仍然有限，特别是高质量的机器人演示数据收集成本高昂。</li>
<li><strong>机器人形态转移挑战：</strong> 机器人形态的多样性（关节配置、传感器类型、运动空间等）使得跨形态策略转移成为一个主要挑战。将人类运动数据映射到机器人可执行动作也非易事。</li>
<li><strong>计算和训练成本：</strong> VLA模型的高维和多模态输入导致巨大的计算需求，尤其是在处理长时序序列、高分辨率图像或额外模态时。</li>
<li><strong>实时性和平滑性：</strong> 离散动作令牌有时缺乏实时响应性和平滑性，而连续动作生成方法（如扩散和流匹配）正在解决这一问题。</li>
<li><strong>评估的标准化不足：</strong> 真实世界环境中VLA模型的评估指标仍然定义不清，泛化评估面临机器人形态差异、安全问题和可复现性等挑战。</li>
<li><strong>持续学习和适应性：</strong> 当前VLA模型通常无法在其初始训练阶段之后继续学习和适应新情况，使其在面对新颖或分布外场景时容易失效。</li>
<li><strong>安全性和故障检测：</strong> 在非结构化环境中部署VLA模型存在安全风险，缺乏检测和避免意外人类存在、以及故障检测和恢复机制。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文提出了以下几个未来研究方向：</p>
<ul>
<li><strong>多模态数据标准化：</strong> 统一传感器配置对于实现可扩展的多模态VLA系统至关重要，尤其是在触觉传感等领域。</li>
<li><strong>推理能力增强：</strong> 提高VLA系统在长周期任务中的推理能力，包括记忆保持、选择性关注关键信息以及时间抽象，以支持更有效的规划和决策。</li>
<li><strong>持续学习和在线适应：</strong> 开发能够持续学习和适应新环境的VLA模型，可能通过强化学习与人类反馈（RLHF）或认知发展启发的主动学习来实现。</li>
<li><strong>世界模型与真实世界部署：</strong> 利用学习到的世界模型进行更安全、更样本高效的RL微调，并结合sim-to-real技术，以克服真实世界探索的风险。</li>
<li><strong>安全性和故障恢复：</strong> 整合VLA模型与基于模型的控制方法，以实现预测性推理，提高安全关键情况下的可靠性。开发故障检测和自适应重规划策略，以应对真实世界环境中的意外故障。</li>
<li><strong>标准化评估：</strong> 建立更严格、可控的评估条件和统计分析方法，以确保VLA模型性能比较的有效性和可靠性。</li>
<li><strong>实际应用：</strong> 尽管VLA系统在医疗保健、辅助技术、工业自动化和自动驾驶等领域具有巨大潜力，但仍需提高其在鲁棒性和适应性方面达到人类水平的性能，以实现实际部署。</li>
</ul>
<hr />
<p>这篇论文为机器人领域的VLA研究提供了一个全面的路线图，不仅总结了现有技术，还明确指出了未来的挑战和机遇，对于希望将VLA模型应用于真实世界机器人系统的研究人员和工程师具有重要的指导意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.07077v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.07077v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08575v1'></a></p>
<h2 id="resplat-learning-recurrent-gaussian-splats"><a href="https://arxiv.org/abs/2510.08575v1">ReSplat: Learning Recurrent Gaussian Splats</a></h2>
<p><strong>Authors:</strong> Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While feed-forward Gaussian splatting models provide computational efficiency
and effectively handle sparse input settings, their performance is
fundamentally limited by the reliance on a single forward pass during
inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients. Our key insight is that the Gaussian splatting rendering error
serves as a rich feedback signal, guiding the recurrent network to learn
effective Gaussian updates. This feedback signal naturally adapts to unseen
data distributions at test time, enabling robust generalization. To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a <script type="math/tex">16 \times</script> subsampled space, producing <script type="math/tex">16 \times</script> fewer
Gaussians than previous per-pixel Gaussian models. This substantially reduces
computational overhead and allows for efficient Gaussian updates. Extensive
experiments across varying of input views (2, 8, 16), resolutions (<script type="math/tex">256 \times
256</script> to <script type="math/tex">540 \times 960</script>), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed. Our project
page is at https://haofeixu.github.io/resplat/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Haofei Xu等人撰写的论文“ReSplat: Learning Recurrent Gaussian Splats”的全面摘要。</p>
<hr />
<h3 id="resplat">ReSplat: 学习循环高斯溅射的全面摘要</h3>
<p><strong>1. 论文解决的主要问题或研究问题</strong></p>
<p>当前的前馈高斯溅射（Gaussian Splatting）模型虽然在计算效率和处理稀疏输入设置方面表现出色，但其性能受限于单次前向推理。这意味着它们难以在复杂场景或面对未见数据分布时保持高性能和泛化能力。传统的基于优化的方法（如3DGS）虽然能达到高质量结果，但计算成本高昂且耗时。因此，论文旨在解决如何在保持前馈模型效率的同时，通过迭代优化提升高斯溅射模型的重建质量和泛化能力，尤其是在稀疏视图和不同分辨率设置下。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>ReSplat模型引入了以下关键创新：</p>
<ul>
<li><strong>循环高斯溅射模型（Recurrent Gaussian Splatting）</strong>：ReSplat提出了一种前馈循环网络，通过迭代地细化3D高斯参数，而无需显式计算梯度。其核心思想是利用高斯溅射的渲染误差作为丰富的反馈信号，指导循环网络学习有效的高斯更新。这种反馈机制使其能够自然地适应测试时未见的数据分布，从而实现鲁棒的泛化。</li>
<li><strong>紧凑的初始化重建模型</strong>：为了启动循环过程并降低计算开销，ReSplat引入了一个在<script type="math/tex">16 \times</script>下采样空间中操作的紧凑重建模型。该模型生成的高斯数量比以往的每像素高斯模型少<script type="math/tex">16 \times</script>，显著减少了计算开销，并允许进行高效的高斯更新。</li>
<li><strong>渲染误差作为反馈信号</strong>：与依赖特征相关性或显式梯度计算的传统循环优化方法不同，ReSplat利用输入视图的渲染误差（在特征空间中计算）作为指导信号。通过全局注意力机制将渲染误差传播到3D高斯，使得每个高斯都能接收到来自所有渲染误差的信息，从而实现更有效的更新。</li>
<li><strong>分阶段训练策略</strong>：模型训练分为两个阶段：首先训练一个初始高斯重建模型以提供紧凑的初始化，然后冻结该模型并端到端地训练循环模型，使用渲染损失和指数递增的权重进行监督。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>ReSplat在多个实验设置下均取得了显著的成果：</p>
<ul>
<li><strong>性能提升与效率兼顾</strong>：在DL3DV数据集上，使用8个输入视图（512 × 960分辨率），ReSplat的PSNR提高了+2.7 dB，同时仅使用<script type="math/tex">1/16</script>的高斯数量，渲染速度快了<script type="math/tex">4 \times</script>。与基于优化的3DGS相比，ReSplat速度快了100倍。</li>
<li><strong>鲁棒的泛化能力</strong>：ReSplat在未见数据集（如RealEstate10K）和不同图像分辨率（从<script type="math/tex">256 \times 256</script>到<script type="math/tex">540 \times 960</script>）下表现出强大的泛化能力，优于以往的单步前馈模型。例如，在从<script type="math/tex">512 \times 960</script>泛化到<script type="math/tex">320 \times 640</script>时，PSNR提高了4dB。</li>
<li><strong>高斯数量和渲染速度优化</strong>：通过<script type="math/tex">16 \times</script>下采样空间中的高斯重建，ReSplat显著减少了高斯数量，并提高了渲染速度，同时保持了最先进的性能。</li>
<li><strong>快速收敛</strong>：循环模型在3次迭代后即可收敛，这得益于其快速收敛特性，使其在实际应用中更高效。</li>
</ul>
<p>这些结果表明，ReSplat成功地平衡了前馈方法的效率和迭代优化的适应性，为稀疏视图下的高质量3D重建和新颖视图合成提供了一种新颖且高效的解决方案。</p>
<p><strong>4. 论文中提及的局限性</strong></p>
<p>论文也坦诚地指出了ReSplat的局限性：</p>
<ul>
<li><strong>kNN注意力计算成本</strong>：当前模型依赖于基于kNN的点注意力机制，当高斯数量非常大（例如超过500K）时，会产生高昂的计算成本。</li>
<li><strong>迭代次数饱和</strong>：模型在3次迭代后性能趋于饱和。作者推测，在循环过程中固定高斯数量可能是潜在原因。</li>
<li><strong>自适应更新策略</strong>：目前模型使用固定的高斯数量进行更新，缺乏更自适应的更新策略。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>更高效的点注意力机制</strong>：探索更高效的基于点的注意力机制（如Wu等人2022, 2024）和稀疏结构（Ren等人2024），以进一步提高模型的可扩展性和效率。</li>
<li><strong>自适应高斯更新策略</strong>：设计更自适应的更新策略，例如在循环过程中动态调整高斯数量，以克服当前模型在固定迭代次数后性能饱和的问题。</li>
<li><strong>扩展测试时计算</strong>：探索如何进一步扩展测试时计算，以应对更复杂或更大规模的场景。</li>
</ul>
<hr />
<p>总而言之，ReSplat通过引入一个利用渲染误差作为反馈信号的循环前馈网络，并结合紧凑的初始化模型，成功地在保持高效率的同时，显著提升了3D高斯溅射模型的重建质量和泛化能力，为稀疏视图下的新颖视图合成开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients.</li>
<li>To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a <script type="math/tex">16 \times</script> subsampled space, producing <script type="math/tex">16 \times</script> fewer
Gaussians than previous per-pixel Gaussian models.</li>
<li>Extensive
experiments across varying of input views (2, 8, 16), resolutions (<script type="math/tex">256 \times
256</script> to <script type="math/tex">540 \times 960</script>), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08575v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08575v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08568v1'></a></p>
<h2 id="novaflow-zero-shot-manipulation-via-actionable-flow-from-generated-videos"><a href="https://arxiv.org/abs/2510.08568v1">NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos</a></h2>
<p><strong>Authors:</strong> Hongyu Li, Lingfeng Sun, Yafei Hu, Duy Ta, Jennifer Barry, George Konidaris, Jiahui Fu</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Hongyu Li等人撰写的论文“NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos”的全面摘要：</p>
<p><strong>论文摘要：NovaFlow：通过生成视频的可操作流实现零样本操作</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决机器人零样本执行新颖操作任务的核心挑战。现有方法通常假设任务在分布内或依赖于与机器人本体匹配的数据进行微调，这限制了跨平台的迁移能力。NovaFlow致力于开发一个无需任何演示即可将任务描述转换为目标机器人可操作计划的自主操作框架。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
NovaFlow框架的核心创新在于其模块化设计和对“3D可操作对象流”这一中间表示的利用。具体贡献包括：
*   <strong>解耦任务理解与低级控制：</strong> NovaFlow将任务理解（通过视频生成模型）与机器人低级控制（通过感知模块和轨迹优化）分离，从而实现了跨机器人本体的自然迁移。
*   <strong>可操作3D对象流的蒸馏：</strong> 框架首先利用视频生成模型（如Wan或Veo）根据任务描述合成一个 plausible 的任务解决视频。然后，通过一系列预训练的感知模块（包括单目深度估计、3D点跟踪和对象接地），将生成的2D视频蒸馏成3D可操作对象流。
*   <strong>处理不同对象类型：</strong>
    *   <strong>刚性对象：</strong> 从对象流中计算相对姿态，并通过抓取提议和轨迹优化将其转化为机器人动作。
    *   <strong>可变形对象：</strong> 对象流作为基于粒子动力学模型的模型预测控制（MPC）的跟踪目标，实现对可变形对象的规划。
*   <strong>零样本和无演示：</strong> 整个流程无需任何机器人特定数据或任务特定训练，实现了真正的零样本操作。
*   <strong>拒绝采样：</strong> 为了过滤掉视频生成模型可能引入的幻觉和不合理运动，NovaFlow采用拒绝采样步骤，利用VLM评估并选择最合理的生成流。</p>
<p><strong>3. 主要结果及其意义：</strong>
NovaFlow在各种真实世界操作任务中取得了显著的成功，包括刚性、关节式和可变形对象的操纵，使用了桌面Franka机械臂和Spot四足移动机器人。
*   <strong>优于基线：</strong> NovaFlow在零样本方法中实现了最高的成功率，并且超越了需要10-30次演示训练的数据依赖型模仿学习策略（如Diffusion Policy和Inverse Dynamics Model）。
*   <strong>跨本体泛化：</strong> 实验证明了NovaFlow在不同机器人本体和对象类型上的泛化能力，无需特定于本体的训练。
*   <strong>3D对象流的重要性：</strong> 论文强调了可操作3D对象流作为中间表示的关键作用，它使得框架能够理解和执行复杂的对象运动。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>物理执行瓶颈：</strong> 失败分析揭示，主要的瓶颈在于物理执行阶段，尤其是在抓取和处理意外动力学方面。这表明开放循环计划与真实世界交互的复杂性之间存在差距。
*   <strong>视频生成模型的局限性：</strong> 视频生成模型有时会产生不符合物理规律、缺乏3D一致性或违反用户指令的内容，尽管拒绝采样有所缓解，但未能完全消除。
*   <strong>跟踪失败：</strong> 3D点跟踪的精度不足，通常由无纹理表面、严重遮挡或视频模型继承的累积不一致性引起。
*   <strong>抓取失败：</strong> 机器人未能正确抓取对象（例如，方法不当、抓取失败和滑移）。
*   <strong>执行失败：</strong> 轨迹执行过程中出现的错误。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>闭环反馈系统：</strong> 未来的工作可以集中于整合一个闭环反馈系统，利用环境的实时反馈来改进或重新规划生成的流，从而使系统更具适应性和鲁棒性，以应对不可预见的挑战。
*   <strong>改进视频生成和3D提升模块：</strong> 论文指出视频生成和3D提升模块是耗时最多的部分，暗示未来可以通过使用更快的模型来提高效率。
*   <strong>增强VLM的推理能力：</strong> 进一步利用VLM在拒绝采样中对运动进行更深入的推理，以更好地过滤不合理的生成视频。</p>
<p>总而言之，NovaFlow提出了一种新颖且高效的零样本机器人操作范式，通过将大规模预训练视频生成模型的常识性任务理解能力转化为可操作的3D对象流，成功克服了传统机器人学习中的数据瓶颈和泛化限制。尽管存在一些物理执行方面的挑战，但其模块化和零样本能力为通用机器人操作的未来发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics.</li>
<li>We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08568v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08568v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08567v1'></a></p>
<h2 id="matrix-multimodal-agent-tuning-for-robust-tool-use-reasoning"><a href="https://arxiv.org/abs/2510.08567v1">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></h2>
<p><strong>Authors:</strong> Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Vision language models (VLMs) are increasingly deployed as controllers with
access to external tools for complex reasoning and decision-making, yet their
effectiveness remains limited by the scarcity of high-quality multimodal
trajectories and the cost of manual annotation. We address this challenge with
a vision-centric agent tuning framework that automatically synthesizes
multimodal trajectories, generates step-wise preference pairs, and trains a VLM
controller for robust tool-use reasoning. Our pipeline first constructs
M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified
trajectories, enabling imitation-based trajectory tuning. Building on this, we
develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool
reasoning. To achieve finer alignment, we further introduce Pref-X, a set of
11K automatically generated preference pairs, and optimize MATRIX on it via
step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use. Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Tajamul Ashraf等人撰写的论文“MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决视觉语言模型（VLMs）在作为控制器访问外部工具进行复杂推理和决策时所面临的挑战。具体来说，现有VLMs的有效性受限于高质量多模态轨迹的稀缺性以及手动标注的高昂成本，这限制了它们在开放式多模态任务中的鲁棒性和泛化能力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了一个名为 <strong>MATRIX</strong> 的两阶段视觉中心代理微调框架，用于实现鲁棒的工具使用推理：
*   <strong>M-TRACE 数据集：</strong> 首先构建了一个大规模的28.5K多模态任务数据集，包含177K条经过验证的轨迹。这些轨迹是通过自动化合成和验证生成的，用于基于模仿的轨迹微调。
*   <strong>MATRIX Agent：</strong> 在M-TRACE上对一个VLM控制器进行微调，以实现分步工具推理。
*   <strong>Pref-X 数据集：</strong> 为了实现更精细的对齐，进一步引入了11K组自动生成的偏好对。
*   <strong>分步偏好学习：</strong> 通过分步偏好学习（Direct Preference Optimization, DPO）在Pref-X上优化MATRIX，以提升决策质量和工具使用对齐。</p>
<p><strong>3. 主要结果及其意义：</strong>
MATRIX在三个基准测试（Agent-X、GTA和GAIA）上均持续超越了开源和闭源的VLMs。这表明MATRIX能够实现可扩展且有效的多模态工具使用。具体来说，在Agent-X上，MATRIX在工具准确性、忠实度和语义准确性方面取得了最高分，相对于Qwen2-VL-7B有显著提升。在GTA和GAIA上，MATRIX也表现出优越的性能，验证了其分步偏好优化在多模态工具使用中的有效性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
尽管MATRIX是有效的，但仍存在一些局限性：
*   目前，它仅在查询/任务级别上对多模态信号进行接地。
*   依赖于基于提示的验证器，在分布偏移下可能会出现问题。
*   在没有轨迹级别信用分配的情况下优化分步偏好。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的工作将通过探索自适应验证器、连续多模态接地和分层偏好建模来解决上述局限性。</p>
<p>总而言之，这篇论文通过引入一个结合轨迹监督和分步偏好优化的创新框架，显著提升了VLMs在复杂多模态工具使用推理方面的能力，为构建更鲁棒、可扩展的多模态代理提供了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use.</li>
<li>Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08567v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08567v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08566v1'></a></p>
<h2 id="d2gs-depth-and-density-guided-gaussian-splatting-for-stable-and-accurate-sparse-view-reconstruction"><a href="https://arxiv.org/abs/2510.08566v1">D<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction</a></h2>
<p><strong>Authors:</strong> Meixi Song, Xin Lin, Dizhe Zhang, Haodong Li, Xiangtai Li, Bo Du, Lu Qi</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in 3D Gaussian Splatting (3DGS) enable real-time,
high-fidelity novel view synthesis (NVS) with explicit 3D representations.
However, performance degradation and instability remain significant under
sparse-view conditions. In this work, we identify two key failure modes under
sparse-view conditions: overfitting in regions with excessive Gaussian density
near the camera, and underfitting in distant areas with insufficient Gaussian
coverage. To address these challenges, we propose a unified framework D<script type="math/tex">^2</script>GS,
comprising two key components: a Depth-and-Density Guided Dropout strategy that
suppresses overfitting by adaptively masking redundant Gaussians based on
density and depth, and a Distance-Aware Fidelity Enhancement module that
improves reconstruction quality in under-fitted far-field areas through
targeted supervision. Moreover, we introduce a new evaluation metric to
quantify the stability of learned Gaussian distributions, providing insights
into the robustness of the sparse-view 3DGS. Extensive experiments on multiple
datasets demonstrate that our method significantly improves both visual quality
and robustness under sparse view conditions. The project page can be found at:
https://insta360-research-team.github.io/DDGS-website/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Meixi Song等人撰写的论文“D<script type="math/tex">^2</script>GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction”的全面摘要。</p>
<hr />
<p><strong>论文摘要：D<script type="math/tex">^2</script>GS: 用于稳定和准确稀疏视图重建的深度与密度引导高斯泼溅</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决3D高斯泼溅（3DGS）在稀疏视图条件下进行新颖视图合成（NVS）时面临的性能下降和不稳定性问题。具体来说，作者识别出两个关键的失败模式：一是相机附近高斯密度过高导致的过拟合，二是远场区域高斯覆盖不足导致的欠拟合。现有的统一dropout策略无法有效解决这些问题，甚至可能损害重建质量。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
为了解决上述挑战，论文提出了一个统一的框架D<script type="math/tex">^2</script>GS，包含两个核心组件：
*   <strong>深度与密度引导Dropout (Depth-and-Density Guided Dropout, DD-Drop) 策略：</strong> 该策略通过自适应地根据高斯点的密度和深度来遮蔽冗余高斯点，从而抑制过拟合。DD-Drop采用局部连续和全局离散机制，为每个高斯点分配一个基于其深度（到相机的欧氏距离）和局部密度（通过k近邻估计）的dropout分数。此外，它还引入了基于深度的分层策略，对近场、中场和远场区域应用不同的衰减因子，以实现更精细的控制。这种概率性和渐进式的dropout机制避免了传统硬性dropout的弊端。
*   <strong>距离感知保真度增强 (Distance-Aware Fidelity Enhancement, DAFE) 模块：</strong> 该模块通过有针对性的监督，提高欠拟合远场区域的重建质量。DAFE利用单目深度估计模型生成深度图，并构建二值掩码来分离近场和远场区域。然后，该掩码被用于调制训练目标，放大远场区域的监督信号，促使模型在该区域生成更密集的高斯点，从而捕捉更精细的细节。
*   <strong>新颖的评估指标——模型间鲁棒性 (Inter-Model Robustness, IMR)：</strong> 为了量化学习到的高斯分布的稳定性，论文引入了IMR指标。该指标基于2-Wasserstein距离和最优传输理论，衡量独立训练模型之间高斯分布的一致性，从而评估模型对初始化和训练噪声的鲁棒性。</p>
<p><strong>3. 主要结果及其重要性：</strong>
论文在LLFF和Mip-NeRF360等多个数据集上进行了广泛的实验，结果表明D<script type="math/tex">^2</script>GS在稀疏视图条件下显著提升了视觉质量和鲁棒性。
*   <strong>视觉质量提升：</strong> D<script type="math/tex">^2</script>GS在PSNR、SSIM、LPIPS和AVGE等指标上均取得了最先进的性能，尤其是在1/8和1/4分辨率的LLFF数据集以及24视图的MipNeRF360数据集上。定性结果也显示，D<script type="math/tex">^2</script>GS能生成更清晰的细节，减少伪影，并保留更多高频结构。
*   <strong>鲁棒性增强：</strong> IMR指标的评估结果显示，D<script type="math/tex">^2</script>GS在3视图和6视图设置下均实现了最低的IMR值，表明其在不同运行中能产生更稳定和一致的高斯重建。
*   <strong>消融研究：</strong> 消融实验验证了DD-Drop和DAFE模块中各个组件的有效性，包括深度和密度分数、深度分层以及DAFE损失的权重，证实了所有组件都对整体性能有互补贡献。</p>
<p><strong>4. 论文中提及的局限性：</strong>
尽管D<script type="math/tex">^2</script>GS在稀疏视图设置下表现出色，但仍存在改进空间：
*   DD-Drop策略依赖于手工设定的深度阈值和固定权重系数，可能无法完全捕捉复杂的场景特定先验。
*   IMR鲁棒性指标侧重于模型间一致性，但尚未考虑动态视图合成下的感知稳定性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：
*   探索自适应dropout调度和可学习的监督掩码，以更好地适应不同场景。
*   开发时间感知的鲁棒性指标，以评估动态视图合成下的稳定性。</p>
<hr />
<p>总而言之，D<script type="math/tex">^2</script>GS通过创新的深度与密度引导dropout和距离感知保真度增强机制，有效解决了稀疏视图3DGS中的过拟合和欠拟合问题。同时，引入的模型间鲁棒性指标为评估稀疏视图3DGS的稳定性提供了新的视角。这项工作为稀疏视图新颖视图合成的稳定性和准确性树立了新的基准。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Recent advances in 3D Gaussian Splatting (3DGS) enable real-time,
high-fidelity novel view synthesis (NVS) with explicit 3D representations.</li>
<li>To address these challenges, we propose a unified framework D<script type="math/tex">^2</script>GS,
comprising two key components: a Depth-and-Density Guided Dropout strategy that
suppresses overfitting by adaptively masking redundant Gaussians based on
density and depth, and a Distance-Aware Fidelity Enhancement module that
improves reconstruction quality in under-fitted far-field areas through
targeted supervision.</li>
<li>Moreover, we introduce a new evaluation metric to
quantify the stability of learned Gaussian distributions, providing insights
into the robustness of the sparse-view 3DGS.</li>
<li>Extensive experiments on multiple
datasets demonstrate that our method significantly improves both visual quality
and robustness under sparse view conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08566v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08566v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08564v1'></a></p>
<h2 id="how-to-teach-large-multimodal-models-new-skills"><a href="https://arxiv.org/abs/2510.08564v1">How to Teach Large Multimodal Models New Skills</a></h2>
<p><strong>Authors:</strong> Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&amp;Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zhen Zhu等人撰写的论文“How to Teach Large Multimodal Models New Skills”的全面摘要。</p>
<hr />
<p><strong>论文题目：</strong> 如何教授大型多模态模型新技能？
<strong>作者：</strong> Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu &amp; Derek Hoiem
<strong>机构：</strong> 伊利诺伊大学厄巴纳-香槟分校</p>
<h3 id="_1">全面摘要</h3>
<p>这篇论文深入探讨了大型多模态模型（LMMs）在学习新技能时如何避免遗忘原有能力的关键问题。研究发现，通过对模型特定组件进行选择性微调，可以在获得强大新技能的同时，有效限制对现有能力的损害。</p>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决大型多模态模型（LMMs）在学习新技能时如何避免“灾难性遗忘”（catastrophic forgetting）的问题，即在狭窄的微调后，模型在原有任务上的性能显著下降。核心问题是：如何在不抹除LMMs原有能力的前提下，教授它们新技能？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>系统性遗忘行为分析：</strong> 论文通过在五个目标技能上进行顺序微调，并监测八个通用基准任务上的性能，系统地研究了LMMs的遗忘行为。研究发现，表观遗忘在后期阶段可以部分恢复。
*   <strong>输出token分布漂移的识别：</strong> 论文将遗忘行为追溯到输出token分布的可测量漂移。通过一个简单的“计数偏差探测器”（counting-bias probe），发现这种漂移与遗忘程度呈正相关。
*   <strong>鲁棒的微调策略：</strong> 基于对输出分布漂移的理解，论文提出了两种简单而鲁棒的微调策略，能够在限制漂移的同时实现强大的学习：
    *   <strong>仅更新自注意力投影层（SA Proj.）：</strong> 这种方法在语言模型中仅调整自注意力投影层，实现了显著的学习效果，同时遗忘极小。
    *   <strong>仅更新MLP的Gate&amp;Up层并冻结Down投影：</strong> 这种方法在保持强大目标学习能力的同时，有效限制了遗忘。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>全模型微调的局限性：</strong> 结果显示，全模型微调虽然能带来最大的目标任务学习增益，但也会导致最严重的遗忘。
*   <strong>视觉侧更新的弱效性：</strong> 仅更新视觉编码器或投影器带来的学习增益很小，并且可能损害模型的通用能力。
*   <strong>语言模型微调的重要性与稳定性：</strong> 语言模型（LLM）的微调对于学习新任务至关重要。在LLM内部，SA Proj.和MLP (Gate&amp;Up) 的选择性微调表现出最佳的学习-稳定性权衡，在不同模型家族和任务中均能保持强大的目标增益，同时显著限制了遗忘。
*   <strong>遗忘与输出分布漂移的关联：</strong> 论文通过计数偏差探测器证实，遗忘很大程度上是输出分布漂移的表现。限制这种漂移的方法（如知识蒸馏或冻结MLP的Down投影）能有效缓解遗忘。
*   <strong>遗忘恢复现象：</strong> 论文观察到，即使在狭窄的微调后，模型在原有任务上的性能下降，但在学习后续专业任务时，这些“遗忘”的知识可以部分恢复，表明信息并非永久丢失，而是暂时不可访问。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>资源限制：</strong> 由于资源有限，论文未能探索替代架构、更长的任务序列、更大规模的模型以及其他模态（如音频）。
*   <strong>未深入探讨的更广泛问题：</strong> 隐私泄露、安全性和社会影响等更广泛的问题有待未来进一步研究。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   探索更复杂的架构和更长的任务序列，以进一步验证和优化所提出的微调策略。
*   将研究扩展到更大规模的LMMs和更多模态（如音频），以测试这些方法的通用性和可扩展性。
*   深入研究LMMs在持续学习中的隐私、安全和社会影响等伦理问题。
*   进一步探索遗忘恢复的机制，以及如何利用这种现象来设计更有效的持续学习策略。</p>
<hr />
<p>这篇论文通过对LMMs微调机制的深入分析，为如何在不牺牲原有能力的前提下教授模型新技能提供了实用的指导和理论见解。其提出的选择性微调策略，特别是针对自注意力投影层和MLP Gate&amp;Up层的更新，为LMMs的持续学习和适应性发展开辟了新的途径，有助于降低模型适应的环境和经济成本。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities?</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08564v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08564v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08562v1'></a></p>
<h2 id="resad-normalized-residual-trajectory-modeling-for-end-to-end-autonomous-driving"><a href="https://arxiv.org/abs/2510.08562v1">ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Zhiyu Zheng, Shaoyu Chen, Haoran Yin, Xinbang Zhang, Jialv Zou, Xinggang Wang, Qian Zhang, Lefei Zhang</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>End-to-end autonomous driving (E2EAD) systems, which learn to predict future
trajectories directly from sensor data, are fundamentally challenged by the
inherent spatio-temporal imbalance of trajectory data. This imbalance creates a
significant optimization burden, causing models to learn spurious correlations
instead of causal inference, while also prioritizing uncertain, distant
predictions, thereby compromising immediate safety. To address these issues, we
propose ResAD, a novel Normalized Residual Trajectory Modeling framework.
Instead of predicting the future trajectory directly, our approach reframes the
learning task to predict the residual deviation from a deterministic inertial
reference. The inertial reference serves as a counterfactual, forcing the model
to move beyond simple pattern recognition and instead identify the underlying
causal factors (e.g., traffic rules, obstacles) that necessitate deviations
from a default, inertially-guided path. To deal with the optimization imbalance
caused by uncertain, long-term horizons, ResAD further incorporates Point-wise
Normalization of the predicted residual. It re-weights the optimization
objective, preventing large-magnitude errors associated with distant, uncertain
waypoints from dominating the learning signal. Extensive experiments validate
the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a
state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two
denoising steps, demonstrating that our approach significantly simplifies the
learning task and improves model performance. The code will be released to
facilitate further research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zhiyu Zheng等人撰写的论文“ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving”的全面摘要。</p>
<hr />
<p><strong>论文摘要：ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
端到端自动驾驶（E2EAD）系统在直接从传感器数据预测未来轨迹时面临核心挑战：轨迹数据固有的时空不平衡。这种不平衡导致模型倾向于学习虚假关联而非因果推理，并优先处理不确定、远距离的预测，从而损害即时安全性。具体来说，这表现为“因果混淆”和“规划视野困境”，使得模型难以理解驾驶行为的根本原因，并被远期、不确定的误差主导优化过程。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为解决上述问题，论文提出了ResAD，一个新颖的<strong>归一化残差轨迹建模框架</strong>。其核心创新点包括：</p>
<ul>
<li><strong>残差轨迹建模（Trajectory Residual Modeling）：</strong> ResAD不直接预测未来轨迹，而是将学习任务重新定义为预测相对于确定性惯性参考的残差偏差。惯性参考（通过车辆当前状态的恒定速度模型外推得到）作为反事实基线，迫使模型超越简单的模式识别，转而识别导致偏离默认惯性路径的潜在因果因素（如交通规则、障碍物）。这使得模型能够学习“为什么必须改变轨迹”，而非“未来轨迹是什么”。</li>
<li><strong>点式残差归一化（Point-wise Residual Normalization, PRNorm）：</strong> 为解决不确定、长期视野导致的优化不平衡问题，ResAD进一步引入了对预测残差的点式归一化。这通过重新加权优化目标，防止与远距离、不确定路点相关的大幅度误差主导学习信号，确保数值上虽小但关键的近场调整也能被有效捕获。</li>
<li><strong>惯性参考扰动（Inertia Reference Perturbation）：</strong> 通过对初始速度进行随机扰动，生成一组不同的惯性参考。这不仅增强了模型对传感器噪声的鲁棒性，还通过生成一系列意图假设，实现了多模态轨迹预测，从而产生与上下文相关的多样化路径，避免了传统方法中固定词汇表的低效和限制。</li>
<li><strong>多模态轨迹排序器（Multimodal Trajectory Ranker）：</strong> 借鉴现有工作，ResAD开发了一个轨迹排序器，用于从多个模态中选择最优轨迹，通过Transformer与感知表示交互，并预测各项指标得分，以蒸馏规划器和真值路点的知识。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
广泛的实验验证了ResAD框架的有效性：</p>
<ul>
<li><strong>最先进性能：</strong> 在NAVSIM基准测试上，ResAD使用仅两步去噪的香草扩散策略，在NAVSIM v1上实现了88.6的PDMS（Planning Driving Metric Score）最先进性能，在更具挑战性的NAVSIM v2上实现了85.5的EPDMS（Extended PDMS），超越了现有方法。</li>
<li><strong>简化学习任务和提升性能：</strong> 结果表明，ResAD显著简化了学习任务，并提高了模型性能。特别是在DAC（Drivable Area Compliance）和EP（Ego Progress）等指标上表现出色，表明模型能更好地遵守车道边界、可行驶区域，并更有效地完成路线。</li>
<li><strong>泛化能力：</strong> 在Transfuser（基于MLP）和TransfuserDP（基于扩散）等异构规划模型上的实验表明，ResAD的归一化残差轨迹建模方法具有良好的泛化能力，能显著提升轨迹质量，提高E2EAD系统的安全性和可靠性。</li>
<li><strong>训练效率：</strong> PRNorm不仅提升了最终性能，还通过加速模型收敛，显著提高了训练效率。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中未明确提及当前ResAD框架的局限性。然而，作为一种基于扩散模型的方法，其计算成本和推理速度（尽管论文中提到仅两步去噪）可能仍是实际部署中需要考虑的因素。此外，虽然惯性参考扰动实现了多模态，但其生成轨迹的“多样性”和“覆盖范围”是否能完全涵盖所有极端或罕见驾驶场景，仍有待进一步探讨。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文明确指出，代码将发布以促进进一步研究，这本身就暗示了社区可以基于此进行扩展。潜在的未来研究方向可能包括：</p>
<ul>
<li><strong>更复杂的惯性参考模型：</strong> 探索除了恒定速度模型之外，更复杂的物理模型或预测模型作为惯性参考，以提供更精确的基线。</li>
<li><strong>自适应扰动策略：</strong> 研究更智能、上下文感知的惯性参考扰动策略，以生成更具相关性和多样性的多模态轨迹。</li>
<li><strong>与强化学习的结合：</strong> 将残差建模与强化学习结合，使模型能够通过与环境的交互，自主学习更优的残差预测策略。</li>
<li><strong>实时性能优化：</strong> 进一步优化扩散模型的推理效率，使其更适用于对延迟敏感的实时自动驾驶系统。</li>
<li><strong>可解释性增强：</strong> 深入研究残差建模如何提升模型的可解释性，并开发新的可视化工具来展示模型学习到的因果因素。</li>
<li><strong>极端场景处理：</strong> 评估和改进ResAD在极端或罕见驾驶场景下的性能，确保其在所有条件下都能安全可靠地运行。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we
propose ResAD, a novel Normalized Residual Trajectory Modeling framework.</li>
<li>Instead of predicting the future trajectory directly, our approach reframes the
learning task to predict the residual deviation from a deterministic inertial
reference.</li>
<li>On the NAVSIM benchmark, ResAD achieves a
state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two
denoising steps, demonstrating that our approach significantly simplifies the
learning task and improves model performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08562v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08562v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08555v1'></a></p>
<h2 id="videocanvas-unified-video-completion-from-arbitrary-spatiotemporal-patches-via-in-context-conditioning"><a href="https://arxiv.org/abs/2510.08555v1">VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</a></h2>
<p><strong>Authors:</strong> Minghong Cai, Qiulin Wang, Zongli Ye, Wenze Liu, Quande Liu, Weicai Ye, Xintao Wang, Pengfei Wan, Kun Gai, Xiangyu Yue</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce the task of arbitrary spatio-temporal video completion, where a
video is generated from arbitrary, user-specified patches placed at any spatial
location and timestamp, akin to painting on a video canvas. This flexible
formulation naturally unifies many existing controllable video generation
tasks--including first-frame image-to-video, inpainting, extension, and
interpolation--under a single, cohesive paradigm. Realizing this vision,
however, faces a fundamental obstacle in modern latent video diffusion models:
the temporal ambiguity introduced by causal VAEs, where multiple pixel frames
are compressed into a single latent representation, making precise frame-level
conditioning structurally difficult. We address this challenge with
VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)
paradigm to this fine-grained control task with zero new parameters. We propose
a hybrid conditioning strategy that decouples spatial and temporal control:
spatial placement is handled via zero-padding, while temporal alignment is
achieved through Temporal RoPE Interpolation, which assigns each condition a
continuous fractional position within the latent sequence. This resolves the
VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen
backbone. To evaluate this new capability, we develop VideoCanvasBench, the
first benchmark for arbitrary spatio-temporal video completion, covering both
intra-scene fidelity and inter-scene creativity. Experiments demonstrate that
VideoCanvas significantly outperforms existing conditioning paradigms,
establishing a new state of the art in flexible and unified video generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Minghong Cai等人撰写的论文“VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning”的全面摘要。</p>
<hr />
<h3 id="videocanvas-unified-video-completion-from-arbitrary-spatiotemporal-patches-via-in-context-conditioning_1">论文摘要：VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决“任意时空视频补全”这一核心问题。传统的视频生成任务（如首帧图像到视频、视频修复、扩展和插值）通常是孤立的、任务特定的，缺乏统一的框架。现代潜在视频扩散模型面临一个根本性障碍：因因果变分自编码器（VAEs）将多个像素帧压缩成单一潜在表示，导致时间模糊性，使得精确的帧级条件控制变得结构性困难。作者希望实现一个灵活的视频生成范式，允许用户在视频画布上的任意时空位置放置任意指定补丁，并生成连贯、高质量的视频。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>统一的任意时空视频补全任务：</strong> 论文首次形式化并引入了“任意时空视频补全”任务，将多种现有和新兴的视频生成场景（如Any-Timestep-Patch/Image-to-Video、In/Outpainting、Camera Control和Cross-scene Video Transitions）统一到一个连贯的范式下。
*   <strong>VideoCanvas框架：</strong> 提出了一个新颖的框架VideoCanvas，首次将“上下文条件（In-Context Conditioning, ICC）”范式应用于任意时空视频补全任务，且无需引入新的参数。
*   <strong>混合条件策略：</strong> 为了解决因果VAEs的时间模糊性和空间不规则性，提出了一种混合条件策略，解耦了空间和时间控制：
    *   <strong>空间放置：</strong> 通过零填充（zero-padding）处理，将条件补丁放置在完整帧画布上，然后独立编码。
    *   <strong>时间对齐：</strong> 通过“时间RoPE插值（Temporal RoPE Interpolation）”实现，为每个条件分配潜在序列中的连续分数位置，从而解决VAEs的时间模糊性，并在冻结的骨干网络上实现像素帧级的精确控制。
*   <strong>VideoCanvasBench基准：</strong> 开发了首个专门用于任意时空视频补全的综合基准，评估模型在场景内保真度（intra-scene fidelity）和场景间创造力（inter-scene creativity）两方面的性能。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>显著优于现有范式：</strong> 实验证明，VideoCanvas在VideoCanvasBench上显著优于现有的条件范式（如潜在替换、通道拼接），在各种视频补全任务中建立了新的技术水平。
*   <strong>解决时间模糊性：</strong> 时间RoPE插值策略成功解决了因果VAEs的时间模糊性，实现了精确的像素帧对齐，并在目标帧处达到PSNR峰值，同时保持了高保真度。
*   <strong>高质量和连贯性：</strong> 定性结果显示，VideoCanvas能够生成平滑、高质量的视频，保持物体身份和结构一致性，避免了基线方法中常见的静态重复、不自然过渡或语义腐败问题。
*   <strong>零填充的鲁棒性：</strong> 实验表明，混合因果视频VAEs对空间零填充具有良好的鲁棒性，而对时间零填充则非常脆弱，这验证了作者解耦空间和时间处理的必要性。
*   <strong>多功能应用：</strong> VideoCanvas展现了强大的新兴能力，包括灵活的时间控制（AnyI2V）、任意时空控制（AnyP2V）、创意视频过渡、长时视频扩展和循环、统一视频绘画和相机控制等。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>密集输入计算成本：</strong> 尽管独立帧编码对于稀疏条件非常有效，但对于密集输入（即条件帧数量较多）会带来计算上的权衡，导致推理时间随条件帧数量的增加而略微增加。
*   <strong>预训练VAE的兼容性：</strong> 大多数领先的视频基础模型使用的因果VAEs并未在零填充的时间数据上进行预训练，这使得它们与朴素的零填充方法不兼容，会导致分布偏移，需要昂贵的VAE和DiT骨干网络重新训练。VideoCanvas通过其混合策略规避了这一问题，但未来基础模型若能预训练在零填充数据上，将是数据驱动范式的补充。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>混合机制探索：</strong> 未来工作可以探索结合VideoCanvas的精细对齐与更高效的token剪枝策略的混合机制，以应对密集条件序列的计算成本问题。
*   <strong>数据驱动范式：</strong> 鼓励未来基础模型在预训练时纳入零填充数据，以使数据驱动范式与VideoCanvas的模型中心化框架互补，进一步提升灵活和统一视频合成的能力。</p>
<hr />
<p>这篇论文通过引入一个统一的框架和创新的混合条件策略，为视频生成领域带来了显著的进步，特别是在解决因果VAEs的时间模糊性方面，为实现更灵活、更精细的视频内容创作奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce the task of arbitrary spatio-temporal video completion, where a
video is generated from arbitrary, user-specified patches placed at any spatial
location and timestamp, akin to painting on a video canvas.</li>
<li>We address this challenge with
VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)
paradigm to this fine-grained control task with zero new parameters.</li>
<li>We propose
a hybrid conditioning strategy that decouples spatial and temporal control:
spatial placement is handled via zero-padding, while temporal alignment is
achieved through Temporal RoPE Interpolation, which assigns each condition a
continuous fractional position within the latent sequence.</li>
<li>To evaluate this new capability, we develop VideoCanvasBench, the
first benchmark for arbitrary spatio-temporal video completion, covering both
intra-scene fidelity and inter-scene creativity.</li>
<li>Experiments demonstrate that
VideoCanvas significantly outperforms existing conditioning paradigms,
establishing a new state of the art in flexible and unified video generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08555v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08555v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.08551v1'></a></p>
<h2 id="artdeco-towards-efficient-and-high-fidelity-on-the-fly-3d-reconstruction-with-structured-scene-representation"><a href="https://arxiv.org/abs/2510.08551v1">ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</a></h2>
<p><strong>Authors:</strong> Guanghao Li, Kerui Ren, Linning Xu, Zhewen Zheng, Changjian Jiang, Xin Gao, Bo Dai, Jian Pu, Mulin Yu, Jiangmiao Pang</p>
<p><strong>Published:</strong> 2025-10-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>On-the-fly 3D reconstruction from monocular image sequences is a
long-standing challenge in computer vision, critical for applications such as
real-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:
per-scene optimization yields high fidelity but is computationally expensive,
whereas feed-forward foundation models enable real-time inference but struggle
with accuracy and robustness. In this work, we propose ARTDECO, a unified
framework that combines the efficiency of feed-forward models with the
reliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose
estimation and point prediction, coupled with a Gaussian decoder that
transforms multi-scale features into structured 3D Gaussians. To sustain both
fidelity and efficiency at scale, we design a hierarchical Gaussian
representation with a LoD-aware rendering strategy, which improves rendering
fidelity while reducing redundancy. Experiments on eight diverse indoor and
outdoor benchmarks show that ARTDECO delivers interactive performance
comparable to SLAM, robustness similar to feed-forward systems, and
reconstruction quality close to per-scene optimization, providing a practical
path toward on-the-fly digitization of real-world environments with both
accurate geometry and high visual fidelity. Explore more demos on our project
page: https://city-super.github.io/artdeco/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Guanghao Li等人撰写的论文“ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：ARTDECO：面向结构化场景表示的高效高保真实时3D重建</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决单目图像序列实时3D重建领域长期存在的挑战。现有方法面临一个主要权衡：基于场景优化的方法能提供高保真度但计算成本高昂，而前馈基础模型虽然能实现实时推理，但在准确性和鲁棒性方面表现不足。因此，研究问题是如何开发一个统一的框架，能够结合前馈模型的效率和基于SLAM（同步定位与建图）管线的可靠性，实现高保真、高效率且鲁棒的实时3D重建，尤其是在大规模和多样化场景中。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
ARTDECO（Accurate localization, Robust reconstruction, and Decoder-based rendering）提出了以下关键创新：
*   <strong>统一框架设计：</strong> ARTDECO是一个集定位、重建和渲染于一体的集成系统，旨在在各种环境中鲁棒地运行。它结合了前馈模型的效率和SLAM管线的可靠性。
*   <strong>融合3D基础模型：</strong> 论文将3D基础模型作为模块化组件集成到姿态估计、回环检测和稠密点预测中。这种集成显著提高了定位精度和建图稳定性，同时保持了效率。
*   <strong>分层半隐式高斯表示：</strong> ARTDECO引入了一种分层高斯表示，并结合了LoD（细节层次）感知的渲染策略。这种设计通过将多尺度特征转换为结构化3D高斯，提高了渲染保真度，同时减少了冗余，对于大规模、可导航环境至关重要。
*   <strong>混合前端-后端架构：</strong> 前端负责估计相对姿态并对帧进行分类（普通帧、映射帧、关键帧），后端通过回环检测和全局束调整（BA）精炼关键帧姿态，映射模块则增量优化3D高斯。</p>
<p><strong>3. 主要结果及其意义：</strong>
ARTDECO在八个多样化的室内外基准测试上进行了广泛实验，结果显示：
*   <strong>交互式性能：</strong> ARTDECO实现了与SLAM系统相当的交互式性能。
*   <strong>鲁棒性：</strong> 其鲁棒性与前馈系统相似，能够应对复杂和多样化的环境，包括运动模糊和噪声。
*   <strong>重建质量：</strong> 重建质量接近于基于场景优化的方法，在PSNR、SSIM和LPIPS等指标上表现出色，尤其是在ScanNet++、TUM和VR-NeRF等挑战性数据集上。
*   <strong>定位精度：</strong> 通过回环检测和协方差矩阵滤波，ARTDECO在多尺度室内外数据集上实现了显著更高的定位精度，优于其他3DGS-based SLAM方法和非3DGS SLAM方法。
*   <strong>效率：</strong> ARTDECO的运行速度快于除OnTheFly-NVS之外的所有3DGS-based方法，其额外的姿态估计时间成本被其卓越的姿态精度所抵消。</p>
<p>这些结果表明，ARTDECO为实时数字化真实世界环境提供了一条实用途径，兼具准确的几何形状和高视觉保真度。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>对前馈3D基础模型的依赖：</strong> ARTDECO部分依赖于前馈3D基础模型进行对应和几何预测。尽管这些模型能实现快速和可扩展的推理，但在噪声、模糊或光照变化下，以及当输入超出训练分布时，其鲁棒性会降低。
*   <strong>环境假设：</strong> 系统假设光照一致且视差充足。违反这些假设（如低纹理表面、重复结构或近乎退化的轨迹）可能导致漂移或伪影。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出了以下未来研究方向：
*   <strong>不确定性估计：</strong> 整合不确定性估计，以提高系统在现实世界环境中的泛化能力和可靠性。
*   <strong>自适应模型选择：</strong> 引入自适应模型选择机制，以更好地处理不同场景和输入条件。
*   <strong>更强的先验：</strong> 探索更强的先验知识，以进一步提高系统的鲁棒性和准确性，尤其是在面临上述局限性时。</p>
<hr />
<p>总而言之，ARTDECO通过巧妙地结合前馈3D基础模型的效率和SLAM管线的可靠性，并引入创新的分层高斯表示，在实时3D重建领域取得了显著进展。它在保持高保真度的同时，实现了交互式性能和强大的鲁棒性，为AR/VR、机器人和数字孪生等应用提供了有前景的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose ARTDECO, a unified
framework that combines the efficiency of feed-forward models with the
reliability of SLAM-based pipelines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.08551v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.08551v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-10 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
