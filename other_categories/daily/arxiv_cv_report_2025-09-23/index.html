<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-23 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-22/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-24/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-23">Arxiv Computer Vision Papers - 2025-09-23</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-22" class="nav-link">Arxiv è®¡ç®æºè§è§ä¸æºå¨å­¦ä¹ æ¥æ¥æ§è¡æè¦ (2025-09-22)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#qwen3-omni-technical-report" class="nav-link">Qwen3-Omni Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#automatic-intermodal-loading-unit-identification-using-computer-vision-a-scoping-review" class="nav-link">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a>
                </li>
                <li class="nav-item">
                    <a href="#composeme-attribute-specific-image-prompts-for-controllable-human-image-generation" class="nav-link">ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#geosvr-taming-sparse-voxels-for-geometrically-accurate-surface-reconstruction" class="nav-link">GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#tempsamp-r1-effective-temporal-sampling-with-reinforcement-fine-tuning-for-video-llms" class="nav-link">TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#composablenav-instruction-following-navigation-in-dynamic-environments-via-composable-diffusion" class="nav-link">ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#does-audio-matter-for-modern-video-llms-and-their-benchmarks" class="nav-link">Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</a>
                </li>
                <li class="nav-item">
                    <a href="#sight-over-site-perception-aware-reinforcement-learning-for-efficient-robotic-inspection" class="nav-link">Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</a>
                </li>
                <li class="nav-item">
                    <a href="#accurate-and-efficient-low-rank-model-merging-in-core-space" class="nav-link">Accurate and Efficient Low-Rank Model Merging in Core Space</a>
                </li>
                <li class="nav-item">
                    <a href="#i2vwm-robust-watermarking-for-image-to-video-generation" class="nav-link">I2VWM: Robust Watermarking for Image to Video Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-23">Arxiv Computer Vision Papers - 2025-09-23</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-22">Arxiv è®¡ç®æºè§è§ä¸æºå¨å­¦ä¹ æ¥æ¥æ§è¡æè¦ (2025-09-22)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»æ¥ Arxiv è®¡ç®æºè§è§ä¸æºå¨å­¦ä¹ é¢åçè®ºæåç°åºå¤æ¨¡æå¤§æ¨¡åï¼å°¤å¶æ¯è§é¢-è¯­è¨æ¨¡åï¼ã3D å ä½éå»ºãå¯æ§çæä»¥åæºå¨äººæç¥ä¸å¯¼èªç­å¤ä¸ªæ´»è·æ¹åãç¹å«å¼å¾å³æ³¨çæ¯ï¼å¤æ¨¡ææ¨¡åå¨å¤çè§é¢ãé³é¢åææ¬ä¿¡æ¯æ¹é¢çè½åæç»­å¢å¼ºï¼å¹¶å¼å§æ¢ç´¢æ´ç²¾ç»çæ§å¶ååºç¨ã</p>
<p><strong>ä¸»è¦ä¸»é¢ä¸è¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æå¤§æ¨¡åä¸è§é¢çè§£ (V-LLMs)ï¼</strong> å¤ç¯è®ºæèç¦äºè§é¢-è¯­è¨æ¨¡å (V-LLMs) çåå±ï¼åæ¬å¶æ¶æãè®­ç»ç­ç¥ï¼å¦æ¶é´éæ ·ï¼ä»¥åå¯¹é³é¢æ¨¡æçèéãè¿è¡¨æ V-LLMs ä»æ¯å½åç ç©¶ç­ç¹ï¼æ¨å¨å®ç°æ´å¨é¢ãæ´é²æ£çè§é¢åå®¹çè§£ã</li>
<li><strong>å¯æ§çæä¸ç¼è¾ï¼</strong> å¨å¾ååè§é¢çæé¢åï¼ç ç©¶äººåæ­£è´åäºæåçæåå®¹çç²¾ç»æ§å¶è½åï¼ä¾å¦éè¿å±æ§ç¹å®æç¤ºè¿è¡äººç©å¾åçæï¼ä»¥åä¸ºå¾åå°è§é¢çææä¾é²æ£çæ°´å°æ¹æ¡ã</li>
<li><strong>3D å ä½éå»ºï¼</strong> ç¨çä½ç´ å¨å ä½ç²¾ç¡®è¡¨é¢éå»ºä¸­çåºç¨æ¯ä¸ä¸ªéè¦æ¹åï¼æ¨å¨åæä¼ ç»æ¹æ³çå±éæ§ï¼å®ç°æ´é«è´¨éç3Dæ¨¡åã</li>
<li><strong>æºå¨äººæç¥ä¸å¯¼èªï¼</strong> æºå¨äººé¢åçç ç©¶ä¾§éäºå¨å¨æç¯å¢ä¸­å®ç°æä»¤éµå¾ªçå¯¼èªï¼å¹¶ç»åæç¥è½åä¼åæºå¨äººæ£æ¥ä»»å¡çæçã</li>
<li><strong>æ¨¡åæçä¸é²æ£æ§ï¼</strong> è®ºæä¹å³æ³¨æ¨¡ååå¹¶çæçååç¡®æ§ï¼ä»¥åçææ¨¡åçæ°´å°ææ¯ï¼ä½ç°äºå¯¹æ¨¡åå®ç¨æ§åå®å¨æ§çèéã</li>
</ol>
<p><strong>ç¹å«æ¾èæåæ°è®ºæï¼</strong></p>
<ul>
<li><strong>"Qwen3-Omni Technical Report" (Jin Xu et al.):</strong> ä½ä¸ºä¸ä»½ææ¯æ¥åï¼å®éå¸¸ä¼ä»ç»ä¸ä¸ªå¤§åãå¤åè½çæ¨¡åï¼é¢ç¤ºçæªæ¥å¤æ¨¡æå¤§æ¨¡åçåå±æ¹ååè½åè¾¹çãå¦æ Qwen3-Omni åå¶åèº«ä¸æ ·å·æå¹¿æ³å½±ååï¼è¿ä»½æ¥åå°æ¯çè§£å¶æ ¸å¿ææ¯åæ½åçå³é®ã</li>
<li><strong>"TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs" (Yunheng Li et al.):</strong> è¯¥è®ºæéè¿å¼ºåå­¦ä¹ å¾®è°æ¥ä¼åè§é¢ LLMs çæ¶é´éæ ·ç­ç¥ï¼è§£å³äºè§é¢çè§£ä¸­çä¸ä¸ªæ ¸å¿ææï¼å³å¦ä½é«æå°ä»é¿è§é¢ä¸­æåå³é®ä¿¡æ¯ãè¿ç§ç»åå¼ºåå­¦ä¹ çä¼åæ¹æ³å·æåæ°æ§ã</li>
<li><strong>"ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion" (Zichao Hu et al.):</strong> å°å¯ç»åæ©æ£æ¨¡ååºç¨äºå¨æç¯å¢ä¸­çæä»¤éµå¾ªå¯¼èªï¼ä¸ºæºå¨äººå¯¼èªæä¾äºä¸ç§æ°é¢ä¸å¯è½æ´é²æ£çè§£å³æ¹æ¡ï¼å°¤å¶æ¯å¨å¤æãä¸å¯é¢æµçåºæ¯ä¸­ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>å¼ºåå­¦ä¹ å¨å¤æ¨¡ææ¨¡åä¼åä¸­çåºç¨ï¼</strong> "TempSamp-R1" å±ç¤ºäºå¼ºåå­¦ä¹ å¨ä¼å V-LLMs åé¨æºå¶ï¼å¦æ¶é´éæ ·ï¼æ¹é¢çæ½åã</li>
<li><strong>å¯ç»åæ©æ£æ¨¡åå¨æºå¨äººæ§å¶ä¸­çåºç¨ï¼</strong> "ComposableNav" æ¢ç´¢äºæ©æ£æ¨¡åå¨çæå¤æãå¤æ­¥éª¤è¡ä¸ºåºåæ¹é¢çè½åï¼ä¸ºæºå¨äººä»»å¡è§ååæ§è¡å¼è¾äºæ°éå¾ã</li>
<li><strong>å¤æ¨¡ææ¨¡åä¸­çé³é¢æ¨¡æéè¦æ§åè¯ä¼°ï¼</strong> "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?" æåºäºå¯¹ç°æ V-LLMs ååºåä¸­é³é¢ä½ç¨çæ·±å¥æèï¼è¿å¯è½å¼å¯¼æªæ¥æ¨¡åè®¾è®¡æ´å å¨é¢å°æ´åé³é¢ä¿¡æ¯ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<ol>
<li><strong>"Qwen3-Omni Technical Report" (Jin Xu et al.):</strong> å¯¹äºå³æ³¨å¤æ¨¡æå¤§æ¨¡åææ°è¿å±åæªæ¥è¶å¿çç ç©¶äººåï¼è¿ä»½æ¥åæ¯å¿è¯»çï¼å®å°æä¾ä¸ä¸ªå¨é¢ä¸åæ²¿çè§è§ã</li>
<li><strong>"TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs" (Yunheng Li et al.):</strong> å¯¹äºä»äºè§é¢çè§£å V-LLMs ä¼åçç ç©¶äººåï¼è¯¥è®ºææä¾äºä¸ç§åæ°çæ¹æ³æ¥è§£å³æ¶é´éæ ·æçé®é¢ï¼å·æå¾é«çåèä»·å¼ã</li>
<li><strong>"ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion" (Zichao Hu et al.):</strong> å¯¹äºæºå¨äººå­¦åå·èº«æºè½é¢åçç ç©¶äººåï¼è¯¥è®ºæå±ç¤ºäºå°çææ¨¡ååºç¨äºå¤æå¯¼èªä»»å¡çæ½åï¼å¼å¾æ·±å¥ç ç©¶ã</li>
<li><strong>"ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation" (Guocheng Gordon Qian et al.):</strong> å¯¹äºå³æ³¨å¯æ§å¾åçæåäººååæçç ç©¶äººåï¼è¯¥è®ºææä¾äºä¸ç§ç²¾ç»æ§å¶çæåå®¹çæ¹æ³ï¼å·æå®éåºç¨ä»·å¼ã</li>
</ol>
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥ Arxiv è®¡ç®æºè§è§ä¸æºå¨å­¦ä¹ é¢åçå³é®åå±ï¼å¹¶ä¸ºè¿ä¸æ­¥æ·±å¥éè¯»æä¾æå¯¼ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.17765v1">Qwen3-Omni Technical Report</a></li>
<li><a href="#2509.17707v1">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></li>
<li><a href="#2509.18092v1">ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</a></li>
<li><a href="#2509.18090v1">GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</a></li>
<li><a href="#2509.18056v1">TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</a></li>
<li><a href="#2509.17941v1">ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</a></li>
<li><a href="#2509.17901v1">Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</a></li>
<li><a href="#2509.17877v1">Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</a></li>
<li><a href="#2509.17786v1">Accurate and Efficient Low-Rank Model Merging in Core Space</a></li>
<li><a href="#2509.17773v1">I2VWM: Robust Watermarking for Image to Video Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.17765v1'></a></p>
<h2 id="qwen3-omni-technical-report"><a href="https://arxiv.org/abs/2509.17765v1">Qwen3-Omni Technical Report</a></h2>
<p><strong>Authors:</strong> Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV, eess.AS</p>
<p><strong>Abstract:</strong></p>
<p>We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾ãQwen3-Omni Technical Reportãçå¨é¢æè¦ã</p>
<p><strong>Qwen3-Omni ææ¯æ¥åæè¦</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå¤æ¨¡ææ¨¡åæ®éå­å¨çæ¨¡æé´æ§è½éåé®é¢ï¼å³å¨æåæä¸æ¨¡ææ§è½æ¶ï¼å¶ä»æ¨¡æçæ§è½ä¼éä¹ä¸éãç ç©¶ç®æ æ¯å¼åä¸ä¸ªç»ä¸çå¤æ¨¡ææ¨¡åï¼Qwen3-Omniï¼ï¼ä½¿å¶å¨ææ¬ãå¾åãé³é¢åè§é¢ç­å¤ç§æ¨¡æä¸é½è½ä¿ææåè¿çæ§è½ï¼ä¸ä¸åçä»»ä½æ§è½éåï¼åæ¶æ¾èå¢å¼ºè·¨æ¨¡æè½åï¼å¦è§é¢çè§£ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>Thinker-Talker MoE æ¶æï¼</strong> Qwen3-Omni éç¨ Thinker-Talker æ··åä¸å®¶ï¼MoEï¼æ¶æï¼ç»ä¸äºææ¬ãå¾åãé³é¢åè§é¢çæç¥ä¸çæï¼å®ç°äºæµççææ¬åèªç¶çå®æ¶è¯­é³ãThinker å Talker é½åçº§ä¸º MoE è®¾è®¡ï¼ä»¥æ¯æé«å¹¶ååå¿«éæ¨çã
*   <strong>AuT é³é¢ç¼ç å¨ï¼</strong> æ¿æ¢äº Whisper é³é¢ç¼ç å¨ï¼éç¨ä»é¶å¼å§è®­ç»ç AuTï¼Audio Transformerï¼ç¼ç å¨ï¼å¨ 2000 ä¸å°æ¶ççç£é³é¢æ°æ®ä¸è®­ç»ï¼çææ´å¼ºçéç¨é³é¢è¡¨ç¤ºï¼å¹¶éç¨åå¼çªå£æ³¨æåå®ç°å®æ¶é¢å¡«åç¼å­ã
*   <strong>å¤ç æ¬è¯­é³çæï¼</strong> å¨è¯­é³çææ¹é¢ï¼éç¨å¤ç æ¬è¡¨ç¤ºï¼éè¿ MTP æ¨¡åèªåå½é¢æµå¤ä¸ªç æ¬å±ï¼å¹¶ç¨è½»éçº§å æ ConvNet æ¿æ¢è®¡ç®å¯éåçåå¼æ©æ£æ¨¡åï¼å®ç°ä»ç¬¬ä¸ä¸ªç æ¬å¸§å¼å§çæµå¼åæï¼æ¾èéä½äºé¦åå»¶è¿ã
*   <strong>Thinking æ¨¡åï¼</strong> å¼å¥äºä¸ä¸ª Thinking æ¨¡åï¼æç¡®å°å¯¹æ¥èªä»»ä½æ¨¡æçè¾å¥è¿è¡æ¨çï¼ä»¥å¢å¼ºå¤æ¨¡ææ¨çè½åã
*   <strong>ééåå¤æ¨¡æè®­ç»ï¼</strong> è®ºææåºå¹¶éªè¯äºå¨ææ¬é¢è®­ç»æ©æé¶æ®µæ··ååæ¨¡æåè·¨æ¨¡ææ°æ®ï¼å¯ä»¥å®ç°æææ¨¡æçæ§è½åç­ï¼åæ¶æ¾èå¢å¼ºè·¨æ¨¡æè½åã
*   <strong>é³é¢å­å¹æ¨¡åï¼</strong> éå¯¹éç¨é³é¢å­å¹æ¨¡åçç¼ºå¤±ï¼å¾®è°äº Qwen3-Omni-30B-A3Bï¼å¾å°äº Qwen3-Omni-30B-A3B-Captionerï¼è½ä¸ºä»»æé³é¢è¾å¥çæè¯¦ç»ãä½å¹»è§çå­å¹ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>SOTA æ§è½ï¼</strong> Qwen3-Omni å¨ 36 ä¸ªé³é¢åé³è§é¢åºåæµè¯ä¸­ï¼å¨ 32 ä¸ªåºåæµè¯ä¸å®ç°äºå¼æº SOTAï¼å¹¶å¨ 22 ä¸ªåºåæµè¯ä¸è¾¾å°äºæ»ä½ SOTAï¼è¶è¶äº Gemini-2.5-ProãSeed-ASR å GPT-4o-Transcribe ç­å¼ºå¤§çé­æºæ¨¡åã
*   <strong>æ æ¨¡æéåï¼</strong> é¦æ¬¡è¯æäºè¯¥æ¨¡åå¨ææ¬ãå¾åãé³é¢åè§é¢ä¸åä¿æäºä¸åç­è§æ¨¡åæ¨¡ææ¨¡åç¸å½çåè¿æ§è½ï¼æ²¡æåºç°æ¨¡æéåã
*   <strong>é³é¢ä»»å¡ä¼å¿ï¼</strong> å¨é³é¢ä»»å¡ä¸è¡¨ç°å°¤ä¸ºåºè²ï¼æ¯æ 119 ç§è¯­è¨çææ¬äº¤äºã19 ç§è¯­è¨çè¯­é³çè§£å 10 ç§è¯­è¨çè¯­é³çæã
*   <strong>ä½å»¶è¿æµå¼åæï¼</strong> å¨å·å¯å¨è®¾ç½®ä¸ï¼å®ç°äº 234 æ¯«ç§ççè®ºç«¯å°ç«¯é¦åå»¶è¿ï¼ç¡®ä¿äºé«å¹¶åå·¥ä¸é¨ç½²ä¸­çä½å»¶è¿è¯­é³äº¤äºã
*   <strong>è·¨æ¨¡ææ¨çå¢å¼ºï¼</strong> Thinking æ¨¡ååå¤æ¨¡æè®­ç»æ¾èæåäºæ¨¡åå¨å¤ææ¨çä»»å¡ä¸çè¡¨ç°ï¼å°¤å¶æ¯å¨éè¦æ´åé³é¢åè§è§ä¿¡æ¯çåºæ¯ä¸­ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é¿è§é¢åºåæµè¯æ§è½æ¬¡ä¼ï¼</strong> å½åæ¨¡åå¨é¿è§é¢åºåæµè¯ä¸çæ§è½æ¬¡ä¼ï¼è¿æºäºä¸¤ä¸ªæ¶æéå¶ï¼ä½ç½®å¤æ¨è½åæéåä¸ä¸æé¿åº¦åéã
*   <strong>è¯­è¨è½åæåä¸ææ¾ï¼</strong> ç»éªè¡¨æï¼æ·»å è§è§æé³é¢ä¿¡å·å¹¶æªå¨è¯­è¨è½åæ¹é¢å¸¦æ¥å¯è¡¡éçæåã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤è¯´è¯äºº ASRï¼</strong> è¿ä¸æ­¥æ¯æå¤è¯´è¯äººèªå¨è¯­é³è¯å«ã
*   <strong>è§é¢ OCRï¼</strong> å¢å¼ºè§é¢ä¸­çåå­¦å­ç¬¦è¯å«è½åã
*   <strong>é³è§é¢ä¸»å¨å­¦ä¹ ï¼</strong> æ¢ç´¢é³è§é¢é¢åçä¸»å¨å­¦ä¹ ã
*   <strong>åºäºä»£ççå·¥ä½æµåå½æ°è°ç¨ï¼</strong> å¢å¼ºå¯¹åºäºä»£ççå·¥ä½æµåå½æ°è°ç¨çæ¯æã
*   <strong>è§£å³é¿è§é¢æ§è½éå¶ï¼</strong> è§£å³å½åæ¨¡åå¨é¿è§é¢çè§£æ¹é¢çæ¶æéå¶ï¼æåå¶å¨è¯¥é¢åçæ§è½ã</p>
<p>æ»èè¨ä¹ï¼Qwen3-Omni ä»£è¡¨äºå¤æ¨¡ææ¨¡ååå±çä¸ä¸ªéç¨ç¢ï¼é¦æ¬¡æä¾äºå¨é¢éæãç«¯å°ç«¯çå¤æ¨¡æè®­ç»å¯ä»¥å¨ä¸éä½æ ¸å¿è¯­è¨è½ååå¶ä»æ¨¡ææ§è½çæåµä¸å®ç°ï¼åæ¶å¨é³é¢ä»»å¡ä¸è¡¨ç°åè¶ï¼å¹¶æ¾èå¢å¼ºäºè·¨æ¨¡ææ¨çè½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts.</li>
<li>To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17765v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17765v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17707v1'></a></p>
<h2 id="automatic-intermodal-loading-unit-identification-using-computer-vision-a-scoping-review"><a href="https://arxiv.org/abs/2509.17707v1">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></h2>
<p><strong>Authors:</strong> Emre GÃ¼lsoylu, Alhassan Abdelhalim, Derya Kara Boztas, Ole Grasse, Carlos Jahn, Simone Frintrop, Janick Edinger</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºEmre GÃ¼lsoyluç­äººæ°åçè®ºæâAutomatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Reviewâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºæé¢ç®ï¼</strong> ä½¿ç¨è®¡ç®æºè§è§çèè¿è£è½½ååèªå¨è¯å«ï¼ä¸é¡¹èå´å®¡æ¥</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³èè¿è£è½½ååï¼ILUsï¼å¦éè£ç®±ãåæè½¦åå¯æ¢ç®±ä½ï¼å¨æ¸¯å£åç å¤´è¿è¡é«æãé²æ£è¯å«çå³é®ç¶é¢ãå°½ç®¡ILUsçæ ååå½»åºæ¹åäºå¨çè´¸æï¼ä½å¶è¯å«è¿ç¨ä»é¢ä¸´ææãè¯¥ç ç©¶éè¿å¯¹è®¡ç®æºè§è§ï¼CVï¼é¢åç°æè§£å³æ¹æ¡çå¨é¢å®¡æ¥ï¼æ¢è®¨äºå¦ä½å©ç¨CVææ¯åæè¿äºææï¼å¹¶è¯å«è¯¥é¢åçåå±è¶å¿ãå±éæ§åæªæ¥æ¹åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¿ç¯è®ºææ¬èº«æ¯ä¸é¡¹èå´å®¡æ¥ï¼å¶ä¸»è¦è´¡ç®å¨äºå¯¹ç°ææç®çç³»ç»æ§åæåç»¼åï¼èéæåºæ°çç®æ³ææ¨¡åãå¶å³é®åæ°åæ¹æ³è®ºè´¡ç®åæ¬ï¼
*   <strong>é¦æ¬¡å¨é¢å®¡æ¥ï¼</strong> è¿æ¯å¯¹åºäºCVçILUè¯å«ç ç©¶çé¦æ¬¡ç»¼åæ§å®¡æ¥ï¼æ¶µçäº1990å¹´è³2025å¹´é´ç63é¡¹å®è¯ç ç©¶ã
*   <strong>é¢åæ¼åè¿½è¸ªï¼</strong> è¯¦ç»è¿½æº¯äºè¯¥é¢åä»æ©ææ°å­å¾åå¤çï¼DIPï¼åä¼ ç»æºå¨å­¦ä¹ ï¼MLï¼å°å½åæ·±åº¦å­¦ä¹ ï¼DLï¼ææ¯ä¸»å¯¼çæ¼åè¿ç¨ã
*   <strong>æ ååæ¯è¯­çå¼åï¼</strong> å¼ºè°äºè¯¥é¢åæ¯è¯­ä¸ç»ä¸çé®é¢ï¼å¹¶å¼åéç¨æ ååæ¯è¯­ï¼ä¾å¦âèè¿è£è½½ååï¼ILUï¼è¯å«âï¼ä»¥æé«æ¦å¿µæ¸æ°åº¦å¹¶ä¿è¿è·¨å­¦ç§ç ç©¶ã
*   <strong>æ°æ®éåè¯ä¼°ææ çç»¼ååæï¼</strong> è¯¦ç»åæäºç°æç ç©¶ä¸­ä½¿ç¨çå¾åééè®¾ç½®ãæ°æ®éç¹æ§ï¼å¯ç¨æ§ãå¤æ ·æ§ï¼åè¯ä¼°ææ ï¼æ­ç¤ºäºå¬å±åºåæ°æ®éçä¸¥éç¼ºä¹ã
*   <strong>è¯å«æ°å´ææï¼</strong> çªåºäºä»åºäºå­ç¬¦çææ¬è¯å«è½¬ååºæ¯ææ¬è¯å«ï¼scene-text spottingï¼çè¶å¿ï¼ä»¥åç§»å¨æåå¤´ï¼å¦æ äººæºãè½¦è½½ä¼ æå¨ï¼å¨å¨æç å¤´çæ§ä¸­çåºç¨æå¸¦æ¥çæ°ææã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>CVä½ä¸ºç»æµé«æçæ¿ä»£æ¹æ¡ï¼</strong> è®¡ç®æºè§è§ææ¯ä¸ºILUè¯å«æä¾äºæ¯RFIDç­å¶ä»ææ¯æ´å·ææ¬æççæ¿ä»£æ¹æ¡ï¼ä¸åç¡®æ§æ´é«ã
*   <strong>DLææ¯çä¸»å¯¼å°ä½ï¼</strong> ç ç©¶è¶å¿æ¾ç¤ºï¼æ·±åº¦å­¦ä¹ æ¹æ³å·²æä¸ºILUè¯å«çä¸»å¯¼èå¼ï¼å°¤å¶æ¯å¨2016-2020å¹´ä¹åï¼å¶å¨å¤çå¤ææ¡ä»¶ä¸çé²æ£æ§ä¼äºä¼ ç»æ¹æ³ã
*   <strong>äºæ´²ç ç©¶ççªåºè´¡ç®ï¼</strong> äºæ´²å°åºå¨ILUè¯å«ç ç©¶ä¸­å æ®ä¸»å¯¼å°ä½ï¼79.71%çè®ºææ¥èªäºæ´²ï¼ï¼è¿åæ äºè¯¥å°åºä½ä¸ºå¨çè´¸ææ¢çº½å¯¹é«æç©æµçè¿«åéæ±åå¤§éæèµã
*   <strong>å¬å±èµéæ¯æï¼</strong> è¶è¿ä¸åä¹ä¸çç ç©¶å¾å°å¬å±èµéæ¯æï¼è¡¨ææ¿åºå¯¹äº¤éåºç¡è®¾æ½åç»æµå¢é¿çéè§ã
*   <strong>æ§è½å·®å¼å·¨å¤§ï¼</strong> ç±äºç¼ºä¹å¬å¼å¯ç¨çåºåæ°æ®éï¼æ¥åçç«¯å°ç«¯åç¡®çå·®å¼å·¨å¤§ï¼ä»5%å°96%ä¸ç­ï¼è¿ä½¿å¾æ¹æ³é´çå¬å¹³æ¯è¾åå¾å°é¾ã
*   <strong>ä»å­ç¬¦è¯å«å°åºæ¯ææ¬è¯å«çè½¬åï¼</strong> éçç§»å¨æåå¤´çæ®åï¼ä»»å¡å·²ä»ç®åçå­ç¬¦è¯å«æ¼åä¸ºæ´å¤æçåºæ¯ææ¬è¯å«ï¼éè¦æ´å¼ºå¤§çæ£æµåè¯å«æ¹æ³ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>ç¼ºä¹å¬å¼å¯ç¨çåºåæ°æ®éï¼</strong> è¿æ¯è¯¥é¢åç ç©¶åå¼åçæå¤§éç¢ãç»å¤§å¤æ°æ°æ®éï¼85.71%ï¼æ¯ç§æçï¼ä¸è®¸å¤ç ç©¶ä¾èµäºåæ§çå¾åééè®¾ç½®ï¼å¯¼è´æ¨¡åæ³åè½ååéï¼å¹¶é»ç¢äºç»æçéç°æ§åå¬å¹³æ¯è¾ã
*   <strong>æ¯è¯­ä¸ç»ä¸ï¼</strong> ç°ææç®ä¸­æ¯è¯­ä½¿ç¨ä¸ä¸è´ï¼å½±åäºç ç©¶çæ¸æ°åº¦åå¯æ¯æ§ã
*   <strong>DLè®­ç»æ°æ®ä¸è¶³ï¼</strong> ç°ææ°æ®éçè§æ¨¡å¯¹äºé²æ£çæ·±åº¦å­¦ä¹ è®­ç»æ¥è¯´å¾å¾è¿å°ï¼è¿«ä½¿ç ç©¶äººåéåºéç¨ææ¬æ£æµåè¯å«æ¨¡åï¼å¯è½æ æ³ååä¼åISO6346ä»£ç çç¹å®è¯å«ã
*   <strong>å¯¹éä¸»æµæåçä¾èµï¼</strong> è¶è¿ä¸åçæç« åè¡¨å¨æªæåçæååä¼è®®ä¸ï¼è¿å¯è½å½±åäºè¯¥é¢åç ç©¶çå¯è§åº¦åå½±ååã
*   <strong>ä¸ä¸ææ å³ææ¬è¯å«çææï¼</strong> ISO6346ä»£ç çä¸ä¸ææ å³æ§è´¨ä½¿å¾åºæ¯ææ¬è¯å«ä»»å¡æ´å·æææ§ï¼å ä¸ºç¼ºä¹èªç¶è¯­è¨çº¿ç´¢ï¼ç°æä¾èµè¯­ä¹ä¸ä¸æçæ¨¡åææä¸ä½³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ åååå¼æ¾è·åæ°æ®éï¼</strong> å¼åå»ºç«å¬å¼å¯ç¨çåºåæ°æ®éï¼å¹¶éç¨æ ååæ¯è¯­åè¯ä¼°æ¹æ³ï¼ä»¥ä¿è¿ç ç©¶ææçå¯æ¯æ§åå¯éç°æ§ã
*   <strong>ä¸ä¸ææ å³åºæ¯ææ¬è¯å«ï¼</strong> éå¯¹ISO6346ä»£ç çä¸ä¸ææ å³ç¹æ§ï¼å¼åä¸é¨ä¼åçåºæ¯ææ¬è¯å«æ¶æï¼é¿åä¾èµèªç¶è¯­è¨æ¨¡åã
*   <strong>å®æ¶å¤çåè¾¹ç¼è®¾å¤ä¼åï¼</strong> å³æ³¨ç®æ³ä¼åï¼ä»¥å®ç°ç§»å¨è®¾å¤ä¸çå®æ¶å¤çï¼å¹¶éè¿è§é¢æµåæå¢å¼ºç³»ç»æ§è½ã
*   <strong>ç§»å¨æåå¤´è®¾ç½®çéç¹ç ç©¶ï¼</strong> æ´å¤å°å³æ³¨è½¦è½½æåå¤´ï¼å¦æ äººæºãä¼ æå¨éå¤çå°é¢è½¦è¾ï¼çè®¾ç½®ï¼ä»¥åºå¯¹å¨æåºæ¯å¸¦æ¥çæ°ææï¼å¹¶æ¢ç´¢ILUå§¿æä¼°è®¡ç­æ°ä»»å¡ï¼å®ç°ç å¤´çç²¾ç¡®çæ§ã
*   <strong>æ°æ®å¢å¼ºååææ°æ®çæï¼</strong> æ¢ç´¢ä½¿ç¨çæå¯¹æç½ç»ï¼GANsï¼ææ¸¸æå¼æç­ææ¯çæåææ°æ®ï¼ä»¥å¼¥è¡¥çå®æ°æ®éçä¸è¶³ï¼å¹¶æ¨¡æåç§æææ§æ¡ä»¶ï¼å¦é´å½±ãéè¿¹ãæ±¡å¢ãè¿å¨æ¨¡ç³ï¼ã
*   <strong>å¾åè´¨éæåååå¤çï¼</strong> ç»§ç»­ç ç©¶å¾åå¢å¼ºææ¯ï¼å¦éåªãè¶åè¾¨çï¼ååå¤çæ­¥éª¤ï¼å¦å©ç¨æ ¡éªä½åææèä»£ç éªè¯è¯å«ç»æï¼ï¼ä»¥æé«è¯å«åç¡®æ§ã
*   <strong>ç»ä¸çç«¯å°ç«¯æ¨¡åï¼</strong> å¼åå°ææ¬æ£æµåè¯å«æ´åå°åä¸ªååä¼ æ­ä¸­çç»ä¸ç«¯å°ç«¯æ¨¡åï¼ä»¥ç®åå¤çæµç¨å¹¶æé«æ´ä½æçã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring.</li>
<li>To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17707v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17707v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.18092v1'></a></p>
<h2 id="composeme-attribute-specific-image-prompts-for-controllable-human-image-generation"><a href="https://arxiv.org/abs/2509.18092v1">ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</a></h2>
<p><strong>Authors:</strong> Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, Kfir Aberman</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating high-fidelity images of humans with fine-grained control over
attributes such as hairstyle and clothing remains a core challenge in
personalized text-to-image synthesis. While prior methods emphasize identity
preservation from a reference image, they lack modularity and fail to provide
disentangled control over specific visual attributes. We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity. Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model. This enables compositional and
disentangled control over multiple visual factors, even across multiple people
within a single image. To promote natural composition and robust
disentanglement, we curate a cross-reference training dataset featuring
subjects in diverse poses and expressions, and propose a multi-attribute
cross-reference training strategy that encourages the model to generate
faithful outputs from misaligned attribute inputs while adhering to both
identity and textual conditioning. Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts. Our framework paves the way for more configurable human image
synthesis by combining visual prompting with text-driven generation. Webpage is
available at: https://snap-research.github.io/composeme/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Guocheng Gordon Qianç­äººæ°åçè®ºæâComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä¸ªæ§åææ¬å°å¾ååæé¢åçä¸ä¸ªæ ¸å¿ææï¼å¦ä½çæé«ä¿ççäººç±»å¾åï¼å¹¶å¯¹ååãæè£åèº«ä»½ç­å±æ§è¿è¡ç»ç²åº¦æ§å¶ãç°ææ¹æ³è½ç¶å¼ºè°ä»åèå¾åä¸­ä¿çèº«ä»½ï¼ä½ç¼ºä¹æ¨¡ååï¼æ æ³å¯¹ç¹å®è§è§å±æ§æä¾è§£è¦æ§å¶ï¼å°¤å¶æ¯å¨éè¦ç»åæ¥èªä¸åæ¥æºçå¤ä¸ªå±æ§æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ComposeMeå¼å¥äºä¸ç§æ°é¢çâå±æ§ç¹å®å¾åæç¤ºâèå¼ï¼å¶æ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>å±æ§ç¹å®å¾åæç¤ºï¼Attribute-Specific Image Promptsï¼ï¼</strong> è¯¥æ¹æ³å°äººç±»ä¸»ä½åè§£ä¸ºå¯éç½®çå±æ§ï¼é¢é¨èº«ä»½ãåååæè£ï¼ï¼å¹¶ä¸ºæ¯ä¸ªå±æ§ä½¿ç¨ä¸åçåèå¾åéè¿è¡å¼å¯¼çæãè¿ä½¿å¾è½å¤ç»åæ¥èªä¸åæ¥æºçè§è§å±æ§ï¼å®ç°æ´çµæ´»çæ§å¶ã</li>
<li><strong>ComposeMeç®¡çº¿ï¼</strong> éç¨åºäºééå¨çè§£å³æ¹æ¡ï¼éè¿ä¸ä¸ªé¶æ®µå®ç°ï¼<ul>
<li><strong>å±æ§ç¹å®æ è®°åï¼Attribute-Specific Tokenizationï¼ï¼</strong> ä¸ºæ¯ä¸ªè§è§ç»ä»¶ï¼é¢é¨ãååãæè£ï¼ä½¿ç¨ä¸ç¨çç¹å¾æ è®°å¨å¤çåèå¾åï¼æè·å±æ§ç¹å®ç¹å¾ã</li>
<li><strong>å¤å±æ§åå¹¶ï¼Multi-Attribute Mergingï¼ï¼</strong> å°æ¥èªä¸åå±æ§çæ è®°åå¹¶ï¼å½¢æå¤å±æ§ä¸»ä½è¡¨ç¤ºã</li>
<li><strong>æ³¨å¥é¢è®­ç»æ©æ£æ¨¡åï¼</strong> å°åå¹¶åçæ è®°æ³¨å¥å»ç»çé¢è®­ç»ææ¬å°å¾åæ©æ£æ¨¡åä¸­ï¼éè¿è§£è¦çäº¤åæ³¨æåæºå¶å®ç°å¾åçæã</li>
</ul>
</li>
<li><strong>å¤å±æ§äº¤åå¼ç¨è®­ç»ï¼Multi-Attribute Cross-Reference Trainingï¼ï¼</strong> æåºäºä¸ç§æ°é¢çä¸¤é¶æ®µè®­ç»ç­ç¥ãå¨ç¬¬ä¸é¶æ®µè¿è¡å¤å¶ç²è´´é¢è®­ç»ä»¥é¢ç­ééå¨ãç¬¬äºé¶æ®µæ¯å¤å±æ§äº¤åå¼ç¨å¾®è°ï¼éè¿ä½¿ç¨æ¥èªä¸åä¸ªä½åå§¿æçè¾å¥åç®æ è¿è¡çç£ï¼æç¡®è§£è¦äºå±æ§ä¹é´ççº ç¼ ï¼ä¾å¦ï¼æè£ãé¢é¨åå¤´åä¸èº«ä½å§¿æãè¡¨ææå¤´é¨æ¹åççº ç¼ ï¼ãè¿ç§è®­ç»é¼å±æ¨¡åä»éä½çå±æ§è¾å¥çæå¿ å®ä¸èªç¶å¯¹é½çè¾åºï¼åæ¶ä¿æèº«ä»½åææ¬æ¡ä»¶ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éªè¡¨æï¼ComposeMeå¨åç¡®éµå¾ªè§è§åææ¬æç¤ºæ¹é¢è¾¾å°äºæåè¿çæ§è½ã</p>
<ul>
<li><strong>é«ä¿çåè§£è¦æ§å¶ï¼</strong> è¯¥æ¹æ³è½å¤çæé«ä¿ççäººç±»å¾åï¼å¯¹å¤ä¸ªè§è§å ç´ ï¼åæ¬é¢é¨è¡¨æãå¤´é¨å§¿æãèº«ä½å§¿æåé£æ ¼ï¼è¿è¡ç»ç²åº¦ãè§£è¦çæ§å¶ï¼å³ä½¿å¨åä¸ªå¾åä¸­åå«å¤ä¸ªäººç©ä¹è½å®ç°ã</li>
<li><strong>ä¼äºç°ææ¹æ³ï¼</strong> å¨å¤å±æ§ãåIDä¸ªæ§åä»»å¡ä¸­ï¼ComposeMeå¨èº«ä»½ãåååæè£çä¿çæ¹é¢æ¾èä¼äºOmniGenåGPT-40ç­ç°ææ¹æ³ãå¨åå±æ§ãå¤IDä¸ªæ§åä»»å¡ä¸­ï¼ComposeMeä¹è¡¨ç°åºæé«çèº«ä»½ä¿çãç»åè´¨éåæ´ä½å¾åè´¨éã</li>
<li><strong>èªç¶ç»ååé²æ£è§£è¦ï¼</strong> äº¤åå¼ç¨è®­ç»å¯¹äºå®ç°é«ä¿çå¾åçæè³å³éè¦ï¼å®è½ææå©ç¨å±æ§ç¹å®çè§è§æç¤ºï¼å³ä½¿è¿äºæç¤ºå­å¨éä½ï¼ä¹è½å®ç°ç²¾ç¡®ãè§£è¦çé¢é¨ãå¤´ååæè£æ§å¶ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>å°é­éä¸ªæ§åï¼</strong> ComposeMeæ¯ä¸ä¸ªå°é­éä¸ªæ§åæ¹æ³ï¼ç®åè®­ç»ç¨äºæå¤2ä¸ªèº«ä»½å3ä¸ªå±æ§ãè½ç¶å¯¹äºå¤§å¤æ°ä»¥äººä¸ºä¸­å¿çç¨ä¾ï¼é¢é¨ãå¤´åãæè£åå§¿æï¼æ¥è¯´ï¼è¿ç§è¦çèå´å·²è¶³å¤ï¼ä½æ©å±å°æ°çè§è§æç¤ºï¼å¦åºæ¯ãéé¥°ï¼ä»éè¿ä¸æ­¥ç ç©¶ã
*   <strong>å°è¸æ ·æ¬çå¤±çï¼</strong> å½ææ¬æç¤ºæå®è¿è·ç¦»æææ¶ï¼çæçé¢é¨ä¼åå¾å¾å°ï¼å°äºå¾åç1%ï¼ï¼å¯¼è´é¢é¨åºç°ææ¾çå¤±çãè¿å½å äºè®­ç»æ°æ®è¿æ»¤å¨ç§»é¤äºå°è¸æ ·æ¬ï¼æªæ¥å·¥ä½å¯ä»¥éè¿åå«å°è¸æ ·æ¬æ¥è§£å³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±å°æ°çè§è§æç¤ºï¼</strong> å¯ä»¥éè¿è¿æ¥æ°çè§è§æç¤ºæ¥å¾®è°ComposeMeï¼æèä¸ºé¢å¤ç±»å«è®­ç»æ°çééå¨å¹¶å¨æ¨çæ¶è¿è¡ç»åã
*   <strong>æé«å¾åè´¨éï¼</strong> å¨é«è´¨éæ°æ®ä¸è¿è¡å¾®è°åRLï¼å¼ºåå­¦ä¹ ï¼å¾®è°æ¯æé«å¾åè´¨éçæåæ¯çæ¹åã
*   <strong>æ´çµæ´»åå¯éç¨çä¸ªæ§åç®¡çº¿ï¼</strong> è¯¥æ¡æ¶ä¸ºæ´å¯éç½®çäººç±»å¾ååæéºå¹³äºéè·¯ï¼åè®¸æ¯ä¸ªå±æ§ç¬ç«å°è¿è¡ç­åãæ´æ°ææ¿æ¢ï¼èæ ééæ°è®­ç»æ´ä¸ªèº«ä»½è¡¨ç¤ºã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity.</li>
<li>Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model.</li>
<li>Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.18092v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.18092v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.18090v1'></a></p>
<h2 id="geosvr-taming-sparse-voxels-for-geometrically-accurate-surface-reconstruction"><a href="https://arxiv.org/abs/2509.18090v1">GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</a></h2>
<p><strong>Authors:</strong> Jiahe Li, Jiawei Zhang, Youmin Zhang, Xiao Bai, Jin Zheng, Xiaohan Yu, Lin Gu</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jiahe Liç­äººæ°åçè®ºæâGeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstructionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½ååºäºè¾å°åºçè¡¨é¢éå»ºæ¹æ³ï¼ç¹å«æ¯åºäºé«æ¯æºå°ï¼Gaussian Splattingï¼çæ¹æ³ï¼å¨å ä½ç²¾åº¦ãç»èä¿çåéå»ºå®æ´æ§æ¹é¢å­å¨çå±éæ§ãè¿äºç°ææ¹æ³å¸¸åéäºè¡¨ç¤ºç¶é¢ï¼ä¾å¦ä¾èµç»æåç¹äºåå§åå¯¼è´ä¸å®æ´æä¸åç¡®çåºåï¼ä»¥åé«æ¯åºåç¼ºä¹æ¸æ°è¾¹ç¼å¯¼è´å ä½æ¨¡ç³ãGeoSVRæ¢ç´¢äºç¨çä½ç´ å¨å®ç°ç²¾ç¡®ãè¯¦ç»åå®æ´è¡¨é¢éå»ºæ¹é¢çæ½åï¼å¹¶è§£å³äºç¨çä½ç´ åºæçåºæ¯çº¦æç¼ºå¤±åè¡¨é¢ç»åå±é¨æ§é®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
GeoSVRæåºäºä¸ä¸ªæ¾å¼çãåºäºä½ç´ çæ¡æ¶ï¼å¶ä¸»è¦åæ°åæ¬ï¼</p>
<ul>
<li><strong>ä½ç´ ä¸ç¡®å®æ§æ·±åº¦çº¦æï¼Voxel-Uncertainty Depth Constraintï¼ï¼</strong> éå¯¹ç¨çä½ç´ ç¼ºä¹å¼ºç»æåéªçé®é¢ï¼è¯¥æ¹æ³å©ç¨åç®æ·±åº¦çº¿ç´¢ä½ä¸ºåºæ¯çº¦æãå®éè¿è¯ä¼°æ¯ä¸ªä½ç´ çå ä½ä¸ç¡®å®æ§ï¼èªéåºå°ç¡®å®å¯¹å¤é¨æ·±åº¦çº¿ç´¢çä¾èµç¨åº¦ï¼ä»èå¨é¿åè´¨éä¸éçåæ¶ï¼å®ç°ææä¸é²æ£çåºæ¯çº¦æï¼å¹¶ä¿çé«ç²¾åº¦çå ä½ç»æã</li>
<li><strong>ç¨çä½ç´ è¡¨é¢æ­£ååï¼Sparse Voxel Surface Regularizationï¼ï¼</strong> ä¸ºäºè§£å³ç¨çä½ç´ å±é¨æ§è¿å¼ºå¯¼è´è¡¨é¢å½¢æä¸åç¡®çé®é¢ï¼è¯¥æ¹æ³è®¾è®¡äºä¸¤ç§ä½ç´ çº§æ­£ååï¼<ul>
<li><strong>å ä½æ­£ååä¸ä½ç´ ä¸¢å¼ï¼Voxel Dropoutï¼ï¼</strong> éè¿éæºä¸¢å¼ä¸é¨åä½ç´ ï¼å¼ºå¶æ¯ä¸ªå¾®å°ä½ç´ å¨æ´å¤§çåºååä¿æå¨å±å ä½ä¸è´æ§ï¼ä»èæ©å¤§æ­£ååèå´ï¼çº æ­£éè¯¯çå ä½ç»æã</li>
<li><strong>è¡¨é¢æ ¡æ­£ï¼Surface Rectificationï¼ï¼</strong> éå¶è¡¨é¢å½¢æä¸å¯ä¸ä½ç´ å¯¹é½ï¼ä»¥åå°æ·±åº¦åå·®ã</li>
<li><strong>ç¼©æ¾æ©ç½ï¼Scaling Penaltyï¼ï¼</strong> æ¶é¤å ä½ä¸åç¡®çå¤§ä½ç´ åä¸è¡¨é¢å½¢æï¼è¿ä¸æ­¥ä¿è¿å°éååç¡®çè¡¨é¢éå»ºã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
GeoSVRå¨DTUãTanks and Templesä»¥åMip-NeRF 360ç­å¤ä¸ªå·ææææ§çæ°æ®éä¸è¿è¡äºå¹¿æ³å®éªï¼ç»æè¡¨æå¶æ§è½ä¼äºç°ææ¹æ³ï¼å¨å ä½ç²¾åº¦ãç»èä¿çåéå»ºå®æ´æ§æ¹é¢è¡¨ç°åºè²ï¼åæ¶ä¿æäºé«æçãå·ä½æ¥è¯´ï¼</p>
<ul>
<li>å¨DTUæ°æ®éä¸ï¼GeoSVRå¨Chamferè·ç¦»ä¸å®ç°äºæé«çéå»ºè´¨éã</li>
<li>å¨Tanks and Templesæ°æ®éä¸ï¼GeoSVRå¨F1åæ°ä¸è¡¨ç°æä½³ï¼å°¤å¶å¨å¤æå»ºç­åå¼±çº¹çåºåå±ç°åºåè¶çç»èææè½åã</li>
<li>å¨Mip-NeRF 360æ°æ®éä¸ï¼GeoSVRå¨æ¸²æè´¨éæ¹é¢ä¹è¡¨ç°åºç«äºåã</li>
<li>è¯¥æ¹æ³ç»§æ¿äºSVRasterçé«æçï¼å¨æ¨çéåº¦ä¸è¡¨ç°å¿«éï¼ä¸GPUåå­å ç¨è¾ä½ã</li>
</ul>
<p>è¿äºç»æè¯æäºGeoSVRå¨å¤çåå°åºåãè¦çä¸è¶³åºåä»¥åéè¦ç²¾ç»ç»èçåºæ¯æ¶ï¼è½å¤æä¾æ´åç¡®ãæ´å®æ´çè¡¨é¢éå»ºï¼åæäºåºäºé«æ¯æºå°æ¹æ³å¨åå§åç¹äºä¸è¶³æ¶çå±éæ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææåºäºGeoSVRå½åå­å¨çå±éæ§ï¼ä¸»è¦åæ¬ï¼</p>
<ul>
<li><strong>åå°åºåï¼</strong> å¨å·æä¸¥éåå°çåºåï¼ç±äºååº¦ä¸ä¸è´çå¼ºçè¯¯å¯¼ï¼æ¨¡åæ§è½å¯è½åéã</li>
<li><strong>æ çº¹çåºåï¼</strong> å¨ç¼ºä¹çº¹çä¿¡æ¯çåºåï¼å ä½éå»ºå¯è½é¢ä¸´ææã</li>
<li><strong>éæè¡¨é¢ï¼</strong> å½åçè¾å°åºæ¹æ³å¨å¤çå¤æåçº¿è¿½è¸ªæ¹é¢å­å¨ä¸è¶³ï¼å¯¼è´éæç©ä½çéå»ºææä¸çæ³ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä¸ºäºè§£å³ä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li>å¼å¥æ´é«æçåçº¿è¿½è¸ªææ¯ï¼ä»¥æ´å¥½å°å¤çåå°åéæè¡¨é¢ã</li>
<li>æ¹è¿ä½ç´ çå¨å±æ§ï¼ä½¿å¶è½å¤æ´å¥½å°åºå¯¹åç§åååæ çº¹çåºåç­ææã</li>
<li>å¼åéå¯¹éæç©ä½çè§£å³æ¹æ¡ï¼ä»¥æé«å¶éå»ºè´¨éã</li>
</ul>
<hr />
<p>è¿ä»½æè¦çªåºäºGeoSVRå¨å©ç¨ç¨çä½ç´ è¿è¡é«ç²¾åº¦è¡¨é¢éå»ºæ¹é¢çåæ°æ§ï¼ä»¥åå¶å¨è§£å³ç°ææ¹æ³å±éæ§æ¹é¢çè´¡ç®ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.18090v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.18090v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.18056v1'></a></p>
<h2 id="tempsamp-r1-effective-temporal-sampling-with-reinforcement-fine-tuning-for-video-llms"><a href="https://arxiv.org/abs/2509.18056v1">TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</a></h2>
<p><strong>Authors:</strong> Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâTempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMsâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼TempSamp-R1ï¼åºäºå¼ºåå­¦ä¹ å¾®è°çè§é¢LLMæææ¶é´éæ ·</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨è§é¢æ¶é´å®ä½ä»»å¡ä¸­é¢ä¸´çææãç°æçå¼ºåå­¦ä¹ ï¼RLï¼æ¹æ³ï¼å¦Group Relative Policy Optimization (GRPO)ï¼ä¸»è¦ä¾èµäºâon-policyâéæ ·è¿è¡ç­ç¥æ´æ°ãç¶èï¼å¨æ¶é´æç´¢ç©ºé´å·¨å¤§çä»»å¡ä¸­ï¼è¿ç§ç­ç¥æçä½ä¸ä¸æ§è½æéï¼å ä¸ºå®å¾å¾é¾ä»¥è¯å«æ¶é´ä¸ç²¾ç¡®çè§£å³æ¹æ¡ãè¿å¯¼è´äºå¨ç¨ççç£ä¸å­¦ä¹ ä¸ç¨³å®ãæ¶æå°æ¬¡ä¼è§£ä»¥åå¨å¤æè§é¢çè§£ä»»å¡ä¸­æ³åçå°é¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
TempSamp-R1å¼å¥äºä¸ä¸ªæ°çå¼ºåå­¦ä¹ å¾®è°æ¡æ¶ï¼éè¿ä»¥ä¸åæ°ç¹è§£å³äºä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>æ··åç­ç¥éæ ·ä¸off-policyæå¯¼ï¼</strong> TempSamp-R1å°é«è´¨éçå¤é¨è§£å³æ¹æ¡ï¼ä¾å¦ï¼ground-truthæ æ³¨ï¼ä½ä¸ºoff-policyçç£å¼å¥ç­ç¥ä¼åè¿ç¨ãè¿æä¾äºæ¶é´ä¸ç²¾ç¡®çæå¯¼ï¼ææå¼¥è¡¥äºon-policyè§£å³æ¹æ¡ä¸­å¸¸è§çç¨çæ§åéä½é®é¢ï¼ä»èæé«äºç­ç¥ä¼åçç¨³å®æ§åæçã</li>
<li><strong>éçº¿æ§è½¯ä¼å¿è®¡ç®ï¼</strong> ä¸ºäºè¿ä¸æ­¥ç¨³å®è®­ç»å¹¶åå°åºäºå¥å±æ´æ°çæ¹å·®ï¼TempSamp-R1æåºäºä¸ç§éçº¿æ§è½¯ä¼å¿è®¡ç®æ¹æ³ãè¯¥æ¹æ³éè¿ä¸å¯¹ç§°åæ¢å¨æéå¡å¥å±åé¦ï¼åºåé«å¥å±åä½å¥å±è§£å³æ¹æ¡çå­¦ä¹ å¨æï¼åç¼©æ¥è¿æä¼è§£å³æ¹æ¡çä¼å¿å¼ï¼å¹¶æ¾å¤§æ¬¡ä¼è§£å³æ¹æ¡ä¹é´çç¸å¯¹å¥å±å·®è·ï¼ä»èçææ´å·ä¿¡æ¯éçæ¢¯åº¦å¹¶ä¿è¿ç¨³å®çç­ç¥ä¼åã</li>
<li><strong>æ··åæç»´é¾ï¼CoTï¼è®­ç»èå¼ï¼</strong> TempSamp-R1éç¨æ··åCoTè®­ç»èå¼ï¼ä¼åä¸ä¸ªç»ä¸æ¨¡åä»¥æ¯æCoTåéCoTæ¨çæ¨¡å¼ãè¿ä½¿å¾æ¨¡åè½å¤é«æå¤çä¸åæ¨çå¤æåº¦çæ¥è¯¢ï¼åæ¶å¨ä¸¤ç§æ¨¡å¼ä¸é½è¡¨ç°åºé²æ£æ§è½ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªç»æè¡¨æï¼TempSamp-R1å¨å¤ä¸ªåºåæ°æ®éä¸è¶è¶äºåºäºGRPOçåºçº¿ï¼å¹¶å»ºç«äºæ°çæåè¿æ§è½ï¼</p>
<ul>
<li><strong>Charades-STAï¼</strong> R1@0.7è¾¾å°52.9%ï¼ç¸å¯¹GRPOæå2.7%ã</li>
<li><strong>ActivityNet Captionsï¼</strong> R1@0.5è¾¾å°56.0%ï¼ç¸å¯¹GRPOæå5.3%ã</li>
<li><strong>QVHighlightsï¼</strong> mAPè¾¾å°30.0%ï¼ç¸å¯¹GRPOæå3.0%ã</li>
</ul>
<p>æ­¤å¤ï¼TempSamp-R1å¨æéæ°æ®ä¸è¡¨ç°åºå¼ºå¤§çå°æ ·æ¬æ³åè½åï¼è¿å¸æ¾äºå¶å¨æ°æ®ç¨ç¼ºåºæ¯ä¸çå®ç¨æ§ãæ¶èç ç©¶è¿ä¸æ­¥è¯å®äºoff-policyçç£åè½¯ä¼å¿æ´å½¢ç­ç¥å¯¹æé«æ§è½åè®­ç»ç¨³å®æ§çéè¦æ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºææå°äºTempSamp-R1çå ä¸ªå±éæ§ï¼</p>
<ul>
<li><strong>ä¾èµé«è´¨éoff-policyçç£ï¼</strong> è¯¥æ¡æ¶ç®åä¾èµäºé«è´¨éçoff-policyçç£ï¼ä¾å¦ï¼ground-truthæ¶é´æ³ï¼ï¼è¿å¨å¼±æ æ³¨åºæ¯ä¸­å¯è½æ æ³è·å¾ã</li>
<li><strong>æªæ¢ç´¢å¶ä»è§é¢æ¨çä»»å¡ï¼</strong> å°½ç®¡TempSamp-R1å¨æ¶é´å®ä½åé«åæ£æµä»»å¡ä¸è¿è¡äºè¯ä¼°ï¼ä½å¶å¨å¶ä»è§é¢æ¨çä»»å¡ï¼ä¾å¦ï¼å¤äºä»¶è·è¸ªï¼ä¸çæææ§ä»æå¾æ¢ç´¢ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è½ç¶è®ºææ²¡ææç¡®ååºæªæ¥ç ç©¶æ¹åï¼ä½ä»å¶å±éæ§å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼</p>
<ul>
<li><strong>å¨å¼±çç£ææ çç£åºæ¯ä¸çåºç¨ï¼</strong> æ¢ç´¢TempSamp-R1å¨æ²¡æé«è´¨éground-truthæ æ³¨çæåµä¸å¦ä½å·¥ä½ï¼ä¾å¦éè¿èªçç£æå¼±çç£å­¦ä¹ æ¥è·åoff-policyæå¯¼ã</li>
<li><strong>æ©å±å°æ´å¹¿æ³çè§é¢æ¨çä»»å¡ï¼</strong> å°TempSamp-R1åºç¨äºå¶ä»å¤æçè§é¢çè§£ä»»å¡ï¼å¦å¤äºä»¶è·è¸ªãè§é¢é®ç­ç­ï¼ä»¥éªè¯å¶éç¨æ§åé²æ£æ§ã</li>
<li><strong>è¿ä¸æ­¥ä¼åoff-policyæå¯¼çè·åï¼</strong> ç ç©¶æ´æºè½ãæ´èªéåºçæ¹æ³æ¥çææéæ©off-policyè§£å³æ¹æ¡ï¼ä»¥åå°å¯¹å¤é¨æ æ³¨çä¾èµã</li>
<li><strong>æ¢ç´¢æ´å¤æçå¥å±æ´å½¢æºå¶ï¼</strong> è¿ä¸æ­¥ç ç©¶éçº¿æ§å¥å±æ´å½¢ï¼ä»¥éåºæ´å¹¿æ³çä»»å¡åæ°æ®åå¸ï¼ä»èå®ç°æ´ç²¾ç»çç­ç¥ä¼åã</li>
</ul>
<p>æ»èè¨ä¹ï¼TempSamp-R1éè¿åæ°çæ··åç­ç¥éæ ·ãéçº¿æ§è½¯ä¼å¿è®¡ç®åæ··åCoTè®­ç»èå¼ï¼æ¾èæåäºMLLMså¨è§é¢æ¶é´å®ä½ä»»å¡ä¸­çæ§è½åç¨³å®æ§ï¼ä¸ºé¿è§é¢çè§£é¢åçå¼ºåå­¦ä¹ å¾®è°å¼è¾äºæ°çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks.</li>
<li>Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.18056v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.18056v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17941v1'></a></p>
<h2 id="composablenav-instruction-following-navigation-in-dynamic-environments-via-composable-diffusion"><a href="https://arxiv.org/abs/2509.17941v1">ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</a></h2>
<p><strong>Authors:</strong> Zichao Hu, Chen Tang, Michael J. Munje, Yifeng Zhu, Alex Liu, Shuijing Liu, Garrett Warnell, Peter Stone, Joydeep Biswas</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>This paper considers the problem of enabling robots to navigate dynamic
environments while following instructions. The challenge lies in the
combinatorial nature of instruction specifications: each instruction can
include multiple specifications, and the number of possible specification
combinations grows exponentially as the robot's skill set expands. For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.
Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training. Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives. Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines. Videos and additional materials can
be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zichao Huç­äººæ°åçè®ºæâComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="composablenav-instruction-following-navigation-in-dynamic-environments-via-composable-diffusion_1">è®ºææè¦ï¼ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººå¨å¨æç¯å¢ä¸­éµå¾ªå¤ææä»¤è¿è¡å¯¼èªçé®é¢ãæ ¸å¿ææå¨äºæä»¤è§èçç»åæ§è´¨ï¼ä¸ä¸ªæä»¤å¯è½åå«å¤ä¸ªå­è§èï¼ä¾å¦ï¼âè¶è½¦è¡äººå¹¶é å³è¡é©¶âåå«âè¶è½¦è¡äººâåâé å³è¡é©¶âä¸¤ä¸ªè§èï¼ï¼éçæºå¨äººæè½éçæ©å±ï¼å¯è½çè§èç»åæ°éåææ°çº§å¢é¿ï¼è¿ä½¿å¾ä¼ ç»çåºäºå­¦ä¹ çæ¹æ³ï¼å¦æ¨¡ä»¿å­¦ä¹ æå¼ºåå­¦ä¹ ï¼å æ°æ®åè®¡ç®èµæºéæ±å·¨å¤§èåå¾ä¸åå®éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºåºå¯¹ä¸è¿°ææï¼ComposableNavæåºäºä»¥ä¸å³é®åæ°ï¼
*   <strong>ç»åå¼æ©æ£æ¨¡åï¼Composable Diffusion Modelsï¼ï¼</strong> è®ºæçæ ¸å¿ææ³æ¯ï¼éµå¾ªæä»¤æ¶åç¬ç«æ»¡è¶³å¶ç»æè§èï¼æ¯ä¸ªè§èå¯¹åºä¸ä¸ªç¬ç¹çè¿å¨åè¯­ãComposableNavå©ç¨æ©æ£æ¨¡åçå¯ç»åæ§ï¼åç¬å­¦ä¹ æ¯ä¸ªè¿å¨åè¯­ï¼ç¶åå¨é¨ç½²æ¶å¹¶è¡ç»åè¿äºåè¯­ï¼ä»¥æ»¡è¶³è®­ç»ä¸­æªè§è¿çè§èç»åã
*   <strong>ä¸¤é¶æ®µè®­ç»ç¨åºï¼</strong> ä¸ºäºé¿åå¯¹æ¯ä¸ªåç¬è¿å¨åè¯­è¿è¡æ¼ç¤ºçç¹ééæ±ï¼è®ºææåºäºä¸ç§ä¸¤é¶æ®µè®­ç»æ¹æ³ï¼
    1.  <strong>çç£å¼é¢è®­ç»ï¼Supervised Pre-trainingï¼ï¼</strong> å­¦ä¹ ä¸ä¸ªç¨äºå¨æå¯¼èªçåºç¡æ©æ£æ¨¡åï¼çæå¤æ ·åãæ ç¢°æãç®æ å¯¼åçè½¨è¿¹ã
    2.  <strong>å¼ºåå­¦ä¹ å¾®è°ï¼Reinforcement Learning Fine-tuningï¼ï¼</strong> å°åºç¡æ¨¡åå¡é æä¸åçè¿å¨åè¯­ï¼éè¿è®¾è®¡ç¹å®äºåè¯­çå¥å±å½æ°æ¥ç¡®ä¿æä»¤ä¾ä»æ§ï¼ä»èé¿åäºå¯¹ç¹å®æ¼ç¤ºæ°æ®éçéæ±ã
*   <strong>å®æ¶é¨ç½²æºå¶ï¼</strong> ç»åæ¨¡åé¢æµæ§å¶å¨ï¼MPCï¼åå¨çº¿éè§åç­ç¥ï¼ç¡®ä¿äºComposableNavå¨çå®ä¸çæºå¨äººä¸çå®æ¶æ§è½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
éè¿ä»¿çåçå®ä¸çå®éªï¼ComposableNavå±ç¤ºäºæ¾èçæ§è½ä¼å¿ï¼
*   <strong>åè¶çæ³åè½åï¼</strong> ComposableNavè½å¤çææ»¡è¶³å¤æ ·åä¸è®­ç»ä¸­æªè§è¿çè§èç»åçè½¨è¿¹ï¼æ¾èä¼äºéç»åå¼VLMï¼è§è§è¯­è¨æ¨¡åï¼ç­ç¥ååºäºææ¬å¾ç»åçåºçº¿æ¹æ³ã
*   <strong>å¤çå¤ææä»¤ï¼</strong> éçæä»¤å¤ææ§ï¼è§èæ°éï¼çå¢å ï¼ComposableNavçæåçä¿æè¾é«ï¼èææåºçº¿æ¹æ³çæ§è½åè¿éä¸éï¼è¿è¯æäºå¶å¨å¤çå¤æãæªè§è¿çæä»¤ç»åæ¹é¢çé²æ£æ§ã
*   <strong>å®æ¶æä½ï¼</strong> å¨çå®ä¸çæºå¨äººä¸é¨ç½²æ¶ï¼ComposableNavå®ç°äºå®æ¶éè§åï¼å³ä½¿å¨æ¶ååä¸ªåè¯­çæå¤ææåµä¸ï¼åå§è§åå¹³åä»é0.4ç§ï¼éè§åä»é0.06ç§ã
*   <strong>æ éç¹å®æ¼ç¤ºæ°æ®ï¼</strong> ä¸¤é¶æ®µè®­ç»æ¹æ³æåå°ä½¿æ©æ£æ¨¡åå­¦ä¹ äºææçè¿å¨åè¯­ï¼èæ éä¸ºæ¯ä¸ªåè¯­æä¾ä¸é¨çæ¼ç¤ºæ°æ®ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è¿å¨åè¯­æ°éæéï¼</strong> ç®ååªèèäºå­ç§å¸¸ç¨çå¯¼èªè¿å¨åè¯­ï¼è¿äºåè¯­ç¸å¯¹ç®åï¼å¯ä»¥ç¨åºäºè§åçå¥å±å½æ°æè¿°ãæå¨è®¾è®¡å¥å±å½æ°çå¯æ©å±æ§æéã
*   <strong>ä¾èµä¸æ¸¸æ¨¡åï¼</strong> è®ºæåè®¾æä»¤è§£æä¸ºè§èåç¸å³ç¯å¢è§æµï¼ä¾å¦ï¼éè¿LLMåVLMï¼å¯ä»¥ç±ç°ææ¹æ³å¤çï¼è¿é¨åå·¥ä½å¨å®éªä¸­è¢«æ½è±¡åã
*   <strong>ç»åç­ç¥çå±éæ§ï¼</strong> å°½ç®¡ComposableNavè¡¨ç°åºè²ï¼ä½éçæä»¤è§èæ°éçå¢å ï¼æåçä»æä¸éãè¿å¯è½æºäºå½åç»åç­ç¥ï¼éè¿å¯¹åä¸ªå»åªç½ç»çé¢æµåªå£°æ±åï¼å¯è½å¯¼è´æ¬¡ä¼ç»æã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å©ç¨VLMä½ä¸ºéªè¯å¨ï¼</strong> æ¢ç´¢ä½¿ç¨VLMä½ä¸ºéªè¯å¨æ¥èªå¨å­¦ä¹ å¤æ ·ååå¤æçè¡ä¸ºï¼ä»¥æé«å¥å±å½æ°è®¾è®¡çå¯æ©å±æ§ã
*   <strong>éæé«å±VLMæ¨¡ååä»»å¡è§åå¨ï¼</strong> å°ComposableNavä¸é«å±VLMæ¨¡ååä»»å¡è§åå¨ç»åï¼ä»¥å®ç°æ´é¿æçæä»¤éµå¾ªå¯¼èªã
*   <strong>æ¹è¿ç»åéæ ·ææ¯ï¼</strong> æ¢ç´¢æ´åè¿çæ©æ£æ¨¡åéæ ·ææ¯ï¼å¦åå¯é¡¿èç¹å¡æ´ï¼ï¼ä»¥æé«å¨æ´é«æä»¤å¤ææ§ä¸çç»åæ§è½ã</p>
<hr />
<p>æ»èè¨ä¹ï¼ComposableNavéè¿å¼å¥å¯ç»åæ©æ£æ¨¡åååæ°çä¸¤é¶æ®µè®­ç»æµç¨ï¼ä¸ºæºå¨äººå¨å¨æç¯å¢ä¸­éµå¾ªå¤ææä»¤å¯¼èªæä¾äºä¸ä¸ªå¼ºå¤§ä¸å¯æ©å±çè§£å³æ¹æ¡ï¼æ¾èæ¨å¨äºè¯¥é¢åçç ç©¶è¿å±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.</li>
<li>Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training.</li>
<li>Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives.</li>
<li>Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17941v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17941v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17901v1'></a></p>
<h2 id="does-audio-matter-for-modern-video-llms-and-their-benchmarks"><a href="https://arxiv.org/abs/2509.17901v1">Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</a></h2>
<p><strong>Authors:</strong> Geewook Kim, Minjoon Seo</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV, cs.MM, cs.SD</p>
<p><strong>Abstract:</strong></p>
<p>Modern multimodal large language models often claim "video understanding,"
yet most evaluations use muted videos or simply discard audio. We ask a direct
question: how much does audio actually matter for contemporary Video-LLMs and
the benchmarks that certify them? We audit widely used suites and observe that
many items are even solvable from a single frame, rendering audio largely
redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio
encoder (e.g., Whisper) and analyze when audio helps, while addressing audio
token explosion with a lightweight Mamba-based state-space token compressor. We
find that audio yields minimal gains on recent video benchmarks but is decisive
on curated, audio-sensitive subsets. To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Geewook KimåMinjoon Seoæ°åçè®ºæâDoes Audio Matter for Modern Video-LLMs and Their Benchmarks?âçå¨é¢æè¦ã</p>
<hr />
<h3 id="does-audio-matter-for-modern-video-llms-and-their-benchmarks_1">è®ºææè¦ï¼Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºæçæ ¸å¿ç ç©¶é®é¢æ¯ï¼å¨å½åçè§é¢å¤§åè¯­è¨æ¨¡åï¼Video-LLMsï¼åå¶è¯ä¼°åºåä¸­ï¼é³é¢ç©¶ç«æå¤éè¦ï¼ä½èæåºï¼å°½ç®¡ç°ä»£å¤æ¨¡æå¤§åè¯­è¨æ¨¡åå£°ç§°å·å¤âè§é¢çè§£âè½åï¼ä½å¤§å¤æ°è¯ä¼°å´ä½¿ç¨éé³è§é¢æç´æ¥å¿½ç¥é³é¢ãè¿å¼åäºä¸ä¸ªå³é®çé®ï¼è¿ç§åæ³æ¯å¦æ©çäºé³é¢å¨çå®è§é¢çè§£ä¸­çæ½å¨ä»·å¼ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼è®ºææåºäºä»¥ä¸å³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼</p>
<ul>
<li><strong>é³é¢ææåºåçå®¡è®¡ä¸ç­å±ï¼</strong> ä½èå®¡è®¡äºå¹¿æ³ä½¿ç¨çè§é¢çè§£åºåå¥ä»¶ï¼åç°è®¸å¤ä»»å¡ä»å­åå¸§è§è§ä¿¡æ¯å³å¯è§£å³ï¼ä½¿å¾é³é¢ä¿¡æ¯å¨å¾å¤§ç¨åº¦ä¸æ¯åä½çãä¸ºäºè§£å³è¿ä¸é®é¢ï¼è®ºæéè¿è¿æ»¤æå¯ä»¥ä»å­åå¸§è§£å³çé¡¹ç®ï¼ç­å±å¹¶åå¸äºä¸¤ä¸ªæ°çãæ´å·æææ§çãé³é¢ææçåºåæ°æ®éï¼<strong>AVQA-Hard</strong> å <strong>Music-AVQA-Hard</strong>ã</li>
<li><strong>LLaVA-AV-SSM æ¨¡åæ¶æï¼</strong> è®ºæåºäºå¼ºå¤§ç LLaVA-OneVision æ¶æï¼éè¿éå ä¸ä¸ªè¯­é³/é³é¢ç¼ç å¨ï¼ä¾å¦ Whisperï¼æ¥æ³¨å¥é³é¢ tokenï¼ä»èæå»ºäº <strong>LLaVA-AV-SSM</strong> æ¨¡åãè¿ç§åç¼ç å¨æ¶æåè®¸å¯¹é³é¢çè¾¹éä»·å¼è¿è¡åæ§ç ç©¶ã</li>
<li><strong>åºäº Mamba çé³é¢ token åç¼©å¨ï¼</strong> éå¯¹é³é¢ token æ°éçç¸çé®é¢ï¼ä¾å¦ï¼ä¸å°æ¶è§é¢å¯è½äº§ççº¦ 90k ä¸ªé³é¢ tokenï¼ï¼è®ºæå¼å¥äºä¸ç§è½»éçº§çãåºäº Mamba çç¶æç©ºé´æ¨¡åï¼SSMï¼token åç¼©å¨ãè¯¥åç¼©å¨è½å¤å°é¿é³é¢æµèåä¸ºç´§åç token éåï¼åæ¶æå¤§éåº¦å°åå°ä¿¡æ¯æå¤±ï¼ä»èå®ç°äºå¯æ©å±çé³è§é¢ Video-LLMsãå®éè¿å¨æ¯ R æ­¥æå¥ä¸ä¸ªå¯è®­ç»æ¥è¯¢å¹¶ä»ä¿çæ¥è¯¢ä½ç½®çè¾åºæ¥å®ç° 25 åçåç¼©ï¼ä» 25Hz éè³ 1Hzï¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæçä¸»è¦åç°åå¶æä¹å¦ä¸ï¼</p>
<ul>
<li><strong>é³é¢å¨ç°æåºåä¸­çä½ç¨æéï¼</strong> å¨å¤§å¤æ°ç°æè§é¢åºåä¸ï¼æ·»å é³é¢å¸¦æ¥çæ§è½æåå¾®ä¹å¶å¾®ï¼çè³å¨æ¹å·®èå´åï¼è¿ä¸ä½èçå®¡è®¡ç»æä¸è´ï¼å³è¿äºåºåå¾å°éè¦é³è½¨ä¿¡æ¯ã</li>
<li><strong>é³é¢å¨ç­å±åºåä¸­çå³å®æ§ä½ç¨ï¼</strong> å¨æ°ç­å±ç AVQA-Hard å Music-AVQA-Hard ç­é³é¢ææå­éä¸ï¼é³é¢å¸¦æ¥äºæ¾èçæ§è½æåãä¾å¦ï¼å¨ AVQA-Hard ä¸ï¼åç¡®çä» 67.13% æé«å° 71.58%ãè¿éªè¯äºå¯¹é³é¢ææè¯ä¼°çéæ±ï¼å¹¶å¼ºè°äºå¨è¿äºä»»å¡ä¸­âå¾å¬âçéè¦æ§ã</li>
<li><strong>Mamba åç¼©å¨çæææ§ï¼</strong> åºäº Mamba çé³é¢ token åç¼©å¨ä¸ä»ææè§£å³äºé³é¢ token çç¸é®é¢ï¼å®ç°äºé¿è§é¢ççº¿æ§æ¶é´æ¨çï¼èä¸å¨é³é¢ææåºåä¸æåäºåç¡®çãè¿è¡¨æ Mamba åç¼©å¨æ¯å®ç°å¯æ©å±é³è§é¢ Video-LLMs çææå·¥å·ã</li>
<li><strong>å­¦æ¯å®è·µä¸ç°å®ä¸çææçå·®è·ï¼</strong> è®ºæçåç°æ­ç¤ºäºå½åå­¦æ¯å®è·µï¼å¿½ç¥é³é¢ï¼ä¸ç°å®ä¸çææï¼ç¨æ·èªç¶å°åè®¾ç³»ç»æ¢âçâåâå¬âï¼ä¹é´æ¥çæ©å¤§çå·®è·ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æåçå±éæ§åæ¬ï¼</p>
<ul>
<li><strong>é³é¢ç¼ç å¨åæ°æ®çæ¢ç´¢èå´ï¼</strong> ç®åçç ç©¶ä¸»è¦ä½¿ç¨äº Qwen2-Audio ç Whisper-based ç¼ç å¨ï¼æªæ¥å¯ä»¥æ¢ç´¢æ´å¹¿æ³çé³é¢ç¼ç å¨åæ°æ®éã</li>
<li><strong>ç»ä¸çè·¨æ¨¡æ SSMï¼</strong> ç®åçé³é¢åç¼©å¨æ¯ç¬ç«çï¼æªæ¥å¯ä»¥æ©å±å°è¶è¶çº¯é³é¢åç¼©ï¼å®ç°ä¸ä¸ªç»ä¸çè·¨æ¨¡æ SSMï¼è½å¤èååéè§è§åé³é¢çé¢ç®ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æ½å¨çæªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li>æ¢ç´¢æ´å¹¿æ³çé³é¢ç¼ç å¨åè®­ç»æ°æ®ï¼ä»¥è¿ä¸æ­¥æåé³è§é¢ Video-LLMs çæ§è½ã</li>
<li>å¼åç»ä¸çè·¨æ¨¡æç¶æç©ºé´æ¨¡åï¼è½å¤æºè½å°å¨è§è§åé³é¢æ¨¡æä¹é´åéè®¡ç®èµæºï¼å®ç°æ´é«æãæ´å¨é¢çè§é¢çè§£ã</li>
<li>ç»§ç»­ç­å±åå¼åæ´å·æææ§çãçæ­£éè¦é³è§é¢èåæ¨ççåºåï¼ä»¥æ¨å¨è¯¥é¢åçåå±ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæå¯¹ç°ä»£ Video-LLMs ä¸­é³é¢çä½ç¨è¿è¡äºæ·±å¥çæ¹å¤æ§å®¡è§ãéè¿å¼å¥é³é¢ææçåºåååºäº Mamba çé«æé³é¢åç¼©æ¹æ³ï¼ä½èä¸ä»æ­ç¤ºäºå½åè¯ä¼°å®è·µçä¸è¶³ï¼è¿ä¸ºæå»ºæ´ç¬¦åç°å®ä¸çéæ±çãè½å¤çæ­£âçâåâå¬âçé³è§é¢å¤§åè¯­è¨æ¨¡åæä¾äºå®ç¨çå·¥å·åæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code.</li>
<li>Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17901v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17901v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17877v1'></a></p>
<h2 id="sight-over-site-perception-aware-reinforcement-learning-for-efficient-robotic-inspection"><a href="https://arxiv.org/abs/2509.17877v1">Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</a></h2>
<p><strong>Authors:</strong> Richard Kuhlmann, Jakob Wolfram, Boyang Sun, Jiaxu Xing, Davide Scaramuzza, Marc Pollefeys, Cesar Cadena</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous inspection is a central problem in robotics, with applications
ranging from industrial monitoring to search-and-rescue. Traditionally,
inspection has often been reduced to navigation tasks, where the objective is
to reach a predefined location while avoiding obstacles. However, this
formulation captures only part of the real inspection problem. In real-world
environments, the inspection targets may become visible well before their exact
coordinates are reached, making further movement both redundant and
inefficient. What matters more for inspection is not simply arriving at the
target's position, but positioning the robot at a viewpoint from which the
target becomes observable. In this work, we revisit inspection from a
perception-aware perspective. We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map. The learned policy
leverages both perceptual and proprioceptive sensing and is trained entirely in
simulation, before being deployed to a real-world robot. We further develop an
algorithm to compute ground-truth shortest inspection paths, which provides a
reference for evaluation. Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings. The project is avialable at
https://sight-over-site.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Richard Kuhlmannç­äººæ°åçè®ºæâSight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspectionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="sight-over-site-perception-aware-reinforcement-learning-for-efficient-robotic-inspection_1">è®ºææè¦ï¼Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ä¼ ç»çæºå¨äººèªä¸»å·¡æ£ä»»å¡éå¸¸è¢«ç®åä¸ºå¯¼èªé®é¢ï¼å³æºå¨äººéè¦å°è¾¾é¢å®ä¹çç©çä½ç½®å¹¶é¿å¼éç¢ç©ãç¶èï¼è¿ç§æ¹æ³æªè½ååææçå®å·¡æ£ä»»å¡çæ¬è´¨ãå¨å®éç¯å¢ä¸­ï¼å·¡æ£ç®æ å¯è½å¨æºå¨äººå°è¾¾å¶ç²¾ç¡®åæ ä¹åå°±å·²ç»å¯è§ï¼å¯¼è´åç»­ç§»å¨åä½ä¸ä½æãå æ­¤ï¼è®ºææ¨å¨è§£å³çæ ¸å¿é®é¢æ¯ï¼å¦ä½ä½¿æºå¨äººè½å¤ä»ä¸ä¸ªâæç¥æç¥âçè§åº¦è¿è¡å·¡æ£ï¼å³ä¸ä»ä»æ¯å°è¾¾ç®æ ä½ç½®ï¼èæ¯å°æºå¨äººå®ä½å¨ä¸ä¸ªè½å¤æ¸æ°è§å¯å°ç®æ çè§ç¹ï¼å¹¶ä»¥æç­è·¯å¾å®ç°è¿ä¸ç®æ ï¼åæ¶ä¸ä¾èµäºé¢åæå»ºçå°å¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>æç¥æç¥å·¡æ£ä»»å¡çéæ°å®ä¹ä¸æ¡æ¶ï¼</strong> è®ºæå°å·¡æ£ä»»å¡éæ°å®ä¹ä¸ºä»¥ç®æ å¯è§æ§ä¸ºæ ¸å¿ï¼èéç®åçç¹ç®æ å¯¼èªãä¸ºæ­¤ï¼ä½èæåºäºä¸ä¸ªç«¯å°ç«¯çå¼ºåå­¦ä¹ ï¼RLï¼æ¡æ¶ï¼å°ç®æ å¯è§æ§æç¡®å°ä½ä¸ºä¸»è¦ä¼åç®æ ã
*   <strong>æ å°å¾çRLç­ç¥ï¼</strong> æåºçç­ç¥ä¸ä¾èµäºå¨å±å°å¾ï¼èæ¯å©ç¨èªæä¸­å¿æ·±åº¦å¾åè¾å¥ãæºå¨äººä¸ç®æ ä¹é´çç¸å¯¹å§¿æä»¥åè¿å»çå¨ä½æ¥å­¦ä¹ é«æçå·¡æ£è¡ä¸ºãè¿ç§æ¹æ³é¿åäºæ¾å¼å°å¾æå»ºçå¼éï¼ä½¿å¶è½å¤æ©å±å°å¤§åç¯å¢ã
*   <strong>æ¨¡æå°çå®ä¸ççæ³åè½åï¼</strong> ç­ç¥å®å¨å¨æ¨¡æç¯å¢ä¸­è¿è¡è®­ç»ï¼ä½è½å¤å¾å¥½å°æ³åå°æªè§çæ¨¡æç¯å¢ä»¥åçå®çåè¶³æºå¨äººï¼Boston Dynamics Spotï¼ï¼å®ç°äºé¶æ ·æ¬è¿ç§»ã
*   <strong>å°é¢çå®æç­å·¡æ£è·¯å¾ç®æ³ï¼</strong> è®ºæå¼åäºä¸ç§ç®æ³æ¥è®¡ç®å°é¢çå®çæç­å·¡æ£è·¯å¾ï¼ä¸ºè¯ä¼°æä¾äºå¯é çåèåºåãè¯¥ç®æ³èèäºå¯éåæ§ãä¼ æå¨èå´åå¯è§æ§çº¦æï¼éè¿A<em>æç´¢æ¾å°ä»èµ·å§ç¹å°ç®æ å¯è§è§ç¹çæç­è·¯å¾ã
*   </em><em>å¥å±å½æ°è®¾è®¡ï¼</em>* å¥å±å½æ°è¢«ç²¾å¿è®¾è®¡ï¼åå«ç¨ççæå/å¤±è´¥å¥å±åå¯éçå¥å±é¡¹ï¼å¦ç®æ æ¹åå¯¹é½ãåæä¼è§ç¹ç§»å¨ãæ©ç½åå°æè½¬åé¼å±æ¢ç´¢ï¼ï¼ä»¥å¹³è¡¡å¯¼èªåå¯è§æ§ï¼å¹¶é¿åå±é¨æä¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ä¼äºç°æå¯¼èªæ¹æ³ï¼</strong> å®éªç»æè¡¨æï¼è¯¥æ¹æ³å¨æ¨¡æåçå®ä¸çç¯å¢ä¸­åä¼äºç°æçç»å¸ååºäºå­¦ä¹ çå¯¼èªæ¹æ³ï¼è½å¤çææ´é«æçå·¡æ£è½¨è¿¹ã
*   <strong>æ´é«çæåå æè·¯å¾é¿åº¦ï¼SPLï¼ï¼</strong> å°½ç®¡å¨æäºç¢°æåæ»è¡åè®¸çè®¾ç½®ä¸ï¼DD-PPOçæåçå¯è½ç¥é«ï¼ä½æ¬æ¹æ³å¨SPLææ ä¸è¡¨ç°æ´ä¼ï¼è¿æå³çå®è½ä»¥æ´ç­çè·¯å¾å®ç°ç®æ å¯è§æ§ã
*   <strong>å¼ºå¤§çç¢°æé¿åè½ååé²æ£æ§ï¼</strong> å¨ä¸¥æ ¼çæ ç¢°æè®¾ç½®ä¸ï¼æ¬æ¹æ³ä»è½ä¿æé«æåçï¼81.49%ï¼ï¼å¹¶å±ç°åºå¼ºå¤§çç¢°æé¿åè½åãä¸DD-PPOç¸æ¯ï¼æ¬ç­ç¥å¨ä¸åç¯å¢è®¾ç½®ä¸è¡¨ç°åºæ´å¥½çé²æ£æ§ï¼ä¸æåæ¨¡æå¨ç¹å®å¨æçå½±åã
*   <strong>å¹³è¡¡å¯¼èªä¸å¯è§æ§ï¼</strong> ç­ç¥è½å¤ææå°å¹³è¡¡å¯¼èªåå¯è§æ§ï¼å¨ç®æ è¾è¿æ¶ä¼åå¯¼èªï¼å¨ç®æ éè¿æ¶åè½¬åå¯»æ¾æä½³è§ç¹ï¼ä»èé¿åäºåä½ç§»å¨ã
*   <strong>æåçSim-to-Realè¿ç§»ï¼</strong> å¨Boston Dynamics Spotæºå¨äººä¸ççå®ä¸çå®éªéªè¯äºç­ç¥çé¶æ ·æ¬è¿ç§»è½åï¼è¯æäºå¶å¨å®éåºç¨ä¸­çæææ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>å°é¢çå®è·¯å¾è®¡ç®çæçé®é¢ï¼</strong> å¨RLè®­ç»è¿ç¨ä¸­ï¼éå¤è®¡ç®å°é¢çå®æç­å·¡æ£è·¯å¾ï¼åæ¬å¯è§æ§æ£æ¥ï¼æçä½ä¸ï¼å æ­¤å¨è®­ç»æ¶éç¨äºåºäºå¥å±å½æ°çè®¾è®¡èéç´æ¥ä½¿ç¨å°é¢çå®è·¯å¾ä½ä¸ºå¥å±ä¿¡å·ã
*   <strong>ä¼ æå¨åªå£°ï¼</strong> çå®ä¸çå®éªä¸­ï¼æ·±åº¦ä¼ æå¨æµéå­å¨åªå£°ï¼éè¦éè¿å¹³åæ·±åº¦å¼ç­æ¹æ³æ¥ç¼è§£å¶å¯¹å¯è§æ§å¤æ­çå½±åã
*   <strong>è®­ç»ç¯å¢çå¤ææ§ï¼</strong> å°½ç®¡å¨Habitatæ¨¡æå¨ä¸è¿è¡äºè®­ç»ï¼ä½çå®ä¸ççå¤ææ§åä¸å¯é¢æµæ§ä»å¯è½å¸¦æ¥ææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¤æçå·¡æ£åºæ¯ï¼</strong> æ¢ç´¢å¨æ´å¤æãå¨ææå¤§è§æ¨¡ç¯å¢ä¸­çå·¡æ£ä»»å¡ï¼ä¾å¦å¤ç®æ å·¡æ£æéè¦ä¸ç¯å¢äº¤äºçå·¡æ£ã
*   <strong>ç»åè¯­ä¹ä¿¡æ¯ï¼</strong> å°è¯­ä¹ä¿¡æ¯ï¼ä¾å¦ç®æ ç±»åãåè½åºåï¼èå¥æç¥æç¥å·¡æ£æ¡æ¶ï¼ä»¥å®ç°æ´æºè½ãæ´å·ä¸ä¸ææè¯çå·¡æ£è¡ä¸ºã
*   <strong>å¨çº¿éåºä¸æç»­å­¦ä¹ ï¼</strong> ç ç©¶å¦ä½å¨é¨ç½²åä½¿ç­ç¥è½å¤å¨çº¿éåºæ°çç¯å¢æç®æ ï¼å®ç°æç»­å­¦ä¹ åæ§è½æåã
*   <strong>å¤æ¨¡ææç¥èåï¼</strong> æ¢ç´¢é¤äºæ·±åº¦å¾åä¹å¤ï¼ç»åRGBå¾åãæ¿åé·è¾¾ç­å¤ç§ä¼ æå¨æ¨¡æï¼ä»¥è·åæ´ä¸°å¯ãæ´é²æ£çç¯å¢æç¥ã
*   <strong>å¯è§£éæ§ä¸å®å¨æ§ï¼</strong> è¿ä¸æ­¥æé«RLç­ç¥çå¯è§£éæ§ï¼å¹¶å¢å¼ºå¶å¨å®å¨å³é®å·¡æ£ä»»å¡ä¸­çå¯é æ§åå®å¨æ§ä¿éã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map.</li>
<li>Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17877v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17877v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17786v1'></a></p>
<h2 id="accurate-and-efficient-low-rank-model-merging-in-core-space"><a href="https://arxiv.org/abs/2509.17786v1">Accurate and Efficient Low-Rank Model Merging in Core Space</a></h2>
<p><strong>Authors:</strong> Aniello Panariello, Daniel Marczak, Simone Magistri, Angelo Porrello, BartÅomiej Twardowski, Andrew D. Bagdanov, Simone Calderara, Joost van de Weijer</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we address the challenges associated with merging low-rank
adaptations of large neural networks. With the rise of parameter-efficient
adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning
has become more accessible. While fine-tuning models with LoRA is highly
efficient, existing merging methods often sacrifice this efficiency by merging
fully-sized weight matrices. We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks. We further provide a formal proof that
projection into Core Space ensures no loss of information and provide a
complexity analysis showing the efficiency gains. Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources. Codebase is available at
https://github.com/apanariello4/core-space-merging.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Aniello Panarielloç­äººæ°åçè®ºæâAccurate and Efficient Low-Rank Model Merging in Core Spaceâçå¨é¢æè¦ã</p>
<hr />
<h3 id="accurate-and-efficient-low-rank-model-merging-in-core-space_1">è®ºææè¦ï¼Accurate and Efficient Low-Rank Model Merging in Core Space</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤§åç¥ç»ç½ç»çä½ç§©éåºï¼å¦LoRAï¼æ¨¡ååå¹¶æé¢ä¸´çææãå°½ç®¡LoRAå¾®è°æ¬èº«æçå¾é«ï¼ä½ç°æçæ¨¡ååå¹¶æ¹æ³éå¸¸éè¿åå¹¶å¨å°ºå¯¸æéç©éµæ¥çºç²è¿ç§æçï¼å¯¼è´è®¡ç®ææ¬é«æä¸æ æ³æ©å±å°å¤§åæ¨¡åãæ ¸å¿é®é¢æ¯å¦ä½å¨ä¿æä½ç§©éåºæççåæ¶ï¼ææä¸åç¡®å°åå¹¶è¿äºæ¨¡åï¼ä»¥å¤çå¤ä»»å¡å­¦ä¹ åºæ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªåä¸ºâCore Spaceâçåæ°åå¹¶æ¡æ¶ï¼å¶æ ¸å¿è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>Core Space åå¹¶æ¡æ¶ï¼</strong> å¼å¥äºä¸ä¸ªåæ°é«æçå­ç©ºé´ââCore Spaceï¼ä½ä¸ºææä»»å¡ç¹å®ä½ç§©ç»ä»¶çéç¨å¯¹é½åºç¡ãå®åè®¸å¨ä¿æä½ç§©éåºæççåæ¶ï¼å¨æ´å°çç»´åº¦ç©ºé´ï¼r x rï¼ä¸­æ§è¡æ¨¡ååå¹¶ï¼èæ ééå»ºå¨å°ºå¯¸æéç©éµã</li>
<li><strong>ä¿¡æ¯æ ææå½±ï¼</strong> è®ºææä¾äºæ­£å¼çè¯æï¼è¡¨ææå½±å°Core Spaceå¹¶è¿ååå§ç©ºé´æ¯æ æçï¼ç¡®ä¿å¨åå¹¶è¿ç¨ä¸­ä¸ä¼ä¸¢å¤±ä¿¡æ¯ãCore Spaceè¢«è®¾è®¡ä¸ºå¯éçï¼å¹¶ä¸å¶ç»´åº¦ä»åå³äºä»»å¡æ°éåLoRAç§©ï¼ä¸åºç¡æ¨¡åå¤§å°æ å³ã</li>
<li><strong>æçæåï¼</strong> éè¿å¨Core Spaceä¸­è¿è¡åå¹¶ï¼è®ºææ¾èéä½äºè®¡ç®ææ¬ãä¸ç°ææ¹æ³ï¼å¦KnOTSï¼ç¸æ¯ï¼Core Spaceåå¹¶å¨è®¡ç®èµæºä½¿ç¨ä¸å®ç°äºæ¾èçå éï¼ä¾å¦ï¼å¯¹äºLlama 3 8Bæ¨¡åï¼éåº¦æåè¶è¿600åï¼ã</li>
<li><strong>å­ç©ºé´å¯¹é½æ¹è¿ï¼</strong> Core Spaceéè¿å¼ºå¶ææä»»å¡å±äº«ä¸ä¸ªå¬å±åºç¡æ¥æé«å­ç©ºé´å¯¹é½æ¯çï¼SARï¼ï¼è¿æ»¤æä»»å¡ç¹å®åªå£°å¹¶ä¿è¿å¯¹é½ï¼ä»èæé«åå¹¶æ¨¡åçæ§è½ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¹¿æ³çå®è¯ç»æè¯æäºCore Spaceåå¹¶çæææ§ï¼</p>
<ul>
<li><strong>æåè¿çæ§è½ï¼</strong> å¨è§è§åè¯­è¨ä»»å¡ï¼åæ¬ViT-B/32ãViT-L/14åLlama 3 8Béª¨å¹²ç½ç»ï¼ä¸ï¼Core Spaceæ¾èä¼äºç°æåå¹¶ææ¯ï¼å¹¶å®ç°äºæåè¿çæ§è½ãä¾å¦ï¼å¨Llama 3 8Bä¸ï¼å®å°TSVçå¹³åå½ä¸ååç¡®çæåè³94.16%ã</li>
<li><strong>è®¡ç®æçï¼</strong> Core Spaceåå¹¶å¨ä¿æé«åç¡®ççåæ¶ï¼ä»ä½¿ç¨äºç«äºæ¹æ³ä¸å°é¨åçè®¡ç®èµæºãå¶æ¶é´å¤æåº¦ä¸Task Arithmeticå¨å¨ç©ºé´ä¸­çå¤æåº¦ç¸å½ï¼ä½é¿åäºé«ç»´SVDæä½ã</li>
<li><strong>å¯¹å¼æç§©åé¢å¤PEFTæ¹æ³çéç¨æ§ï¼</strong> Core Spaceè½å¤æ ç¼å¤çå·æå¼æLoRAç§©çæ¨¡ååå¹¶ï¼å¹¶ä¸å¯ä»¥æ©å±å°å¶ä»PEFTæ¹æ³ï¼å¦VeRAï¼åæ ·è¡¨ç°åºä¼è¶çæ§è½ã</li>
<li><strong>ä¿¡æ¯å¯åº¦ï¼</strong> åæè¡¨æï¼Core Spaceæ¯ä¸ä¸ªä¿¡æ¯å¯éçç©ºé´ï¼æªæ­ä»»ä½ç»ä»¶é½ä¼å¯¼è´æ§è½ä¸éï¼è¿ä¸å¨ç©ºé´ä¸­å­å¨å¤§éåä½ææªä½¿ç¨ç»ä»¶å½¢æå¯¹æ¯ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåæ¾èçå±éæ§ãç¶èï¼å¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼</p>
<ul>
<li><strong>çº¿æ§åå¹¶å½æ°çåè®¾ï¼</strong> å°½ç®¡è®ºæè¯æäºCore Spaceå¨éçº¿æ§åå¹¶å½æ°ä¸ä¹è½æé«æ§è½ï¼ä½å¶ä¿¡æ¯æ æç¹æ§å¨çè®ºä¸ä¸»è¦ä¾èµäºåå¹¶å½æ°æ¯çº¿æ§çæåµï¼å¦Task Arithmeticï¼ãå¯¹äºæ´å¤æçéçº¿æ§åå¹¶ç­ç¥ï¼å¶çè®ºä¿è¯å¯è½éè¦è¿ä¸æ­¥çç»åã</li>
<li><strong>åèåºç¡çéæ©ï¼</strong> è®ºæè½ç¶å±ç¤ºäºå¶éæ©çåèåºç¡ï¼éè¿åç´å å A(t)åæ°´å¹³å å B(t)çSVDè·å¾ï¼æ¯æä½³çï¼ä½å¯¹äºå¶ä»å¯è½çåèåºç¡éæ©ï¼å¶æ§è½å¯è½ä¼ææä¸åã</li>
<li><strong>ç¹å®ä»»å¡è®¾ç½®ï¼</strong> è®ºæä¸»è¦å³æ³¨å¤ä»»å¡åå¹¶ï¼å³åå¹¶å¤ä¸ªå¨ä¸åä»»å¡ä¸å¾®è°çæ¨¡åãå¯¹äºå¶ä»åå¹¶åºæ¯ï¼ä¾å¦ï¼æ¨¡ååç¼©æç¥è¯è¸é¦ï¼ï¼Core Spaceçéç¨æ§å¯è½éè¦è¿ä¸æ­¥æ¢ç´¢ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸ºæªæ¥çç ç©¶æä¾äºå ä¸ªæ½å¨æ¹åï¼</p>
<ul>
<li><strong>æ´å¤æçåå¹¶ç­ç¥ï¼</strong> æ¢ç´¢å¨Core Spaceä¸­åºç¨æ´åè¿ãæ´å¤æçéçº¿æ§åå¹¶ç­ç¥ï¼ä»¥è¿ä¸æ­¥æé«æ§è½æè§£å³ç¹å®ææã</li>
<li><strong>çè®ºæ³åï¼</strong> è¿ä¸æ­¥ç ç©¶Core Spaceæ¡æ¶å¨æ´å¹¿æ³çPEFTæ¹æ³åæ¨¡åæ¶æä¸ççè®ºæ³åè½åã</li>
<li><strong>å¨æCore Spaceï¼</strong> æ¢ç´¢å¨æè°æ´Core Spaceç»´åº¦æç»æçæ¹æ³ï¼ä»¥éåºä¸æ­ååçä»»å¡æ°éææ¨¡åå¤ææ§ã</li>
<li><strong>ä¸å¶ä»æ¨¡ååç¼©ææ¯çç»åï¼</strong> å°Core Spaceåå¹¶ä¸éåãåªæç­å¶ä»æ¨¡ååç¼©ææ¯ç¸ç»åï¼ä»¥å®ç°æ´æè´çæ¨¡åæçã</li>
<li><strong>å¨æ´å¹¿æ³åºç¨ä¸­çè¯ä¼°ï¼</strong> å¨æ´å¤æ ·åçåºç¨é¢åï¼ä¾å¦ï¼æºå¨äººãå¼ºåå­¦ä¹ ç­ï¼ä¸­è¯ä¼°Core Spaceåå¹¶çæææ§ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥Core Spaceæ¡æ¶ï¼ä¸ºä½ç§©éåºæ¨¡åçåå¹¶æä¾äºä¸ä¸ªé«æä¸åç¡®çè§£å³æ¹æ¡ãå®ä¸ä»è§£å³äºç°ææ¹æ³å¨æçä¸çä¸è¶³ï¼è¿å¨å¤ä»»å¡è§è§åè¯­è¨ä»»å¡ä¸åå¾äºæ¾èçæ§è½æåï¼ä¸ºå¤§åæ¨¡åçåæ°é«æéåºåå¤ä»»å¡å­¦ä¹ å¼è¾äºæ°çéå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks.</li>
<li>Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17786v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17786v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17773v1'></a></p>
<h2 id="i2vwm-robust-watermarking-for-image-to-video-generation"><a href="https://arxiv.org/abs/2509.17773v1">I2VWM: Robust Watermarking for Image to Video Generation</a></h2>
<p><strong>Authors:</strong> Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The rapid progress of image-guided video generation (I2V) has raised concerns
about its potential misuse in misinformation and fraud, underscoring the urgent
need for effective digital watermarking. While existing watermarking methods
demonstrate robustness within a single modality, they fail to trace source
images in I2V settings. To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos. Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.
I2VWM leverages a video-simulation noise layer during training and employs an
optical-flow-based alignment module during inference. Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.
\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code
Released.}</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhangæ°åçè®ºæâI2VWM: Robust Watermarking for Image to Video Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="i2vwm">è®ºææè¦ï¼I2VWMï¼å¾åå°è§é¢çæä¸­çé²æ£æ°´å°ææ¯</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
éçå¾åå¼å¯¼è§é¢çæï¼I2Vï¼ææ¯çå¿«éåå±ï¼å¶å¨èåä¿¡æ¯ä¼ æ­åæ¬ºè¯æ¹é¢çæ½å¨æ»¥ç¨å¼èµ·äºå¹¿æ³å³æ³¨ãç°æçæ°å­æ°´å°æ¹æ³è½ç¶å¨åä¸æ¨¡æåè¡¨ç°åºé²æ£æ§ï¼ä½å¨I2Våºæ¯ä¸­ï¼å®ä»¬æ æ³ææè¿½è¸ªæºå¾åï¼å³æ°´å°ä¿¡å·å¨è§é¢çæè¿ç¨ä¸­é¾ä»¥ä¿æå¶æ¶é´ä¸çé²æ£æ§ãè¿å¯¼è´äºä¸ä¸ªæ ¸å¿é®é¢ï¼å¦ä½å¨è·¨æ¨¡æï¼å¾åå°è§é¢ï¼çæè¿ç¨ä¸­ï¼ç¡®ä¿æ°´å°ä¿¡å·çæä¹æ§åå¯éªè¯æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼æ¬è®ºææåºäºä»¥ä¸å³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼
*   <strong>é²æ£æ©æ£è·ç¦»ï¼Robust Diffusion Distance, RDDï¼æ¦å¿µçå¼å¥ï¼</strong> é¦æ¬¡å®ä¹äºRDDï¼ç¨äºéåæ°´å°ä¿¡å·å¨çæè§é¢ä¸­éæ¶é´æ¨ç§»çæä¹æ§ï¼å³æ°´å°å¨è§é¢ä¸­ä»è½å¯é éªè¯çæå¤§å¸§ç´¢å¼ãè¿ä¸ºè¯ä¼°I2Våºæ¯ä¸çæ°´å°é²æ£æ§æä¾äºä¸ä¸ªæ°çéåææ ã
*   <strong>I2VWMæ¡æ¶ï¼</strong> æåºäºä¸ç§è·¨æ¨¡ææ°´å°æ¡æ¶I2VWMï¼æ¨å¨å¢å¼ºæ°´å°å¨æ¶é´ç»´åº¦ä¸çé²æ£æ§ãè¯¥æ¡æ¶åºäºç¼ç å¨-è§£ç å¨æ¶æï¼å¹¶åå«ä»¥ä¸æ ¸å¿ç»ä»¶ï¼
    *   <strong>è§é¢æ¨¡æåªå£°å±ï¼Video-simulation Noise Layerï¼ï¼</strong> å¨è®­ç»é¶æ®µå¼å¥ï¼éè¿æ¨¡æè§é¢çæåºæçåç¼©ãåªå£°æ·»å /ç§»é¤ååç´ åç§»ç­å¤±çï¼ä½¿æ°´å°æ¨¡åè½å¤æ¾å¼å°å­¦ä¹ å¹¶æµæè¿äºçæå¼èµ·çä¿®æ¹ï¼ä»èæé«æ°´å°ä¿¡å·çé²æ£æ§ã
    *   <strong>åºäºåæµçå¯¹é½æ¨¡åï¼Optical-flow-based Alignment Moduleï¼ï¼</strong> å¨æ¨çé¶æ®µä½¿ç¨ï¼éè¿ä¼°è®¡è§é¢å¸§ä¸åèå¸§ä¹é´çåæµï¼å°è¿ç¦»åå§å¸§çè§é¢å¸§å¯¹é½ï¼ä»èè¡¥å¿æ¶é´ä¸çéåï¼æ¾èå»¶é¿æ°´å°çé²æ£æ©æ£è·ç¦»ã
    *   <strong>æç¥¨æºå¶ï¼Voting Schemeï¼ï¼</strong> å¨è§é¢æ°´å°æåæ¨¡å¼ä¸ï¼å¯¹ææå¯¹é½å¸§æåçæ°´å°è¿è¡èåï¼ä»¥è·å¾æç»çæ°´å°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èæé«é²æ£æ§ï¼</strong> å¨å¤ä¸ªå¼æºï¼å¦CogVideoX, Wan, Hunyuan, Stable Video Diffusionï¼ååä¸I2Væ¨¡åä¸çå®éªè¡¨æï¼I2VWMå¨I2Våºæ¯ä¸æ¾èæé«äºæ°´å°çé²æ£æ§ï¼å°¤å¶æ¯å¨é²æ£æ©æ£è·ç¦»ï¼RDDï¼åå¸§åç¡®çï¼FACCï¼æ¹é¢ä¼äºç°æåºçº¿æ¹æ³ã
*   <strong>ä¿æä¸å¯æç¥æ§ï¼</strong> I2VWMå¨æé«é²æ£æ§çåæ¶ï¼ä»è½ä¿ææ°´å°çä¸å¯æç¥æ§ï¼éè¿PSNRãSSIMåLPIPSç­è§è§è´¨éææ éªè¯äºå¶æ§è½ã
*   <strong>æ³åè½åå¼ºï¼</strong> I2VWMå¨ä¸åçææ¨¡åä¸çè¡¨ç°ä¸è´ï¼è¡¨æå¶å·æå¼ºå¤§çæ³åè½åï¼ä¸ä¾èµäºç¹å®ççæå¨ã
*   <strong>å¯¹ç»å¸åªå£°çé²æ£æ§ï¼</strong> I2VWMå¯¹ç»å¸å¾åå¤±çï¼å¦è£åªãé«æ¯æ¨¡ç³ãJPEGåç¼©ç­ï¼ä¹è¡¨ç°åºå·æç«äºåçé²æ£æ§ï¼è¿å¾çäºå¶åªå£°å±è®¾è®¡ä¸­å¯¹æ°å¼åå ä½å¤±ççç»¼åèèã
*   <strong>åæµå¯¹é½æ¨¡åçæææ§ï¼</strong> æ¶èå®éªè¯å®ï¼åºäºåæµçå¯¹é½æ¨¡åè½å¤æ¾èæåæ°´å°æåçåç¡®æ§ï¼å°¤å¶æ¯å¨æ¶é´ä¸è¿ç¦»åå§å¸§çè§é¢å¸§ä¸­ï¼è¿ä¸æ­¥éªè¯äºå¶å¯¹æ°´å°ä¿¡å·æä¹æ§çè´¡ç®ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¯¹æ¶é´é¡ºåºçä¾èµï¼</strong> I2VWMçæææ§ä¾èµäºè§é¢ä¸­ä¸è´çæ¶é´é¡ºåºåè®¾ï¼æ æ³å¤çæ¶é´è£åªæå¸§æä¹±ç­å¤±çã
*   <strong>æªå®å¨è§£å³ç»å¸å¤±çï¼</strong> è®ºææåºï¼I2VWMå°æªå®å¨è§£å³ææç»å¸å¤±çï¼ä¾å¦éæºæè½¬ï¼è¿å¨è§é¢å¸§å¤çä¸­ä¹å¸¸è§ã
*   <strong>ç¼ºä¹è§é¢çæè´¨éææ ï¼</strong> è®ºæçè¯ä¼°ç¼ºä¹å¯¹è§é¢çæè´¨éçææ ãä½èè®¤ä¸ºï¼é«è´¨éçè§é¢å¾å¾è½æ´å¥½å°ä¿çæºå¾åä¸­çæ°´å°ä¿¡å·ï¼å æ­¤å¹³è¡¡æ°´å°é²æ£æ§ä¸è§é¢è´¨éæ¯ä¸ä¸ªéè¦çèéã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤çæ¶é´æ åºå¤±çï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½ä½¿I2VWMè½å¤åºå¯¹æ¶é´è£åªæå¸§æä¹±ç­ç ´åæ¶é´é¡ºåºçå¤±çï¼ä»¥æé«æ°æ®ä¿æ¤è½åã
*   <strong>å¢å¼ºå¯¹ææç»å¸å¤±ççé²æ£æ§ï¼</strong> æ¢ç´¢æ´å¨é¢çæ¹æ³æ¥è§£å³åæ¬éæºæè½¬å¨åçææç»å¸å¤±çã
*   <strong>æ´åè§é¢çæè´¨éè¯ä¼°ï¼</strong> å¨æªæ¥çå·¥ä½ä¸­ï¼å°è§é¢çæè´¨éææ çº³å¥è¯ä¼°ä½ç³»ï¼ä»¥æ´å¨é¢å°è¡¡éæ°´å°æ¹æ³å¨ä¿ææ°´å°é²æ£æ§åè§é¢è´¨éä¹é´çå¹³è¡¡ã
*   <strong>æ·±å¥ç ç©¶åªå£°å±è®¾è®¡ï¼</strong> è¿ä¸æ­¥æ¢ç´¢åä¼ååªå£°å±è®¾è®¡ï¼ä»¥åºå¯¹æ´å¤æçæ°å¼åå ä½å¤±çç»åã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæé¦æ¬¡æåºäºå¾åå°è§é¢çæåºæ¯ä¸çè·¨æ¨¡æé²æ£æ°´å°ææï¼å¹¶å¼å¥äºâé²æ£æ©æ£è·ç¦»âè¿ä¸æ°é¢ææ ãéè¿ç»åè§é¢æ¨¡æåªå£°å±ååºäºåæµçå¯¹é½æ¨¡åï¼I2VWMæ¡æ¶å¨ä¿ææ°´å°ä¸å¯æç¥æ§çåæ¶ï¼æ¾èæåäºæ°´å°ä¿¡å·å¨çæè§é¢ä¸­çæ¶é´é²æ£æ§ãè¿é¡¹å·¥ä½ä¸ºçæè§é¢æ¶ä»£ä¸çè·¨æ¨¡ææ°´å°ææ¯å¼è¾äºæ°èå¼ï¼å·æéè¦ççè®ºåå®éæä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos.</li>
<li>Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.</li>
<li>Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17773v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17773v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-23 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
