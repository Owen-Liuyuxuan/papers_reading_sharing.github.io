<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-23 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-22/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-24/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-23">Arxiv Computer Vision Papers - 2025-09-23</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-22" class="nav-link">Arxiv 计算机视觉与机器学习日报执行摘要 (2025-09-22)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#qwen3-omni-technical-report" class="nav-link">Qwen3-Omni Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#automatic-intermodal-loading-unit-identification-using-computer-vision-a-scoping-review" class="nav-link">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a>
                </li>
                <li class="nav-item">
                    <a href="#composeme-attribute-specific-image-prompts-for-controllable-human-image-generation" class="nav-link">ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#geosvr-taming-sparse-voxels-for-geometrically-accurate-surface-reconstruction" class="nav-link">GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#tempsamp-r1-effective-temporal-sampling-with-reinforcement-fine-tuning-for-video-llms" class="nav-link">TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#composablenav-instruction-following-navigation-in-dynamic-environments-via-composable-diffusion" class="nav-link">ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#does-audio-matter-for-modern-video-llms-and-their-benchmarks" class="nav-link">Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</a>
                </li>
                <li class="nav-item">
                    <a href="#sight-over-site-perception-aware-reinforcement-learning-for-efficient-robotic-inspection" class="nav-link">Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</a>
                </li>
                <li class="nav-item">
                    <a href="#accurate-and-efficient-low-rank-model-merging-in-core-space" class="nav-link">Accurate and Efficient Low-Rank Model Merging in Core Space</a>
                </li>
                <li class="nav-item">
                    <a href="#i2vwm-robust-watermarking-for-image-to-video-generation" class="nav-link">I2VWM: Robust Watermarking for Image to Video Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-23">Arxiv Computer Vision Papers - 2025-09-23</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-22">Arxiv 计算机视觉与机器学习日报执行摘要 (2025-09-22)</h2>
<p><strong>概述：</strong></p>
<p>今日 Arxiv 计算机视觉与机器学习领域的论文呈现出多模态大模型（尤其是视频-语言模型）、3D 几何重建、可控生成以及机器人感知与导航等多个活跃方向。特别值得关注的是，多模态模型在处理视频、音频和文本信息方面的能力持续增强，并开始探索更精细的控制和应用。</p>
<p><strong>主要主题与趋势：</strong></p>
<ol>
<li><strong>多模态大模型与视频理解 (V-LLMs)：</strong> 多篇论文聚焦于视频-语言模型 (V-LLMs) 的发展，包括其架构、训练策略（如时间采样）以及对音频模态的考量。这表明 V-LLMs 仍是当前研究热点，旨在实现更全面、更鲁棒的视频内容理解。</li>
<li><strong>可控生成与编辑：</strong> 在图像和视频生成领域，研究人员正致力于提升生成内容的精细控制能力，例如通过属性特定提示进行人物图像生成，以及为图像到视频生成提供鲁棒的水印方案。</li>
<li><strong>3D 几何重建：</strong> 稀疏体素在几何精确表面重建中的应用是一个重要方向，旨在克服传统方法的局限性，实现更高质量的3D模型。</li>
<li><strong>机器人感知与导航：</strong> 机器人领域的研究侧重于在动态环境中实现指令遵循的导航，并结合感知能力优化机器人检查任务的效率。</li>
<li><strong>模型效率与鲁棒性：</strong> 论文也关注模型合并的效率和准确性，以及生成模型的水印技术，体现了对模型实用性和安全性的考量。</li>
</ol>
<p><strong>特别显著或创新论文：</strong></p>
<ul>
<li><strong>"Qwen3-Omni Technical Report" (Jin Xu et al.):</strong> 作为一份技术报告，它通常会介绍一个大型、多功能的模型，预示着未来多模态大模型的发展方向和能力边界。如果 Qwen3-Omni 像其前身一样具有广泛影响力，这份报告将是理解其核心技术和潜力的关键。</li>
<li><strong>"TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs" (Yunheng Li et al.):</strong> 该论文通过强化学习微调来优化视频 LLMs 的时间采样策略，解决了视频理解中的一个核心挑战，即如何高效地从长视频中提取关键信息。这种结合强化学习的优化方法具有创新性。</li>
<li><strong>"ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion" (Zichao Hu et al.):</strong> 将可组合扩散模型应用于动态环境中的指令遵循导航，为机器人导航提供了一种新颖且可能更鲁棒的解决方案，尤其是在复杂、不可预测的场景中。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>强化学习在多模态模型优化中的应用：</strong> "TempSamp-R1" 展示了强化学习在优化 V-LLMs 内部机制（如时间采样）方面的潜力。</li>
<li><strong>可组合扩散模型在机器人控制中的应用：</strong> "ComposableNav" 探索了扩散模型在生成复杂、多步骤行为序列方面的能力，为机器人任务规划和执行开辟了新途径。</li>
<li><strong>多模态模型中的音频模态重要性再评估：</strong> "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?" 提出了对现有 V-LLMs 和基准中音频作用的深入思考，这可能引导未来模型设计更加全面地整合音频信息。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<ol>
<li><strong>"Qwen3-Omni Technical Report" (Jin Xu et al.):</strong> 对于关注多模态大模型最新进展和未来趋势的研究人员，这份报告是必读的，它将提供一个全面且前沿的视角。</li>
<li><strong>"TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs" (Yunheng Li et al.):</strong> 对于从事视频理解和 V-LLMs 优化的研究人员，该论文提供了一种创新的方法来解决时间采样效率问题，具有很高的参考价值。</li>
<li><strong>"ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion" (Zichao Hu et al.):</strong> 对于机器人学和具身智能领域的研究人员，该论文展示了将生成模型应用于复杂导航任务的潜力，值得深入研究。</li>
<li><strong>"ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation" (Guocheng Gordon Qian et al.):</strong> 对于关注可控图像生成和人像合成的研究人员，该论文提供了一种精细控制生成内容的方法，具有实际应用价值。</li>
</ol>
<p>这份摘要旨在帮助您快速把握今日 Arxiv 计算机视觉与机器学习领域的关键发展，并为进一步深入阅读提供指导。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.17765v1">Qwen3-Omni Technical Report</a></li>
<li><a href="#2509.17707v1">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></li>
<li><a href="#2509.18092v1">ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</a></li>
<li><a href="#2509.18090v1">GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</a></li>
<li><a href="#2509.18056v1">TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</a></li>
<li><a href="#2509.17941v1">ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</a></li>
<li><a href="#2509.17901v1">Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</a></li>
<li><a href="#2509.17877v1">Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</a></li>
<li><a href="#2509.17786v1">Accurate and Efficient Low-Rank Model Merging in Core Space</a></li>
<li><a href="#2509.17773v1">I2VWM: Robust Watermarking for Image to Video Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.17765v1'></a></p>
<h2 id="qwen3-omni-technical-report"><a href="https://arxiv.org/abs/2509.17765v1">Qwen3-Omni Technical Report</a></h2>
<p><strong>Authors:</strong> Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV, eess.AS</p>
<p><strong>Abstract:</strong></p>
<p>We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供《Qwen3-Omni Technical Report》的全面摘要。</p>
<p><strong>Qwen3-Omni 技术报告摘要</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决当前多模态模型普遍存在的模态间性能退化问题，即在提升某一模态性能时，其他模态的性能会随之下降。研究目标是开发一个统一的多模态模型（Qwen3-Omni），使其在文本、图像、音频和视频等多种模态上都能保持最先进的性能，且不发生任何性能退化，同时显著增强跨模态能力，如视频理解。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>Thinker-Talker MoE 架构：</strong> Qwen3-Omni 采用 Thinker-Talker 混合专家（MoE）架构，统一了文本、图像、音频和视频的感知与生成，实现了流畅的文本和自然的实时语音。Thinker 和 Talker 都升级为 MoE 设计，以支持高并发和快速推理。
*   <strong>AuT 音频编码器：</strong> 替换了 Whisper 音频编码器，采用从零开始训练的 AuT（Audio Transformer）编码器，在 2000 万小时的监督音频数据上训练，生成更强的通用音频表示，并采用块式窗口注意力实现实时预填充缓存。
*   <strong>多码本语音生成：</strong> 在语音生成方面，采用多码本表示，通过 MTP 模块自回归预测多个码本层，并用轻量级因果 ConvNet 替换计算密集型的块式扩散模型，实现从第一个码本帧开始的流式合成，显著降低了首包延迟。
*   <strong>Thinking 模型：</strong> 引入了一个 Thinking 模型，明确地对来自任何模态的输入进行推理，以增强多模态推理能力。
*   <strong>非退化多模态训练：</strong> 论文提出并验证了在文本预训练早期阶段混合单模态和跨模态数据，可以实现所有模态的性能均等，同时显著增强跨模态能力。
*   <strong>音频字幕模型：</strong> 针对通用音频字幕模型的缺失，微调了 Qwen3-Omni-30B-A3B，得到了 Qwen3-Omni-30B-A3B-Captioner，能为任意音频输入生成详细、低幻觉的字幕。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>SOTA 性能：</strong> Qwen3-Omni 在 36 个音频和音视频基准测试中，在 32 个基准测试上实现了开源 SOTA，并在 22 个基准测试上达到了总体 SOTA，超越了 Gemini-2.5-Pro、Seed-ASR 和 GPT-4o-Transcribe 等强大的闭源模型。
*   <strong>无模态退化：</strong> 首次证明了该模型在文本、图像、音频和视频上均保持了与同等规模单模态模型相当的先进性能，没有出现模态退化。
*   <strong>音频任务优势：</strong> 在音频任务上表现尤为出色，支持 119 种语言的文本交互、19 种语言的语音理解和 10 种语言的语音生成。
*   <strong>低延迟流式合成：</strong> 在冷启动设置下，实现了 234 毫秒的理论端到端首包延迟，确保了高并发工业部署中的低延迟语音交互。
*   <strong>跨模态推理增强：</strong> Thinking 模型和多模态训练显著提升了模型在复杂推理任务上的表现，尤其是在需要整合音频和视觉信息的场景中。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>长视频基准测试性能次优：</strong> 当前模型在长视频基准测试上的性能次优，这源于两个架构限制：位置外推能力有限和上下文长度受限。
*   <strong>语言能力提升不明显：</strong> 经验表明，添加视觉或音频信号并未在语言能力方面带来可衡量的提升。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>多说话人 ASR：</strong> 进一步支持多说话人自动语音识别。
*   <strong>视频 OCR：</strong> 增强视频中的光学字符识别能力。
*   <strong>音视频主动学习：</strong> 探索音视频领域的主动学习。
*   <strong>基于代理的工作流和函数调用：</strong> 增强对基于代理的工作流和函数调用的支持。
*   <strong>解决长视频性能限制：</strong> 解决当前模型在长视频理解方面的架构限制，提升其在该领域的性能。</p>
<p>总而言之，Qwen3-Omni 代表了多模态模型发展的一个里程碑，首次提供了全面集成、端到端的多模态训练可以在不降低核心语言能力和其他模态性能的情况下实现，同时在音频任务上表现卓越，并显著增强了跨模态推理能力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts.</li>
<li>To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17765v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17765v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17707v1'></a></p>
<h2 id="automatic-intermodal-loading-unit-identification-using-computer-vision-a-scoping-review"><a href="https://arxiv.org/abs/2509.17707v1">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></h2>
<p><strong>Authors:</strong> Emre Gülsoylu, Alhassan Abdelhalim, Derya Kara Boztas, Ole Grasse, Carlos Jahn, Simone Frintrop, Janick Edinger</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Emre Gülsoylu等人撰写的论文“Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review”的全面摘要：</p>
<p><strong>论文题目：</strong> 使用计算机视觉的联运装载单元自动识别：一项范围审查</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决联运装载单元（ILUs，如集装箱、半挂车和可换箱体）在港口和码头进行高效、鲁棒识别的关键瓶颈。尽管ILUs的标准化彻底改变了全球贸易，但其识别过程仍面临挑战。该研究通过对计算机视觉（CV）领域现有解决方案的全面审查，探讨了如何利用CV技术克服这些挑战，并识别该领域的发展趋势、局限性及未来方向。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
这篇论文本身是一项范围审查，其主要贡献在于对现有文献的系统性分析和综合，而非提出新的算法或模型。其关键创新和方法论贡献包括：
*   <strong>首次全面审查：</strong> 这是对基于CV的ILU识别研究的首次综合性审查，涵盖了1990年至2025年间的63项实证研究。
*   <strong>领域演变追踪：</strong> 详细追溯了该领域从早期数字图像处理（DIP）和传统机器学习（ML）到当前深度学习（DL）技术主导的演变过程。
*   <strong>标准化术语的呼吁：</strong> 强调了该领域术语不统一的问题，并呼吁采用标准化术语，例如“联运装载单元（ILU）识别”，以提高概念清晰度并促进跨学科研究。
*   <strong>数据集和评估指标的综合分析：</strong> 详细分析了现有研究中使用的图像采集设置、数据集特性（可用性、多样性）和评估指标，揭示了公共基准数据集的严重缺乏。
*   <strong>识别新兴挑战：</strong> 突出了从基于字符的文本识别转向场景文本识别（scene-text spotting）的趋势，以及移动摄像头（如无人机、车载传感器）在动态码头监控中的应用所带来的新挑战。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>CV作为经济高效的替代方案：</strong> 计算机视觉技术为ILU识别提供了比RFID等其他技术更具成本效益的替代方案，且准确性更高。
*   <strong>DL技术的主导地位：</strong> 研究趋势显示，深度学习方法已成为ILU识别的主导范式，尤其是在2016-2020年之后，其在处理复杂条件下的鲁棒性优于传统方法。
*   <strong>亚洲研究的突出贡献：</strong> 亚洲地区在ILU识别研究中占据主导地位（79.71%的论文来自亚洲），这反映了该地区作为全球贸易枢纽对高效物流的迫切需求和大量投资。
*   <strong>公共资金支持：</strong> 超过三分之一的研究得到公共资金支持，表明政府对交通基础设施和经济增长的重视。
*   <strong>性能差异巨大：</strong> 由于缺乏公开可用的基准数据集，报告的端到端准确率差异巨大，从5%到96%不等，这使得方法间的公平比较变得困难。
*   <strong>从字符识别到场景文本识别的转变：</strong> 随着移动摄像头的普及，任务已从简单的字符识别演变为更复杂的场景文本识别，需要更强大的检测和识别方法。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>缺乏公开可用的基准数据集：</strong> 这是该领域研究和开发的最大障碍。绝大多数数据集（85.71%）是私有的，且许多研究依赖于受控的图像采集设置，导致模型泛化能力受限，并阻碍了结果的重现性和公平比较。
*   <strong>术语不统一：</strong> 现有文献中术语使用不一致，影响了研究的清晰度和可比性。
*   <strong>DL训练数据不足：</strong> 现有数据集的规模对于鲁棒的深度学习训练来说往往过小，迫使研究人员适应通用文本检测和识别模型，可能无法充分优化ISO6346代码的特定识别。
*   <strong>对非主流期刊的依赖：</strong> 超过一半的文章发表在未排名的期刊和会议上，这可能影响了该领域研究的可见度和影响力。
*   <strong>上下文无关文本识别的挑战：</strong> ISO6346代码的上下文无关性质使得场景文本识别任务更具挑战性，因为缺乏自然语言线索，现有依赖语义上下文的模型效果不佳。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>标准化和开放获取数据集：</strong> 呼吁建立公开可用的基准数据集，并采用标准化术语和评估方法，以促进研究成果的可比性和可重现性。
*   <strong>上下文无关场景文本识别：</strong> 针对ISO6346代码的上下文无关特性，开发专门优化的场景文本识别架构，避免依赖自然语言模型。
*   <strong>实时处理和边缘设备优化：</strong> 关注算法优化，以实现移动设备上的实时处理，并通过视频流分析增强系统性能。
*   <strong>移动摄像头设置的重点研究：</strong> 更多地关注车载摄像头（如无人机、传感器配备的地面车辆）的设置，以应对动态场景带来的新挑战，并探索ILU姿态估计等新任务，实现码头的精确监控。
*   <strong>数据增强和合成数据生成：</strong> 探索使用生成对抗网络（GANs）或游戏引擎等技术生成合成数据，以弥补真实数据集的不足，并模拟各种挑战性条件（如阴影、锈迹、污垢、运动模糊）。
*   <strong>图像质量提升和后处理：</strong> 继续研究图像增强技术（如降噪、超分辨率）和后处理步骤（如利用校验位和所有者代码验证识别结果），以提高识别准确性。
*   <strong>统一的端到端模型：</strong> 开发将文本检测和识别整合到单个前向传播中的统一端到端模型，以简化处理流程并提高整体效率。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring.</li>
<li>To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17707v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17707v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.18092v1'></a></p>
<h2 id="composeme-attribute-specific-image-prompts-for-controllable-human-image-generation"><a href="https://arxiv.org/abs/2509.18092v1">ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</a></h2>
<p><strong>Authors:</strong> Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, Kfir Aberman</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating high-fidelity images of humans with fine-grained control over
attributes such as hairstyle and clothing remains a core challenge in
personalized text-to-image synthesis. While prior methods emphasize identity
preservation from a reference image, they lack modularity and fail to provide
disentangled control over specific visual attributes. We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity. Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model. This enables compositional and
disentangled control over multiple visual factors, even across multiple people
within a single image. To promote natural composition and robust
disentanglement, we curate a cross-reference training dataset featuring
subjects in diverse poses and expressions, and propose a multi-attribute
cross-reference training strategy that encourages the model to generate
faithful outputs from misaligned attribute inputs while adhering to both
identity and textual conditioning. Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts. Our framework paves the way for more configurable human image
synthesis by combining visual prompting with text-driven generation. Webpage is
available at: https://snap-research.github.io/composeme/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Guocheng Gordon Qian等人撰写的论文“ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决个性化文本到图像合成领域的一个核心挑战：如何生成高保真的人类图像，并对发型、服装和身份等属性进行细粒度控制。现有方法虽然强调从参考图像中保留身份，但缺乏模块化，无法对特定视觉属性提供解耦控制，尤其是在需要组合来自不同来源的多个属性时。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
ComposeMe引入了一种新颖的“属性特定图像提示”范式，其核心创新包括：</p>
<ul>
<li><strong>属性特定图像提示（Attribute-Specific Image Prompts）：</strong> 该方法将人类主体分解为可配置的属性（面部身份、发型和服装），并为每个属性使用不同的参考图像集进行引导生成。这使得能够组合来自不同来源的视觉属性，实现更灵活的控制。</li>
<li><strong>ComposeMe管线：</strong> 采用基于适配器的解决方案，通过三个阶段实现：<ul>
<li><strong>属性特定标记化（Attribute-Specific Tokenization）：</strong> 为每个视觉组件（面部、发型、服装）使用专用的特征标记器处理参考图像，捕获属性特定特征。</li>
<li><strong>多属性合并（Multi-Attribute Merging）：</strong> 将来自不同属性的标记合并，形成多属性主体表示。</li>
<li><strong>注入预训练扩散模型：</strong> 将合并后的标记注入冻结的预训练文本到图像扩散模型中，通过解耦的交叉注意力机制实现图像生成。</li>
</ul>
</li>
<li><strong>多属性交叉引用训练（Multi-Attribute Cross-Reference Training）：</strong> 提出了一种新颖的两阶段训练策略。在第一阶段进行复制粘贴预训练以预热适配器。第二阶段是多属性交叉引用微调，通过使用来自不同个体和姿态的输入和目标进行监督，明确解耦了属性之间的纠缠（例如，服装、面部和头发与身体姿态、表情或头部方向的纠缠）。这种训练鼓励模型从错位的属性输入生成忠实且自然对齐的输出，同时保持身份和文本条件。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
广泛的实验表明，ComposeMe在准确遵循视觉和文本提示方面达到了最先进的性能。</p>
<ul>
<li><strong>高保真和解耦控制：</strong> 该方法能够生成高保真的人类图像，对多个视觉因素（包括面部表情、头部姿态、身体姿态和风格）进行细粒度、解耦的控制，即使在单个图像中包含多个人物也能实现。</li>
<li><strong>优于现有方法：</strong> 在多属性、单ID个性化任务中，ComposeMe在身份、发型和服装的保留方面显著优于OmniGen和GPT-40等现有方法。在单属性、多ID个性化任务中，ComposeMe也表现出最高的身份保留、组合质量和整体图像质量。</li>
<li><strong>自然组合和鲁棒解耦：</strong> 交叉引用训练对于实现高保真图像生成至关重要，它能有效利用属性特定的视觉提示，即使这些提示存在错位，也能实现精确、解耦的面部、头发和服装控制。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>封闭集个性化：</strong> ComposeMe是一个封闭集个性化方法，目前训练用于最多2个身份和3个属性。虽然对于大多数以人为中心的用例（面部、头发、服装和姿态）来说，这种覆盖范围已足够，但扩展到新的视觉提示（如场景、配饰）仍需进一步研究。
*   <strong>小脸样本的失真：</strong> 当文本提示指定远距离拍摄时，生成的面部会变得很小（小于图像的1%），导致面部出现明显的失真。这归因于训练数据过滤器移除了小脸样本，未来工作可以通过包含小脸样本来解决。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展到新的视觉提示：</strong> 可以通过连接新的视觉提示来微调ComposeMe，或者为额外类别训练新的适配器并在推理时进行组合。
*   <strong>提高图像质量：</strong> 在高质量数据上进行微调和RL（强化学习）微调是提高图像质量的有前景的方向。
*   <strong>更灵活和可重用的个性化管线：</strong> 该框架为更可配置的人类图像合成铺平了道路，允许每个属性独立地进行策划、更新或替换，而无需重新训练整个身份表示。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity.</li>
<li>Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model.</li>
<li>Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.18092v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.18092v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.18090v1'></a></p>
<h2 id="geosvr-taming-sparse-voxels-for-geometrically-accurate-surface-reconstruction"><a href="https://arxiv.org/abs/2509.18090v1">GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</a></h2>
<p><strong>Authors:</strong> Jiahe Li, Jiawei Zhang, Youmin Zhang, Xiao Bai, Jin Zheng, Xiaohan Yu, Lin Gu</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jiahe Li等人撰写的论文“GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction”的全面摘要。</p>
<hr />
<p><strong>论文摘要：GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决当前基于辐射场的表面重建方法（特别是基于高斯溅射（Gaussian Splatting）的方法）在几何精度、细节保留和重建完整性方面存在的局限性。这些现有方法常受限于表示瓶颈，例如依赖结构化点云初始化导致不完整或不准确的区域，以及高斯基元缺乏清晰边缘导致几何模糊。GeoSVR探索了稀疏体素在实现精确、详细和完整表面重建方面的潜力，并解决了稀疏体素固有的场景约束缺失和表面细化局部性问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
GeoSVR提出了一个显式的、基于体素的框架，其主要创新包括：</p>
<ul>
<li><strong>体素不确定性深度约束（Voxel-Uncertainty Depth Constraint）：</strong> 针对稀疏体素缺乏强结构先验的问题，该方法利用单目深度线索作为场景约束。它通过评估每个体素的几何不确定性，自适应地确定对外部深度线索的依赖程度，从而在避免质量下降的同时，实现有效且鲁棒的场景约束，并保留高精度的几何结构。</li>
<li><strong>稀疏体素表面正则化（Sparse Voxel Surface Regularization）：</strong> 为了解决稀疏体素局部性过强导致表面形成不准确的问题，该方法设计了两种体素级正则化：<ul>
<li><strong>几何正则化与体素丢弃（Voxel Dropout）：</strong> 通过随机丢弃一部分体素，强制每个微小体素在更大的区域内保持全局几何一致性，从而扩大正则化范围，纠正错误的几何结构。</li>
<li><strong>表面校正（Surface Rectification）：</strong> 限制表面形成与唯一体素对齐，以减少深度偏差。</li>
<li><strong>缩放惩罚（Scaling Penalty）：</strong> 消除几何不准确的大体素参与表面形成，进一步促进尖锐和准确的表面重建。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
GeoSVR在DTU、Tanks and Temples以及Mip-NeRF 360等多个具有挑战性的数据集上进行了广泛实验，结果表明其性能优于现有方法，在几何精度、细节保留和重建完整性方面表现出色，同时保持了高效率。具体来说：</p>
<ul>
<li>在DTU数据集上，GeoSVR在Chamfer距离上实现了最高的重建质量。</li>
<li>在Tanks and Temples数据集上，GeoSVR在F1分数上表现最佳，尤其在复杂建筑和弱纹理区域展现出卓越的细节捕捉能力。</li>
<li>在Mip-NeRF 360数据集上，GeoSVR在渲染质量方面也表现出竞争力。</li>
<li>该方法继承了SVRaster的高效率，在推理速度上表现快速，且GPU内存占用较低。</li>
</ul>
<p>这些结果证明了GeoSVR在处理反射区域、覆盖不足区域以及需要精细细节的场景时，能够提供更准确、更完整的表面重建，克服了基于高斯溅射方法在初始化点云不足时的局限性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文指出了GeoSVR当前存在的局限性，主要包括：</p>
<ul>
<li><strong>反射区域：</strong> 在具有严重反射的区域，由于光度不一致的强烈误导，模型性能可能受限。</li>
<li><strong>无纹理区域：</strong> 在缺乏纹理信息的区域，几何重建可能面临挑战。</li>
<li><strong>透明表面：</strong> 当前的辐射场方法在处理复杂光线追踪方面存在不足，导致透明物体的重建效果不理想。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
为了解决上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li>引入更高效的光线追踪技术，以更好地处理反射和透明表面。</li>
<li>改进体素的全局性，使其能够更好地应对光照变化和无纹理区域等挑战。</li>
<li>开发针对透明物体的解决方案，以提高其重建质量。</li>
</ul>
<hr />
<p>这份摘要突出了GeoSVR在利用稀疏体素进行高精度表面重建方面的创新性，以及其在解决现有方法局限性方面的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.18090v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.18090v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.18056v1'></a></p>
<h2 id="tempsamp-r1-effective-temporal-sampling-with-reinforcement-fine-tuning-for-video-llms"><a href="https://arxiv.org/abs/2509.18056v1">TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</a></h2>
<p><strong>Authors:</strong> Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs”的全面摘要：</p>
<p><strong>论文摘要：TempSamp-R1：基于强化学习微调的视频LLM有效时间采样</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决多模态大型语言模型（MLLMs）在视频时间定位任务中面临的挑战。现有的强化学习（RL）方法，如Group Relative Policy Optimization (GRPO)，主要依赖于“on-policy”采样进行策略更新。然而，在时间搜索空间巨大的任务中，这种策略效率低下且性能有限，因为它往往难以识别时间上精确的解决方案。这导致了在稀疏监督下学习不稳定、收敛到次优解以及在复杂视频理解任务中泛化的困难。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
TempSamp-R1引入了一个新的强化学习微调框架，通过以下创新点解决了上述问题：</p>
<ul>
<li><strong>混合策略采样与off-policy指导：</strong> TempSamp-R1将高质量的外部解决方案（例如，ground-truth标注）作为off-policy监督引入策略优化过程。这提供了时间上精确的指导，有效弥补了on-policy解决方案中常见的稀疏性和错位问题，从而提高了策略优化的稳定性和效率。</li>
<li><strong>非线性软优势计算：</strong> 为了进一步稳定训练并减少基于奖励更新的方差，TempSamp-R1提出了一种非线性软优势计算方法。该方法通过不对称变换动态重塑奖励反馈，区分高奖励和低奖励解决方案的学习动态，压缩接近最优解决方案的优势值，并放大次优解决方案之间的相对奖励差距，从而生成更具信息量的梯度并促进稳定的策略优化。</li>
<li><strong>混合思维链（CoT）训练范式：</strong> TempSamp-R1采用混合CoT训练范式，优化一个统一模型以支持CoT和非CoT推理模式。这使得模型能够高效处理不同推理复杂度的查询，同时在两种模式下都表现出鲁棒性能。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
实验结果表明，TempSamp-R1在多个基准数据集上超越了基于GRPO的基线，并建立了新的最先进性能：</p>
<ul>
<li><strong>Charades-STA：</strong> R1@0.7达到52.9%，相对GRPO提升2.7%。</li>
<li><strong>ActivityNet Captions：</strong> R1@0.5达到56.0%，相对GRPO提升5.3%。</li>
<li><strong>QVHighlights：</strong> mAP达到30.0%，相对GRPO提升3.0%。</li>
</ul>
<p>此外，TempSamp-R1在有限数据下表现出强大的少样本泛化能力，这凸显了其在数据稀缺场景下的实用性。消融研究进一步证实了off-policy监督和软优势整形策略对提高性能和训练稳定性的重要性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文提到了TempSamp-R1的几个局限性：</p>
<ul>
<li><strong>依赖高质量off-policy监督：</strong> 该框架目前依赖于高质量的off-policy监督（例如，ground-truth时间戳），这在弱标注场景中可能无法获得。</li>
<li><strong>未探索其他视频推理任务：</strong> 尽管TempSamp-R1在时间定位和高光检测任务上进行了评估，但其在其他视频推理任务（例如，多事件跟踪）上的有效性仍有待探索。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
虽然论文没有明确列出未来研究方向，但从其局限性可以推断出以下潜在方向：</p>
<ul>
<li><strong>在弱监督或无监督场景下的应用：</strong> 探索TempSamp-R1在没有高质量ground-truth标注的情况下如何工作，例如通过自监督或弱监督学习来获取off-policy指导。</li>
<li><strong>扩展到更广泛的视频推理任务：</strong> 将TempSamp-R1应用于其他复杂的视频理解任务，如多事件跟踪、视频问答等，以验证其通用性和鲁棒性。</li>
<li><strong>进一步优化off-policy指导的获取：</strong> 研究更智能、更自适应的方法来生成或选择off-policy解决方案，以减少对外部标注的依赖。</li>
<li><strong>探索更复杂的奖励整形机制：</strong> 进一步研究非线性奖励整形，以适应更广泛的任务和数据分布，从而实现更精细的策略优化。</li>
</ul>
<p>总而言之，TempSamp-R1通过创新的混合策略采样、非线性软优势计算和混合CoT训练范式，显著提升了MLLMs在视频时间定位任务中的性能和稳定性，为长视频理解领域的强化学习微调开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks.</li>
<li>Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.18056v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.18056v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17941v1'></a></p>
<h2 id="composablenav-instruction-following-navigation-in-dynamic-environments-via-composable-diffusion"><a href="https://arxiv.org/abs/2509.17941v1">ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</a></h2>
<p><strong>Authors:</strong> Zichao Hu, Chen Tang, Michael J. Munje, Yifeng Zhu, Alex Liu, Shuijing Liu, Garrett Warnell, Peter Stone, Joydeep Biswas</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>This paper considers the problem of enabling robots to navigate dynamic
environments while following instructions. The challenge lies in the
combinatorial nature of instruction specifications: each instruction can
include multiple specifications, and the number of possible specification
combinations grows exponentially as the robot's skill set expands. For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.
Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training. Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives. Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines. Videos and additional materials can
be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zichao Hu等人撰写的论文“ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion”的全面摘要。</p>
<hr />
<h3 id="composablenav-instruction-following-navigation-in-dynamic-environments-via-composable-diffusion_1">论文摘要：ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决机器人在动态环境中遵循复杂指令进行导航的问题。核心挑战在于指令规范的组合性质：一个指令可能包含多个子规范（例如，“超车行人并靠右行驶”包含“超车行人”和“靠右行驶”两个规范），随着机器人技能集的扩展，可能的规范组合数量呈指数级增长，这使得传统的基于学习的方法（如模仿学习或强化学习）因数据和计算资源需求巨大而变得不切实际。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了应对上述挑战，ComposableNav提出了以下关键创新：
*   <strong>组合式扩散模型（Composable Diffusion Models）：</strong> 论文的核心思想是，遵循指令涉及独立满足其组成规范，每个规范对应一个独特的运动原语。ComposableNav利用扩散模型的可组合性，单独学习每个运动原语，然后在部署时并行组合这些原语，以满足训练中未见过的规范组合。
*   <strong>两阶段训练程序：</strong> 为了避免对每个单独运动原语进行演示的繁重需求，论文提出了一种两阶段训练方法：
    1.  <strong>监督式预训练（Supervised Pre-training）：</strong> 学习一个用于动态导航的基础扩散模型，生成多样化、无碰撞、目标导向的轨迹。
    2.  <strong>强化学习微调（Reinforcement Learning Fine-tuning）：</strong> 将基础模型塑造成不同的运动原语，通过设计特定于原语的奖励函数来确保指令依从性，从而避免了对特定演示数据集的需求。
*   <strong>实时部署机制：</strong> 结合模型预测控制器（MPC）和在线重规划策略，确保了ComposableNav在真实世界机器人上的实时性能。</p>
<p><strong>3. 主要结果及其意义：</strong>
通过仿真和真实世界实验，ComposableNav展示了显著的性能优势：
*   <strong>卓越的泛化能力：</strong> ComposableNav能够生成满足多样化且训练中未见过的规范组合的轨迹，显著优于非组合式VLM（视觉语言模型）策略和基于成本图组合的基线方法。
*   <strong>处理复杂指令：</strong> 随着指令复杂性（规范数量）的增加，ComposableNav的成功率保持较高，而所有基线方法的性能则迅速下降，这证明了其在处理复杂、未见过的指令组合方面的鲁棒性。
*   <strong>实时操作：</strong> 在真实世界机器人上部署时，ComposableNav实现了实时重规划，即使在涉及四个原语的最复杂情况下，初始规划平均仅需0.4秒，重规划仅需0.06秒。
*   <strong>无需特定演示数据：</strong> 两阶段训练方法成功地使扩散模型学习了有效的运动原语，而无需为每个原语提供专门的演示数据。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>运动原语数量有限：</strong> 目前只考虑了六种常用的导航运动原语，这些原语相对简单，可以用基于规则的奖励函数描述。手动设计奖励函数的可扩展性有限。
*   <strong>依赖上游模块：</strong> 论文假设指令解析为规范和相关环境观测（例如，通过LLM和VLM）可以由现有方法处理，这部分工作在实验中被抽象化。
*   <strong>组合策略的局限性：</strong> 尽管ComposableNav表现出色，但随着指令规范数量的增加，成功率仍有下降。这可能源于当前组合策略（通过对单个去噪网络的预测噪声求和）可能导致次优结果。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>利用VLM作为验证器：</strong> 探索使用VLM作为验证器来自动学习多样化和复杂的行为，以提高奖励函数设计的可扩展性。
*   <strong>集成高层VLM模块和任务规划器：</strong> 将ComposableNav与高层VLM模块和任务规划器结合，以实现更长期的指令遵循导航。
*   <strong>改进组合采样技术：</strong> 探索更先进的扩散模型采样技术（如哈密顿蒙特卡洛），以提高在更高指令复杂性下的组合性能。</p>
<hr />
<p>总而言之，ComposableNav通过引入可组合扩散模型和创新的两阶段训练流程，为机器人在动态环境中遵循复杂指令导航提供了一个强大且可扩展的解决方案，显著推动了该领域的研究进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.</li>
<li>Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training.</li>
<li>Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives.</li>
<li>Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17941v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17941v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17901v1'></a></p>
<h2 id="does-audio-matter-for-modern-video-llms-and-their-benchmarks"><a href="https://arxiv.org/abs/2509.17901v1">Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</a></h2>
<p><strong>Authors:</strong> Geewook Kim, Minjoon Seo</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV, cs.MM, cs.SD</p>
<p><strong>Abstract:</strong></p>
<p>Modern multimodal large language models often claim "video understanding,"
yet most evaluations use muted videos or simply discard audio. We ask a direct
question: how much does audio actually matter for contemporary Video-LLMs and
the benchmarks that certify them? We audit widely used suites and observe that
many items are even solvable from a single frame, rendering audio largely
redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio
encoder (e.g., Whisper) and analyze when audio helps, while addressing audio
token explosion with a lightweight Mamba-based state-space token compressor. We
find that audio yields minimal gains on recent video benchmarks but is decisive
on curated, audio-sensitive subsets. To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Geewook Kim和Minjoon Seo撰写的论文“Does Audio Matter for Modern Video-LLMs and Their Benchmarks?”的全面摘要。</p>
<hr />
<h3 id="does-audio-matter-for-modern-video-llms-and-their-benchmarks_1">论文摘要：Does Audio Matter for Modern Video-LLMs and Their Benchmarks?</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文的核心研究问题是：在当前的视频大型语言模型（Video-LLMs）及其评估基准中，音频究竟有多重要？作者指出，尽管现代多模态大型语言模型声称具备“视频理解”能力，但大多数评估却使用静音视频或直接忽略音频。这引发了一个关键疑问：这种做法是否掩盖了音频在真实视频理解中的潜在价值？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，论文提出了以下关键创新和方法论贡献：</p>
<ul>
<li><strong>音频敏感基准的审计与策展：</strong> 作者审计了广泛使用的视频理解基准套件，发现许多任务仅凭单帧视觉信息即可解决，使得音频信息在很大程度上是冗余的。为了解决这一问题，论文通过过滤掉可以仅凭单帧解决的项目，策展并发布了两个新的、更具挑战性的、音频敏感的基准数据集：<strong>AVQA-Hard</strong> 和 <strong>Music-AVQA-Hard</strong>。</li>
<li><strong>LLaVA-AV-SSM 模型架构：</strong> 论文基于强大的 LLaVA-OneVision 架构，通过附加一个语音/音频编码器（例如 Whisper）来注入音频 token，从而构建了 <strong>LLaVA-AV-SSM</strong> 模型。这种双编码器架构允许对音频的边际价值进行受控研究。</li>
<li><strong>基于 Mamba 的音频 token 压缩器：</strong> 针对音频 token 数量爆炸的问题（例如，一小时视频可能产生约 90k 个音频 token），论文引入了一种轻量级的、基于 Mamba 的状态空间模型（SSM）token 压缩器。该压缩器能够将长音频流聚合为紧凑的 token 集合，同时最大限度地减少信息损失，从而实现了可扩展的音视频 Video-LLMs。它通过在每 R 步插入一个可训练查询并仅保留查询位置的输出来实现 25 倍的压缩（从 25Hz 降至 1Hz）。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
论文的主要发现及其意义如下：</p>
<ul>
<li><strong>音频在现有基准中的作用有限：</strong> 在大多数现有视频基准上，添加音频带来的性能提升微乎其微，甚至在方差范围内，这与作者的审计结果一致，即这些基准很少需要音轨信息。</li>
<li><strong>音频在策展基准中的决定性作用：</strong> 在新策展的 AVQA-Hard 和 Music-AVQA-Hard 等音频敏感子集上，音频带来了显著的性能提升。例如，在 AVQA-Hard 上，准确率从 67.13% 提高到 71.58%。这验证了对音频敏感评估的需求，并强调了在这些任务中“倾听”的重要性。</li>
<li><strong>Mamba 压缩器的有效性：</strong> 基于 Mamba 的音频 token 压缩器不仅有效解决了音频 token 爆炸问题，实现了长视频的线性时间推理，而且在音频敏感基准上提升了准确率。这表明 Mamba 压缩器是实现可扩展音视频 Video-LLMs 的有效工具。</li>
<li><strong>学术实践与现实世界期望的差距：</strong> 论文的发现揭示了当前学术实践（忽略音频）与现实世界期望（用户自然地假设系统既“看”又“听”）之间日益扩大的差距。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提及的局限性包括：</p>
<ul>
<li><strong>音频编码器和数据的探索范围：</strong> 目前的研究主要使用了 Qwen2-Audio 的 Whisper-based 编码器，未来可以探索更广泛的音频编码器和数据集。</li>
<li><strong>统一的跨模态 SSM：</strong> 目前的音频压缩器是独立的，未来可以扩展到超越纯音频压缩，实现一个统一的跨模态 SSM，能够联合分配视觉和音频的预算。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文指出了以下潜在的未来研究方向：</p>
<ul>
<li>探索更广泛的音频编码器和训练数据，以进一步提升音视频 Video-LLMs 的性能。</li>
<li>开发统一的跨模态状态空间模型，能够智能地在视觉和音频模态之间分配计算资源，实现更高效、更全面的视频理解。</li>
<li>继续策展和开发更具挑战性的、真正需要音视频联合推理的基准，以推动该领域的发展。</li>
</ul>
<hr />
<p>总而言之，这篇论文对现代 Video-LLMs 中音频的作用进行了深入的批判性审视。通过引入音频敏感的基准和基于 Mamba 的高效音频压缩方法，作者不仅揭示了当前评估实践的不足，还为构建更符合现实世界需求的、能够真正“看”和“听”的音视频大型语言模型提供了实用的工具和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code.</li>
<li>Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17901v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17901v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17877v1'></a></p>
<h2 id="sight-over-site-perception-aware-reinforcement-learning-for-efficient-robotic-inspection"><a href="https://arxiv.org/abs/2509.17877v1">Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</a></h2>
<p><strong>Authors:</strong> Richard Kuhlmann, Jakob Wolfram, Boyang Sun, Jiaxu Xing, Davide Scaramuzza, Marc Pollefeys, Cesar Cadena</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous inspection is a central problem in robotics, with applications
ranging from industrial monitoring to search-and-rescue. Traditionally,
inspection has often been reduced to navigation tasks, where the objective is
to reach a predefined location while avoiding obstacles. However, this
formulation captures only part of the real inspection problem. In real-world
environments, the inspection targets may become visible well before their exact
coordinates are reached, making further movement both redundant and
inefficient. What matters more for inspection is not simply arriving at the
target's position, but positioning the robot at a viewpoint from which the
target becomes observable. In this work, we revisit inspection from a
perception-aware perspective. We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map. The learned policy
leverages both perceptual and proprioceptive sensing and is trained entirely in
simulation, before being deployed to a real-world robot. We further develop an
algorithm to compute ground-truth shortest inspection paths, which provides a
reference for evaluation. Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings. The project is avialable at
https://sight-over-site.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Richard Kuhlmann等人撰写的论文“Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection”的全面摘要。</p>
<hr />
<h3 id="sight-over-site-perception-aware-reinforcement-learning-for-efficient-robotic-inspection_1">论文摘要：Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection</h3>
<p><strong>1. 主要问题或研究问题：</strong>
传统的机器人自主巡检任务通常被简化为导航问题，即机器人需要到达预定义的物理位置并避开障碍物。然而，这种方法未能充分捕捉真实巡检任务的本质。在实际环境中，巡检目标可能在机器人到达其精确坐标之前就已经可见，导致后续移动冗余且低效。因此，论文旨在解决的核心问题是：如何使机器人能够从一个“感知感知”的角度进行巡检，即不仅仅是到达目标位置，而是将机器人定位在一个能够清晰观察到目标的视点，并以最短路径实现这一目标，同时不依赖于预先构建的地图。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>感知感知巡检任务的重新定义与框架：</strong> 论文将巡检任务重新定义为以目标可见性为核心，而非简单的点目标导航。为此，作者提出了一个端到端的强化学习（RL）框架，将目标可见性明确地作为主要优化目标。
*   <strong>无地图的RL策略：</strong> 提出的策略不依赖于全局地图，而是利用自我中心深度图像输入、机器人与目标之间的相对姿态以及过去的动作来学习高效的巡检行为。这种方法避免了显式地图构建的开销，使其能够扩展到大型环境。
*   <strong>模拟到真实世界的泛化能力：</strong> 策略完全在模拟环境中进行训练，但能够很好地泛化到未见的模拟环境以及真实的四足机器人（Boston Dynamics Spot），实现了零样本迁移。
*   <strong>地面真实最短巡检路径算法：</strong> 论文开发了一种算法来计算地面真实的最短巡检路径，为评估提供了可靠的参考基准。该算法考虑了可遍历性、传感器范围和可见性约束，通过A<em>搜索找到从起始点到目标可见视点的最短路径。
*   </em><em>奖励函数设计：</em>* 奖励函数被精心设计，包含稀疏的成功/失败奖励和密集的奖励项（如目标方向对齐、向最优视点移动、惩罚原地旋转和鼓励探索），以平衡导航和可见性，并避免局部最优。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>优于现有导航方法：</strong> 实验结果表明，该方法在模拟和真实世界环境中均优于现有的经典和基于学习的导航方法，能够生成更高效的巡检轨迹。
*   <strong>更高的成功加权路径长度（SPL）：</strong> 尽管在某些碰撞和滑行允许的设置下，DD-PPO的成功率可能略高，但本方法在SPL指标上表现更优，这意味着它能以更短的路径实现目标可见性。
*   <strong>强大的碰撞避免能力和鲁棒性：</strong> 在严格的无碰撞设置下，本方法仍能保持高成功率（81.49%），并展现出强大的碰撞避免能力。与DD-PPO相比，本策略在不同环境设置下表现出更好的鲁棒性，不易受模拟器特定动态的影响。
*   <strong>平衡导航与可见性：</strong> 策略能够有效地平衡导航和可见性，在目标较远时优先导航，在目标附近时则转向寻找最佳视点，从而避免了冗余移动。
*   <strong>成功的Sim-to-Real迁移：</strong> 在Boston Dynamics Spot机器人上的真实世界实验验证了策略的零样本迁移能力，证明了其在实际应用中的有效性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>地面真实路径计算的效率问题：</strong> 在RL训练过程中，重复计算地面真实最短巡检路径（包括可见性检查）效率低下，因此在训练时采用了基于奖励函数的设计而非直接使用地面真实路径作为奖励信号。
*   <strong>传感器噪声：</strong> 真实世界实验中，深度传感器测量存在噪声，需要通过平均深度值等方法来缓解其对可见性判断的影响。
*   <strong>训练环境的复杂性：</strong> 尽管在Habitat模拟器上进行了训练，但真实世界的复杂性和不可预测性仍可能带来挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更复杂的巡检场景：</strong> 探索在更复杂、动态或大规模环境中的巡检任务，例如多目标巡检或需要与环境交互的巡检。
*   <strong>结合语义信息：</strong> 将语义信息（例如目标类型、功能区域）融入感知感知巡检框架，以实现更智能、更具上下文意识的巡检行为。
*   <strong>在线适应与持续学习：</strong> 研究如何在部署后使策略能够在线适应新的环境或目标，实现持续学习和性能提升。
*   <strong>多模态感知融合：</strong> 探索除了深度图像之外，结合RGB图像、激光雷达等多种传感器模态，以获取更丰富、更鲁棒的环境感知。
*   <strong>可解释性与安全性：</strong> 进一步提高RL策略的可解释性，并增强其在安全关键巡检任务中的可靠性和安全性保障。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map.</li>
<li>Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17877v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17877v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17786v1'></a></p>
<h2 id="accurate-and-efficient-low-rank-model-merging-in-core-space"><a href="https://arxiv.org/abs/2509.17786v1">Accurate and Efficient Low-Rank Model Merging in Core Space</a></h2>
<p><strong>Authors:</strong> Aniello Panariello, Daniel Marczak, Simone Magistri, Angelo Porrello, Bartłomiej Twardowski, Andrew D. Bagdanov, Simone Calderara, Joost van de Weijer</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we address the challenges associated with merging low-rank
adaptations of large neural networks. With the rise of parameter-efficient
adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning
has become more accessible. While fine-tuning models with LoRA is highly
efficient, existing merging methods often sacrifice this efficiency by merging
fully-sized weight matrices. We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks. We further provide a formal proof that
projection into Core Space ensures no loss of information and provide a
complexity analysis showing the efficiency gains. Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources. Codebase is available at
https://github.com/apanariello4/core-space-merging.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Aniello Panariello等人撰写的论文“Accurate and Efficient Low-Rank Model Merging in Core Space”的全面摘要。</p>
<hr />
<h3 id="accurate-and-efficient-low-rank-model-merging-in-core-space_1">论文摘要：Accurate and Efficient Low-Rank Model Merging in Core Space</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决大型神经网络的低秩适应（如LoRA）模型合并所面临的挑战。尽管LoRA微调本身效率很高，但现有的模型合并方法通常通过合并全尺寸权重矩阵来牺牲这种效率，导致计算成本高昂且无法扩展到大型模型。核心问题是如何在保持低秩适应效率的同时，有效且准确地合并这些模型，以处理多任务学习场景。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了一个名为“Core Space”的创新合并框架，其核心贡献包括：</p>
<ul>
<li><strong>Core Space 合并框架：</strong> 引入了一个参数高效的子空间——Core Space，作为所有任务特定低秩组件的通用对齐基础。它允许在保持低秩适应效率的同时，在更小的维度空间（r x r）中执行模型合并，而无需重建全尺寸权重矩阵。</li>
<li><strong>信息无损投影：</strong> 论文提供了正式的证明，表明投影到Core Space并返回原始空间是无损的，确保在合并过程中不会丢失信息。Core Space被设计为可逆的，并且其维度仅取决于任务数量和LoRA秩，与基础模型大小无关。</li>
<li><strong>效率提升：</strong> 通过在Core Space中进行合并，论文显著降低了计算成本。与现有方法（如KnOTS）相比，Core Space合并在计算资源使用上实现了显著的加速（例如，对于Llama 3 8B模型，速度提升超过600倍）。</li>
<li><strong>子空间对齐改进：</strong> Core Space通过强制所有任务共享一个公共基础来提高子空间对齐比率（SAR），过滤掉任务特定噪声并促进对齐，从而提高合并模型的性能。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
论文通过广泛的实证结果证明了Core Space合并的有效性：</p>
<ul>
<li><strong>最先进的性能：</strong> 在视觉和语言任务（包括ViT-B/32、ViT-L/14和Llama 3 8B骨干网络）上，Core Space显著优于现有合并技术，并实现了最先进的性能。例如，在Llama 3 8B上，它将TSV的平均归一化准确率提升至94.16%。</li>
<li><strong>计算效率：</strong> Core Space合并在保持高准确率的同时，仅使用了竞争方法一小部分的计算资源。其时间复杂度与Task Arithmetic在全空间中的复杂度相当，但避免了高维SVD操作。</li>
<li><strong>对异构秩和额外PEFT方法的通用性：</strong> Core Space能够无缝处理具有异构LoRA秩的模型合并，并且可以扩展到其他PEFT方法，如VeRA，同样表现出优越的性能。</li>
<li><strong>信息密度：</strong> 分析表明，Core Space是一个信息密集的空间，截断任何组件都会导致性能下降，这与全空间中存在大量冗余或未使用组件形成对比。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及显著的局限性。然而，可以推断出一些潜在的方面：</p>
<ul>
<li><strong>线性合并函数的假设：</strong> 尽管论文证明了Core Space在非线性合并函数下也能提高性能，但其信息无损特性在理论上主要依赖于合并函数是线性的情况（如Task Arithmetic）。对于更复杂的非线性合并策略，其理论保证可能需要进一步的细化。</li>
<li><strong>参考基础的选择：</strong> 论文虽然展示了其选择的参考基础（通过垂直堆叠A(t)和水平堆叠B(t)的SVD获得）是最佳的，但对于其他可能的参考基础选择，其性能可能会有所不同。</li>
<li><strong>特定任务设置：</strong> 论文主要关注多任务合并，即合并多个在不同任务上微调的模型。对于其他合并场景（例如，模型压缩或知识蒸馏），Core Space的适用性可能需要进一步探索。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文为未来的研究提供了几个潜在方向：</p>
<ul>
<li><strong>更复杂的合并策略：</strong> 探索在Core Space中应用更先进、更复杂的非线性合并策略，以进一步提高性能或解决特定挑战。</li>
<li><strong>理论泛化：</strong> 进一步研究Core Space框架在更广泛的PEFT方法和模型架构上的理论泛化能力。</li>
<li><strong>动态Core Space：</strong> 探索动态调整Core Space维度或结构的方法，以适应不断变化的任务数量或模型复杂性。</li>
<li><strong>与其他模型压缩技术的结合：</strong> 将Core Space合并与量化、剪枝等其他模型压缩技术相结合，以实现更极致的模型效率。</li>
<li><strong>在更广泛应用中的评估：</strong> 在更多样化的应用领域（例如，机器人、强化学习等）中评估Core Space合并的有效性。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入Core Space框架，为低秩适应模型的合并提供了一个高效且准确的解决方案。它不仅解决了现有方法在效率上的不足，还在多任务视觉和语言任务上取得了显著的性能提升，为大型模型的参数高效适应和多任务学习开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks.</li>
<li>Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17786v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17786v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17773v1'></a></p>
<h2 id="i2vwm-robust-watermarking-for-image-to-video-generation"><a href="https://arxiv.org/abs/2509.17773v1">I2VWM: Robust Watermarking for Image to Video Generation</a></h2>
<p><strong>Authors:</strong> Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The rapid progress of image-guided video generation (I2V) has raised concerns
about its potential misuse in misinformation and fraud, underscoring the urgent
need for effective digital watermarking. While existing watermarking methods
demonstrate robustness within a single modality, they fail to trace source
images in I2V settings. To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos. Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.
I2VWM leverages a video-simulation noise layer during training and employs an
optical-flow-based alignment module during inference. Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.
\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code
Released.}</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang撰写的论文“I2VWM: Robust Watermarking for Image to Video Generation”的全面摘要。</p>
<hr />
<h3 id="i2vwm">论文摘要：I2VWM：图像到视频生成中的鲁棒水印技术</h3>
<p><strong>1. 主要问题或研究问题：</strong>
随着图像引导视频生成（I2V）技术的快速发展，其在虚假信息传播和欺诈方面的潜在滥用引起了广泛关注。现有的数字水印方法虽然在单一模态内表现出鲁棒性，但在I2V场景中，它们无法有效追踪源图像，即水印信号在视频生成过程中难以保持其时间上的鲁棒性。这导致了一个核心问题：如何在跨模态（图像到视频）生成过程中，确保水印信号的持久性和可验证性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，本论文提出了以下关键创新和方法论贡献：
*   <strong>鲁棒扩散距离（Robust Diffusion Distance, RDD）概念的引入：</strong> 首次定义了RDD，用于量化水印信号在生成视频中随时间推移的持久性，即水印在视频中仍能可靠验证的最大帧索引。这为评估I2V场景下的水印鲁棒性提供了一个新的量化指标。
*   <strong>I2VWM框架：</strong> 提出了一种跨模态水印框架I2VWM，旨在增强水印在时间维度上的鲁棒性。该框架基于编码器-解码器架构，并包含以下核心组件：
    *   <strong>视频模拟噪声层（Video-simulation Noise Layer）：</strong> 在训练阶段引入，通过模拟视频生成固有的压缩、噪声添加/移除和像素偏移等失真，使水印模型能够显式地学习并抵抗这些生成引起的修改，从而提高水印信号的鲁棒性。
    *   <strong>基于光流的对齐模块（Optical-flow-based Alignment Module）：</strong> 在推理阶段使用，通过估计视频帧与参考帧之间的光流，将远离初始帧的视频帧对齐，从而补偿时间上的退化，显著延长水印的鲁棒扩散距离。
    *   <strong>投票机制（Voting Scheme）：</strong> 在视频水印提取模式下，对所有对齐帧提取的水印进行聚合，以获得最终的水印。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著提高鲁棒性：</strong> 在多个开源（如CogVideoX, Wan, Hunyuan, Stable Video Diffusion）和商业I2V模型上的实验表明，I2VWM在I2V场景下显著提高了水印的鲁棒性，尤其是在鲁棒扩散距离（RDD）和帧准确率（FACC）方面优于现有基线方法。
*   <strong>保持不可感知性：</strong> I2VWM在提高鲁棒性的同时，仍能保持水印的不可感知性，通过PSNR、SSIM和LPIPS等视觉质量指标验证了其性能。
*   <strong>泛化能力强：</strong> I2VWM在不同生成模型上的表现一致，表明其具有强大的泛化能力，不依赖于特定的生成器。
*   <strong>对经典噪声的鲁棒性：</strong> I2VWM对经典图像失真（如裁剪、高斯模糊、JPEG压缩等）也表现出具有竞争力的鲁棒性，这得益于其噪声层设计中对数值和几何失真的综合考虑。
*   <strong>光流对齐模块的有效性：</strong> 消融实验证实，基于光流的对齐模块能够显著提升水印提取的准确性，尤其是在时间上远离初始帧的视频帧中，进一步验证了其对水印信号持久性的贡献。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>对时间顺序的依赖：</strong> I2VWM的有效性依赖于视频中一致的时间顺序假设，无法处理时间裁剪或帧打乱等失真。
*   <strong>未完全解决经典失真：</strong> 论文指出，I2VWM尚未完全解决所有经典失真，例如随机旋转，这在视频帧处理中也常见。
*   <strong>缺乏视频生成质量指标：</strong> 论文的评估缺乏对视频生成质量的指标。作者认为，高质量的视频往往能更好地保留源图像中的水印信号，因此平衡水印鲁棒性与视频质量是一个重要的考量。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>处理时间无序失真：</strong> 进一步研究如何使I2VWM能够应对时间裁剪或帧打乱等破坏时间顺序的失真，以提高数据保护能力。
*   <strong>增强对所有经典失真的鲁棒性：</strong> 探索更全面的方法来解决包括随机旋转在内的所有经典失真。
*   <strong>整合视频生成质量评估：</strong> 在未来的工作中，将视频生成质量指标纳入评估体系，以更全面地衡量水印方法在保持水印鲁棒性和视频质量之间的平衡。
*   <strong>深入研究噪声层设计：</strong> 进一步探索和优化噪声层设计，以应对更复杂的数值和几何失真组合。</p>
<hr />
<p>总而言之，这篇论文首次提出了图像到视频生成场景下的跨模态鲁棒水印挑战，并引入了“鲁棒扩散距离”这一新颖指标。通过结合视频模拟噪声层和基于光流的对齐模块，I2VWM框架在保持水印不可感知性的同时，显著提升了水印信号在生成视频中的时间鲁棒性。这项工作为生成视频时代下的跨模态水印技术开辟了新范式，具有重要的理论和实际意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos.</li>
<li>Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.</li>
<li>Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17773v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17773v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-23 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
