<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-05 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-04/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-08/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-05">Arxiv Computer Vision Papers - 2025-12-05</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#sima-2-a-generalist-embodied-agent-for-virtual-worlds" class="nav-link">SIMA 2: A Generalist Embodied Agent for Virtual Worlds</a>
                </li>
                <li class="nav-item">
                    <a href="#the-universal-weight-subspace-hypothesis" class="nav-link">The Universal Weight Subspace Hypothesis</a>
                </li>
                <li class="nav-item">
                    <a href="#light-x-generative-4d-video-rendering-with-camera-and-illumination-control" class="nav-link">Light-X: Generative 4D Video Rendering with Camera and Illumination Control</a>
                </li>
                <li class="nav-item">
                    <a href="#value-gradient-guidance-for-flow-matching-alignment" class="nav-link">Value Gradient Guidance for Flow Matching Alignment</a>
                </li>
                <li class="nav-item">
                    <a href="#splannequin-freezing-monocular-mannequin-challenge-footage-with-dual-detection-splatting" class="nav-link">Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#draco-draft-as-cot-for-text-to-image-preview-and-rare-concept-generation" class="nav-link">DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#arm-thinker-reinforcing-multimodal-generative-reward-models-with-agentic-tool-use-and-visual-reasoning" class="nav-link">ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#stare-vla-progressive-stage-aware-reinforcement-for-fine-tuning-vision-language-action-models" class="nav-link">STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</a>
                </li>
                <li class="nav-item">
                    <a href="#neuralremaster-phase-preserving-diffusion-for-structure-aligned-generation" class="nav-link">NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#tv2tv-a-unified-framework-for-interleaved-language-and-video-generation" class="nav-link">TV2TV: A Unified Framework for Interleaved Language and Video Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-05">Arxiv Computer Vision Papers - 2025-12-05</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月4日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月4日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>通用智能体（Embodied AI）、多模态生成（Multimodal Generation）、三维/四维内容生成与理解（3D/4D Content Generation and Understanding）</strong>以及<strong>模型泛化与效率（Model Generalization and Efficiency）</strong>方面的显著进展。特别值得注意的是，研究正朝着更强大的交互式虚拟世界代理、更精细的视觉内容生成控制以及更高效的模型训练和推理方法发展。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>通用智能体与交互式虚拟世界：</strong> <strong>SIMA 2</strong> 展现了在虚拟世界中执行复杂任务的通用智能体能力，预示着AI在模拟环境中的应用潜力。</li>
<li><strong>精细化内容生成与控制：</strong> <strong>Light-X</strong> 在4D视频渲染方面实现了相机和光照的精细控制，<strong>Splannequin</strong> 能够从单目视频中生成高质量的3D人体模型，而 <strong>NeuralRemaster</strong> 则通过相位保持扩散模型实现结构对齐生成。</li>
<li><strong>多模态与推理增强：</strong> <strong>ARM-Thinker</strong> 和 <strong>STARE-VLA</strong> 分别探索了通过强化学习、工具使用和视觉推理来增强多模态生成模型，显示了AI在理解和生成与视觉相关的动作和语言方面的进步。</li>
<li><strong>文本到图像的创新：</strong> <strong>DraCo</strong> 提出了“草稿即思维链”的方法，用于文本到图像的预览和稀有概念生成，为提高生成质量和可控性提供了新思路。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>具身智能（Embodied AI）的泛化能力：</strong> 从特定任务到通用任务的智能体发展是重要趋势。</li>
<li><strong>4D内容生成与实时渲染：</strong> 对动态三维场景的生成和控制需求日益增长。</li>
<li><strong>扩散模型的精细化控制与结构对齐：</strong> 克服扩散模型在生成特定结构和保持相位信息方面的挑战。</li>
<li><strong>多模态模型的强化与推理能力：</strong> 将视觉、语言和动作信息更有效地结合，并引入推理机制。</li>
<li><strong>模型训练效率与泛化性：</strong> <strong>The Universal Weight Subspace Hypothesis</strong> 提出了一个关于模型权重泛化性的理论假设，可能对未来模型设计产生深远影响。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在影响和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>SIMA 2: A Generalist Embodied Agent for Virtual Worlds:</strong> 对于关注具身智能和AI在模拟环境中的应用的研究者至关重要。</li>
<li><strong>Light-X: Generative 4D Video Rendering with Camera and Illumination Control:</strong> 在动态三维内容生成领域具有重要意义，尤其是在控制方面。</li>
<li><strong>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation:</strong> 对于文本到图像生成的研究者，其新颖的生成策略值得关注。</li>
<li><strong>ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning:</strong> 在多模态AI和强化学习交叉领域具有前沿性。</li>
<li><strong>The Universal Weight Subspace Hypothesis:</strong> 如果您对模型泛化和理论基础感兴趣，这篇论文提供了新的视角。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速了解近期 Arxiv 计算机视觉领域的关键进展。希望它能为您提供有价值的参考。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.04797v1">SIMA 2: A Generalist Embodied Agent for Virtual Worlds</a></li>
<li><a href="#2512.05117v1">The Universal Weight Subspace Hypothesis</a></li>
<li><a href="#2512.05115v1">Light-X: Generative 4D Video Rendering with Camera and Illumination Control</a></li>
<li><a href="#2512.05116v1">Value Gradient Guidance for Flow Matching Alignment</a></li>
<li><a href="#2512.05113v1">Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a></li>
<li><a href="#2512.05112v1">DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</a></li>
<li><a href="#2512.05111v1">ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</a></li>
<li><a href="#2512.05107v1">STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</a></li>
<li><a href="#2512.05106v1">NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</a></li>
<li><a href="#2512.05103v1">TV2TV: A Unified Framework for Interleaved Language and Video Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.04797v1'></a></p>
<h2 id="sima-2-a-generalist-embodied-agent-for-virtual-worlds"><a href="https://arxiv.org/abs/2512.04797v1">SIMA 2: A Generalist Embodied Agent for Virtual Worlds</a></h2>
<p><strong>Authors:</strong>  SIMA team, Adrian Bolton, Alexander Lerchner, Alexandra Cordell, Alexandre Moufarek, Andrew Bolt, Andrew Lampinen, Anna Mitenkova, Arne Olav Hallingstad, Bojan Vujatovic, Bonnie Li, Cong Lu, Daan Wierstra, Daniel P. Sawyer, Daniel Slater, David Reichert, Davide Vercelli, Demis Hassabis, Drew A. Hudson, Duncan Williams, Ed Hirst, Fabio Pardo, Felix Hill, Frederic Besse, Hannah Openshaw, Harris Chan, Hubert Soyer, Jane X. Wang, Jeff Clune, John Agapiou, John Reid, Joseph Marino, Junkyung Kim, Karol Gregor, Kaustubh Sridhar, Kay McKinney, Laura Kampis, Lei M. Zhang, Loic Matthey, Luyu Wang, Maria Abi Raad, Maria Loks-Thompson, Martin Engelcke, Matija Kecman, Matthew Jackson, Maxime Gazeau, Ollie Purkiss, Oscar Knagg, Peter Stys, Piermaria Mendolicchio, Raia Hadsell, Rosemary Ke, Ryan Faulkner, Sarah Chakera, Satinder Singh Baveja, Shane Legg, Sheleem Kashem, Tayfun Terzi, Thomas Keck, Tim Harley, Tim Scholtes, Tyson Roberts, Volodymyr Mnih, Yulan Liu, Zhengdong Wang, Zoubin Ghahramani</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：SIMA 2: A Generalist Embodied Agent for Virtual Worlds</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>SIMA 2 是一项重大的进展，它引入了一个能够理解并执行广泛 3D 虚拟世界任务的通用具身智能体。该智能体基于 Gemini 基础模型，能够进行高层次的推理、多模态交互（语言和图像）以及与用户进行对话，显著缩小了与人类在虚拟环境中表现的差距，并展现出强大的泛化能力。更重要的是，SIMA 2 能够通过 Gemini 生成任务和奖励，实现开放式的自我学习和技能提升，为构建通用且持续学习的虚拟和物理世界智能体铺平了道路。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>Gemini 基础模型的赋能：</strong> 这是 SIMA 2 最核心的创新之一。将强大的通用基础模型（Gemini）应用于具身智能体，使其能够继承和利用其强大的语言理解、推理和多模态处理能力。这与以往仅依赖特定任务训练的具身智能体截然不同。</li>
<li><strong>从简单指令到交互式伙伴的飞跃：</strong> SIMA 2 不再局限于执行简单的语言命令，而是能够理解高层次的目标，与用户进行对话，并处理更复杂的指令。这标志着具身智能体从“执行者”向“合作者”的转变。</li>
<li><strong>多模态输入处理（语言与图像）：</strong> 能够同时理解语言和图像指令，这对于在复杂的虚拟环境中进行导航、操作和决策至关重要。这直接关联到计算机视觉中的场景理解、目标识别和意图推断。</li>
<li><strong>强大的泛化能力：</strong> 能够在未曾见过的环境中展现出鲁棒的性能，这表明其学习到的策略和表征具有高度的通用性，而非仅仅针对特定环境进行过拟合。</li>
<li><strong>开放式自我改进机制：</strong> 利用 Gemini 生成任务和奖励，实现自主学习新技能。这是一种强大的强化学习范式，能够让智能体在没有人工干预的情况下不断提升能力，这对于构建真正智能的代理至关重要。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动通用具身智能体的发展：</strong> SIMA 2 的成功将极大地推动通用具身智能体（Generalist Embodied Agents）的研究和发展。它证明了通过强大的基础模型和有效的训练策略，可以构建出能够适应多种环境和任务的智能体。</li>
<li><strong>加速人机交互在虚拟世界中的应用：</strong> 这种能够理解复杂指令、进行对话并与用户协作的智能体，将极大地提升虚拟世界中的人机交互体验，使其更加自然、高效和富有成效。</li>
<li><strong>为物理世界的具身智能体奠定基础：</strong> 尽管论文聚焦于虚拟世界，但其在泛化、多模态理解和自主学习方面的进展，为未来在物理世界中构建类似的通用具身智能体提供了重要的理论和技术基础。</li>
<li><strong>重新定义游戏 AI：</strong> 在游戏领域，SIMA 2 的能力将带来前所未有的游戏体验，玩家可以与更智能、更具互动性的 NPC 进行交流和合作。</li>
<li><strong>促进多模态学习和强化学习的融合：</strong> 该研究有效地融合了多模态学习（语言与视觉）和强化学习，展示了这种融合的巨大潜力。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> SIMA 2 可以作为虚拟环境中的智能助手、向导或交互伙伴，极大地提升用户在 VR/AR 中的沉浸感和交互性。</li>
<li><strong>游戏开发：</strong> 创造更智能、更具挑战性的游戏 NPC，以及更具动态性和适应性的游戏环境。</li>
<li><strong>机器人学：</strong> 为物理世界的机器人提供更高级的感知、理解和决策能力，使其能够更好地与人类协作并适应复杂环境。</li>
<li><strong>教育和培训：</strong> 在虚拟环境中创建逼真的模拟场景，用于技能培训和教育，智能体可以扮演教师、学生或模拟对象。</li>
<li><strong>数字孪生和模拟：</strong> 在复杂的数字孪生环境中，SIMA 2 可以作为操作员或分析师，执行任务、监控系统并提供洞察。</li>
<li><strong>内容创作：</strong> 在虚拟世界中辅助用户进行内容创作，例如通过语言指令生成场景元素或进行场景布局。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>“虚拟世界”的局限性：</strong> 尽管论文强调了其对物理世界的潜在影响，但目前的研究成果仍局限于虚拟环境。将这些能力完全迁移到物理世界仍然面临巨大的挑战，例如传感器噪声、物理交互的复杂性、安全性和实时性要求等。</li>
<li><strong>“通用性”的程度：</strong> 摘要中提到“广泛的 3D 虚拟世界”，但“广泛”的程度和具体涵盖的领域并未详细说明。其在高度专业化或非常规的虚拟环境中的表现仍有待验证。</li>
<li><strong>“接近人类表现”的量化：</strong> 摘要提到“ substantially closes the gap with human performance”，但“substantially”是一个相对模糊的词语。具体在哪些任务上、在多大程度上接近人类表现，需要更详细的数据和评估指标来支撑。</li>
<li><strong>“核心推理能力”的保留：</strong> 摘要提到“retaining the base model's core reasoning capabilities”。这暗示了在适应具身环境的过程中，可能存在一定程度的推理能力损失或需要额外的微调来维持。</li>
<li><strong>“开放式自我改进”的效率和安全性：</strong> 虽然自主学习是强大的能力，但其学习效率、学习过程的稳定性以及潜在的“学坏”风险（例如，学习到不安全或不期望的行为）是需要关注的问题。</li>
<li><strong>计算资源需求：</strong> 基于 Gemini 这样的基础模型，以及在复杂虚拟环境中进行训练和推理，很可能需要巨大的计算资源，这可能会限制其部署和应用范围。</li>
<li><strong>对“SIMA 1”的改进：</strong> 摘要提到与 SIMA 1 的对比，暗示 SIMA 1 在某些方面存在不足，例如指令的复杂性。SIMA 2 的改进是显著的，但其在哪些方面仍然不如人类，或者在哪些方面仍然存在 SIMA 1 的遗留问题，并未在摘要中详述。</li>
</ul>
<p><strong>对计算机视觉领域的趣味性或重要性：</strong></p>
<p>对于计算机视觉领域而言，SIMA 2 的研究具有以下几个关键的趣味性和重要性：</p>
<ul>
<li><strong>场景理解与表征的进步：</strong> SIMA 2 需要在复杂的 3D 虚拟环境中进行导航和交互，这意味着它必须具备对场景的深度理解，包括物体识别、空间关系推理、场景语义理解等。这直接推动了计算机视觉在这些方面的研究。</li>
<li><strong>多模态融合的典范：</strong> 能够同时处理语言和图像指令，是多模态计算机视觉研究的一个重要方向。SIMA 2 的成功表明，将强大的语言模型与视觉感知能力有效融合，能够解锁更高级的智能行为。这对于理解图像内容、关联文本描述以及根据指令进行视觉搜索和操作至关重要。</li>
<li><strong>具身视觉（Embodied Vision）的深化：</strong> SIMA 2 是具身智能体研究的代表，而具身视觉是其核心组成部分。它强调了视觉信息不仅仅是用于“看”，更是用于“行动”和“交互”。这促使计算机视觉研究从被动感知转向主动探索和利用视觉信息来完成任务。</li>
<li><strong>泛化能力的研究：</strong> 强大的泛化能力意味着 SIMA 2 学习到的视觉表征和推理机制具有高度的鲁棒性，能够适应不同的环境和任务。这对于开发能够在真实世界复杂多变环境中工作的视觉系统至关重要。</li>
<li><strong>强化学习与视觉的结合：</strong> SIMA 2 的自主学习能力，依赖于其能够从视觉输入中学习到有效的策略。这推动了强化学习与计算机视觉的深度结合，例如通过视觉信息来指导探索、评估奖励以及更新策略。</li>
<li><strong>对未来视觉应用的影响：</strong> SIMA 2 的成功预示着未来视觉系统将不再仅仅是识别和分类，而是能够理解、推理并主动参与到复杂的现实世界交互中，这将极大地拓展计算机视觉的应用边界。</li>
</ul>
<p>总而言之，SIMA 2 的研究代表了具身智能体领域的一个重要里程碑，其核心创新在于利用强大的基础模型实现通用性、多模态交互和自主学习。对于计算机视觉领域而言，这项工作不仅验证了多模态融合和具身视觉的潜力，更指明了未来视觉系统向更高级的理解和行动能力发展的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds.</li>
<li>Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.04797v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.04797v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05117v1'></a></p>
<h2 id="the-universal-weight-subspace-hypothesis"><a href="https://arxiv.org/abs/2512.05117v1">The Universal Weight Subspace Hypothesis</a></h2>
<p><strong>Authors:</strong> Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“The Universal Weight Subspace Hypothesis”的全面中文摘要：</p>
<p><strong>论文题目：</strong> The Universal Weight Subspace Hypothesis (通用权重子空间假设)</p>
<p><strong>作者：</strong> Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille</p>
<p><strong>摘要：</strong></p>
<p>这篇论文的核心研究问题是：<strong>深度神经网络在训练过程中，其权重参数是否会系统性地收敛到相似的低维子空间，并且这种现象是否独立于模型的初始化、训练任务和数据领域。</strong> 作者提出了“通用权重子空间假设”，认为深度神经网络在训练过程中，无论采用何种架构、任务或数据，都会倾向于收敛到一个共享的、低维的参数子空间。</p>
<p><strong>关键创新与方法论贡献：</strong></p>
<ol>
<li><strong>大规模实证分析：</strong> 论文提供了首个大规模的实证证据，通过对超过1100个模型（包括500个Mistral-7B LoRA适配器、500个Vision Transformer模型和50个LLaMA-8B模型）进行模式（mode-wise）谱分析，来验证通用权重子空间假设。</li>
<li><strong>谱分解技术：</strong> 作者应用谱分解技术（特别是截断零均值高阶SVD，Truncated Zero-Centered Higher-Order SVD - HOSVD）来分析不同模型权重矩阵的谱特性。</li>
<li><strong>识别稀疏、联合子空间：</strong> 通过分析，论文识别出了在不同任务和数据集上训练的共享架构模型中，普遍存在的稀疏、联合的低维子空间。这些子空间能够捕获模型权重中绝大部分的方差。</li>
<li><strong>理论分析：</strong> 论文还提供了理论分析，证明了在一定条件下，学习到的共享子空间可以收敛到真实的通用子空间，并给出了收敛速率的界限。</li>
</ol>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>普遍存在的低维子空间：</strong> 研究发现，不同架构、不同任务、不同初始化设置的深度神经网络，其权重参数会系统性地收敛到相似的低维谱子空间。这表明神经网络内部存在一种内在的组织结构，即“通用权重子空间”。</li>
<li><strong>模型压缩与效率提升：</strong> 这一发现具有重大的实际意义。它意味着我们可以通过存储这些低维子空间的系数，而不是完整的模型权重，来实现大规模模型的显著压缩（例如，100倍的内存节省）。</li>
<li><strong>高效的模型适应与迁移：</strong> 通用子空间的存在使得模型能够更高效地适应新任务，只需学习少量任务特定的系数，而无需重新训练或存储完整的模型。这对于多任务学习、模型合并（model merging）和参数高效的微调（parameter-efficient fine-tuning）至关重要。</li>
<li><strong>对神经网络泛化和学习机制的洞察：</strong> 这一发现为理解神经网络为何能够泛化到未见过的数据、为何不同的初始化能够收敛到相似的表示，以及为何权重共享和参数高效微调技术有效等难题提供了新的视角。</li>
<li><strong>环境效益：</strong> 通过提高训练和推理效率，减少计算资源需求，该研究有助于降低大型AI模型的碳足迹，推动AI的可持续发展。</li>
</ul>
<p><strong>论文中提到的局限性：</strong></p>
<ul>
<li><strong>解释性挑战：</strong> 论文承认，对通用共享子空间及其具体方向的解释性分析仍然是一个具有挑战性的研究领域，尤其是在处理大型模型和多层网络时。</li>
<li><strong>对预训练模型的依赖：</strong> 当前的方法依赖于预训练的特定任务模型来提取通用子空间，而对于新任务，可能无法轻易获得这些预训练模型。</li>
<li><strong>跨架构比较的开放性问题：</strong> 论文提出了跨架构比较的问题：不同架构的通用子空间有何差异？能否显式设计架构来优化子空间的几何形状？</li>
<li><strong>多样性瓶颈的担忧：</strong> 论文也提出了一个根本性的问题：如果神经网络系统性地收敛到相同的子空间，是否会带来多样性不足的瓶颈？是否需要设计方法来打破这种收敛？</li>
</ul>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>模型无关的通用子空间学习：</strong> 探索不依赖于预训练模型，而是直接从数据中学习通用共享子空间的方法。</li>
<li><strong>跨架构的通用子空间比较与设计：</strong> 深入研究不同模型架构的通用子空间特性，并探索如何设计架构以优化这些子空间的几何形状。</li>
<li><strong>打破收敛以增加多样性：</strong> 研究如何开发方法来打破神经网络对通用子空间的系统性收敛，以增加模型的多样性，避免潜在的瓶颈。</li>
<li><strong>任务算术（Task Arithmetic）在通用子空间中的应用：</strong> 探索如何在通用共享子空间的框架下进行任务算术，这可能是一个非平凡但有价值的研究方向。</li>
<li><strong>更深入的解释性分析：</strong> 尽管存在挑战，但对通用子空间及其方向进行更深入的解释性分析仍然是未来研究的重要方向。</li>
</ul>
<p><strong>总结：</strong></p>
<p>“The Universal Weight Subspace Hypothesis”论文通过大规模的实证研究和理论分析，有力地证明了深度神经网络在训练过程中会收敛到一个共享的、低维的参数子空间。这一发现不仅为理解神经网络的学习机制提供了新的见解，更重要的是，它为提高模型效率、实现模型重用、加速新任务适应以及降低AI的计算和环境成本开辟了新的途径，对整个深度学习领域具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces.</li>
<li>Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05117v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05117v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05115v1'></a></p>
<h2 id="light-x-generative-4d-video-rendering-with-camera-and-illumination-control"><a href="https://arxiv.org/abs/2512.05115v1">Light-X: Generative 4D Video Rendering with Camera and Illumination Control</a></h2>
<p><strong>Authors:</strong> Tianqi Liu, Zhaoxi Chen, Zihao Huang, Shaocong Xu, Saining Zhang, Chongjie Ye, Bohan Li, Zhiguo Cao, Wei Li, Hao Zhao, Ziwei Liu</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Light-X: Generative 4D Video Rendering with Camera and Illumination Control</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>本论文提出了Light-X，一个创新的视频生成框架，首次实现了在单目视频输入下，对生成视频的相机视角和场景光照进行联合、精细化控制。其核心贡献在于提出了一种解耦几何与光照信号的设计，并通过Light-Syn数据合成管线解决了训练数据稀缺的问题，从而在联合相机-光照控制和视频重光照方面取得了显著的性能提升。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<ul>
<li>
<p><strong>解耦几何与光照信号的设计 (Disentangled Design):</strong> 这是Light-X最核心的创新。它将场景的几何信息（包括动态点云和运动）与光照信息进行明确的分离。</p>
<ul>
<li><strong>几何与运动表示:</strong> 通过用户定义的相机轨迹，将动态点云沿着轨迹投影，从而捕捉场景的几何结构和运动。这种方式允许用户直接控制相机视角。</li>
<li><strong>光照表示:</strong> 利用一个“重光照帧”（relit frame）来提供光照线索，并将其一致地投影到相同的几何结构上。这种方式使得光照信息能够被独立地提取和控制。</li>
<li><strong>优势:</strong> 这种显式、细粒度的解耦是实现高质量光照和联合控制的关键，避免了传统方法中光照和几何信息相互干扰的问题。</li>
</ul>
</li>
<li>
<p><strong>Light-Syn 数据合成管线 (Degradation-based Pipeline with Inverse-Mapping):</strong> 针对缺乏成对的多视角、多光照视频数据这一普遍难题，作者提出了一种创新的数据合成方法。</p>
<ul>
<li><strong>方法:</strong> 利用“in-the-wild”的单目视频作为基础，通过一种基于降质（degradation）和逆映射（inverse-mapping）的管线来合成训练所需的成对数据。</li>
<li><strong>数据集:</strong> 这种策略能够生成覆盖静态、动态以及AI生成场景的数据集，确保了模型的鲁棒性训练。</li>
<li><strong>重要性:</strong> 解决了现实世界中数据获取的瓶颈，使得模型能够学习到更广泛的场景和光照变化。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动生成式视频模型的发展:</strong> Light-X在生成式视频领域迈出了重要一步，它不仅能生成逼真的视频，还能对视频的关键视觉元素（相机视角和光照）进行精细控制，这在以往是难以实现的。</li>
<li><strong>提升视频内容创作的灵活性和效率:</strong> 允许用户在生成视频时同时调整相机运动和光照条件，极大地增强了内容创作者的自由度和效率，为虚拟场景的构建和影视制作提供了新的工具。</li>
<li><strong>促进对光照和几何相互作用的理解:</strong> 通过显式地解耦和控制光照与几何，该研究有助于更深入地理解它们在视觉感知中的相互作用，为后续更复杂的场景理解和生成任务奠定基础。</li>
<li><strong>为视频重光照技术设定新标准:</strong> 在视频重光照（video relighting）领域，Light-X在文本和背景条件下的表现优于现有方法，预示着该领域将朝着更精细、更可控的方向发展。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 在VR/AR环境中，需要实时渲染逼真的场景，并允许用户自由探索和改变视角。Light-X的技术可以用于生成更具沉浸感和交互性的虚拟环境，并根据用户意图调整光照效果。</li>
<li><strong>电影和游戏制作:</strong> 艺术家和设计师可以利用Light-X快速生成具有特定视角和光照效果的视频素材，极大地缩短制作周期，降低成本。例如，可以轻松地为同一场景生成不同时间、不同天气下的光照效果。</li>
<li><strong>3D内容生成:</strong> 结合3D重建技术，Light-X可以用于从单目视频生成具有可控相机和光照的4D（三维空间+时间）内容。</li>
<li><strong>机器人和自动驾驶:</strong> 在模拟环境中训练机器人或自动驾驶系统时，需要生成多样化的场景和光照条件以提高模型的鲁棒性。Light-X可以提供这种多样化的训练数据。</li>
<li><strong>数字人和虚拟形象:</strong> 为数字人或虚拟形象生成逼真的动画，并根据场景或用户需求调整其光照表现，使其更加生动和真实。</li>
</ul>
<p><strong>5. 可推断的局限性</strong></p>
<ul>
<li><strong>对输入视频质量的依赖:</strong> 虽然论文提到了“in-the-wild”的单目视频，但其合成管线和最终生成效果很可能仍然对输入视频的质量（如清晰度、运动平滑度、光照变化范围等）有一定要求。过于模糊或剧烈变化的输入视频可能难以有效处理。</li>
<li><strong>计算复杂度:</strong> 4D视频生成，尤其是涉及动态点云和光照解耦的任务，通常计算量巨大。Light-X的训练和推理过程可能需要强大的计算资源。</li>
<li><strong>“重光照帧”的获取和一致性:</strong> 虽然提出了Light-Syn来合成数据，但在实际应用中，如何获取高质量且与场景几何一致的“重光照帧”可能是一个挑战。如果“重光照帧”本身存在问题，会直接影响最终生成视频的光照质量。</li>
<li><strong>对复杂几何和遮挡的处理:</strong> 摘要中提到“动态点云”，这可能意味着对于非常精细的几何细节、非刚性形变或严重的遮挡情况，其几何表示和重构能力可能存在一定的局限性。</li>
<li><strong>“AI-generated scenes”的泛化能力:</strong> 论文提到Light-Syn能够处理AI生成场景，但其对不同类型、不同质量的AI生成内容的泛化能力仍需进一步验证。</li>
</ul>
<p>总而言之，Light-X是一篇非常有前景的研究，它通过创新的解耦设计和数据合成策略，有效地解决了视频生成中相机和光照联合控制的难题，为未来的多模态、可控视频生成打开了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control.</li>
<li>1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry.</li>
<li>2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage.</li>
<li>Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05115v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05115v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05116v1'></a></p>
<h2 id="value-gradient-guidance-for-flow-matching-alignment"><a href="https://arxiv.org/abs/2512.05116v1">Value Gradient Guidance for Flow Matching Alignment</a></h2>
<p><strong>Authors:</strong> Zhen Liu, Tim Z. Xiao, Carles Domingo-Enrich, Weiyang Liu, Dinghuai Zhang</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Value Gradient Guidance for Flow Matching Alignment”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> Value Gradient Guidance for Flow Matching Alignment (基于值梯度引导的流匹配对齐)</p>
<p><strong>作者：</strong> Zhen Liu, Tim Z. Xiao, Carles Domingo-Enrich, Weiyang Liu, Dinghuai Zhang</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>流匹配模型（Flow Matching Models）作为一类强大的生成模型，在生成高维数据（如图像、视频、3D对象）方面表现出色。然而，将这些模型与人类偏好对齐（即生成更符合用户期望的样本）是一个重要的研究方向。现有方法在实现高效适应和保持模型原有概率分布（prior preservation）方面存在不足。具体来说，流匹配模型基于确定性常微分方程（ODE）的采样路径，这使得它们难以直接应用扩散模型（diffusion models）中常用的基于随机微分方程（SDE）的对齐技术。核心挑战在于如何在不破坏模型原有生成能力的前提下，高效且概率上合理地将流匹配模型与奖励模型（reward model）对齐。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<p>本文提出了一种名为 <strong>VGG-Flow</strong>（Value Gradient Guidance for Flow Matching Alignment）的新型梯度匹配方法，用于微调（finetune）预训练的流匹配模型。其核心思想源于<strong>最优控制理论</strong>，特别是<strong>Hamilton-Jacobi-Bellman (HJB) 方程</strong>。</p>
<ul>
<li><strong>基于最优控制的松弛目标：</strong> VGG-Flow 将对齐问题重新表述为一个最优控制问题，其目标是最小化微调后的速度场与预训练模型速度场之间的差异，并结合奖励函数。</li>
<li><strong>值梯度匹配：</strong> 该方法的核心创新在于，将微调后的速度场与预训练模型速度场之间的<strong>最优差异</strong>建模为<strong>值函数（value function）的梯度场</strong>。这意味着模型学习到的残差速度场（residual velocity field）应该匹配一个潜在值函数的梯度。</li>
<li><strong>高效值函数梯度参数化：</strong> 为了解决直接学习值函数及其梯度计算的复杂性，VGG-Flow 提出了一种<strong>前瞻性（forward-looking）的参数化方法</strong>来学习值函数的梯度。这种方法利用了流匹配模型在特定时间步长的预测，从而简化了学习过程并加速了收敛。</li>
<li><strong>统一的损失函数：</strong> VGG-Flow 整合了<strong>值梯度匹配（gradient matching）</strong>、<strong>值一致性（value consistency）</strong>和<strong>边界条件（boundary condition）</strong>的损失项，形成一个统一的优化目标。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>有效性与效率：</strong> 在流行的文本到图像流匹配模型 <strong>Stable Diffusion 3</strong> 上进行了广泛的实验。结果表明，VGG-Flow 在有限的计算预算下，能够实现高效且鲁棒的对齐。</li>
<li><strong>性能提升：</strong> 与现有基线方法（如 ReFL, DRaFT, Adjoint Matching）相比，VGG-Flow 在<strong>奖励（reward）</strong>、<strong>样本多样性（diversity）</strong>和<strong>先验保持（prior preservation）</strong>方面均取得了更好的结果。尤其是在保持生成样本的语义信息和模型原有分布方面表现突出。</li>
<li><strong>收敛速度：</strong> VGG-Flow 展现出更快的收敛速度，并且在权衡奖励与多样性/先验保持方面，能够达到更好的帕累托前沿（Pareto front）。</li>
<li><strong>泛化能力：</strong> 该方法在不同的奖励模型（Aesthetic Score, HPSv2, PickScore）上都表现出良好的泛化能力。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>近似性：</strong> 为了提高计算效率，VGG-Flow 在计算值一致性损失时使用了<strong>有限差分</strong>来近似二阶梯度，这可能引入一定的偏差。</li>
<li><strong>超参数敏感性：</strong> 尽管方法整体鲁棒，但超参数（如温度系数 β）的选择仍然会影响收敛速度和样本质量的权衡。</li>
<li><strong>探索-利用权衡：</strong> 与许多强化学习方法类似，VGG-Flow 也面临探索-利用（exploration-exploitation）的权衡问题，尤其是在计算资源有限的情况下，可能更容易导致模式崩溃（mode collapse）。</li>
<li><strong>架构设计：</strong> 文章未深入探索更优的网络架构设计，而这对于高效稳定的基础模型微调至关重要。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精确的梯度估计：</strong> 探索更先进的二阶梯度计算方法或更优的近似技术，以进一步减少偏差。</li>
<li><strong>自适应超参数调整：</strong> 开发更智能的超参数调整策略，以自动适应不同的任务和模型。</li>
<li><strong>架构优化：</strong> 研究更适合值梯度匹配的神经网络架构，以提升整体性能和稳定性。</li>
<li><strong>更广泛的应用：</strong> 将 VGG-Flow 应用于其他类型的生成模型（如扩散模型）或更复杂的生成任务（如视频生成、3D内容创作）。</li>
<li><strong>理论分析深化：</strong> 进一步深化理论分析，例如更精细地界定 KL 散度与 L2 损失之间的关系，以及探索其他理论保证。</li>
</ul>
<p><strong>总结：</strong></p>
<p>VGG-Flow 是一种基于最优控制理论和值梯度匹配的新型流匹配模型对齐方法。它通过将对齐目标转化为学习值函数梯度，并采用高效的前瞻性参数化技术，成功地解决了现有方法在效率和先验保持方面的不足。实验结果表明，VGG-Flow 在生成质量、多样性和先验保持方面均优于现有方法，为流匹配模型的个性化和对齐提供了有力的新工具，对构建更可控、更符合人类期望的生成式AI系统具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05116v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05116v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05113v1'></a></p>
<h2 id="splannequin-freezing-monocular-mannequin-challenge-footage-with-dual-detection-splatting"><a href="https://arxiv.org/abs/2512.05113v1">Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</a></h2>
<p><strong>Authors:</strong> Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</p>
<p><strong>作者：</strong> Hao-Jen Chien, Yi-Chuan Huang, Chung-Ho Wu, Wei-Lun Chao, Yu-Lun Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
本论文旨在解决从单目“Mannequin Challenge”（MC）视频中合成高保真度、静态的3D冻结场景的问题。与标准的动态场景重建不同，MC视频通常包含轻微的身体运动，这使得直接将动态场景模型固定在某个时间点进行渲染时，会出现鬼影和模糊等伪影，尤其是在相机运动占主导的情况下。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
作者提出了<strong>Splannequin</strong>，一种<strong>架构无关的正则化方法</strong>，用于解决上述问题。其核心创新在于：</p>
<ul>
<li><strong>动态高斯喷绘（Dynamic Gaussian Splatting）的创新应用：</strong> 将场景建模为动态的，以保留局部时间变化，但通过固定时间参数来渲染静态场景。</li>
<li><strong>双重检测与时间锚定：</strong> 提出了一种检测高斯原语（Gaussian primitives）两种失效状态的方法：<ul>
<li><strong>隐藏（Hidden）高斯：</strong> 当高斯原语中心超出相机视锥时，接收不到监督信号。</li>
<li><strong>缺陷（Defective）高斯：</strong> 当高斯原语中心在相机视锥内，但对渲染图像的贡献可忽略不计，导致梯度更新为零。</li>
<li><strong>时间锚定：</strong> 对于隐藏高斯，将其锚定到最近的、已良好观察到的过去状态；对于缺陷高斯，将其锚定到未来具有更强监督的状态。这种锚定策略利用了相机运动的特点，特别是向前运动的场景。</li>
</ul>
</li>
<li><strong>无架构修改与零推理开销：</strong> Splannequin作为简单的损失项集成到现有的动态高斯喷绘管线中，无需修改现有模型架构，并且在推理时没有额外的计算开销。</li>
<li><strong>置信度加权：</strong> 通过时间距离的指数衰减来加权锚定，确保更近的参考帧提供更强的约束，从而实现鲁棒的时间一致性和伪影消除。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
Splannequin在真实世界的MC视频上取得了显著的性能提升。</p>
<ul>
<li><strong>视觉质量显著提升：</strong> 实验结果表明，Splannequin能够生成更清晰、时间上更一致的冻结帧，有效抑制了鬼影和模糊等伪影。</li>
<li><strong>用户可选择的冻结时刻：</strong> 该方法允许用户灵活地选择任意时间点进行冻结，并生成高保真度的静态视频，为艺术创作和用户体验提供了极大的便利。</li>
<li><strong>用户偏好验证：</strong> 用户研究表明，Splannequin的生成结果在视觉吸引力和伪影抑制方面获得了96%的用户偏好。80%的用户认为其结果比原始捕捉更“完美冻结”。</li>
<li><strong>效率高：</strong> 尽管性能优越，但该方法在RTX 4090上实现了超过280 FPS的推理速度。</li>
<li><strong>新基准和评估：</strong> 作者构建了一个新的、具有挑战性的真实世界MC视频数据集，并提出了相应的评估协议。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>对快速、非刚性变化敏感：</strong> 该方法假设场景基本是静态的，对于快速、非刚性的变化（如快速移动的阴影、光照变化或大幅度运动）可能效果不佳，因为这些变化缺乏可靠的时间锚定。
*   <strong>未来工作方向：</strong> 需要进一步研究运动阈值和帧位置依赖性，以及开发更自适应的锚定策略来处理更具挑战性的场景。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更自适应的锚定策略：</strong> 针对不同类型的运动和场景特性，开发更精细的锚定策略。
*   <strong>处理快速、非刚性变化：</strong> 探索能够处理更动态场景的方法，可能需要结合更先进的运动估计或场景理解技术。
*   <strong>量化分析运动依赖性：</strong> 对运动阈值和帧位置依赖性进行更深入的量化分析，以理解方法的鲁棒性边界。</p>
<p><strong>总结：</strong>
Splannequin通过引入一种创新的双重检测和时间锚定机制，成功解决了从单目MC视频中合成高保真度冻结场景的难题。该方法不仅显著提升了视觉质量，抑制了伪影，还赋予了用户灵活选择冻结时刻的能力，为消费级视频的后期处理和VR/AR应用开辟了新的可能性。其架构无关性和高效性使其易于集成到现有工作流程中，具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter.</li>
<li>We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring.</li>
<li>Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05113v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05113v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05112v1'></a></p>
<h2 id="draco-draft-as-cot-for-text-to-image-preview-and-rare-concept-generation"><a href="https://arxiv.org/abs/2512.05112v1">DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</a></h2>
<p><strong>Authors:</strong> Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本论文提出了一种新颖的“Draft-as-CoT”（DraCo）范式，通过引入低分辨率草图作为中间推理步骤，实现了文本到图像生成中更精细的规划和验证。DraCo能够有效解决现有方法在文本规划粒度不足和生成罕见概念组合方面的挑战，并通过一个名为DraCo-240K的数据集和DraCo-CFG的引导策略进行训练和优化，显著提升了图像生成质量和评估指标。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>DraCo的核心创新在于其<strong>“草图即思维链”（Draft-as-CoT）的交错推理范式</strong>。具体来说，其方法论的关键点包括：</p>
<ul>
<li><strong>低分辨率草图作为视觉规划和引导：</strong> 不同于纯粹的文本规划，DraCo首先生成一个低分辨率的草图图像。这个草图充当了更具体、结构化的视觉规划，为后续的精细化生成提供了坚实的基础和方向。这解决了传统文本规划粒度粗糙的问题。</li>
<li><strong>利用模型内在理解能力进行验证和修正：</strong> DraCo利用多模态大语言模型（MLLM）本身强大的理解能力，来识别草图与输入提示之间的潜在语义不匹配。一旦检测到不一致，模型会进行选择性修正，并通过超分辨率技术进行精细化。</li>
<li><strong>解决罕见概念组合的生成难题：</strong> 通过草图的引入和迭代修正，DraCo能够更好地处理那些属性组合罕见、难以直接通过文本描述准确生成的概念，提高了生成的多样性和准确性。</li>
<li><strong>DraCo-240K数据集：</strong> 为了支持这种新的推理范式，作者构建了一个大规模数据集DraCo-240K，专门用于训练模型在三个关键原子能力上进行提升：通用修正、实例操作和布局重组。</li>
<li><strong>DraCo-CFG引导策略：</strong> 针对交错推理的特点，论文还提出了DraCo-CFG，一种专门的分类器自由引导（CFG）策略，以优化生成过程。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>DraCo的提出对文本到图像生成领域具有重要的潜在影响：</p>
<ul>
<li><strong>提升生成质量和可控性：</strong> 通过引入视觉中间表示（草图），DraCo有望显著提升生成图像的整体质量、结构准确性和语义保真度，尤其是在处理复杂场景和精细细节时。</li>
<li><strong>增强模型的可解释性和可调试性：</strong> 草图作为推理过程中的一个可见步骤，可能为理解模型生成决策提供新的视角，使得调试和改进模型更加直观。</li>
<li><strong>推动更高级的推理能力：</strong> DraCo展示了将思维链（CoT）从纯文本推理扩展到包含视觉元素的交错推理的可能性，为开发更具智能和鲁棒性的多模态模型开辟了新方向。</li>
<li><strong>促进罕见概念的生成：</strong> 对于那些在现有数据集中稀少或难以描述的组合，DraCo的方法可能提供一种更有效的生成途径，从而丰富了AI生成内容的范围。</li>
<li><strong>为下游应用提供更可靠的基础：</strong> 更高质量、更可控的图像生成能力将直接受益于各种下游应用，如内容创作、虚拟现实、设计辅助等。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作与设计：</strong> 艺术家、设计师和内容创作者可以利用DraCo生成更符合其创意意图的图像，尤其是在需要精确控制构图、风格和特定元素组合时。</li>
<li><strong>虚拟现实与游戏开发：</strong> 能够生成更逼真、更具多样性的场景和角色，加速虚拟世界的构建。</li>
<li><strong>教育与科普：</strong> 生成更直观、更准确的插图，帮助解释复杂的概念或展示罕见的现象。</li>
<li><strong>产品设计与原型制作：</strong> 快速生成产品概念图，并进行迭代修改。</li>
<li><strong>辅助视觉障碍者：</strong> 生成更详细、更准确的场景描述图像，帮助理解周围环境。</li>
<li><strong>多模态理解与生成研究：</strong> 为研究如何更好地融合文本和视觉信息进行推理和生成提供新的框架和思路。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<p>尽管摘要描绘了DraCo的强大能力，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算成本增加：</strong> 引入草图生成、验证和超分辨率等中间步骤，可能会显著增加模型的计算复杂度和推理时间，尤其是在需要多次迭代修正的情况下。</li>
<li><strong>对草图质量的依赖：</strong> 草图的质量和准确性直接影响最终生成结果。如果草图生成阶段出现严重错误，后续的修正可能难以完全弥补。</li>
<li><strong>数据集的有效性：</strong> DraCo-240K数据集的质量和多样性将直接影响DraCo模型在实际应用中的泛化能力。如果数据集存在偏差或覆盖不足，模型在某些场景下可能表现不佳。</li>
<li><strong>“罕见概念”的定义和覆盖范围：</strong> 摘要提到DraCo擅长生成罕见概念组合，但“罕见”的定义是相对的，并且数据集可能无法覆盖所有可能的罕见组合。</li>
<li><strong>超分辨率技术的局限性：</strong> 尽管有超分辨率技术，但其在放大图像时仍可能引入伪影或细节丢失，尤其是在大幅度放大时。</li>
<li><strong>模型复杂性：</strong> 这种交错推理范式可能需要更复杂的模型架构和训练过程，对模型的训练资源和技术要求更高。</li>
</ul>
<p>总而言之，DraCo通过引入视觉草图作为推理过程中的关键中间步骤，为文本到图像生成带来了新的思路和显著的性能提升，尤其是在处理复杂规划和罕见概念方面。然而，其潜在的计算成本和对中间表示质量的依赖性是值得关注的方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification.</li>
<li>Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance.</li>
<li>In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05112v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05112v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05111v1'></a></p>
<h2 id="arm-thinker-reinforcing-multimodal-generative-reward-models-with-agentic-tool-use-and-visual-reasoning"><a href="https://arxiv.org/abs/2512.05111v1">ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</a></h2>
<p><strong>Authors:</strong> Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了ARM-Thinker，一个创新的代理式多模态奖励模型，它能够自主调用外部工具来验证其判断，从而克服了现有奖励模型在幻觉、视觉基础薄弱以及缺乏工具验证能力方面的局限性。通过引入工具使用和视觉推理能力，ARM-Thinker显著提升了多模态理解任务中奖励模型的准确性和可解释性。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>代理式工具使用 (Agentic Tool Use):</strong> 这是最核心的创新。ARM-Thinker不再是静态的奖励评分器，而是能够根据任务需求，动态地决定何时、如何调用外部工具（如图像裁剪、文档检索等）来收集证据。这种“思考”和“行动”的代理式范式是其区别于现有方法的关键。</li>
<li><strong>多模态视觉推理 (Multimodal Visual Reasoning):</strong> 模型被设计用来处理复杂的视觉和文本信息，并进行推理。工具的使用直接服务于这种推理过程，例如通过裁剪来聚焦细节，或通过检索来交叉验证信息。</li>
<li><strong>多阶段强化学习 (Multi-stage Reinforcement Learning):</strong> 为了训练这种复杂的代理行为，论文采用了多阶段强化学习。这表明模型不仅学习如何做出最终判断，还学习如何有效地调用工具，并优化这两者之间的协同。</li>
<li><strong>ARMBench-VL 基准测试:</strong> 为了评估这种新的代理式奖励建模能力，论文引入了一个专门的基准测试集，涵盖了细粒度视觉基础、多页文档理解和指令遵循等关键能力。这为后续研究提供了重要的评估平台。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升多模态系统的可靠性:</strong> 现有的视觉-语言模型（如大型语言模型在处理图像时）常常出现幻觉，即生成不准确或虚假的信息。ARM-Thinker通过引入可验证的证据，极大地提高了多模态系统输出的可靠性，使其在关键应用中更值得信赖。</li>
<li><strong>推动更复杂的推理能力:</strong> 能够使用工具进行验证，意味着模型可以处理更复杂、需要多步推理的任务，而不仅仅是简单的信息匹配。这为实现更高级的AI助手和智能体铺平了道路。</li>
<li><strong>增强模型的可解释性:</strong> 当模型能够展示其做出判断所依赖的证据（例如，通过展示裁剪的图像区域或检索到的文档片段），其决策过程将变得更加透明和可解释，这对于调试和信任至关重要。</li>
<li><strong>重新定义奖励模型的设计范式:</strong> 该研究表明，奖励模型不应局限于静态的评分，而可以发展成动态的、具备自主行动能力的智能体，这可能引发对未来奖励模型设计的全新思考。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>视觉问答 (Visual Question Answering - VQA):</strong> 特别是需要细粒度细节理解或跨越多个图像区域才能回答的问题。</li>
<li><strong>多模态对话系统 (Multimodal Dialogue Systems):</strong> 能够引用证据来支持其回答，使对话更具说服力。</li>
<li><strong>文档理解与问答 (Document Understanding and QA):</strong> 对于包含大量文本和图像的复杂文档，如研究论文、技术手册、法律文件等，模型可以更准确地提取信息。</li>
<li><strong>内容审核与事实核查 (Content Moderation and Fact-Checking):</strong> 通过验证信息来源和视觉证据，提高内容审核的准确性。</li>
<li><strong>辅助诊断与医疗影像分析 (Assisted Diagnosis and Medical Imaging Analysis):</strong> 在医疗领域，准确性和可解释性至关重要，ARM-Thinker的验证能力将非常有价值。</li>
<li><strong>机器人与具身智能 (Robotics and Embodied AI):</strong> 机器人需要理解和与物理世界交互，工具使用能力是其执行复杂任务的关键。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>计算成本和效率:</strong> 引入工具使用和强化学习训练通常意味着更高的计算成本和更长的训练时间。模型在实际部署时的效率和延迟可能是一个挑战。</li>
<li><strong>工具的质量和可用性:</strong> ARM-Thinker的性能高度依赖于其能够调用的外部工具的质量和覆盖范围。如果工具本身存在缺陷或无法处理某些类型的查询，模型的性能将受到限制。</li>
<li><strong>泛化能力:</strong> 虽然引入了新的基准测试，但模型在未见过的新工具或新类型的任务上的泛化能力仍需进一步验证。</li>
<li><strong>训练数据的需求:</strong> 训练一个能够有效进行工具选择和使用的代理模型，可能需要大量精心标注的训练数据，包括工具调用序列和相应的奖励信号。</li>
<li><strong>“代理”的复杂性:</strong> 尽管论文强调了“代理”能力，但其“自主性”的程度可能仍有上限，例如，它可能仍然需要预定义的工具集，或者其决策过程可能受到一定规则的约束。</li>
</ul>
<p><strong>总结：</strong></p>
<p>ARM-Thinker在计算机视觉领域具有重要的研究价值，因为它直接解决了当前多模态模型在可靠性、视觉基础和推理能力上的关键痛点。通过将“代理式工具使用”和“视觉推理”相结合，该研究为构建更强大、更可信赖的多模态AI系统提供了一条新颖且富有前景的路径。其方法论的创新性以及对新基准测试的贡献，预示着它将对该领域的研究方向产生积极影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring.</li>
<li>To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification).</li>
<li>ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05111v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05111v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05107v1'></a></p>
<h2 id="stare-vla-progressive-stage-aware-reinforcement-for-fine-tuning-vision-language-action-models"><a href="https://arxiv.org/abs/2512.05107v1">STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</a></h2>
<p><strong>Authors:</strong> Feng Xu, Guangyao Zhai, Xin Kong, Tingzhong Fu, Daniel F. N. Gordon, Xueli An, Benjamin Busam</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -&gt; Preference -&gt; Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models”的全面中文摘要：</p>
<p><strong>论文摘要：STARE-VLA：用于微调视觉-语言-动作模型的渐进式阶段感知强化学习</strong></p>
<p><strong>1. 研究问题与背景</strong></p>
<p>视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展，它们结合了大型语言模型（LLM）和强化学习（RL）进行微调。然而，现有方法通常将长时序动作视为单一的语言序列，并采用整体轨迹级别的优化方法（如TPO、PPO）。这种方法存在几个关键问题：</p>
<ul>
<li><strong>粗粒度的信用分配：</strong> 在长时序任务中，很难准确地将成功或失败归因于特定的动作阶段，导致训练不稳定。</li>
<li><strong>学习难度不均：</strong> 与语言不同，动作轨迹由一系列因果相连且难度各异的阶段组成（例如，抓取比移动更难）。现有方法未能有效处理这种阶段性的学习难度差异。</li>
<li><strong>信息损失：</strong> 将整个轨迹视为一个整体，忽略了其中有意义的子阶段信息，限制了学习的精细度和效率。</li>
</ul>
<p><strong>2. 核心创新与方法贡献</strong></p>
<p>为了解决上述问题，本文提出了<strong>阶段感知强化学习（STARE）</strong>模块，其核心创新在于：</p>
<ul>
<li><strong>动作轨迹的阶段分解：</strong> STARE能够将长时序动作轨迹分解为语义上可理解的、因果相连的阶段。它通过检测任务相关的事件（如末端执行器与物体的接触、抓取、提起等）来自动识别阶段边界。</li>
<li><strong>阶段级别的奖励信号：</strong> STARE为每个阶段计算成本（cost）和潜在函数（potential），从而生成密集、可解释且与阶段对齐的奖励信号。这使得模型能够获得更精细的反馈，而不仅仅是最终的稀疏奖励。</li>
<li><strong>阶段感知微调方法：</strong><ul>
<li><strong>STA-TPO（Stage-Aware Trajectory-Wise Preference Optimization）：</strong> 将STARE集成到TPO中，实现了阶段级别的偏好对齐。它通过在阶段层面比较轨迹，并引入阶段成本作为惩罚项，使得模型能够区分不同阶段的质量，从而进行更精细的信用分配。</li>
<li><strong>STA-PPO（Stage-Aware Proximal Policy Optimization）：</strong> 将STARE集成到PPO中，通过奖励塑形（reward shaping）将稀疏的终端奖励转化为密集的阶段奖励。这显著提高了在线RL训练的稳定性和效率，尤其是在复杂操作任务中。</li>
</ul>
</li>
<li><strong>IPI（Imitation → Preference → Interaction）三步微调流水线：</strong> 提出了一种串联的微调策略，首先通过监督式微调（SFT）初始化策略，然后利用STA-TPO进行离线阶段偏好对齐，最后通过STA-PPO进行在线阶段交互式优化。这种流水线整合了模仿学习、偏好学习和强化学习的优势，实现了更鲁棒和高效的VLA模型微调。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>显著的性能提升：</strong> 在SimplerEnv和ManiSkill3两大机器人操作基准上，IPI方法取得了显著的性能提升，在SimplerEnv上达到了98.0%的成功率，在ManiSkill3上达到了96.4%的成功率，均达到或超越了当时的最先进水平。</li>
<li><strong>阶段感知的重要性：</strong> 实验结果表明，STARE模块在处理需要高精度和多阶段协调的任务（如堆叠、提起并扶正物体）时尤为关键，能够显著加速收敛并提高最终性能。</li>
<li><strong>通用性：</strong> STARE模块可以灵活地集成到现有的TPO和PPO框架中，并且IPI流水线能够与不同的VLA模型骨干（如OpenVLA-7B和pi0.5_base）协同工作。</li>
</ul>
<p><strong>4. 局限性</strong></p>
<p>论文中未明确提及具体的局限性，但可以推断：</p>
<ul>
<li><strong>阶段定义依赖性：</strong> STARE的有效性在一定程度上依赖于能够准确检测任务相关的事件来定义阶段。对于一些定义模糊或阶段过渡不明显的任务，其效果可能会受到影响。</li>
<li><strong>计算开销：</strong> 引入阶段分解和计算阶段成本/奖励可能会增加一定的计算开销，尤其是在线阶段识别时。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<ul>
<li><strong>更通用的阶段定义：</strong> 探索更通用的、任务无关的阶段定义方法，使其能够适应更广泛的机器人操作任务。</li>
<li><strong>自适应阶段分解：</strong> 研究如何让模型根据任务的复杂性和难度自适应地调整阶段的粒度或数量。</li>
<li><strong>跨任务阶段迁移：</strong> 探索将从一个任务中学到的阶段知识迁移到其他相关任务的能力。</li>
<li><strong>与LLM的更深层融合：</strong> 进一步探索如何利用LLM的语言理解能力来指导更精细的阶段分解和奖励设计。</li>
</ul>
<p><strong>总结：</strong></p>
<p>STARE-VLA论文提出了一种新颖的<strong>阶段感知强化学习（STARE）</strong>方法，通过将长时序动作分解为语义有意义的阶段，并为每个阶段提供精细的奖励信号，有效解决了现有VLA模型在信用分配、学习难度处理和效率方面的问题。结合STA-TPO和STA-PPO，以及IPI流水线，该方法在多个机器人操作基准上取得了显著的性能提升，证明了阶段感知在提升VLA模型能力方面的关键作用。这项工作为构建更强大、更鲁棒的机器人操作策略提供了新的视角和有力的工具。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals.</li>
<li>Further building on supervised fine-tuning as initialization, we propose the Imitation -&gt; Preference -&gt; Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models.</li>
<li>Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05107v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05107v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05106v1'></a></p>
<h2 id="neuralremaster-phase-preserving-diffusion-for-structure-aligned-generation"><a href="https://arxiv.org/abs/2512.05106v1">NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</a></h2>
<p><strong>Authors:</strong> Yu Zeng, Charles Ochoa, Mingyuan Zhou, Vishal M. Patel, Vitor Guizilini, Rowan McAllister</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.CV, cs.GR, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. φ-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, φ-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, φ-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</p>
<p><strong>作者：</strong> Yu Zeng, Charles Ochoa, Mingyuan Zhou, Vishal M. Patel, Vitor Guizilini, Rowan McAllister</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>现有扩散模型在生成高质量图像方面取得了巨大成功，但其标准的扩散过程（通过注入具有随机幅度和随机相位的噪声来逐步破坏数据）在需要保持几何一致性的任务中表现不佳。这些任务包括图像重渲染、模拟增强和图像到图像的翻译。这是因为扩散过程中的相位信息在信号处理中被认为是决定空间结构的关键，而随机化相位会破坏这种结构，迫使模型从头开始重建。这使得现有方法在结构对齐生成方面效率低下，通常需要复杂的模型架构修改或额外的参数。</p>
<p><strong>2. 主要创新/方法贡献：</strong></p>
<p>该论文提出了 <strong>Phase-Preserving Diffusion (φ-PD)</strong>，一种对扩散过程的<strong>模型无关</strong>的重新设计。其核心思想是：</p>
<ul>
<li><strong>相位保持的结构化噪声：</strong> φ-PD 在扩散的<strong>前向过程中，保留输入图像的相位信息，同时随机化其幅度</strong>。这使得在整个采样过程中能够自然地保持空间对齐，而无需对扩散模型本身进行任何架构修改或增加额外参数。</li>
<li><strong>频率选择性结构化噪声 (FSS)：</strong> 为了提供对结构刚性的<strong>连续控制</strong>，论文引入了 FSS 噪声。它通过一个<strong>单一的频率截止参数</strong>来插值输入相位和纯高斯噪声，从而允许在严格的结构对齐和创意灵活性之间进行权衡。</li>
<li><strong>模型无关性与高效性：</strong> φ-PD 不会增加推理时间成本，并且与任何用于图像或视频的 DDPM 或流匹配模型兼容。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>多任务表现优异：</strong> 在<strong>照片级真实感重渲染、风格化重渲染以及自动驾驶模拟器（如 CARLA）的模拟增强</strong>等任务中，φ-PD 均取得了可控、空间对齐的结果。</li>
<li><strong>显著的模拟到真实 (Sim-to-Real) 提升：</strong> 当应用于 CARLA 模拟器时，φ-PD 将 CARLA 到 Waymo 规划器的性能<strong>提高了 50%</strong>，显著缩小了模拟到真实之间的差距。</li>
<li><strong>结构对齐能力：</strong> 论文通过实验证明，φ-PD 在保持图像结构一致性方面优于现有方法，即使在外观发生变化的情况下也能保持良好的空间对齐。</li>
<li><strong>通用性与互补性：</strong> 该方法与现有的条件化方法（如 ControlNet）是<strong>互补</strong>的，并且广泛适用于图像到图像和视频到视频的生成任务。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对输入模态的假设：</strong> φ-PD 假设输入是图像。对于深度图或法线图等其他模态，可能需要一个<strong>轻量级的先验</strong>来生成初始图像表示。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>与其他方法的集成：</strong> φ-PD 与现有的条件化或适配器方法是<strong>正交</strong>的，可以与它们集成以实现更强的控制。</li>
<li><strong>扩展到其他图像恢复任务：</strong> 未来工作可以探索将 φ-PD 扩展到<strong>去模糊、光照调整、超分辨率和一般的图像恢复</strong>等任务。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文提出了一种名为 φ-PD 的创新性扩散模型重构方法，通过在扩散过程中保留图像的相位信息来解决结构对齐生成中的核心挑战。其模型无关、高效且易于集成的特性，以及引入的 FSS 噪声提供的精细控制能力，使其在图像重渲染、风格化和模拟增强等多个领域展现出强大的性能，尤其是在缩小模拟到真实差距方面取得了显著成果。这为开发更具结构感知能力的生成模型开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05106v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05106v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.05103v1'></a></p>
<h2 id="tv2tv-a-unified-framework-for-interleaved-language-and-video-generation"><a href="https://arxiv.org/abs/2512.05103v1">TV2TV: A Unified Framework for Interleaved Language and Video Generation</a></h2>
<p><strong>Authors:</strong> Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, Shang-Wen Li, Sreya Dutta Roy, Jakob Verbeek, XuDong Wang, Marjan Ghazvininejad, Luke Zettlemoyer, Emily Dinan</p>
<p><strong>Published:</strong> 2025-12-04</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“TV2TV: A Unified Framework for Interleaved Language and Video Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> TV2TV: A Unified Framework for Interleaved Language and Video Generation</p>
<p><strong>作者：</strong> Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, Shang-Wen Li, Sreya Dutta Roy, Jakob Verbeek, XuDong Wang, Marjan Ghazvininejad, Luke Zettlemoyer, Emily Dinan</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
尽管视频生成模型在视觉质量上取得了显著进展，但它们在生成需要复杂语义分支或反复进行高级推理的视频时仍面临挑战，难以准确预测下一步应该发生什么。现有的模型难以在生成高质量视频的同时，保持对内容的高级理解和控制。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
本文提出了一种名为 <strong>TV2TV</strong> 的新型<strong>全能型视频-文本模型</strong>，它将视频生成分解为<strong>交错的文本和视频生成过程</strong>。TV2TV 的核心创新在于：</p>
<ul>
<li><strong>交错生成框架：</strong> TV2TV 采用一种“先思考，后行动”的推理模式，在生成视频帧之前，会先生成文本来“思考”后续内容。这种交错过程允许模型在生成像素之前，利用语言模型进行语义推理和规划。</li>
<li><strong>联合学习：</strong> 模型联合学习语言建模（预测下一个文本 token）和视频流匹配（预测下一个视频帧）任务，利用<strong>Transformer 的混合架构（Mixture-of-Transformers, MoT）</strong>，为文本和视频模态提供独立的处理塔，同时保持全局的自注意力机制。</li>
<li><strong>推理时的动态切换：</strong> 在推理时，TV2TV 可以动态地在生成文本和视频帧之间切换，从而实现细粒度的控制。</li>
<li><strong>文本干预与可控性：</strong> 用户可以在生成过程的任何时间点通过文本干预来修改视频生成轨迹，极大地增强了视频生成的可控性。</li>
<li><strong>数据增强流水线：</strong> 针对真实世界视频数据缺乏对齐文本的问题，论文提出了一种数据增强流水线，利用视觉语言模型（VLMs）为体育视频合成交错的自然语言动作描述，从而扩展了 TV2TV 的应用范围。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
TV2TV 在两个主要领域进行了评估，并取得了显著成果：</p>
<ul>
<li><strong>视频游戏数据：</strong> 在视频游戏（CS:GO）数据上，TV2TV 在<strong>视觉质量</strong>和<strong>可控性</strong>方面均大幅优于现有方法。在人类评估中，TV2TV 生成的视频在视觉质量上获得了 91% 的偏好，在细粒度指令遵循准确率上比“先思考后行动”的方法提高了 19 个百分点。</li>
<li><strong>真实世界体育视频：</strong> 通过数据增强流水线处理的体育视频数据上，TV2TV 同样展现出强大的<strong>视觉质量</strong>和<strong>提示对齐能力</strong>，能够生成复杂、真实的动作序列。在与 T2V 和 Think2V 等基线模型的比较中，TV2TV 在视觉质量和整体偏好上均表现出色，尤其在长视频生成方面优势更为明显。</li>
</ul>
<p>这些结果表明，TV2TV 是一个非常有前景的框架，它能够将语言模型的推理能力与高度可控的视频生成系统相结合，为开放式文本推理和控制的视频生成开辟了新的道路。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>体育视频数据的文本质量：</strong> 论文提到，与游戏数据相比，体育视频的合成字幕（由 VLM 生成）可能包含幻觉，并且文本密度较低（平均每 1.9 秒一个文本），这可能影响了 TV2TV 在体育视频上的表现。
*   <strong>模型规模与计算资源：</strong> 虽然论文展示了 TV2TV 的可扩展性，但训练和推理大型模型仍然需要大量的计算资源。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>提高交错文本的粒度和准确性：</strong> 未来工作可以专注于改进训练数据中交错文本的粒度和准确性，尤其是在更多视频领域。
*   <strong>更广泛的视频领域应用：</strong> 将 TV2TV 的范式扩展到更多样化的视频领域，探索其在不同类型内容上的表现。
*   <strong>更精细的控制与交互：</strong> 进一步探索用户如何通过更复杂的文本指令来精细地控制视频生成过程。</p>
<p>总而言之，TV2TV 论文提出了一种创新的交错式文本-视频生成框架，通过让模型在生成视频前进行“思考”，显著提升了视频的视觉质量和可控性，并为实现更智能、更灵活的视频生成技术奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge.</li>
<li>More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process.</li>
<li>TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.05103v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.05103v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-05 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
