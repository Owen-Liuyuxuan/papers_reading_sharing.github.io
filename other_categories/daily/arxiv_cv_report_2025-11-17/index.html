<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-17 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-14/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-18/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-17">Arxiv Computer Vision Papers - 2025-11-17</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#toward-generalized-detection-of-synthetic-media-limitations-challenges-and-the-path-to-multimodal-solutions" class="nav-link">Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions</a>
                </li>
                <li class="nav-item">
                    <a href="#larm-a-large-articulated-object-reconstruction-model" class="nav-link">LARM: A Large Articulated-Object Reconstruction Model</a>
                </li>
                <li class="nav-item">
                    <a href="#bridging-hidden-states-in-vision-language-models" class="nav-link">Bridging Hidden States in Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#vp-bench-a-comprehensive-benchmark-for-visual-prompting-in-multimodal-large-language-models" class="nav-link">VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#weave-unleashing-and-benchmarking-the-in-context-interleaved-comprehension-and-generation" class="nav-link">WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#comprehension-of-multilingual-expressions-referring-to-target-objects-in-visual-inputs" class="nav-link">Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs</a>
                </li>
                <li class="nav-item">
                    <a href="#q-doc-benchmarking-document-image-quality-assessment-capabilities-in-multi-modal-large-language-models" class="nav-link">Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#ycb-ev-sd-synthetic-event-vision-dataset-for-6dof-object-pose-estimation" class="nav-link">YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#docslm-a-small-vision-language-model-for-long-multimodal-document-understanding" class="nav-link">DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#moon-embedding-multimodal-representation-learning-for-e-commerce-search-advertising" class="nav-link">MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-17">Arxiv Computer Vision Papers - 2025-11-17</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年11月14日Arxiv计算机视觉论文的执行摘要，旨在帮助您快速了解该领域的最新进展。</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025年11月14日)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文主要围绕以下几个核心主题展开：</p>
<ul>
<li><strong>多模态大语言模型 (MLLMs) 的能力扩展与评估：</strong> 大量研究致力于探索和基准测试 MLLMs 在视觉提示、上下文理解、文档理解以及多语言表达理解方面的能力。这表明 MLLMs 仍然是当前研究的热点，并且研究重点正从基础模型构建转向其在复杂任务中的应用和性能评估。</li>
<li><strong>合成媒体检测与真实性挑战：</strong> 随着生成式AI的发展，对合成媒体的检测变得日益重要，研究开始关注其局限性并寻求多模态解决方案。</li>
<li><strong>特定领域的数据集与基准：</strong> 针对文档图像质量评估、事件视觉数据以及电商搜索广告等特定应用场景，新的数据集和基准被提出，以推动这些领域的研究。</li>
<li><strong>3D 重建与姿态估计：</strong> 针对大型可动对象的3D重建和6DoF物体姿态估计仍然是活跃的研究方向，尤其是在合成数据生成方面。</li>
</ul>
<p><strong>2. 显著或创新性论文：</strong></p>
<ul>
<li><strong>"Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions" (Hussain et al.)</strong>: 这篇论文具有重要的社会意义和研究价值。它不仅指出了当前合成媒体检测方法的局限性，更提出了多模态解决方案的未来方向，为应对日益复杂的深度伪造挑战提供了前瞻性思考。</li>
<li><strong>"LARM: A Large Articulated-Object Reconstruction Model" (Yuan et al.)</strong>: 针对大型可动对象的3D重建是一个具有挑战性的问题。LARM 的提出可能在机器人、虚拟现实和工业设计等领域带来显著影响，其创新性在于处理复杂结构和运动的能力。</li>
<li><strong>"Bridging Hidden States in Vision-Language Models" (Fein-Ashley, Fein-Ashley)</strong>: 这篇论文可能在理论层面具有重要意义。理解和连接视觉-语言模型中的隐藏状态有助于提升模型的可解释性和性能，为更深层次的多模态融合提供了新的视角。</li>
<li><strong>"WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation" (Chow et al.)</strong>: WEAVE 提出的上下文交错理解和生成能力，代表了 MLLMs 迈向更高级认知能力的关键一步，其基准测试将推动该领域的发展。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>多模态大语言模型 (MLLMs) 的“上下文交错理解与生成”：</strong> WEAVE 论文强调了 MLLMs 在复杂、多轮对话中理解和生成交错信息的能力，这预示着 MLLMs 将从简单的问答转向更复杂的交互式任务。</li>
<li><strong>事件视觉数据在6DoF姿态估计中的应用：</strong> "YCB-Ev SD" 论文表明事件相机数据因其高时间分辨率和低延迟特性，正被探索用于解决传统帧式相机在快速运动场景下的局限性，尤其是在机器人和自动驾驶领域。</li>
<li><strong>小型化多模态模型 (Small Vision-Language Models) 用于特定文档理解：</strong> DocSLM 的出现表明，除了追求超大型模型外，针对特定任务（如长文档理解）优化的小型化模型也具有重要的实用价值和研究潜力。</li>
<li><strong>多模态表示学习在电商领域的深化应用：</strong> MOON Embedding 强调了将视觉、文本和用户行为等多模态信息融合，以优化电商搜索广告，这代表了多模态技术在商业应用中的精细化和深入化。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对于关注社会影响和安全的研究人员：</strong><ul>
<li>"Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions" (Hussain et al.)</li>
</ul>
</li>
<li><strong>对于关注 MLLMs 核心能力和基准测试的研究人员：</strong><ul>
<li>"VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models" (Xu et al.)</li>
<li>"WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation" (Chow et al.)</li>
<li>"Bridging Hidden States in Vision-Language Models" (Fein-Ashley, Fein-Ashley)</li>
</ul>
</li>
<li><strong>对于关注 3D 视觉和机器人领域的研究人员：</strong><ul>
<li>"LARM: A Large Articulated-Object Reconstruction Model" (Yuan et al.)</li>
<li>"YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation" (Rojtberg, Kühn)</li>
</ul>
</li>
<li><strong>对于关注文档理解和信息抽取的研究人员：</strong><ul>
<li>"Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models" (Huang et al.)</li>
<li>"DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding" (Hannan et al.)</li>
</ul>
</li>
<li><strong>对于关注商业应用和推荐系统的研究人员：</strong><ul>
<li>"MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising" (Fu et al.)</li>
</ul>
</li>
</ul>
<hr />
<p>希望这份摘要能帮助您高效地了解今天的 Arxiv 计算机视觉领域动态！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.11116v1">Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions</a></li>
<li><a href="#2511.11563v1">LARM: A Large Articulated-Object Reconstruction Model</a></li>
<li><a href="#2511.11526v1">Bridging Hidden States in Vision-Language Models</a></li>
<li><a href="#2511.11438v1">VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models</a></li>
<li><a href="#2511.11434v1">WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation</a></li>
<li><a href="#2511.11427v1">Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs</a></li>
<li><a href="#2511.11410v1">Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models</a></li>
<li><a href="#2511.11344v1">YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation</a></li>
<li><a href="#2511.11313v1">DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding</a></li>
<li><a href="#2511.11305v1">MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.11116v1'></a></p>
<h2 id="toward-generalized-detection-of-synthetic-media-limitations-challenges-and-the-path-to-multimodal-solutions"><a href="https://arxiv.org/abs/2511.11116v1">Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions</a></h2>
<p><strong>Authors:</strong> Redwan Hussain, Mizanur Rahman, Prithwiraj Bhattacharjee</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV, cs.NE</p>
<p><strong>Abstract:</strong></p>
<p>Artificial intelligence (AI) in media has advanced rapidly over the last decade. The introduction of Generative Adversarial Networks (GANs) improved the quality of photorealistic image generation. Diffusion models later brought a new era of generative media. These advances made it difficult to separate real and synthetic content. The rise of deepfakes demonstrated how these tools could be misused to spread misinformation, political conspiracies, privacy violations, and fraud. For this reason, many detection models have been developed. They often use deep learning methods such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). These models search for visual, spatial, or temporal anomalies. However, such approaches often fail to generalize across unseen data and struggle with content from different models. In addition, existing approaches are ineffective in multimodal data and highly modified content. This study reviews twenty-four recent works on AI-generated media detection. Each study was examined individually to identify its contributions and weaknesses, respectively. The review then summarizes the common limitations and key challenges faced by current approaches. Based on this analysis, a research direction is suggested with a focus on multimodal deep learning models. Such models have the potential to provide more robust and generalized detection. It offers future researchers a clear starting point for building stronger defenses against harmful synthetic media.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Redwan Hussain, Mizanur Rahman, Prithwiraj Bhattacharjee撰写的论文“Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决人工智能（AI）生成媒体（如深度伪造、合成图像和视频）日益增长的威胁。随着生成对抗网络（GANs）和扩散模型等技术的快速发展，AI生成的内容变得极其逼真，使得区分真实和合成媒体变得越来越困难。这导致了虚假信息传播、政治阴谋、隐私侵犯和欺诈等严重问题。因此，论文的核心研究问题是：<strong>如何开发一种鲁棒且通用的检测模型，能够有效识别各种AI生成媒体，克服现有检测方法的局限性，并应对不断演进的生成技术带来的挑战？</strong></p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要贡献在于其<strong>全面的文献综述和对现有AI生成媒体检测方法的批判性分析</strong>。它系统地审查了24篇近期关于AI生成媒体检测的论文，逐一分析了它们的贡献、方法、结果和局限性。通过这种分析，论文识别并总结了当前检测方法面临的<strong>共同局限性和关键挑战</strong>，包括：
*   <strong>数据集多样性不足</strong>：现有模型往往在特定生成器生成的数据上表现良好，但难以泛化到未见过的数据或不同生成器生成的内容。
*   <strong>泛化能力差</strong>：基于深度学习（如CNN和ViT）的模型通常难以泛化到未见数据和不同模型生成的内容。
*   <strong>多模态数据处理不足</strong>：现有方法在处理多模态数据（如音视频融合）和高度修改的内容时效果不佳。
*   <strong>计算成本高昂</strong>：大型数据集和复杂模型带来了显著的计算负担。
*   <strong>时间异常捕获不足</strong>：CNN模型在捕获静态空间模式方面表现出色，但在捕获视频中的微妙时间动态方面存在不足。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文的主要“结果”并非是提出了一种新的检测模型，而是<strong>通过对现有研究的深入分析，揭示了当前AI生成媒体检测领域的现状、瓶颈和未来方向</strong>。其主要意义在于：
*   <strong>明确了领域挑战</strong>：论文清晰地阐述了当前AI生成媒体检测面临的复杂挑战，为研究人员提供了明确的问题导向。
*   <strong>强调了多模态方法的潜力</strong>：通过分析，论文指出传统单模态检测方法已接近极限，而<strong>多模态深度学习模型</strong>（结合视觉、听觉等多种信息）在提供更鲁棒和通用检测方面具有巨大潜力。
*   <strong>提出了未来研究方向</strong>：论文基于分析，提出了构建<strong>通用多模态深度学习模型</strong>作为未来研究的核心方向，强调了需要解决数据集多样性、泛化能力、时间动态捕获和计算效率等问题。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文本身作为一篇综述，其局限性主要体现在：
*   <strong>未提出新的检测模型</strong>：论文的重点是分析现有工作并指出未来方向，而非提出具体的创新算法或模型。
*   <strong>综述范围的限制</strong>：尽管审查了24篇论文，但AI生成媒体检测领域发展迅速，可能存在未被涵盖的最新进展。
*   <strong>对现有模型局限性的总结</strong>：论文详细列举了现有检测模型的各种局限性，例如：
    *   对预训练模型和特定特征选择的依赖限制了泛化能力。
    *   难以处理低分辨率视频内容。
    *   在后训练阶段性能可能下降（“post-training bottleneck”）。
    *   对人脸检测质量的严重依赖。
    *   未设计用于捕获时间伪影。
    *   在类不平衡和预训练模型选择特征方面存在局限性。
    *   在未见数据上的泛化能力有限（尤其是在少样本学习中）。
    *   对准确光流的依赖，难以处理微妙的I2V视频，对预处理敏感。
    *   高计算成本，依赖音频质量。
    *   对LLM的依赖可能导致在未见数据上泛化困难。
    *   对音视频同步的依赖，可能在异步视频上表现不佳。
    *   在修改后的合成视频上可能被欺骗。
    *   在异步或损坏模态下可能表现不佳。
    *   视觉检测较弱，需要基于情感的特征。
    *   缺乏高质量、大规模数据集。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述分析，论文为未来的研究指明了清晰的方向，主要包括：
*   <strong>构建通用多模态深度学习模型</strong>：这是核心方向，旨在开发能够整合图像、音频、视频等多种模态信息的模型，以实现更全面、更鲁棒的检测。
*   <strong>开发多样化的基准数据集</strong>：需要构建包含各种生成技术和模型生成内容的综合性数据集，以促进模型在不同场景下的泛化能力。
*   <strong>改进ViT和CNN模型以增强泛化能力</strong>：探索如何结合预训练ViT与少样本学习，同时避免后训练瓶颈，以及如何改进CNN以更好地捕获时间动态。
*   <strong>关注音视频融合Transformer</strong>：设计能够有效融合音视频信息，并捕获跨模态一致性和异常的Transformer架构。
*   <strong>解决计算效率问题</strong>：在追求模型复杂性和数据多样性的同时，需要研究如何优化计算资源，使检测系统更具实用性。</p>
<hr />
<p>总而言之，这篇论文通过对现有研究的全面回顾和批判性分析，不仅清晰地描绘了AI生成媒体检测领域的当前挑战和局限性，更重要的是，它为未来的研究提供了一个结构化的起点，明确指出了开发<strong>通用、鲁棒的多模态深度学习模型</strong>是应对日益复杂的合成媒体威胁的关键路径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Diffusion models later brought a new era of generative media.</li>
<li>Each study was examined individually to identify its contributions and weaknesses, respectively.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11116v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11116v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11563v1'></a></p>
<h2 id="larm-a-large-articulated-object-reconstruction-model"><a href="https://arxiv.org/abs/2511.11563v1">LARM: A Large Articulated-Object Reconstruction Model</a></h2>
<p><strong>Authors:</strong> Sylvia Yuan, Ruoxi Shi, Xinyue Wei, Xiaoshuai Zhang, Hao Su, Minghua Liu</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: https://sylviayuan-sy.github.io/larm-site/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="larm-a-large-articulated-object-reconstruction-model_1">论文摘要分析：LARM: A Large Articulated-Object Reconstruction Model</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>LARM 提出了一个统一的、前馈式框架，能够从稀疏视角图像中重建 3D 铰接物体，同时恢复其详细几何形状、真实纹理和准确的关节结构。它通过将静态物体的新视角合成方法 (LVSM) 扩展到铰接物体领域，利用基于 Transformer 的架构联合推理相机姿态和关节变化，从而实现了可扩展且准确的新视角合成和高质量的 3D 网格重建。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>统一的前馈框架：</strong> LARM 最大的创新在于其统一的前馈架构，它摆脱了传统优化方法对密集多视角输入和昂贵逐实例优化的依赖，也避免了现有前馈方法中常见的粗糙几何、缺乏纹理重建以及脆弱的多阶段管道问题。</li>
<li><strong>LVSM 到铰接物体的扩展：</strong> 核心方法是将最近用于静态 3D 物体的新视角合成 (NVS) 方法 LVSM 扩展到铰接物体设置。这通过一个基于 Transformer 的架构实现，该架构能够联合推理相机姿态和铰接变化，从而在处理复杂动态物体时保持鲁棒性。</li>
<li><strong>联合恢复几何、纹理和关节结构：</strong> LARM 不仅关注几何重建，还同时恢复逼真的纹理和准确的关节结构，这对于铰接物体的完整建模至关重要。</li>
<li><strong>辅助输出生成：</strong> 生成深度图和部件掩码等辅助输出，这些输出有助于显式的 3D 网格提取和关节估计，从而提高了重建的精度和可用性。</li>
<li><strong>稀疏视角输入和高保真重建：</strong> 能够从稀疏视角图像进行重建，同时实现高保真度，这显著降低了数据采集的门槛。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动铰接物体重建的范式转变：</strong> 从传统的优化驱动、多阶段、高成本方法转向更高效、可扩展的前馈统一框架，将极大地加速该领域的研究和应用。</li>
<li><strong>降低 3D 内容创作门槛：</strong> 能够从稀疏图像快速生成高质量的 3D 铰接模型，将极大地降低 3D 资产创建的成本和复杂性，使更多用户和应用能够受益。</li>
<li><strong>提升新视角合成和新状态合成的质量：</strong> 在新视角合成和新状态合成方面的卓越表现，将为虚拟现实、增强现实和元宇宙等应用提供更真实、更沉浸式的体验。</li>
<li><strong>促进多模态学习和 Transformer 在 3D 领域的应用：</strong> 成功地将 Transformer 架构应用于联合推理相机姿态和铰接变化，进一步证明了其在复杂 3D 几何和运动建模中的强大潜力。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 快速生成高保真度的 3D 铰接物体模型，用于构建沉浸式虚拟环境和增强现实体验。</li>
<li><strong>机器人学：</strong> 为机器人提供更精确的物体模型，用于抓取、操作和环境理解。</li>
<li><strong>电影和游戏产业：</strong> 简化 3D 角色和道具的建模流程，提高生产效率和内容质量。</li>
<li><strong>数字人与虚拟试穿：</strong> 精确重建人体模型及其关节，用于虚拟试穿、运动分析和数字人创建。</li>
<li><strong>文化遗产数字化：</strong> 对历史文物或艺术品中的铰接部件进行高精度 3D 建模。</li>
<li><strong>产品设计与原型制作：</strong> 从稀疏图像快速创建产品原型，进行可视化和迭代。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 尽管是前馈模型，但基于 Transformer 的架构和处理复杂 3D 数据的特性，可能仍然需要相当大的计算资源（例如 GPU 内存和计算能力）进行训练和推理，尤其是在处理“大型”铰接物体时。</li>
<li><strong>泛化能力：</strong> 摘要提到“支持高保真重建跨多样对象类别”，但具体在训练数据中包含了哪些“多样对象类别”，以及模型对完全未见过的新类别或极端铰接配置的泛化能力如何，仍需在论文正文中详细探讨。</li>
<li><strong>实时性：</strong> 摘要强调“更快”，但对于特定应用（如实时 AR/VR）是否能达到严格的实时性能，还需要进一步的性能指标。</li>
<li><strong>输入图像质量和数量的敏感性：</strong> 尽管支持“稀疏视角图像”，但图像的数量和质量（例如分辨率、光照条件、遮挡程度）对最终重建质量的影响程度，以及在何种稀疏程度下性能开始显著下降，是值得关注的问题。</li>
<li><strong>“LVSM”的局限性继承：</strong> LARM 是 LVSM 的扩展，如果 LVSM 本身存在某些固有的局限性（例如对特定材质、透明度或复杂拓扑结构的处理能力），LARM 可能会继承这些局限性。</li>
<li><strong>“Large Articulated-Object”的定义：</strong> 摘要中“Large”的含义可能指物体尺寸大，也可能指关节数量多或复杂性高。这可能暗示模型在处理非常精细或微小的铰接结构时可能面临挑战，或者对具有大量自由度的物体有特定的处理策略。</li>
</ul>
<hr />
<p>总的来说，LARM 提出了一种非常有前景的方法，有望解决 3D 铰接物体重建领域长期存在的效率和质量难题，其统一的前馈框架和对 Transformer 的巧妙应用，使其成为计算机视觉领域一个值得关注的重要进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures.</li>
<li>LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis.</li>
<li>Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11563v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11563v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11526v1'></a></p>
<h2 id="bridging-hidden-states-in-vision-language-models"><a href="https://arxiv.org/abs/2511.11526v1">Bridging Hidden States in Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Benjamin Fein-Ashley, Jacob Fein-Ashley</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities "think". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Benjamin Fein-Ashley和Jacob Fein-Ashley撰写的论文“Bridging Hidden States in Vision-Language Models”的全面摘要。</p>
<hr />
<h3 id="bridging-hidden-states-in-vision-language-models_1">论文《Bridging Hidden States in Vision-Language Models》摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前视觉-语言模型（VLMs）在融合视觉和文本信息时，通常采用“早期融合”（在编码器内部混合token/特征）或“晚期融合”（比较池化后的嵌入）。这两种方法各有弊端：早期融合可能模糊单模态的专业性，而晚期融合则限制了细粒度的跨模态推理。此外，许多方法将融合与自回归解码器绑定，导致理解和生成过程耦合。论文旨在解决如何更有效地对齐视觉和文本模态的隐藏状态，以实现更深层次、细粒度的跨模态理解，同时保持双编码器的效率和解耦的生成能力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了一个名为 <strong>BRIDGE</strong>（bidirectional hidden-state exchange）的轻量级融合模块，其核心创新在于：
*   <strong>直接对齐隐藏状态：</strong> BRIDGE在视觉和文本编码器顶部附近插入少量“仅交叉”（cross-only）的双向注意力层，直接对齐两个模态的完整隐藏状态序列，而非仅仅池化后的嵌入。
*   <strong>共享潜在空间与门控残差更新：</strong> 每个交叉注意力层将视觉和文本编码器的隐藏状态序列投影到一个共享空间，进行跨模态注意力，并通过门控残差更新将信息反馈回各自的模态，同时采用简单的稳定器来改善对齐。
*   <strong>解耦的理解与生成：</strong> 编码器保持非因果性，专注于理解，而生成任务则通过一个可选的解码器（通常是LLM） cleanly decoupled，避免了融合与生成任务的强耦合。
*   <strong>新颖的训练策略：</strong> 结合了标准的对比学习（InfoNCE）、图像-文本匹配（ITM）目标，以及一个<strong>循环一致性交叉注意力损失</strong>，显式鼓励图像和文本token之间稳定的双向对齐。
*   <strong>分阶段训练方案：</strong> 采用三阶段训练：稳定（冻结编码器，训练交互层）、对齐（解冻编码器顶部K个块）、任务微调（针对特定任务）。</p>
<p><strong>3. 主要结果及其意义：</strong>
BRIDGE在多个标准基准测试中表现出色，验证了其方法的有效性：
*   <strong>图像-文本检索：</strong> 在MSCOCO和Flickr30K数据集上，BRIDGE在可比较的ViT-B/16和BERT-Base骨干网络下，始终优于CLIP、ALBEF和BLIP等模型。增加交互层数量能单调提升检索性能，表明更深层次的跨模态融合有助于组合推理。
*   <strong>视觉问答（VQA）：</strong> 在VQAv2基准测试中，BRIDGE提供了具有竞争力的VQA性能，再次证明了其在不同交互层数量下的有效性。
*   <strong>自然语言视觉推理（NLVR2）：</strong> 在NLVR2基准测试中，BRIDGE也取得了优异的准确率。
*   <strong>消融研究：</strong> 结果表明，隐藏状态融合（而非仅池化融合）、交互层的晚期放置（靠近编码器顶部）以及所有损失函数（InfoNCE、ITM和循环一致性交叉注意力损失）的组合是实现最佳性能的关键因素。
*   <strong>定性分析（UMAP可视化）：</strong> UMAP可视化显示，BRIDGE生成的联合视觉-文本嵌入空间具有紧密、高度重叠的簇，同一示例的视觉和文本嵌入几乎共置，表明模态间隙显著缩小，共享表示更具连贯性。</p>
<p>这些结果表明，BRIDGE在保持双编码器检索效率的同时，显著提升了跨模态理解和推理能力，为VLMs的融合机制提供了一个强大且模块化的新范式。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>模型规模和交互层数量：</strong> 论文中使用的骨干编码器模型相对较小，交互层数量也因计算预算限制而较少，这可能低估了该方法在更大规模模型上的潜在收益。
*   <strong>基准测试范围：</strong> 评估主要集中在特定基准测试上。对于开放式对话或高度指令遵循的场景（如LLaVA等指令微调的MLLMs），大型对比或字幕模型（如CLIP和BLIP）可能仍优于BRIDGE，因为它们是专门为此类任务优化的。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更大规模模型的探索：</strong> 在更大的骨干编码器和更多交互层上测试BRIDGE，以充分挖掘其潜力。
*   <strong>与现有解码器的结合：</strong> 将BRIDGE作为模块化融合机制，与现有的LLM解码器或指令微调的MLLMs结合，以提升其在开放式对话和复杂指令遵循任务中的表现。
*   <strong>更广泛的基准测试：</strong> 扩展评估范围，包括更多样化的视觉-语言任务和数据集。
*   <strong>优化训练效率：</strong> 进一步研究和优化分阶段训练方案，以提高训练效率和稳定性。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Vision-Language Models (VLMs) are a new family of models that align image content with natural language.</li>
<li>We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders.</li>
<li>Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11526v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11526v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11438v1'></a></p>
<h2 id="vp-bench-a-comprehensive-benchmark-for-visual-prompting-in-multimodal-large-language-models"><a href="https://arxiv.org/abs/2511.11438v1">VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models</a></h2>
<p><strong>Authors:</strong> Mingjie Xu, Jinpeng Chen, Yuzhi Zhao, Jason Chun Lok Li, Yue Qiu, Zekang Du, Mengyang Wu, Pingping Zhang, Kun Li, Hongzheng Yang, Wenao Ma, Jiaheng Wei, Qinbin Li, Kangcheng Liu, Wenqiang Lei</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use "visual prompts" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Mingjie Xu等人撰写的论文“VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models”的全面摘要：</p>
<p><strong>1. 论文主要问题或研究问题</strong></p>
<p>该论文旨在解决多模态大型语言模型（MLLMs）在理解和利用视觉提示（VPs）方面的系统性评估缺失问题。尽管人类在查询图像特定区域或对象时自然会使用边界框等视觉提示，但目前尚不清楚现有MLLMs能否有效识别这些直观的视觉提示，并利用它们解决实际问题。因此，研究问题是：如何系统地评估MLLMs在视觉提示感知和利用方面的能力？</p>
<p><strong>2. 主要创新或方法学贡献</strong></p>
<p>为了解决上述问题，论文提出了<strong>VP-Bench</strong>，这是一个全面的基准测试，具有以下关键创新和方法学贡献：</p>
<ul>
<li><strong>两阶段评估框架：</strong><ul>
<li><strong>第一阶段（模型感知）：</strong> 评估模型在自然场景中感知VPs的能力。该阶段使用了3万个可视化提示，涵盖8种形状和355种属性组合，以评估MLLMs对VP存在性、枚举、定位和引用等核心能力的理解。</li>
<li><strong>第二阶段（VP对下游任务的影响）：</strong> 评估VPs对下游任务的影响，衡量其在真实世界问题解决场景中的有效性。该阶段引入了六个结合VP推理的任务，包括医学图像分析、3D对象识别、面部情感识别、街景识别、GUI元素识别和场景图生成。</li>
</ul>
</li>
<li><strong>全面的VP属性组合：</strong> VP-Bench涵盖了8种VP形状（标签、边界框、箭头、蒙版、轮廓、椭圆、点、涂鸦）及其355种属性组合（如线宽、颜色、样式等），远超现有基准。</li>
<li><strong>去偏问题（Debias Questions）：</strong> 引入去偏问题，以减轻MLLMs在VP存在性方面的幻觉，确保评估结果的可靠性。</li>
<li><strong>统一的VP描述方案：</strong> 将每个视觉提示转换为简短、结构化的文本短语，附加到指令中，以明确VP的语义，减少歧义并改善视觉-语言对齐。</li>
<li><strong>广泛的模型评估：</strong> 使用VP-Bench评估了28个MLLMs，包括GPT-4o等专有系统和InternVL3、Qwen2.5-VL等开源模型。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<ul>
<li><strong>MLLMs在VP感知方面接近人类水平，但仍有提升空间：</strong> 人类基线在所有任务上的平均准确率为90.03%，而表现最好的模型InternVL3-78B达到了87.97%。这表明MLLMs在VP感知方面已接近人类水平，但在某些VP类型（如点感知）上仍有约10%的差距。</li>
<li><strong>开源模型在VP感知方面超越专有模型：</strong> 大多数测试的开源模型平均准确率约为80%，InternVL3-78B达到88.62%，而专有模型GPT-4o仅为68.93%。在下游任务中，Molmo-72B甚至在MIA、街景识别、3D对象识别和SGG任务上超越了GPT-4o。</li>
<li><strong>VP形状对模型性能至关重要：</strong> 规则形状（如边界框、椭圆）通常比不规则形状（如蒙版、点、涂鸦）更有效。模型在规则VP上的准确率平均约为80%，而不规则VP则低于70%。</li>
<li><strong>明确的VP描述有助于上下文理解：</strong> 实验结果表明，在指令中包含VP描述可以显著提高模型对VPs的理解，尤其是在蒙版和点等复杂形状场景中。</li>
<li><strong>模型规模与VP感知准确性强相关：</strong> 随着参数规模的增加，MLLMs在VP感知和下游任务性能上表现出一致的提升，大型模型在不规则VP类型上表现出明显优势。</li>
<li><strong>单纯的VP数据训练无明显益处：</strong> 简单地使用VP相关数据进行指令微调，可能导致下游任务性能下降，这表明需要更平衡的数据组成和更精细的训练策略。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong></p>
<ul>
<li><strong>MLLMs在处理不规则VP形状方面表现较差：</strong> 现有模型在蒙版、点、涂鸦等不规则形状上的准确率显著低于规则形状，且与人类基线差距更大。这可能与训练数据中不规则形状的示例较少有关。</li>
<li><strong>VP选择策略的鲁棒性：</strong> 尽管模型感知准确性是选择VP的主要标准，但鲁棒的VP选择策略对于最大化下游性能也至关重要，因为最佳VP组合在不同任务中可能有所不同。</li>
<li><strong>模型性能对领域知识的依赖：</strong> 实验结果表明，模型性能在很大程度上仍然依赖于领域知识，这限制了其在通用场景下的泛化能力。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>优化VP属性表示：</strong> 改进VP属性的表示方式，使其能更有效地被MLLMs理解和利用。</li>
<li><strong>增强空间推理能力：</strong> 进一步研究如何提升MLLMs的空间推理能力，以更好地整合视觉提示信息。</li>
<li><strong>提高模型可解释性和实际适用性：</strong> 通过改进VP理解和空间推理，最终提高模型的可解释性，使其在真实世界应用中更可靠。</li>
<li><strong>平衡训练数据组成和精炼训练策略：</strong> 针对VP感知和下游任务性能的权衡，开发更有效的训练方法，以避免单纯增加VP数据导致性能下降。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization.</li>
<li>VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11438v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11438v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11434v1'></a></p>
<h2 id="weave-unleashing-and-benchmarking-the-in-context-interleaved-comprehension-and-generation"><a href="https://arxiv.org/abs/2511.11434v1">WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation</a></h2>
<p><strong>Authors:</strong> Wei Chow, Jiachun Pan, Yongyuan Liang, Mingze Zhou, Xue Song, Liyu Jia, Saining Zhang, Siliang Tang, Juncheng Li, Fengda Zhang, Weijia Wu, Hanwang Zhang, Tat-Seng Chua</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于引入了WEAVE，一个用于研究上下文交错式跨模态理解和生成的综合套件。WEAVE通过其大规模数据集WEAVE-100k和人工标注基准WEAVEBench，解决了现有统一多模态模型(UMMs)在处理多轮、上下文依赖的图像创建和编辑任务方面的不足。它为评估和训练UMMs在视觉记忆、世界知识推理以及理解-生成协作能力方面提供了新的视角和基础。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>关键创新在于其对“上下文交错式跨模态理解和生成”这一新范式的关注和系统性构建。具体方法论包括：</p>
<ul>
<li><strong>WEAVE-100k 数据集：</strong> 这是一个大规模数据集，包含100K交错样本，涵盖超过370K对话轮次和500K图像。它专门设计用于训练模型进行需要历史上下文推理的理解、编辑和生成任务。这与现有主要关注单轮交互的数据集形成鲜明对比。</li>
<li><strong>WEAVEBench 基准：</strong> 这是一个高质量的人工标注基准，包含100个任务和480张图像。它引入了一个混合VLM判别器评估框架，该框架结合了参考图像以及原始图像与编辑指令，以评估模型在多轮生成、视觉记忆和世界知识推理方面的能力。这种混合评估方式比单一指标更全面。</li>
<li><strong>“上下文交错式”范式：</strong> 论文明确提出并构建了这一范式，强调了在真实世界图像交互中，理解和生成是相互穿插、依赖历史上下文的。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>WEAVE有望对计算机视觉和多模态领域产生深远影响：</p>
<ul>
<li><strong>推动UMMs发展：</strong> 它为UMMs提供了一个更真实、更具挑战性的训练和评估环境，促使模型从单轮交互向多轮、上下文感知能力发展。</li>
<li><strong>催生新研究方向：</strong> “上下文交错式理解和生成”这一范式本身就是一个新的研究方向，将鼓励研究人员探索如何更好地建模和利用历史上下文信息。</li>
<li><strong>促进视觉记忆和推理研究：</strong> WEAVEBench特别强调了视觉记忆和世界知识推理，这将推动UMMs在这些高级认知能力上的进步。</li>
<li><strong>加速实际应用：</strong> 更好的多轮图像编辑和生成能力将直接惠及创意设计、内容创作、虚拟现实/增强现实等需要复杂图像交互的应用。</li>
<li><strong>成为新标准：</strong> WEAVE-100k和WEAVEBench有可能成为未来评估多模态模型在复杂交互能力方面的新标准。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>多模态大模型 (LLMs/VLMs)：</strong> 直接受益者，WEAVE将帮助这些模型提升其在视觉领域的对话和交互能力。</li>
<li><strong>图像编辑和生成：</strong> 尤其是需要多步操作、上下文依赖的复杂编辑任务，如专业图像处理软件中的智能助手。</li>
<li><strong>人机交互 (HCI)：</strong> 改进用户与图像生成/编辑工具的自然语言交互体验。</li>
<li><strong>内容创作和设计：</strong> 艺术家、设计师和内容创作者可以利用更智能的工具进行迭代式创作。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 在这些环境中，用户可能需要通过多轮对话来创建或修改虚拟对象。</li>
<li><strong>具身智能 (Embodied AI)：</strong> 机器人或智能体在与环境交互时，可能需要根据历史视觉信息进行理解和生成动作。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>“混合VLM判别器评估框架”的鲁棒性：</strong> 摘要提到使用“混合VLM判别器”，虽然这比纯人工评估更高效，但VLM判别器本身可能存在偏见或局限性，其评估结果的可靠性和泛化性需要进一步验证。</li>
<li><strong>数据收集的复杂性：</strong> 构建100K交错样本、370K对话轮次和500K图像的数据集是一个巨大的工程，其数据质量控制、多样性和潜在的偏见（例如，如果数据主要来自特定领域或用户群体）是需要关注的问题。</li>
<li><strong>“新兴视觉记忆能力”的定义和量化：</strong> 摘要提到训练WEAVE-100k“使UMMs发展出新兴视觉记忆能力”，但这种“新兴”能力的具体定义、如何量化以及其稳定性仍需在论文正文中详细阐述。</li>
<li><strong>“世界知识推理”的深度：</strong> 虽然WEAVEBench评估了世界知识推理，但多模态模型在复杂、常识性世界知识推理方面仍面临挑战。WEAVE能推动到何种深度，仍有待观察。</li>
<li><strong>计算资源需求：</strong> 训练和评估如此大规模的数据集和复杂任务，无疑需要大量的计算资源，这可能限制了小型研究团队的参与。</li>
</ul>
<hr />
<p>总而言之，WEAVE这篇论文通过引入一个全新的数据集和基准，成功地将多模态模型的研究推向了更真实、更复杂的“上下文交错式”交互范式。它不仅揭示了现有模型的局限性，也为未来UMMs在视觉记忆、推理和多轮交互能力上的发展指明了方向，具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11434v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11434v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11427v1'></a></p>
<h2 id="comprehension-of-multilingual-expressions-referring-to-target-objects-in-visual-inputs"><a href="https://arxiv.org/abs/2511.11427v1">Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs</a></h2>
<p><strong>Authors:</strong> Francisco Nogueira, Alexandre Bernardino, Bruno Martins</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions. Research on the area remains predominantly English-centric, despite increasing global deployment demands. This work addresses multilingual REC through two main contributions. First, we construct a unified multilingual dataset spanning 10 languages, by systematically expanding 12 existing English REC benchmarks through machine translation and context-based translation enhancement. The resulting dataset comprises approximately 8 million multilingual referring expressions across 177,620 images, with 336,882 annotated objects. Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders. Our attention-based approach generates coarse spatial anchors from attention distributions, which are subsequently refined through learned residuals. Experimental evaluation demonstrates competitive performance on standard benchmarks, e.g. achieving 86.9% accuracy at IoU@50 on RefCOCO aggregate multilingual evaluation, compared to an English-only result of 91.3%. Multilingual evaluation shows consistent capabilities across languages, establishing the practical feasibility of multilingual visual grounding systems. The dataset and model are available at <script type="math/tex">\href{https://multilingual.franreno.com}{multilingual.franreno.com}</script>.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs</strong></p>
<p><strong>1. 论文主要贡献的简明总结 (2-3 句话)</strong></p>
<p>这篇论文主要解决了多语言指代表达理解（REC）领域中英语中心化的问题。它通过系统地扩展现有英语数据集并引入上下文增强的机器翻译，构建了一个包含10种语言、约800万个指代表达的统一多语言数据集。在此基础上，作者提出了一种基于注意力机制的神经网络架构，该架构利用多语言SigLIP2编码器生成粗略空间锚点并进行精细化，从而实现了在多语言REC任务上的竞争性性能。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>多语言数据集构建：</strong> 这是最核心的创新之一。论文通过<strong>系统地扩展12个现有英语REC基准</strong>，并结合<strong>机器翻译和上下文增强翻译</strong>，创建了一个前所未有的、大规模的统一多语言REC数据集。这种方法解决了多语言REC研究中数据稀缺的根本问题。</li>
<li><strong>注意力锚定神经网络架构：</strong> 提出的模型利用<strong>多语言SigLIP2编码器</strong>，并通过<strong>注意力分布生成粗略的空间锚点</strong>。这种“注意力锚定”的思想是关键，它允许模型首先识别目标对象的潜在区域，然后通过<strong>学习残差</strong>对这些锚点进行精细化。这种两阶段的定位策略可能比直接预测边界框更鲁棒，尤其是在处理多语言和多样化表达时。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动多语言REC研究：</strong> 该论文的数据集和模型将极大地促进多语言REC领域的研究。它为研究人员提供了一个标准化的基准和强大的起点，以开发更先进的多语言视觉-语言模型。</li>
<li><strong>促进全球化部署：</strong> 解决REC的语言障碍对于将计算机视觉系统部署到全球市场至关重要。这项工作证明了构建实用多语言视觉定位系统的可行性，将加速智能助手、机器人、自动驾驶等应用在全球范围内的普及。</li>
<li><strong>启发跨语言视觉-语言任务：</strong> 论文的方法和数据集构建策略可以启发其他跨语言视觉-语言任务（如图像字幕、视觉问答）的数据集构建和模型设计。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>智能助手和人机交互：</strong> 用户可以用母语描述图像中的对象，智能助手可以准确理解并执行相关操作。</li>
<li><strong>机器人学：</strong> 机器人可以根据多语言指令识别和操作环境中的特定物体。</li>
<li><strong>自动驾驶：</strong> 车辆可以理解来自不同语言的导航指令或对特定交通标志、障碍物的描述。</li>
<li><strong>图像检索和内容管理：</strong> 用户可以使用多种语言的查询来查找图像中的特定内容。</li>
<li><strong>辅助技术：</strong> 帮助视障人士通过多语言描述理解图像内容。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>性能差距：</strong> 摘要明确指出，在RefCOCO聚合多语言评估中，多语言模型的准确率为86.9%，而英语模型的准确率为91.3%。这表明多语言模型在性能上仍与英语专用模型存在一定差距，可能需要进一步的研究来弥合。</li>
<li><strong>机器翻译的质量依赖：</strong> 数据集构建严重依赖机器翻译。尽管提到了“上下文增强翻译”，但机器翻译固有的错误、歧义和文化差异可能仍然会引入噪声，影响数据集的整体质量和模型的学习效果。</li>
<li><strong>语言覆盖范围：</strong> 虽然覆盖了10种语言，但全球语言种类繁多。对于资源较少或结构差异较大的语言，模型的泛化能力可能仍是一个挑战。</li>
<li><strong>“粗略空间锚点”的定义和限制：</strong> 摘要中提到“粗略空间锚点”，但没有详细说明其粒度或如何定义“粗略”。如果锚点过于粗略，后续的精细化可能需要更强的模型能力；如果锚点不够准确，也可能限制最终的定位精度。</li>
<li><strong>SigLIP2编码器的适用性：</strong> 虽然SigLIP2是强大的多模态编码器，但其在处理特定语言或特定视觉特征时的局限性可能需要进一步探讨。</li>
</ul>
<hr />
<p>总而言之，这篇论文在解决多语言REC的实际挑战方面迈出了重要一步，其大规模多语言数据集和创新的注意力锚定架构为该领域未来的发展奠定了坚实基础。尽管存在一些可推断的局限性，但其对全球化部署和跨语言视觉-语言研究的潜在影响是巨大的。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This work addresses multilingual REC through two main contributions.</li>
<li>Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11427v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11427v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11410v1'></a></p>
<h2 id="q-doc-benchmarking-document-image-quality-assessment-capabilities-in-multi-modal-large-language-models"><a href="https://arxiv.org/abs/2511.11410v1">Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models</a></h2>
<p><strong>Authors:</strong> Jiaxi Huang, Dongxu Wu, Hanwei Zhu, Lingyu Zhu, Jun Xing, Xu Wang, Baoliang Chen</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:   https://github.com/cydxf/Q-Doc.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jiaxi Huang等人撰写的论文“Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models”的全面摘要。</p>
<hr />
<h3 id="q-doc">论文摘要：Q-Doc: 基准测试多模态大语言模型在文档图像质量评估方面的能力</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决多模态大语言模型（MLLMs）在文档图像质量评估（DIQA）方面的潜力尚未得到充分探索的问题。尽管MLLMs在高级视觉任务中表现出色，但它们在低级视觉质量感知，特别是针对文档图像的DIQA任务中的能力仍是未知数。DIQA对于OCR、信息检索和文档理解等下游任务至关重要，而现有基准主要侧重于高级语义理解或通用视觉识别，缺乏针对文档图像质量的系统性评估。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了弥补这一空白，作者提出了Q-Doc，一个全面的三层评估框架，用于系统地探究MLLMs的DIQA能力：</p>
<ul>
<li><strong>粗粒度（Coarse-Level）评估：</strong> 评估MLLMs对文档图像进行整体质量评分（如“优秀”、“差”），并分析其与人工标注的质量相关性。这被视为一个质量分类任务，通过SRCC和PLCC衡量相关性。</li>
<li><strong>中粒度（Middle-Level）评估：</strong> 设计了失真类型识别任务，包括单选和多选测试，以应对多失真场景。这评估了MLLMs识别图像中存在的失真类型（如模糊、光照不足、失焦）的能力。</li>
<li><strong>细粒度（Fine-Level）评估：</strong> 引入了失真严重性评估，要求MLLMs根据人工标注的参考，对失真强度进行分类（如“好”、“中等”、“差”）。</li>
<li><strong>链式思考（Chain-of-Thought, CoT）提示：</strong> 为了增强MLLMs的推理过程和质量估计，Q-Doc将CoT提示策略整合到评估流程中。通过鼓励模型明确思考文本可读性及其与特定视觉伪影的相关性，CoT显著提高了所有层面的可解释性和预测性能。</li>
<li><strong>数据集：</strong> Q-Doc基于SmartDoc-QA数据集，包含4260张真实世界的文档图像，涵盖30种独特文档，具有单失真和复合失真，来自现代表格、历史信件和收据三个实际领域。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
作者对六个代表性MLLMs（包括开源模型如Co-Instruct、Llama-3.2-11B-Vision-Instruct、mPLUG-Owl3-7B-241101、GLM-4V-9B、DeepSeek-VL2以及商业模型GPT-4o-241120）进行了零样本设置下的评估。</p>
<ul>
<li><strong>MLLMs的DIQA能力尚处于萌芽阶段：</strong> 评估结果表明，MLLMs虽然具备初步的DIQA能力，但存在显著局限性，包括评分不一致、失真误识别和严重性误判。</li>
<li><strong>性能不一致性：</strong> 大多数模型在不同粒度级别上的质量感知能力不一致。例如，Llama-3.2-11B-Vision-Instruct在中粒度失真分类上表现出色，但在粗粒度质量评分和细粒度严重性估计上表现不佳。GPT-4o在中间级别分类和精细级别粒度方面表现出色，但在整体质量评分方面却出人意料地表现不佳，这表明高分辨率感知与全局文档质量判断之间可能存在不匹配。</li>
<li><strong>DeepSeek-VL2表现出最均衡的能力：</strong> 在所有评估模型中，DeepSeek-VL2是唯一一个在粗、中、细粒度级别上始终保持竞争性性能的模型，这可能归因于其混合专家（MoE）架构，使其能够动态激活专门的子模块进行不同粒度的视觉推理。</li>
<li><strong>CoT提示的显著增强作用：</strong> 链式思考（CoT）提示显著提高了MLLMs在所有层面的性能，尤其是在中粒度多失真识别任务中。这表明，即使对于需要视觉-文本对齐的任务，明确的推理指导也能显著提高MLLM预测的可靠性和可解释性。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>严格的评估标准：</strong> 在多失真任务中，作者采用了严格的匹配标准，即只有当模型完全匹配所有真实失真类型时才算正确，这使得任务特别严苛，尤其是在多种失真同时出现时。这可能导致绝对准确率看起来较低，但反映了在高精度标准下MLLM能力的有意义区别。
*   <strong>数据集子变体的多样性：</strong> 在细粒度评估中，单失真图像包含更广泛的子变体，可能增加了类内模糊性，而多失真评估则侧重于校准良好且感知上可区分的子类型，这可能导致在某些情况下多失真样本上的性能优于单失真样本，这在直觉上可能显得反常。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>开发质量感知、鲁棒和可读的文档理解系统：</strong> Q-Doc的提出旨在激发未来基于MLLMs的质量感知、鲁棒和可读文档理解系统的发展。
*   <strong>改进MLLMs的架构和训练：</strong> 论文结果揭示了当前MLLMs在文档质量评估中存在碎片化的优势，一些模型在孤立的子任务中表现出色，但缺乏跨层一致性。DeepSeek-VL2的模块化设计被认为是一个有前景的方向，可以构建具有通用和分层感知能力的质量感知MLLMs。未来的研究可以探索如何通过改进视觉令牌集成、提示理解和训练语料平衡来优化MLLMs的质量感知能力。
*   <strong>进一步探索CoT提示：</strong> CoT提示的有效性表明，进一步研究和优化CoT策略，以更好地指导MLLMs进行视觉-文本对齐和推理，将是重要的方向。</p>
<hr />
<p>总而言之，Q-Doc为评估和提升MLLMs在文档图像质量评估方面的能力提供了一个重要的基准和工具。它不仅揭示了当前MLLMs的优势和不足，也为未来开发更智能、更可靠的文档处理系统指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels.</li>
<li>c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references.</li>
<li>Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11410v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11410v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11344v1'></a></p>
<h2 id="ycb-ev-sd-synthetic-event-vision-dataset-for-6dof-object-pose-estimation"><a href="https://arxiv.org/abs/2511.11344v1">YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation</a></h2>
<p><strong>Authors:</strong> Pavel Rojtberg, Julius Kühn</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation. While synthetic data has become fundamental in frame-based computer vision, event-based vision lacks comparable comprehensive resources. Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology. Our generation framework employs simulated linear camera motion to ensure complete scene coverage, including background activity.   Through systematic evaluation of event representations for CNN-based inference, we demonstrate that time-surfaces with linear decay and dual-channel polarity encoding achieve superior pose estimation performance, outperforming exponential decay and single-channel alternatives by significant margins. Our analysis reveals that polarity information contributes most substantially to performance gains, while linear temporal encoding preserves critical motion information more effectively than exponential decay. The dataset is provided in a structured format with both raw event streams and precomputed optimal representations to facilitate immediate research use and reproducible benchmarking.   The dataset is publicly available at https://huggingface.co/datasets/paroj/ycbev_sd.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Pavel Rojtberg和Julius Kühn撰写的论文“YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：YCB-Ev SD: 用于6DoF物体姿态估计的合成事件视觉数据集</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决事件相机数据在6DoF物体姿态估计领域缺乏综合性、大规模数据集的问题。尽管合成数据在基于帧的计算机视觉中已成为基础，但事件视觉领域仍缺乏可与之媲美的资源。现有的事件数据集要么分辨率较低，要么缺乏合成数据，无法提供精确的标注和多样化的场景。此外，论文还探讨了哪种事件表示方法最适合基于CNN的推理，以实现最佳的姿态估计性能。</p>
<p><strong>2. 主要创新或方法论贡献：</strong>
*   <strong>大规模合成事件数据集YCB-Ev SD：</strong> 作者引入了一个包含50000个事件序列的合成数据集，每个序列持续34毫秒。这些序列是从YCB-Video物体的物理渲染（PBR）场景中合成的，遵循BOP（Benchmark for 6D Object Pose）方法论，并采用标准清晰度（SD）分辨率。
*   <strong>模拟线性相机运动：</strong> 生成框架采用模拟线性相机运动，以确保全面的场景覆盖，包括背景活动，从而创建丰富且具有挑战性的基准。
*   <strong>事件表示的系统评估：</strong> 论文系统评估了多种事件表示方法（事件直方图、时间表面，以及不同的极性编码和衰减函数），以用于基于CNN的推理。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最佳事件表示：</strong> 具有线性衰减和双通道极性编码的时间表面在姿态估计性能方面表现最佳，显著优于指数衰减和单通道替代方案。
*   <strong>极性信息的重要性：</strong> 分析表明，极性信息对性能提升的贡献最大。
*   <strong>线性时间编码的优势：</strong> 线性时间编码比指数衰减更有效地保留了关键的运动信息。
*   <strong>数据集的可用性：</strong> YCB-Ev SD数据集以结构化格式提供，包含原始事件流和预计算的最佳表示，便于研究人员立即使用和进行可复现的基准测试。数据集已公开在Hugging Face平台发布。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>真实世界验证：</strong> 论文中提到，在真实世界事件相机数据上验证研究结果对于评估数据集的模拟到真实迁移能力和实用性至关重要。
*   <strong>相机运动类型：</strong> 目前的工作主要关注线性相机运动，尚未探索旋转和加速运动，以及不同物体运动速度。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>模拟到真实迁移：</strong> 在真实世界事件相机数据上验证数据集的有效性，以评估其在实际应用中的潜力。
*   <strong>更复杂的相机和物体运动：</strong> 探索旋转和加速运动，以及不同物体运动速度，以增强数据集的真实性和适用性。
*   <strong>先进的神经网络架构：</strong> 探索Transformer等先进神经网络架构，利用其建模稀疏、异步数据中长距离依赖关系的能力，以进一步提升事件姿态估计的性能。</p>
<hr />
<p>这篇论文通过引入一个大规模、高质量的合成事件数据集，并对事件表示方法进行了系统性评估，为6DoF物体姿态估计的事件视觉领域做出了重要贡献。其结果为未来在该领域的研究提供了宝贵的见解和资源。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation.</li>
<li>Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology.</li>
<li>Through systematic evaluation of event representations for CNN-based inference, we demonstrate that time-surfaces with linear decay and dual-channel polarity encoding achieve superior pose estimation performance, outperforming exponential decay and single-channel alternatives by significant margins.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11344v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11344v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11313v1'></a></p>
<h2 id="docslm-a-small-vision-language-model-for-long-multimodal-document-understanding"><a href="https://arxiv.org/abs/2511.11313v1">DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding</a></h2>
<p><strong>Authors:</strong> Tanveer Hannan, Dimitrios Mallios, Parth Pathak, Faegheh Sardari, Thomas Seidl, Gedas Bertasius, Mohsen Fayyaz, Sunando Sengupta</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material.</p>
<p><strong>Analysis:</strong></p>
<p>以下是论文“DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
大型视觉-语言模型（LVLMs）在理解长而复杂的多模态文档方面表现出强大的推理能力，但其高内存占用使其在资源受限的边缘设备上部署不切实际。因此，该研究旨在解决如何在内存和计算资源受限的条件下，实现对任意长多模态文档的高效、可靠理解。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
DocSLM 引入了两个关键创新来解决上述问题：</p>
<ul>
<li><strong>分层多模态压缩（Hierarchical Multimodal Compression）模块：</strong> 该模块旨在将每页文档的视觉、文本和布局特征压缩成固定长度的紧凑 token 序列。它通过两阶段融合过程实现：<ul>
<li><strong>局部 OCR 压缩：</strong> 将每个视觉区域与其对应的 OCR 文本和布局信息通过局部注意力对齐，合并为紧凑的区域级特征。</li>
<li><strong>全局视觉压缩：</strong> 将这些区域级特征聚合成固定长度的页面表示，同时保留空间对齐和整体文档语义。这种方法确保每页文档（无论 OCR token 数量多少）都由固定数量的 token 表示，从而显著减少内存开销。</li>
</ul>
</li>
<li><strong>流式弃权（Streaming Abstention）机制：</strong> 为了处理任意长的文档输入，该机制将文档分割成更小的段落，并顺序处理。每个段落独立编码，生成中间预测和基于熵的不确定性分数。通过这种方式，DocSLM 可以在不存储跨段激活的情况下，保持恒定的内存占用。不确定性校准器聚合所有有效的段级预测，并根据不确定性选择最可靠的文档级答案，从而在保持上下文连续性的同时，实现全文档理解。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
DocSLM 在多个长多模态文档基准测试中取得了显著成果：</p>
<ul>
<li><strong>效率提升：</strong> DocSLM 使用比现有大型 LVLMs 少 82% 的视觉 token、少 75% 的参数和低 71% 的延迟，实现了高效的文档理解。例如，在 MMLongDoc 上，它比 DocOwl2 实现了 +9.3% 的准确率提升，同时 token 预算相当；比 InternVL2-RAG 等大型 RAG 模型少 75% 的参数；比 Docopilot-2B 性能高 +0.9%，尽管其 token 预算显著更大。</li>
<li><strong>内存优化：</strong> DocSLM 在处理超过 10 页的文档时，能将峰值 GPU 内存使用量保持在约 14 GB 的恒定水平，这得益于其流式机制，使其能够在资源受限的设备上进行可扩展的推理。</li>
<li><strong>性能表现：</strong> 尽管模型尺寸紧凑，DocSLM 在 MMLongDoc、MP-DocVQA 和 DUDE 等基准测试中匹配或超越了最先进的方法，甚至在 DUDE 上比 InternVL2-8B 提高了 +5.7%，并比 Docopilot-2B 提高了 +26.3%。</li>
<li><strong>跨模态泛化：</strong> DocSLM 即使主要在文档上训练，也能有效泛化到视频问答任务，在 NewsVQA 上实现了 66.2 的 ANLS 分数，超越了包括 Idefics3、LLaVA-Next 和 SV-RAG 在内的更大模型。</li>
</ul>
<p><strong>4. 论文中提到的局限性</strong>
论文中明确提到了以下局限性：</p>
<ul>
<li><strong>有限的视频数据训练：</strong> 尽管 DocSLM 已经展示了强大的跨模态泛化能力，但其主要是在文档数据上训练的，视频数据量相对有限（8.6K 视频样本对比 6.75M 文档标注）。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
论文提出了以下未来研究方向：</p>
<ul>
<li><strong>扩展到其他模态：</strong> 未来的工作将扩展到其他模态，例如音频。</li>
<li><strong>平衡的文档-视频训练：</strong> 追求平衡的文档-视频训练，以实现面向边缘部署的全模态基础模型。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources.</li>
<li>To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator.</li>
<li>Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11313v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11313v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.11305v1'></a></p>
<h2 id="moon-embedding-multimodal-representation-learning-for-e-commerce-search-advertising"><a href="https://arxiv.org/abs/2511.11305v1">MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising</a></h2>
<p><strong>Authors:</strong> Chenghan Fu, Daoze Zhang, Yukang Lin, Zhanheng Nie, Xiang Zhang, Jianyu Liu, Yueran Liu, Wanxian Guan, Pengjie Wang, Jian Xu, Bo Zheng</p>
<p><strong>Published:</strong> 2025-11-14</p>
<p><strong>Categories:</strong> cs.IR, cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Chenghan Fu等人撰写的论文“MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决电子商务搜索广告中多模态表示学习的关键问题，特别是如何有效地将多模态内容（如图像和文本）整合到点击率（CTR）预测模型中，以提高广告效果和用户体验。传统端到端训练方法在处理多模态内容与CTR模型之间的异构性时面临挑战，导致性能受限。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>三阶段训练范式 (Pretraining, Post-training, and Application)</strong>：MOON提出并验证了一个解耦的多阶段训练架构，将多模态表示学习与下游任务（如CTR预测）有效整合。这解决了传统端到端训练中多模态模型和CTR模型目标不一致的问题。
*   <strong>交换率 (Exchange Rate) 概念</strong>：为弥合多模态表示学习目标与下游训练目标之间的不一致，论文定义了“交换率”来量化中间指标的改进如何有效地转化为下游收益。
*   <strong>图像搜索召回率 (Image-based Search Recall) 作为关键中间指标</strong>：通过交换率分析，论文将图像搜索召回率确定为指导多模态模型优化的关键中间指标，因为它与下游CTR性能呈强正相关。
*   <strong>生成式MLLM-based多模态表示学习</strong>：MOON的最新迭代创新性地采用了生成式多模态大语言模型（MLLMs），特别是内部开发的TBStars-VL模型，用于产品内容理解，能够更灵活地建模来自多张图像或文本的丰富信息。
*   <strong>Matryoshka Representation Learning (MRL)</strong>：在训练中引入MRL，使MOON模型能够生成多种维度（128、256、512、1024和3072）的表示，以适应不同的资源和性能约束，确保低维嵌入保留尽可能多的语义信息。
*   <strong>硬负样本采样 (Hard Negative Sampling) 和时空负样本采样 (Spatial-Temporal Negative Sampling)</strong>：在后训练阶段，通过构建硬负样本和利用时空负样本采样策略，显著扩大负样本池，增强模型区分语义相似产品的能力，从而学习更鲁棒和判别性的表示。
*   <strong>内容感知用户行为提取器 (CUBE)</strong>：在应用阶段，MOON设计了CUBE，通过计算目标商品与用户行为序列中商品的相似度，并将这些相似度与ID嵌入进行融合，以丰富CTR模型的特征空间。
*   <strong>高效可扩展的基础设施</strong>：论文详细介绍了覆盖多模态表示整个生命周期（从生产到消费）的高效、可扩展基础设施解决方案，包括高性能数据存储（ALake）、端到端性能加速、计算内存多模态表示中心、统一推理工具链、弹性异步推理以及实时感知管道。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>CTR预测显著提升</strong>：MOON在在线A/B测试中实现了整体CTR预测+20.00%的显著提升，是过去三年中CTR预测任务的最大改进。
*   <strong>中间指标表现优异</strong>：在图像搜索召回率这一关键中间指标上，MOON取得了强劲表现，例如在Top-1召回率达到95.1%，在Top-50召回率仍保持在90%以上，表明其强大的检索能力。
*   <strong>多模态理解能力增强</strong>：定性分析和跨模态对齐可视化（热力图）表明，MOON能够精确理解用户意图，识别产品图像和文本中的关键属性，并实现高度相关的语义对齐。
*   <strong>对新产品、时尚品类和低层级商家场景的优势</strong>：MOON在这些特定业务场景中分别实现了+34.80%、+35.74%和+23.03%的CTR提升，证明了其在冷启动问题、视觉主导产品理解和流量受限下的有效性。
*   <strong>扩展定律探索</strong>：系统性研究了训练令牌数量、负样本数量和用户行为序列长度对多模态表示学习性能的影响，发现存在边际收益递减的模式，为未来扩展提供了指导。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中并未明确提及当前MOON系统的具体局限性，但其“未来工作”部分暗示了当前版本仍有改进空间，例如数据覆盖范围、模型容量、训练策略和基础设施的进一步优化。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>数据质量和扩展</strong>：计划通过扩展数据覆盖范围（包括图像、文本、视频、音频和ID信号）、应用基于大模型的样本合成和特征增强来进一步提升多模态表示学习。
*   <strong>训练范式</strong>：未来工作将探索更丰富的多阶段和多任务训练策略，整合专家混合（mix-of-experts）和跨模态注意力架构，以改进细粒度对齐、模型容量和泛化能力，同时缓解多模态联合训练中的不稳定性。
*   <strong>基础设施</strong>：将推进统一的稀疏-密集架构（如RecIS），以加速10B+规模模型的训练和推理，提高吞吐量和效率。下游稀疏模型将针对长序列（100K+）用户建模和多模态表示的更好利用进行优化。
*   <strong>应用</strong>：除了CTR预测，多模态表示将扩展到推荐的其他阶段，如检索、相关性、战略机制和商家推广。</p>
<p>总而言之，MOON提供了一套全面且可持续的多模态表示学习实践，通过创新的三阶段训练范式、关键中间指标指导和强大的基础设施，在电子商务搜索广告领域取得了显著的性能提升，并为未来的研究和应用奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.11305v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.11305v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-17 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
