<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-15 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-14/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-16/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-15">Arxiv Computer Vision Papers - 2026-01-15</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#step3-vl-10b-technical-report" class="nav-link">STEP3-VL-10B Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#fast-thinkact-efficient-vision-language-action-reasoning-via-verbalizable-latent-planning" class="nav-link">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</a>
                </li>
                <li class="nav-item">
                    <a href="#efficient-camera-controlled-video-generation-of-static-scenes-via-sparse-diffusion-and-3d-rendering" class="nav-link">Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</a>
                </li>
                <li class="nav-item">
                    <a href="#sce-slam-scale-consistent-monocular-slam-via-scene-coordinate-embeddings" class="nav-link">SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings</a>
                </li>
                <li class="nav-item">
                    <a href="#self-supervised-animal-identification-for-long-videos" class="nav-link">Self-Supervised Animal Identification for Long Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#liteembed-adapting-clip-to-rare-classes" class="nav-link">LiteEmbed: Adapting CLIP to Rare Classes</a>
                </li>
                <li class="nav-item">
                    <a href="#identifying-models-behind-text-to-image-leaderboards" class="nav-link">Identifying Models Behind Text-to-Image Leaderboards</a>
                </li>
                <li class="nav-item">
                    <a href="#cograil-benchmarking-vlms-in-cognitive-intrusion-perception-for-intelligent-railway-transportation-systems" class="nav-link">CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems</a>
                </li>
                <li class="nav-item">
                    <a href="#sim2real-image-translation-enables-viewpoint-robust-policies-from-fixed-camera-datasets" class="nav-link">Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-signal-processing-for-thermo-visible-lidar-fusion-in-real-time-3d-semantic-mapping" class="nav-link">Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-15">Arxiv Computer Vision Papers - 2026-01-15</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对您提供的 Arxiv 计算机视觉论文列表的简明执行摘要，旨在帮助忙碌的研究人员快速了解该领域的最新进展。</p>
<hr />
<p><strong>Arxiv 计算机视觉论文每日报告 - 执行摘要 (2026-01-14)</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>多模态理解与生成</strong>、<strong>高效模型设计</strong>以及<strong>真实世界应用落地</strong>方面的显著进展。特别值得关注的是，<strong>视觉-语言-动作（VLA）的联合推理</strong>和<strong>视频生成</strong>技术正在快速发展，同时<strong>SLAM（同步定位与地图构建）</strong>在单目和多模态融合方面也取得了新的突破。此外，<strong>自监督学习</strong>在处理长视频和识别特定对象方面展现出强大潜力，而<strong>模型适应性与鲁棒性</strong>（如针对稀有类别和跨域迁移）也是研究的热点。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>STEP3-VL-10B Technical Report</strong> 似乎是本期中一个具有里程碑意义的工作，其庞大的模型规模（10B）预示着在视觉-语言理解能力上可能达到新的高度。</li>
<li><strong>Fast-ThinkAct</strong> 在视觉-语言-动作推理方面提出了高效的解决方案，通过“可言语化的潜在规划”来提升效率，这对于需要复杂交互的机器人和智能体至关重要。</li>
<li><strong>Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</strong> 结合了扩散模型和3D渲染技术，为静态场景的视频生成提供了更高效且可控的途径。</li>
<li><strong>SCE-SLAM</strong> 在单目 SLAM 领域提出了“场景坐标嵌入”的概念，旨在实现尺度一致性，这对于提升单目 SLAM 的精度和鲁棒性具有重要意义。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>高效多模态推理：</strong> 以 Fast-ThinkAct 为代表，研究者们正积极探索如何在保证性能的同时，大幅提升视觉-语言-动作等多种模态联合推理的效率。</li>
<li><strong>可控视频生成：</strong> 结合扩散模型与3D渲染技术，实现对视频内容（如相机视角、场景动态）的精细控制，是视频生成领域的新趋势。</li>
<li><strong>自监督学习在长视频分析中的应用：</strong> Xuyang Fang 等人的工作表明，自监督学习能够有效地从长视频中提取有用的信息，用于动物识别等任务，这为处理大规模视频数据提供了新的思路。</li>
<li><strong>模型适应性与泛化能力：</strong> LiteEmbed 针对 CLIP 模型在稀有类别上的局限性提出改进，以及 Sim2real Image Translation 的研究，都指向了提升模型在不同领域和数据分布下的泛化能力。</li>
<li><strong>多模态融合在特定场景下的应用：</strong> CogRail 和 Multimodal Signal Processing for Thermo-Visible-Lidar Fusion 都展示了将多种传感器数据（如文本、热成像、LiDAR）融合，以解决特定领域（如铁路交通、3D语义地图构建）问题的潜力。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的影响力和技术创新性，以下论文值得优先阅读全文：</p>
<ol>
<li><strong>STEP3-VL-10B Technical Report:</strong> 了解其大规模模型架构和在视觉-语言任务上的表现。</li>
<li><strong>Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning:</strong> 深入理解其高效推理机制和潜在规划方法。</li>
<li><strong>Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering:</strong> 学习其结合扩散模型与3D渲染的视频生成新范式。</li>
<li><strong>SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings:</strong> 探索其在单目 SLAM 尺度一致性方面的新颖方法。</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.09668v1">STEP3-VL-10B Technical Report</a></li>
<li><a href="#2601.09708v1">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</a></li>
<li><a href="#2601.09697v1">Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</a></li>
<li><a href="#2601.09665v1">SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings</a></li>
<li><a href="#2601.09663v1">Self-Supervised Animal Identification for Long Videos</a></li>
<li><a href="#2601.09661v1">LiteEmbed: Adapting CLIP to Rare Classes</a></li>
<li><a href="#2601.09647v1">Identifying Models Behind Text-to-Image Leaderboards</a></li>
<li><a href="#2601.09613v1">CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems</a></li>
<li><a href="#2601.09605v1">Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets</a></li>
<li><a href="#2601.09578v1">Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.09668v1'></a></p>
<h2 id="step3-vl-10b-technical-report"><a href="https://arxiv.org/abs/2601.09668v1">STEP3-VL-10B Technical Report</a></h2>
<p><strong>Authors:</strong> Ailin Huang, Chengyuan Yao, Chunrui Han, Fanqi Wan, Hangyu Guo, Haoran Lv, Hongyu Zhou, Jia Wang, Jian Zhou, Jianjian Sun, Jingcheng Hu, Kangheng Lin, Liang Zhao, Mitt Huang, Song Yuan, Wenwen Qu, Xiangfeng Wang, Yanlin Lai, Yingxiu Zhao, Yinmin Zhang, Yukang Shi, Yuyang Chen, Zejia Weng, Ziyang Meng, Ang Li, Aobo Kong, Bo Dong, Changyi Wan, David Wang, Di Qi, Dingming Li, En Yu, Guopeng Li, Haiquan Yin, Han Zhou, Hanshan Zhang, Haolong Yan, Hebin Zhou, Hongbo Peng, Jiaran Zhang, Jiashu Lv, Jiayi Fu, Jie Cheng, Jie Zhou, Jisheng Yin, Jingjing Xie, Jingwei Wu, Jun Zhang, Junfeng Liu, Kaijun Tan, Kaiwen Yan, Liangyu Chen, Lina Chen, Mingliang Li, Qian Zhao, Quan Sun, Shaoliang Pang, Shengjie Fan, Shijie Shang, Siyuan Zhang, Tianhao You, Wei Ji, Wuxun Xie, Xiaobo Yang, Xiaojie Hou, Xiaoran Jiao, Xiaoxiao Ren, Xiangwen Kong, Xin Huang, Xin Wu, Xing Chen, Xinran Wang, Xuelin Zhang, Yana Wei, Yang Li, Yanming Xu, Yeqing Shen, Yuang Peng, Yue Peng, Yu Zhou, Yusheng Li, Yuxiang Yang, Yuyang Zhang, Zhe Xie, Zhewei Huang, Zhenyi Lu, Zhimin Fan, Zihui Cheng, Daxin Jiang, Qi Han, Xiangyu Zhang, Yibo Zhu, Zheng Ge</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10<script type="math/tex">\times</script>-20<script type="math/tex">\times</script> larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对您提供的“STEP3-VL-10B Technical Report”论文的中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文摘要：STEP3-VL-10B Technical Report</strong></p>
<p><strong>1. 主要问题与研究目标</strong></p>
<p>该论文旨在解决当前多模态大语言模型（MLLMs）在追求前沿智能时，往往伴随着巨大的计算需求和部署门槛的问题。研究的核心目标是开发一款<strong>轻量级、开源的 foundation model（基础模型）</strong>，能够在<strong>紧凑的参数规模（10B）下，实现与更大模型相媲美甚至超越的通用多模态智能</strong>，特别是在<strong>视觉感知、复杂推理和人类对齐</strong>方面，从而重新定义模型效率与智能水平之间的权衡。</p>
<p><strong>2. 关键创新与方法贡献</strong></p>
<p>STEP3-VL-10B 的成功主要归功于两个核心策略：</p>
<ul>
<li>
<p><strong>统一、全量解冻的预训练策略 (Unified, Fully Unfrozen Pre-training Strategy)</strong>：</p>
<ul>
<li>在 <strong>1.2T 个多模态 token</strong> 上进行单阶段、全量解冻的预训练。</li>
<li>集成了<strong>语言对齐的感知编码器 (Perception Encoder)</strong> 和 <strong>Qwen3-8B 解码器</strong>，旨在建立<strong>内在的视觉-语言协同</strong>，提升模型对视觉信息的理解和语言生成能力。</li>
<li>预训练数据覆盖了<strong>知识、教育、光学字符识别 (OCR)、图形用户界面 (GUI)</strong> 等多个关键领域，确保模型具备广泛的感知和推理基础。</li>
</ul>
</li>
<li>
<p><strong>规模化的后训练流水线与并行推理 (Scaled Post-training Pipeline &amp; Parallel Reasoning)</strong>：</p>
<ul>
<li>通过<strong>两阶段的监督微调 (SFT)</strong> 和<strong>超过 1000 次的强化学习 (RL)</strong>（包括 RLVR 和 RLHF）进行精细化训练。</li>
<li>引入了<strong>并行协调推理 (PaCoRe)</strong> 技术，该技术在<strong>测试时（test-time）</strong>动态分配计算资源，<strong>并行探索和综合多种视觉假设</strong>，以解决复杂感知和推理任务，有效<strong>弥合了小型模型在推理和感知能力上的差距</strong>。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>性能卓越</strong>：尽管参数量仅为 10B，STEP3-VL-10B 在多项基准测试中表现出色，<strong>超越了同等规模（7B-10B）的开源模型，并能与 10-20 倍更大的模型（如 GLM-4.6V-106B, Qwen3-VL-235B）以及顶级的闭源模型（如 Gemini 2.5 Pro, Seed-1.5-VL）相媲美甚至超越</strong>。<ul>
<li>在 <strong>MM-Bench</strong> 上达到 <strong>92.2%</strong>，在 <strong>MMMU</strong> 上达到 <strong>80.11%</strong>，展现了强大的通用多模态理解能力。</li>
<li>在复杂推理任务上表现突出，<strong>AIME2025</strong> 达到 <strong>94.43%</strong>，<strong>MathVision</strong> 达到 <strong>75.95%</strong>。</li>
</ul>
</li>
<li><strong>效率与可复现性</strong>：STEP3-VL-10B 在保持紧凑模型尺寸的同时，实现了<strong>前沿水平的多模态智能</strong>，打破了“轻量级模型即受限”的传统认知。</li>
<li><strong>开源贡献</strong>：论文<strong>公开发布了完整的模型权重和详细的训练文档</strong>，为社区提供了一个<strong>强大、高效且可复现的基线模型</strong>，极大地推动了多模态 AI 的发展。</li>
<li><strong>PaCoRe 的有效性</strong>：PaCoRe 技术在测试时通过并行推理显著提升了模型在<strong>密集推理和感知密集型任务</strong>上的表现，尤其是在<strong>空间理解、OCR 和计数</strong>等领域。</li>
</ul>
<p><strong>4. 局限性</strong></p>
<p>论文中并未明确列出模型的局限性，但从其研究方向和未来工作展望中可以推断：</p>
<ul>
<li><strong>计算密度与物理接地</strong>：论文提到“计算密度和物理接地”是当前面临的挑战，暗示模型在处理需要极高计算资源或与物理世界深度交互的任务时仍有提升空间。</li>
<li><strong>“现实差距” (Reality Gap)</strong>：模型在数字任务上表现优异，但与物理世界的交互和理解仍是“关键前沿”，需要进一步探索。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<p>论文提出了几个重要的未来研究方向：</p>
<ul>
<li><strong>最大化 Token 效率与通用 RL 扩展</strong>：将计算资源更多地投入到 RL 阶段，通过深度（顺序推理）和宽度（并行探索）的扩展，挖掘更高价值的多模态感知和推理模式。</li>
<li><strong>优化推理密度</strong>：旨在<strong>内化（internalize）并行探索的优势</strong>，压缩推理路径，将复杂的“慢思考”转化为高效的“系统 1”式直觉响应。</li>
<li><strong>弥合“现实差距”</strong>：<ul>
<li><strong>从语义到物理世界模型</strong>：将多模态合成扩展到<strong>视频轨迹和传感器-动作序列</strong>，构建能够理解物理因果关系和时空动态的<strong>整体世界模型</strong>。</li>
<li><strong>物理作为终极验证器</strong>：利用<strong>高保真模拟环境</strong>，将学习范式从依赖代理标签转向<strong>基于物理定律的交互式掌握</strong>。</li>
<li><strong>具身链式思考 (Embodied Chain-of-Thought, E-CoT)</strong>：通过显式建模<strong>时间动态和物理状态转换</strong>，使模型能够进行<strong>长时序规划</strong>，实现动态开放世界中的鲁棒性。</li>
</ul>
</li>
</ul>
<p>总而言之，STEP3-VL-10B 的发布标志着在构建高效、强大且开源的多模态基础模型方面取得了重要进展，其创新的预训练和后训练策略，特别是 PaCoRe 技术，为未来多模态 AI 的发展开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09668v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09668v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09708v1'></a></p>
<h2 id="fast-thinkact-efficient-vision-language-action-reasoning-via-verbalizable-latent-planning"><a href="https://arxiv.org/abs/2601.09708v1">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</a></h2>
<p><strong>Authors:</strong> Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Fast-ThinkAct: 高效的视觉-语言-动作推理通过可言语化的潜在规划</p>
<p><strong>作者：</strong> Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决视觉-语言-动作（VLA）任务中的一个关键挑战：如何在复杂动态环境中实现高效且高性能的推理和动作执行。现有的基于显式链式思考（CoT）的VLA模型虽然能提升泛化能力，但由于推理过程冗长，导致推理延迟过高，这严重阻碍了其在需要实时响应的具身AI应用中的部署。因此，研究的核心问题是如何在保持推理能力的同时，显著降低推理延迟，实现紧凑且高效的规划。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
Fast-ThinkAct 提出了一种高效的推理框架，其核心创新在于<strong>可言语化的潜在推理（verbalizable latent reasoning）</strong>。主要贡献包括：</p>
<ul>
<li><strong>紧凑的潜在CoT蒸馏：</strong> 通过“教师-学生”模型，将教师模型（Textual Teacher）生成的冗长文本CoT蒸馏到学生模型（Latent Student）的紧凑连续潜在空间中。</li>
<li><strong>偏好引导的蒸馏（Preference-Guided Distillation）：</strong> 利用教师模型的奖励信号，通过偏好学习框架，指导学生模型学习高质量的推理模式，同时抑制低质量的模式。</li>
<li><strong>动作对齐的视觉规划蒸馏（Action-Aligned Visual Plan Distillation）：</strong> 引入了将教师模型的视觉规划能力转移到学生模型中的机制，通过对齐轨迹级别的表示，确保潜在表征能够捕捉具身控制所需的空间规划能力。</li>
<li><strong>可言语化潜在表征：</strong> 学生模型生成的潜在表征可以通过一个“言语化器”（Verbalizer）转换为文本，这不仅有助于理解，也为训练过程提供了额外的监督信号。</li>
<li><strong>推理增强的策略学习：</strong> 将学习到的紧凑潜在推理表征与动作模型相结合，实现了从高层推理到低层动作执行的有效桥接。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
Fast-ThinkAct 在多个具身操作和推理基准测试中取得了显著的成果：</p>
<ul>
<li><strong>推理效率大幅提升：</strong> 与最先进的推理VLA模型相比，Fast-ThinkAct 的推理延迟降低了高达 89.3%，显著解决了现有方法的瓶颈问题。</li>
<li><strong>性能保持甚至提升：</strong> 在大幅降低延迟的同时，Fast-ThinkAct 保持了甚至超越了现有方法的任务成功率，证明了其紧凑推理的有效性。</li>
<li><strong>长时序规划能力：</strong> 在需要多步推理和长时序规划的任务中表现出色，如 RoboTwin2.0 的长时序任务。</li>
<li><strong>少样本适应能力：</strong> 在仅使用少量演示数据进行微调时，Fast-ThinkAct 能够显著提升性能，展现了其良好的少样本适应能力。</li>
<li><strong>故障恢复能力：</strong> 在 RoboFAC 等基准测试中，Fast-ThinkAct 能够有效地识别和分析操作失败的原因，并提出恢复策略，显示了其对复杂场景的理解和处理能力。</li>
</ul>
<p>这些结果表明，Fast-ThinkAct 是一种高效且强大的VLA推理框架，能够有效解决现有方法的局限性，并在具身AI领域具有重要的应用潜力。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>言语化器的局限性：</strong> 作者提到，由于言语化器（Verbalizer）是基于预训练的LLM构建的，它可能继承LLM的局限性，例如产生幻觉，生成看似合理但实际上不准确的描述。然而，作者强调这在推理阶段并不影响动作执行，因为动作预测使用的是从视觉规划蒸馏中获得的、经过接地（grounded）的潜在表征。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提升言语化推理的忠实度：</strong> 为了进一步提高言语化推理的准确性，未来的工作可以考虑引入<strong>接地感知目标（grounding-aware objectives）</strong>或<strong>幻觉抑制技术（hallucination suppression techniques）</strong>。</p>
<p>总而言之，Fast-ThinkAct 提出了一种创新的方法，通过将冗长的文本推理压缩到紧凑的、可言语化的潜在表征中，显著提高了VLA任务的推理效率，同时保持了强大的性能，为具身AI在实时、复杂环境中的应用开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning.</li>
<li>Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09708v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09708v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09697v1'></a></p>
<h2 id="efficient-camera-controlled-video-generation-of-static-scenes-via-sparse-diffusion-and-3d-rendering"><a href="https://arxiv.org/abs/2601.09697v1">Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</a></h2>
<p><strong>Authors:</strong> Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering (通过稀疏扩散和3D渲染实现高效的相机控制静态场景视频生成)</p>
<p><strong>作者：</strong> Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前先进的生成式视频模型（如基于扩散的模型）虽然能生成高度逼真的视频片段，但存在严重的计算效率问题。生成几秒钟的视频可能需要数分钟的GPU时间，这极大地阻碍了其在需要实时交互的应用（如具身AI、VR/AR）中的部署。论文旨在解决这一效率瓶颈，探索一种更高效的相机控制静态场景视频生成方法。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
该论文提出了一种名为 <strong>SRENDER</strong> 的新颖框架，其核心思想是<strong>将视频生成过程分解为两个阶段：稀疏关键帧生成和3D重建与渲染</strong>。
*   <strong>稀疏关键帧生成：</strong> 利用扩散模型生成一个稀疏的关键帧集合，而不是为视频中的每一帧都进行扩散生成。这显著减少了扩散模型的调用次数。
*   <strong>自适应关键帧密度预测：</strong> 引入了一个关键帧密度预测模型，该模型能够根据给定的相机轨迹分析场景的复杂性，并自适应地决定生成多少关键帧。对于简单的相机运动，生成较少的关键帧；对于复杂的运动，则生成更多的关键帧，以确保3D重建的完整性。
*   <strong>3D重建与渲染：</strong> 利用生成的稀疏关键帧，通过先进的3D重建技术（如3D高斯溅射 AnySplat）来构建场景的3D表示。然后，利用这个3D模型沿着目标相机轨迹高效地渲染出完整的、几何一致的视频。这种方法将生成成本分摊到数百帧上，并利用了3D场景的内在结构。
*   <strong>时间分块（Temporal Chunking）：</strong> 对于长视频或复杂场景，为了解决长距离相机运动可能导致的生成关键帧不一致问题，论文采用了时间分块策略。将长视频分割成较短的时间段，为每个时间段独立进行3D重建，然后将这些重建结果进行对齐，以生成全局一致的视频。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>显著的效率提升：</strong> SRENDER 在生成20秒视频时，比基于扩散的基线方法（如HG）快 <strong>40倍以上</strong>。在DL3DV数据集上，该方法实现了<strong>实时性能</strong>，平均生成20秒30fps的视频仅需16.21秒。
*   <strong>高质量的视频生成：</strong> 在提高效率的同时，SRENDER 保持了<strong>可比甚至更好的视觉质量和时间稳定性</strong>。它避免了纯扩散模型可能出现的高频伪影，并提供了更稳定的几何结构。
*   <strong>相机控制能力：</strong> 该方法能够精确控制视频的相机视角，并且在生成3D模型后，可以<strong>在秒级内渲染出具有不同相机轨迹的新视频</strong>，这是纯扩散模型无法比拟的。
*   <strong>意义：</strong> SRENDER 提供了一条<strong>高效且可控的视频合成实用路径</strong>，为将生成式视频技术应用于实时交互场景（如具身AI、VR/AR）打开了大门。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>静态场景限制：</strong> 该方法目前仅适用于<strong>静态场景</strong>，不处理场景中的物体运动或形变。
*   <strong>高频细节的权衡：</strong> 3D渲染的视频可能比纯扩散模型生成的视频<strong>略微平滑，细节可能稍少</strong>。然而，这换来了更好的几何一致性和避免了扩散模型的伪影。
*   <strong>长视频的挑战：</strong> 对于非常长的视频，即使有时间分块，也可能需要仔细调整参数以确保全局一致性。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>动态场景生成：</strong> 将SRENDER的核心思想（稀疏生成、3D重建、自适应采样）扩展到<strong>动态场景</strong>的生成，这是一个重要的前沿研究方向。
*   <strong>提高3D重建的细节：</strong> 随着3D重建技术的不断进步，未来有望进一步提升渲染视频的视觉保真度，使其在细节上更接近纯扩散模型。
*   <strong>更复杂的场景和轨迹：</strong> 探索在更复杂、更具挑战性的场景和相机轨迹下，如何进一步优化关键帧选择和3D重建的鲁棒性。</p>
<p>总而言之，SRENDER 通过巧妙地结合稀疏扩散生成、自适应关键帧选择和高效的3D重建渲染，成功地解决了现有生成式视频模型在效率上的关键瓶颈，为实现实时、可控的视频生成提供了开创性的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering.</li>
<li>By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09697v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09697v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09665v1'></a></p>
<h2 id="sce-slam-scale-consistent-monocular-slam-via-scene-coordinate-embeddings"><a href="https://arxiv.org/abs/2601.09665v1">SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings</a></h2>
<p><strong>Authors:</strong> Yuchen Wu, Jiahe Li, Xiaohan Yu, Lina Yu, Jin Zheng, Xiao Bai</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SCE-SLAM: 通过场景坐标嵌入实现尺度一致的单目 SLAM</p>
<p><strong>作者：</strong> Yuchen Wu, Jiahe Li, Xiaohan Yu, Lina Yu, Jin Zheng, Xiao Bai</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
单目视觉 SLAM（Simultaneous Localization and Mapping）在资源受限平台上的 3D 重建和自主导航中至关重要，但面临一个根本性挑战：尺度漂移（scale drift）。由于缺乏全局约束，现有的逐帧 SLAM 方法在长时间序列中估计的尺度会逐渐发散，导致地图碎片化和长期建图的不可靠性。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决尺度漂移问题，本文提出了 <strong>SCE-SLAM</strong>，一个端到端的 SLAM 系统，通过引入 <strong>场景坐标嵌入（Scene Coordinate Embeddings）</strong> 来维持尺度一致性。这些嵌入是学习到的、在规范尺度参考下的 3D 几何关系补丁级表示。SCE-SLAM 的核心贡献在于其两个协同模块：</p>
<ul>
<li><strong>几何引导尺度传播（Geometry-Guided Scale Propagation）：</strong> 该模块利用 3D 空间邻近性，通过几何调制注意力机制，从历史观测中传播尺度信息。它通过选择可靠的参考补丁，并结合特征相似性和 3D 空间距离来聚合尺度信息，从而避免了计算成本过高和几何不相关的关联。</li>
<li><strong>场景坐标捆绑调整（Scene Coordinate Bundle Adjustment）：</strong> 该模块将场景坐标嵌入解码为尺度锚定的 3D 坐标预测，并利用这些显式的 3D 坐标约束来增强传统的重投影优化。这有助于将当前估计值锚定到规范尺度参考，形成一个持续的反馈循环，不断强化尺度一致性。</li>
</ul>
<p>SCE-SLAM 采用双分支架构，一个分支用于像素级光流约束，另一个分支（包含上述两个模块）负责维护全局尺度一致性。</p>
<p><strong>3. 主要结果与意义：</strong>
在 KITTI、Waymo 和 vKITTI 数据集上的实验表明，SCE-SLAM 取得了显著的性能提升：</p>
<ul>
<li><strong>精度提升：</strong> 在 KITTI 数据集上，与现有最佳方法相比，绝对轨迹误差（ATE）降低了 8.36m。</li>
<li><strong>尺度一致性：</strong> 论文通过可视化展示了 SCE-SLAM 在长序列中能够保持高度的尺度一致性（颜色一致），而其他逐帧方法则出现明显的尺度漂移（颜色变化）。这直接验证了其核心贡献的有效性。</li>
<li><strong>实时性能：</strong> 该方法在保持高精度的同时，实现了 36 FPS 的实时性能，与现有的逐帧方法相当。</li>
<li><strong>鲁棒性：</strong> 在复杂的城市场景（Waymo 数据集）和多样的天气条件（vKITTI 数据集）下，SCE-SLAM 均表现出优越的性能和鲁棒性。</li>
<li><strong>可靠的闭环：</strong> 在具有挑战性的 4Seasons 数据集上，SCE-SLAM 成功实现了尺度一致的闭环，而其他方法则因尺度累积误差而失败。</li>
</ul>
<p>SCE-SLAM 的意义在于，它在不依赖外部度量深度先验或全局优化的情况下，通过学习到的内部表示实现了单目 SLAM 的尺度一致性，为资源受限的自主系统和大规模 3D 重建提供了更可靠的解决方案。</p>
<p><strong>4. 提及的局限性：</strong>
论文中并未明确列出局限性，但从其方法和实验设置来看，可以推断出一些潜在的方面：</p>
<ul>
<li><strong>对初始尺度的依赖：</strong> 虽然系统旨在维持尺度一致性，但初始尺度的准确性仍然对整体性能有影响。论文中提到“两阶段引导初始化策略”来解决这个问题，但初始阶段的性能仍是关键。</li>
<li><strong>计算成本：</strong> 尽管实现了实时性能，但与最简单的光流方法相比，SCE-SLAM 的计算量仍然更高，尤其是在特征提取和场景坐标分支上。</li>
<li><strong>对特征匹配的依赖：</strong> 系统的性能在很大程度上依赖于 DINOv3 等预训练模型的特征提取能力，以及 SuperPoint 等方法提供的可靠关键点。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于本文的研究，以下是一些潜在的未来研究方向：</p>
<ul>
<li><strong>更强的初始化策略：</strong> 探索更鲁棒、更快速的尺度初始化方法，以进一步提高系统在各种场景下的适应性。</li>
<li><strong>多模态融合：</strong> 将 SCE-SLAM 的尺度一致性机制与 IMU 或其他传感器信息融合，以构建更全面的视觉-惯性 SLAM 系统。</li>
<li><strong>动态场景处理：</strong> 扩展该方法以处理动态物体，并保持动态物体和静态场景的尺度一致性。</li>
<li><strong>更高效的嵌入表示：</strong> 研究更紧凑、更高效的场景坐标嵌入表示，以进一步降低计算开销并提高内存效率。</li>
<li><strong>大规模场景的长期一致性：</strong> 探索如何进一步增强系统在超大规模、长期运行场景下的尺度和几何一致性，例如在城市规模的地图构建中。</li>
</ul>
<p>总而言之，SCE-SLAM 是一项重要的工作，它通过创新的场景坐标嵌入和协同模块，成功解决了单目 SLAM 中的尺度漂移难题，在精度、鲁棒性和实时性方面取得了显著的平衡，为单目 SLAM 的发展开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference.</li>
<li>Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09665v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09665v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09663v1'></a></p>
<h2 id="self-supervised-animal-identification-for-long-videos"><a href="https://arxiv.org/abs/2601.09663v1">Self-Supervised Animal Identification for Long Videos</a></h2>
<p><strong>Authors:</strong> Xuyang Fang, Sion Hannuna, Edwin Simpson, Neill Campbell</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy (<script type="math/tex">></script>97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Self-Supervised Animal Identification for Long Videos”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Self-Supervised Animal Identification for Long Videos (面向长视频的自监督动物个体识别)</p>
<p><strong>作者：</strong> Xuyang Fang, Sion Hannuna, Edwin Simpson, Neill Campbell (布里斯托大学)</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决在长时程视频中准确识别个体动物的核心挑战。这对于行为生态学、野生动物监测和畜牧管理至关重要。传统方法依赖于耗时且昂贵的人工标注，而现有的自监督方法则面临计算需求高、内存限制大以及长序列中时间误差累积等问题，使其难以应用于实际场景。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
该研究提出了一种高效的自监督方法，将动物个体识别重塑为一个<strong>全局聚类任务</strong>，而非传统的序列跟踪问题。其核心创新点包括：
*   <strong>假设与重构问题：</strong> 假设视频中存在已知且固定的个体数量（这是实际场景中的常见情况），仅需目标的边界框检测和总数信息，无需任何身份标签。
*   <strong>高效的自监督学习框架：</strong>
    *   <strong>帧对采样与增强：</strong> 从视频中采样帧对，并进行数据增强，生成多个视图。
    *   <strong>冻结预训练骨干网络：</strong> 利用预训练的视觉骨干网络提取特征，并将其冻结，仅训练一个轻量级的投影头，大幅降低内存消耗。
    *   <strong>自举式伪标签生成：</strong> 利用<strong>匈牙利算法</strong>在批次内动态分配正样本对（基于特征相似度），生成伪标签来指导表示学习。
    *   <strong>简化的损失函数：</strong> 采用从视觉-语言模型（如SigLIP）改编的<strong>二元交叉熵（BCE）损失</strong>，或监督对比损失（SupCon），简化了超参数搜索并降低了计算复杂度。
*   <strong>内存效率：</strong> 通过精巧的批次构建（仅采样两帧）、优化的增强策略和冻结骨干网络，实现了<strong>每批次低于1GB的GPU内存占用</strong>，比标准对比学习方法低一个数量级。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>高性能：</strong> 在具有挑战性的真实世界数据集（如3D-POP鸽子和8头牛喂食视频）上，该方法实现了<strong>超过97%的准确率</strong>，与在超过1000个标注帧上训练的监督基线方法相匹配或超越。
*   <strong>显著的内存效率：</strong> 相比于SimCLR和MoCo等方法需要超过10GB的内存，该方法将内存占用降低到1GB以下。
*   <strong>消除人工标注瓶颈：</strong> 该方法无需任何真实身份标签即可实现高精度识别，极大地<strong>消除了对人工标注的依赖</strong>，为资源受限的研究场景提供了实际可行的解决方案。
*   <strong>易于部署：</strong> 低内存需求使其能够<strong>在消费级硬件上运行</strong>，提高了可访问性。
*   <strong>克服时间误差累积：</strong> 作为全局聚类任务，避免了传统序列跟踪方法中常见的帧间误差累积问题，在长视频中表现出鲁棒性。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>固定个体数量假设：</strong> 该方法依赖于视频中个体数量已知的假设。对于个体数量未知或动态变化的“开放世界”场景，该方法可能不适用。
*   <strong>需要边界框检测：</strong> 方法需要预先提取的边界框信息作为输入。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>动态场景扩展：</strong> 将框架扩展到更动态的设置，例如个体数量未知（开放世界）的场景。
*   <strong>整合时间一致性：</strong> 引入时间一致性模型，以进一步优化非常长序列中的特征表示。
*   <strong>跨领域应用：</strong> 将这种资源高效、特定任务的自监督学习方法推广到其他数据稀缺且计算资源有限的领域。</p>
<p>总而言之，这篇论文提出了一种创新的自监督学习方法，通过将动物个体识别转化为全局聚类任务，并采用内存高效的设计和自举式伪标签生成机制，在不依赖人工标注的情况下，实现了在长视频中高精度的个体识别，显著降低了计算和内存需求，为实际应用和资源受限的研究场景带来了重要价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem.</li>
<li>Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count.</li>
<li>By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels.</li>
<li>We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy (<script type="math/tex">></script>97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09663v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09663v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09661v1'></a></p>
<h2 id="liteembed-adapting-clip-to-rare-classes"><a href="https://arxiv.org/abs/2601.09661v1">LiteEmbed: Adapting CLIP to Rare Classes</a></h2>
<p><strong>Authors:</strong> Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并提供以下中文解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了一种名为 LiteEmbed 的轻量级框架，旨在解决大型视觉-语言模型（如 CLIP）在处理罕见类别时性能下降的问题。LiteEmbed 通过在不重新训练 CLIP 编码器的情况下，对文本嵌入进行子空间引导优化，从而有效地将新类别添加到模型中，实现了对 CLIP 的个性化适配。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<p>LiteEmbed 的核心创新在于其<strong>子空间引导优化（subspace-guided optimization）</strong>的文本嵌入方法。具体来说，它利用了基于 PCA（主成分分析）的分解技术，将文本嵌入的语义信息解耦为<strong>粗粒度语义方向（coarse semantic directions）</strong>和<strong>细粒度变化（fine-grained variations）</strong>。</p>
<p>论文提出了两个互补的优化目标：</p>
<ul>
<li><strong>粗粒度对齐（coarse alignment）</strong>：旨在保持全局语义的一致性，确保新类别与现有语义空间保持合理的关联。</li>
<li><strong>细粒度分离（fine separation）</strong>：旨在增强视觉上相似的类别之间的区分度，使得模型能够更好地识别细微的差异。</li>
</ul>
<p>通过联合优化这两个目标，LiteEmbed 能够在不破坏 CLIP 原有强大语义理解能力的前提下，有效地提升模型对罕见类别的识别能力。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>LiteEmbed 的提出对计算机视觉领域具有重要的潜在影响，主要体现在以下几个方面：</p>
<ul>
<li><strong>提升模型的泛化能力和鲁棒性</strong>：解决了大型预训练模型在面对长尾分布数据（即罕见类别）时的固有缺陷，使得模型能够更好地适应真实世界中不均衡的数据分布。</li>
<li><strong>降低模型适配成本</strong>：通过“即插即用”（plug-and-play）的方式，无需重新训练庞大的编码器，大大降低了将新类别集成到现有模型中的计算成本和时间成本。</li>
<li><strong>促进模型的个性化和定制化</strong>：使得研究人员和开发者能够更容易地为特定领域或特定应用场景定制模型，例如针对特定文化背景下的物品识别，或新兴技术领域的概念识别。</li>
<li><strong>推动零样本/少样本学习的发展</strong>：为在数据稀缺的情况下实现更有效的类别识别提供了新的思路和工具，进一步推动了零样本和少样本学习的研究进展。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<p>LiteEmbed 的研究成果可以广泛应用于以下领域：</p>
<ul>
<li><strong>罕见物品识别</strong>：例如，在特定行业的专业设备识别、稀有动植物分类、古董鉴定等场景。</li>
<li><strong>新兴实体识别</strong>：随着科技和社会的发展，不断涌现新的概念和实体，LiteEmbed 可以帮助模型快速适应这些新类别。</li>
<li><strong>文化特定内容理解</strong>：例如，识别特定文化习俗、艺术品、节日相关的物品等，这些类别在通用预训练数据中可能非常罕见。</li>
<li><strong>个性化推荐系统</strong>：为用户提供更精准的个性化内容推荐，即使是用户感兴趣的、相对小众的物品。</li>
<li><strong>医疗影像分析</strong>：识别罕见的疾病或病变，这些在医学影像数据集中通常是长尾类别。</li>
<li><strong>自动驾驶中的特殊场景识别</strong>：例如，识别道路上罕见的障碍物或特殊交通标志。</li>
<li><strong>内容审核与安全</strong>：识别和过滤掉不常见但有害的内容。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管 LiteEmbed 展现出强大的潜力，但从摘要中可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对“罕见”的定义和度量</strong>：摘要中提到“rare classes”，但“罕见”的程度和定义可能对方法的有效性产生影响。如果一个类别的样本数量极其稀少，甚至少于“few-shot”的范畴，其效果可能需要进一步验证。</li>
<li><strong>PCA 分解的假设</strong>：PCA 分解假设数据存在线性结构，并且主成分能够有效捕捉语义信息。对于高度非线性的语义空间，PCA 的效果可能受到限制。</li>
<li><strong>“粗粒度”与“细粒度”的平衡</strong>：虽然论文提出了两个互补的目标，但如何精确地平衡“粗粒度对齐”和“细粒度分离”以达到最佳效果，可能需要精细的超参数调整。</li>
<li><strong>对 CLIP 架构的依赖</strong>：LiteEmbed 是一个适配 CLIP 的框架，其效果可能在很大程度上依赖于 CLIP 本身的预训练质量和语义表示能力。对于其他视觉-语言模型，可能需要进行相应的调整。</li>
<li><strong>计算成本的相对性</strong>：虽然 LiteEmbed 避免了重新训练整个编码器，但“子空间引导优化”本身仍然需要一定的计算资源，尤其是在处理大量新类别时。其“轻量级”是相对于完全重新训练而言的，而非零计算成本。</li>
<li><strong>潜在的语义漂移</strong>：在优化文本嵌入时，虽然目标是增强区分度，但仍存在一定风险导致新类别嵌入在语义空间中发生一定程度的漂移，从而影响与其他类别的整体关系。</li>
</ul>
<p>总而言之，LiteEmbed 是一项非常有前景的研究，它提供了一种高效且经济的方式来扩展大型视觉-语言模型的能力，使其能够更好地处理现实世界中普遍存在的长尾数据问题。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories.</li>
<li>We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09661v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09661v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09647v1'></a></p>
<h2 id="identifying-models-behind-text-to-image-leaderboards"><a href="https://arxiv.org/abs/2601.09647v1">Identifying Models Behind Text-to-Image Leaderboards</a></h2>
<p><strong>Authors:</strong> Ali Naseh, Yuefeng Peng, Anshuman Suri, Harsh Chaudhari, Alina Oprea, Amir Houmansadr</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV, cs.CR, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Identifying Models Behind Text-to-Image Leaderboards”的全面中文摘要，重点关注其研究问题、方法、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Identifying Models Behind Text-to-Image Leaderboards (识别文本到图像排行榜背后的模型)</p>
<p><strong>作者：</strong> Ali Naseh, Yuefeng Peng, Anshuman Suri, Harsh Chaudhari, Alina Oprea, Amir Houmansadr</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文的核心研究问题在于，当前文本到图像（T2I）模型排行榜普遍采用匿名化模型输出来确保公平性，但这种匿名化是否足够安全？作者们发现，这种匿名性很容易被打破，从而对排行榜的公平性和可信度构成了根本性威胁。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>模型生成模式的独特性：</strong> 作者们的核心发现是，不同T2I模型在生成相同文本提示（prompt）的图像时，会展现出系统性的、模型特有的“签名”或模式。这些模式体现在图像的风格、构图、细节等方面，即使在相同的提示下，同一模型生成的图像之间变异性较低（低<strong>模型内变异</strong>），而不同模型生成的图像之间变异性较高（高<strong>模型间变异</strong>）。</li>
<li><strong>基于嵌入空间的聚类分析：</strong> 作者们利用先进的图像编码器（如CLIP）将生成的图像映射到嵌入空间。在这个空间中，来自同一模型的图像会形成紧密且可分离的簇，而不同模型的簇则相对分离。</li>
<li><strong>质心（Centroid）为基础的去匿名化方法：</strong> 作者们提出了一种无需提示控制或训练数据（<strong>无监督、无提示</strong>）的去匿名化方法。该方法通过为每个模型生成参考图像，计算其在嵌入空间中的质心，然后将排行榜上的未知图像的嵌入与其质心进行比对，从而推断出生成该图像的模型。</li>
<li><strong>提示级可区分性（Distinguishability）指标：</strong> 为了量化哪些提示更容易暴露模型的独特签名，作者们引入了一个“提示级可区分性”指标。该指标衡量在给定提示下，不同模型生成的图像在嵌入空间中的分离程度，从而识别出对去匿名化最有利的提示。</li>
</ul>
<p><strong>3. 主要研究结果及其意义：</strong></p>
<ul>
<li><strong>高精度去匿名化：</strong> 作者们使用22个T2I模型和280个提示（约15万张图像）进行了实验，证明了其基于质心的去匿名化方法能够达到非常高的准确率（例如，在标准设置下，Top-1准确率高达91%）。即使在更具挑战性的“一对多”场景下（即只知道目标模型），准确率也接近完美（99.16%）。</li>
<li><strong>模型签名普遍存在：</strong> 研究表明，模型特有的生成签名是普遍存在的，并且可以被有效利用进行去匿名化。</li>
<li><strong>提示的重要性：</strong> “提示级可区分性”分析揭示了某些提示（如“油画”、“动漫肖像”）比其他提示（如“城市街道”、“风景”）更能暴露模型的独特性，从而使去匿名化更容易。</li>
<li><strong>对排行榜的根本性安全威胁：</strong> 这些发现揭示了当前T2I排行榜在模型匿名性方面的根本性安全漏洞。这意味着恶意行为者可以通过去匿名化模型来操纵排行榜的排名，从而影响模型的声誉和发展。</li>
<li><strong>现有基线方法的局限性：</strong> 与作者提出的方法相比，传统的基于指纹识别和监督学习的分类方法在泛化能力和准确率上表现较差，尤其是在面对未见过（unseen）的提示时。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>对抗性后处理的有效性有限：</strong> 虽然作者们提出了一种对抗性后处理方法来扰乱图像的嵌入，以增加去匿名化的难度，但实验表明，即使有这种防御，攻击者仍然可以达到相当高的准确率。此外，这种后处理可能会引入可见的视觉伪影，影响图像质量。</li>
<li><strong>成本问题：</strong> 虽然作者们指出，去匿名化单个排行榜图像的成本相对较低（约1.08美元），但对于大规模组织或有协调的对手来说，这仍然是可以接受的。</li>
<li><strong>防御的权衡：</strong> 论文提到，任何防御措施都会在公平性、可用性和透明度之间产生权衡。例如，限制使用高区分度的提示会影响排名的意义。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更强的匿名化防御机制：</strong> 需要开发更鲁棒、更不易被逆转的匿名化技术，以应对不断演进的去匿名化攻击。</li>
<li><strong>排行榜设计和评估协议的改进：</strong> 鼓励排行榜运营商采用更安全的模型匿名化和评估协议，以确保公平性和可信度。</li>
<li><strong>区分度指标的应用：</strong> 利用“提示级可区分性”指标来识别和过滤那些容易导致模型暴露的提示，从而提高排行榜的安全性。</li>
<li><strong>对其他生成模型类型的研究：</strong> 将此研究扩展到其他类型的生成模型（如文本生成模型、音频生成模型等），以评估其匿名性的脆弱性。</li>
<li><strong>理解模型签名产生的根源：</strong> 深入研究模型训练数据、架构和参数等因素如何具体影响生成图像的独特签名。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文是一项重要的研究，它揭示了当前文本到图像排行榜在模型匿名性方面存在的严重安全漏洞。作者们通过创新的基于嵌入空间聚类和质心的方法，证明了去匿名化T2I模型生成图像的容易程度，并提出了一个量化提示区分度的指标。这项工作不仅对T2I排行榜的公平性提出了质疑，也为未来更安全、更可信的AI模型评估和排行榜设计提供了重要的启示和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we show that such anonymity can be easily broken.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09647v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09613v1'></a></p>
<h2 id="cograil-benchmarking-vlms-in-cognitive-intrusion-perception-for-intelligent-railway-transportation-systems"><a href="https://arxiv.org/abs/2601.09613v1">CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems</a></h2>
<p><strong>Authors:</strong> Yonglin Tian, Qiyao Zhang, Wei Xu, Yutong Wang, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang, Zhiyong Cui, Baoqing Guo, Zujun Yu, Yisheng Lv</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems”的全面中文摘要：</p>
<p><strong>论文摘要：CogRail：为智能铁路交通系统中的认知入侵感知基准化视觉语言模型</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
论文旨在解决当前铁路交通系统中<strong>早期、准确感知潜在入侵目标</strong>的难题。现有系统多侧重于固定视域内的物体识别，并依赖规则判断入侵状态，常常忽略那些具有潜在入侵风险的目标。要实现对潜在入侵风险的预判，需要对目标物体（OOI）的空间上下文和时间动态进行认知，这对传统的视觉模型提出了巨大挑战。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>CogRail 基准数据集：</strong> 作者提出了一个新颖的基准数据集 CogRail，该数据集整合了精选的开源数据集，并加入了认知驱动的问答标注，以支持时空推理和预测。CogRail 包含三个核心任务：<strong>位置感知（RailPos）</strong>、<strong>运动状态预测（RailMove）</strong>和<strong>威胁等级分析（RailThreat）</strong>。
*   <strong>RailGPT 框架：</strong> 基于 CogRail 基准，作者提出了 RailGPT，一个<strong>基于智能体的多模态框架</strong>，能够支持各种视觉语言模型（VLMs）。该框架包含三个组件：
    *   <strong>多模态提示（Multimodal Prompting）：</strong> 用于将感知和语义信息进行关联，引导模型理解场景。
    *   <strong>智能体构建（Agent Construction）：</strong> 为位置、运动和威胁评估任务分别设计了专门的智能体。
    *   <strong>联合微调（Joint Fine-tuning）：</strong> 提出了一种<strong>多任务联合微调框架</strong>，整合了三个核心任务，以实现通用基础模型向特定领域模型的有效适应。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>SOTA VLMs 的评估：</strong> 作者对多种先进的视觉语言模型（VLMs）进行了系统性评估，揭示了它们在认知入侵感知任务上的优势和局限性。实验表明，当前大型多模态模型在处理复杂的时空推理方面存在困难，凸显了现有基础模型在这一安全关键领域的不足。
*   <strong>联合微调的有效性：</strong> 提出的<strong>多任务联合微调框架</strong>显著提升了模型性能。通过针对领域特定推理需求进行定向适应，该框架在提高准确性和可解释性方面展现了结构化多任务学习的优势。实验结果表明，联合微调在所有子任务（RailPos, RailMove, RailThreat）上都带来了显著的性能提升。
*   <strong>代码开源：</strong> 作者承诺将提供代码，以便社区进行进一步的研究和开发。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>现有基础模型的局限性：</strong> 实验结果表明，当前大型多模态模型在处理复杂的时空推理方面存在不足，难以满足安全关键的铁路入侵感知任务需求。
*   <strong>模型性能的波动：</strong> 尽管整体性能有所提升，但作者也观察到了一些“反直觉”的案例，例如某些模型在特定任务配置下性能下降。这可能归因于统一的微调策略（包括超参数和数据划分）并非对所有模型架构都最优。
*   <strong>单帧输入的挑战：</strong> 仅基于单帧图像进行运动推理和威胁估计在认知上是模糊且具有挑战性的，这凸显了对更丰富的上下文建模和时间理解的需求。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>长期推理和鲁棒性：</strong> 未来工作将探索<strong>长时序推理</strong>以及在<strong>复杂操作环境下的鲁棒性适应</strong>。
*   <strong>更精细的上下文建模：</strong> 进一步研究如何整合更丰富的上下文信息，以提高模型的认知能力。
*   <strong>多模态融合的深化：</strong> 探索更有效的多模态融合策略，以应对铁路安全场景的复杂性。</p>
<p><strong>论文的创新性和重要性：</strong>
这篇论文的重要贡献在于<strong>首次提出了 CogRail 基准数据集</strong>，为铁路入侵感知领域提供了一个标准化的评估平台，特别关注了时空推理和威胁预测等关键但被忽视的方面。同时，<strong>RailGPT 框架及其提出的多任务联合微调方法</strong>，有效地解决了现有 VLM 在该领域面临的挑战，展示了通过领域特定微调和结构化学习来提升模型性能和可解释性的巨大潜力。这为构建更智能、更安全的铁路交通系统奠定了重要基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction.</li>
<li>Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09613v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09613v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09605v1'></a></p>
<h2 id="sim2real-image-translation-enables-viewpoint-robust-policies-from-fixed-camera-datasets"><a href="https://arxiv.org/abs/2601.09605v1">Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets</a></h2>
<p><strong>Authors:</strong> Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\% on views that the non-augmented policy fails completely on.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets</p>
<p><strong>作者：</strong> Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira</p>
<p><strong>摘要：</strong></p>
<p>这篇论文旨在解决机器人操作领域中，基于视觉的策略在面对相机视角变化时鲁棒性不足的问题。现有机器人操作数据集通常缺乏视角多样性，而模拟环境虽然可以生成大量不同视角的演示数据，但存在“模拟到真实”（sim2real）的视觉鸿沟。为了弥合这一差距，作者提出了 <strong>MANGO</strong>（Multiview Augmentation with Novel Generated Observations）方法，一种新颖的非配对图像翻译技术。</p>
<p><strong>1. 研究问题：</strong></p>
<ul>
<li>机器人操作策略对相机视角变化非常敏感，导致在部署时性能急剧下降。</li>
<li>真实的机器人演示数据稀缺且视角多样性不足。</li>
<li>利用模拟数据进行训练时，如何有效地跨越“模拟到真实”的视觉鸿沟，并生成具有多样化视角的逼真图像。</li>
</ul>
<p><strong>2. 主要创新和方法贡献：</strong></p>
<p>MANGO 的核心贡献在于其新颖的图像翻译模型，它结合了以下关键要素：</p>
<ul>
<li><strong>分割条件化 InfoNCE 损失 (Segmentation-conditioned InfoNCE loss)：</strong> 这是一种新颖的对比学习损失，利用模拟数据中的分割信息来指导图像翻译，确保翻译后的图像能够保留原始模拟场景的结构和物体关系，从而维持视角一致性。</li>
<li><strong>高度正则化的判别器设计 (Highly-regularized discriminator design)：</strong> 通过随机采样局部图像块并进行旋转，MANGO 的判别器能够更好地学习目标域（真实世界）的风格，同时避免对重复性背景细节的过度记忆。</li>
<li><strong>改进的 PatchNCE 损失 (Modified PatchNCE loss)：</strong> 对原始 PatchNCE 损失进行了修改，以处理机器人数据集中的相似图像块和纹理问题，减少了错误负样本的影响。</li>
<li><strong>轻量级 GAN 架构：</strong> MANGO 使用生成对抗网络 (GAN) 架构，相比于扩散模型，其训练和推理速度更快，更适合处理大规模机器人数据集的增强需求。</li>
</ul>
<p>MANGO 的训练仅需少量固定视角的真实世界数据，但能够将模拟数据翻译成具有多样化、逼真视角的新视角图像。</p>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>图像翻译性能：</strong> 在“抓取可乐”任务的测试集中，MANGO 在随机视角测试集上取得了最低的 FID 分数，优于包括 CUT 和 CycleGAN 在内的多种图像翻译基线方法。</li>
<li><strong>策略鲁棒性提升：</strong> 使用 MANGO 生成的合成数据增强的模仿学习策略，在面对未见过的相机视角时，成功率显著提高，甚至能达到在未增强策略完全失败的视角下获得 60% 的成功率。</li>
<li><strong>效率优势：</strong> MANGO 的训练和数据生成过程比 state-of-the-art 的扩散模型（如 ZeroNVS）快约 2700 倍，使其在实际应用中更具可行性。</li>
<li><strong>跨领域迁移能力：</strong> MANGO 在模拟到模拟 (sim2sim) 的实验中也表现出色，生成的翻译图像在视觉质量和策略成功率上均优于其他方法。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li>MANGO 仍然需要少量目标域（真实世界）的固定视角数据进行训练。</li>
<li>在某些真实世界任务的评估中，MANGO 的性能略逊于 VISTA（一个使用了大型预训练模型的模型），尤其是在处理未见过的相机视角时。</li>
<li>MANGO 的优势在于其轻量级和高效性，但大型预训练模型可能在理解更复杂的 3D 几何和场景方面具有优势。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li>将 MANGO 的新颖损失函数（特别是分割条件化 InfoNCE 损失）集成到更大型的预训练模型中，以进一步提升 sim2real 视觉观测翻译或 real2real 数据增强的效果。</li>
<li>探索 MANGO 在更广泛的机器人操作任务和更复杂的场景中的应用。</li>
<li>研究如何进一步减少对真实世界数据的依赖，甚至实现零样本的 sim2real 翻译。</li>
</ul>
<p>总而言之，MANGO 是一项重要的工作，它通过创新的图像翻译技术，有效地解决了机器人操作中相机视角变化带来的鲁棒性问题，为利用模拟数据提升真实世界机器人策略的性能提供了一种高效且实用的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss.</li>
<li>When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations.</li>
<li>In this domain, MANGO outperforms all other image translation methods we tested.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09605v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09605v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.09578v1'></a></p>
<h2 id="multimodal-signal-processing-for-thermo-visible-lidar-fusion-in-real-time-3d-semantic-mapping"><a href="https://arxiv.org/abs/2601.09578v1">Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping</a></h2>
<p><strong>Authors:</strong> Jiajun Sun, Yangyi Ou, Haoyuan Zheng, Chao yang, Yue Ma</p>
<p><strong>Published:</strong> 2026-01-14</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面摘要。</p>
<p><strong>论文题目：</strong> Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping</p>
<p><strong>作者：</strong> Jiajun Sun, Yangyi Ou, Haoyuan Zheng, Chao Yang, Yue Ma</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>在复杂的现实环境中，自主机器人导航和环境感知对同步定位与地图构建（SLAM）技术提出了更高的要求。传统的3D点云地图主要依赖于几何信息，缺乏对环境的语义理解，尤其是在识别和定位高温目标方面存在不足。现有的2D热成像技术受环境因素影响大，且难以提供精确的几何参数和结构性缺陷的量化信息。因此，该研究旨在解决如何将热成像信息与可见光和LiDAR数据有效融合，以构建具有高精度几何信息和丰富语义（特别是温度信息）的实时3D地图，从而提升环境感知的鲁棒性和准确性。</p>
<p><strong>2. 关键创新点/方法学贡献：</strong></p>
<ul>
<li><strong>三模态融合框架：</strong> 提出了一种新颖的框架，整合了LiDAR（空间信息）、可见光相机（纹理信息）和热红外相机（温度信息），实现了多模态信号的融合。</li>
<li><strong>像素级可见光-热红外图像融合：</strong> 通过像素级融合可见光和红外图像，生成包含温度信息的复合纹理图像。</li>
<li><strong>LiDAR点云与融合图像的投影：</strong> 将实时LiDAR点云投影到融合后的图像流上，实现几何信息与温度纹理的对齐。</li>
<li><strong>热源特征分割与语义增强：</strong> 在热红外通道中分割热源特征，识别高温目标，并将温度信息作为语义层叠加到最终的3D地图上。</li>
<li><strong>目标无关的外部校准方法：</strong> 提出了一种无需特定目标即可进行LiDAR与相机之间外部参数校准的方法，并进行了基准验证。</li>
<li><strong>实时性与鲁棒性：</strong> 系统设计旨在实现实时性能，并通过多模态冗余来确保在传感器失效情况下的鲁棒性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>高精度语义3D地图：</strong> 成功构建了包含精确几何、真实纹理和温度语义的3D点云地图。</li>
<li><strong>实时高温目标检测与定位：</strong> 能够实时识别和精确定位环境中的高温目标，并将其空间化到3D地图中。</li>
<li><strong>环境理解的提升：</strong> 显著增强了对环境的理解能力，能够捕捉动态热演变和识别细微的温度差异，克服了传统2D热成像的局限性。</li>
<li><strong>应用价值：</strong> 该方法在快速灾害评估和工业预防性维护等领域具有重要的应用价值，能够提供更全面、更准确的环境信息。</li>
<li><strong>实验验证：</strong> 在大学体育场和教学楼等实际场景中进行了实验，证明了系统在不同光照和环境条件下的稳定性和有效性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>环境依赖性：</strong> 尽管系统具有一定的鲁棒性，但热成像的检测效果仍可能受到环境条件（如温度梯度变化、阳光强度、风速等）的影响。</li>
<li><strong>复杂环境的扩展性：</strong> 论文提到未来工作将扩展到更复杂的环境，暗示当前系统在极端复杂场景下的表现可能需要进一步优化。</li>
<li><strong>机器学习的集成：</strong> 目前的缺陷分类主要依赖于热成像特征，未来计划集成机器学习方法进行更高级的缺陷分类，表明当前系统在自动化缺陷分类方面仍有提升空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到更复杂的环境：</strong> 将系统应用于更具挑战性的环境，如室内复杂结构、动态变化的工业场景等。</li>
<li><strong>机器学习驱动的缺陷分类：</strong> 集成机器学习算法，实现更智能、更自动化的缺陷检测和分类。</li>
<li><strong>大规模基础设施监测：</strong> 将该框架应用于大规模基础设施的长期监测和健康评估。</li>
<li><strong>多传感器融合的进一步优化：</strong> 探索更先进的多传感器融合技术，以提高精度、鲁棒性和效率。</li>
</ul>
<hr />
<p><strong>总结：</strong></p>
<p>这篇论文提出了一种创新的多模态SLAM框架，通过将LiDAR、可见光和热红外传感器的数据进行深度融合，实现了实时3D语义地图的构建，尤其是在识别和定位高温目标方面取得了显著进展。该方法克服了传统3D地图缺乏温度语义以及2D热成像的局限性，为机器人环境感知和特定应用（如灾害评估、工业维护）提供了强大的技术支持。其关键贡献在于像素级图像融合、点云投影以及将温度信息作为语义层集成到3D地图中，展现了在提升环境理解和缺陷检测方面的巨大潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.09578v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.09578v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-15 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
