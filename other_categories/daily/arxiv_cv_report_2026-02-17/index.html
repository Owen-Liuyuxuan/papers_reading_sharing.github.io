<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-02-17 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-13
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-02-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-19
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-02-13/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-02-18/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-02-17">Arxiv Computer Vision Papers - 2026-02-17</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#advances-in-global-solvers-for-3d-vision" class="nav-link">Advances in Global Solvers for 3D Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#editctrl-disentangled-local-and-global-control-for-real-time-generative-video-editing" class="nav-link">EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#editctrl" class="nav-link">论文方法分析与总结：EditCtrl</a>
                </li>
                <li class="nav-item">
                    <a href="#image-generation-with-a-sphere-encoder" class="nav-link">Image Generation with a Sphere Encoder</a>
                </li>
                <li class="nav-item">
                    <a href="#neurosim-a-fast-simulator-for-neuromorphic-robot-perception" class="nav-link">Neurosim: A Fast Simulator for Neuromorphic Robot Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#neurosimcortex" class="nav-link">论文方法分析与总结：Neurosim与Cortex</a>
                </li>
                <li class="nav-item">
                    <a href="#thermeval-a-structured-benchmark-for-evaluation-of-vision-language-models-on-thermal-imagery" class="nav-link">ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery</a>
                </li>
                <li class="nav-item">
                    <a href="#_1" class="nav-link">论文方法分析与总结</a>
                </li>
                <li class="nav-item">
                    <a href="#dm0-an-embodied-native-vision-language-action-model-towards-physical-ai" class="nav-link">DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI</a>
                </li>
                <li class="nav-item">
                    <a href="#pact-part-decomposed-single-view-articulated-object-generation" class="nav-link">PAct: Part-Decomposed Single-View Articulated Object Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#pact-part-decomposed-single-view-articulated-object-generation_1" class="nav-link">论文方法分析与总结：PAct: Part-Decomposed Single-View Articulated Object Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#anchorweave-world-consistent-video-generation-with-retrieved-local-spatial-memories" class="nav-link">AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories</a>
                </li>
                <li class="nav-item">
                    <a href="#web-scale-multimodal-summarization-using-clip-based-semantic-alignment" class="nav-link">Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment</a>
                </li>
                <li class="nav-item">
                    <a href="#_2" class="nav-link">论文方法分析与总结</a>
                </li>
                <li class="nav-item">
                    <a href="#integrating-affordances-and-attention-models-for-short-term-object-interaction-anticipation" class="nav-link">Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-02-17">Arxiv Computer Vision Papers - 2026-02-17</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对您提供的 Arxiv 计算机视觉论文的简明执行摘要，旨在帮助忙碌的研究人员快速了解该领域的最新进展。</p>
<hr />
<p><strong>执行摘要：2026年2月16日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>日期：</strong> 2026年2月16日</p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>生成模型、多模态理解、机器人感知与交互</strong>等前沿领域。特别值得注意的是，研究人员在<strong>实时视频编辑、三维视觉的全局求解、以及具身智能（Embodied AI）</strong>方面取得了显著进展。此外，对<strong>热成像数据、物体交互预测</strong>等特定应用场景的关注也在增加。</p>
<p><strong>亮点论文与创新：</strong></p>
<ul>
<li><strong>EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing</strong> 提出了一种新颖的视频编辑方法，实现了<strong>局部与全局控制的分离</strong>，并在<strong>实时性</strong>上取得了突破，为视频内容创作提供了强大工具。</li>
<li><strong>DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI</strong> 标志着<strong>具身智能</strong>领域的重要一步，该模型原生支持视觉、语言和动作的融合，为构建更具物理交互能力的 AI 系统奠定了基础。</li>
<li><strong>AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories</strong> 在<strong>视频生成</strong>方面展现了创新，通过检索局部空间记忆来确保<strong>世界一致性</strong>，有望生成更连贯、逼真的长时序视频。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>具身智能（Embodied AI）的融合模型：</strong> DM0 的出现表明，将视觉、语言和动作紧密集成的模型将是未来具身智能研究的关键。</li>
<li><strong>实时、可控的生成式视频编辑：</strong> EditCtrl 的成功预示着对更精细、更高效的视频生成与编辑技术的需求日益增长。</li>
<li><strong>三维视觉的全局求解：</strong> "Advances in Global Solvers for 3D Vision" 指出，在复杂三维场景理解方面，全局优化方法仍是重要的研究方向。</li>
<li><strong>多模态理解的特定领域应用：</strong> ThermEval 的提出，强调了针对特定模态（如热成像）进行视觉-语言模型评估的重要性，预示着多模态模型将在更多专业领域得到应用。</li>
</ul>
<p><strong>建议阅读的论文：</strong></p>
<p>基于其创新性和对未来研究方向的潜在影响，以下论文值得深入阅读：</p>
<ol>
<li><strong>EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing</strong> (视频生成与编辑的实时性与可控性突破)</li>
<li><strong>DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI</strong> (具身智能领域的重要进展)</li>
<li><strong>AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories</strong> (高质量、世界一致性视频生成的新方法)</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2602.14662v1">Advances in Global Solvers for 3D Vision</a></li>
<li><a href="#2602.15031v1">EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing</a></li>
<li><a href="#2602.15030v1">Image Generation with a Sphere Encoder</a></li>
<li><a href="#2602.15018v1">Neurosim: A Fast Simulator for Neuromorphic Robot Perception</a></li>
<li><a href="#2602.14989v1">ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery</a></li>
<li><a href="#2602.14974v1">DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI</a></li>
<li><a href="#2602.14965v1">PAct: Part-Decomposed Single-View Articulated Object Generation</a></li>
<li><a href="#2602.14941v1">AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories</a></li>
<li><a href="#2602.14889v1">Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment</a></li>
<li><a href="#2602.14837v1">Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2602.14662v1'></a></p>
<h2 id="advances-in-global-solvers-for-3d-vision"><a href="https://arxiv.org/abs/2602.14662v1">Advances in Global Solvers for 3D Vision</a></h2>
<p><strong>Authors:</strong> Zhenjun Zhao, Heng Yang, Bangyan Liao, Yingping Zeng, Shaocheng Yan, Yingdong Gu, Peidong Liu, Yi Zhou, Haoang Li, Javier Civera</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域专业高水平研究生，专注于深入分析论文的方法部分，并重点关注其创新之处。请提供您希望我分析的论文。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.14662v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.14662v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.15031v1'></a></p>
<h2 id="editctrl-disentangled-local-and-global-control-for-real-time-generative-video-editing"><a href="https://arxiv.org/abs/2602.15031v1">EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing</a></h2>
<p><strong>Authors:</strong> Yehonathan Litman, Shikun Liu, Dario Seyb, Nicholas Milef, Yang Zhou, Carl Marshall, Shubham Tulsiani, Caleb Leak</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇关于“EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing”的论文，重点关注其方法创新点、设计逻辑、优势与不足，并提供实用的分析和借鉴。</p>
<hr />
<h2 id="editctrl">论文方法分析与总结：EditCtrl</h2>
<h3 id="1">1. 摘要翻译</h3>
<p><strong>EditCtrl：解耦局部与全局控制，实现实时生成式视频编辑</strong></p>
<p>高保真生成式视频编辑已通过利用预训练视频基础模型取得了显著的质量提升。然而，其计算成本是一个主要瓶颈，因为它们通常被设计为低效地处理整个视频上下文，即使是对于稀疏、局部编辑。在本文中，我们提出了 EditCtrl，一个高效的视频修复控制框架，它将计算集中在需要的地方。我们的方法包含一个新颖的局部视频上下文模块，该模块仅在被遮蔽的 token 上操作，从而产生与编辑大小成比例的计算成本。这种局部优先的生成然后由一个轻量级的时域全局上下文嵌入器指导，该嵌入器以最小的开销确保视频范围的上下文一致性。EditCtrl 不仅比最先进的生成式编辑方法在计算效率上高出 10 倍，甚至在编辑质量上优于那些设计有全注意力的方法。最后，我们展示了 EditCtrl 如何解锁新功能，包括带有文本提示的多区域编辑和自回归内容传播。</p>
<h3 id="2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>提升视频编辑效率</strong>：现有的生成式视频编辑方法，尤其是基于大型预训练视频基础模型的方法，计算成本高昂，难以满足实时应用的需求。</li>
<li><strong>实现高质量、上下文一致的编辑</strong>：在局部编辑时，仍需保证全局视频的连贯性和一致性。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>计算成本高昂</strong>：现有方法通常处理整个视频的全部时空上下文，即使是针对视频中很小的编辑区域，也造成了巨大的计算浪费。这使得实时应用（如 AR）和高分辨率视频编辑变得不可行。</li>
<li><strong>效率与质量的权衡</strong>：为了提高效率，一些方法可能牺牲了编辑质量或上下文一致性。</li>
<li><strong>缺乏灵活性</strong>：难以同时处理多个不连续的编辑区域，或在视频帧不可用的情况下进行内容传播。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>视频编辑的计算成本可以与编辑区域的大小解耦，而不是与视频的整体分辨率或长度成正比。</li>
<li>通过将局部编辑的计算与全局视频上下文的轻量级表示分开，可以在不牺牲质量的情况下显著提高效率。</li>
<li>一个解耦的框架可以更容易地支持多区域编辑和内容传播等高级功能。</li>
</ul>
</li>
</ul>
<h3 id="3">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：</p>
<p>EditCtrl 的核心思想是将视频编辑任务分解为两个主要部分：<strong>局部编辑</strong>和<strong>全局上下文感知</strong>。其pipeline可以概括为：</p>
<ol>
<li>
<p><strong>输入处理与上下文提取</strong>：</p>
<ul>
<li><strong>源视频 (Vsrc)</strong>：输入待编辑的视频。</li>
<li><strong>编辑掩码 (Vm)</strong>：用户定义的掩码，指示需要编辑的区域。</li>
<li><strong>背景视频 (Vb)</strong>：通过将源视频中被掩码区域的像素值设置为一个固定值（例如 0.5）来生成。这使得 Vb 仅包含背景信息，为后续的全局上下文提取做准备。</li>
<li><strong>局部上下文 (Clocal)</strong>：<ul>
<li>将编辑掩码 Vm 降采样到潜在空间分辨率。</li>
<li>将 Vm 应用于源视频的潜在表示（通过 VAE 编码得到）中，以提取出仅包含目标编辑区域的 token。这些 token 构成了局部上下文 Clocal。</li>
<li>为了更好地融合周围信息，掩码 Vm 会进行膨胀处理，以包含邻近的背景像素。</li>
</ul>
</li>
<li><strong>全局上下文 (Zb)</strong>：<ul>
<li>将背景视频 Vb 通过 VAE 编码器 E 得到其潜在表示。</li>
<li>将 Vb 的潜在表示进行空间降采样（例如到 256x256），以增加对长宽比和帧数的鲁棒性，并生成全局上下文表示 Zb。</li>
<li>Zb 经过一个可训练的 patch 层，生成全局上下文 token 嵌入。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>解耦的编辑架构</strong>：</p>
<ul>
<li><strong>基础视频扩散模型 (Frozen)</strong>：使用一个预训练的、具有全注意力机制的视频扩散模型（如 DiT），但其权重被冻结。</li>
<li><strong>局部上下文编码器 (Cp)</strong>：<ul>
<li>这是一个可训练的模块，负责处理局部上下文 Clocal。</li>
<li>它被设计成仅在被遮蔽的 token 上操作，从而实现计算成本与编辑区域大小成比例。</li>
<li>为了适应局部编辑，该模块被<strong>微调</strong>，使用一个<strong>掩码感知损失函数</strong> (Lp)，以确保生成的局部内容与文本提示对齐，同时避免过度依赖提示而导致不自然的生成。</li>
</ul>
</li>
<li><strong>全局上下文嵌入器 (Gy)</strong>：<ul>
<li>这是一个新提出的、轻量级的模块，负责整合全局视频上下文。</li>
<li>它通过<strong>交叉注意力机制</strong>将全局上下文 token 嵌入（来自 Zb）注入到基础视频扩散模型的交叉注意力层中。</li>
<li>Gy 接收查询 token (Q) 和全局特征键值 (Kg, Vg)，并计算注意力权重，将全局信息融入到特征中。</li>
<li>这个模块的目的是确保局部编辑与整个视频在外观、场景线索、光照、结构、动态和相机运动等方面保持一致性，而<strong>无需处理整个视频的全部时空信息</strong>。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>条件扩散过程</strong>：</p>
<ul>
<li>基础视频扩散模型在去噪过程中，同时接收文本提示 (p)、局部上下文编码器 Cp 的输出以及全局上下文嵌入器 Gy 的输出。</li>
<li>条件扩散损失函数 Lψ 结合了局部和全局的控制信息。</li>
<li><strong>分段训练策略</strong>：为了稳定训练，作者采用了分段训练策略，先训练局部编码器 Cp，再训练全局嵌入器 Gy，或者反之，通过一个迭代计数器 n 来切换损失函数。</li>
</ul>
</li>
<li>
<p><strong>输出生成与后处理</strong>：</p>
<ul>
<li>扩散模型在局部编辑区域生成去噪后的 token。</li>
<li>这些 token 被<strong>散射</strong>回原始视频的潜在空间中，与背景信息融合，形成最终的编辑视频潜在表示。</li>
<li>通过 VAE 解码器 D，将潜在表示解码为最终的编辑视频。</li>
</ul>
</li>
</ol>
<p><strong>模型结构</strong>：</p>
<ul>
<li><strong>VAE (Encoder E, Decoder D)</strong>：用于将视频压缩到低维潜在空间，降低计算复杂度。</li>
<li><strong>基础视频扩散模型 (DiT)</strong>：一个预训练的、冻结权重的模型，提供强大的生成能力。</li>
<li><strong>局部上下文编码器 (Cp)</strong>：一个可训练的适配器模块，专注于处理被遮蔽区域的 token。</li>
<li><strong>全局上下文嵌入器 (Gy)</strong>：一个新提出的轻量级模块，通过交叉注意力整合全局视频信息。</li>
<li><strong>控制模块</strong>：Cp 和 Gy 共同构成了控制模块，用于引导冻结的基础扩散模型。</li>
</ul>
<p><strong>算法解释</strong>：</p>
<ul>
<li><strong>损失函数 LDM (Eq. 1)</strong>：标准的潜在扩散模型损失函数，用于预测噪声。</li>
<li><strong>条件扩散损失函数 LCDM (Eq. 2)</strong>：在 LDM 的基础上，加入了条件信息 C（包括编辑掩码和背景信息），用于引导生成。</li>
<li><strong>局部损失函数 Lp (Eq. 3)</strong>：专门针对局部编辑设计的损失函数，使用掩码 Vm 来约束损失计算只在目标区域。</li>
<li><strong>全局与局部联合损失函数 Lψ (Eq. 5)</strong>：结合了局部和全局的控制信息，用于训练整个 EditCtrl 模型。</li>
<li><strong>分段训练损失函数 L (Eq. 6)</strong>：通过在训练的不同阶段切换 Lp 和 Lψ，来稳定训练过程。</li>
</ul>
<h3 id="4">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>计算效率</strong>：EditCtrl 的核心在于将计算<strong>解耦</strong>为局部和全局两部分，并使局部编辑的计算量与编辑区域大小成正比，而大多数现有方法（如 VACE）仍然依赖于全注意力机制，处理整个视频的上下文，导致计算成本与视频分辨率/长度高度相关。</li>
<li><strong>控制机制</strong>：EditCtrl 引入了专门的<strong>局部上下文编码器</strong>和<strong>轻量级全局上下文嵌入器</strong>，分别处理局部细节和全局一致性，而许多方法可能将这两者混合处理或仅依赖于全局上下文。</li>
<li><strong>模型适配</strong>：EditCtrl 在冻结的基础扩散模型上添加了<strong>可训练的适配器模块</strong>（Cp 和 Gy），避免了对整个基础模型进行微调，保留了预训练模型的强大生成先验，并提高了效率。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>解耦的局部/全局控制</strong>：首次提出将视频编辑的局部细节生成与全局上下文感知进行解耦，并设计了相应的模块。</li>
<li><strong>高效的局部上下文处理</strong>：通过仅在被遮蔽 token 上操作，实现了计算成本与编辑区域大小的线性关系。</li>
<li><strong>轻量级全局上下文嵌入器</strong>：设计了一个高效的模块来整合全局视频信息，确保编辑的一致性，而无需昂贵的计算。</li>
<li><strong>支持多区域编辑和内容传播</strong>：解耦的设计自然地支持了这些高级功能，这是全注意力方法难以实现的。</li>
<li><strong>实时性能</strong>：通过显著降低计算成本，实现了接近实时的视频编辑。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>实时视频编辑</strong>：如 AR 应用中的实时内容修改。</li>
<li><strong>高分辨率视频编辑</strong>：由于计算成本与分辨率解耦，可以处理更高分辨率的视频。</li>
<li><strong>局部、稀疏的视频编辑</strong>：如移除小物体、改变局部颜色、添加小元素等。</li>
<li><strong>多区域同时编辑</strong>：可以同时处理视频中多个不连续的编辑区域。</li>
<li><strong>视频内容传播</strong>：在未来帧不可用时，能够将编辑内容传播到后续帧。</li>
</ul>
</li>
</ul>
<h3 id="5">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：VPBench-Edit, DAVIS, VPBench-Inp 等标准视频编辑和修复数据集。</li>
<li><strong>对比方法</strong>：ReVideo, VideoPainter, VACE 等先进的视频编辑和修复方法。</li>
<li><strong>评估指标</strong>：<ul>
<li><strong>Masked Region Preservation</strong>：PSNR, SSIM, LPIPS, MSE, MAE (衡量背景区域的保持程度)。</li>
<li><strong>Text Alignment</strong>：CLIP, CLIP (M) (衡量编辑内容与文本提示的匹配度)。</li>
<li><strong>Temp. Coherence</strong>：CLIP Sim (衡量时间连贯性)。</li>
<li><strong>Throughput</strong>：FPS (衡量推理速度)。</li>
</ul>
</li>
<li><strong>消融实验</strong>：通过移除或替换局部编码器和全局嵌入器来验证各模块的贡献。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>效率提升</strong>：EditCtrl 在计算效率上比生成式基线方法高出 <strong>50x</strong> 以上，并且在 4K 视频编辑中实现了 <strong>10x</strong> 的加速。</li>
<li><strong>质量匹配/超越</strong>：在编辑质量、背景保持和文本对齐方面，EditCtrl 匹配甚至略微优于全注意力基线模型。</li>
<li><strong>多区域编辑和内容传播</strong>：在实验中展示了这些功能的有效性，如图 1、图 7、图 8 所示。</li>
<li><strong>消融实验</strong>：移除全局嵌入器 (Gy) 会导致质量下降，而仅使用局部编码器 (Cp) 可能导致过度拟合提示。两者结合才能达到最佳效果。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li>在需要<strong>高效率</strong>和<strong>实时性</strong>的场景下表现突出，如 AR 应用。</li>
<li>在处理<strong>局部、小范围编辑</strong>时，其效率优势尤为明显。</li>
<li>在需要<strong>多区域同时编辑</strong>或<strong>内容传播</strong>的复杂场景下，其解耦设计提供了天然的优势。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>VAE 的背景退化</strong>：论文提到 VAE 可能导致背景上下文的显著退化。</li>
<li><strong>快速运动视频的挑战</strong>：在处理快速运动的视频时，局部编码器可能遇到困难，这可能与 VAE 的限制以及时空局部上下文的快速变化有关。</li>
<li><strong>4K 视频的 VRAM 限制</strong>：对于 4K 视频，VAE 的编码/解码开销会成为瓶颈，需要分块处理以应对 VRAM 限制。</li>
<li><strong>训练稳定性</strong>：分段训练策略虽然有助于稳定，但训练过程仍可能比单阶段训练更复杂。</li>
</ul>
</li>
</ul>
<h3 id="6">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文中提供了 GitHub 链接（https://yehonathanlitman.github.io/edit_ctrl），表明代码是开源的，这对于复现和应用至关重要。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>基础模型</strong>：需要一个预训练的视频扩散模型（如基于 DiT 的模型）。</li>
<li><strong>适配器训练</strong>：局部上下文编码器 Cp 需要微调，全局上下文嵌入器 Gy 是新训练的。</li>
<li><strong>损失函数</strong>：注意分段训练策略的使用，以及局部损失 Lp 和联合损失 Lψ 的定义。</li>
<li><strong>数据预处理</strong>：掩码的膨胀处理、背景视频的生成、以及将视频降采样到潜在空间是关键步骤。</li>
<li><strong>超参数</strong>：如学习率、批次大小、迭代次数 n 等需要根据具体任务和数据集进行调整。</li>
<li><strong>硬件要求</strong>：虽然效率很高，但训练仍可能需要多 GPU（如 A100）。推理速度在 NVIDIA A6000Ada 上进行了评估。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他视频生成任务</strong>：EditCtrl 的解耦思想可以迁移到其他需要局部控制和全局一致性的视频生成任务中，例如视频风格迁移、视频修复等。</li>
<li><strong>图像编辑</strong>：虽然论文专注于视频，但其局部/全局解耦的思想也可以借鉴到图像编辑领域，特别是对于需要精细局部控制和全局上下文感知的任务。</li>
<li><strong>多模态融合</strong>：全局上下文嵌入器 Gy 的交叉注意力机制可以用于融合其他模态的信息（如音频、3D 信息）来指导视频生成。</li>
</ul>
</li>
</ul>
<h3 id="7">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：解耦局部细节与全局上下文，实现高效视频编辑。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>提取局部/全局信息</strong>：分别从掩码区域和背景视频中提取关键信息。</li>
<li><strong>局部/全局模块处理</strong>：用专门模块处理局部细节和全局一致性。</li>
<li><strong>引导基础模型</strong>：用处理后的信息指导预训练视频模型进行生成。</li>
<li><strong>合成最终视频</strong>：将生成内容与背景融合，输出编辑后的视频。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed.</li>
<li>Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size.</li>
<li>Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention.</li>
<li>Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.15031v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.15031v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.15030v1'></a></p>
<h2 id="image-generation-with-a-sphere-encoder"><a href="https://arxiv.org/abs/2602.15030v1">Image Generation with a Sphere Encoder</a></h2>
<p><strong>Authors:</strong> Kaiyu Yue, Menglin Jia, Ji Hou, Tom Goldstein</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Image Generation with a Sphere Encoder</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种名为“Sphere Encoder”的高效图像生成框架，能够在单次前向传播中生成高质量图像，并且在极少的步骤（少于五步）内就能达到与多步扩散模型相媲美的性能。其核心在于学习一个将自然图像均匀映射到球形潜在空间（spherical latent space）的编码器，以及一个将随机潜在向量映射回图像空间的解码器。通过这种方式，模型仅需解码球面上一个随机点即可生成图像，显著降低了推理成本。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<ul>
<li><strong>球形潜在空间（Spherical Latent Space）的引入与均匀映射：</strong> 这是该方法最核心的创新。传统的生成模型通常使用欧几里得空间作为潜在空间，而该论文将潜在空间设计为一个球体。通过学习一个编码器，将自然图像“均匀地”映射到这个球体上，意味着潜在空间中的点能够更有效地覆盖图像的分布，并且点之间的距离关系可能更符合图像的语义或结构关系。这种均匀映射的特性对于后续从随机点生成图像至关重要。</li>
<li><strong>单次前向传播的高效生成：</strong> 与扩散模型需要多步迭代才能生成图像不同，Sphere Encoder 仅需一次解码操作即可完成图像生成。这极大地提高了推理速度，使其在实际应用中更具吸引力。</li>
<li><strong>仅通过图像重建损失进行训练：</strong> 论文提到模型仅通过图像重建损失进行训练，这是一种相对简单且直接的训练范式。这意味着模型学习到的潜在表示能够忠实地重构原始图像，并且这种重构能力被用来驱动生成过程。</li>
<li><strong>通过迭代编码器/解码器增强图像质量：</strong> 论文指出，通过“循环编码器/解码器几次”可以进一步提升图像质量。这暗示了虽然基础生成是单步的，但可以通过一个简单的迭代过程来优化生成结果，这可能是一种后处理或微调机制，但仍然比扩散模型的完整迭代过程要快得多。</li>
<li><strong>自然支持条件生成：</strong> 论文提到该架构“自然支持条件生成”，这表明其潜在空间的结构或编码器的设计能够方便地融入条件信息（如文本描述、类别标签等），从而实现条件图像生成。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>加速图像生成技术的普及：</strong> 极低的推理成本将使得高质量图像生成技术更容易被部署到资源受限的环境中，例如移动设备、边缘计算设备，或者需要大规模实时生成的应用场景。</li>
<li><strong>挑战现有主流生成模型范式：</strong> Sphere Encoder 的成功将对当前占主导地位的扩散模型（Diffusion Models）构成直接挑战。它提供了一种在效率和性能之间取得更优平衡的替代方案，可能会促使研究人员探索更多非迭代或低迭代次数的生成方法。</li>
<li><strong>推动潜在空间研究：</strong> 该工作强调了精心设计的潜在空间（如球形空间）对生成模型性能的重要性。这可能会激发对其他非欧几里得或特殊几何形状潜在空间的研究，以期获得更好的数据表示和生成效果。</li>
<li><strong>降低生成模型的门槛：</strong> 简单高效的训练和推理过程，可能降低研究和应用生成模型的门槛。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>内容创作与设计：</strong> 快速生成高质量的图像素材，用于广告、游戏、电影特效、艺术创作等。</li>
<li><strong>虚拟现实/增强现实（VR/AR）：</strong> 实时生成逼真的虚拟场景或对象，提升用户体验。</li>
<li><strong>图像编辑与修复：</strong> 快速生成图像的缺失部分或进行风格迁移。</li>
<li><strong>数据增强：</strong> 为训练其他机器学习模型生成多样化的合成数据。</li>
<li><strong>机器人视觉：</strong> 快速生成模拟场景，用于训练和测试机器人感知系统。</li>
<li><strong>个性化推荐系统：</strong> 根据用户偏好快速生成定制化图像。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>“均匀映射”的实现细节与挑战：</strong> 摘要中提到“将自然图像均匀地映射到球形潜在空间”。如何精确地实现这种“均匀映射”是一个关键的技术挑战。如果映射不够均匀，可能会导致潜在空间中某些区域的图像质量下降，或者某些类型的图像难以生成。</li>
<li><strong>潜在空间的几何特性与语义的关联：</strong> 虽然球形空间可能在几何上具有吸引力，但其几何特性与图像的语义信息之间是否存在更深层次、更直观的关联，仍需进一步验证。例如，球体上的“距离”是否能很好地反映图像之间的语义差异？</li>
<li><strong>对特定类型图像的泛化能力：</strong> 摘要提到“跨越几个数据集”，但具体的数据集类型和规模并未详述。该模型在处理非常规、高分辨率或具有复杂纹理的图像时的泛化能力仍需观察。</li>
<li><strong>“循环编码器/解码器”的优化程度：</strong> 虽然循环几次可以提升质量，但这种迭代过程的效率和效果上限，以及其与扩散模型迭代过程的根本区别，需要更详细的实验数据来支撑。</li>
<li><strong>训练稳定性与收敛性：</strong> 仅通过重建损失训练，其训练过程的稳定性和收敛性是否优于或媲美其他方法，摘要中未提及。</li>
<li><strong>可解释性：</strong> 球形潜在空间的几何特性是否会带来更好的模型可解释性，或者反而增加理解难度，尚不明确。</li>
</ul>
<p>总而言之，Sphere Encoder 是一项非常有前景的研究，它通过巧妙地设计潜在空间和生成机制，有望在图像生成领域带来效率上的飞跃，并可能成为扩散模型之外的一个重要替代方案。其核心创新在于利用球形潜在空间的特性来实现高效、高质量的图像生成。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps.</li>
<li>Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.15030v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.15030v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.15018v1'></a></p>
<h2 id="neurosim-a-fast-simulator-for-neuromorphic-robot-perception"><a href="https://arxiv.org/abs/2602.15018v1">Neurosim: A Fast Simulator for Neuromorphic Robot Perception</a></h2>
<p><strong>Authors:</strong> Richeek Das, Pratik Chaudhari</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇关于Neurosim和Cortex的论文，重点关注其方法创新、设计逻辑、优势与不足，并提供实用的指南。</p>
<hr />
<h2 id="neurosimcortex">论文方法分析与总结：Neurosim与Cortex</h2>
<h3 id="1_1">1. 摘要翻译</h3>
<p><strong>Neurosim: A Fast Simulator for Neuromorphic Robot Perception</strong></p>
<p>Neurosim是一个快速、实时、高性能的库，用于模拟动态视觉传感器（DVS）、RGB摄像头、深度传感器和惯性传感器。它还可以模拟多旋翼飞行器在复杂动态环境中的敏捷动力学。Neurosim在桌面GPU上可实现高达约2700 FPS的帧率。Neurosim集成了基于ZeroMQ的通信库Cortex，以促进与机器学习和机器人工作流的无缝集成。Cortex为Python和C++应用程序提供了一个高吞吐量、低延迟的消息传递系统，并原生支持NumPy数组和PyTorch张量。本文讨论了Neurosim和Cortex的设计理念。它演示了如何利用它们来（i）训练神经形态感知和控制算法，例如使用时间同步的多模态数据进行自监督学习，以及（ii）在闭环中测试这些算法的实时实现。Neurosim和Cortex可在https://github.com/grasp-lyrl/neurosim获取。</p>
<h3 id="2_1">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>神经形态感知研究的兴起</strong>：事件相机等新型传感器提供了高时间分辨率、高动态范围和低功耗的感知能力，为机器人感知带来了新的可能性，尤其是在快速运动、复杂光照和资源受限场景下。</li>
<li><strong>真实数据获取的挑战</strong>：获取大规模、高质量、多模态同步的真实世界事件数据非常困难且成本高昂。现有的事件数据集规模有限（如M3ED数据集仅有不到四小时数据），且标注困难。</li>
<li><strong>模拟器在研究中的关键作用</strong>：为了克服真实数据获取的限制，开发一个能够高效、高保真地模拟事件相机和其他传感器，并支持多模态数据生成和闭环测试的模拟器至关重要。</li>
<li><strong>机器学习与机器人工作流的集成需求</strong>：将模拟器与现有的机器学习训练框架（如PyTorch）和机器人操作系统（如ROS）无缝集成，是加速研究和开发的关键。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>事件模拟效率低下</strong>：现有的CPU实现非常缓慢，GPU实现虽然有所提升，但仍存在CUDA核函数启动开销大、内存利用率低的问题。</li>
<li><strong>多模态数据同步与校准困难</strong>：真实世界中多传感器数据同步和精确的内外参校准是复杂且耗时的任务。</li>
<li><strong>数据存储与加载瓶颈</strong>：生成高帧率的模拟数据（尤其是事件数据）会产生海量数据，存储和加载这些数据会成为训练的瓶颈，并增加计算开销。</li>
<li><strong>闭环测试的局限性</strong>：现有模拟器难以支持在极端动态场景下进行高频率的闭环感知与控制测试，这可能导致真实硬件损坏。</li>
<li><strong>集成与易用性差</strong>：许多模拟器缺乏与主流机器学习框架和机器人中间件的良好集成，使用起来不够便捷。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>模拟数据与真实数据之间的“sim-to-real”差距</strong>：通过高保真、高帧率的事件模拟，可以减小模拟数据与真实数据之间的差距，使得在模拟环境中训练的算法能够更好地迁移到真实世界。</li>
<li><strong>高效通信是关键</strong>：低延迟、高吞吐量的通信机制是实现实时闭环控制和高效数据流的关键，能够避免数据存储瓶颈，并支持大规模分布式训练。</li>
<li><strong>模块化设计提升灵活性</strong>：将模拟器分解为独立的模块（渲染、动力学、通信等），可以方便地扩展、替换和维护，适应不同的研究需求。</li>
</ul>
</li>
</ul>
<h3 id="3_1">3. 方法设计详解</h3>
<p><strong>整体pipeline</strong>：Neurosim是一个模块化的仿真框架，其核心在于高效的事件生成和与Cortex通信库的集成，以支持多模态数据生成、实时闭环控制和在线训练。</p>
<p><strong>Neurosim 核心组件与流程</strong>：</p>
<ol>
<li>
<p><strong>数据资产 (Data Assets)</strong>：</p>
<ul>
<li><strong>功能</strong>：包含3D场景资产（如Matterport3D, Gibson, Replica等数据集）、多旋翼模型（包括其动力学参数、传感器配置）以及环境的规范。</li>
<li><strong>输入</strong>：用户定义的场景和机器人模型。</li>
<li><strong>输出</strong>：用于渲染和动力学仿真的基础数据。</li>
</ul>
</li>
<li>
<p><strong>渲染引擎 (Renderer)</strong>：</p>
<ul>
<li><strong>技术选型</strong>：基于Habitat-Sim [8]，一个以速度和灵活性著称的3D渲染引擎。</li>
<li><strong>功能</strong>：<ul>
<li><strong>高保真场景渲染</strong>：支持多种3D资产格式，能够渲染逼真的室内场景。</li>
<li><strong>多传感器模拟</strong>：支持模拟RGB-D、语义、航位推算（egomotion）摄像头，并可配置独立的内参和传感器参数。</li>
<li><strong>高帧率渲染</strong>：在单GPU上可实现高达~3000 FPS的VGA（640x480）分辨率渲染。</li>
</ul>
</li>
<li><strong>流程</strong>：接收场景和相机配置，生成高帧率的RGB、深度等图像帧。</li>
</ul>
</li>
<li>
<p><strong>事件生成器 (Event Generator)</strong>：</p>
<ul>
<li><strong>核心算法</strong>：基于对比度阈值模型（Contrast-Threshold Model）。</li>
<li><strong>工作原理</strong>：<ul>
<li><strong>状态跟踪</strong>：为每个像素维护一个状态，记录该像素上一次触发事件时的强度值。</li>
<li><strong>事件触发</strong>：当当前帧的像素强度变化超过预设阈值时，触发一个事件。</li>
<li><strong>事件信息</strong>：每个事件包含时间戳 (<script type="math/tex">t_i</script>)、像素坐标 (<script type="math/tex">u_i</script>) 和极性 (<script type="math/tex">p_i \in \{-1, 1\}</script>)，表示强度增加或减少。</li>
</ul>
</li>
<li><strong>优化</strong>：<ul>
<li><strong>CUDA核函数</strong>：采用高度并行的CUDA核函数在GPU上计算事件，每个线程处理一个像素。</li>
<li><strong>Warp同步与共享内存</strong>：利用Warp内的线程协作计算事件掩码（bitmask），并使用GPU共享内存（SRAM）进行中间结果的存储和Warp级归约（reduction），显著减少了全局原子操作（atomic operations）的数量（从每事件一次降至每Warp一次），从而大幅提升了事件生成效率。</li>
<li><strong>实时生成</strong>：事件数据是“即时生成”（on-the-fly）的，无需存储中间的RGB图像，节省了大量存储空间。</li>
</ul>
</li>
<li><strong>性能</strong>：在VGA分辨率下可达31 kHz，HD分辨率下可达23 kHz。</li>
</ul>
</li>
<li>
<p><strong>动力学模拟器 (Dynamics Simulator)</strong>：</p>
<ul>
<li><strong>技术选型</strong>：集成了RotorPy [12]，一个用于多旋翼动力学和惯性传感器的高速、精确模型。</li>
<li><strong>功能</strong>：<ul>
<li><strong>多旋翼动力学</strong>：模拟空气动力学、物理特性、姿态控制（如转子转速、推力、角速度）。</li>
<li><strong>惯性传感器模拟</strong>：模拟IMU（陀螺仪、加速度计、磁力计）的读数。</li>
<li><strong>运动规划</strong>：支持基于微分平坦性（differential flatness）的轨迹生成，实现动态可行的飞行轨迹。</li>
</ul>
</li>
<li><strong>性能</strong>：可在单CPU核心上以超过1 kHz的速率进行模拟。</li>
</ul>
</li>
<li>
<p><strong>通信接口 (Communication Interface) - Cortex</strong>：</p>
<ul>
<li><strong>核心功能</strong>：一个轻量级、低延迟、高吞吐量的消息传递库，用于Neurosim与其他进程（如机器学习训练器、控制器）之间的通信。</li>
<li><strong>技术基础</strong>：基于ZeroMQ [14]，并进行了优化。</li>
<li><strong>关键特性</strong>：<ul>
<li><strong>零拷贝（Zero-copy）</strong>：在可能的情况下，通过暴露内存缓冲区实现消息的零拷贝传输，减少数据复制开销。</li>
<li><strong>原生支持NumPy和PyTorch</strong>：能够高效地序列化和反序列化NumPy数组和PyTorch张量，直接支持机器学习数据格式。</li>
<li><strong>动态节点发现</strong>：通过一个轻量级的发现守护进程，实现节点间的动态连接和话题订阅，无需中心化代理。</li>
<li><strong>类型安全</strong>：通过消息类型的哈希值进行校验，确保数据格式的兼容性。</li>
<li><strong>ROS 2桥接</strong>：提供与ROS 2的双向桥接，方便集成到现有ROS生态系统中。</li>
</ul>
</li>
<li><strong>性能</strong>：在现代CPU上可实现高达7 GB/s的吞吐量，消息速率可达100+ kHz（小消息）。</li>
</ul>
</li>
<li>
<p><strong>应用接口 (Application API)</strong>：</p>
<ul>
<li><strong>功能</strong>：提供Python接口，方便用户与Neurosim交互。</li>
<li><strong>支持</strong>：<ul>
<li><strong>ROS 2桥接</strong>：与ROS 2消息进行转换。</li>
<li><strong>在线数据加载器 (Online Dataloader)</strong>：直接将模拟数据流式传输到机器学习训练管道，无需中间磁盘存储。支持多模态数据批处理、预取和多GPU训练。</li>
<li><strong>闭环算法验证 (Closed Loop Algorithm Validation)</strong>：支持同步模式，控制器可以实时发送指令给模拟器。</li>
<li><strong>轨迹生成 (Trajectories for RL)</strong>：为强化学习生成轨迹。</li>
<li><strong>可视化 (Visualize)</strong>：通过Rerun [7]等工具实时可视化模拟数据。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>整体工作流程</strong>：
1.  <strong>场景与机器人配置</strong>：用户定义3D场景、多旋翼模型和传感器配置。
2.  <strong>渲染与动力学模拟</strong>：渲染引擎生成高帧率的RGB、深度等图像，动力学模拟器更新机器人状态。
3.  <strong>事件生成</strong>：基于渲染的强度图像，事件生成器高效地计算事件流。
4.  <strong>数据发布</strong>：Neurosim通过Cortex将多模态传感器数据（RGB、深度、事件、IMU、状态等）发布为消息。
5.  <strong>数据订阅与处理</strong>：
    *   <strong>在线训练</strong>：机器学习训练器（如PyTorch）通过Cortex订阅数据，直接进行模型训练，无需磁盘存储。
    *   <strong>闭环控制</strong>：控制器订阅传感器数据，计算控制指令，并通过Cortex发送给Neurosim，影响机器人行为。
6.  <strong>可视化</strong>：通过Cortex连接的可视化工具（如Rerun）实时展示模拟场景和数据。</p>
<h3 id="4_1">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>事件生成效率</strong>：Neurosim采用高度优化的CUDA核函数和Warp同步机制，在GPU上实现了远超现有方法的事件生成速度，解决了现有模拟器在事件生成上的性能瓶颈。</li>
<li><strong>通信与数据流</strong>：Neurosim与Cortex的深度集成，实现了真正意义上的“零磁盘I/O”和“实时流式数据传输”，直接将高吞吐量、低延迟的多模态数据喂给下游应用，这是许多仅提供数据文件输出的模拟器所不具备的。</li>
<li><strong>闭环能力</strong>：Neurosim支持在极高频率下进行闭环控制测试，能够探索硬件性能的极限，而许多模拟器受限于渲染或通信速度，难以支持此类极端场景。</li>
<li><strong>模块化与易用性</strong>：Neurosim的设计理念强调模块化和Pythonic易用性，方便用户进行定制和集成。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>高效GPU事件生成器</strong>：通过Warp同步和共享内存优化，实现了前所未有的事件生成速度。</li>
<li><strong>与Cortex的无缝集成</strong>：构建了一个端到端的、低延迟、高吞吐量的仿真与训练/控制框架，解决了数据瓶颈问题。</li>
<li><strong>支持极端闭环测试</strong>：为研究者提供了在真实硬件可能损坏的场景下进行算法测试的平台。</li>
<li><strong>在线数据加载与训练</strong>：极大地简化了大规模数据处理流程，加速了模型训练。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>神经形态感知算法开发与测试</strong>：尤其适用于事件相机相关的感知任务，如深度估计、光流、目标跟踪、视觉里程计等。</li>
<li><strong>快速响应的机器人控制算法研究</strong>：如高速避障、敏捷飞行控制、运动规划等。</li>
<li><strong>多模态传感器融合研究</strong>：支持RGB、深度、事件、IMU等多种传感器数据的同步模拟。</li>
<li><strong>强化学习在机器人控制中的应用</strong>：提供真实感强的模拟环境和高效的数据流。</li>
<li><strong>Sim-to-Real研究</strong>：为训练和验证迁移到真实世界的算法提供了一个强大的平台。</li>
</ul>
</li>
</ul>
<h3 id="5_1">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>性能基准测试</strong>：通过与现有事件模拟器（如VID2E, ESIM, PyTorch实现）进行对比，在事件模拟延迟、端到端仿真速度和事件模拟器在总仿真时间中的占比等方面展示了Neurosim的优越性（图3）。</li>
<li><strong>闭环控制性能分析</strong>：通过在不同控制器速率下测试Neurosim的控制到观测延迟和传感器速率，验证了其在实时闭环控制中的能力（图6A）。</li>
<li><strong>在线训练演示</strong>：通过使用Neurosim生成数据，在线训练一个事件单目深度估计网络（F³-Depth Anything V2），展示了其在加速模型训练方面的潜力（图6B，论文第10页）。</li>
<li><strong>复杂场景下的轨迹跟踪</strong>：展示了模拟器在复杂室内场景下，支持高速、动态可行的四旋翼轨迹跟踪的能力（图5）。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>事件模拟速度</strong>：Neurosim的事件模拟器比其他GPU实现快8-13倍，比CPU实现快55-121倍。</li>
<li><strong>端到端仿真速度</strong>：在VGA传感器下，Neurosim可实现约2300 FPS的端到端仿真速度。</li>
<li><strong>事件模拟器开销</strong>：Neurosim的事件模拟器仅占总仿真时间的约8%，而其他模拟器占40-90%，表明其性能瓶颈已显著转移。</li>
<li><strong>闭环控制延迟</strong>：在5 kHz的控制器速率下，Neurosim的平均端到端延迟低于0.7 ms，远低于理想延迟。</li>
<li><strong>传感器速率</strong>：在VGA传感器下，控制器可达到的最高传感器速率约为2.3 kHz，接近GPU渲染的极限。</li>
<li><strong>在线训练效率</strong>：通过Neurosim在线训练深度模型，避免了海量数据存储，并能有效利用GPU资源。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>高帧率、低延迟的感知任务</strong>：如需要快速响应的视觉伺服、避障等。</li>
<li><strong>多模态数据同步</strong>：在需要RGB、深度、事件、IMU等多种传感器数据协同工作的场景。</li>
<li><strong>极端动态场景</strong>：如模拟四旋翼的翻滚、快速机动等，这些场景在真实世界中难以安全测试。</li>
<li><strong>大规模数据生成与训练</strong>：当需要生成海量数据用于训练深度学习模型时，Neurosim的在线生成能力优势明显。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>渲染能力</strong>：虽然基于Habitat-Sim，但其渲染的逼真度可能仍不如Unreal Engine等专业引擎，尤其是在复杂光照和材质方面（论文中也提到了未来计划支持Unreal Engine 5）。</li>
<li><strong>传感器模型精度</strong>：虽然模拟了事件相机的基本特性，但对于更复杂的传感器非理想特性（如噪声、带宽限制、饱和效应等），可能需要进一步细化（论文中也提到了未来计划添加更多噪声模型）。</li>
<li><strong>计算资源需求</strong>：高性能的GPU是实现其高帧率模拟的关键，对于资源受限的平台可能存在门槛。</li>
<li><strong>动力学模型精度</strong>：RotorPy虽然精确，但对于非常规的动力学行为或复杂环境交互（如与物体碰撞的细节）可能仍有局限。</li>
</ul>
</li>
</ul>
<h3 id="6_1">6. 实用指南</h3>
<ul>
<li>
<p><strong>开源情况</strong>：</p>
<ul>
<li><strong>开源</strong>：Neurosim和Cortex均已开源，并提供了GitHub链接：<code>https://github.com/grasp-lyrl/neurosim</code>。</li>
<li><strong>实现/复现关键步骤</strong>：<ol>
<li><strong>环境搭建</strong>：按照README文件配置Python环境、CUDA、PyTorch等依赖。</li>
<li><strong>场景与模型加载</strong>：准备或下载Habitat-Sim支持的3D场景数据集，并配置多旋翼模型文件。</li>
<li><strong>配置传感器</strong>：在代码中定义所需的传感器类型（RGB, Depth, Event, IMU等）及其参数。</li>
<li><strong>运行模拟器</strong>：启动Neurosim主程序，可以选择运行模式（如实时闭环、数据流式输出）。</li>
<li><strong>集成Cortex</strong>：编写订阅者（如训练脚本、控制器）来通过Cortex接收数据或发送指令。</li>
<li><strong>可视化</strong>：使用Rerun等工具连接到Cortex，实时查看模拟结果。</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>实现细节</strong>：</p>
<ul>
<li><strong>超参数</strong>：事件阈值、渲染分辨率、动力学参数等需要根据具体任务进行调整。</li>
<li><strong>数据预处理</strong>：对于事件数据，可能需要进行时间窗口聚合、极性编码等预处理。</li>
<li><strong>训练细节</strong>：在线数据加载器支持批处理和预取，需要合理设置批大小和预取数量以平衡吞吐量和内存占用。</li>
<li><strong>闭环控制</strong>：注意控制器与模拟器之间的通信延迟，以及控制器自身的计算延迟，以确保闭环的稳定性。</li>
<li><strong>GPU资源</strong>：确保有足够显存和计算能力的GPU来运行模拟器和训练模型。</li>
</ul>
</li>
<li>
<p><strong>迁移可能</strong>：</p>
<ul>
<li><strong>迁移到其他任务</strong>：该框架非常适合用于各种机器人感知和控制任务，特别是那些受益于事件相机或需要高频多模态数据输入的任务。</li>
<li><strong>迁移到其他传感器</strong>：虽然核心是事件相机，但其模块化设计允许集成其他类型的传感器模型。</li>
<li><strong>迁移到其他渲染引擎</strong>：论文提到未来计划支持Unreal Engine 5，这表明其架构具有一定的可扩展性，可以集成其他渲染后端。</li>
<li><strong>迁移到其他通信库</strong>：虽然Cortex是其首选，但理论上可以通过修改通信接口部分，适配ROS 2的<code>rclcpp</code>或纯ZeroMQ等其他通信方式。</li>
</ul>
</li>
</ul>
<h3 id="7_1">7. 总结</h3>
<ul>
<li>
<p><strong>核心思想</strong>：
    <strong>高效事件模拟与实时多模态数据流，赋能机器人感知与控制研究。</strong></p>
</li>
<li>
<p><strong>速记版pipeline</strong>：</p>
<ol>
<li><strong>配置场景与机器人</strong>：定义仿真环境和机器人模型。</li>
<li><strong>高帧率渲染与事件生成</strong>：GPU加速生成图像和事件流。</li>
<li><strong>多模态数据实时发布</strong>：通过Cortex高速传输传感器数据。</li>
<li><strong>在线训练或闭环控制</strong>：直接喂给ML模型或反馈给机器人。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop.</li>
<li>Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.15018v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.15018v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.14989v1'></a></p>
<h2 id="thermeval-a-structured-benchmark-for-evaluation-of-vision-language-models-on-thermal-imagery"><a href="https://arxiv.org/abs/2602.14989v1">ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery</a></h2>
<p><strong>Authors:</strong> Ayush Shrivastava, Kirtan Gangani, Laksh Jain, Mayank Goel, Nipun Batra</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇论文的方法部分，重点关注其创新之处、设计逻辑、优势与不足，并提供实用的分析和指导。</p>
<hr />
<h2 id="_1">论文方法分析与总结</h2>
<h3 id="1_2">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery (ThermEval：一个用于评估热成像视觉语言模型的结构化基准)</p>
<p><strong>中文摘要：</strong> 视觉语言模型（VLMs）在RGB图像上表现出色，但它们在热成像图像上的泛化能力不足。热成像传感在可见光失效的环境中扮演着关键角色，包括夜间监控、搜索救援、自动驾驶和医疗筛查。与RGB图像不同，热成像图像编码的是物理温度而非颜色或纹理，这需要现有的RGB为中心的基准无法评估的感知和推理能力。我们引入了ThermEval-B，一个包含约55,000个热成像视觉问答对的结构化基准，旨在评估热成像视觉语言理解所需的基础性原语。ThermEval-B整合了公共数据集以及我们新收集的ThermEval-D，后者是第一个提供密集逐像素温度图和语义身体部位标注的、跨越多样室内外环境的数据集。通过评估25个开源和闭源的VLM，我们发现模型在温度相关的推理方面持续失败，在颜色图转换下性能下降，并倾向于依赖语言先验或固定回答，仅在提示或监督微调下获得微弱提升。这些结果表明，热成像理解需要超越RGB为中心的假设进行专门评估，ThermEval旨在推动热成像视觉语言建模的进步。</p>
<h3 id="2_2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>热成像的实际应用价值</strong>：论文强调了热成像在可见光不足场景（如夜间、恶劣天气）下的关键作用，如监控、搜救、自动驾驶、医疗等。这些场景下，温度信息比颜色和纹理更重要。</li>
<li><strong>现有VLM在热成像上的性能鸿沟</strong>：尽管VLM在RGB图像上表现优异，但它们在热成像上的泛化能力非常差，存在显著的“领域差距”（Domain Gap）。</li>
<li><strong>现有基准的局限性</strong>：现有的视觉语言理解基准（如MMBench, SEED-Bench等）主要集中在RGB图像上，无法有效评估模型在热成像领域所需的特定感知和推理能力（如温度理解、颜色图转换鲁棒性等）。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>缺乏专门的热成像VLM评估基准</strong>：无法系统地研究和量化VLM在热成像任务上的表现。</li>
<li><strong>模型对RGB的过度依赖</strong>：VLM在训练中主要接触RGB图像，学习到了与颜色、纹理相关的视觉特征，而对温度这种物理信号的理解不足。</li>
<li><strong>对颜色图转换的敏感性</strong>：热成像图像通常会通过颜色图进行可视化，不同的颜色图会显著改变图像的外观，导致模型性能下降。</li>
<li><strong>语言先验的滥用</strong>：模型可能依赖于训练数据中的语言模式（如“人体体温通常是37°C”）来回答问题，而不是真正理解热成像信号。</li>
<li><strong>缺乏细粒度的温度理解能力</strong>：模型难以进行精确的温度估计、相对温度比较以及温度与物理位置的关联。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>热成像理解需要专门的基准</strong>：现有的RGB基准不足以评估VLM在热成像领域的真实能力。</li>
<li><strong>VLM在热成像上的失败源于领域差距和缺乏物理信号的理解</strong>：模型未能有效将语言与热成像的物理温度信号进行关联。</li>
<li><strong>结构化、多任务的基准能更好地揭示VLM在热成像领域的弱点</strong>：通过设计一系列递进的、涵盖不同挑战的任务，可以更全面地评估模型。</li>
</ul>
</li>
</ul>
<h3 id="3_2">3. 方法设计详解</h3>
<p><strong>核心方法：</strong> 提出 <strong>ThermEval</strong> 框架，包含一个结构化的基准 <strong>ThermEval-B</strong> 和一个数据集 <strong>ThermEval-D</strong>。</p>
<p><strong>流程总结：</strong></p>
<ol>
<li>
<p><strong>数据集构建 (ThermEval-D)</strong>：</p>
<ul>
<li><strong>目的</strong>：提供高质量、多样的热成像数据，包含精确的温度信息和语义标注，以支持细粒度的热成像理解任务。</li>
<li><strong>数据来源</strong>：<ul>
<li>整合了现有的公开数据集：FLIR-ADAS (用于T1, T2, T3) 和 LLVIP (用于T1, T2, T3)。</li>
<li><strong>新收集 ThermEval-D</strong>：包含1000+张室内外场景（办公室、公园、工作区等）的热成像图像，具有<strong>逐像素的温度图</strong>和<strong>语义身体部位标注</strong>（如额头、胸部、鼻子、全身）。</li>
</ul>
</li>
<li><strong>数据特点</strong>：<ul>
<li><strong>高精度温度信息</strong>：逐像素温度图是关键，支持精确的温度估计和推理。</li>
<li><strong>语义身体部位标注</strong>：为细粒度的温度分析（如比较不同身体部位的温度）提供了基础。</li>
<li><strong>多样性</strong>：涵盖室内外环境和不同活动场景，增加了模型的泛化性挑战。</li>
</ul>
</li>
<li><strong>数据采集与标注</strong>：遵循了严格的伦理协议，由三位专家进行标注，并量化了标注者间的一致性（IoU和Dice系数），确保了标注质量。</li>
</ul>
</li>
<li>
<p><strong>基准设计 (ThermEval-B)</strong>：</p>
<ul>
<li><strong>目的</strong>：系统地评估VLM在热成像理解方面的基础能力，从简单到复杂，涵盖了七个任务（T1-T7）。</li>
<li><strong>任务设计逻辑</strong>：任务难度递增，层层递进，旨在探测模型在不同层面的热成像理解能力。</li>
<li><strong>七个任务详解</strong>：<ul>
<li><strong>T1: Modality Identification (模态识别)</strong><ul>
<li><strong>目的</strong>：模型能否区分热成像和RGB图像。</li>
<li><strong>输入</strong>：成对的热成像和RGB图像。</li>
<li><strong>输出</strong>：判断图像类型（热成像/RGB）。</li>
<li><strong>数据源</strong>：FLIR, LLVIP。</li>
<li><strong>关键点</strong>：这是最基础的任务，测试模型是否能识别出热成像的独特视觉特征。</li>
</ul>
</li>
<li><strong>T2: Modality Identification under Colormap Transformations (颜色图转换下的模态识别)</strong><ul>
<li><strong>目的</strong>：评估模型对颜色图变化的鲁棒性。</li>
<li><strong>输入</strong>：热成像图像，但使用不同的颜色图（如Magma, Viridis, Summer, Spring）进行可视化。</li>
<li><strong>输出</strong>：判断图像类型。</li>
<li><strong>关键点</strong>：测试模型是否依赖于颜色图的“表面”特征，还是能理解其背后的物理温度信号。这是对T1的挑战。</li>
</ul>
</li>
<li><strong>T3: Human Presence and Counting (人体存在与计数)</strong><ul>
<li><strong>目的</strong>：评估模型在热成像中识别和计数人体的基本感知能力。</li>
<li><strong>输入</strong>：包含行人的热成像图像。</li>
<li><strong>输出</strong>：图像中人的数量。</li>
<li><strong>数据源</strong>：FLIR, LLVIP。</li>
<li><strong>关键点</strong>：测试模型能否从热成像中提取有意义的语义信息。</li>
</ul>
</li>
<li><strong>T4: Colorbar Interpretation (颜色条解读)</strong><ul>
<li><strong>目的</strong>：评估模型能否理解热成像图像中的颜色条（colorbar），这是温度估计和推理的前提。</li>
<li><strong>子任务</strong>：<ul>
<li><strong>检测</strong>：图像中是否存在颜色条。</li>
<li><strong>定位</strong>：颜色条的位置（上、左、下、右）。</li>
<li><strong>范围提取</strong>：从颜色条中提取温度范围（最大值、最小值）。</li>
</ul>
</li>
<li><strong>数据源</strong>：ThermEval-D。</li>
<li><strong>关键点</strong>：模型需要理解颜色条的视觉模式和数值映射关系。</li>
</ul>
</li>
<li><strong>T5: Thermal Reasoning (热成像推理)</strong><ul>
<li><strong>目的</strong>：评估模型在相对温度上的推理能力。</li>
<li><strong>子任务</strong>：<ul>
<li><strong>跨个体比较</strong>：比较两人之间身体部位的温度。</li>
<li><strong>个体内部比较</strong>：对同一人的不同身体部位按温度排序。</li>
</ul>
</li>
<li><strong>数据源</strong>：ThermEval-D。</li>
<li><strong>关键点</strong>：测试模型是否能进行温度的相对比较，而不是依赖语言先验（如“额头通常比鼻子热”）。</li>
</ul>
</li>
<li><strong>T6: Absolute Temperature Estimation (绝对温度估计)</strong><ul>
<li><strong>目的</strong>：评估模型从热成像中估计绝对温度的能力。</li>
<li><strong>子任务</strong>：<ul>
<li><strong>坐标估计</strong>：根据像素坐标估计温度。</li>
<li><strong>标记点估计</strong>：根据箭头等标记点估计温度。</li>
<li><strong>区域估计</strong>：估计语义区域（如额头、胸部）的平均温度。</li>
</ul>
</li>
<li><strong>数据源</strong>：ThermEval-D。</li>
<li><strong>关键点</strong>：这是核心任务，需要模型准确地将颜色条映射到物理温度值。</li>
</ul>
</li>
<li><strong>T7: Temperature Estimation at Varying Depth (不同距离下的温度估计)</strong><ul>
<li><strong>目的</strong>：评估模型在不同距离下估计温度的鲁棒性。</li>
<li><strong>输入</strong>：同一场景但不同距离（2ft, 6ft, 10ft）的图像。</li>
<li><strong>输出</strong>：估计的温度值。</li>
<li><strong>数据源</strong>：ThermEval-D。</li>
<li><strong>关键点</strong>：测试模型在距离变化对温度感知的影响下的表现。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构/算法解释：</strong></p>
<ul>
<li><strong>VLM评估</strong>：论文评估了25个VLM，包括开源和闭源模型，从0.3B到200B+参数。评估主要采用<strong>零样本（Zero-shot）</strong>提示，并对部分模型进行了<strong>上下文增强提示（Context-augmented prompts）</strong>和<strong>监督微调（Supervised Fine-tuning, SFT）</strong>的实验。</li>
<li><strong>解析器（Parser）</strong>：为了标准化VLM的输出（尤其是开放式回答），作者使用了Gemini 2.5模型作为“解析器”，用于提取分类任务的类别标签和回归任务的数值。这是一种创新的方法，用于处理VLM输出格式不一致的问题。</li>
<li><strong>评估指标</strong>：根据任务类型，使用准确率（Accuracy）、平均绝对误差（MAE）、均方根误差（RMSE）、偏差（Bias）和标准差（STD）等指标。</li>
</ul>
<h3 id="4_2">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>领域专注性</strong>：ThermEval是第一个专门为热成像视觉语言模型设计的全面基准，而现有基准（如MMBench, MME等）主要面向RGB图像。</li>
<li><strong>任务设计</strong>：ThermEval的任务设计直接针对热成像的独特性质（温度、颜色图、物理信号），如颜色图鲁棒性、绝对温度估计等，这些是RGB基准中不常见的。</li>
<li><strong>数据集的温度精度</strong>：ThermEval-D提供了逐像素的精确温度图，这是许多现有热成像数据集所缺乏的，后者通常只有RGB图像或粗略的类别标注。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>ThermEval-B基准</strong>：提供了一个结构化、多层次的评估框架，系统地揭示了VLM在热成像理解方面的弱点。</li>
<li><strong>ThermEval-D数据集</strong>：第一个包含逐像素温度图和身体部位标注的热成像数据集，为细粒度热成像理解研究提供了基础。</li>
<li><strong>对VLM在热成像领域局限性的深入分析</strong>：通过大量实验，揭示了模型在温度推理、颜色图鲁棒性、语言先验依赖等方面的具体问题。</li>
<li><strong>对提示工程和微调效果的系统性评估</strong>：展示了这些方法在多大程度上能缓解热成像理解的挑战。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>评估和研究VLM在热成像领域的性能</strong>：适用于任何需要评估或改进VLM在热成像应用（如自动驾驶、监控、医疗诊断）中表现的研究。</li>
<li><strong>推动热成像VLM的发展</strong>：为研究人员提供了一个标准化的测试平台，以衡量模型进步。</li>
<li><strong>分析模型在物理信号理解上的弱点</strong>：特别适用于研究模型如何处理非RGB的物理传感器数据。</li>
</ul>
</li>
</ul>
<h3 id="5_2">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>模型选择</strong>：评估了25个不同规模和架构的VLM，覆盖了主流的开源和闭源模型。</li>
<li><strong>评估协议</strong>：主要采用零样本（Zero-shot）设置，以测试模型的通用能力。同时，对部分模型进行了上下文增强提示和监督微调的实验，以探索提升性能的途径。</li>
<li><strong>数据集划分</strong>：使用FLIR, LLVIP和ThermEval-D数据集，根据任务需求进行分配。</li>
<li><strong>结果分析</strong>：通过详细的表格和图表，展示了模型在各个任务上的表现，并与人类表现进行对比。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>普遍失败</strong>：大多数VLM在热成像理解任务上表现不佳，尤其是在温度推理（T5）、绝对温度估计（T6, T7）方面。</li>
<li><strong>颜色图敏感性</strong>：模型在不同颜色图下表现不稳定，尤其是在复杂颜色图（Summer, Spring）下性能下降明显。</li>
<li><strong>语言先验依赖</strong>：模型常依赖于语言先验（如“人体体温37°C”）而非实际的温度信号。</li>
<li><strong>微调的局限性</strong>：监督微调（SFT）能显著提升性能，但仍无法完全解决温度推理和精确估计的挑战，表明问题根源在于模型缺乏物理信号的“领域接地”（Domain Grounding）。</li>
<li><strong>T4（颜色条解读）是关键前置任务</strong>：模型在颜色条解读上的失败会严重影响后续的温度估计和推理任务。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>T1（模态识别）</strong>：大多数模型在区分RGB和热成像图像方面表现良好，表明模型能识别出热成像的基本视觉模式。</li>
<li><strong>T3（人体计数）</strong>：较新的模型（如InternVL, Qwen-VL）在人体计数任务上表现出显著进步，但仍低于人类水平。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>计算资源</strong>：评估大量模型需要大量的计算资源。</li>
<li><strong>数据依赖</strong>：ThermEval-D虽然提供了逐像素温度图，但其规模相对有限。</li>
<li><strong>模型能力上限</strong>：即使是微调，也未能完全解决热成像理解的根本问题，表明现有VLM架构和训练范式存在根本性限制。</li>
<li><strong>解析器引入的潜在误差</strong>：虽然作者进行了验证，但使用LLM作为解析器仍可能引入一定误差。</li>
</ul>
</li>
</ul>
<h3 id="6_2">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提到“The working code used for all experiments and analyses is publicly available in the github repository and project page”。这意味着代码和数据集是开源的，方便复现和进一步研究。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>环境配置</strong>：需要配置Python 3.8.10环境，并安装<code>requirements.txt</code>中的依赖。</li>
<li><strong>数据集准备</strong>：下载FLIR-ADAS, LLVIP, ThermEval-D数据集，并放置在指定目录。</li>
<li><strong>模型集成</strong>：需要为待评估的模型实现<code>load_{model_name}</code>和<code>infer_{model_name}</code>函数，并将其集成到<code>model_inference.py</code>文件中。</li>
<li><strong>运行脚本</strong>：使用<code>Run.py</code>脚本，指定模型名称即可运行评估。</li>
<li><strong>超参数</strong>：论文中提到评估时使用了统一的硬件配置（NVIDIA A100 GPU），并且评估是单次前向传播（无集成、无模型内部访问）。微调实验中使用了LoRA，并给出了具体的超参数（rank, alpha, learning rate等）。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>迁移到其他物理传感器模态</strong>：ThermEval的设计思路（如多任务、逐像素物理信息、颜色图鲁棒性）可以借鉴到其他物理传感器模态（如LiDAR点云、雷达数据、光谱图像）的VLM评估中。关键在于构建对应模态的精细化数据集和设计针对性的评估任务。</li>
<li><strong>改进VLM的物理信号理解能力</strong>：该研究揭示了VLM在物理信号理解上的不足，为未来VLM的预训练策略提供了方向，例如在预训练阶段引入更多样的物理传感器数据，或者设计更侧重于物理信号理解的预训练任务。</li>
<li><strong>开发更鲁棒的VLM架构</strong>：研究结果表明，现有VLM在处理物理信号时存在根本性问题，可能需要开发新的模型架构来更好地融合物理信息。</li>
</ul>
</li>
</ul>
<h3 id="7_2">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：构建热成像VLM评估基准，揭示模型在物理信号理解上的根本性不足。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>收集/构建</strong>：整合现有数据，并创建包含精确温度图和身体部位标注的新热成像数据集（ThermEval-D）。</li>
<li><strong>设计任务</strong>：设计一系列从易到难的七个任务（T1-T7），覆盖模态识别、颜色图鲁棒性、计数、颜色条解读、温度推理和估计。</li>
<li><strong>评估模型</strong>：使用零样本、提示增强和微调等方式，在基准上评估大量VLM。</li>
<li><strong>分析结果</strong>：揭示模型在温度理解上的普遍失败，以及对颜色图和语言先验的依赖。</li>
<li><strong>指出问题</strong>：强调模型缺乏物理信号的“领域接地”是核心挑战。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding.</li>
<li>ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.14989v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.14989v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.14974v1'></a></p>
<h2 id="dm0-an-embodied-native-vision-language-action-model-towards-physical-ai"><a href="https://arxiv.org/abs/2602.14974v1">DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI</a></h2>
<p><strong>Authors:</strong> En Yu, Haoran Lv, Jianjian Sun, Kangheng Lin, Ruitao Zhang, Yukang Shi, Yuyang Chen, Ze Chen, Ziheng Zhang, Fan Jia, Kaixin Liu, Meng Zhang, Ruitao Hao, Saike Huang, Songhan Xie, Yu Liu, Zhao Wu, Bin Xie, Pengwei Zhang, Qi Yang, Xianchi Deng, Yunfei Wei, Enwen Zhang, Hongyang Peng, Jie Zhao, Kai Liu, Wei Sun, Yajun Wei, Yi Yang, Yunqiao Zhang, Ziwei Yan, Haitao Yang, Hao Liu, Haoqiang Fan, Haowei Zhang, Junwen Huang, Yang Chen, Yunchao Ma, Yunhuan Yang, Zhengyuan Du, Ziming Liu, Jiahui Niu, Yucheng Zhao, Daxin Jiang, Wenbin Tang, Xiangyu Zhang, Zheng Ge, Erjin Zhou, Tiancai Wang</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域高水平研究生的角色，深入分析您提供的论文方法部分，并按照您设定的框架进行输出。请提供论文的PDF文件或文本内容，我将开始进行分析。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI.</li>
<li>Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training.</li>
<li>Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space.</li>
<li>Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.14974v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.14974v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.14965v1'></a></p>
<h2 id="pact-part-decomposed-single-view-articulated-object-generation"><a href="https://arxiv.org/abs/2602.14965v1">PAct: Part-Decomposed Single-View Articulated Object Generation</a></h2>
<p><strong>Authors:</strong> Qingming Liu, Xinyue Yao, Shuyuan Zhang, Yueci Deng, Guiliang Liu, Zhen Liu, Kui Jia</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入剖析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="pact-part-decomposed-single-view-articulated-object-generation_1">论文方法分析与总结：PAct: Part-Decomposed Single-View Articulated Object Generation</h2>
<h3 id="1_3">1. 摘要翻译</h3>
<p><strong>PAct：部件分解的单视角可动对象生成</strong></p>
<p>QINGMING LIU, The Chinese University of Hong Kong, Shenzhen, China and DexForce Technology, China
XINYUE YAO, The Chinese University of Hong Kong, Shenzhen, China
SHUYUAN ZHANG, The Chinese University of Hong Kong, Shenzhen, China
YUECI DENG, The Chinese University of Hong Kong, Shenzhen, China and DexForce Technology, China
GUILIANG LIU, The Chinese University of Hong Kong, Shenzhen, China
ZHEN LIU*, The Chinese University of Hong Kong, Shenzhen, China
KUI JIA, The Chinese University of Hong Kong, Shenzhen, China and DexForce Technology, China</p>
<p>可动对象对于包括具身AI、机器人和VR/AR在内的交互式3D应用至关重要，其中功能性部件分解和运动是必不可少的。然而，生成高质量的可动资产在规模化上仍然很困难，因为它需要可靠的部件分解和运动绑定。现有方法主要分为两种范式：基于优化的重建或蒸馏，虽然准确但通常需要数十分钟到数小时；以及推理时的方法，依赖模板或部件检索，能产生合理结果但可能不匹配输入对象。我们提出了一种部件中心生成框架，用于可动对象创建，该框架通过显式的部件感知条件生成部件几何、组合和运动。我们的表示将对象建模为一组可移动部件，每个部件都由增强了部件身份和运动线索的潜在令牌编码。在单张图像的条件下，该模型生成可动3D资产，在保持有效部件结构和运动的同时，保留实例级别的对应关系。该方法避免了每实例优化，实现了快速前馈推理，并支持可控的组合和运动，这对于具身交互至关重要。在常见可动类别（如抽屉和门）上的实验表明，与基于优化和检索的方法相比，我们的方法在输入一致性、部件准确性和运动合理性方面有所提高，同时显著减少了推理时间。项目主页：<a href="https://PAct-project.github.io">https://PAct-project.github.io</a></p>
<h3 id="2_3">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>交互式3D应用的需求</strong>：虚拟现实、机器人、具身AI等领域需要逼真且可交互的3D对象，而可动对象（如门、抽屉、椅子等）是现实世界交互的关键组成部分。</li>
<li><strong>现有方法效率低下</strong>：当前主流的可动对象生成方法要么是基于优化的，需要大量计算时间（数十分钟到数小时），要么是基于检索的，生成的对象可能与输入图像不匹配。</li>
<li><strong>单视角输入的挑战</strong>：从单张2D图像恢复3D结构和运动信息本身就具有挑战性，需要强大的先验知识和生成能力。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>基于优化的方法</strong>：<ul>
<li><strong>计算成本高</strong>：每实例优化耗时过长，不适用于实时或大规模生成。</li>
<li><strong>易受优化失败影响</strong>：单视角约束弱，问题本身 ill-posed，优化过程可能不稳定。</li>
</ul>
</li>
<li><strong>基于检索/模板的方法</strong>：<ul>
<li><strong>缺乏实例特异性</strong>：生成的对象可能与输入图像的真实几何和外观不符，只是一个“相似”的模板。</li>
<li><strong>泛化能力有限</strong>：难以处理训练集中未出现过的对象或新颖的部件组合。</li>
</ul>
</li>
<li><strong>缺乏统一框架</strong>：现有方法往往侧重于几何、纹理或运动中的某一方面，难以同时生成高质量、可控且物理合理的完整可动对象。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>部件中心表示的有效性</strong>：将可动对象分解为独立的、具有明确几何和外观的部件，并独立建模其运动，是解决复杂可动对象生成问题的有效途径。</li>
<li><strong>预训练3D生成模型的迁移能力</strong>：利用在大量静态3D对象上预训练的强大生成模型（如TRELLIS）作为基础，通过微调可以有效地学习可动对象的部件分解和运动生成。</li>
<li><strong>多步特征聚合的潜力</strong>：在扩散模型生成过程中，不同时间步的中间特征包含了丰富的语义和结构信息，聚合这些信息可以更准确地预测部件的运动参数。</li>
</ul>
</li>
</ul>
<h3 id="3_3">3. 方法设计详解</h3>
<p><strong>流程总结</strong></p>
<p>PAct 采用一个两阶段的生成框架，从单张RGB图像生成一个可动3D对象。</p>
<p><strong>阶段 1：部件感知稀疏结构生成 (Part-Aware Flow Model)</strong></p>
<ul>
<li><strong>目标</strong>：从单张输入图像中预测出对象的部件级稀疏结构，包括每个部件的粗略几何和位置信息。</li>
<li><strong>输入</strong>：单张RGB图像 <script type="math/tex">I</script>。</li>
<li><strong>核心组件</strong>：<ul>
<li><strong>图像编码器 (Image Encoder)</strong>：使用预训练的DINOv2模型提取图像特征。</li>
<li><strong>部件感知流模型 (Part-Aware Flow Model)</strong>：这是基于TRELLIS [Xiang et al. 2024] 的修改版本。<ul>
<li><strong>TRELLIS 基础</strong>：TRELLIS 是一个两阶段的3D生成模型，第一阶段生成稀疏的64x64x64的体素表示（occupancy grid），并将其编码为紧凑的潜在表示 <script type="math/tex">z</script>。然后使用一个流匹配模型 (rectified flow model) <script type="math/tex">RF_{ss}</script> 在此潜在空间进行条件生成。</li>
<li><strong>PAct 的修改</strong>：<ul>
<li><strong>Mask-based Part Conditioning</strong>：为了提供更明确的部件空间引导，引入了2D部件掩码 <script type="math/tex">M</script>。该掩码将图像中的每个像素分配一个部件索引。通过一个可学习的部件身份嵌入矩阵 <script type="math/tex">E</script> 将掩码转换为密集的部件嵌入图 <script type="math/tex">P_{u,v}</script>。这个嵌入图被下采样后，与图像特征一起通过交叉注意力层注入到模型中。</li>
<li><strong>Part-Aware Denoising Transformer</strong>：TRELLIS 的Transformer被修改为“部件感知”。部分注意力层被限制在同一部件内部进行自注意力计算（<strong>Within-part Local Attention</strong>），而其他层则保留全局注意力以捕捉部件间的交互。这种设计允许模型在利用预训练的全局建模能力的同时，也能精细化处理单个部件。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出</strong>：部件级别的稀疏体素表示，为后续阶段提供部件的粗略几何和分割信息。</li>
</ul>
<p><strong>阶段 2：部件结构条件生成与运动预测 (Structure Conditioned Generation and Articulation Prediction)</strong></p>
<ul>
<li><strong>目标</strong>：基于阶段1生成的部件稀疏结构，合成精细的部件几何和外观，并预测每个部件的运动参数。</li>
<li><strong>输入</strong>：阶段1输出的部件级稀疏体素表示，以及原始输入图像 <script type="math/tex">I</script>。</li>
<li><strong>核心组件</strong>：<ul>
<li><strong>部件结构条件生成 (Structure Condition Object Generation)</strong>：<ul>
<li><strong>TRELLIS/OmniPart 借鉴</strong>：借鉴了OmniPart [Yang et al. 2025] 的思想，直接以阶段1预测的部件稀疏体素作为条件，而不是依赖额外的边界框预测器。</li>
<li><strong>Tokenization 和 Positional Encoding</strong>：将稀疏结构体素展平为1D token序列，并注入空间信息。</li>
<li><strong>部件身份嵌入 (Part Identity Embedding)</strong>：引入一个与阶段1不同的、可学习的部件身份嵌入，并将其加到属于同一部件的所有token上，以在全局去噪过程中保持部件的区分度。</li>
<li><strong>稀疏解码器 (Sparse Decoder)</strong>：使用TRELLIS中预训练的稀疏解码器 <script type="math/tex">D_{SLAT}</script> 重建每个部件的精细3D表示。</li>
</ul>
</li>
<li><strong>运动预测 (Articulation Prediction)</strong>：<ul>
<li><strong>多步特征聚合 (Multi-step Feature Aggregation)</strong>：从阶段2的去噪Transformer中，缓存一系列（例如最后20个）扩散时间步的中间特征。这些特征被平均聚合（<strong>Feature Averaging</strong>），以捕捉不同时间步的语义信息。</li>
<li><strong>部件参数化 (Articulation Parameterization)</strong>：<ul>
<li><strong>简化树结构</strong>：假设对象具有深度为1的树状结构（所有可动部件直接连接到固定根部），并合并固定连接的子组件到其父部件。</li>
<li><strong>参数定义</strong>：每个部件的运动由关节类型 (<script type="math/tex">t_i</script>)、语义标签 (<script type="math/tex">s_i</script>)、关节原点 (<script type="math/tex">o_i</script>)、关节轴 (<script type="math/tex">u_i</script>)、运动范围 (<script type="math/tex">p_i</script>) 和父节点索引 (<script type="math/tex">q_i</script>) 定义。</li>
</ul>
</li>
<li><strong>关节参数回归 (Articulation Regression)</strong>：将聚合后的部件特征输入到一个轻量级的MLP（<strong>Articulation Module</strong>）中，直接回归预测关节参数。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出</strong>：<ul>
<li>精细的部件几何和外观 <script type="math/tex">\{G_i\}_{i=1}^K</script>。</li>
<li>每个部件的运动参数 <script type="math/tex">\{A_i\}_{i=1}^K</script>。</li>
<li>最终的可动3D对象（由部件几何和运动参数组合而成）。</li>
</ul>
</li>
</ul>
<p><strong>模型结构</strong></p>
<ul>
<li><strong>阶段 1</strong>：主要是一个修改过的<strong>Part-Aware Flow Model</strong>，其中包含一个图像编码器、一个部件感知Transformer（具有全局和局部注意力混合）和一个稀疏解码器。</li>
<li><strong>阶段 2</strong>：包含一个<strong>稀疏Transformer</strong>（用于部件级去噪和精细几何/外观生成），以及一个<strong>Articulation Module</strong>（一个MLP，用于从Transformer的中间特征中回归预测运动参数）。</li>
</ul>
<p><strong>算法解释</strong></p>
<ul>
<li><strong>部件感知注意力 (Part-Aware Attention)</strong>：
    <script type="math/tex">H_i = z_i + \text{Attn}(z_i W^Q, z_i W^K, z_i W^V)</script>
    这里的关键在于，当对部件 <script type="math/tex">i</script> 的token进行自注意力计算时，注意力范围被限制在部件 <script type="math/tex">i</script> 内部。这使得模型能够专注于部件内部的细节，同时全局注意力层则负责捕捉部件间的整体关系。</li>
<li><strong>多步特征聚合 (Multi-step Feature Aggregation)</strong>：
    <script type="math/tex">H_t = \frac{1}{|T|}\sum_{t \in T} H_t^X</script>
    其中 <script type="math/tex">H_t^X</script> 是在时间步 <script type="math/tex">t</script> 下，部件 <script type="math/tex">i</script> 的token特征。通过对最后 <script type="math/tex">S</script> 个时间步的特征进行平均，模型能够融合不同抽象层次的信息，从而更鲁棒地预测运动参数。</li>
<li><strong>运动参数回归 (Articulation Regression)</strong>：
    <script type="math/tex">\hat{A}_i = \{\hat{t}_i, \hat{s}_i, \hat{o}_i, \hat{u}_i, \hat{p}_i\} = g_\phi(h_i)</script>
<script type="math/tex">h_i = [\text{MeanPool}(H_i) || \text{MaxPool}(H_i)]</script>
    这里，将部件 <script type="math/tex">i</script> 的所有token特征进行均值和最大值池化，得到一个固定维度的向量 <script type="math/tex">h_i</script>，然后输入到一个MLP <script type="math/tex">g_\phi</script> 中，直接输出预测的运动参数 <script type="math/tex">\hat{A}_i</script>。这是一个高效的回归方法。</li>
</ul>
<h3 id="4_3">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>生成式 vs. 检索式/优化式</strong>：PAct 是一个端到端的<strong>生成式</strong>框架，直接从单张图像生成完整的、实例特异性的可动3D对象。这与依赖预定义部件库进行检索的SINGAPO、Articulate-Anything，以及需要耗时优化的FreeArt3D等方法有本质区别。</li>
<li><strong>部件中心 vs. 整体建模</strong>：PAct 明确地将对象分解为部件，并在生成过程中显式地利用部件信息（如部件感知注意力、部件身份嵌入），而许多其他方法可能将对象视为一个整体进行建模，或在后期才进行部件分解。</li>
<li><strong>单视角前馈 vs. 多视角/耗时优化</strong>：PAct 实现了<strong>单视角、前馈</strong>的推理，速度远超基于优化的方法，并且比多视角方法更易于使用。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>部件感知生成框架</strong>：首次提出将部件分解、几何生成和运动预测统一在一个端到端的、部件中心的前馈生成框架中。</li>
<li><strong>部件感知Transformer</strong>：通过在Transformer中引入部件内局部注意力，有效结合了全局上下文和局部细节，提升了部件的生成质量。</li>
<li><strong>多步特征聚合用于运动预测</strong>：利用扩散模型生成过程中的多步中间特征来预测运动参数，提高了运动预测的准确性和鲁棒性。</li>
<li><strong>Mask-based Part Conditioning</strong>：引入了基于2D掩码的条件生成，增强了对部件分解的控制能力，解决了单视角下部件分解的歧义性。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>单张图像输入</strong>：适用于只需要提供一张图片即可生成可动3D对象的场景。</li>
<li><strong>具有清晰部件结构的对象</strong>：尤其擅长处理具有明确可动部件（如门、抽屉、柜子、椅子等）的对象。</li>
<li><strong>需要快速推理的场景</strong>：如实时交互、大规模3D资产生成等。</li>
<li><strong>需要可控部件分解的场景</strong>：通过提供部件掩码，可以引导生成特定结构的部件组合。</li>
</ul>
</li>
</ul>
<h3 id="5_3">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：在PartNet-Mobility和ACD两个标准可动对象数据集上进行了评估。</li>
<li><strong>评估指标</strong>：使用了多种指标，包括：<ul>
<li><strong>几何保真度</strong>：dgIoU (部件包围盒IoU), dcDist (部件中心距离), dCD (Chamfer Distance)。</li>
<li><strong>运动合理性</strong>：AOR (Average Overlapping Ratio，衡量部件间碰撞)。</li>
<li><strong>视觉一致性</strong>：CLIP Similarity (衡量生成对象与输入图像的视觉相似度)。</li>
</ul>
</li>
<li><strong>消融实验</strong>：对关键组件（如特征聚合步数、使用Stage 1 vs. Stage 2特征、回归 vs. 生成式运动预测）进行了详细的消融研究。</li>
<li><strong>定性比较</strong>：与SINGAPO, Articulate-Anything, FreeArt3D等方法进行了可视化比较。</li>
<li><strong>真实世界数据评估</strong>：在收集的“in-the-wild”图像上进行了评估，验证了方法的泛化能力。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>定量结果</strong>：在PartNet-Mobility和ACD数据集上，PAct 在大多数指标上（尤其是dgIoU, dcDist, dCD, CLIP）均显著优于所有基线方法，表明其在几何保真度、运动合理性和视觉一致性方面表现出色。</li>
<li><strong>定性结果</strong>：PAct 生成的对象具有更精细的几何细节、更准确的部件形状和更逼真的运动配置。与检索式方法相比，PAct 能够生成更具实例特异性的结果。与FreeArt3D相比，PAct 在单视角下也能生成更清晰、更少畸变的部件。</li>
<li><strong>CLIP Similarity</strong>：PAct 在CLIP相似度上表现最佳，证明了其生成结果与输入图像的高度视觉一致性。</li>
<li><strong>Mask-Controlled Generation</strong>：通过提供不同的部件掩码，PAct 可以生成具有不同部件组合的可动对象，展示了其可控性。</li>
<li><strong>消融实验</strong>：<ul>
<li>特征聚合步数：聚合20步效果最佳。</li>
<li>Stage 1 vs. Stage 2 特征：使用Stage 2特征进行运动预测效果显著优于Stage 1特征。</li>
<li>回归 vs. 生成式运动预测：简单的MLP回归在当前设置下效果更好。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>PartNet-Mobility 和 ACD 数据集</strong>：在这些标准数据集上，PAct 取得了最好的定量和定性结果。</li>
<li><strong>需要高视觉一致性的场景</strong>：CLIP Similarity 的优异表现表明 PAct 能很好地保留输入图像的视觉特征。</li>
<li><strong>需要实例特异性几何和运动的场景</strong>：PAct 的生成式方法使其能够捕捉到输入对象的独特细节，而非简单地检索和组合。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>多部件对象的扩展性</strong>：论文提到，模型在处理具有大量部件（超过8个）的对象时会遇到困难，这可能与模型容量、优化挑战以及数据集的偏见有关。</li>
<li><strong>未见或遮挡的部件</strong>：模型假设所有相关部件在输入图像中可见。对于被遮挡或仅在特定运动状态下才出现的部件，模型可能无法正确推断。</li>
<li><strong>非浅层树状结构</strong>：当前方法主要针对深度为1的树状结构，对于更复杂的闭链或共享约束的运动机制（如更复杂的机械臂）可能难以处理。</li>
</ul>
</li>
</ul>
<h3 id="6_3">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文作者提供了项目主页（<a href="https://PAct-project.github.io">https://PAct-project.github.io</a>），通常意味着代码会开源。在论文中也提到了使用了TRELLIS和OmniPart的代码库，这为复现提供了基础。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>预训练模型</strong>：PAct 依赖于预训练的TRELLIS和OmniPart模型，因此需要获取这些模型的检查点。</li>
<li><strong>部件掩码生成</strong>：论文中提到了使用VLM（如GPT-5）和SAM2来自动生成部件掩码，这部分是实现的关键，可能需要复杂的Prompt Engineering和模型集成。</li>
<li><strong>超参数</strong>：论文中提到了一些关键超参数，如学习率、训练步数、CFG scale、采样步数（25步）、特征聚合步数（最后20步）。</li>
<li><strong>硬件要求</strong>：训练使用了4块NVIDIA A800 GPU，表明需要较高的计算资源。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他可动对象生成任务</strong>：该框架的核心思想（部件中心生成、部件感知Transformer、多步特征聚合）可以迁移到其他需要生成可动对象的任务中，例如生成具有更复杂运动的机器人部件。</li>
<li><strong>静态3D生成</strong>：阶段2的精细几何和外观生成部分可以作为静态3D对象生成的基础。</li>
<li><strong>运动预测模块</strong>：多步特征聚合和MLP回归的运动预测方法可以独立出来，用于其他需要从图像或3D表示中预测运动参数的任务。</li>
<li><strong>部件掩码生成流程</strong>：VLM+SAM的部件掩码生成流程可以作为其他需要精细部件分割和语义理解的任务的辅助工具。</li>
</ul>
</li>
</ul>
<h3 id="7_3">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：单视角生成实例特异性、部件分解的可动3D对象。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>图像理解</strong>：用模型看懂输入图片，并利用掩码引导部件划分。</li>
<li><strong>部件粗定位</strong>：预测出每个部件的大致形状和位置。</li>
<li><strong>精细部件生成</strong>：根据粗定位，生成每个部件的详细几何和外观。</li>
<li><strong>运动参数预测</strong>：通过分析生成过程中的多步信息，预测部件如何运动。</li>
<li><strong>组合成形</strong>：将所有部件和它们的运动组合起来，形成最终的可动3D对象。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.14965v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.14965v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.14941v1'></a></p>
<h2 id="anchorweave-world-consistent-video-generation-with-retrieved-local-spatial-memories"><a href="https://arxiv.org/abs/2602.14941v1">AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories</a></h2>
<p><strong>Authors:</strong> Zun Wang, Han Lin, Jaehong Yoon, Jaemin Cho, Yue Zhang, Mohit Bansal</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析：</p>
<p><strong>论文摘要分析：AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）：</strong></p>
<p>AnchorWeave 提出了一种新颖的视频生成框架，通过引入多个局部几何记忆来解决现有方法在长时视频生成中存在的空间世界一致性问题。该框架通过局部记忆检索和多锚点编织控制器，有效弥合了不同视角下几何信息的不一致性，从而显著提升了视频生成在长期场景一致性方面的表现，同时保持了高质量的视觉效果。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>AnchorWeave 的核心创新在于其对“全局记忆”的替代以及处理“局部记忆”的方法。</p>
<ul>
<li><strong>从全局到局部记忆的转变：</strong> 传统的基于记忆的方法倾向于构建一个单一的、全局性的3D场景表示。然而，这种全局表示容易受到多视角重建过程中累积的姿态和深度估计误差的影响，导致几何不一致性，进而污染生成信号。AnchorWeave 巧妙地将单一的全局记忆替换为<strong>多个独立的、干净的局部几何记忆</strong>。这意味着它不再试图构建一个完美的整体3D模型，而是专注于利用更可靠、更局部的几何信息。</li>
<li><strong>覆盖驱动的局部记忆检索：</strong> 为了有效地利用这些局部记忆，AnchorWeave 引入了“覆盖驱动的局部记忆检索”。这意味着在生成视频的特定帧时，系统会根据目标轨迹（即摄像机的运动路径）来智能地选择最相关的局部几何记忆。这种检索方式确保了在生成过程中，模型能够访问到与当前视角和场景区域最匹配的几何信息，从而最大程度地减少不相关或错误的几何信息的影响。</li>
<li><strong>多锚点编织控制器：</strong> 即使是局部记忆，也可能存在细微的跨视图不一致性。AnchorWeave 的“多锚点编织控制器”是解决这一问题的关键。它能够学习如何整合来自多个选定局部记忆的几何信息，并<strong>协调它们之间的潜在不一致性</strong>。这可能涉及到一种注意力机制或融合策略，使得模型能够从多个局部几何“锚点”中提取出最一致、最可靠的几何线索，并将其无缝地编织到视频生成过程中。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<p>AnchorWeave 的研究对计算机视觉领域的视频生成方向具有重要的潜在影响：</p>
<ul>
<li><strong>提升长时视频生成的可信度：</strong> 长期以来，视频生成模型在保持场景的物理一致性方面存在巨大挑战，尤其是在生成较长时长的视频时。AnchorWeave 的方法直接解决了这一痛点，有望显著提升生成视频的真实感和可信度，使其在模拟现实世界场景方面更进一步。</li>
<li><strong>推动更鲁棒的3D感知与生成：</strong> 该研究表明，通过更精细地处理几何信息，即使存在一定程度的误差，也能实现高质量的生成。这可能促使研究人员重新思考如何利用不完美的3D数据进行生成，并推动更鲁棒的3D感知和生成技术的发展。</li>
<li><strong>为新一代视频生成模型奠定基础：</strong> AnchorWeave 的局部记忆和编织机制提供了一种新的范式，可以作为未来视频生成模型的基础架构。这种模块化的方法也可能使得模型更容易扩展和适应不同的场景和任务。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用：</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 在 VR/AR 中，需要生成高度逼真且空间一致的虚拟环境。AnchorWeave 的技术可以用于生成更稳定、更沉浸式的虚拟场景，减少用户因场景不一致而产生的眩晕感。</li>
<li><strong>电影和游戏制作：</strong> 自动生成长篇幅、具有连贯场景的视频内容是电影和游戏制作中的一个重要需求。AnchorWeave 的方法可以加速内容创作过程，并生成更具视觉吸引力的内容。</li>
<li><strong>机器人导航和模拟：</strong> 机器人需要在复杂环境中进行导航，而模拟环境的真实性至关重要。AnchorWeave 的技术可以用于生成更逼真的模拟场景，帮助机器人进行更有效的训练和测试。</li>
<li><strong>自动驾驶：</strong> 生成逼真的交通场景模拟对于训练自动驾驶系统至关重要。AnchorWeave 的方法可以生成具有长期一致性的道路和环境，提高模拟的有效性。</li>
<li><strong>内容创作和编辑工具：</strong> 为视频编辑和内容创作者提供更强大的工具，允许他们轻松地生成和修改具有空间一致性的视频片段。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<p>尽管摘要中强调了 AnchorWeave 的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>局部记忆的定义和边界：</strong> 摘要中提到了“局部几何记忆”，但其具体的定义、大小以及如何划分这些局部区域可能是一个挑战。如果局部记忆的划分不当，或者单个局部记忆本身包含的几何信息不足，仍然可能导致生成问题。</li>
<li><strong>检索的准确性和效率：</strong> “覆盖驱动的局部记忆检索”的有效性很大程度上取决于检索算法的准确性和效率。如果检索系统无法准确地找到最相关的局部记忆，或者检索过程过于耗时，都会影响整体性能。</li>
<li><strong>编织控制器的复杂性：</strong> “多锚点编织控制器”需要学习如何有效地整合和协调来自多个局部记忆的信息。这种学习过程可能需要大量的训练数据和复杂的模型架构，并且在处理高度复杂或动态变化的场景时，其性能可能受到限制。</li>
<li><strong>对初始3D几何的依赖：</strong> 尽管 AnchorWeave 旨在处理“misaligned global memory”，但它仍然依赖于从多视角数据中提取的局部几何信息。如果原始的3D重建质量非常差，即使是局部信息也可能存在严重问题，从而影响最终的生成效果。</li>
<li><strong>计算成本：</strong> 引入多个局部记忆、进行检索和复杂的编织控制，很可能会增加模型的计算成本，尤其是在生成长视频时。</li>
</ul>
<p>总而言之，AnchorWeave 是一项非常有前景的研究，它通过创新的局部记忆策略和编织机制，有效地解决了长时视频生成中的关键挑战。其对计算机视觉领域，特别是生成模型的研究，具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.14941v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.14941v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.14889v1'></a></p>
<h2 id="web-scale-multimodal-summarization-using-clip-based-semantic-alignment"><a href="https://arxiv.org/abs/2602.14889v1">Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment</a></h2>
<p><strong>Authors:</strong> Mounvik K, N Harshit</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.LG, cs.CV, cs.ET, cs.HC, cs.NE</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇关于“Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment”的论文。我将重点关注其方法部分的创新点、设计逻辑、优势与不足，并提供实用的分析和指导。</p>
<hr />
<h2 id="_2">论文方法分析与总结</h2>
<h3 id="1_4">1. 摘要翻译</h3>
<p>本文提出了一种名为“Web-Scale Multimodal Summarization”的轻量级框架，通过整合检索到的文本和图像数据来生成摘要。该系统接收用户定义的查询主题，并行执行网络、新闻和图像搜索。检索到的图像使用经过微调的CLIP模型进行排序，以衡量其与主题和文本的语义一致性。可选的BLIP图像描述生成功能进一步增强了仅图像摘要的多模态一致性。该框架支持可调的获取限制、语义过滤、摘要风格化以及结构化输出下载等功能。系统通过一个支持流式传输的Gradio API对外开放，并提供可控参数和预设配置。在包含500个图像-描述对和20:1对比负样本的评估中，系统取得了ROC-AUC 0.9270、F1分数0.6504和96.99%的准确率，证明了其强大的多模态对齐能力。这项工作提供了一个可配置、可部署的Web规模摘要工具，整合了语言、检索和视觉模型，构建了一个用户可扩展的流水线。</p>
<h3 id="2_4">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力</strong>：随着网络信息爆炸式增长，用户越来越需要能够整合文本和视觉信息的高效摘要工具。现有的摘要方法大多局限于单一模态（文本），忽略了图像等视觉信息提供的互补性洞察，这在新闻、教育、科研等领域尤为明显。此外，现有系统多依赖静态数据集和中心化模型，缺乏实时动态检索和个性化定制的能力。</li>
<li><strong>现有方法痛点</strong>：<ul>
<li><strong>单模态局限</strong>：忽略了视觉信息对理解和摘要的重要性。</li>
<li><strong>静态数据集</strong>：无法适应实时变化的Web内容。</li>
<li><strong>中心化模型</strong>：缺乏灵活性和可扩展性。</li>
<li><strong>缺乏跨模态对齐评估</strong>：现有工作多关注文本的流畅性和事实准确性，而忽视了图像与文本摘要之间的语义关联。</li>
<li><strong>用户控制不足</strong>：难以根据具体需求调整摘要生成过程。</li>
</ul>
</li>
<li><strong>研究假设</strong>：通过结合强大的跨模态检索（如CLIP）和图像描述生成（如BLIP）技术，可以构建一个能够实时从Web上检索、对齐并生成高质量多模态摘要的系统，并且该系统应具备高度的可配置性和可扩展性。</li>
</ul>
<h3 id="3_4">3. 方法设计详解</h3>
<p>该系统构建了一个端到端的Web规模多模态摘要流水线，其核心在于<strong>检索、对齐、生成</strong>三个关键环节。</p>
<p><strong>流水线流程总结：</strong></p>
<ol>
<li>
<p><strong>用户输入与主题定义 (User Input &amp; Topic Definition)</strong>:</p>
<ul>
<li>用户提供一个<strong>主题查询 (topic query)</strong>。这是整个流程的起点。</li>
</ul>
</li>
<li>
<p><strong>多模态数据检索 (Multimodal Data Retrieval)</strong>:</p>
<ul>
<li><strong>Web和新闻检索 (Web and News Retrieval)</strong>: 利用<strong>DuckDuckGo API</strong>，根据用户主题查询，检索相关的网页和新闻文章。</li>
<li><strong>图像检索 (Image Extraction)</strong>: 同时进行<strong>外部图像搜索</strong>，并提取网页中嵌入的图像。</li>
<li><strong>图像过滤 (Image Filtering)</strong>: 检索到的图像会根据<strong>分辨率和尺寸</strong>进行初步过滤，以保证质量。</li>
</ul>
</li>
<li>
<p><strong>数据预处理与结构化 (Data Preprocessing &amp; Structuring)</strong>:</p>
<ul>
<li><strong>文本清洗 (Text Cleaning)</strong>:<ul>
<li><strong>去重 (Deduplication)</strong>: 移除重复的文本内容。</li>
<li><strong>分段 (Segmentation)</strong>: 将文本内容分割成更小的、可管理的单元（段落或句子）。</li>
<li><strong>结构化组织 (Structured Organization)</strong>: 将文本段落组织起来，为后续的评分做准备。</li>
</ul>
</li>
<li><strong>可选BLIP图像描述生成 (Optional BLIP Captioning)</strong>:<ul>
<li>对于检索到的图像，可以选择使用<strong>BLIP模型</strong>为其生成描述性标题（caption）。</li>
<li><strong>目的</strong>：增强图像的语义信息，使其更容易与文本内容进行对齐。</li>
<li><strong>扩展性</strong>：文中提到，该BLIP模型可以被更强大的指令微调的视觉语言模型（VLMs）如BLIP-2或InstructBLIP替换，以获得更具描述性、更上下文感知的标题。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>跨模态语义对齐与评分 (Cross-Modal Semantic Alignment &amp; Scoring)</strong>:</p>
<ul>
<li><strong>核心模型</strong>：使用一个<strong>在500个图像-描述对上微调的CLIP模型</strong>。</li>
<li><strong>对齐机制</strong>：<ul>
<li>CLIP模型通过其<strong>编码器</strong>，将用户主题查询、文本段落以及图像（或其描述）映射到同一个语义空间。</li>
<li>计算<strong>查询主题与文本段落的语义相似度</strong>。</li>
<li>计算<strong>查询主题与图像（或其描述）的语义相似度</strong>。</li>
</ul>
</li>
<li><strong>多模态评分约束 (Multimodal Scoring Constraints)</strong>:<ul>
<li>引入一个<strong>权重超参数 <code>α</code></strong> 来平衡文本和图像的贡献度。</li>
<li>当 <code>α = 1.0</code> 时，仅考虑文本相关性。</li>
<li>当 <code>α = 0.0</code> 时，仅优先考虑图像-描述对齐。</li>
<li><code>0 &lt; α &lt; 1</code> 的中间值允许对两种模态进行精细的平衡。</li>
<li>最终的<strong>段落分数</strong>是根据这个 <code>α</code> 值加权计算得出的。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>内容选择与摘要生成 (Content Selection &amp; Summary Generation)</strong>:</p>
<ul>
<li><strong>文本段落排序 (Text Passage Ranking)</strong>: 根据计算出的分数，对文本段落进行<strong>相关性和多样性</strong>排序。</li>
<li><strong>内容池化 (Content Pooling)</strong>: 根据可配置的阈值（如最小分数阈值）选择最相关的文本段落和图像。</li>
<li><strong>摘要生成器 (Summary Generator)</strong>: 利用选定的内容，生成一个<strong>混合的多模态摘要</strong>。</li>
<li><strong>输出格式 (Output Formats)</strong>: 支持多种输出格式，包括Markdown、JSON，以及可下载的文件。</li>
</ul>
</li>
<li>
<p><strong>用户控制与API接口 (User Control &amp; API Interface)</strong>:</p>
<ul>
<li><strong>可配置参数 (Configurable Parameters)</strong>:<ul>
<li><strong>Segment Limit</strong>: 限制最终摘要中包含的顶级片段数量（Top-K）。</li>
<li><strong>Minimum Score Threshold</strong>: 过滤掉分数低于此阈值的片段。</li>
<li><strong>Image Resize &amp; Caching</strong>: 控制图像输入的质量和缓存策略。</li>
<li><strong>Fast Mode</strong>: 一个快速模式开关，通过限制内容深度和跳过图像描述生成来减少延迟。</li>
</ul>
</li>
<li><strong>API暴露 (API Exposure)</strong>: 系统通过一个<strong>Gradio API</strong>对外提供服务，支持流式传输，允许用户通过可控参数和预设配置来定制摘要生成过程。</li>
</ul>
</li>
</ol>
<p><strong>模型结构与算法解释：</strong></p>
<ul>
<li><strong>CLIP模型 (CLIP Model)</strong>:<ul>
<li><strong>核心功能</strong>：用于计算文本和图像之间的语义相似度。通过在大量图像-文本对上进行对比学习，CLIP模型能够理解文本描述与图像内容之间的关联。</li>
<li><strong>微调 (Fine-tuning)</strong>: 论文中提到，CLIP模型是在500个从真实网络查询中提取的图像-描述对上进行了微调。这种微调的目的是使其更好地适应Web规模、可能存在噪声的数据，并提升其在评估任务中的判别准确性。</li>
<li><strong>负采样 (Negative Sampling)</strong>: 在微调过程中使用了负采样技术，即用不相关的图像-描述对来训练模型区分正负样本，从而提高其对语义关联的敏感度。</li>
</ul>
</li>
<li><strong>BLIP模型 (BLIP Model)</strong>:<ul>
<li><strong>核心功能</strong>：用于为图像生成文本描述（caption）。这使得系统能够处理纯图像输入，并将其转化为可与文本进行语义比较的信息。</li>
<li><strong>作用</strong>：在生成仅图像摘要时，BLIP生成的描述可以作为图像的“文本表示”，从而实现多模态一致性。</li>
</ul>
</li>
<li><strong><code>α</code> 超参数 (Alpha Hyperparameter)</strong>:<ul>
<li>这是该方法的一个关键创新点，它提供了一种<strong>动态平衡文本和视觉信息</strong>的机制。通过调整<code>α</code>，用户可以控制摘要生成过程中对文本内容和图像内容的侧重程度。这使得系统能够适应不同类型的查询和用户偏好。例如，对于需要视觉证据支持的查询，可以调低<code>α</code>以更侧重图像；对于纯信息性查询，可以调高<code>α</code>以更侧重文本。</li>
</ul>
</li>
</ul>
<h3 id="4_4">4. 方法对比分析</h3>
<ul>
<li><strong>本质区别</strong>：<ul>
<li><strong>实时Web规模检索 vs. 静态数据集</strong>：大多数现有方法（如VMSMO, MSMO）依赖于预先构建的、固定的多模态数据集。本文提出的方法能够实时从互联网上检索信息，这使其更具动态性和时效性。</li>
<li><strong>端到端多模态对齐与生成 vs. 独立模块</strong>：本文将检索、跨模态对齐评分和摘要生成无缝集成在一个流水线中。CLIP模型不仅用于检索到的图像排序，还用于文本段落的评分，确保了整个摘要生成过程都受到跨模态语义对齐的指导。</li>
<li><strong>可控的模态平衡机制</strong>：通过<code>α</code>参数，用户可以精细控制文本和视觉信息在摘要生成中的权重，这是许多现有方法所不具备的。</li>
<li><strong>零样本/少样本对齐能力</strong>：CLIP模型本身具有强大的零样本学习能力，即使在微调数据之外的主题，也能进行有效的语义对齐。</li>
</ul>
</li>
<li><strong>创新贡献</strong>：<ul>
<li><strong>Web规模多模态摘要框架</strong>：首次提出一个能够实时从Web检索并生成多模态摘要的完整框架。</li>
<li><strong>CLIP驱动的语义对齐</strong>：将CLIP模型应用于Web规模多模态摘要的语义对齐和内容评分，显著提升了摘要的跨模态一致性。</li>
<li><strong>灵活的模态平衡控制</strong>：通过<code>α</code>参数，实现了文本和视觉信息在摘要生成中的动态权重调整。</li>
<li><strong>可部署的API接口</strong>：提供了一个易于访问和集成的Gradio API，降低了研究和应用的门槛。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li><strong>新闻摘要</strong>：整合新闻文本和相关图片，生成更全面的报道摘要。</li>
<li><strong>产品评论/信息聚合</strong>：结合产品描述、用户评论和产品图片，生成更直观的总结。</li>
<li><strong>教育内容生成</strong>：为学习材料整合文本解释和示意图。</li>
<li><strong>科研文献辅助阅读</strong>：快速提取研究要点，并关联相关图表。</li>
<li><strong>任何需要整合文本和视觉信息的场景</strong>，特别是当信息来源分散在Web上时。</li>
</ul>
</li>
</ul>
<h3 id="5_4">5. 实验分析</h3>
<ul>
<li><strong>验证方法</strong>：<ul>
<li><strong>评估指标</strong>：主要关注<strong>语义对齐</strong>的评估，而非摘要的语言流畅性。使用了以下指标：<ul>
<li><strong>Accuracy</strong>: 整体匹配/不匹配对的正确率。</li>
<li><strong>Precision &amp; Recall</strong>: 评估错误匹配和召回能力。</li>
<li><strong>F1-Score</strong>: Precision和Recall的调和平均值。</li>
<li><strong>ROC-AUC</strong>: 衡量评分模型在不同阈值下的区分能力。</li>
<li><strong>PR-AUC</strong>: 适用于评估高度不平衡的正负样本对。</li>
<li><strong>Ranking Performance</strong>: Top-K准确率和位置召回率，评估相关内容被优先排序的能力。</li>
</ul>
</li>
<li><strong>实验设置</strong>：<ul>
<li><strong>数据集</strong>：构建了一个包含500个<strong>正样本图像-描述对</strong>和大量<strong>负样本（20:1对比负样本）</strong>的对比测试集，模拟高噪声的Web场景。</li>
<li><strong>基线方法 (Baseline Methods)</strong>:<ul>
<li>纯文本摘要模型（如BERTSUM, PEGASUS）。</li>
<li>对比了微调前后的CLIP模型在对齐任务上的表现。</li>
<li>对比了使用BLIP图像描述生成与否的影响。</li>
<li>与使用静态数据集的模型（VMSMO, MSMO）以及现代MLLMs（如Qwen-VL）进行了对比。</li>
</ul>
</li>
<li><strong>消融实验 (Ablation Study)</strong>:<ul>
<li><strong>Baseline Methods</strong>: 评估纯文本摘要的性能。</li>
<li><strong>Semantic Matching Models</strong>: 比较不同CLIP模型（预训练 vs. 微调）的效果。</li>
<li><strong>Visual Caption Integration</strong>: 评估BLIP图像描述生成的作用。</li>
<li><strong>Multimodal Scoring Constraints</strong>: 通过改变<code>α</code>值，分析文本和视觉模态在摘要生成中的贡献度。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>高准确率和对齐能力</strong>：最终模型在评估集上达到了<strong>96.99%的准确率</strong>，ROC-AUC为<strong>0.9270</strong>，F1-Score为<strong>0.6504</strong>。这表明系统在对齐文本和视觉信息方面表现出色。</li>
<li><strong>多模态优势</strong>：与纯文本基线模型相比，多模态方法显著提高了检索精度，并且没有牺牲语义对齐。</li>
<li><strong><code>α</code>参数的重要性</strong>：通过改变<code>α</code>值，可以观察到不同模态对最终摘要质量的影响，证明了其平衡机制的有效性。</li>
</ul>
</li>
<li><strong>优势场景</strong>：<ul>
<li>在<strong>高噪声、信息分散的Web场景</strong>下表现尤为突出，能够有效过滤无关信息，提取关键内容。</li>
<li>在需要<strong>强视觉证据支持</strong>的查询中，通过BLIP描述生成和CLIP对齐，能够生成更具说服力的摘要。</li>
</ul>
</li>
<li><strong>局限性</strong>：<ul>
<li><strong>计算开销</strong>：实时检索和CLIP/BLIP模型的推理会带来一定的计算开销，尤其是在Fast Mode关闭的情况下。</li>
<li><strong>对CLIP/BLIP模型的依赖</strong>：模型的性能在很大程度上依赖于CLIP和BLIP模型的预训练质量和微调效果。</li>
<li><strong>摘要的语言流畅性</strong>：论文的评估侧重于语义对齐，并未深入评估生成摘要的语言流畅性、连贯性和可读性。虽然CLIP和BLIP模型在语言理解方面有进步，但生成摘要的质量仍可能受限于下游的摘要生成器。</li>
<li><strong>“噪声”的定义</strong>：虽然系统能处理“噪声”，但对于非常规或低质量的网络内容，其处理能力仍有待进一步验证。</li>
</ul>
</li>
</ul>
<h3 id="6_4">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文中提到“The entire pipeline is publicly accessible via a streaming-capable API and Gradio interface”，这暗示了系统可能以某种形式（如API服务）对公众开放，但具体代码是否开源需要进一步确认。通常，学术论文会附带代码链接。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>DuckDuckGo API</strong>: 需要获取并使用DuckDuckGo的API密钥来执行网络和新闻检索。</li>
<li><strong>CLIP模型微调</strong>:<ul>
<li><strong>数据集</strong>: 500个图像-描述对，以及20:1的负样本。这些数据需要从真实的网络查询中提取和标注。</li>
<li><strong>模型选择</strong>: 可以使用Hugging Face Transformers等库中预训练的CLIP模型作为起点，然后根据论文描述进行微调。</li>
<li><strong>训练参数</strong>: 需要仔细调整学习率、批大小、优化器等参数，以获得最佳的微调效果。</li>
</ul>
</li>
<li><strong>BLIP模型</strong>: 可以使用预训练的BLIP模型进行图像描述生成。如果需要更强的性能，可以考虑使用BLIP-2或InstructBLIP。</li>
<li><strong><code>α</code>参数调优</strong>: <code>α</code>值的选择对最终摘要质量至关重要。建议根据具体应用场景进行实验，找到最优的<code>α</code>值。</li>
<li><strong>Gradio API</strong>: 部署系统时，可以利用Gradio快速构建一个用户友好的Web界面，方便用户交互和测试。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>迁移到其他模态</strong>：该框架的设计具有良好的模块化，理论上可以替换检索模块（如使用其他搜索引擎API）或集成其他模态（如视频、音频）的检索和编码器。</li>
<li><strong>迁移到其他任务</strong>：<ul>
<li><strong>多模态问答 (Multimodal QA)</strong>: 检索到的信息可以作为上下文，用于回答关于特定主题的问题。</li>
<li><strong>多模态信息检索 (Multimodal Information Retrieval)</strong>: 核心的检索和对齐机制可以用于更广泛的信息检索任务。</li>
<li><strong>跨模态推荐 (Cross-Modal Recommendation)</strong>: 结合用户偏好和内容（文本+图像）的语义信息进行推荐。</li>
</ul>
</li>
<li><strong>关键在于</strong>：保持跨模态对齐的核心思想，并根据新任务调整输入、输出和评估指标。</li>
</ul>
</li>
</ul>
<h3 id="7_4">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：实时Web检索+CLIP对齐，生成多模态摘要。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>用户输入主题</strong>。</li>
<li><strong>Web/新闻/图像检索</strong>。</li>
<li><strong>CLIP/BLIP对齐评分</strong>。</li>
<li><strong>加权选择内容生成摘要</strong>。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources.</li>
<li>Given a user-defined topic, the system performs parallel web, news, and image searches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.14889v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.14889v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.14837v1'></a></p>
<h2 id="integrating-affordances-and-attention-models-for-short-term-object-interaction-anticipation"><a href="https://arxiv.org/abs/2602.14837v1">Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation</a></h2>
<p><strong>Authors:</strong> Lorenzo Mur Labadia, Ruben Martinez-Cantin, Jose J. Guerrero, Giovanni M. Farinella, Antonino Furnari</p>
<p><strong>Published:</strong> 2026-02-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文分析：Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了一种新颖的基于注意力机制的架构（STAformer 和 STAformer++），通过结合图像-视频对、帧引导的时间池化、双向图像-视频注意力以及多尺度特征融合，显著提升了短期物体交互预测（STA）的性能。更重要的是，论文引入了两个新颖的模块来增强 STA 的可解释性和鲁棒性：一个环境交互先验模型（affordance model）作为场景的持久记忆，以及一个基于手部和物体轨迹预测交互热点的模块，从而更准确地定位和预测用户意图。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li>
<p><strong>STAformer 和 STAformer++ 架构：</strong> 这是论文的核心技术贡献。</p>
<ul>
<li><strong>帧引导的时间池化 (Frame-guided temporal pooling):</strong> 这种技术可能意味着模型能够根据帧的内容和重要性来动态地聚合时间信息，而不是简单地平均或最大池化，从而更好地捕捉交互的动态过程。</li>
<li><strong>双向图像-视频注意力 (Dual image-video attention):</strong> 这种设计允许模型同时关注当前帧的视觉信息和视频序列中的时序信息，并且可能是在图像和视频之间进行相互增强的注意力机制，这对于理解物体交互的上下文至关重要。</li>
<li><strong>多尺度特征融合 (Multiscale feature fusion):</strong> 结合不同尺度的特征可以帮助模型捕捉到从局部细节（如手部姿势）到全局场景（如物体摆放）的丰富信息，这对于准确预测交互至关重要。</li>
<li><strong>图像-视频对输入:</strong> 明确指出模型可以处理图像和视频的组合输入，这为在不同场景下（例如，只有静态图像或视频流）应用提供了灵活性。</li>
</ul>
</li>
<li>
<p><strong>环境交互先验模型 (Environment affordance model):</strong></p>
<ul>
<li><strong>持久记忆:</strong> 将场景中可能发生的交互（affordances）建模为一种持久的记忆，这意味着模型能够学习到特定环境的固有属性，例如在厨房里，刀具和砧板的交互概率很高。这为预测提供了强大的先验知识，减少了对纯粹从视频数据中学习的依赖。</li>
<li><strong>自适应融合:</strong> 论文探索了两种融合方式：简单的晚期融合和一种能够自适应学习如何最佳融合先验知识与端到端预测的方法。自适应融合是关键，它意味着模型可以根据当前场景和视频内容动态地调整先验知识的重要性，而不是僵化地应用。</li>
</ul>
</li>
<li>
<p><strong>交互热点预测 (Interaction hotspots prediction):</strong></p>
<ul>
<li><strong>基于手部和物体轨迹:</strong> 通过分析手部和物体的运动轨迹来预测交互发生的“热点”区域，这是一种非常直观且有效的方法。手部是执行交互的主要执行者，其轨迹直接指示了意图，而物体的轨迹则反映了其被交互的可能性。</li>
<li><strong>增加预测置信度:</strong> 将热点预测与整体 STA 预测相结合，可以显著提高预测的局部化精度和置信度，尤其是在交互发生的确切位置。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升 STA 性能的标杆:</strong> 论文在 Ego4D 和 EPIC-Kitchens 数据集上取得了显著的性能提升（+23p.p 和 +31p.p 的 Overall Top-5 mAP），这表明其方法在短期物体交互预测领域具有突破性。这将促使该领域的研究者们采用或借鉴其架构和模块。</li>
<li><strong>引入更具可解释性的 STA 模型:</strong> 通过引入 affordance model，论文为 STA 模型增加了“常识性”知识，使其预测不再仅仅是黑箱操作，而是能够部分解释为什么会做出某个预测（例如，因为这个场景通常会发生这种交互）。</li>
<li><strong>推动更鲁棒的交互理解:</strong> 结合视觉信息、时序信息、环境先验和运动轨迹，该方法有望构建出更鲁棒、更准确的交互理解系统，即使在复杂或不完整的视频数据中也能表现良好。</li>
<li><strong>促进可穿戴设备和人机交互的发展:</strong> 论文明确指出了其在可穿戴助手和人机交互中的应用价值。更准确的交互预测意味着更及时、更智能的辅助，这将极大地提升用户体验和机器人协作的效率。</li>
<li><strong>开源贡献:</strong> 论文承诺开源代码、标注和预提取的 affordances，这将极大地加速该领域的研究进展，降低其他研究者的门槛。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>智能助手和可穿戴计算:</strong> 如论文所述，用于理解用户意图，提供主动式、情境感知的帮助。例如，在用户拿起工具前就提供相关信息或建议。</li>
<li><strong>人机协作机器人:</strong> 机器人可以预测人类操作员的下一步动作，从而更安全、更高效地协同工作。例如，在装配线上，机器人可以预测工人将要抓取的零件。</li>
<li><strong>自动驾驶:</strong> 预测行人、骑行者或其他车辆的意图，尤其是在复杂的城市环境中，可以提高驾驶安全性。</li>
<li><strong>视频理解和内容分析:</strong> 自动识别视频中的关键交互事件，用于内容检索、事件检测或视频摘要。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 在虚拟环境中，预测用户与虚拟物体的交互，以提供更自然的交互体验。</li>
<li><strong>老年人或残障人士辅助:</strong> 预测用户可能需要的帮助，并及时提供支持。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算复杂度:</strong> 引入多尺度特征融合、双向注意力以及环境 affordance 模型，很可能会增加模型的计算复杂度和内存需求，这可能对实时性要求极高的应用构成挑战。</li>
<li><strong>对 affordance 模型的依赖和泛化能力:</strong> 环境 affordance 模型需要预先训练或构建。其泛化能力如何，能否很好地适应全新的、未见过或非常规的场景，是一个潜在的问题。如果 affordance 模型构建不当，可能会引入错误的先验知识，反而影响预测。</li>
<li><strong>数据依赖性:</strong> 尽管引入了 affordance，但模型的性能仍然高度依赖于训练数据的质量和数量。Ego4D 和 EPIC-Kitchens 是大型数据集，但对于特定领域或更细粒度的交互，可能需要更多定制化的数据。</li>
<li><strong>“短期”的定义:</strong> 摘要中提到“Short-Term Object Interaction Anticipation”，但“短期”的具体时间范围并未明确定义。这可能意味着模型在预测更长期的交互时性能会下降。</li>
<li><strong>对“主动对象”的定义:</strong> 论文提到“detecting the location of the next active objects”。“主动对象”的定义和识别可能是一个挑战，尤其是在场景中存在多个潜在交互对象时。</li>
<li><strong>对“时间到接触”的精度:</strong> 虽然提到了预测“time to contact”，但摘要并未详细说明其预测精度，这可能是评估模型性能的一个重要维度。</li>
</ul>
<p>总而言之，这篇论文在短期物体交互预测领域提出了非常有前景的方法，通过结合先进的注意力机制和引入 affordance 概念，显著提升了预测的准确性和一定程度的可解释性。其开源贡献也预示着将对该领域产生积极而深远的影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present a method to improve the performance of STA predictions.</li>
<li>Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances.</li>
<li>Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.14837v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.14837v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-02-17 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
