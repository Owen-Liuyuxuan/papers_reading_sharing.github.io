<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-02-06 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-13
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-02-04/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-02-09/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-02-06">Arxiv Computer Vision Papers - 2026-02-06</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#generative-modeling-via-drifting" class="nav-link">Generative Modeling via Drifting</a>
                </li>
                <li class="nav-item">
                    <a href="#shared-lora-subspaces-for-almost-strict-continual-learning" class="nav-link">Shared LoRA Subspaces for almost Strict Continual Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#shared-lora-subspaces-for-almost-strict-continual-learning_1" class="nav-link">论文方法分析与总结：《Shared LoRA Subspaces for almost Strict Continual Learning》</a>
                </li>
                <li class="nav-item">
                    <a href="#pseudo-invertible-neural-networks" class="nav-link">Pseudo-Invertible Neural Networks</a>
                </li>
                <li class="nav-item">
                    <a href="#pseudo-invertible-neural-networks_1" class="nav-link">论文方法分析与总结：Pseudo-Invertible Neural Networks</a>
                </li>
                <li class="nav-item">
                    <a href="#predicting-camera-pose-from-perspective-descriptions-for-spatial-reasoning" class="nav-link">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#predicting-camera-pose-from-perspective-descriptions-for-spatial-reasoning_1" class="nav-link">论文方法分析与总结：Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#swimbird-eliciting-switchable-reasoning-mode-in-hybrid-autoregressive-mllms" class="nav-link">SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#_4" class="nav-link">论文方法分析与总结</a>
                </li>
                <li class="nav-item">
                    <a href="#thinking-with-geometry-active-geometry-integration-for-spatial-reasoning" class="nav-link">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</a>
                </li>
                <li class="nav-item">
                    <a href="#v-retrver-evidence-driven-agentic-reasoning-for-universal-multimodal-retrieval" class="nav-link">V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#splat-and-distill-augmenting-teachers-with-feed-forward-3d-reconstruction-for-3d-aware-distillation" class="nav-link">Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation</a>
                </li>
                <li class="nav-item">
                    <a href="#splat-and-distill-augmenting-teachers-with-feed-forward-3d-reconstruction-for-3d-aware-distillation_1" class="nav-link">论文方法分析与总结：《SPLAT AND DISTILL: AUGMENTING TEACHERS WITH FEED-FORWARD 3D RECONSTRUCTION FOR 3D-AWARE DISTILLATION》</a>
                </li>
                <li class="nav-item">
                    <a href="#context-forcing-consistent-autoregressive-video-generation-with-long-context" class="nav-link">Context Forcing: Consistent Autoregressive Video Generation with Long Context</a>
                </li>
                <li class="nav-item">
                    <a href="#context-forcing-consistent-autoregressive-video-generation-with-long-context_1" class="nav-link">论文方法分析与总结：《Context Forcing: Consistent Autoregressive Video Generation with Long Context》</a>
                </li>
                <li class="nav-item">
                    <a href="#genarena-how-can-we-achieve-human-aligned-evaluation-for-visual-generation-tasks" class="nav-link">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</a>
                </li>
                <li class="nav-item">
                    <a href="#genarena" class="nav-link">论文方法分析与总结：GenArena</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-02-06">Arxiv Computer Vision Papers - 2026-02-06</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对您提供的 Arxiv 计算机视觉论文列表的简明执行摘要，旨在帮助忙碌的研究人员快速了解该领域的最新进展。</p>
<hr />
<p><strong>执行摘要：2026年2月4日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>生成模型、持续学习、多模态理解与推理、以及三维视觉</strong>等方面的持续探索。特别值得关注的是，研究人员正积极寻求更高效、更具泛化能力和更符合人类认知的方式来处理视觉信息。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>生成模型与三维重建的融合：</strong> "Generative Modeling via Drifting" 和 "Splat and Distill" 均在生成模型方面有所突破，前者探索了新的生成范式，后者则通过蒸馏技术提升了三维感知能力，预示着生成模型在三维场景理解和合成方面将有更广阔的应用。</li>
<li><strong>多模态推理与空间理解：</strong> "Predicting Camera Pose from Perspective Descriptions"、"SwimBird" 和 "Thinking with Geometry" 共同指向了多模态模型在空间推理方面的进步。通过结合几何信息、视角描述以及切换推理模式，模型正变得更能理解和处理复杂的空间关系。</li>
<li><strong>持续学习与模型效率：</strong> "Shared LoRA Subspaces for almost Strict Continual Learning" 提出了一种在持续学习中保持模型性能和效率的新方法，这对于模型在动态环境中部署至关重要。</li>
<li><strong>评估与对齐：</strong> "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?" 提出了一个重要的问题，即如何实现与人类认知对齐的视觉生成任务评估，这对于推动生成模型向更实用、更符合用户期望的方向发展具有指导意义。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>几何驱动的推理：</strong> 将几何原理和主动几何集成到模型中，以增强空间推理能力，是本期论文中的一个突出趋势。</li>
<li><strong>可切换的推理模式：</strong> "SwimBird" 提出的模型能够根据任务需求切换不同的推理模式，这为构建更灵活、更强大的多模态模型提供了思路。</li>
<li><strong>基于证据的代理推理：</strong> "V-Retrver" 提出的方法通过证据驱动的代理推理，实现了通用多模态检索，预示着更智能、更具解释性的检索系统。</li>
<li><strong>伪逆神经网络：</strong> "Pseudo-Invertible Neural Networks" 探索了具有伪逆特性的神经网络，这可能为模型的可解释性、鲁棒性或高效训练带来新的可能性。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在影响和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>"Generative Modeling via Drifting"</strong>: 探索新的生成模型范式，可能带来突破性的生成能力。</li>
<li><strong>"SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs"</strong>: 在多模态模型推理能力和灵活性方面具有重要意义。</li>
<li><strong>"Thinking with Geometry: Active Geometry Integration for Spatial Reasoning"</strong>: 几何在空间推理中的应用是当前多模态研究的热点。</li>
<li><strong>"GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?"</strong>: 对于评估和指导未来生成模型的研究方向至关重要。</li>
<li><strong>"Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation"</strong>: 在三维视觉和模型蒸馏方面提供了新的技术路径。</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2602.04770v1">Generative Modeling via Drifting</a></li>
<li><a href="#2602.06043v1">Shared LoRA Subspaces for almost Strict Continual Learning</a></li>
<li><a href="#2602.06042v1">Pseudo-Invertible Neural Networks</a></li>
<li><a href="#2602.06041v1">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</a></li>
<li><a href="#2602.06040v1">SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs</a></li>
<li><a href="#2602.06037v1">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</a></li>
<li><a href="#2602.06034v1">V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</a></li>
<li><a href="#2602.06032v1">Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation</a></li>
<li><a href="#2602.06028v1">Context Forcing: Consistent Autoregressive Video Generation with Long Context</a></li>
<li><a href="#2602.06013v1">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2602.04770v1'></a></p>
<h2 id="generative-modeling-via-drifting"><a href="https://arxiv.org/abs/2602.04770v1">Generative Modeling via Drifting</a></h2>
<p><strong>Authors:</strong> Mingyang Deng, He Li, Tianhong Li, Yilun Du, Kaiming He</p>
<p><strong>Published:</strong> 2026-02-04</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Generative Modeling via Drifting</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本论文提出了一种名为“Drifting Models”的新型生成模型范式。该范式通过在训练过程中动态演化样本的分布，实现了自然的一步推理（one-step inference）。其核心在于引入一个“漂移场”（drifting field），引导样本向目标数据分布收敛，并在分布匹配时达到平衡。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>核心创新：动态分布演化与一步推理。</strong> 与传统的扩散模型（diffusion models）或流模型（flow-based models）在推理时需要多步迭代才能从噪声生成样本不同，Drifting Models 在训练阶段就学习了如何使样本分布在一步内达到目标。</li>
<li><strong>方法论：漂移场（Drifting Field）。</strong> 论文引入了一个“漂移场”，这是一个由神经网络参数化的函数，它定义了样本在潜在空间中的运动方向和速度。这个漂移场的作用是驱动样本分布朝着与真实数据分布匹配的方向“漂移”，直到达到一个平衡状态，此时生成样本的分布就近似于真实数据分布。</li>
<li><strong>训练目标：基于分布演化的优化。</strong> 训练目标的设计使得神经网络优化器能够通过调整漂移场来“演化”样本的分布，从而学习到能够实现一步推理的映射。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>颠覆性的推理速度：</strong> 一步推理是生成模型领域的一个重要目标，因为它极大地提高了生成速度，使得实时生成或大规模生成成为可能。如果Drifting Models能够稳定且高质量地实现一步推理，将对现有生成模型（如扩散模型）的效率构成挑战。</li>
<li><strong>新的生成模型范式：</strong> 提出了一种全新的生成模型框架，不同于现有的基于ODE/SDE或连续流的生成方法，为生成模型的研究开辟了新的方向。</li>
<li><strong>理论与实践的结合：</strong> 将分布匹配的理论概念（如推前分布）与具体的神经网络实现（漂移场）相结合，提供了一种新的视角来理解和构建生成模型。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>图像生成：</strong> 摘要中明确提到了在ImageNet上的实验结果，表明其在高质量图像生成方面具有潜力。</li>
<li><strong>视频生成：</strong> 如果能够扩展到时序数据，一步生成高质量视频将是巨大的突破。</li>
<li><strong>3D内容生成：</strong> 同样可以应用于生成3D模型、点云等。</li>
<li><strong>数据增强：</strong> 高效的生成能力可以用于生成更多样化的训练数据。</li>
<li><strong>科学模拟：</strong> 在物理、化学等领域，生成逼真的模拟数据可以加速研究进程。</li>
<li><strong>对抗性攻击与防御：</strong> 理解和控制数据分布的演化对于生成对抗性样本或设计鲁棒模型至关重要。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>泛化能力与稳定性：</strong> 尽管在ImageNet上取得了SOTA结果，但摘要并未提及模型在不同数据集、不同分辨率或不同模态上的泛化能力。一步推理的稳定性（即是否总是能收敛到高质量的样本）也需要进一步验证。</li>
<li><strong>训练复杂度：</strong> 虽然推理一步，但训练过程可能仍然复杂。摘要中提到“neural network optimizer to evolve the distribution”，这暗示了训练过程可能需要精细的调优。</li>
<li><strong>理论保证：</strong> 摘要侧重于实验结果，关于漂移场如何保证分布匹配的理论分析可能需要进一步的论文内容来阐述。例如，是否能保证全局最优解，或者收敛到真实数据分布的充分必要条件是什么。</li>
<li><strong>“漂移场”的定义与实现细节：</strong> 摘要中“drifting field”是一个核心概念，但其具体的数学形式、神经网络架构以及如何有效地学习它，这些细节在摘要中并未完全披露，是理解其技术细节的关键。</li>
<li><strong>与现有方法的比较：</strong> 虽然提到了SOTA结果，但摘要并未详细说明与哪些具体模型（如最新的扩散模型变体）进行了比较，以及在哪些方面（如样本质量、多样性、计算效率）具有绝对优势。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文的摘要展示了一个非常有前景的生成模型新方向。其核心吸引力在于<strong>“动态分布演化”</strong>和<strong>“一步推理”</strong>的结合，这有望解决当前许多生成模型在推理速度上的瓶颈。通过引入<strong>“漂移场”</strong>这一新颖的概念，作者提供了一种新的机制来学习从噪声到数据的映射。如果其在实践中能够稳定且高效地实现，将对生成模型领域产生重大影响，尤其是在需要快速生成高质量数据的应用场景下。然而，如同所有新兴技术一样，其泛化能力、训练稳定性以及理论基础的完善程度，将是未来研究和应用的关键考量点。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference.</li>
<li>We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match.</li>
<li>In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space.</li>
<li>We hope that our work opens up new opportunities for high-quality one-step generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.04770v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.04770v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06043v1'></a></p>
<h2 id="shared-lora-subspaces-for-almost-strict-continual-learning"><a href="https://arxiv.org/abs/2602.06043v1">Shared LoRA Subspaces for almost Strict Continual Learning</a></h2>
<p><strong>Authors:</strong> Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域高水平研究生，深入分析您提供的论文，重点关注其方法部分的创新点、设计逻辑、优势与不足，并以结构化的方式呈现。</p>
<hr />
<h2 id="shared-lora-subspaces-for-almost-strict-continual-learning_1">论文方法分析与总结：《Shared LoRA Subspaces for almost Strict Continual Learning》</h2>
<h3 id="1">1. 摘要翻译</h3>
<p><strong>论文题目：</strong> 共享 LoRA 子空间用于近乎严格的持续学习</p>
<p><strong>摘要：</strong>
高效且持续地适应大型预训练模型以应对新任务，对于实际部署至关重要，但由于灾难性遗忘和高昂的重新训练成本而面临挑战。虽然参数高效的微调方法（如 LoRA）可以降低计算需求，但它们缺乏严格持续学习和知识整合的机制，并且通常依赖于数据回放或多个适配器。我们提出 Share，一种新颖的参数高效持续微调方法，它学习并动态更新一个单一的、共享的低秩子空间，从而实现跨多个任务和模态的无缝适应。Share 构建了一个基础子空间，该子空间提取过去任务的核心知识，并通过识别关键子空间方向来逐步整合新信息。每个新任务的知识都被纳入这个不断演进的子空间，从而促进前向知识迁移，同时最大限度地减少灾难性干扰。该方法实现了高达 100 倍的参数缩减和 281 倍的内存节省（相比于传统 LoRA 方法），性能与联合训练的模型相当。一个单一的 Share 模型可以取代数百个特定任务的 LoRA 适配器，支持可扩展、异步的持续学习。在图像分类、自然语言理解、3D 姿态估计和文本到图像生成方面的实验验证了其有效性，使 Share 成为大规模 AI 系统中终身学习的实用且可扩展的解决方案。</p>
<h3 id="2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>持续学习的必要性</strong>：大型预训练模型在实际应用中需要不断适应新任务和新数据，而传统的微调方法成本高昂且容易遗忘。</li>
<li><strong>参数高效微调（PEFT）的局限性</strong>：LoRA 等 PEFT 方法虽然降低了计算和存储成本，但并未提供有效的持续学习机制，容易导致灾难性遗忘，并且需要为每个任务维护独立的适配器，这在任务数量庞大时变得不可持续。</li>
<li><strong>知识整合与迁移的需求</strong>：希望模型能够从过去的任务中学习并整合知识，以促进新任务的学习，并避免遗忘。</li>
<li><strong>对“严格持续学习”的追求</strong>：希望在不依赖数据回放、不增加模型大小、不引入额外模型的情况下实现持续学习，这更符合人类的学习方式。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>灾难性遗忘 (Catastrophic Forgetting)</strong>：模型在学习新任务时会遗忘旧任务的知识。</li>
<li><strong>高昂的计算和存储成本</strong>：全参数微调或为每个任务训练独立适配器需要大量资源。</li>
<li><strong>缺乏知识整合机制</strong>：现有 PEFT 方法（如 LoRA）通常是任务独立的，难以实现跨任务的知识共享和迁移。</li>
<li><strong>对数据回放的依赖</strong>：许多持续学习方法需要访问旧数据，这在实际部署中往往不可行。</li>
<li><strong>模型数量爆炸</strong>：为每个任务维护一个独立的适配器会导致模型数量急剧增长，难以管理和部署。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>共享低秩子空间假设 (Universal Weight Subspace Hypothesis)</strong>：作者基于“神经网络权重在不同任务和数据集之间常常收敛到层级共享的子空间”的理论（如文献 [15]），假设不同任务的 LoRA 适配器（即低秩矩阵的分解）也共享一个低秩子空间。</li>
<li><strong>子空间表示能力</strong>：这个共享的低秩子空间能够有效地捕捉跨任务的核心知识，并且可以通过增量学习来发现和细化。</li>
</ul>
</li>
</ul>
<h3 id="3">3. 方法设计详解</h3>
<p><strong>方法名称：</strong> Share (Shared LoRA Subspaces)</p>
<p><strong>核心思想：</strong> Share 提出了一种参数高效的持续学习框架，它不为每个任务维护独立的 LoRA 适配器，而是学习并动态更新一个<strong>共享的、低秩的基础子空间</strong>。新任务的适配器被表示为这个共享子空间中一组<strong>任务特定的系数</strong>，而不是独立的低秩矩阵。</p>
<p><strong>方法 Pipeline：</strong></p>
<p>Share 的方法可以分为三个主要阶段：<strong>初始化 (Initialization)</strong>、<strong>持续适应 (Continual Adaptation)</strong> 和 <strong>合并与微调 (Merging &amp; Finetuning)</strong>。</p>
<p><strong>a. 初始化 (Initialization)</strong></p>
<ul>
<li><strong>目标</strong>：构建一个初始的共享低秩子空间（即一组主要的基向量）。</li>
<li><strong>输入</strong>：<ul>
<li><code>N &gt; 1</code> 个已有的 LoRA 适配器（<code>{∆Wt = (At, Bt)}_{t=1}^{N}</code>），其中 <code>Bt ∈ Rn×r</code>, <code>At ∈ Rr×d</code>，<code>r</code> 是 LoRA 的秩。</li>
<li>或者，如果 LoRA 适配器不可用，则使用第一个任务的数据进行初始化。</li>
</ul>
</li>
<li><strong>操作</strong>：<ol>
<li><strong>提取 Rank Vectors</strong>：将所有已有的 LoRA 适配器的 <code>B</code> 矩阵（<code>Bt</code>）和 <code>A</code> 矩阵（<code>At</code>）堆叠起来，形成两个大的矩阵 <code>DB ∈ Rn×(N*r)</code> 和 <code>DA ∈ R(N*r)×d</code>。</li>
<li><strong>中心化</strong>：对 <code>DB</code> 和 <code>DA</code> 进行中心化处理（减去均值）。</li>
<li><strong>SVD 分解</strong>：对中心化后的矩阵 <code>DB</code> 和 <code>DA</code> 分别进行奇异值分解 (SVD)。<ul>
<li><code>DB = UB ΣB VB^T</code></li>
<li><code>DA = UA ΣA VA^T</code></li>
</ul>
</li>
<li><strong>提取主基向量</strong>：从 SVD 结果中，选择前 <code>k</code> 个奇异值对应的左奇异向量（<code>VB</code> 的前 <code>k</code> 列）作为共享的低秩子空间的主基向量 <code>β^0 ∈ Rn×k</code>，选择前 <code>k</code> 个奇异值对应的右奇异向量（<code>VA</code> 的前 <code>k</code> 列）作为共享的低秩子空间的主基向量 <code>α^0 ∈ Rd×k</code>。这里的 <code>k</code> 是一个超参数，代表共享子空间的维度。</li>
<li><strong>初始化系数</strong>：为每个任务初始化一组随机的、低秩的系数 <code>ε^0 ∈ Rk×p</code>，其中 <code>p</code> 是一个伪秩（pseudo-rank）超参数。</li>
</ol>
</li>
<li><strong>输出</strong>：初始的共享基向量 <code>α^0</code>, <code>β^0</code> 和任务系数 <code>ε^0</code>。</li>
</ul>
<p><strong>b. 持续适应 (Continual Adaptation)</strong></p>
<ul>
<li><strong>目标</strong>：在接收新任务的数据或适配器时，逐步更新共享子空间和任务系数。</li>
<li><strong>输入</strong>：<ul>
<li>当前时间步 <code>t</code> 的共享基向量 <code>β^{t-1}</code>, <code>α^{t-1}</code> 和任务系数 <code>{ε^{t-1}}</code>。</li>
<li>新任务的数据 <code>xt</code> 或新的 LoRA 适配器 <code>∆Wt</code>。</li>
</ul>
</li>
<li>
<p><strong>操作</strong>：</p>
<ol>
<li><strong>临时扩展子空间</strong>：<ul>
<li>如果接收到<strong>任务数据 <code>xt</code></strong>：<ul>
<li>初始化一组<strong>临时</strong>的基向量 <code>β^{t-1→t} ∈ Rn×φ</code> 和 <code>α^{t-1→t} ∈ Rd×φ</code>，其中 <code>φ</code> 是一个超参数（临时子空间的维度）。这些临时基向量是从现有的 <code>β^{t-1}</code> 和 <code>α^{t-1}</code> 中选择一部分（例如，前 <code>φ</code> 个）。</li>
<li>初始化与这些临时基向量对应的<strong>临时</strong>任务系数 <code>ε^{t-1→t} ∈ Rφ×p</code>，通常从高斯分布采样。</li>
<li><strong>修改前向传播</strong>：模型的前向传播变为 <code>h = Wox + (β^{t-1}ε^{t-1} + β^{t-1→t}ε^{t-1→t}) (α^{t-1}ε^{t-1} + α^{t-1→t}ε^{t-1→t})^T x</code>。注意这里是<strong>合并</strong>了旧的共享表示和新的临时表示。</li>
<li><strong>优化临时系数和基向量</strong>：在任务数据 <code>xt</code> 上，优化临时系数 <code>ε^{t-1→t}</code> 和临时基向量 <code>β^{t-1→t}</code>, <code>α^{t-1→t}</code>，以最小化损失。这使得模型能够适应新任务的数据。</li>
<li><strong>更新当前任务的适配器表示</strong>：将优化后的临时基向量和系数组合起来，形成当前任务 <code>t</code> 的适配器表示 <code>∆Wt = (α^{t-1→t}ε^{t-1→t})^T (β^{t-1→t}ε^{t-1→t})</code>。</li>
</ul>
</li>
<li>如果接收到<strong>LoRA 适配器 <code>∆Wt</code></strong>：<ul>
<li>直接使用该适配器 <code>∆Wt = (At, Bt)</code>。</li>
</ul>
</li>
</ul>
</li>
<li><strong>合并与微调 (Merging &amp; Finetuning)</strong>：<ul>
<li><strong>知识整合</strong>：<ul>
<li><strong>重构旧适配器</strong>：使用当前的共享基向量 <code>β^{t-1}</code> 和 <code>α^{t-1}</code> 以及对应的系数 <code>{ε^{t-1}}</code>，重构之前所有任务 <code>1</code> 到 <code>t-1</code> 的适配器表示。</li>
<li><strong>构建新的因子矩阵</strong>：将重构的旧适配器表示与新任务的适配器表示（如果是数据驱动的，则使用优化得到的 <code>∆Wt</code>；如果是适配器驱动的，则直接使用 <code>∆Wt</code>）堆叠起来，形成新的因子矩阵 <code>DB_new ∈ Rn×(t*r)</code> 和 <code>DA_new ∈ R(t*r)×d</code>。</li>
<li><strong>SVD 更新基向量</strong>：对新的因子矩阵 <code>DB_new</code> 和 <code>DA_new</code> 进行 SVD，提取新的主基向量 <code>β^t</code> 和 <code>α^t</code>（同样选择前 <code>k</code> 个奇异值对应的向量）。这一步是<strong>更新</strong>共享子空间。</li>
</ul>
</li>
<li><strong>系数重计算</strong>：<ul>
<li>使用新的共享基向量 <code>β^t</code> 和 <code>α^t</code>，以及所有任务（包括新任务）的适配器表示，通过<strong>解析方法</strong>（如投影和伪逆）重新计算所有任务的系数 <code>{ε^t}</code>。这个过程旨在找到一组系数，使得它们与新的共享基向量组合后，能够最好地重构所有任务的适配器。</li>
</ul>
</li>
<li><strong>可选微调</strong>：如果允许少量数据访问（Share-full），可以对新计算出的系数 <code>ε^t</code> 进行进一步微调，以提升性能。</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>输出</strong>：更新后的共享基向量 <code>α^t</code>, <code>β^t</code> 和任务系数 <code>{ε^t}</code>。</p>
</li>
</ul>
<p><strong>c. 模型结构</strong></p>
<ul>
<li><strong>预训练模型 <code>W0</code></strong>：保持冻结，作为基础。</li>
<li><strong>共享基向量 <code>α</code>, <code>β</code></strong>：这些是共享的、固定的（在初始化后）低秩子空间的基向量。它们是 Share 方法的核心，代表了跨任务的通用知识。</li>
<li><strong>任务特定系数 <code>ε</code></strong>：这些是与共享基向量 <code>α</code>, <code>β</code> 关联的、可学习的参数。每个任务都有自己的一组系数，它们决定了如何组合共享基向量来适应特定任务。</li>
<li><strong>临时基向量和系数</strong>：在持续适应阶段，用于临时扩展子空间以适应新数据。</li>
</ul>
<p><strong>d. 算法解释</strong></p>
<ul>
<li><strong>SVD (Singular Value Decomposition)</strong>：用于从堆叠的 LoRA 适配器中提取低秩表示，找到最能捕捉数据方差的主成分（即基向量）。</li>
<li><strong>解析重计算系数</strong>：这是 Share 的一个关键创新。作者利用了线性代数中的投影和伪逆等工具，在<strong>不依赖梯度下降</strong>的情况下，直接计算出任务系数，使得它们与更新后的共享基向量能够最好地重构所有任务的适配器。这大大提高了效率，并避免了梯度传播带来的潜在问题。</li>
<li><strong>低秩表示</strong>：LoRA 本身就是一种低秩表示方法，Share 在此基础上进一步利用了低秩表示的共享特性。</li>
</ul>
<h3 id="4">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>Share vs. 传统 LoRA</strong>：传统 LoRA 为每个任务训练独立的低秩矩阵 <code>(A, B)</code>。Share 则学习一组共享的基向量 <code>(α, β)</code>，并将每个任务的适配器表示为这些基向量的线性组合，通过学习任务特定的系数 <code>ε</code> 来实现。</li>
<li><strong>Share vs. 其他 PEFT 方法 (如 Adapter-tuning, Prompt-tuning)</strong>：Share 专注于利用低秩适配器的共享子空间特性，而其他方法可能采用不同的参数共享或知识迁移策略。</li>
<li><strong>Share vs. 传统持续学习方法</strong>：Share 是一种<strong>参数高效</strong>的持续学习方法，它不增加模型大小，不依赖数据回放，并且通过共享子空间实现知识整合，这与许多需要额外存储或计算资源的传统方法有本质区别。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ol>
<li><strong>共享低秩子空间框架</strong>：首次提出利用 LoRA 适配器共享的低秩子空间进行参数高效的持续学习。</li>
<li><strong>解析系数重计算</strong>：提出了一种数据和梯度无关的解析方法来更新任务系数，实现了高效的知识整合和模型更新。</li>
<li><strong>近乎严格的持续学习</strong>：在不依赖数据回放、不显著增加模型参数的情况下，实现了持续学习，接近严格持续学习的要求。</li>
<li><strong>统一的适配器管理</strong>：一个共享子空间和一组系数可以替代数百个独立的 LoRA 适配器，极大地简化了模型管理和部署。</li>
</ol>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>大规模预训练模型</strong>：尤其适用于 LLMs, VLMs, Diffusion Models 等大型模型。</li>
<li><strong>持续学习场景</strong>：当需要模型在不遗忘旧知识的情况下不断学习新任务时。</li>
<li><strong>资源受限环境</strong>：参数和内存效率高，适合部署在计算资源有限的场景。</li>
<li><strong>适配器管理</strong>：当需要管理大量 LoRA 适配器时，Share 提供了一种高效的整合方案。</li>
<li><strong>跨模态和跨任务学习</strong>：实验表明其在图像、文本、3D 等多种模态和任务上都有效。</li>
</ul>
</li>
</ul>
<h3 id="5">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：在 Continual GLUE (NLU), Continual Image Classification (CIFAR-100, Food-101, Caltech-101, Flowers-102), Continual 3D Pose Estimation, Continual Text-to-Image Generation 等多样化的任务和数据集上进行了评估。</li>
<li><strong>基线模型</strong>：<ul>
<li><strong>非持续学习 LoRA (non-CL LoRA)</strong>：为每个任务独立训练 LoRA 适配器，作为性能上限的参考。</li>
<li><strong>联合训练 LoRA (Joint LoRA)</strong>：在所有任务上同时训练 LoRA，作为理论性能上限。</li>
<li><strong>其他先进的持续学习方法</strong>：包括基于正则化的方法 (EWC, LwF)、基于提示的方法 (L2P, DAP, CODA-Prompt, Dual-Prompt) 和基于适配器的方法 (EASE)。</li>
</ul>
</li>
<li><strong>评估指标</strong>：准确率 (Acc.), 遗忘率 (Forg.↓), Matthews correlation, Pearson correlation, Rouge-L score 等，以及参数量和内存占用。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>参数效率</strong>：Share 实现了高达 <strong>100× 参数缩减</strong>和 <strong>281× 内存节省</strong>，远超传统 LoRA 方法。</li>
<li><strong>性能</strong>：在大多数任务上，Share 的性能与联合训练模型相当，并且显著优于其他参数高效的持续学习方法。</li>
<li><strong>知识保留与迁移</strong>：Share 能够有效保留旧知识，并在某些情况下表现出<strong>前向和后向知识迁移</strong>的现象（例如，在 NLU 任务中，学习新任务后早期任务的性能有所提升）。</li>
<li><strong>可扩展性</strong>：在“Continual Asynchronous Learning and Serving of LoRAs at Scale”实验中，Share 能够高效地整合和管理数百个 LoRA 适配器，并保持良好的性能。</li>
<li><strong>鲁棒性</strong>：对低质量的 LoRA 适配器和不同模态的任务都表现出一定的鲁棒性。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>NLU (GLUE)</strong>：在 Table 1 中，Share-full 取得了 83.44% 的平均性能，仅用 0.012M 参数，而传统 LoRA 需要 7.2M 参数。</li>
<li><strong>Image Classification (CIFAR-100)</strong>：在 Table 2 中，Share 取得了 94.20% 的准确率，参数量仅为 0.10M，优于其他方法，并且遗忘率最低。</li>
<li><strong>3D Pose Estimation</strong>：在 Table 3 中，Share 仅用 1M 参数，在 L1, L2, L3 等遮挡级别上均优于其他方法。</li>
<li><strong>Text-to-Image Generation</strong>：在 Figure 3 中，Share 能够生成高质量的图像，并且模型尺寸相比于 20 个 LoRA 适配器减小了 20 倍。</li>
<li><strong>大规模 LoRA 管理</strong>：在 Table 6 和 7 中，Share 在整合大量 LoRA 时表现出优越的性能和效率。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>对初始适配器质量的依赖</strong>：在没有额外数据的情况下，Share 的性能上限可能受限于初始 LoRA 适配器的质量。</li>
<li><strong>不支持跨模型架构</strong>：目前 Share 主要针对单一类型的预训练模型架构。</li>
<li><strong>不支持跨任务的持续学习</strong>：虽然 Share 能够整合不同任务的知识，但它本身并不直接支持在不同任务之间进行“持续学习”的迁移（即一个任务的输出直接作为另一个任务的输入）。</li>
<li><strong>超参数选择</strong>：<code>k</code>, <code>φ</code>, <code>p</code> 等超参数的选择对性能有影响，需要进行调优（尽管论文提供了指导）。</li>
</ul>
</li>
</ul>
<h3 id="6">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提到代码将开源（“We will release Share code which is compatible with HuggingFace PeFT library and a tutorial video here: https://anonymous.4open.science/r/Share-8FF2/”）。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>初始化</strong>：如果已有 LoRA 适配器，优先使用它们进行初始化。如果没有，则需要用第一个任务的数据训练一个 LoRA 适配器。</li>
<li><strong>超参数选择</strong>：<ul>
<li><code>k</code> (共享子空间维度)：根据解释方差阈值（如 60%）选择，或通过实验在性能和效率之间权衡。</li>
<li><code>φ</code> (临时子空间维度)：论文建议 <code>φ = [1, k/4]</code>，实验中 <code>φ=2</code> 或 <code>φ=4</code> 效果较好。</li>
<li><code>p</code> (伪秩)：论文建议 <code>p=1</code> 效果较好，或从 <code>r/3</code> 开始尝试。</li>
</ul>
</li>
<li><strong>数据处理</strong>：根据具体任务进行标准的数据预处理。</li>
<li><strong>训练</strong>：在持续适应阶段，优化临时系数和基向量时，使用任务数据和标准的损失函数。在合并阶段，系数的重计算是解析的，不需要梯度。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>迁移到其他 PEFT 方法</strong>：理论上，如果其他 PEFT 方法也产生低秩表示，Share 的共享子空间思想可能可以被借鉴，但需要修改适配器提取和系数计算的方式。</li>
<li><strong>迁移到不同模型架构</strong>：Share 的核心是低秩子空间的共享，理论上可以应用于任何支持低秩适配器（如 LoRA）的模型。但需要确保模型架构兼容 LoRA 的实现。</li>
<li><strong>迁移到跨任务学习</strong>：这是未来工作方向，目前 Share 主要用于在不同任务上学习，而不是让任务之间直接传递信息。</li>
</ul>
</li>
</ul>
<h3 id="7">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：通过学习共享低秩子空间，用任务系数表示适配器，实现高效持续学习。</li>
<li><strong>速记版 pipeline</strong>：<ol>
<li><strong>初始化</strong>：从现有适配器中提取共享基向量。</li>
<li><strong>适应</strong>：新任务来临时，临时扩展子空间并学习新系数。</li>
<li><strong>合并</strong>：解析地更新共享基向量和所有任务的系数。</li>
<li><strong>部署</strong>：用一套共享基向量和少量系数管理所有任务。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining.</li>
<li>We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities.</li>
<li>Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions.</li>
<li>Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06043v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06043v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06042v1'></a></p>
<h2 id="pseudo-invertible-neural-networks"><a href="https://arxiv.org/abs/2602.06042v1">Pseudo-Invertible Neural Networks</a></h2>
<p><strong>Authors:</strong> Yamit Ehrlich, Nimrod Berman, Assaf Shocher</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or "Back-Projection", <script type="math/tex">x' = x + A^\dagger(y-Ax)</script>, which moves a sample <script type="math/tex">x</script> to its closest consistent state <script type="math/tex">x'</script> satisfying <script type="math/tex">Ax=y</script>. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings <script type="math/tex">f(x)=y</script> via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, "degradation" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将按照您提供的分析框架，对这篇关于“伪逆可微神经网络”（Pseudo-Invertible Neural Networks）的论文进行深入分析。</p>
<hr />
<h2 id="pseudo-invertible-neural-networks_1">论文方法分析与总结：Pseudo-Invertible Neural Networks</h2>
<h3 id="1_1">1. 摘要翻译</h3>
<p>本文提出了一种将摩尔-彭罗斯伪逆（Moore-Penrose Pseudo-inverse, PInv）的概念自然地推广到非线性领域，特别是神经网络。我们引入了“<strong>可逆伪逆神经网络</strong>”（Surjective Pseudo-invertible Neural Networks, SPNN），一类明确设计用于实现可处理的非线性伪逆的架构。我们提出的非线性伪逆及其在 SPNN 中的实现，满足了基本的几何性质。其中一个关键性质是“<strong>零空间投影</strong>”或“<strong>反向投影</strong>”（Back-Projection），它将样本 <script type="math/tex">x</script> 映射到满足 <script type="math/tex">Ax=y</script> 的最近一致状态 <script type="math/tex">x'</script>。我们形式化了“<strong>非线性反向投影</strong>”（Non-Linear Back-Projection, NLBP），一种通过我们定义的伪逆来保证非线性映射 <script type="math/tex">f(x)=y</script> 具有相同一致性约束的方法。我们利用 SPNN 来扩展零样本逆问题的范围。基于扩散的零空间投影已彻底改变了线性逆问题的零样本求解，通过利用闭式反向投影。我们将此方法扩展到非线性退化。这里的“退化”被广泛地泛化，以包含从光学失真到语义抽象（如分类）的任何非线性信息丢失。这种方法能够实现复杂退化的零样本逆变换，并允许在不重新训练扩散先验的情况下，对生成输出进行精确的语义控制。</p>
<h3 id="2_1">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力</strong>: 作者旨在将线性代数中强大的“伪逆”概念推广到非线性领域，特别是深度学习模型。线性伪逆在解决欠定或超定线性系统时提供了最小范数解和最佳最小二乘近似，并能实现精确的一致性强制。作者希望在非线性情况下也能获得类似的数学保证和应用能力。</li>
<li><strong>现有方法痛点</strong>:<ul>
<li><strong>深度学习中的“逆”问题</strong>: 当前深度学习中处理逆问题通常依赖于回归（如自编码器）或概率生成（如条件扩散模型）。这些方法虽然在经验上有效，但缺乏严格的数学保证，例如无法保证“<strong>反射一致性</strong>”（reflexive consistency），即 <script type="math/tex">f(f^{-1}(f(x))) \neq f(x)</script>。</li>
<li><strong>严格可逆网络（INNs, Normalizing Flows）的局限性</strong>: 这些网络强制要求严格的双射性（bijectivity），为了保证计算的可行性，它们必须保持输入输出维度一致。这使得它们无法处理信息丢失的场景，如分类任务，因为分类本身就是一种降维和信息压缩的过程。</li>
<li><strong>现有非线性伪逆方法的不足</strong>: 作者提到 Gofer &amp; Gilboa (2023) 提出的非线性伪逆框架，虽然有理论基础，但在选择唯一伪逆时，作者认为其基于最小范数约束的定义可能忽略了某些几何结构，不如他们提出的基于“<strong>双射补全</strong>”（Bijective Completion）的定义自然。</li>
</ul>
</li>
<li><strong>研究假设</strong>:<ul>
<li>非线性算子可以通过某种形式的“<strong>双射补全</strong>”来构建一个高维空间中的双射映射，从而在其中定义一个唯一的、具有良好性质的伪逆。</li>
<li>SPNN 架构可以通过一种“<strong>可逆耦合机制</strong>”（surjective coupling mechanism）和学习到的辅助网络，在保持信息丢失（降维）的同时，实现一个结构化的、可处理的非线性伪逆。</li>
<li>这种非线性伪逆和反向投影方法可以有效地应用于零样本逆问题，特别是那些涉及复杂非线性退化的场景。</li>
</ul>
</li>
</ul>
<h3 id="3_1">3. 方法设计详解</h3>
<h4 id="_1">流程总结</h4>
<p>本文的核心在于定义一个“<strong>自然非线性伪逆</strong>”（Natural Non-Linear Pseudo-Inverse）并构建一个能够实现它的神经网络架构 SPNN。</p>
<p><strong>核心概念：</strong></p>
<ol>
<li>
<p><strong>双射补全 (Bijective Completion)</strong>:</p>
<ul>
<li><strong>动机</strong>: 对于一个非线性算子 <script type="math/tex">g: X \to Y</script>，如果它不是双射的（即可能不是单射或不是满射），直接定义伪逆很困难。作者借鉴了线性代数中通过添加零空间来处理非满射算子的思想。</li>
<li><strong>定义</strong>: 对于一个<strong>满射连续算子</strong> <script type="math/tex">g: X \to Y</script>，其双射补全 <script type="math/tex">G: X \to Y \times Z</script> 是一个<strong>微分同胚</strong>（diffeomorphism），定义为 <script type="math/tex">G(x) = [g(x)^T, q(x)^T]^T</script>。其中，<script type="math/tex">q: X \to Z</script> 是一个<strong>满射</strong>函数，用于“填充” <script type="math/tex">g</script> 的零空间（或非单射部分）。<script type="math/tex">Y \times Z</script> 是一个高维空间，使得 <script type="math/tex">G</script> 在这个空间中是双射的。</li>
<li><strong>关键</strong>: <script type="math/tex">q</script> 的选择是任意的，但为了定义一个“自然”的伪逆，需要一个一致的选择标准。</li>
</ul>
</li>
<li>
<p><strong>自然非线性伪逆 (Natural Non-Linear Pseudo-Inverse, <script type="math/tex">g^\dagger</script>)</strong>:</p>
<ul>
<li><strong>动机</strong>: 在线性情况下，伪逆 <script type="math/tex">A^\dagger</script> 使得 <script type="math/tex">A^\dagger y</script> 是所有满足 <script type="math/tex">Ax=y</script> 的 <script type="math/tex">x</script> 中范数最小的那个。作者希望在非线性情况下，找到一个在补全空间中“最接近原点”的解。</li>
<li><strong>定义</strong>: 给定一个满射算子 <script type="math/tex">g</script> 和其双射补全 <script type="math/tex">G</script>，自然非线性伪逆 <script type="math/tex">g^\dagger</script> 被定义为：
    <script type="math/tex">g^\dagger(y) = \arg \min_{x \in g^{-1}(y)} \|G(x) - G(0)\|_2^2</script>
    这意味着，对于给定的输出 <script type="math/tex">y</script>，我们寻找一个输入 <script type="math/tex">x</script>（在 <script type="math/tex">g</script> 的原像集 <script type="math/tex">g^{-1}(y)</script> 中），使得在补全空间 <script type="math/tex">Y \times Z</script> 中，<script type="math/tex">G(x)</script> 与 <script type="math/tex">G(0)</script> 的距离最小。</li>
<li><strong>“自然”的来源</strong>: 这种定义通过最小化 <script type="math/tex">G(x)</script> 与 <script type="math/tex">G(0)</script> 的距离，隐式地利用了 <script type="math/tex">G</script> 的结构，特别是 <script type="math/tex">q(x)</script> 的选择，从而比仅仅最小化 <script type="math/tex">\|x\|</script> 更能反映底层的几何结构。</li>
</ul>
</li>
<li>
<p><strong>非线性反向投影 (Non-Linear Back-Projection, NLBP)</strong>:</p>
<ul>
<li><strong>动机</strong>: 线性反向投影 <script type="math/tex">x' = x + A^\dagger(y - Ax)</script> 能够将一个估计值 <script type="math/tex">x</script> 投影到满足 <script type="math/tex">Ax'=y</script> 的最近一致状态。作者希望在非线性情况下实现类似功能。</li>
<li><strong>定义</strong>: 对于一个满射算子 <script type="math/tex">g</script> 和其双射补全 <script type="math/tex">G</script>，以及一个伪逆 <script type="math/tex">g^\dagger</script>（满足前两个 Penrose 恒等式），NLBP 定义为：
    <script type="math/tex">x' = G^{-1}(G(x) - G(g^\dagger(g(x))) + G(g^\dagger(y)))</script>
</li>
<li><strong>直观理解</strong>:<ul>
<li>
<script type="math/tex">G(x)</script> 是输入 <script type="math/tex">x</script> 在补全空间中的表示。</li>
<li>
<script type="math/tex">G(g^\dagger(g(x)))</script> 是将 <script type="math/tex">x</script> 通过 <script type="math/tex">g</script> 再通过伪逆 <script type="math/tex">g^\dagger</script> 映射回来的结果在补全空间中的表示。这个项可以看作是 <script type="math/tex">x</script> 在 <script type="math/tex">g</script> 映射下的“一致性”部分。</li>
<li>
<script type="math/tex">G(g^\dagger(y))</script> 是目标 <script type="math/tex">y</script> 通过伪逆 <script type="math/tex">g^\dagger</script> 映射回来的结果在补全空间中的表示。这个项代表了目标的一致性。</li>
<li>
<script type="math/tex">G(x) - G(g^\dagger(g(x))) + G(g^\dagger(y))</script> 实际上是在补全空间中进行了一个线性插值或修正。它将 <script type="math/tex">x</script> 的“一致性”部分替换为目标 <script type="math/tex">y</script> 的“一致性”部分，同时保持了 <script type="math/tex">x</script> 在补全空间中的其他自由度（零空间部分）。</li>
<li>最后通过 <script type="math/tex">G^{-1}</script> 映射回原始空间。</li>
</ul>
</li>
<li><strong>关键</strong>: 这个公式保证了 <script type="math/tex">g(x') = y</script>（Claim 1），并且在 <script type="math/tex">G</script> 度量下，<script type="math/tex">x'</script> 是 <script type="math/tex">x</script> 在流形 <script type="math/tex">g^{-1}(y)</script> 上的正交投影（Claim 2）。</li>
</ul>
</li>
<li>
<p><strong>可逆伪逆神经网络 (Surjective Pseudo-invertible Neural Networks, SPNN)</strong>:</p>
<ul>
<li><strong>动机</strong>: 构建一个能够实现上述非线性伪逆和 NLBP 的深度学习架构。SPNN 的核心是设计一个<strong>降维（信息丢失）但仍能定义伪逆</strong>的架构。</li>
<li><strong>架构</strong>: SPNN 基于“<strong>仿射可逆耦合块</strong>”（Affine Surjective Coupling Block）。<ul>
<li><strong>输入分割</strong>: 输入 <script type="math/tex">x \in \mathbb{R}^D</script> 被分割成两部分：<script type="math/tex">x \to [x_0, x_1]</script>，其中 <script type="math/tex">x_0 \in \mathbb{R}^d</script> (d &lt; D) 是“内容”部分，<script type="math/tex">x_1 \in \mathbb{R}^{D-d}</script> 是“冗余”部分。</li>
<li><strong>前向传播 (g)</strong>: <script type="math/tex">x_1</script> 通过学习到的尺度（<script type="math/tex">s</script>）和位移（<script type="math/tex">t</script>）函数来调制 <script type="math/tex">x_0</script>，生成输出 <script type="math/tex">y \in \mathbb{R}^d</script>。
    <script type="math/tex">y = x_0 \odot s(x_1) + t(x_1)</script>
    这个过程是<strong>满射</strong>的，因为输出维度 <script type="math/tex">d</script> 小于输入维度 <script type="math/tex">D</script>，信息丢失是故意的。</li>
<li><strong>反向传播 (g†)</strong>: 为了定义伪逆，需要恢复丢失的 <script type="math/tex">x_1</script>。SPNN 引入了一个<strong>可学习的辅助网络 <script type="math/tex">r: \mathbb{R}^d \to \mathbb{R}^{D-d}</script></strong>，它仅从输出 <script type="math/tex">y</script> 预测出“冗余”部分 <script type="math/tex">x_1</script>。
    <script type="math/tex">x_1 = r(y)</script>
    然后，利用 <script type="math/tex">y, s(x_1), t(x_1)</script> 来恢复 <script type="math/tex">x_0</script>:
    <script type="math/tex">x_0 = (y - t(x_1)) \oslash s(x_1)</script> (这里 <script type="math/tex">\oslash</script> 表示逐元素除法)
    最终重构输入 <script type="math/tex">x = [x_0, x_1]</script>。</li>
</ul>
</li>
<li><strong>训练策略</strong>: SPNN 采用<strong>两阶段训练</strong>：<ul>
<li><strong>阶段 I: 任务学习 (Task Learning)</strong>: 仅训练前向传播网络 <script type="math/tex">g</script>（即 <script type="math/tex">s, t</script> 函数）来完成特定任务（如分类、压缩），最小化任务损失 <script type="math/tex">L_{task}</script>。此时辅助网络 <script type="math/tex">r</script> 不参与。</li>
<li><strong>阶段 II: 自然逆学习 (Natural Inverse Learning)</strong>: 冻结 <script type="math/tex">g</script> 的参数，仅训练辅助网络 <script type="math/tex">r</script> 来满足自然非线性伪逆的几何要求。具体来说，最小化损失 <script type="math/tex">L_{natural} = \mathbb{E}_y [\|G(g^\dagger(y)) - G(0)\|_2^2]</script>。这使得 <script type="math/tex">r</script> 学习到的伪逆 <script type="math/tex">g^\dagger</script> 能够选择一个在补全空间中“最接近原点”的解。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="_2">模型结构</h4>
<ul>
<li><strong>SPNN 块</strong>:<ul>
<li><strong>输入分割</strong>: 将输入 <script type="math/tex">x</script> 分为 <script type="math/tex">x_0</script> (d维) 和 <script type="math/tex">x_1</script> (D-d维)。</li>
<li><strong>仿射耦合层</strong>: <script type="math/tex">x_1</script> 驱动 <script type="math/tex">x_0</script> 的尺度 <script type="math/tex">s</script> 和位移 <script type="math/tex">t</script> 函数，生成降维输出 <script type="math/tex">y</script>。</li>
<li><strong>辅助网络 <script type="math/tex">r</script></strong>: 独立于前向传播，用于从 <script type="math/tex">y</script> 预测 <script type="math/tex">x_1</script>。</li>
<li><strong>正交混合 (Orthogonal Mixing)</strong>: 为了提高表达能力，在分割前会应用一个可学习的<strong>正交变换</strong>（通过 Cayley 变换参数化），以发现最佳的通道分割方式。</li>
</ul>
</li>
<li><strong>多尺度架构</strong>: SPNN 可以堆叠多个块，并结合 PixelUnshuffle 操作来处理高维数据（如图像），实现多尺度特征提取和降维。</li>
<li><strong>扩散模型</strong>: SPNN 架构本身可以作为前向算子 <script type="math/tex">g</script>，或者与预训练的扩散模型结合，用于零样本逆问题。</li>
</ul>
<h4 id="_3">算法解释</h4>
<ul>
<li><strong>双射补全 <script type="math/tex">G(x) = [g(x)^T, q(x)^T]^T</script></strong>: 核心思想是将一个降维的满射映射 <script type="math/tex">g</script> 提升到一个高维空间，在这个空间中通过引入一个满射函数 <script type="math/tex">q</script> 来“补全”零空间，使得整体映射 <script type="math/tex">G</script> 成为一个双射。</li>
<li><strong>自然伪逆 <script type="math/tex">g^\dagger(y) = \arg \min_{x \in g^{-1}(y)} \|G(x) - G(0)\|_2^2</script></strong>: 这是对线性伪逆最小范数解的非线性推广。它不是简单地最小化 <script type="math/tex">\|x\|</script>，而是最小化在补全空间中，<script type="math/tex">G(x)</script> 与 <script type="math/tex">G(0)</script> 的距离。这使得伪逆的选择更加“自然”，因为它考虑了 <script type="math/tex">g</script> 和 <script type="math/tex">q</script> 的联合结构。</li>
<li><strong>非线性反向投影 <script type="math/tex">x' = G^{-1}(G(x) - G(g^\dagger(g(x))) + G(g^\dagger(y)))</script></strong>: 这是 NLBP 的核心公式。它通过在补全空间中进行向量运算，将当前估计 <script type="math/tex">x</script> 的“一致性”部分（由 <script type="math/tex">g^\dagger(g(x))</script> 决定）调整到目标 <script type="math/tex">y</script> 的“一致性”部分（由 <script type="math/tex">g^\dagger(y)</script> 决定），同时保持了零空间信息。这是一种结构化的、受几何约束的修正方法。</li>
<li><strong>SPNN 训练</strong>:<ul>
<li><strong>阶段 I</strong>: 训练 <script type="math/tex">g</script> (即 <script type="math/tex">s, t</script>) 来完成任务，例如，让 <script type="math/tex">g(x)</script> 预测图像的属性向量。</li>
<li><strong>阶段 II</strong>: 训练 <script type="math/tex">r</script> 来学习 <script type="math/tex">g</script> 的伪逆，使得 <script type="math/tex">g(g^\dagger(y)) = y</script> 并且 <script type="math/tex">g^\dagger</script> 满足“自然”伪逆的定义，即最小化 <script type="math/tex">\|G(g^\dagger(y)) - G(0)\|_2^2</script>。</li>
</ul>
</li>
</ul>
<h3 id="4_1">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>:</p>
<ul>
<li><strong>与 INNs/Normalizing Flows</strong>: SPNN 允许信息丢失（降维），而 INNs 强制保持维度。SPNN 的目标是实现一个“<strong>可逆的降维映射</strong>”及其伪逆，而 INNs 的目标是实现一个“<strong>可逆的等维映射</strong>”。</li>
<li><strong>与传统回归/生成模型</strong>: SPNN 提供了数学上更强的“逆”的保证（反射一致性，以及通过 NLBP 实现的精确一致性强制），而回归模型通常只提供近似解，生成模型则侧重于概率分布的匹配。</li>
<li><strong>与 Gofer &amp; Gilboa (2023) 的非线性伪逆</strong>: 两者都提出了非线性伪逆的理论框架。主要区别在于定义“唯一”伪逆的方式。Gofer &amp; Gilboa 基于最小化原像的范数 <script type="math/tex">\|x\|</script>，而本文基于“<strong>双射补全</strong>”和最小化补全空间中 <script type="math/tex">G(x)</script> 与 <script type="math/tex">G(0)</script> 的距离 <script type="math/tex">\|G(x) - G(0)\|_2^2</script>。作者认为后者的定义更“自然”，因为它能更好地保留底层几何结构，尤其是在 <script type="math/tex">g</script> 的非线性是坐标变换时。</li>
<li><strong>与线性 IBP/Null-Space Methods</strong>: SPNN 和 NLBP 将反向投影的思想推广到了非线性退化。线性方法依赖于矩阵的 SVD 或伪逆，而 SPNN/NLBP 使用学习到的 SPNN 架构和其伪逆来处理非线性算子。</li>
<li><strong>与 PnP-Diffusion/DPS</strong>: 这些方法通过梯度下降来强制执行非线性约束，可能面临梯度不稳定问题。SPNN/NLBP 提供了一种更结构化、更稳定的方法，通过学习到的伪逆直接进行“投影”。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>:</p>
<ul>
<li><strong>自然非线性伪逆的定义</strong>: 基于双射补全和补全空间距离最小化，提供了一个新的、更具几何意义的非线性伪逆定义。</li>
<li><strong>SPNN 架构</strong>: 第一个能够实现可处理的、降维的非线性伪逆的深度学习架构。它通过仿射耦合和学习到的辅助网络来实现这一目标。</li>
<li><strong>非线性反向投影 (NLBP)</strong>: 将反向投影的概念成功推广到非线性领域，为零样本逆问题提供了一种新的、强大的工具。</li>
<li><strong>统一框架</strong>: 将非线性伪逆理论、SPNN 架构和 NLBP 结合起来，形成了一个解决复杂非线性逆问题的统一框架。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>:</p>
<ul>
<li><strong>信息丢失的逆问题</strong>: 任何输入信息被压缩或丢失的逆问题，例如：<ul>
<li><strong>语义重建</strong>: 从低维语义表示（如属性向量、分类标签）重建高维数据（如图像）。</li>
<li><strong>低分辨率重建</strong>: 从低分辨率图像重建高分辨率图像（虽然这里降维是故意的，但原理相通）。</li>
<li><strong>压缩感知</strong>: 从稀疏测量重建信号。</li>
</ul>
</li>
<li><strong>需要精确一致性强制的场景</strong>: 当需要确保输出严格满足某个非线性约束时。</li>
<li><strong>需要可控生成/编辑的场景</strong>: 通过 NLBP 引导生成过程，实现对特定属性的精确控制。</li>
</ul>
</li>
</ul>
<h3 id="5_1">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>:</p>
<ul>
<li><strong>语义恢复 (Semantic Restoration)</strong>: 使用 CelebA-HQ 数据集，将图像映射到 40 维属性向量。然后使用 NLBP 和预训练的扩散模型，从属性向量重建图像。</li>
<li><strong>属性编辑 (Attribute Editing)</strong>: 动态修改单个或多个属性，观察生成图像的变化。</li>
<li><strong>消融实验 (Ablation Study)</strong>: 对比了不同的辅助网络 <script type="math/tex">r</script> 的选择（随机、最小范数）和不同的反向投影方法（Naive BP vs. Gentle BP），以证明本文提出的“自然”伪逆和 NLBP 的重要性。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>:</p>
<ul>
<li><strong>语义重建</strong>: 成功从 40 维属性向量重建出具有高度一致性的面部图像，平均二元一致性达到 92.3%。</li>
<li><strong>属性控制</strong>: 能够精确控制单个或多个属性（如“戴眼镜”、“男性”），生成多样化且符合约束的图像。</li>
<li><strong>消融实验</strong>: 表明使用本文提出的“自然”伪逆和 NLBP 方法，相比于其他基线方法，能够避免生成灾难性的失败（如高频噪声），并能更有效地引导生成过程。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>:</p>
<ul>
<li><strong>低维语义到高维图像的重建</strong>: 在 CelebA-HQ 数据集上，从低维属性向量重建图像表现出色。</li>
<li><strong>需要精确语义控制的生成任务</strong>: 能够实现对生成图像特定属性的精确控制。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>:</p>
<ul>
<li><strong>辅助网络 <script type="math/tex">r</script> 的表达能力</strong>: “自然”伪逆的有效性依赖于辅助网络 <script type="math/tex">r</script> 是否能准确捕捉训练数据的零空间统计信息。如果 <script type="math/tex">r</script> 不够好，伪逆虽然满足 <script type="math/tex">gg^\dagger=I</script>，但可能产生不真实的预像。</li>
<li><strong>算子 <script type="math/tex">g</script> 的满射性假设</strong>: 当前定义依赖于 <script type="math/tex">g</script> 是满射的。如果 <script type="math/tex">y</script> 超出了 <script type="math/tex">g</script> 的值域，定义需要进一步扩展。</li>
<li><strong>数据依赖性</strong>: “自然”伪逆的定义依赖于训练数据的几何结构。如果训练数据存在偏差，重构出的解也会带有这些偏差。</li>
</ul>
</li>
</ul>
<h3 id="6_1">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>: 论文中未明确提及开源，但通常这类研究会伴随代码发布。</li>
<li><strong>实现细节</strong>:<ul>
<li><strong>SPNN 块</strong>: 仿射耦合层中的尺度 <script type="math/tex">s</script> 和位移 <script type="math/tex">t</script> 函数通常是小型 MLP 或 CNN。辅助网络 <script type="math/tex">r</script> 也是一个神经网络。</li>
<li><strong>正交混合</strong>: 使用 Cayley 变换参数化正交矩阵，以保证正交性。</li>
<li><strong>训练</strong>: 两阶段训练是关键。第一阶段训练前向网络，第二阶段训练辅助网络。损失函数 <script type="math/tex">L_{natural}</script> 的计算需要 <script type="math/tex">G</script> 的定义，其中 <script type="math/tex">q</script> 的选择需要一致。</li>
<li><strong>NLBP 与扩散模型结合</strong>: NLBP 作为扩散模型采样过程中的一个修正步骤，需要调整其在采样过程中的应用时机和强度（通过 <script type="math/tex">\lambda</script>）。</li>
</ul>
</li>
<li><strong>迁移可能</strong>:<ul>
<li><strong>迁移到其他任务</strong>: SPNN 架构和 NLBP 方法可以迁移到任何需要处理信息丢失的非线性逆问题。例如，可以将 SPNN 用作图像压缩的编码器，或用于从文本描述生成图像的逆向过程。</li>
<li><strong>迁移到其他生成模型</strong>: NLBP 可以集成到其他生成模型（如 GANs）的采样或编辑过程中，以实现更精确的控制。</li>
<li><strong>定义“自然”伪逆</strong>: 对于不同的 <script type="math/tex">g</script> 和不同的 <script type="math/tex">q</script> 的选择，可以探索不同的“自然”伪逆定义，以适应不同的应用场景。</li>
</ul>
</li>
</ul>
<h3 id="7_1">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>: <strong>用可逆降维网络实现非线性伪逆，并用反向投影进行精确控制。</strong></li>
<li><strong>速记版 pipeline</strong>:<ol>
<li><strong>设计降维网络</strong>: 构建一个能丢弃信息但仍可逆的 SPNN。</li>
<li><strong>学习伪逆</strong>: 训练一个辅助网络来“补全”丢失的信息，形成伪逆。</li>
<li><strong>定义“自然”解</strong>: 通过在补全空间中最小化距离，选择一个有几何意义的伪逆解。</li>
<li><strong>非线性反向投影</strong>: 利用伪逆和补全空间，将估计值精确地“拉回”到目标约束。</li>
<li><strong>与扩散模型结合</strong>: 将反向投影用于引导扩散模型进行零样本逆问题求解。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular.</li>
<li>We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06042v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06042v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06041v1'></a></p>
<h2 id="predicting-camera-pose-from-perspective-descriptions-for-spatial-reasoning"><a href="https://arxiv.org/abs/2602.06041v1">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</a></h2>
<p><strong>Authors:</strong> Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="predicting-camera-pose-from-perspective-descriptions-for-spatial-reasoning_1">论文方法分析与总结：Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</h2>
<h3 id="1_2">1. 摘要翻译</h3>
<p><strong>中文翻译：</strong></p>
<p><strong>从视角描述预测相机位姿以实现空间推理</strong></p>
<p>多图像空间推理对于当前多模态大语言模型（MLLMs）来说仍然充满挑战。虽然单视图感知本质上是二维的，但推理多个视图需要跨视角构建连贯的场景理解。特别地，我们研究了视角转换（perspective taking），即模型必须从多视图观测中构建连贯的三维理解，并利用这种理解从一个新、语言指定的视角进行推理。我们提出了CAMCUE，一个位姿感知（pose-aware）的多图像框架，它使用相机位姿作为显式的几何锚点来进行跨视图融合和新视角推理。CAMCUE将每视图的位姿信息注入视觉令牌（visual tokens），将自然语言的视角描述与目标相机位姿进行关联，并合成一个位姿条件下的想象目标视图来支持回答。为了支持这一设定，我们构建了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，配对多视图图像和位姿，以及多样的目标视角描述和视角转换问题。我们还在测试集中包含了人工标注的视角描述，以评估对人类语言的泛化能力。CAMCUE将整体准确率提高了9.06%，并能以超过90%的旋转精度（在20°内）和0.5的平移误差阈值内，从自然语言视角描述中预测目标位姿。这种直接关联避免了耗时的测试时搜索匹配，将推理时间从每个示例的256.6秒减少到1.45秒，从而在真实场景中实现了快速、交互式的使用。</p>
<h3 id="2_2">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力</strong>：<ul>
<li>当前MLLMs在处理多视图场景时，难以构建连贯的3D场景理解，尤其是在需要从一个语言描述的新视角进行推理时（即“视角转换”）。</li>
<li>现有的方法往往缺乏一个明确的几何锚点来有效地融合多视图信息，并从指定的视角进行推理。</li>
</ul>
</li>
<li><strong>现有方法痛点</strong>：<ul>
<li><strong>缺乏明确的几何约束</strong>：许多方法将多视图图像视为独立的2D快照，或仅进行简单的聚合，未能有效利用视图间的几何关系。</li>
<li><strong>视角理解不准确</strong>：模型难以准确地将自然语言描述的视角（如“坐在黑桌子后面的沙发上”）映射到具体的相机位姿。</li>
<li><strong>推理能力受限</strong>：即使模型能生成想象视图，也可能与目标视角不一致，导致推理错误。</li>
<li><strong>效率低下</strong>：一些方法依赖于耗时的测试时搜索或迭代过程来找到合适的视图或生成信息性视图，不适用于实时交互场景。</li>
<li><strong>语言与位姿的脱节</strong>：现有的新视角合成（novel-view synthesis）模型通常需要明确的相机位姿作为输入，而MLLMs本身难以从语言描述中可靠地推断出目标相机位姿，存在一个“语言驱动的视角指定”与“位姿控制生成”之间的不匹配。</li>
</ul>
</li>
<li><strong>研究假设</strong>：<ul>
<li>将相机位姿作为显式的几何锚点，能够有效地连接多视图信息，并为从指定视角进行推理提供坚实的基础。</li>
<li>通过预测目标相机位姿，并利用该位姿条件生成目标视图，可以显著提升MLLMs在视角转换任务上的空间推理能力。</li>
</ul>
</li>
</ul>
<h3 id="3_2">3. 方法设计详解</h3>
<p><strong>流程总结：</strong></p>
<p>CAMCUE框架旨在解决多视图场景下的视角转换空间推理问题。其核心思想是将相机位姿作为连接多视图信息和语言描述的桥梁，并利用预测的位姿来指导新视角推理。整个流程可以概括为以下几个关键步骤：</p>
<ol>
<li>
<p><strong>输入</strong>：</p>
<ul>
<li><strong>上下文图像 (Contextual Images)</strong>：一组（V个）用于提供场景信息的图像。</li>
<li><strong>相机位姿 (Camera Poses)</strong>：与每个上下文图像对应的相机外参（Extrinsics）和内参（Intrinsics）。</li>
<li><strong>文本提示 (Text Prompt)</strong>：包含一个自然语言描述的目标视角（Target Viewpoint Description）和一个问题（Question）。</li>
</ul>
</li>
<li>
<p><strong>特征提取与融合 (Pose-aware Token Fusion)</strong>：</p>
<ul>
<li><strong>视觉特征提取</strong>：使用标准的Vision Transformer（ViT）主干网络（如QwenVL2.5, InternVL2.5）处理每个上下文图像，提取图像块（patch）级别的视觉令牌（Image Tokens），记为 <script type="math/tex">X_i \in \mathbb{R}^{S \times d}</script>。</li>
<li><strong>位姿特征编码</strong>：对于每个上下文图像 <script type="math/tex">I_i</script>，其相机外参 <script type="math/tex">C_i</script> 和内参 <script type="math/tex">K_i</script> 被转换为一个像素对齐的Plücker射线图（Pixel-aligned Plücker ray map）<script type="math/tex">R_i \in \mathbb{R}^{H \times W \times 6}</script>。这个射线图编码了每个像素点的3D方向信息。</li>
<li><strong>位姿令牌生成</strong>：通过一个轻量级的Plücker编码器（Epose），将射线图 <script type="math/tex">R_i</script> 编码成与视觉令牌在空间上对齐的位姿令牌（Camera Tokens）<script type="math/tex">Z_i \in \mathbb{R}^{S \times d}</script>。这个编码器遵循与Vision Backbone相同的patchification和空间聚合方案。</li>
<li><strong>融合</strong>：将每视图的视觉令牌 <script type="math/tex">X_i</script> 和对应的位姿令牌 <script type="math/tex">Z_i</script> 在patch维度上进行拼接（concatenation），然后通过一个MLP投影层，生成一个更新的、包含位姿信息的视觉令牌 <script type="math/tex">X'_i \in \mathbb{R}^{S \times d}</script>。这个过程将几何信息注入到视觉表示中，使得模型能够利用几何关系进行推理。公式为：<script type="math/tex">X'_i = X_i + W [Z_i; X_i]</script>。</li>
</ul>
</li>
<li>
<p><strong>目标位姿预测 (Target Pose Prediction)</strong>：</p>
<ul>
<li><strong>多模态融合</strong>：将所有上下文图像的融合后的视觉令牌 <script type="math/tex">X = [X'_1, ..., X'_V]</script> 与文本提示的隐藏状态 <script type="math/tex">H</script> 进行拼接，形成一个统一的输入序列。</li>
<li><strong>查询注意力</strong>：引入一组可学习的查询向量 <script type="math/tex">Q_o \in \mathbb{R}^{N \times d}</script>，通过多头注意力机制（Multi-head Attention）与文本和视觉令牌序列进行交互，捕捉与目标视角相关的几何信息。公式为：<script type="math/tex">Y = \text{Attn}(Q_o, [H; X])</script>。</li>
<li><strong>位姿回归</strong>：将注意力输出 <script type="math/tex">Y</script> 通过一个线性投影层，映射到 <script type="math/tex">N</script> 个位姿查询令牌 <script type="math/tex">U \in \mathbb{R}^{N \times d_q}</script>。然后，将这 <script type="math/tex">N</script> 个令牌重塑（reshape）并映射成一个4x4的相机到世界变换矩阵 <script type="math/tex">C_{tgt} \in \mathbb{R}^{4 \times 4}</script>，即目标相机位姿。公式为：<script type="math/tex">C_{tgt} = \text{reshape}(g(U))</script>。这里 <script type="math/tex">N=16</script>。</li>
</ul>
</li>
<li>
<p><strong>答案生成 (Answer Generation)</strong>：</p>
<ul>
<li><strong>联合生成</strong>：模型（MLLM）以自回归的方式生成答案。它首先生成一个表示目标位姿的特殊令牌（pose slot segment），然后生成最终的文本答案。</li>
<li><strong>可选的想象视图增强</strong>：在推理时，可以选择使用预测的目标相机位姿 <script type="math/tex">C_{tgt}</script>，通过一个图像解码器（如LVSM）合成一个目标视图图像（Imagined Target View）。</li>
<li><strong>证据增强</strong>：将合成的目标视图图像作为额外的视觉证据，再次输入给MLLM，以获得一个更优的、经过增强的答案。</li>
</ul>
</li>
</ol>
<p><strong>模型结构：</strong></p>
<ul>
<li><strong>Vision Backbone</strong>：负责从每个输入图像中提取视觉特征。</li>
<li><strong>Plücker Encoder (Epose)</strong>：将相机位姿信息（外参、内参）编码为与视觉特征对齐的位姿令牌。</li>
<li><strong>Pose Adapter</strong>：包含查询向量和注意力机制，用于从融合的多模态特征中预测目标相机位姿。</li>
<li><strong>MLLM (e.g., QwenVL2.5, InternVL2.5)</strong>：核心的语言模型，负责理解文本提示，进行多视图融合，生成答案，并（可选地）利用预测的位姿生成新视图。</li>
<li><strong>Fusion MLP</strong>：用于融合视觉令牌和位姿令牌。</li>
<li><strong>Image Decoder (Optional)</strong>：用于根据预测的相机位姿合成目标视图图像。</li>
</ul>
<p><strong>算法解释：</strong></p>
<ul>
<li><strong>Plücker Ray Map</strong>：将相机位姿（外参 <script type="math/tex">C_i</script>, 内参 <script type="math/tex">K_i</script>）转换为一个表示每个像素点3D射线的几何表示。这比直接使用矩阵更适合作为图像块的特征。</li>
<li><strong>Pose-aware Token Fusion</strong>：通过将位姿令牌与视觉令牌在空间上对齐并进行融合，使得模型在处理视觉信息时，能够直接感知到每个图像块对应的相机几何信息。这是一种显式的几何注入。</li>
<li><strong>Query-based Cross-Attention for Pose Prediction</strong>：引入可学习的查询向量，通过注意力机制从多模态输入中提取与目标视角最相关的几何和语义信息，并将其映射到相机位姿。这种方式比直接回归更灵活，能更好地捕捉语言描述的细微差别。</li>
<li><strong>Pose-Conditioned View Synthesis</strong>：利用预测的相机位姿来指导新视图的生成，确保生成视图在几何上与目标视角一致，从而为推理提供更可靠的依据。</li>
</ul>
<h3 id="4_2">4. 方法对比分析</h3>
<ul>
<li><strong>本质区别</strong>：<ul>
<li><strong>显式位姿建模</strong>：CAMCUE的核心在于将相机位姿作为显式的几何锚点，贯穿于特征融合、目标位姿预测和新视角生成等多个环节。而许多现有方法要么不考虑位姿，要么仅将其作为辅助信息。</li>
<li><strong>语言到位姿的直接映射</strong>：CAMCUE直接学习从自然语言视角描述到精确相机位姿的映射，解决了现有新视角合成模型与MLLMs之间在视角指定上的脱节问题。</li>
<li><strong>端到端视角转换推理</strong>：CAMCUE将视角转换推理过程整合在一个端到端的框架中，包括位姿预测和可选的视图合成，而不是依赖于独立的、耗时的模块或搜索过程。</li>
</ul>
</li>
<li><strong>创新贡献</strong>：<ul>
<li><strong>CAMCUE框架</strong>：提出了一个新颖的位姿感知多图像MLLM框架。</li>
<li><strong>CAMCUE-DATA数据集</strong>：构建了一个专门用于视角转换空间推理的数据集，包含多视图图像、相机位姿、详细的自然语言视角描述和QA对。</li>
<li><strong>位姿注入与融合</strong>：通过将相机位姿信息注入视觉令牌，实现了几何感知的跨视图融合。</li>
<li><strong>语言到位姿的直接预测</strong>：实现了从自然语言描述到目标相机位姿的精确预测。</li>
<li><strong>高效的视角转换推理</strong>：通过直接预测位姿和可选的视图合成，显著提高了推理效率。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>需要从多视图信息中理解场景的3D结构，并从一个语言指定的、未直接观测到的视角进行推理的任务。</li>
<li>例如：机器人导航、虚拟现实交互、场景理解和问答等。</li>
<li>特别适用于需要快速响应的交互式应用。</li>
</ul>
</li>
</ul>
<h3 id="5_2">5. 实验分析</h3>
<ul>
<li><strong>验证方法</strong>：<ul>
<li><strong>数据集</strong>：在自建的CAMCUE-DATA数据集上进行评估，该数据集专门设计用于视角转换推理。</li>
<li><strong>基线模型</strong>：与纯粹的MLLM基线（Base）、仅使用MindJourney（一种测试时想象方法）以及CAMCUE的不同组件（如Pose-Only）进行对比。</li>
<li><strong>评估指标</strong>：在CAMCUE-DATA上，使用Overall（平均准确率）以及Attribute, Visibility, Distance Order, Relative Relation, Count等细分任务的准确率进行评估。</li>
<li><strong>其他基准</strong>：在MindCube Tiny和MMSI等通用多图像空间推理基准上进行评估，以验证CAMCUE在不提供相机位姿输入时，其训练是否会损害通用能力。</li>
<li><strong>相机位姿预测精度</strong>：单独评估了CAMCUE的相机位姿预测能力，使用旋转和翻译误差阈值。</li>
<li><strong>消融实验</strong>：通过移除或替换CAMCUE的关键组件（如QA-FT，Pose-Only，CAMCUE(GT)）来分析各部分贡献。</li>
</ul>
</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>CAMCUE-DATA上的显著提升</strong>：CAMCUE在CAMCUE-DATA上比基线模型（Base）和MindJourney都有显著的性能提升，例如在Qwen2.5-VL-7B上，CAMCUE的Overall准确率达到80.12%，比Base（71.06%）高出9.06%。</li>
<li><strong>位姿预测精度高</strong>：CAMCUE能够以超过90%的旋转精度（在20°内）和0.5的平移误差阈值内，从自然语言描述中预测目标相机位姿。</li>
<li><strong>效率提升巨大</strong>：推理时间从256.6秒/示例（MindJourney）大幅降低到1.45秒/示例。</li>
<li><strong>消融实验验证</strong>：<ul>
<li>仅QA-FT（无位姿监督）提升有限，表明仅靠答案监督不足以实现视角转换。</li>
<li>Pose-Only（有位姿监督但无想象视图）已有提升，说明位姿信息是关键。</li>
<li>CAMCUE（加入想象视图）带来进一步显著提升，证明了将位姿转换为具体视觉证据的重要性。</li>
<li>CAMCUE(GT)（使用真实目标视图作为证据）作为上限，表明当前想象视图的质量仍有提升空间。</li>
</ul>
</li>
<li><strong>通用能力保持</strong>：在MindCube Tiny和MMSI等不提供位姿输入的基准上，CAMCUE（在不使用目标视图合成时）也能带来性能提升，说明其训练不会损害通用多图像推理能力。</li>
</ul>
</li>
<li><strong>优势场景</strong>：<ul>
<li><strong>视角敏感任务</strong>：在Visibility, Distance Order, Relative Relation等对视角变化非常敏感的任务上，CAMCUE表现出最大的优势。</li>
<li><strong>需要精确视角定位的场景</strong>：如Table 4所示，CAMCUE在从自然语言描述预测相机位姿方面表现出色。</li>
</ul>
</li>
<li><strong>局限性</strong>：<ul>
<li><strong>想象视图质量</strong>：虽然CAMCUE生成的视图比Nano Banana更可靠，但仍可能存在模糊渲染或轻微的几何不一致，尤其是在处理小物体或精细空间关系时。</li>
<li><strong>对语言描述的依赖</strong>：预测的位姿精度受限于自然语言描述的清晰度和详细程度。人工标注的描述效果优于GPT-4生成的描述。</li>
<li><strong>计算开销</strong>：虽然比现有方法快，但引入位姿编码和预测模块仍会增加一定的计算负担（尽管远低于迭代搜索）。</li>
</ul>
</li>
</ul>
<h3 id="6_2">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提供了项目主页链接（https://xuejunzhang2002.github.io/camcue/），通常意味着代码和数据集会公开。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>Backbone选择</strong>：可以使用QwenVL2.5, InternVL2.5等现有的大型多模态模型作为基础。</li>
<li><strong>位姿编码</strong>：Plücker编码器需要与Vision Backbone的patchification和空间聚合方案保持一致。</li>
<li><strong>Pose Adapter</strong>：关键在于查询向量的数量（N=16）和注意力机制的设计。</li>
<li><strong>训练</strong>：混合数据训练（CAMCUE-DATA + MindCube）是一种有效策略。需要同时优化语言建模损失和位姿回归损失。</li>
<li><strong>超参数</strong>：如Table 8和Table 9所示，需要关注LoRA rank, dropout, loss weights (<script type="math/tex">\lambda_{lang}, \lambda_{pose}</script>)等。</li>
<li><strong>想象视图</strong>：可以使用现有的新视角合成模型（如LVSM）作为图像解码器。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他视角推理任务</strong>：该框架的核心思想——将相机位姿作为显式几何锚点，可以迁移到其他需要理解3D空间和多视图几何的任务中。</li>
<li><strong>机器人导航/控制</strong>：可以将CAMCUE的位姿预测能力与导航策略结合，实现更精确的基于语言指令的导航。</li>
<li><strong>增强现有MLLMs</strong>：可以将CAMCUE的位姿编码和预测模块集成到其他MLLMs中，以提升其在3D空间理解方面的能力。</li>
<li><strong>数据需求</strong>：迁移到新任务时，需要相应的数据集，包含多视图图像、相机位姿以及与任务相关的语言描述。</li>
</ul>
</li>
</ul>
<h3 id="7_2">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：<strong>位姿锚定，语言驱动，多视图推理</strong></li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>注入位姿</strong>：将相机位姿信息融入图像特征。</li>
<li><strong>预测目标位姿</strong>：从语言描述和多视图特征中，直接预测相机位置和朝向。</li>
<li><strong>生成答案</strong>：利用预测的位姿和多视图信息回答问题。</li>
<li><strong>(可选)合成新视图</strong>：根据预测位姿生成目标视角图像，作为额外证据。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint.</li>
<li>We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06041v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06041v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06040v1'></a></p>
<h2 id="swimbird-eliciting-switchable-reasoning-mode-in-hybrid-autoregressive-mllms"><a href="https://arxiv.org/abs/2602.06040v1">SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs</a></h2>
<p><strong>Authors:</strong> Jintao Tong, Shilin Yan, Hongwei Xue, Xiaojun Tang, Kunyu Shi, Guannan Zhang, Ruixuan Li, Yixiong Zou</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="_4">论文方法分析与总结</h2>
<h3 id="1_3">1. 摘要翻译</h3>
<p><strong>SwimBird：在混合自回归多模态大模型中诱导可切换的推理模式</strong></p>
<p>多模态大语言模型（MLLMs）通过融合视觉和语言，在多模态感知和推理方面取得了显著进展。然而，现有的大多数MLLMs主要依赖文本链式思考（CoT）进行推理，这限制了它们在视觉密集型任务上的表现。近期一些方法通过注入固定数量的连续隐藏状态作为“视觉思想”来改进视觉性能，但往往以牺牲文本逻辑推理能力为代价。我们认为，核心的局限性在于一种僵化的、预定义的推理模式，无法自适应地为不同的用户查询选择最合适的思考模态。我们提出了SwimBird，一个可切换推理模式的MLLM，它能根据输入动态地在三种推理模式之间切换：（1）纯文本推理，（2）纯视觉推理（将连续隐藏状态作为视觉思想），以及（3）交错的视觉-文本推理。为了实现这一能力，我们采用了一种混合自回归的公式化方法，它统一了文本思想的下一个词预测和视觉思想的下一个嵌入预测，并设计了一个系统的推理模式策选策略来构建SwimBird-SFT-92K，一个涵盖所有三种推理模式的多样化监督微调数据集。通过实现灵活的、查询自适应的模式选择，SwimBird在保持强大的文本逻辑能力的同时，显著提升了在视觉密集型任务上的性能。在涵盖文本推理和挑战性视觉理解的各种基准测试上的实验表明，SwimBird在文本推理和视觉理解任务上均取得了最先进的性能，并显著优于先前固定的多模态推理方法。</p>
<h3 id="2_3">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力</strong>：作者旨在解决当前多模态大语言模型（MLLMs）在处理不同类型任务时，推理模式过于僵化，无法有效适应多样化查询的需求。</li>
<li><strong>现有方法痛点</strong>：<ul>
<li><strong>固定推理模式</strong>：现有方法要么完全依赖文本CoT，要么固定地引入视觉思想（连续嵌入），或者采用固定的交错模式。这种固定模式在面对不同任务时会产生“模态不匹配”问题。</li>
<li><strong>文本CoT的局限性</strong>：对于视觉密集型任务，纯文本CoT难以有效表达和处理精细的视觉信息，导致推理脆弱和错误累积。</li>
<li><strong>视觉思想的成本</strong>：引入视觉思想虽然提升了视觉任务性能，但可能损害文本逻辑推理能力。</li>
<li><strong>固定视觉思想长度</strong>：一些方法为视觉思想设定了固定的长度，这可能导致在简单任务上浪费计算，在复杂任务上信息不足。</li>
</ul>
</li>
<li><strong>研究假设</strong>：核心假设是，一个更强大的MLLM应该能够根据用户查询的特性，动态地选择最适合的推理模态（文本、视觉或两者结合），并且能够自适应地调整视觉思考的深度（即视觉思想的长度），以实现最佳的性能和效率。</li>
</ul>
<h3 id="3_3">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：</p>
<p>SwimBird的核心在于其“可切换推理模式”和“动态视觉令牌预算”的能力。其整体流程可以概括为以下几个关键阶段：</p>
<ol>
<li>
<p><strong>混合自回归建模 (Hybrid Autoregressive Modeling)</strong>：</p>
<ul>
<li><strong>目标</strong>：统一文本和视觉信息的生成方式，为推理模式的切换奠定基础。</li>
<li><strong>文本思想 (Textual Thought)</strong>：采用标准的自回归模型，进行<strong>下一个词预测 (Next Token Prediction)</strong>。给定已生成的文本序列 <script type="math/tex">\{w_1, \dots, w_{t-1}\}</script> 和输入 <script type="math/tex">x</script>（图像和上下文），模型预测下一个词 <script type="math/tex">w_t</script> 的概率分布 <script type="math/tex">p_\theta(w_t | w_{<t}, x)</script>。<ul>
<li><strong>损失函数</strong>：使用标准的交叉熵损失 <script type="math/tex">L_{text} = -\sum_{t=1}^T \log p_\theta(w_t | w_{<t}, x)</script> 来优化文本推理部分。这保留了语言模型的离散符号操作和逻辑一致性。</li>
</ul>
</li>
<li><strong>视觉思想 (Visual Thought)</strong>：采用自回归模型，进行<strong>下一个嵌入预测 (Next Embedding Prediction)</strong>。对于视觉推理，模型生成一系列连续的<strong>视觉思想</strong> <script type="math/tex">\{z_1, \dots, z_K\}</script>，每个 <script type="math/tex">z_k</script> 是一个隐藏状态嵌入。模型以自回归方式预测下一个嵌入 <script type="math/tex">z_k</script>：<script type="math/tex">z_k = f_\theta(z_{<k}, w_{<t}, x)</script>。<ul>
<li><strong>损失函数</strong>：使用<strong>均方误差 (MSE) 损失</strong> <script type="math/tex">L_{vis} = \sum_{k=1}^K ||\hat{z}_k - z_k||^2</script> 来监督预测的嵌入 <script type="math/tex">\hat{z}_k</script> 与目标嵌入 <script type="math/tex">z_k</script> 之间的差距。目标嵌入 <script type="math/tex">z_k</script> 是通过同一Vision Encoder对中间思考图像进行编码得到的，从而将视觉思想与语义上有意义的视觉状态联系起来。</li>
</ul>
</li>
<li><strong>统一训练目标</strong>：对于一个训练样本，可能包含纯文本CoT、纯视觉CoT或交错的片段。模型会根据样本中激活的模态计算相应的损失，并进行加权求和：<script type="math/tex">L = \lambda_{text} L_{text} + \lambda_{vis} L_{vis}</script>。这里的 <script type="math/tex">\lambda_{text}</script> 和 <script type="math/tex">\lambda_{vis}</script> 是平衡系数。</li>
</ul>
</li>
<li>
<p><strong>推理模式切换 (Mode Switching)</strong>：</p>
<ul>
<li><strong>机制</strong>：通过引入<strong>特殊分隔符 (Special Delimiters)</strong> 来控制推理模式的切换。例如，使用 <code>&lt;|latent_start|&gt;</code> 和 <code>&lt;|latent_end|&gt;</code> 来标记视觉思想的开始和结束。</li>
<li><strong>训练时</strong>：这些分隔符定义了模型应该生成连续的视觉嵌入还是离散的文本令牌。</li>
<li><strong>推理时</strong>：模型根据输入查询，<strong>自回归地生成这些分隔符</strong>。这使得模式选择成为查询自适应的：模型可以决定何时进入视觉思考阶段，何时保持纯文本推理，或何时两者交替。</li>
</ul>
</li>
<li>
<p><strong>动态视觉令牌预算 (Dynamic Latent Token Budget)</strong>：</p>
<ul>
<li><strong>动机</strong>：解决固定视觉令牌数量的不足，即在视觉密集任务上信息不足，在简单任务上浪费计算。</li>
<li><strong>机制</strong>：SwimBird采用<strong>分辨率感知 (Resolution-Aware)</strong> 的动态令牌预算。<ul>
<li><strong>训练时</strong>：为输入图像和中间思考图像分配不同的最大像素预算（通过控制像素/补丁预算），这直接控制了Vision Encoder为每种图像生成的视觉令牌的最大数量。这允许模型根据图像分辨率动态调整视觉令牌数量，避免过度池化丢失细节，也防止过长的序列占用过多计算。</li>
<li><strong>推理时</strong>：在视觉-仅或交错模式下，模型会<strong>动态地生成视觉令牌</strong>，直到模型决定停止（通过输出结束分隔符 <code>&lt;/latent&gt;</code>）。这种<strong>可变长度的潜在跨度 (Variable-Length Latent Span)</strong> 自然地将视觉思考的量与查询的感知难度相匹配。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>推理过程 (Inference)</strong>：</p>
<ul>
<li>在推理时，SwimBird根据输入查询，动态地选择以下三种模式之一或组合：<ul>
<li><strong>纯文本 (Text Only)</strong>：仅使用文本CoT。</li>
<li><strong>纯视觉 (Vision Only)</strong>：使用动态长度的视觉令牌。</li>
<li><strong>交错视觉-文本 (Interleave Vision-Text)</strong>：交错生成文本和视觉令牌。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构</strong>：</p>
<ul>
<li><strong>Vision Encoder</strong>：负责将图像编码为视觉特征。</li>
<li><strong>LLM Backbone</strong>：作为核心的语言模型，负责文本生成和推理。</li>
<li><strong>Multimodal Projector</strong>：用于将Vision Encoder的输出与LLM的输入对齐。</li>
<li><strong>Hybrid Autoregressive Formulation</strong>：这是核心设计，它允许LLM同时处理文本令牌预测和视觉嵌入预测。</li>
</ul>
<p><strong>算法解释</strong>：</p>
<ul>
<li><strong>文本思想的下一个词预测</strong>：这是标准语言模型的工作方式，通过最大化生成文本序列的概率来学习。</li>
<li><strong>视觉思想的下一个嵌入预测</strong>：这是一种新颖的生成方式，将视觉信息表示为连续的嵌入序列，而不是离散的文本。通过MSE损失直接学习视觉表示，使其更贴近图像的语义信息。</li>
<li><strong>特殊分隔符</strong>：是实现模式切换的关键，它们充当了控制信号，指导模型在不同模态之间切换。</li>
<li><strong>动态令牌预算</strong>：通过引入分辨率感知和可变长度生成，使得视觉思考的计算量更加灵活和高效。</li>
</ul>
<h3 id="4_3">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>固定模式 vs. 可切换模式</strong>：SwimBird最大的区别在于其<strong>查询自适应的推理模式切换能力</strong>，而大多数现有方法采用固定的推理模式（纯文本、纯视觉或固定交错）。</li>
<li><strong>固定长度 vs. 动态长度视觉令牌</strong>：SwimBird引入了<strong>动态视觉令牌预算</strong>，能够根据任务复杂度和图像分辨率调整视觉思考的深度，而许多方法使用固定数量的视觉令牌。</li>
<li><strong>统一生成接口</strong>：SwimBird通过混合自回归框架，统一了文本和视觉思想的生成方式，为动态切换提供了技术基础。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>可切换推理模式</strong>：这是核心创新，解决了模态不匹配问题，使得模型能更有效地处理多样化的多模态任务。</li>
<li><strong>动态视觉令牌预算</strong>：提高了视觉推理的效率和效果，避免了信息丢失或计算浪费。</li>
<li><strong>系统化的数据集构建</strong>：SwimBird-SFT-92K数据集的构建策略，为训练可切换推理模式的模型提供了高质量的监督信号。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>视觉密集型任务</strong>：如精细视觉搜索、空间推理、图像理解等，SwimBird可以通过激活视觉或交错模式来获得更好的性能。</li>
<li><strong>文本逻辑推理任务</strong>：如数学问题、逻辑分析等，SwimBird会倾向于使用纯文本模式，避免不必要的视觉计算干扰。</li>
<li><strong>混合任务</strong>：需要结合视觉信息和逻辑推理的任务，SwimBird可以通过交错模式有效处理。</li>
</ul>
</li>
</ul>
<h3 id="5_3">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：在多种基准测试上进行了评估，包括：<ul>
<li><strong>精细视觉理解</strong>：V* Bench, HR-Bench 4K/8K, MME-RealWorld。</li>
<li><strong>通用VQA和多模态推理</strong>：MMStar, RealWorldQA, WeMath, DynaMath, MathVerse_MINI。</li>
</ul>
</li>
<li><strong>对比模型</strong>：与文本推理模型（如GPT-4o, Qwen2.5/3-VL）、视觉推理模型（如Monet, LVR, SkiLa）以及多模态Agentic模型（如Pixel Reasoner, DeepEyes）进行了比较。</li>
<li><strong>消融实验</strong>：对最大视觉令牌预算 (Nmax) 和MSE损失权重 (<script type="math/tex">\lambda_{vis}</script>) 进行了消融研究，以验证动态预算和混合训练目标的重要性。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>State-of-the-Art (SOTA) 性能</strong>：SwimBird在精细视觉理解任务上取得了SOTA性能，例如在V* Bench上达到85.5分。</li>
<li><strong>平衡文本和视觉能力</strong>：SwimBird在文本推理任务上表现良好，同时显著提升了视觉任务的性能，没有牺牲文本逻辑能力。</li>
<li><strong>优于固定模式模型</strong>：在视觉密集型任务上，SwimBird的查询自适应能力使其优于固定模式的模型。</li>
<li><strong>动态预算的有效性</strong>：消融实验表明，动态令牌预算和适中的MSE损失权重对性能提升至关重要。例如，Nmax=32被证明是最佳设置，<script type="math/tex">\lambda_{vis}=0.2</script> 提供了最佳的平衡。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>高分辨率图像理解</strong>：HR-Bench 4K/8K上的性能提升，表明动态预算能更好地处理高分辨率图像中的精细细节。</li>
<li><strong>需要视觉定位和文本推理的任务</strong>：如图5所示的读取电话号码的例子，SwimBird能够先进行视觉定位，再进行文本比对和决策。</li>
<li><strong>纯逻辑推理任务</strong>：如图5所示的算术题，SwimBird能有效切换到纯文本模式。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>计算开销</strong>：虽然动态预算提高了效率，但相比纯文本模型，多模态模型通常仍有更高的计算开销。</li>
<li><strong>数据集依赖</strong>：SwimBird-SFT-92K数据集的质量和多样性直接影响模型的性能，构建高质量的多模态CoT数据集仍然是一个挑战。</li>
<li><strong>模式切换的鲁棒性</strong>：虽然实验证明了模式切换的有效性，但在某些极端或模糊的查询下，模式选择的准确性仍可能受到影响。</li>
</ul>
</li>
</ul>
<h3 id="6_3">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提供了项目页面、GitHub仓库和HuggingFace数据集链接，表明代码和数据集是开源的。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>基础模型</strong>：论文使用了Qwen3-VL 8B作为基础模型。</li>
<li><strong>训练</strong>：在A100-80G GPU上进行微调，批次大小为128。Vision Encoder和多模态投影器被冻结，仅更新LLM参数。使用余弦学习率调度器，初始学习率为1e-5。</li>
<li><strong>数据集</strong>：SwimBird-SFT-92K数据集是关键，包含文本、视觉和交错三种模式的样本。</li>
<li><strong>超参数</strong>：<script type="math/tex">\lambda_{text}</script> 和 <script type="math/tex">\lambda_{vis}</script> 的选择需要根据具体任务进行调整，论文中默认使用 <script type="math/tex">\lambda_{vis}=0.2</script>。最大视觉令牌预算 Nmax=32。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他LLM架构</strong>：SwimBird的核心思想（混合自回归、模式切换、动态预算）可以迁移到其他大型语言模型架构上。</li>
<li><strong>其他多模态任务</strong>：该方法可以应用于更广泛的多模态理解和生成任务，特别是那些需要灵活处理不同模态信息和推理策略的任务。</li>
<li><strong>迁移的关键</strong>：需要构建或适配相应的多模态CoT数据集，并设计合适的训练目标和模式切换机制。</li>
</ul>
</li>
</ul>
<h3 id="7_3">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：<strong>动态推理模式切换与自适应视觉思考</strong>。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>理解问题</strong>：分析查询是偏向文本还是视觉。</li>
<li><strong>选择模式</strong>：决定是纯文本、纯视觉还是两者结合。</li>
<li><strong>生成思考</strong>：根据模式生成文本或视觉（或两者交替）的推理过程。</li>
<li><strong>动态调整</strong>：视觉思考时，根据图像复杂度和分辨率调整思考深度。</li>
<li><strong>给出答案</strong>：整合所有思考过程，输出最终答案。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning.</li>
<li>Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06040v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06040v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06037v1'></a></p>
<h2 id="thinking-with-geometry-active-geometry-integration-for-spatial-reasoning"><a href="https://arxiv.org/abs/2602.06037v1">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</a></h2>
<p><strong>Authors:</strong> Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>本论文提出了一种名为 GeoThinker 的新框架，旨在解决当前多模态大语言模型 (MLLMs) 在空间推理中被动融合几何信息导致语义-几何不匹配和信号冗余的问题。GeoThinker 通过“主动感知”范式，使模型能够根据其内部推理需求选择性地检索和整合几何信息，从而显著提升了空间智能的性能和泛化能力，并在 VSI-Bench 上取得了新的 SOTA 成绩。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>GeoThinker 的核心创新在于其<strong>主动几何整合 (Active Geometry Integration)</strong> 的范式转变，从被动融合转向主动感知。具体方法论体现在以下几个关键点：</p>
<ul>
<li><strong>空间接地融合 (Spatial-Grounded Fusion):</strong> GeoThinker 在视觉语言模型 (VLM) 的特定层级进行融合，而不是在全局层面。这种选择性融合允许语义视觉先验（来自 VLM 的视觉理解）主动查询和整合与任务相关的几何信息。</li>
<li><strong>帧严格交叉注意力 (Frame-Strict Cross-Attention):</strong> 这种交叉注意力机制确保了语义信息与几何信息之间的精确对齐。通过“帧严格”的约束，模型能够更精确地将语义概念与特定的几何结构关联起来，避免了全局融合带来的模糊性。</li>
<li><strong>重要性门控 (Importance Gating):</strong> 该机制进一步校准了每帧的注意力权重，将注意力偏向于与任务最相关的几何结构。这意味着模型不会平均分配注意力，而是会优先关注那些对当前推理任务至关重要的几何元素。</li>
</ul>
<p>总而言之，GeoThinker 的方法论是通过<strong>选择性、精细化、有偏向性</strong>的几何信息整合，来模拟人类在进行空间推理时，能够主动聚焦和利用关键几何线索的能力。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>GeoThinker 的研究对计算机视觉和多模态学习领域具有重要的潜在影响：</p>
<ul>
<li><strong>提升空间推理能力:</strong> 通过主动整合几何信息，模型能够更深入地理解场景的结构和物体之间的空间关系，从而在各种需要空间推理的任务上取得突破。</li>
<li><strong>解决语义-几何不匹配问题:</strong> 当前模型在融合多模态信息时，常常出现语义理解与几何表示不一致的情况。GeoThinker 的主动机制有望缓解这一问题，使模型能够更准确地将视觉语义与几何结构联系起来。</li>
<li><strong>提高模型效率和鲁棒性:</strong> 通过选择性地关注相关几何信息，模型可以减少冗余信号的处理，从而提高计算效率。同时，更精准的几何整合也可能带来更强的鲁棒性，使其在复杂多变的场景下表现更佳。</li>
<li><strong>推动下一代空间智能发展:</strong> 论文明确指出，主动整合空间结构的能力是下一代空间智能的关键。GeoThinker 的成功将为未来更强大、更智能的空间理解系统奠定基础。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学 (Robotics):</strong> 机器人需要在复杂环境中进行导航、抓取和操作，这高度依赖于对三维空间的精确理解和推理。GeoThinker 的方法可以帮助机器人更好地感知和利用环境的几何信息。</li>
<li><strong>增强现实/虚拟现实 (AR/VR):</strong> AR/VR 应用需要精确地将虚拟对象叠加到真实世界中，或者在虚拟世界中构建逼真的场景。对空间关系的深入理解是实现沉浸式体验的关键。</li>
<li><strong>自动驾驶 (Autonomous Driving):</strong> 自动驾驶汽车需要实时感知周围环境的三维结构，预测其他车辆和行人的运动轨迹。GeoThinker 的空间推理能力可以提升自动驾驶系统的感知和决策能力。</li>
<li><strong>三维重建与场景理解 (3D Reconstruction &amp; Scene Understanding):</strong> 从图像或点云数据中重建三维场景并理解其结构是计算机视觉中的一个重要方向。GeoThinker 的方法可以帮助模型更有效地从几何信息中提取有用的结构特征。</li>
<li><strong>医学影像分析 (Medical Imaging Analysis):</strong> 在医学影像中，理解器官、病灶的空间位置和相互关系对于诊断和治疗至关重要。GeoThinker 的空间推理能力可能有助于提高医学影像分析的精度。</li>
<li><strong>内容创作与设计 (Content Creation &amp; Design):</strong> 在游戏开发、建筑设计等领域，对三维空间的理解和操作是核心。GeoThinker 的技术可能为这些领域提供更智能的工具。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了 GeoThinker 的强大能力，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算复杂度:</strong> “帧严格交叉注意力”和“重要性门控”等精细化的融合机制，可能会增加模型的计算复杂度和训练时间，尤其是在处理高分辨率或长序列的几何数据时。</li>
<li><strong>对几何先验的依赖:</strong> 该方法依赖于“3D 编码器”提供的几何先验。如果这些几何先验本身存在不足或不准确，可能会影响 GeoThinker 的整体性能。</li>
<li><strong>模型架构的复杂性:</strong> GeoThinker 引入了新的融合模块和门控机制，这可能使得模型的整体架构更加复杂，增加了模型设计和实现的难度。</li>
<li><strong>对特定任务的适应性:</strong> 虽然摘要提到了在 VSI-Bench 上的 SOTA 表现和在下游任务上的泛化能力，但其在更广泛、更具挑战性的空间推理任务上的表现仍需进一步验证。例如，对于需要高度抽象或非欧几里得几何推理的任务，其效果可能需要评估。</li>
<li><strong>“主动感知”的实现细节:</strong> 摘要描述了“主动感知”的理念，但具体的“内部推理需求”如何被精确地捕捉和转化为查询信号，以及“选择性检索”的具体实现机制，在摘要中并未完全展开，这可能是未来研究需要深入探讨的方面。</li>
</ul>
<p>总而言之，GeoThinker 是一项非常有前景的研究，它通过引入主动几何整合的范式，显著提升了多模态模型在空间推理方面的能力。其核心创新在于精细化的融合策略，有望为下一代空间智能系统带来突破。然而，在实际应用中，也需要关注其潜在的计算成本和对高质量几何先验的依赖。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception.</li>
<li>Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06037v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06037v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06034v1'></a></p>
<h2 id="v-retrver-evidence-driven-agentic-reasoning-for-universal-multimodal-retrieval"><a href="https://arxiv.org/abs/2602.06034v1">V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</a></h2>
<p><strong>Authors:</strong> Dongyang Chen, Chaoyang Wang, Dezhao SU, Xi Xiao, Zeyu Zhang, Jing Xiong, Qing Li, Yuzhang Shang, Shichao Ka</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域高水平研究生，深入分析您提供的论文，重点关注其创新之处和方法细节。请提供论文内容，我将按照您设定的分析框架进行详细解读。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06034v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06034v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06032v1'></a></p>
<h2 id="splat-and-distill-augmenting-teachers-with-feed-forward-3d-reconstruction-for-3d-aware-distillation"><a href="https://arxiv.org/abs/2602.06032v1">Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation</a></h2>
<p><strong>Authors:</strong> David Shavin, Sagie Benaim</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then <code>splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model,</code>distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，重点关注其创新点、设计逻辑、优势与不足，并提供实用的实现指南。</p>
<hr />
<h2 id="splat-and-distill-augmenting-teachers-with-feed-forward-3d-reconstruction-for-3d-aware-distillation_1">论文方法分析与总结：《SPLAT AND DISTILL: AUGMENTING TEACHERS WITH FEED-FORWARD 3D RECONSTRUCTION FOR 3D-AWARE DISTILLATION》</h2>
<h3 id="1_4">1. 摘要翻译</h3>
<p><strong>SPLAT AND DISTILL：通过前馈3D重建增强教师模型以实现3D感知蒸馏</strong></p>
<p>视觉基础模型（VFMs）在各种2D下游任务中取得了显著的成功。然而，它们往往缺乏3D感知能力。为此，我们提出了Splat and Distill（SnD）框架，通过在教师模型中集成一个快速、前馈的3D重建管线来增强其3D感知能力。我们的方法首先将教师模型产生的2D特征以<strong>前馈</strong>的方式提升到显式的3D高斯表示。然后，这些3D特征被“splat”到新的视角，生成一组新的2D特征图，用于监督学生模型，从而“蒸馏”出几何上更具约束性的知识。通过用<strong>前馈提升</strong>取代慢速的<strong>逐场景优化</strong>，我们的框架避免了特征平均化的伪影，创造了一个动态的学习过程，教师和学生的性能都能同步提升。我们在单目深度估计、表面法线估计、多视图对应和语义分割等一系列下游任务上进行了全面的评估。我们的方法显著优于现有工作，不仅在3D感知能力上取得了实质性提升，还增强了2D特征的底层语义丰富性。</p>
<h3 id="2_4">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li>当前主流的视觉基础模型（VFMs），如DINO和DINOv2，在2D任务上表现出色，但其学习到的特征在3D感知任务（如深度估计、表面法线估计、3D对应等）上表现不足。</li>
<li>作者希望在不牺牲2D任务性能的前提下，提升VFMs的3D感知能力，使其能够更好地理解和处理3D场景信息。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>逐场景优化（Per-scene optimization）的低效与不准确</strong>：<ul>
<li><strong>FiT3D (Yue et al., 2024)</strong>：通过逐场景优化将2D特征提升到3D，然后渲染生成“一致”的2D特征用于微调。这种方法计算成本高昂，且由于不同视图的2D特征本身存在不一致性，优化过程会产生“最小二乘妥协”，导致语义模糊。</li>
<li><strong>MEF (You et al., 2024)</strong>：通过对应点强制多视图特征一致性，绕过了显式重建。但其监督依赖于强制特征相似性，不足以灌输密集几何理解。</li>
</ul>
</li>
<li><strong>缺乏直接的3D几何监督</strong>：现有方法主要在2D特征空间内进行操作，未能充分利用3D几何信息来指导特征学习。</li>
<li><strong>特征平均化伪影</strong>：逐场景优化方法容易导致特征在不同视图间进行平均，丢失细节，产生模糊。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>通过将一个<strong>快速、前馈的3D重建模块</strong>集成到现有的<strong>教师-学生蒸馏框架</strong>中，可以有效地将3D几何知识注入到2D VFM的特征表示中。</li>
<li>这种集成方式可以避免逐场景优化的低效和不准确性，同时利用3D几何信息来正则化和增强2D特征的语义和几何一致性。</li>
</ul>
</li>
</ul>
<h3 id="3_4">3. 方法设计详解</h3>
<p><strong>流程总结</strong></p>
<p>Splat and Distill (SnD) 框架的核心思想是<strong>在教师模型的训练过程中，引入一个前馈3D重建模块来生成3D几何信息，并将这些信息以一种“splatting”的方式投影回2D，作为对学生模型的监督信号</strong>。整个流程可以概括为：</p>
<ol>
<li>
<p><strong>Teacher Feature Extraction &amp; 3D Reconstruction (教师特征提取与3D重建)</strong>:</p>
<ul>
<li><strong>输入</strong>: 两个<strong>上下文视图</strong> (context views) <script type="math/tex">I_{ctx}</script> 及其对应的相机参数 <script type="math/tex">P_{ctx}</script>。</li>
<li><strong>教师特征提取</strong>: 教师模型 <script type="math/tex">f_t</script> 处理这两个上下文视图，提取低分辨率的2D特征图 <script type="math/tex">F_{ctx} \in \mathbb{R}^{h \times w \times C}</script>。</li>
<li><strong>3D几何重建</strong>: 使用一个<strong>预训练的、前馈的3D重建模型</strong>（如MVSplat）处理这两个上下文视图，预测场景的3D高斯表示 <script type="math/tex">G_{geom} = \{\mu_i, \Sigma_i, \alpha_i\}_{i=1}^M</script>，其中 <script type="math/tex">\mu_i</script> 是高斯中心，<script type="math/tex">\Sigma_i</script> 是协方差矩阵，<script type="math/tex">\alpha_i</script> 是不透明度。这个模型<strong>不使用</strong>高斯的外观参数（SH系数 <script type="math/tex">c_i</script>）。这个过程是<strong>前馈</strong>的，并且提供了<strong>像素到高斯的一一对应关系</strong>。</li>
</ul>
</li>
<li>
<p><strong>Mask-Aware Feature Lifting (掩码感知特征提升)</strong>:</p>
<ul>
<li><strong>挑战</strong>: 教师提取的低分辨率特征图 <script type="math/tex">F_{ctx}</script> (h x w) 与3D高斯所处的全分辨率图像 (H x W) 之间存在显著的分辨率不匹配。直接双线性插值会导致模糊和跨对象边界的特征混合。</li>
<li><strong>解决方案</strong>: 利用<strong>实例语义分割掩码</strong>（在训练时可用）进行<strong>掩码感知上采样</strong>。<ul>
<li>对于目标高分辨率网格中的每个像素 <script type="math/tex">u</script>，其特征值 <script type="math/tex">F_{high}(u)</script> 通过插值得到，但插值仅考虑与 <script type="math/tex">u</script>
<strong>具有相同语义标签</strong>的邻近低分辨率特征点 <script type="math/tex">v</script>。</li>
<li>公式 (2) 和 (3) 定义了这种插值方式：权重 <script type="math/tex">w_{uv}</script> 基于标准双线性插值权重，但仅在 <script type="math/tex">v</script> 的语义掩码与 <script type="math/tex">u</script> 的语义掩码相同时才生效。</li>
</ul>
</li>
<li><strong>结果</strong>: 生成高分辨率的2D特征图 <script type="math/tex">F_{high} \in \mathbb{R}^{H \times W \times C}</script>，这些特征图保留了更清晰的语义边界。</li>
<li><strong>特征提升</strong>: 利用3D重建模型提供的像素到高斯对应关系，将 <script type="math/tex">F_{high}</script> 中的特征向量<strong>附加</strong>到对应的3D高斯上，形成一个<strong>3D特征场景</strong> <script type="math/tex">G_{feat} = \{\mu_i, \Sigma_i, \alpha_i, f_i\}_{i=1}^M</script>，其中 <script type="math/tex">f_i</script> 是与高斯 <script type="math/tex">G_i</script> 关联的语义特征向量。</li>
</ul>
</li>
<li>
<p><strong>Splatting and Blending (Splatting与融合)</strong>:</p>
<ul>
<li><strong>Splatting</strong>: 将3D特征场景 <script type="math/tex">G_{feat}</script> 从<strong>目标视图</strong> (target view) 的相机参数 <script type="math/tex">P_{tgt}</script> 渲染（splat）出来，生成一个目标视图的2D特征图 <script type="math/tex">F_{render} \in \mathbb{R}^{H \times W \times C}</script>。</li>
<li><strong>Semantic Blending (语义融合)</strong>: 为了进一步正则化渲染出的特征图，并处理3D重建中可能存在的几何伪影，引入了语义融合步骤。<ul>
<li>对于目标视图中的每个像素 <script type="math/tex">u</script>，其最终融合特征 <script type="math/tex">F_{blend}(u)</script> 是原始渲染特征 <script type="math/tex">F_{render}(u)</script> 与其<strong>同语义掩码区域内所有像素的平均渲染特征</strong>的加权平均。</li>
<li>公式 (4) 定义了融合：<script type="math/tex">F_{blend}(u) = \alpha \cdot F_{render}(u) + (1 - \alpha) \cdot \frac{1}{|M_u|} \sum_{v \in M_u} F_{render}(v)</script>，其中 <script type="math/tex">M_u</script> 是与像素 <script type="math/tex">u</script> 具有相同语义掩码的像素集合，<script type="math/tex">\alpha</script> 是融合因子（论文中设为0.5）。</li>
<li><strong>目的</strong>: 强制局部特征一致性，平滑由不完美3D重建引入的噪声，同时保留物体边界的清晰度。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Student Distillation (学生蒸馏)</strong>:</p>
<ul>
<li><strong>输入</strong>: 目标视图的2D图像 <script type="math/tex">I_{tgt}</script>。</li>
<li><strong>学生特征提取</strong>: 学生模型 <script type="math/tex">f_s</script> 处理 <script type="math/tex">I_{tgt}</script>，生成其自身的2D特征图 <script type="math/tex">F_{tgt} \in \mathbb{R}^{h \times w \times C}</script>。</li>
<li><strong>下采样</strong>: 将教师生成的<strong>高分辨率融合特征图</strong> <script type="math/tex">F_{blend}</script> 下采样到与学生特征图相同的分辨率（h x w），使用双线性插值。</li>
<li><strong>DINO Head</strong>: 将学生特征图 <script type="math/tex">F_{tgt}</script> 和下采样后的教师特征图 <script type="math/tex">F_{blend}</script> 分别通过一个<strong>共享的DINO head</strong>（一个小的MLP）。</li>
<li><strong>蒸馏损失</strong>: 优化学生模型参数 <script type="math/tex">\theta_s</script>，使其输出的概率分布与教师（经过DINO head）输出的概率分布尽可能一致，使用<strong>交叉熵损失</strong> <script type="math/tex">L_{distill}</script> (公式 (5))。</li>
<li><strong>教师更新</strong>: 教师模型参数 <script type="math/tex">\theta_t</script> 通过学生模型参数 <script type="math/tex">\theta_s</script> 的<strong>指数移动平均（EMA）</strong>进行更新，遵循DINOv2的策略。</li>
</ul>
</li>
</ol>
<p><strong>模型结构</strong></p>
<ul>
<li><strong>Teacher Network (<script type="math/tex">f_t</script>)</strong>: 一个预训练的VFM（如DINOv2），用于提取2D上下文特征。</li>
<li><strong>3D Reconstruction Module (<script type="math/tex">g_{geom}</script>)</strong>: 一个<strong>预训练的、前馈的3D高斯重建模型</strong>（如MVSplat），用于从多视图图像预测3D高斯几何表示。<strong>关键点</strong>：此模块是<strong>冻结</strong>的，并且是<strong>前馈</strong>的，不涉及逐场景优化。</li>
<li><strong>Mask-Aware Upscaling Module</strong>: 利用语义分割掩码将低分辨率2D特征提升到全分辨率。</li>
<li><strong>Splatting Module</strong>: 将3D特征高斯渲染到目标视图。</li>
<li><strong>Semantic Blending Module</strong>: 对渲染的特征图进行局部一致性正则化。</li>
<li><strong>Student Network (<script type="math/tex">f_s</script>)</strong>: 一个与教师结构相同的VFM，用于学习3D感知特征。</li>
<li><strong>DINO Head</strong>: 一个小的MLP，将教师和学生的特征映射到概率分布，用于计算蒸馏损失。</li>
</ul>
<p><strong>算法解释</strong></p>
<ul>
<li><strong>前馈3D重建 (Feed-forward 3D Reconstruction)</strong>: 这是核心创新之一。与以往依赖慢速、迭代的逐场景优化来估计3D几何不同，这里使用一个已经训练好的、能够直接从图像预测3D高斯表示的模型。这极大地提高了3D几何信息的获取效率，使其能够集成到端到端的训练流程中。</li>
<li><strong>掩码感知特征提升 (Mask-Aware Feature Lifting)</strong>: 解决2D特征图与3D几何表示在分辨率上的不匹配问题。通过利用语义分割掩码，确保上采样过程不会将不同语义区域的特征混合，从而保留了特征的清晰度和语义边界。这比简单的双线性插值更精细。</li>
<li><strong>语义融合 (Semantic Blending)</strong>: 这是一个正则化步骤，用于平滑由不完美的3D重建（即使是前馈模型也可能存在误差）产生的渲染特征。通过在同一语义区域内进行平均，可以减少噪声和几何不一致性，生成更鲁棒的监督信号。</li>
<li><strong>3D-Aware Distillation (3D感知蒸馏)</strong>: 整个框架的最终目标。通过将经过3D几何信息增强（splatting和blending）的教师特征作为监督信号，学生模型被迫学习具有更强3D几何理解能力的特征。这与传统的2D数据增强蒸馏不同，后者主要关注2D不变性。</li>
</ul>
<h3 id="4_4">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>与FiT3D/MEF等方法的根本区别</strong>：SnD的核心在于<strong>“前馈3D重建+Splatting+蒸馏”</strong>的组合。它<strong>避免了逐场景优化</strong>，而是利用一个<strong>预训练的前馈3D模型</strong>来快速获取3D几何信息，并将这些信息<strong>直接投影回2D</strong>作为监督。FiT3D依赖慢速优化，MEF则不进行显式重建。</li>
<li><strong>与DUNE等方法的区别</strong>：DUNE也尝试融合2D和3D教师，但它直接从教师继承特征，可能继承不一致性。SnD则通过<strong>显式的3D重建和渲染过程</strong>来<strong>纠正和增强</strong>教师的3D感知能力，然后才进行蒸馏。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>前馈3D重建的集成</strong>：将高效的前馈3D重建模型引入到VFM的3D感知蒸馏流程中，解决了效率瓶颈。</li>
<li><strong>掩码感知特征提升</strong>：一种新颖的特征上采样方法，利用语义掩码来保留特征的语义边界。</li>
<li><strong>语义融合</strong>：一种有效的特征正则化技术，用于平滑渲染特征并提高监督信号的质量。</li>
<li><strong>3D几何知识的注入</strong>：通过将3D几何信息（通过splatting）投影回2D特征空间，直接为学生模型提供3D几何监督，从而提升其3D感知能力。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>核心场景</strong>: 提升现有2D VFM（如DINOv2）的3D感知能力，使其在单目深度估计、表面法线估计、多视图对应、语义分割等任务上表现更好。</li>
<li><strong>数据要求</strong>: 需要包含3D几何信息的<strong>多视图场景数据</strong>进行训练（如ScanNet, ScanNet++等）。语义分割掩码在训练阶段是必需的（用于掩码感知提升和语义融合）。</li>
</ul>
</li>
</ul>
<h3 id="5_4">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>下游任务评估</strong>: 在单目深度估计、表面法线估计、多视图对应和语义分割等多个3D感知相关任务上进行评估。</li>
<li><strong>数据集</strong>: 使用了ScanNet++, ScanNet, NYUv2等室内数据集进行<strong>in-domain</strong>评估，以及ADE20K, Pascal VOC, KITTI等数据集进行<strong>out-of-domain</strong>评估，以验证泛化能力。</li>
<li><strong>基线对比</strong>: 与Vanilla DINOv2、FiT3D、MEF等先进方法进行比较。</li>
<li><strong>消融实验 (Ablation Studies)</strong>: 系统地评估了各个组件（如语义融合、掩码感知上采样、蒸馏损失等）的有效性。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li>在所有评估的下游任务上，SnD方法均显著优于基线方法，尤其是在3D感知任务上。</li>
<li>例如，在单目深度估计和表面法线估计任务上，SnD相比于最接近的基线（如FiT3D）取得了显著的相对提升（如RMSE降低5.90%等）。</li>
<li>在语义分割任务上，SnD也带来了mIoU的提升。</li>
<li>消融实验证明了掩码感知上采样、语义融合以及所提出的蒸馏损失的重要性。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>精细几何细节捕捉</strong>: 在深度估计和表面法线估计的定性结果中，SnD生成的地图具有更精细的结构细节和更平滑的表面，例如在椅子、床等物体上表现更佳。</li>
<li><strong>更强的3D一致性</strong>: 在多视图对应任务中，SnD在不同视角变化下都能保持更高的召回率，表明其学习到的特征具有更好的多视图一致性。</li>
<li><strong>更清晰的语义分割掩码</strong>: 在语义分割任务中，SnD生成的掩码边界更清晰，能够准确分割细小的物体（如椅子腿），显示出增强的空间和语义一致性。</li>
<li><strong>良好的泛化能力</strong>: 在out-of-domain数据集（如KITTI）上的评估也显示出SnD能够将学到的3D感知能力迁移到新的场景。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>对3D重建质量的依赖</strong>: 方法的监督信号质量直接取决于<strong>前馈3D重建模型的性能</strong>。如果3D重建不佳，则监督信号会受到影响，导致整体性能下降。</li>
<li><strong>多视图数据依赖</strong>: 目前的训练主要依赖于<strong>多视图图像数据集</strong>，这类数据集的可用性和多样性相对有限。</li>
<li><strong>对语义掩码的依赖</strong>: 掩码感知上采样和语义融合步骤需要<strong>训练时的语义分割掩码</strong>。虽然可以通过SAM等模型生成，但仍是一个额外的依赖。</li>
</ul>
</li>
</ul>
<h3 id="6_4">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>: 论文提供了项目主页链接（https://davidshavin4.github.io/Splat-and-Distill/），通常意味着代码会开源。</li>
<li><strong>实现细节</strong>:<ul>
<li><strong>3D重建模型</strong>: 使用预训练的MVSplat模型，并可能需要针对ScanNet++数据集进行微调（论文中提到“fine-tune this model on ScanNet++”）。</li>
<li><strong>教师与学生模型</strong>: 初始化为预训练的DINOv2模型。</li>
<li><strong>训练数据</strong>: 需要包含3D几何信息（如ScanNet++）以及对应的语义分割掩码。</li>
<li><strong>超参数</strong>: 语义融合因子 <script type="math/tex">\alpha=0.5</script>。EMA动量系数为0.999。训练步数50,000步。</li>
<li><strong>DINO Head</strong>: 使用一个3层MLP，隐藏层维度2048，瓶颈层维度256。</li>
</ul>
</li>
<li><strong>迁移可能</strong>:<ul>
<li><strong>迁移到其他VFM</strong>: 可以尝试将此框架应用于其他预训练的2D VFM，如MAE、CLIP等，只要它们能提供2D特征。</li>
<li><strong>迁移到其他3D重建模型</strong>: 可以尝试替换MVSplat为其他高效的前馈3D重建模型（如基于3DGS的预测模型），只要它们能提供几何表示和像素-高斯对应。</li>
<li><strong>迁移到其他3D感知任务</strong>: 该框架的核心是提升特征的3D感知能力，因此理论上可以应用于任何受益于更强3D理解的下游任务。</li>
</ul>
</li>
</ul>
<h3 id="7_4">7. 总结</h3>
<ul>
<li>
<p><strong>核心思想</strong>: <strong>用前馈3D重建生成几何监督，通过Splatting和掩码感知提升来增强2D特征的3D感知能力。</strong> (19字)</p>
</li>
<li>
<p><strong>速记版pipeline</strong>:</p>
<ol>
<li><strong>教师提取2D特征</strong>。</li>
<li><strong>前馈3D模型重建场景几何</strong>。</li>
<li><strong>用掩码将2D特征提升到3D高斯</strong>。</li>
<li><strong>将3D特征渲染回2D并融合</strong>。</li>
<li><strong>用融合后的特征蒸馏学生模型</strong>。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline.</li>
<li>Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner.</li>
<li>These 3D features are then <code>splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model,</code>distilling" geometrically grounded knowledge.</li>
<li>Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06032v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06032v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06028v1'></a></p>
<h2 id="context-forcing-consistent-autoregressive-video-generation-with-long-context"><a href="https://arxiv.org/abs/2602.06028v1">Context Forcing: Consistent Autoregressive Video Generation with Long Context</a></h2>
<p><strong>Authors:</strong> Shuo Chen, Cong Wei, Sun Sun, Ping Nie, Kai Zhou, Ge Zhang, Ming-Hsuan Yang, Wenhu Chen</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一位AI领域的研究生，深入分析这篇关于“Context Forcing: Consistent Autoregressive Video Generation with Long Context”的论文。我将重点关注其方法论，并以结构化的方式进行解读。</p>
<hr />
<h2 id="context-forcing-consistent-autoregressive-video-generation-with-long-context_1">论文方法分析与总结：《Context Forcing: Consistent Autoregressive Video Generation with Long Context》</h2>
<h3 id="1_5">1. 摘要翻译</h3>
<p><strong>中文摘要：</strong></p>
<p>本文提出了一种名为“Context Forcing”的新型框架，旨在解决长视频生成中的“遗忘-漂移”困境。现有方法受限于短上下文窗口（3.0-9.2秒），导致生成视频在长期一致性上表现不佳（遗忘）。而一些尝试扩展上下文的流式长上下文微调基线（如LongLive），在推理时扩大上下文窗口（3.0→5.25秒）会累积误差并导致分布偏移（漂移）。本文提出的Context Forcing框架，通过训练一个长上下文学生模型，并由一个同样具备长上下文感知能力的教师模型进行监督，消除了这种结构性不匹配。通过确保教师模型能够感知完整的生成历史，我们消除了监督不匹配，从而能够训练出具备长期一致性的模型。为了应对极长时长（例如2分钟）的计算挑战，我们引入了一个上下文管理系统，将线性增长的上下文转化为“慢-快记忆”（Slow-Fast Memory）架构，显著降低了视觉冗余。实验证明，Context Forcing能够支持20秒以上的上下文，同时保持强大的长期一致性，在多种长视频评估指标上超越了最先进的基线。</p>
<h3 id="2_5">2. 方法动机分析</h3>
<ul>
<li><strong>驱动力：</strong> 作者希望实现能够生成<strong>长时段内（例如几十秒甚至几分钟）保持高度一致性</strong>的视频。当前的视频生成模型在处理长视频时，面临着严重的挑战，导致视频内容出现漂移、物体身份变化或场景突然重置等问题。</li>
<li><strong>现有方法痛点：</strong><ol>
<li><strong>短上下文窗口限制（遗忘）：</strong> 大多数现有模型（包括一些基于Transformer的扩散模型）的上下文窗口非常有限（通常在3-9.2秒），这使得模型在生成长视频时会“忘记”早期内容，导致不一致。</li>
<li><strong>长上下文的漂移问题：</strong> 为了解决遗忘问题，一些方法尝试在推理时扩展上下文窗口（例如，从3秒扩展到5.25秒）。然而，这种做法会导致误差累积和分布偏移（漂移），即生成的视频逐渐偏离真实数据分布，出现不自然的现象。</li>
<li><strong>学生-教师模型不匹配：</strong> 许多长视频生成方法采用“学生-教师”蒸馏的范式，其中一个长上下文的学生模型由一个短上下文的（记忆力有限的）教师模型监督。这种不匹配是核心问题：短上下文的教师无法提供全局的、长期的指导，从而限制了学生模型能学习到的上下文长度。</li>
</ol>
</li>
<li><strong>研究假设：</strong><ul>
<li><strong>核心假设：</strong> 解决长视频生成中的“遗忘-漂移”困境的关键在于<strong>消除学生模型和教师模型之间的上下文长度不匹配</strong>。一个具备长上下文感知能力的教师模型能够有效地指导一个长上下文的学生模型，从而实现真正的长期一致性。</li>
<li><strong>计算可行性假设：</strong> 即使是长上下文，也可以通过高效的上下文管理策略（如慢-快记忆）来处理，使其在计算上可行。</li>
</ul>
</li>
</ul>
<h3 id="3_5">3. 方法设计详解</h3>
<p><strong>方法Pipeline总结：</strong></p>
<p>Context Forcing的核心在于构建一个<strong>长上下文教师-长上下文学生</strong>的训练范式，并辅以一个高效的<strong>上下文管理系统</strong>。整个过程可以分解为以下几个关键阶段：</p>
<ol>
<li>
<p><strong>长上下文教师的构建与训练（Robust Context Teacher Training）：</strong></p>
<ul>
<li><strong>动机：</strong> 为了让教师模型能够提供长期的、可靠的指导，它本身需要具备处理长上下文的能力，并且对学生模型产生的“漂移”具有鲁棒性。</li>
<li><strong>实现：</strong><ul>
<li><strong>基础模型：</strong> 使用一个预训练好的、能够进行视频续写的模型（如Wan2.1-T2V-1.3B）作为基础。</li>
<li><strong>鲁棒性训练（Error-Recycling Fine-Tuning, ERFT）：</strong> 这是关键创新点。为了让教师模型在面对学生模型可能产生的错误（漂移）时仍能提供有效指导，作者对其进行了特殊训练。具体做法是：<ul>
<li><strong>注入扰动：</strong> 将学生模型生成的上下文（X1:k）与一个“漂移”误差（edrift）进行叠加，构造一个扰动后的上下文。这个误差是从过去模型残差中采样得到的。</li>
<li><strong>目标：</strong> 训练教师模型，使其能够从这个扰动后的上下文中恢复出正确的视频续写（即预测目标速度Vtarget）。</li>
<li><strong>效果：</strong> 这种训练使得教师模型能够“纠正”学生模型的错误，从而在学生模型上下文退化时，其预测分布 pT(· | X1:k) 仍然是 Pdata(X1:k) 的可靠代理。</li>
</ul>
</li>
<li><strong>数据：</strong> 使用包含长视频的数据集（如Sekai, Ultravideo）进行训练。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>长上下文学生模型的训练（Context Forcing Framework）：</strong></p>
<ul>
<li><strong>目标：</strong> 训练一个长上下文的学生模型，使其能够生成具有长期一致性的视频。</li>
<li><strong>核心思想：</strong> 采用<strong>两阶段课程学习（Two-Stage Curriculum）</strong>来优化一个全局目标 Lglobal = KL(po(X1:N) || Pdata(X1:N))，该目标旨在最小化学生模型生成长视频分布与真实数据分布之间的KL散度。由于直接优化全局目标不可行，将其分解为局部和上下文两个部分：<ul>
<li><strong>Lglobal = Llocal + Lcontext</strong></li>
<li><strong>Llocal: Local Dynamics (Stage 1)</strong><ul>
<li><strong>目标：</strong> 匹配学生模型对短视频片段（X1:k，k为1-5秒）的预测分布与真实数据分布。</li>
<li><strong>实现：</strong> 使用<strong>条件分布匹配蒸馏（Contextual Distribution Matching Distillation, CDMD）</strong>，并利用<strong>分数匹配（Score Matching）</strong>来估计梯度。学生模型（GΦ）的输出与教师模型（Sreal）的输出之间的差异被用来计算梯度，以使学生模型在短片段上表现良好。</li>
<li><strong>公式：</strong> Llocal = KL(Pe(X1:k) || PT(X1:k))，梯度计算涉及学生和教师的分数函数。</li>
<li><strong>作用：</strong> 确保学生模型能够生成高质量、符合真实数据分布的短视频片段，为后续的长上下文学习提供可靠的“基础”。</li>
</ul>
</li>
<li><strong>Lcontext: Global Continuation Dynamics (Stage 2)</strong><ul>
<li><strong>目标：</strong> 匹配学生模型对长视频续写（Xk+1:N）的预测分布与真实数据分布。</li>
<li><strong>实现：</strong> 同样使用<strong>条件分布匹配蒸馏（CDMD）</strong>，但这次的教师是<strong>长上下文教师（Context Teacher）</strong>，它能够感知完整的生成历史 X1:k。学生模型需要学习预测 Xk+1:N。</li>
<li><strong>公式：</strong> Lcontext = KL(Po(Xk+1:N|X1:k) || Pdata(Xk+1:N|X1:k))。由于 Pdata 不可直接获得，使用 PT(Xk+1:N | X1:k) 作为代理。</li>
<li><strong>作用：</strong> 训练学生模型学习长期的时序依赖关系，确保视频在长时段内保持一致性，克服“漂移”问题。</li>
</ul>
</li>
</ul>
</li>
<li><strong>长自回归课程（Long Self-Rollout Curriculum）：</strong><ul>
<li><strong>动机：</strong> 直接在长序列上进行训练会导致分布偏移。</li>
<li><strong>实现：</strong> 采用动态的上下文长度（rollout length）策略。训练开始时使用较短的上下文长度（k ≈ kmin），然后逐渐增加上下文长度（k ~ U(kmin, Nmax)），直到接近完整的序列长度 N。</li>
<li><strong>作用：</strong> 逐步暴露模型于长上下文，避免早期训练中的严重分布偏移。</li>
</ul>
</li>
<li><strong>干净上下文策略（Clean Context Policy）：</strong><ul>
<li><strong>动机：</strong> 确保用于蒸馏的上下文（X1:k）是完全去噪的，以提供高质量的监督信号。</li>
<li><strong>实现：</strong> 对用于计算上下文的帧进行完全的几步去噪处理。</li>
<li><strong>作用：</strong> 保证了上下文的有效性和与教师训练分布的一致性。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>上下文管理系统（Context Management System）：</strong></p>
<ul>
<li><strong>动机：</strong> 处理极长视频（例如2分钟）的计算成本问题。直接存储所有历史帧的KV Cache会非常庞大。</li>
<li><strong>模型结构：</strong> 借鉴了“慢-快记忆”理论，将KV Cache划分为三个部分：<ul>
<li><strong>Attention Sink (S)：</strong> 初始的少量Token，用于稳定注意力机制，类似于StreamingLLM。</li>
<li><strong>Slow Memory (Cslow)：</strong> 一个长期的缓冲区，存储最多Ne个Token。它存储高熵的关键帧，并且只在检测到显著的新信息时更新。</li>
<li><strong>Fast Memory (Lfast)：</strong> 一个滚动式的FIFO队列，大小为Nl。它捕捉即时的局部上下文，具有短期记忆。</li>
</ul>
</li>
<li><strong>关键机制：</strong><ul>
<li><strong>惊喜度（Surprisal）驱动的合并策略：</strong><ul>
<li><strong>动机：</strong> 并非所有新信息都同等重要。需要优先存储那些携带重要状态转换或视觉变化（高惊喜度）的Token。</li>
<li><strong>实现：</strong> 通过比较当前Token的关键向量（kt）与前一个Token的关键向量（kt-1）的相似度（sim(kt, kt-1)）。如果相似度低于阈值τ，则认为该Token具有高惊喜度，被合并到Slow Memory。否则，被丢弃。</li>
<li><strong>作用：</strong> 确保Slow Memory存储的是真正有信息量的、代表时间演变的Token，而不是冗余的静态信息。</li>
</ul>
</li>
<li><strong>有界位置编码（Bounded Positional Encoding）：</strong><ul>
<li><strong>动机：</strong> 标准的自回归模型位置编码会随着时间无限增长，导致分布偏移。</li>
<li><strong>实现：</strong> 将所有Token的时间位置（ROPE位置）限制在一个固定的范围 [0, Ns+Nc+Nl-1] 内，无论生成步数t如何。</li>
<li><strong>作用：</strong> 稳定了注意力机制在长序列上的行为，防止位置编码带来的分布偏移。</li>
</ul>
</li>
</ul>
</li>
<li><strong>整体作用：</strong> 该系统有效地压缩了视觉冗余，使得模型能够处理比以往长得多的上下文（20秒以上），同时保持了计算效率。</li>
</ul>
</li>
</ol>
<h3 id="4_5">4. 方法对比分析</h3>
<ul>
<li><strong>本质区别：</strong><ul>
<li><strong>与短上下文模型（如LTX-Video, Wan2.1）：</strong> Context Forcing直接解决了长视频的遗忘问题，而短上下文模型本质上无法处理长视频。</li>
<li><strong>与长上下文微调基线（如LongLive, Self-Forcing++）：</strong><ul>
<li><strong>教师-学生匹配：</strong> LongLive等方法通常使用短上下文教师监督长上下文学生。Context Forcing的核心是使用<strong>长上下文教师</strong>来监督长上下文学生，消除了根本性的不匹配。</li>
<li><strong>鲁棒性训练：</strong> Context Forcing的教师模型经过ERFT训练，对学生产生的漂移具有鲁棒性，而其他方法可能直接使用标准训练的教师，在学生漂移时指导效果会下降。</li>
<li><strong>上下文管理：</strong> Context Forcing引入了创新的慢-快记忆系统和惊喜度驱动的合并策略，以高效管理长上下文，而其他方法可能采用更简单的KV Cache扩展或窗口策略。</li>
</ul>
</li>
<li><strong>与基于记忆的模型（如Framepack, WorldPlay）：</strong> Context Forcing将记忆机制（慢-快记忆）集成到蒸馏框架中，并且通过长上下文教师进行端到端的训练，而一些记忆模型可能侧重于记忆结构的创新，但训练范式上可能仍受限于短上下文教师。</li>
</ul>
</li>
<li><strong>创新贡献：</strong><ol>
<li><strong>长上下文教师-学生蒸馏范式：</strong> 首次提出并实现了长上下文教师指导长上下文学生的框架，根本上解决了长视频生成中的学生-教师不匹配问题。</li>
<li><strong>鲁棒上下文教师训练（ERFT）：</strong> 创新性地通过注入误差来训练教师模型，使其对学生模型的漂移具有鲁棒性，这是实现长期一致性的关键。</li>
<li><strong>慢-快记忆上下文管理系统：</strong> 设计了一个高效的上下文管理机制，结合惊喜度驱动的合并和有界位置编码，实现了对超长上下文（20秒以上）的高效处理。</li>
</ol>
</li>
<li><strong>适用场景：</strong><ul>
<li><strong>核心适用场景：</strong> 需要生成<strong>长时段内（几十秒到几分钟）保持高度视觉一致性</strong>的视频，例如：<ul>
<li>故事性视频生成</li>
<li>电影片段生成</li>
<li>虚拟世界模拟</li>
<li>需要角色、场景长期稳定的内容创作</li>
</ul>
</li>
<li><strong>不适用场景：</strong> 对视频的短期细节要求不高，或者对计算资源极其敏感，且对长期一致性要求不高的任务。</li>
</ul>
</li>
</ul>
<h3 id="5_5">5. 实验分析</h3>
<ul>
<li><strong>验证方法：</strong><ul>
<li><strong>定量评估：</strong><ul>
<li><strong>VBench数据集：</strong> 使用其官方扩展提示，评估5秒和60秒视频生成。</li>
<li><strong>MovieGenBench数据集：</strong> 使用100个文本提示，评估长视频（10秒）的连续性。</li>
<li><strong>评估指标：</strong><ul>
<li><strong>DINOv2, CLIP-F, CLIP-T：</strong> 用于评估结构身份、语义内容和提示对齐。</li>
<li><strong>Clip-F Score, Clip-T Score, Background Consistency, Subject Consistency：</strong> 用于衡量视频的长期一致性。</li>
<li><strong>Total Quality, Semantic Score, Background Consistency, Subject Consistency：</strong> 在不同上下文长度下进行评估。</li>
</ul>
</li>
</ul>
</li>
<li><strong>定性评估：</strong><ul>
<li><strong>可视化结果：</strong> 展示了与基线模型在1分钟视频生成上的对比（图4、图5），直观展示了Context Forcing在保持背景和主体一致性方面的优势。</li>
<li><strong>消融实验：</strong> 分析了慢内存采样策略、上下文DMD蒸馏、有界位置编码以及ERFT对模型性能的影响。</li>
</ul>
</li>
</ul>
</li>
<li><strong>关键结果：</strong><ul>
<li><strong>上下文长度提升：</strong> Context Forcing支持20秒以上的上下文，比现有SOTA方法（1.5-9.2秒）长2-10倍。</li>
<li><strong>一致性显著提升：</strong> 在60秒视频生成任务上，Context Forcing在Subject Consistency和Background Consistency等指标上大幅超越基线。例如，在Table 1中，Ours (teacher) 和 Ours (student) 在60s的Subject Consistency上分别达到了95.68和95.95，远高于其他模型。</li>
<li><strong>消融实验证明有效性：</strong><ul>
<li>慢内存采样策略（惊喜度驱动）优于均匀采样。</li>
<li>上下文DMD蒸馏是关键，移除后性能显著下降。</li>
<li>有界位置编码对稳定性和一致性至关重要。</li>
<li>ERFT训练的教师模型在面对学生漂移时表现更鲁棒（图7）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>优势场景：</strong><ul>
<li><strong>长视频生成：</strong> 在60秒甚至更长视频的生成任务上，Context Forcing展现出压倒性优势，尤其是在保持主体和背景的长期一致性方面。</li>
<li><strong>需要稳定身份和场景的模型：</strong> 对于需要角色、物体在长时间内保持不变的任务，Context Forcing效果显著。</li>
</ul>
</li>
<li><strong>局限性：</strong><ul>
<li><strong>计算开销：</strong> 尽管有慢-快记忆系统，但处理长视频仍然需要相当大的计算资源。</li>
<li><strong>上下文压缩效率：</strong> 论文提到“当前的记忆压缩策略仍然有优化信息密度的空间”，暗示慢-快记忆系统在压缩效率上仍有提升潜力。</li>
<li><strong>对教师模型的依赖：</strong> 教师模型的质量直接影响学生模型的性能，ERFT虽然提高了鲁棒性，但教师模型本身的生成能力仍是基础。</li>
</ul>
</li>
</ul>
<h3 id="6_5">6. 实用指南</h3>
<ul>
<li><strong>开源情况：</strong> 论文提供了代码链接（<code>https://github.com/TIGER-AI-Lab/Context-Forcing</code>），表明是开源的。</li>
<li><strong>实现细节：</strong><ul>
<li><strong>基础模型：</strong> Wan2.1-T2V-1.3B是其基础模型，可能需要预训练好的权重。</li>
<li><strong>数据集：</strong> VidProM, Sekai, Ultravideo等。</li>
<li><strong>超参数：</strong><ul>
<li>KV Cache大小：N₃=3, Nc=12, Nl=6。</li>
<li>惊喜度阈值τ=0.95。</li>
<li>课程学习的上下文长度范围：kmin, Nmax。</li>
<li>ERFT的误差注入方式和教师训练步数。</li>
</ul>
</li>
<li><strong>训练流程：</strong> 分为Stage 1（局部匹配）和Stage 2（上下文蒸馏），并采用动态上下文长度课程。</li>
</ul>
</li>
<li><strong>迁移可能：</strong><ul>
<li><strong>迁移到其他自回归视频生成模型：</strong> 核心思想（长上下文教师-学生蒸馏、ERFT、慢-快记忆）可以应用于其他基于Transformer或自回归的视频生成模型。</li>
<li><strong>迁移到其他模态：</strong> 理论上，长序列生成中的“遗忘-漂移”问题在其他模态（如长文本生成、长音频生成）中也存在。ERFT的鲁棒性训练和慢-快记忆的上下文管理思想可能可以迁移，但需要根据具体模态调整实现细节。</li>
</ul>
</li>
</ul>
<h3 id="7_5">7. 总结</h3>
<ul>
<li><strong>核心思想：</strong> <strong>长上下文教师指导长上下文学生，辅以高效记忆管理，解决视频长时一致性问题。</strong></li>
<li><strong>速记版pipeline：</strong><ol>
<li><strong>训练一个“不怕错”的长上下文教师。</strong></li>
<li><strong>用这个教师，分两步教学生生成短视频（打基础）和长视频（学连贯）。</strong></li>
<li><strong>用“慢-快记忆”系统，高效管理长视频的上下文信息。</strong></li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher.</li>
<li>To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy.</li>
<li>Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE.</li>
<li>By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06028v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06028v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2602.06013v1'></a></p>
<h2 id="genarena-how-can-we-achieve-human-aligned-evaluation-for-visual-generation-tasks"><a href="https://arxiv.org/abs/2602.06013v1">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</a></h2>
<p><strong>Authors:</strong> Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang</p>
<p><strong>Published:</strong> 2026-02-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇论文的方法部分，重点关注其创新之处和核心贡献。</p>
<hr />
<h2 id="genarena">论文方法分析与总结：GenArena</h2>
<h3 id="1_6">1. 摘要翻译</h3>
<p><strong>GenArena：如何实现视觉生成任务的面向人类对齐的评估？</strong></p>
<p>视觉生成模型的快速发展已经超越了传统的评估方法，迫切需要采用视觉语言模型（VLMs）作为代理裁判。在本文中，我们系统地研究了在广泛的视觉生成任务中，当前占主导地位的绝对点式评分标准的可靠性。我们的分析揭示，这种范式由于随机不一致性和与人类感知的不匹配而受到限制。为了解决这些局限性，我们引入了GENARENA，一个利用成对比较范式来确保稳定且面向人类对齐的评估的统一评估框架。关键的是，我们的实验揭示了一个颠覆性的发现：仅仅采用这种成对协议就能使现成的开源模型超越顶级的专有模型。具体来说，我们的方法将评估准确率提高了20%以上，并实现了与权威LMArena排行榜0.86的Spearman相关性，远超点式方法的0.36相关性。基于GENARENA，我们对各种任务上的最先进视觉生成模型进行了基准测试，为社区提供了一个严谨且自动化的视觉生成评估标准。</p>
<h3 id="2_6">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li>视觉生成模型（如扩散模型、多模态模型）的飞速发展，其能力已从基础的文本到图像合成扩展到复杂的图像编辑、组合等任务。</li>
<li>现有评估方法（如FID、CLIP Score）难以捕捉高保真生成任务中精细的语义对齐和美学细节。</li>
<li>人工评估虽然是金标准，但成本高昂且难以扩展。</li>
<li>迫切需要一种既可靠又可扩展的自动化评估方法。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>绝对点式评分（Pointwise Scoring）</strong>：<ul>
<li><strong>随机不一致性（Stochastic Inconsistency）</strong>：VLM在对同一样本进行多次评分时，结果可能不稳定，导致排名波动（如图1a所示）。</li>
<li><strong>与人类感知不匹配（Poor Alignment with Human Perception）</strong>：评分标准可能无法准确反映人类的偏好和判断。</li>
<li><strong>校准漂移（Calibration Drift）</strong>：VLM的评分可能受到输入顺序、提示词等因素的影响，导致偏差。</li>
<li><strong>“懒惰偏见”（Laziness Bias）</strong>：在面对复杂或难以区分的样本时，模型倾向于给出中间分数或“Tie”，而非做出明确判断（如Table 5和Figure A.3所示）。</li>
</ul>
</li>
<li><strong>专有模型依赖</strong>：当前高质量的评估往往依赖于顶级的专有VLM，这限制了研究的可及性和成本效益。</li>
<li><strong>开源模型潜力未被充分挖掘</strong>：现有的点式评分范式可能掩盖了开源VLM在评估方面的潜力。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>成对比较（Pairwise Comparison）范式比绝对点式评分更能保证评估的稳定性和与人类偏好的对齐。</li>
<li>现成的、未经过专门微调的开源VLM，通过采用成对比较范式，可以实现与专有模型相当甚至更优的评估性能。</li>
<li>通过大规模的成对比较并结合Elo评分系统，可以构建一个可靠、可复现且面向人类对齐的视觉生成评估基准。</li>
</ul>
</li>
</ul>
<h3 id="3_6">3. 方法设计详解</h3>
<p><strong>GENARENA 框架pipeline</strong></p>
<p>GENARENA是一个统一的、自动化的评估框架，其核心是利用VLMs进行成对比较，并使用Elo评分系统聚合结果，构建模型排行榜。</p>
<p><strong>Stage 1: Competitive Sampling (竞争性采样)</strong>
*   <strong>目标</strong>：为评估准备一组候选模型及其生成结果。
*   <strong>流程</strong>：
    1.  <strong>构建指令集 (I)</strong>：精心设计或收集一系列多样化的、具有挑战性的提示词（prompts），涵盖基础编辑、推理编辑和多参考组合等任务。
    2.  <strong>模型池 (M)</strong>：选择一组待评估的视觉生成模型（包括专有和开源模型）。
    3.  <strong>生成样本</strong>：让模型池中的每个模型根据指令集生成对应的输出样本。
    4.  <strong>成对组合</strong>：将模型生成的样本进行成对组合，为后续的VLM裁判做准备。</p>
<p><strong>Stage 2: Robust Pairwise Judging (鲁棒的成对裁判)</strong>
*   <strong>目标</strong>：利用VLM对成对的生成样本进行比较，并输出一个明确的偏好判断。
*   <strong>核心设计</strong>：<strong>Bi-Directional Consistency Protocol (双向一致性协议)</strong>
    1.  <strong>强制选择（Forced-Choice Constraint）</strong>：
        *   <strong>动机</strong>：避免“Tie”的出现，强制VLM做出选择，克服“懒惰偏见”。
        *   <strong>操作</strong>：对于一对模型A和B的输出（OA, OB），VLM会进行两次评估：一次是(OA, OB)，另一次是(OB, OA)。
        *   <strong>结果判断</strong>：
            *   如果两次评估结果一致（例如，第一次判断A&gt;B，第二次也判断A&gt;B），则记录为A&gt;B。
            *   如果两次评估结果不一致（例如，第一次判断A&gt;B，第二次判断B&gt;A），则该对被标记为“Tie”（值为0.5）。
            *   <strong>目的</strong>：这种双向检查旨在检测和过滤掉因位置偏见（position bias）或VLM内部不一致性导致的无效判断。只有在两次评估中都一致地偏向同一模型时，才被视为一个有效的“Win”。
    2.  <strong>多维度评估标准</strong>：VLM裁判被赋予一套详细的评估标准，包括：
        *   <strong>Text Faithfulness (文本忠实度)</strong>：是否准确遵循了编辑指令。
        *   <strong>Image Faithfulness (图像忠实度)</strong>：是否保留了原始图像的关键元素（构图、光照、风格等）。
        *   <strong>Overall Image Quality (整体图像质量)</strong>：输出的美学和技术质量，是否包含伪影或失真。
        *   <strong>Text Rendering (文本渲染)</strong>：如果输出包含文本，则评估文本的准确性、可读性和与图像的融合度。
    3.  <strong>JSON输出格式</strong>：VLM裁判输出包含详细的理由（reasoning）、哪个响应更好（better_response）、评分（score，1-6分制）和置信度（confidence）。</p>
<p><strong>Stage 3: Global Elo Aggregation (全局Elo聚合)</strong>
*   <strong>目标</strong>：将大量的成对比较结果转化为一个稳定、可复现的模型排行榜。
*   <strong>核心算法</strong>：<strong>Elo Rating System (Elo评分系统)</strong>
    1.  <strong>模型</strong>：基于Bradley-Terry (BT)模型，将每个模型视为一个具有潜在“技能”或“等级”R的玩家。
    2.  <strong>概率模型</strong>：模型i击败模型j的概率 P(i &gt; j) 由它们技能差的逻辑函数决定：<script type="math/tex">P(i > j) = \frac{1}{1 + 10^{(R_j - R_i)/\xi}}</script>，其中<script type="math/tex">\xi</script>是缩放因子（通常设为400）。
    3.  <strong>数据构建</strong>：构建一个Win-matrix <script type="math/tex">W_{ij}</script>，记录模型i相对于模型j的总胜场数。如果成对比较结果是Tie (0.5)，则<script type="math/tex">W_{ij}</script>和<script type="math/tex">W_{ji}</script>都增加0.5。
    4.  <strong>优化</strong>：通过最大化似然函数 <script type="math/tex">L(R) = \sum_{i \neq j} W_{ij} \ln P(i > j)</script> 来估计所有模型的Elo评分<script type="math/tex">R</script>。这可以通过逻辑回归或L-BFGS算法求解。
    5.  <strong>输出</strong>：生成一个基于Elo分数的模型排行榜。</p>
<p><strong>模型结构/算法解释</strong></p>
<ul>
<li><strong>Bi-Directional Consistency Protocol</strong>：这是核心创新之一。通过强制VLM进行两次反向评估，并仅在结果一致时才记录为有效胜负，有效地消除了位置偏见和VLM内部的随机波动。这使得VLM的判断更加稳定和可靠。</li>
<li><strong>Elo Rating System</strong>：这是一个成熟的、用于评估玩家（模型）相对技能的系统。其优势在于能够从大量的成对比较中提取出全局的、相对的排名，并且对单个比较结果的噪声具有鲁棒性。它将离散的胜负关系转化为连续的评分，从而提供更精细的排名。</li>
<li><strong>多维度评估标准</strong>：为VLM裁判提供了明确的指导，使其评估更具针对性和可解释性，覆盖了文本忠实度、图像忠实度、整体质量和文本渲染等关键方面。</li>
</ul>
<h3 id="4_6">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>点式评分 vs. 成对比较</strong>：<ul>
<li>点式评分：给每个样本打一个绝对分数，然后比较分数。容易受VLM内部校准、评分尺度不一致等问题影响，导致不稳定。</li>
<li>成对比较：直接比较两个样本的优劣，只做相对判断。这种二元决策更符合人类的直觉，且能有效规避绝对评分的许多问题。</li>
</ul>
</li>
<li><strong>强制选择 vs. 允许Tie</strong>：<ul>
<li>允许Tie：VLM可以直接输出“Tie”，这可能导致模型回避困难判断，产生“懒惰偏见”。</li>
<li>强制选择（结合双向一致性）：迫使VLM做出选择，并通过双向检查来过滤掉无效的Tie或错误判断，确保最终的有效比较。</li>
</ul>
</li>
<li><strong>Elo评分系统</strong>：将成对比较结果聚合为全局排名，比简单的胜率统计更鲁棒，能处理更复杂的比较关系。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ol>
<li><strong>系统性揭示点式评分的局限性</strong>：通过实验证明了点式评分在一致性、人类对齐方面的不足。</li>
<li><strong>提出并验证了成对比较范式作为更优选择</strong>：证明了仅改变评分范式就能显著提升VLM评估能力。</li>
<li><strong>引入Bi-Directional Consistency Protocol</strong>：一种有效解决VLM评估中位置偏见和随机性的新方法。</li>
<li><strong>构建GENARENA基准</strong>：一个结合了成对比较、Elo评分系统和多维度评估标准的全面、自动化评估框架。</li>
<li><strong>证明了开源VLM的潜力</strong>：通过GENARENA，展示了现成的开源VLM在成对比较范式下可以超越专有模型。</li>
</ol>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>视觉生成任务</strong>：特别是图像编辑、图像组合、文本到图像生成等，这些任务的评估往往需要细致的语义和视觉判断。</li>
<li><strong>需要高精度、可复现、可扩展的自动化评估场景</strong>。</li>
<li><strong>研究和开发过程中，用于模型迭代和比较</strong>。</li>
</ul>
</li>
</ul>
<h3 id="5_6">5. 实验分析</h3>
<ul>
<li><strong>验证方法</strong>：<ul>
<li><strong>实验设计</strong>：<ol>
<li><strong>点式 vs. 成对比较的对比</strong>：在多个数据集（GenAI-Bench, EditScore-Bench, VideoGen-RewardBench）上，使用相同的开源VLM（如Qwen3-VL 8B Instruct）分别采用点式和成对比较范式进行评估，比较其准确率和与人类偏好的相关性。</li>
<li><strong>人类对齐度验证</strong>：将GENARENA（成对比较+Elo）生成的排行榜与权威的LMArena排行榜进行Spearman相关性分析。</li>
<li><strong>VLM裁判的Scalability分析</strong>：测试不同参数规模的Qwen3-VL模型作为裁判时的表现，以确定最佳裁判模型。</li>
<li><strong>Tie策略的消融实验</strong>：比较“强制选择”和“允许Tie”策略对评估准确性的影响。</li>
</ol>
</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>成对比较显著优于点式评分</strong>：准确率提升超过20%，Spearman相关性从0.36提升到0.86（与LMArena）。</li>
<li><strong>开源VLM潜力释放</strong>：使用成对比较范式，开源VLM（如Qwen3-VL 8B Instruct）的性能超越了许多专有模型。</li>
<li><strong>Bi-Directional Consistency Protocol有效</strong>：显著提高了评估的稳定性和准确性。</li>
<li><strong>模型规模影响对齐度</strong>：更大的VLM模型（如Qwen3-VL-32B FP8）与人类偏好对齐度更高。</li>
<li><strong>强制选择策略优于允许Tie</strong>：避免了“懒惰偏见”，提高了准确性。</li>
</ul>
</li>
<li><strong>优势场景</strong>：<ul>
<li><strong>复杂编辑和组合任务</strong>：在这些任务中，细微的差异对人类判断至关重要，而成对比较更能捕捉这些差异。</li>
<li><strong>需要区分细微差别的场景</strong>：如Figure A.4和A.5所示，成对比较能更准确地判断出哪个模型在细节上做得更好。</li>
</ul>
</li>
<li><strong>局限性</strong>：<ul>
<li><strong>VLM的固有偏见</strong>：尽管努力缓解，但VLM仍可能继承训练数据中的社会偏见。</li>
<li><strong>计算开销</strong>：大规模的成对比较需要大量的计算资源。</li>
<li><strong>裁判模型的选择</strong>：虽然作者推荐了Qwen3-VL-32B FP8，但最佳裁判模型可能随时间推移和新模型出现而变化。</li>
<li><strong>数据集的覆盖范围</strong>：虽然使用了多个数据集，但仍可能存在未覆盖到的特定任务或场景。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="6_6">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提供了GitHub链接（https://github.com/ruihanglix/genarena）和Hugging Face上的leaderboard/datasets，表明代码和数据是开源的。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>VLM裁判选择</strong>：推荐使用Qwen3-VL-32B Instruct FP8，因为它在对齐度和效率上表现最佳。</li>
<li><strong>提示词工程</strong>：VLM裁判的系统提示词（prompt）非常关键，需要包含明确的评估标准和指导方针。</li>
<li><strong>成对比较协议</strong>：必须严格执行Bi-Directional Consistency Protocol，包括强制选择和双向检查。</li>
<li><strong>Elo评分系统</strong>：使用标准的Elo评分算法进行聚合。</li>
<li><strong>数据集</strong>：可以使用论文提供的GenAI-Bench, EditScore-Bench, VideoGen-RewardBench等数据集。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>迁移到其他视觉生成任务</strong>：GENARENA的框架（成对比较+Elo）是通用的，可以应用于任何需要比较模型输出的视觉生成任务，只需调整评估标准和数据集。</li>
<li><strong>迁移到其他模态</strong>：理论上，如果能为其他模态（如文本生成、多模态理解）设计合适的评估标准，并且有合适的VLM作为裁判，该框架也可以被迁移。</li>
<li><strong>使用不同的VLM裁判</strong>：如果现有VLM不适用，可以尝试其他强大的多模态模型作为裁判，但需要重新进行对齐度和可靠性验证。</li>
</ul>
</li>
</ul>
<h3 id="7_6">7. 总结</h3>
<ul>
<li><strong>核心思想</strong>：用成对比较和Elo评分系统，实现更可靠、人类对齐的视觉生成模型评估。</li>
<li><strong>速记版pipeline</strong>：<ol>
<li><strong>准备样本</strong>：让模型生成结果。</li>
<li><strong>VLM成对裁判</strong>：强制VLM比较，并进行双向检查。</li>
<li><strong>Elo评分聚合</strong>：用Elo系统计算模型排名。</li>
<li><strong>生成排行榜</strong>：得到最终的评估结果。</li>
</ol>
</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation.</li>
<li>Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods.</li>
<li>Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2602.06013v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2602.06013v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-02-06 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
