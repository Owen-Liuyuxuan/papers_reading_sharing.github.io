<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-15 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-12/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-16/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-15">Arxiv Computer Vision Papers - 2025-09-15</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#compressed-video-quality-enhancement-classifying-and-benchmarking-over-standards" class="nav-link">Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-understanding-visual-grounding-in-visual-language-models" class="nav-link">Towards Understanding Visual Grounding in Visual Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#event-camera-guided-visual-media-restoration-3d-reconstruction-a-survey" class="nav-link">Event Camera Guided Visual Media Restoration &amp; 3D Reconstruction: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-sam-adapter-for-semantic-segmentation" class="nav-link">Multimodal SAM-adapter for Semantic Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#compute-only-16-tokens-in-one-timestep-accelerating-diffusion-transformers-with-cluster-driven-feature-caching" class="nav-link">Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching</a>
                </li>
                <li class="nav-item">
                    <a href="#magicmirror-a-large-scale-dataset-and-benchmark-for-fine-grained-artifacts-assessment-in-text-to-image-generation" class="nav-link">MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#layerlock-non-collapsing-representation-learning-with-progressive-freezing" class="nav-link">LayerLock: Non-collapsing Representation Learning with Progressive Freezing</a>
                </li>
                <li class="nav-item">
                    <a href="#varco-vision-20-technical-report" class="nav-link">VARCO-VISION-2.0 Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#bevtraj-map-free-end-to-end-trajectory-prediction-in-birds-eye-view-with-deformable-attention-and-sparse-goal-proposals" class="nav-link">BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals</a>
                </li>
                <li class="nav-item">
                    <a href="#lav-cot-language-aware-visual-cot-with-multi-aspect-reward-optimization-for-real-world-multilingual-vqa" class="nav-link">LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-15">Arxiv Computer Vision Papers - 2025-09-15</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹æ¨æä¾ç Arxiv è®ºæåè¡¨çæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ï¼</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-12)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºææ¶µçäºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçå¤ä¸ªåæ²¿æ¹åï¼ä¸»è¦è¶å¿åæ¬ï¼</p>
<ul>
<li><strong>å¤æ¨¡æèåä¸è§è§è¯­è¨æ¨¡å (VLM) çæ·±å¥æ¢ç´¢ï¼</strong> å¤ç¯è®ºæå³æ³¨ VLM ççè§£ãåºç¨åä¼åï¼ç¹å«æ¯è§è§æ¥å° (Visual Grounding) åå¤è¯­è¨è§è§é®ç­ (VQA)ã</li>
<li><strong>é«æä¸å éï¼</strong> éå¯¹è§é¢å¤çãæ©æ£æ¨¡ååè¡¨ç¤ºå­¦ä¹ çæçæåæ¯éè¦ä¸»é¢ï¼æ¨å¨éä½è®¡ç®ææ¬å¹¶æé«å®æ¶æ§è½ã</li>
<li><strong>æ°æ®ä¸åºåï¼</strong> æ°æ°æ®éååºåçåå¸ï¼ç¹å«æ¯éå¯¹ææ¬å°å¾åçæä¸­çä¼ªå½±è¯ä¼°ï¼å¸æ¾äºé«è´¨éæ°æ®å¨æ¨¡åå¼åä¸­çå³é®ä½ç¨ã</li>
<li><strong>ç¹å®åºç¨é¢åçè¿å±ï¼</strong> è§é¢è´¨éå¢å¼ºãäºä»¶ç¸æºåºç¨ãèªå¨é©¾é©¶ä¸­çè½¨è¿¹é¢æµä»¥åè¯­ä¹åå²ç­é¢ååæåæ°ã</li>
<li><strong>åºç¡æ¨¡åéåºä¸ä¼åï¼</strong> å¦ä½å°å¤§ååºç¡æ¨¡åï¼å¦ SAMï¼éåºå°ç¹å®ä»»å¡ï¼ä»¥åè¡¨ç¤ºå­¦ä¹ çç¨³å®æ§é®é¢ä¹åå°å³æ³¨ã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching" (Zhixin Zheng et al.)ï¼</strong> è¿ç¯è®ºæå¨å éæ©æ£æ¨¡åæ¹é¢å±ç°äºæ¾èçåæ°ãéè¿å¼å¥èç±»é©±å¨çç¹å¾ç¼å­ï¼å®ææå¤§å¹éä½æ©æ£ Transformer çè®¡ç®ææ¬ï¼å¯¹äºå®æ¶çæåèµæºåéç¯å¢ä¸çåºç¨å·æéè¦æä¹ã</li>
<li><strong>"MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation" (Jia Wang et al.)ï¼</strong> éçææ¬å°å¾åçææ¨¡åçæ®åï¼è¯ä¼°å¶çæè´¨éï¼ç¹å«æ¯ç»ç²åº¦ä¼ªå½±ï¼åå¾è³å³éè¦ãMagicMirror æä¾äºä¸ä¸ªæ¥éçå¤§è§æ¨¡æ°æ®éååºåï¼å°æå¤§å°æ¨å¨è¯¥é¢åçç ç©¶åæ¨¡åæ¹è¿ã</li>
<li><strong>"LayerLock: Non-collapsing Representation Learning with Progressive Freezing" (Goker Erdogan et al.)ï¼</strong> å¨è¡¨ç¤ºå­¦ä¹ ä¸­ï¼é¿åæ¨¡åå´©æºæ¯ä¸ä¸ªæ ¸å¿ææãLayerLock æåºçæ¸è¿å¼å»ç»ç­ç¥ä¸ºè§£å³è¿ä¸é®é¢æä¾äºä¸ç§æ°é¢ä¸å¯è½æ´ç¨³å®çæ¹æ³ï¼å¯¹èªçç£å­¦ä¹ ååºç¡æ¨¡åè®­ç»å·ææ½å¨å½±åã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>äºä»¶ç¸æºä¸å¤æ¨¡æèåï¼</strong> "Event Camera Guided Visual Media Restoration &amp; 3D Reconstruction: A Survey" å¼ºè°äºäºä»¶ç¸æºå¨ä¼ ç»è§è§ä»»å¡ä¸­çç¬ç¹ä¼å¿ï¼é¢ç¤ºçå¶ä¸ä¼ ç»ç¸æºæ°æ®èåçæ´å¤åºç¨ã</li>
<li><strong>æ©æ£æ¨¡åå éä¸ä¼åï¼</strong> é¤äºä¸è¿°çç¹å¾ç¼å­ï¼å¯¹æ©æ£æ¨¡åè®¡ç®æççæç»­å³æ³¨å°æ¯æªæ¥ç ç©¶çéç¹ã</li>
<li><strong>å¤è¯­è¨ VQA ä¸å¥å±ä¼åï¼</strong> "LaV-CoT" æåºçè¯­è¨æç¥è§è§ CoT åå¤æ¹é¢å¥å±ä¼åï¼ä¸ºå¤ççå®ä¸çå¤è¯­è¨ VQA å¸¦æ¥äºæ°çæè·¯ï¼å¼ºè°äºå¯¹è¯­è¨åæåå¤æ ·æ§çå³æ³¨ã</li>
<li><strong>åºç¡æ¨¡åï¼å¦ SAMï¼çè½»éçº§ééï¼</strong> "Multimodal SAM-adapter" å±ç¤ºäºå¦ä½é«æå°å°å¤§ååºç¡æ¨¡ååºç¨äºç¹å®ä»»å¡ï¼è¿æ¯ä¸ç§éè¦çå·¥ç¨åç ç©¶æ¹åã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼ä»¥ä¸è®ºæå¼å¾ä¼åéè¯»ï¼</p>
<ul>
<li><strong>å¯¹äºå³æ³¨æ¨¡åæçåçæå¼ AI çç ç©¶äººåï¼</strong><ul>
<li><strong>"Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching"</strong> (Zhixin Zheng et al.)</li>
<li><strong>"MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation"</strong> (Jia Wang et al.)</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨è§è§è¯­è¨æ¨¡ååå¤æ¨¡æçè§£çç ç©¶äººåï¼</strong><ul>
<li><strong>"Towards Understanding Visual Grounding in Visual Language Models"</strong> (Georgios Pantazopoulos, Eda B. ÃzyiÄit)</li>
<li><strong>"LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA"</strong> (Jing Huang et al.)</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨åºç¡æ¨¡åãè¡¨ç¤ºå­¦ä¹ åéç¨è§è§ä»»å¡çç ç©¶äººåï¼</strong><ul>
<li><strong>"LayerLock: Non-collapsing Representation Learning with Progressive Freezing"</strong> (Goker Erdogan et al.)</li>
<li><strong>"Multimodal SAM-adapter for Semantic Segmentation"</strong> (Iacopo Curti et al.)</li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨èªå¨é©¾é©¶åå®æ¶æç¥çç ç©¶äººåï¼</strong><ul>
<li><strong>"BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals"</strong> (Minsang Kong et al.)</li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæãå»ºè®®æ ¹æ®æ¨çå·ä½å´è¶£ï¼è¿ä¸æ­¥æ·±å¥éè¯»ææ¨èçè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.10407v1">Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards</a></li>
<li><a href="#2509.10345v1">Towards Understanding Visual Grounding in Visual Language Models</a></li>
<li><a href="#2509.09971v1">Event Camera Guided Visual Media Restoration &amp; 3D Reconstruction: A Survey</a></li>
<li><a href="#2509.10408v1">Multimodal SAM-adapter for Semantic Segmentation</a></li>
<li><a href="#2509.10312v1">Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching</a></li>
<li><a href="#2509.10260v1">MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation</a></li>
<li><a href="#2509.10156v1">LayerLock: Non-collapsing Representation Learning with Progressive Freezing</a></li>
<li><a href="#2509.10105v1">VARCO-VISION-2.0 Technical Report</a></li>
<li><a href="#2509.10080v1">BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals</a></li>
<li><a href="#2509.10026v1">LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.10407v1'></a></p>
<h2 id="compressed-video-quality-enhancement-classifying-and-benchmarking-over-standards"><a href="https://arxiv.org/abs/2509.10407v1">Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards</a></h2>
<p><strong>Authors:</strong> Xiem HoangVan, Dang BuiDinh, Sang NguyenQuang, Wen-Hsiao Peng</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xiem HoangVanç­äººæ°åçè®ºæâCompressed Video Quality Enhancement: Classifying and Benchmarking over Standardsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼åç¼©è§é¢è´¨éå¢å¼ºï¼åºäºæ åçåç±»ä¸åºåæµè¯</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æåç¼©è§é¢è´¨éå¢å¼ºï¼CVQEï¼é¢åç ç©¶ä¸­å­å¨çå±éæ§ãå°½ç®¡åºäºæ·±åº¦å­¦ä¹ çCVQEæ¹æ³åå¾äºæ¾èè¿å±ï¼ä½ç°æç»¼è¿°ç¼ºä¹ç³»ç»æ§åç±»ï¼æªè½å°æ¹æ³ä¸ç¹å®æ ååä¼ªå½±å³èèµ·æ¥ï¼å¯¹ä¸åç¼ç ç±»åï¼å¸§å/å¸§é´ï¼çæ¶æèå¼ç¼ºä¹ååçæ¯è¾åæï¼å°¤å¶æ¯å¨æ°å´æ åå¦H.266/VVCæ¹é¢ï¼ä»¥ååºåæµè¯å®è·µä¸å®åï¼å¯¼è´æ§è½è¯ä¼°ä¸ä¸è´ä¸æ¨¡åéæ©ç¼ºä¹ä¾æ®ãæ ¸å¿é®é¢æ¯å¦ä½å»ºç«ä¸ä¸ªç»ä¸ãç³»ç»ä¸å¨é¢çæ¡æ¶ï¼ä»¥åç±»ãè¯ä¼°åæ¯è¾ä¸åCVQEæ¹æ³ï¼ä»èä¿è¿è¯¥é¢åçç ç©¶åå®éé¨ç½²ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼è¯¥è®ºææåºäºä¸é¡¹å³é®è´¡ç®ï¼</p>
<ul>
<li><strong>æ°é¢çåç±»æ³ï¼</strong> è®ºæå¼å¥äºä¸ä¸ªå¨æ°çåç±»ä½ç³»ï¼å°CVQEæ¹æ³æ ¹æ®å¶<strong>æ¶æèå¼</strong>ï¼å¦CNNãæ³¨æåæºå¶ãæ··åæ¨¡åï¼ã<strong>è§é¢ç¼ç æ å</strong>ï¼H.264/AVCãH.265/HEVCãH.266/VVCï¼ä»¥å<strong>åç¼©åç¹å¾å©ç¨</strong>æåµè¿è¡ç³»ç»æ§åç±»ãè¿ä¸ºçè§£åç»ç»ç°ææ¹æ³æä¾äºæ¸æ°çç»æã</li>
<li><strong>ç»ä¸çåºåæµè¯æ¡æ¶ï¼</strong> è®ºææåºäºä¸ä¸ªéæäºç°ä»£åç¼©åè®®åæ åæµè¯åºåçç»ä¸åºåæµè¯æ¡æ¶ãè¯¥æ¡æ¶æ¯æ<strong>å¤æ åãå¤éååæ°ï¼QPï¼</strong>ä¸ç<strong>å¤ååè¯ä¼°</strong>ï¼åæ¬å®¢è§è´¨éææ ï¼APSNRãASSIMï¼åè®¡ç®å¤æåº¦ææ ï¼åæ°éãFLOPsï¼ï¼ç¡®ä¿äºè¯ä¼°çå¬å¹³æ§åå¯å¤ç°æ§ã</li>
<li><strong>ç³»ç»æ§åæä¸æè¡¡ï¼</strong> è®ºæå¯¹æåè¿çCVQEæ¹æ³å¨<strong>éå»ºæ§è½</strong>å<strong>è®¡ç®å¤æåº¦</strong>ä¹é´çå³é®æè¡¡è¿è¡äºç³»ç»æ§åæãéè¿è¯¦ç»çæ°æ®åæï¼æ­ç¤ºäºä¸åæ¹æ³å¨è´¨éæååèµæºæ¶èä¹é´çå³ç³»ï¼å¹¶æåºäºæªæ¥ç ç©¶çæ½å¨æ¹åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæçåææ­ç¤ºäºä»¥ä¸ä¸»è¦ç»æåæä¹ï¼</p>
<ul>
<li><strong>HEVCæ¯ç ç©¶ç­ç¹ï¼</strong> H.265/HEVCä»ç¶æ¯CVQEç ç©¶ä¸­æä¸»è¦çç¼ç æ åï¼ç¸å³æ¹æ³æ°éæå¤ï¼ä¸å¨å¸§ååå¸§é´å¢å¼ºç­ç¥ä¸åææ¾èè¿å±ã</li>
<li><strong>æ³¨æåæºå¶åæ··åæ¶æçä¼å¿ï¼</strong> åºäºæ³¨æåæºå¶çæ¨¡åï¼å¦OVQEï¼å¨APSNRåASSIMææ ä¸è¡¨ç°åºæé«çæ§è½ï¼ä½éå¸¸ä¼´éçè¾é«çè®¡ç®å¤æåº¦ãæ··åæ¶æï¼å¦CTVEãSTFFï¼å¨æ§è½åæçä¹é´åå¾äºæ´å¥½çå¹³è¡¡ï¼è½å¤ä»¥è¾ä½çè®¡ç®ææ¬å®ç°æ¥è¿Transformerçéå»ºè´¨éã</li>
<li><strong>å¸§é´æ¹æ³ä¼äºå¸§åæ¹æ³ï¼</strong> å¸§é´å¢å¼ºæ¹æ³ï¼å©ç¨å¤å¸§æ¶ç©ºç¸å³æ§ï¼éå¸¸æ¯å¸§åæ¹æ³ï¼ä»å¤çåå¸§ï¼è¡¨ç°åºæ´ä¼å¼çæ§è½ã</li>
<li><strong>VVCç ç©¶çåæé¶æ®µï¼</strong> éå¯¹H.266/VVCçCVQEç ç©¶ä»å¤äºæ©æé¶æ®µï¼ç°ææ¹æ³å¤ä¸ºå¯¹HEVCæ¨¡åçç´æ¥ééï¼å°æªååå©ç¨VVCç¹æçç¼ç å·¥å·åç»æãå°½ç®¡OVQEå¨VVCåºåä¸è¡¨ç°æä½³ï¼ä½è¿è¡¨æVVCä¸å±è®¾è®¡ä»æå¾æ·±å¥æ¢ç´¢ã</li>
<li><strong>åç¼©åä¿¡æ¯å©ç¨çæ½åï¼</strong> è®ºæå¼ºè°äºå©ç¨åç¼©åä¿¡æ¯ï¼å¦è¿å¨ç¢éãæ®å·®ä¿¡å·ãCTUç»æï¼çæ½åï¼è¿å¯ä»¥æ¾èéä½è®¡ç®å¼éå¹¶ä¿çå³é®çåç¼©æç¥çº¿ç´¢ï¼ä½å¦ä½ææå©ç¨è¿äºç¹å¾ä»æ¯ä¸ä¸ªææã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææ¬èº«æåºäºç°æCVQEç ç©¶ååºåæµè¯çå±éæ§ï¼è¿ä¹æ¯å¶å·¥ä½å¨æºï¼</p>
<ul>
<li>ç°æç»¼è¿°ç¼ºä¹ç³»ç»æ§åç±»ï¼æªè½å°æ¹æ³ä¸ç¹å®æ ååä¼ªå½±å³èã</li>
<li>å¯¹ä¸åç¼ç ç±»åï¼å¸§å/å¸§é´ï¼çæ¶æèå¼ç¼ºä¹ååçæ¯è¾åæï¼å°¤å¶æ¯å¨æ°å´æ åå¦H.266/VVCæ¹é¢ã</li>
<li>åºåæµè¯å®è·µä¸å®åï¼å¯¼è´æ§è½è¯ä¼°ä¸ä¸è´ä¸æ¨¡åéæ©ç¼ºä¹ä¾æ®ã</li>
<li>éå¯¹VVCçCVQEç ç©¶ä»å¤äºæ©æé¶æ®µï¼ç°ææ¹æ³å¤ä¸ºå¯¹HEVCæ¨¡åçç´æ¥ééï¼æªè½ååå©ç¨VVCç¹æçç¼ç å·¥å·åç»æã</li>
<li>å¨å®æ¶é¨ç½²æ¹é¢ï¼é«æ§è½çæ³¨æåæºå¶åå¸§é´æ¹æ³éå¸¸å·æè¾é«çè®¡ç®éæ±ï¼è¿éå¶äºå®ä»¬å¨èµæºåéåºæ¯ä¸çåºç¨ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºè®ºæçåæååç°ï¼æªæ¥çç ç©¶æ¹ååæ¬ï¼</p>
<ul>
<li><strong>VVCä¸å±çCVQEè®¾è®¡ï¼</strong> é´äºVVCçææ°æ§åå¶ç¬ç¹çç¼ç å·¥å·ï¼å¼åä¸é¨éå¯¹VVCåç¼©ä¼ªå½±çæ·±åº¦å­¦ä¹ æ¨¡åï¼ååå©ç¨å¶ç¼ç ç»æåä¿¡æ¯ï¼æ¯æªæ¥çéè¦æ¹åã</li>
<li><strong>é«æçæ··åæ¶æï¼</strong> è¿ä¸æ­¥æ¢ç´¢åä¼åæ··åæ¶æï¼ä»¥å¨éå»ºæ§è½åè®¡ç®æçä¹é´åå¾æ´å¥½çå¹³è¡¡ï¼ä½¿å¶æ´éç¨äºå®æ¶åèµæºåéçåºç¨ã</li>
<li><strong>åç¼©åä¿¡æ¯å©ç¨ï¼</strong> æ·±å¥ç ç©¶å¦ä½æ´ææå°å©ç¨åç¼©åä¿¡æ¯ï¼å¦è¿å¨ç¢éãæ®å·®ãCTUç»æç­ï¼ï¼ä»¥éä½è®¡ç®å¼éå¹¶æé«å¢å¼ºæ§è½ï¼åæ¶ä¿ææ¶ç©ºä¸è´æ§ã</li>
<li><strong>å¤ä»»å¡å­¦ä¹ åèªéåºæ¹æ³ï¼</strong> å¼åè½å¤èªéåºä¸ååç¼©çº§å«ãè§é¢åå®¹åç¼ç æ åçCVQEæ¨¡åï¼æé«å¶æ³åè½ååé²æ£æ§ã</li>
<li><strong>æç¥è´¨éä¼åï¼</strong> é¤äºä¼ ç»çPSNR/SSIMç­å®¢è§ææ ï¼æªæ¥ç ç©¶åºæ´å¤å³æ³¨äººç±»è§è§ç³»ç»æç¥çè´¨éï¼å¼åæ´ç¬¦åäººç¼æç¥çæå¤±å½æ°åè¯ä¼°ææ ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¨é¢æ¦æ¬è®ºæçæ ¸å¿åå®¹ï¼çªåºå¶å¨CVQEé¢ååç±»ãåºåæµè¯åæ§è½åææ¹é¢çéè¦è´¡ç®ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these gaps, this paper presents three key
contributions.</li>
<li>First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization.</li>
<li>Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10407v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10407v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10345v1'></a></p>
<h2 id="towards-understanding-visual-grounding-in-visual-language-models"><a href="https://arxiv.org/abs/2509.10345v1">Towards Understanding Visual Grounding in Visual Language Models</a></h2>
<p><strong>Authors:</strong> Georgios Pantazopoulos, Eda B. ÃzyiÄit</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Georgios PantazopoulosåEda B. ÃzyiÄitæ°åçè®ºæâTowards Understanding Visual Grounding in Vision-Language Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_2">è®ºææè¦ï¼çè§£è§è§è¯­è¨æ¨¡åä¸­çè§è§åºç¡</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨å¨é¢çè§£è§è§åºç¡ï¼Visual Groundingï¼å¨ç°ä»£è§è§è¯­è¨æ¨¡åï¼VLMsï¼ä¸­çä½ç¨ãåå±ãææåæªæ¥æ¹åãè§è§åºç¡æ¯ææ¨¡åæ ¹æ®ææ¬æè¿°å¨è§è§è¾å¥ï¼å¾åæè§é¢ï¼ä¸­è¯å«åå®ä½ç¹å®åºåçè½åãä½èè®¤ä¸ºï¼å°½ç®¡è§è§åºç¡å¨å¤æ¨¡æäººå·¥æºè½ä¸­è³å³éè¦ï¼ä½å¶å¨VLMsä¸­çå¤æ¹é¢å½±åãæ¶æéæ©ãè®­ç»èå¼ä»¥åä¸å¤æ¨¡ææ¨ççå¤æå³ç³»ä»éæ·±å¥æ¢è®¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¨é¢ç»¼è¿°è§è§åºç¡çæ¼åï¼</strong> è®ºæè¿½æº¯äºè§è§åºç¡ä»æ©æç»åCNNsåRNNsçæ¹æ³ï¼å°åºäºTransformerçå¤ä»»å¡è§è§è¯­è¨é¢è®­ç»ï¼VLPï¼æ¨¡åï¼åå°å½åå©ç¨VLMså®ç°æ¥å°ææ¬çæçåå±åç¨ã
*   <strong>ç»è´åæåºåè¡¨ç¤ºæ¹æ³ï¼</strong> æ¢è®¨äºå¯¹è±¡ä¸­å¿ï¼Object-centricï¼ååç´ çº§ï¼Pixel-levelï¼ä¸¤ç§è§è§åèè¡¨ç¤ºæ¹æ³ï¼å¹¶è¿ä¸æ­¥ç»åäºåç´ çº§æ¹æ³ä¸­çç¦»æ£åæ ååå§åæ è¡¨ç¤ºï¼å¼ºè°äºå®ä»¬å¯¹æ¨¡åæ§è½åç©ºé´ç²¾åº¦çå½±åã
*   <strong>æ·±å¥åæVLMæ¶æç»ä»¶ï¼</strong> è¯¦ç»åæäºè§è§ç¼ç å¨ï¼Vision Encoderï¼ãå¤æ¨¡æè¿æ¥å¨ï¼Multimodal Connectorï¼åè¯­è¨éª¨å¹²ï¼Language Backboneï¼ç­æ ¸å¿ç»ä»¶å¦ä½å½±åè§è§åºç¡è½åãç¹å«è®¨è®ºäºå¾ååè¾¨çå¤çï¼ä½ç½®ç¼ç æå¼ãä»»æåè¾¨çå¾åå¤çï¼åè§è§-ææ¬è¡¨ç¤ºè¿æ¥ï¼çº¿æ§/éçº¿æ§æ å°ãåºååç¼©æ¹æ³å¦æ± åãå·ç§¯ãäº¤åæ³¨æåãééæ ·å¨ï¼ç­å³é®è®¾è®¡éæ©ã
*   <strong>è®­ç»æµç¨ä¸å¯¹é½æºå¶ï¼</strong> éè¿°äºVLMå¤é¶æ®µè®­ç»æµç¨ï¼åæ¬å¾å-ææ¬å¯¹é½é¢è®­ç»åå¾®è°ï¼SFTãå¼ºåå­¦ä¹ ï¼ãå¼ºè°äºå¨è®­ç»ä¸­èå¥è§è§åºç¡ç®æ çéè¦æ§ï¼ä»¥åå¦ä½éè¿é¾å¼æèï¼Chain-of-Thoughtï¼åæ¨çæ¥å¢å¼ºå¤æ¨¡æçè§£ã
*   <strong>å¹¿æ³çè¯ä¼°é¢ååææ ï¼</strong> æ»ç»äºè§è§åºç¡å¨å¤ç§åºç¨ä¸­çè¯ä¼°åºååæ°æ®éï¼åæ¬æä»£è¡¨è¾¾çè§£ï¼RECï¼ãæ¥å°è§è§é®ç­ï¼GVQAï¼ãæ¥å°å¾åæè¿°ï¼GCï¼åGUIä»£çäº¤äºï¼å¹¶è¯¦ç»ä»ç»äºå¸¸ç¨çè¯ä¼°ææ ï¼å¦IoUãPrecision@F1ãCLIPScoreç­ï¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥è®ºæä½ä¸ºä¸ç¯ç»¼è¿°ï¼æ²¡ææåºæ°çå®éªç»æï¼èæ¯å¯¹ç°æç ç©¶è¿è¡äºç³»ç»æ§æ¢³çåæ»ç»ï¼å¶æä¹å¨äºï¼
*   <strong>å¼ºè°è§è§åºç¡çæ ¸å¿å°ä½ï¼</strong> æç¡®æåºè§è§åºç¡ä¸ä»æ¯å®ç°ç»ç²åº¦å¤æ¨¡æçè§£çå³é®ï¼ä¹æ¯æé«VLMè¾åºå¯è§£éæ§åå¯é æ§çéè¦ææ®µï¼å°¤å¶æ¯å¨åå°å¹»è§æ¹é¢ã
*   <strong>æ­ç¤ºæ¶æéæ©çå½±åï¼</strong> å¼ºè°äºåç´ çº§è¡¨ç¤ºï¼ç¹å«æ¯åå§åæ ï¼åTransformeréª¨å¹²å¨ç°ä»£VLMsä¸­çä¸»å¯¼å°ä½ï¼å¹¶æåºä¸åè¿æ¥å¨è®¾è®¡ï¼å¦MLPãQ-formerãééæ ·å¨ï¼å¯¹æ§è½åæççæè¡¡ã
*   <strong>æå¯¼æªæ¥æ¨¡åå¼åï¼</strong> éè¿å¯¹ç°æææåå±éæ§çåæï¼ä¸ºä¸ä¸ä»£æ¥å°VLMçè®¾è®¡åè®­ç»æä¾äºæ¸æ°çæå¯¼ï¼ä¾å¦å¨é¢è®­ç»é¶æ®µå¼å¥æ¥å°ç®æ ãå¤çé«åè¾¨çå¾åãä»¥åå¹³è¡¡è¯­è¨æ¨¡ååè§è§ç¼ç å¨çè´¨éã
*   <strong>ä¿è¿å¤æ¨¡ææ¨çåå±ï¼</strong> æ¢è®¨äºè§è§åºç¡ä¸å¤æ¨¡æé¾å¼æèåæ¨ççäºè¡¥å³ç³»ï¼æåºå°æ¥å°ä¿¡å·æ´åå°æ¨çè¿ç¨ä¸­å¯ä»¥æé«æ¨¡åçéæåº¦åå¨å¤æåºæ¯ä¸çæ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®è´¨éåå¯å¤ç°æ§ï¼</strong> è®¸å¤ç°ææ°æ®éä¾èµä¼ªæ ç­¾ãå¤æç®¡éæä¸ææ¨¡åï¼å¯¼è´æ°æ®è´¨éæ¬¡ä¼ï¼å¹¶å¼åå¯å¤ç°æ§ãå¯è®¿é®æ§åæ°æ®æ±¡ææ¹é¢çæå¿§ã
*   <strong>åºåæµè¯ççææææ§ï¼</strong> ç°æRECåºåæµè¯ï¼å¦RefCOCOç³»åï¼å¯è½å·²é¥±åï¼ä¸å¨å¯¹è±¡åè¯æ±åå¼æ§æ¹é¢å­å¨å±éæ§ï¼éè¦æ´å·çææææ§çåºåæ¥è¯ä¼°æ¨¡åçæ³åè½åã
*   <strong>æ¨¡åæ³åè½åï¼</strong> å°½ç®¡å¾®è°å¯ä»¥æé«åºååæ°ï¼ä½å¶å¯¹æ¨¡åæ³åå°çå®ä¸çåºæ¯çè½åæä¾æéè¯æ®ã
*   <strong>Mambaéª¨å¹²çå±éæ§ï¼</strong> å°½ç®¡Mambaæ¨¡åå¨åºåå»ºæ¨¡æ¹é¢è¡¨ç°åºæ½åï¼ä½å¨æ¥å°ä»»å¡ä¸ï¼åºäºTransformerçéª¨å¹²æ¨¡åè¡¨ç°æ¾èæ´å¥½ï¼è¡¨æMambaå¨è§è§åºç¡ä»»å¡ä¸ä»æå¾è¿ä¸æ­¥æ¢ç´¢ã
*   <strong>è®­ç»èå¼çå¤ææ§ï¼</strong> VLMçå¼åæ¯ä¸ä¸ªå¤æçå¤é¶æ®µè¿ç¨ï¼éè¦ç²¾å¿å¹³è¡¡ç²ç²åº¦åç»ç²åº¦ä»»å¡ï¼å¹¶è§£å³å åå¸åç§»ãä»»å¡å¹²æ°ååæ°å²çªå¯¼è´çéå¿é®é¢ã
*   <strong>æ å°ä¸åç¼©çæè¡¡ï¼</strong> å¾åçº§çè§£ä¸­çåç¼©å¯è½éç¨äºè®¸å¤å¤æ¨¡æä»»å¡ï¼ä½å¯¹äºéè¦ç»ç²åº¦çè§£çæç¥ä»»å¡ï¼å¦è§è§åºç¡ï¼ï¼ç¹å¾ä¿çæ¹æ³éå¸¸æ´æçã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>é¢è®­ç»é¶æ®µçæ¥å°ç®æ ï¼</strong> è¿«åéè¦å¼åå·æååææ¡£åéææ°æ®åå»ºæµç¨çå¬å±èµæºï¼ä»¥å¨VLMå¼åçææé¶æ®µèå¥æ¥å°ç®æ ã
*   <strong>æ°ä¸ä»£åºåæµè¯ï¼</strong> å¼åæ´å·çææææ§çåºåï¼ä»¥æ´å¥½å°è¯ä¼°VLMså¨çå®ä¸çåºæ¯ä¸­çæ³åè½åï¼å¹¶ç¡®ä¿è¯ä¼°æ°æ®ä¸è¢«ç¨äºè®­ç»ã
*   <strong>GUIä»£ççè¿ä¸æ­¥åå±ï¼</strong> æç»­å¼åæ¨¡åãæ°æ®éåè¯ä¼°ææ ï¼ä»¥å¢å¼ºGUIä»£ççå¤æ¨¡ææ¥å°è½åï¼ä½¿å¶è½æ´ææå°ä¸Webãæä½ç³»ç»åç§»å¨è®¾å¤äº¤äºã
*   <strong>éªè¯æ¶æè®¾è®¡éæ©ï¼</strong> è¿ä¸æ­¥ç ç©¶VLMçè®¾è®¡ç©ºé´ï¼ä»¥éªè¯ä¸åæ¶æéæ©ï¼å¦äº¤åæ³¨æåä¸èªæ³¨æåãæ å°ä¸åç¼©ï¼å¨è§è§åºç¡ä»»å¡ä¸­çå½±åã
*   <strong>å¤æ¨¡ææ¥å°ä¸æ¨ççæ´åï¼</strong> æ¢ç´¢å°æ¥å°ä¿¡å·æ´åå°é¾å¼æèæ¨çè½¨è¿¹ä¸­ï¼ä»¥æé«æ¨¡åçè§£éæ§ãäºå®åç¡®æ§åå¨å¤æåºæ¯ä¸çæ§è½ã
*   <strong>é«åè¾¨çå¾åå¤çï¼</strong> è¿ä¸æ­¥ä¼åé«åè¾¨çå¾åçå¤çç­ç¥ï¼ä»¥å¨ä¿æè®¡ç®æççåæ¶ï¼ç¡®ä¿æ¨¡åè½å¤æè·ç»ç²åº¦çè§è§ä¿¡æ¯ã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºçè§£è§è§åºç¡å¨VLMsä¸­çå³é®ä½ç¨æä¾äºä¸ä¸ªå¨é¢çæ¡æ¶ï¼å¹¶ä¸ºæªæ¥ç ç©¶ææäºæ¹åï¼ä»¥åæç°æææï¼æ¨å¨å¤æ¨¡æäººå·¥æºè½çè¿ä¸æ­¥åå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation.</li>
<li>We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10345v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10345v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.09971v1'></a></p>
<h2 id="event-camera-guided-visual-media-restoration-3d-reconstruction-a-survey"><a href="https://arxiv.org/abs/2509.09971v1">Event Camera Guided Visual Media Restoration &amp; 3D Reconstruction: A Survey</a></h2>
<p><strong>Authors:</strong> Aupendu Kar, Vishnu Raj, Guan-Ming Su</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºAupendu Kar, Vishnu RajåGuan-Ming Suæ°åçè®ºæâEvent Camera Guided Visual Media Restoration &amp; 3D Reconstruction: A Surveyâçå¨é¢æè¦ï¼åå®¹åºäºæ¨æä¾çPDFå¨æï¼</p>
<p><strong>è®ºææè¦ï¼Event Camera Guided Visual Media Restoration &amp; 3D Reconstruction: A Survey</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç»¼è¿°è®ºææ¨å¨æ¢è®¨å¦ä½å©ç¨äºä»¶ç¸æºï¼ä¸ç§å¼æ­¥ææåç´ äº®åº¦ååççç©å¯åå¼ä¼ æå¨ï¼ä¸ä¼ ç»åºäºå¸§çç¸æºææ¯ç¸ç»åï¼ä»¥è§£å³ä¼ ç»è§è§ç³»ç»å¨æç«¯åç§ãå¿«éè¿å¨åå¸¦å®½æçä½ä¸ç­æææ§æ¡ä»¶ä¸ï¼å¨è§é¢åªä½æ¢å¤å3Déå»ºä»»å¡ä¸­é¢ä¸´çåºæå±éæ§ãå·ä½èè¨ï¼è®ºæå³æ³¨å¦ä½éè¿äºä»¶é©±å¨çèåææ¯ï¼æ¾èæåå¾å/è§é¢çè´¨éï¼åæ¬æ¶é´å¢å¼ºåç©ºé´å¢å¼ºï¼ä»¥å3Déå»ºçé²æ£æ§ååç¡®æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææ¬èº«æ¯ä¸ç¯ç»¼è¿°ï¼å¶ä¸»è¦è´¡ç®å¨äºç³»ç»æ§å°æ¢³çåæ»ç»äºè¯¥é¢ååçå³é®åæ°åæ¹æ³è®ºï¼</p>
<ul>
<li><strong>äºä»¶æ°æ®è¡¨ç¤ºä¸å¤çï¼</strong> è®ºæè¯¦ç»ä»ç»äºå¤ç§äºä»¶æ°æ®è¡¨ç¤ºæ¹æ³ï¼å¦åºäºå¾åãåºäºä½ç´ ãåºäºå¾ãåºäºèå²ååºäºå­¦ä¹ çè¡¨ç¤ºï¼ï¼ä»¥åäºä»¶æ°æ®å¢å¼ºææ¯ï¼å¦ç¹å¾æåãç©ºé´åæ¶é´ä¸éæ ·ãå»åªï¼ï¼ä»¥åæäºä»¶æ°æ®çç¨çæ§ãåªå£°åä½åè¾¨çç­ææï¼ä½¿å¶è½ä¸æ·±åº¦å­¦ä¹ æ¨¡åææèåã</li>
<li><strong>æ¶é´å¢å¼ºï¼</strong> ç»¼è¿°äºäºä»¶ç¸æºå¨è§é¢éå»ºãå¸§æå¼åè¿å¨å»æ¨¡ç³æ¹é¢çåºç¨ãå®æ¶µçäºä»æ©æåºäºæ¨¡åçæ¹æ³å°è¿æåºäºæ·±åº¦å­¦ä¹ ï¼CNN/RNNãTransformerãSNNãæ©æ£æ¨¡åï¼çæ··åæ¹æ³ï¼å¼ºè°äºä»¶ç¸æºå¦ä½å©ç¨å¶å¾®ç§çº§æ¶é´åè¾¨çåæ è¿å¨æ¨¡ç³ç¹æ§æ¥æ¢å¤ç²¾ç¡®è¿å¨çº¿ç´¢åå¡«åæ¶é´ç©ºç½ã</li>
<li><strong>ç©ºé´å¢å¼ºï¼</strong> è®ºææ¢è®¨äºäºä»¶ç¸æºå¦ä½éè¿ä¸ä¼ ç»RGBå¸§èåï¼å®ç°è¶åè¾¨çãHDRå¢å¼ºãä½åç§å¢å¼ºãé®æ¡ç§»é¤ãé¨æ°´ç§»é¤åç¦ç¹æ§å¶ãè¿äºæ¹æ³å©ç¨äºä»¶ç¸æºææé«é¢ç»èåå¨æèå´çä¼å¿ï¼å¼¥è¡¥äºä¼ ç»ç¸æºå¨è¿äºæ¹é¢çä¸è¶³ã</li>
<li><strong>3Déå»ºï¼</strong> ç»¼è¿°äºäºä»¶ç¸æºå¨3Déå»ºé¢åçè¿å±ï¼ç¹å«æ¯ä¸ç¥ç»è¾å°åºï¼NeRFï¼å3Dé«æ¯æ³¼æºï¼3DGSï¼ç­åçå®æ3Déå»ºæ¹æ³çèåãäºä»¶ç¸æºè½å¤å¤çè¿å¨æ¨¡ç³ãä½åç§åå§¿æä¼°è®¡ç­ææï¼ä¸ºå®æ¶ãé«è´¨éçåºæ¯å»ºæ¨¡å¼è¾äºæ°éå¾ã</li>
<li><strong>æ°æ®éæ±ç¼ï¼</strong> è®ºææä¾äºä¸ä¸ªå¨é¢çå¬å¼æ°æ®éåè¡¨ï¼ä¸ºå¯å¤ç°çç ç©¶ååºåæµè¯æä¾äºèµæºã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥ç»¼è¿°çæ ¸å¿åç°åæä¹å¨äºï¼</p>
<ul>
<li><strong>äºä»¶ç¸æºä½ä¸ºäºè¡¥ä¼ æå¨çå·¨å¤§æ½åï¼</strong> äºä»¶ç¸æºå­åå¶ä½å»¶è¿ãä½åèãè¶é«ææçåé«å¨æèå´ç­ç¬ç¹ä¼å¿ï¼è½å¤ææå¼¥è¡¥ä¼ ç»åºäºå¸§ç¸æºå¨å¨æåºæ¯åæç«¯åç§æ¡ä»¶ä¸çä¸è¶³ã</li>
<li><strong>æ·±åº¦å­¦ä¹ å¨äºä»¶èåä¸­çæ ¸å¿ä½ç¨ï¼</strong> æ·±åº¦å­¦ä¹ æ¶æï¼å¦TransformerãSNNãæ©æ£æ¨¡åï¼çè¿æ­¥ï¼ä½¿å¾å¤çå¼æ­¥ãç¨ççäºä»¶æ°æ®æµæä¸ºå¯è½ï¼å¹¶æ¾èæåäºè§è§åªä½æ¢å¤å3Déå»ºçæ§è½ã</li>
<li><strong>å¤æ¨¡æèåçååæåºï¼</strong> äºä»¶æ°æ®ä¸RGBå¾åãæ·±åº¦ä¿¡æ¯ç­ä¼ ç»æ¨¡æçèåï¼è½å¤äº§çè¶è¶åä¸æ¨¡æçååæåºï¼å¨åç§æææ§æ¡ä»¶ä¸å®ç°åææªæçè§è§ä¿çåº¦ã</li>
<li><strong>æ¨å¨æ°åºç¨é¢åï¼</strong> äºä»¶ç¸æºææ¯çåå±ï¼ç¹å«æ¯ä¸æ·±åº¦å­¦ä¹ çç»åï¼æ­£å¨æ¨å¨èªå¨é©¾é©¶ãè®¡ç®æå½±åå®æ¶å¢å¼ºç°å®ç­é¢åçè¿æ­¥ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
å°½ç®¡äºä»¶ç¸æºææ¯åå¾äºæ¾èè¿å±ï¼è®ºæä¹æåºäºç°æç ç©¶çä¸äºå±éæ§ï¼</p>
<ul>
<li><strong>äºä»¶æ°æ®ç¨çæ§ä¸åªå£°ï¼</strong> å¨ä½äº®åº¦ææ¢éåºæ¯ä¸­ï¼äºä»¶éå¸¸ç¨çä¸åæï¼è¿ç»äºä»¶å¤çå¸¦æ¥äºææã</li>
<li><strong>æ°æ®éå¤æ ·æ§ä¸è¶³ï¼</strong> ç°æäºä»¶ç¸æºæ°æ®éå¨åºæ¯å¤æ ·æ§ãç©ä½ç±»åãè¿å¨æ¨¡å¼åèæ¯æ¹é¢å­å¨å±éæ§ï¼ä¸è¯­ä¹çº§æ æ³¨ä¸è¶³ï¼å½©è²äºä»¶æ°æ®éç¨ç¼ºï¼å¤è§è§äºä»¶æ°æ®éä¹æªå¾å°ååä»£è¡¨ã</li>
<li><strong>æ¨¡åæ³åè½åï¼</strong> è®¸å¤ç°ææ¨¡åå¨é¨ç½²å°ä¸åé¢çæä¸ååçæ°æ®æ¶ï¼æ³åè½åè¾å·®ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> æäºé«çº§èåæéå»ºæ¹æ³ï¼å¦SPADE-E2VIDï¼å¯è½ä¼å¢å è®¡ç®ææ¬ã</li>
<li><strong>æ ¡åä¸åæ­¥ï¼</strong> å¨å¤ç¸æºè®¾ç½®ä¸­ï¼äºä»¶ç¸æºä¸RGBç¸æºä¹é´çç²¾ç¡®æ ¡åååæ­¥ä»ç¶æ¯ä¸ä¸ªææã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸å ä¸ªæªæ¥ç ç©¶æ¹åï¼ä»¥è¿ä¸æ­¥æ¨å¨è¯¥é¢åçåå±ï¼</p>
<ul>
<li><strong>äºä»¶é©±å¨çå¤æ¨¡æèåï¼</strong> å¼åæ´é²æ£çèåæ¡æ¶ï¼å¨æå¹³è¡¡RGBãæ·±åº¦åäºä»¶æ¨¡æï¼ä»¥å®ç°å®æ¶æ¢å¤ã</li>
<li><strong>ä½èµæºåè¾¹ç¼é¨ç½²ï¼</strong> è®¾è®¡è½»éçº§æ¶æï¼ä¼åäºä»¶å¤çå¨èµæºåéçç§»å¨ååµå¥å¼å¹³å°ä¸çæ§è½ã</li>
<li><strong>èªçç£åæ çç£å­¦ä¹ ï¼</strong> æ¢ç´¢åéåºãå¯¹æ¯å­¦ä¹ åçææ¨¡åï¼åå°å¯¹æ æ³¨æ°æ®çä¾èµï¼æé«æ¨¡åæ³åè½åã</li>
<li><strong>äºä»¶åºæ ¡åååæ­¥ï¼</strong> ç ç©¶ä½¿ç¨äºä»¶æµè¿è¡æ æ ¡åå¤ç¸æºè®¾ç½®ï¼ä»¥å®ç°å¨æç¯å¢ä¸­çé²æ£åæ­¥åå¯¹é½ã</li>
<li><strong>äºä»¶å¼å¯¼ççææ¨¡åï¼</strong> å°äºä»¶æ°æ®ä¸æ©æ£æ¨¡åç»åï¼å¨æç«¯æ¡ä»¶ä¸å®ç°é«è´¨éçåæåæ¢å¤ã</li>
<li><strong>äºä»¶åºæ·±åº¦åç¦ç¹ä¼°è®¡ï¼</strong> æ¨è¿åºäºäºä»¶çæ·±åº¦åç¦ç¹æ§å¶ç®æ³ï¼åºç¨äºæºå¨äººãAR/VRåè®¡ç®æå½±ã</li>
<li><strong>è·¨é¢ååºç¨ï¼</strong> å°äºä»¶åºæ¢å¤ææ¯åºç¨äºå»å­¦æåãé¥æåå·¥ä¸æ£æµç­é¢åã</li>
<li><strong>å½©è²äºä»¶ç¸æºï¼</strong> æ©å±å½©è²äºä»¶æ°æ®ç ç©¶ï¼ä»¥æ¹è¿è§è§åªä½æ¢å¤åéå»ºã</li>
<li><strong>åºåæµè¯åæ°æ®éæ©å±ï¼</strong> åå»ºå¤æ ·åãå¸¦æ æ³¨çå¤è§è§æ°æ®éï¼æ¶µçä¸ååç§ãè¿å¨åè¯­ä¹ä¸ä¸æï¼ä»¥æ¯æå¯å¤ç°çç ç©¶ã</li>
<li><strong>å®å¨æ§åé²æ£æ§ï¼</strong> è§£å³äºä»¶åºç³»ç»ä¸­çæ¼æ´ï¼åæ¬å¯¹ææ§æ»å»ååé¨å¨èï¼ç¡®ä¿å³é®åºç¨ä¸­çå®å¨é¨ç½²ã</li>
</ul>
<p>è¿ä»½ç»¼è¿°ä¸ºäºä»¶ç¸æºå¨è§è§åªä½æ¢å¤å3Déå»ºé¢åçåºç¨æä¾äºä¸ä¸ªå¨é¢çè§è§ï¼å¹¶ä¸ºæªæ¥çç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.09971v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.09971v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10408v1'></a></p>
<h2 id="multimodal-sam-adapter-for-semantic-segmentation"><a href="https://arxiv.org/abs/2509.10408v1">Multimodal SAM-adapter for Semantic Segmentation</a></h2>
<p><strong>Authors:</strong> Iacopo Curti, Pierluigi Zama Ramirez, Alioscia Petrelli, Luigi Di Stefano</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Iacopo Curtiç­äººæ°åçè®ºæâMultimodal SAM-adapter for Semantic Segmentationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="multimodal-sam-adapter-for-semantic-segmentation_1">ãMultimodal SAM-adapter for Semantic Segmentationãè®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯­ä¹åå²å¨èªå¨é©¾é©¶ãå»å­¦æååæºå¨äººç­é¢åè³å³éè¦ï¼ä½ç°ææ¹æ³å¨æ¶å£åç§ãé®æ¡åä¸å©å¤©æ°ç­æææ§æ¡ä»¶ä¸è¡¨ç°ä¸ä½³ãè¿äºéå¶æºäºä»ä¾èµRGBå¾åçåºæå±éæ§ãä¸ºäºæé«é²æ£æ§ï¼å¤æ¨¡ææ¹æ³ï¼æ´åè¾å©ä¼ æå¨æ°æ®å¦LiDARãçº¢å¤ç­ï¼å·²å´­é²å¤´è§ï¼ä½å¦ä½ææå°å°è¿äºå¤æ¨¡æä¿¡æ¯ä¸å¼ºå¤§çRGBåºç¡æ¨¡åï¼å¦Segment Anything Model, SAMï¼ç»åï¼åæ¶ä¿çå¶æ³åè½åå¹¶é¿åç¾é¾æ§éå¿ï¼æ¯ä¸ä¸ªå³é®ææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäº<strong>MM SAM-adapter</strong>ï¼ä¸ä¸ªæ°é¢çæ¡æ¶ï¼æ¨å¨æ©å±Segment Anything Model (SAM)å¨å¤æ¨¡æè¯­ä¹åå²æ¹é¢çè½åãå¶æ ¸å¿åæ°åæ¹æ³è®ºè´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>SAMçéåºæ§æ©å±ï¼</strong> MM SAM-adapteréè¿ä¸ä¸ªå¤é¨ééå¨ç½ç»ï¼å°èååçå¤æ¨¡æç¹å¾æ³¨å¥å°SAMä¸°å¯çRGBç¹å¾ä¸­ãè¿ç§è®¾è®¡ä½¿å¾æ¨¡åè½å¤ä¿çSAMå¼ºå¤§çRGBç¹å¾æ³åè½åï¼åæ¶ä»å¨è¾å©æ¨¡ææä¾é¢å¤æç¨çº¿ç´¢æ¶éæ©æ§å°æ´åå®ä»¬ï¼ä»èå®ç°å¤æ¨¡æä¿¡æ¯çå¹³è¡¡åé«æå©ç¨ã</li>
<li><strong>éå¯¹ç§°æ¶æè®¾è®¡ï¼</strong> è®ºææåºäºä¸ç§éå¯¹ç§°çç½ç»æ¶æï¼å¶ä¸­SAMä¸»å¹²ï¼å¤çRGBç¹å¾ï¼æ¥ææ¯å¤æ¨¡æèåç¼ç å¨åééå¨æ¨¡åï¼å¤çè¾å©æ¨¡æï¼æ´å¤çåæ°ãè¿ç§è®¾è®¡ä¼åèèSAMçRGBåºç¡ç¥è¯ï¼åæ¶å©ç¨å¤æ¨¡æèåç¥è¯æ¥å¤çæææ§åºæ¯ï¼é¿åè¾å©æ¨¡æå¨RGBä¿¡æ¯åè¶³æ¶å¼å¥åªå£°ã</li>
<li><strong>å¤æ¨¡æèåç¼ç å¨ï¼</strong> éç¨äºä¸ä¸ªå¤æ¨¡æèåç¼ç å¨ï¼å®å¤çRGBå¾ååè¾å©æ¨¡æçç¹å¾ï¼çæå¤æ¨¡æè¡¨ç¤ºï¼ç¶åè¾å¥å°ééå¨æ¨¡åãè¯¥ç¼ç å¨è½å¤å­¦ä¹ æè¡¡ä¸åæ¨¡æçè´¡ç®ï¼å¹¶å¨æ¨çæ¶å¨æéæ©ç¸å³æ¨¡æã</li>
<li><strong>RGB-hardåRGB-easyæ°æ®éååï¼</strong> ä¸ºäºæ´æ·±å¥å°åææ¨¡æè´¡ç®ï¼ä½èå°DeLiVERåFMBæ°æ®éååä¸ºRGB-easyåRGB-hardå­éï¼ä»¥è¯ä¼°æ¹æ³å¨ä¸åææç¨åº¦ä¸çæ§è½ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
MM SAM-adapterå¨ä¸ä¸ªå·ææææ§çåºåæ°æ®éï¼DeLiVERãFMBåMUSESï¼ä¸åå¾äº<strong>æåè¿çæ§è½</strong>ã</p>
<ul>
<li><strong>DeLiVERåºåï¼</strong> å¨RGB-DepthãRGB-LiDARåRGB-Eventè®¾ç½®ä¸­ï¼MM SAM-adapteråè¡¨ç°åºè²ãç¹å«æ¯å¨RGB-hardåºæ¯ä¸ï¼å½è¾å©æ¨¡æï¼å¦LiDARåEventï¼æ´å·æææ§æ¶ï¼MM SAM-adapterçæ§è½æåå°¤ä¸ºæ¾èï¼è¡¨æå¶è½ææå©ç¨è¾å©ä¿¡æ¯ã</li>
<li><strong>FMBåºåï¼</strong> å¨RGB-Thermalè®¾ç½®ä¸­ï¼MM SAM-adapteråæ ·è¾¾å°äºæåè¿çæ§è½ï¼å¹¶å¨RGB-hardåºæ¯ä¸å±ç°åºå¯¹è¾å©æ¨¡æä¿¡æ¯çåè¶å©ç¨è½åã</li>
<li><strong>MUSESåºåï¼</strong> å¨RGB-LiDARåRGB-Eventè®¾ç½®ä¸­ï¼MM SAM-adapterå¨æ´ä½ä»¥ååç§æ¼å¤åå¤©æ°æ¡ä»¶ä¸ååå¾äºæåè¿çæ§è½ï¼å°¤å¶å¨é¾å¤©åå¤é´ç­å¯¹RGBæ¨¡ææå·æææ§çåºæ¯ä¸­è¡¨ç°çªåºã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> ç»æè¡¨æï¼ä¾§è°ï¼side-tuningï¼SAMééå¨ä¼äºæ åå¾®è°åLoRAéåºç­ç¥ï¼è½æ´å¥½å°ä¿çSAMçåéªç¥è¯ãéå¯¹ç§°æ¶æè®¾è®¡è¢«è¯å®ä¼äºå¯¹ç§°æ¶æï¼å¼ºè°äºä¼åèèRGBä¿¡æ¯çéè¦æ§ãæ­¤å¤ï¼å³ä½¿SAMä¸»å¹²å®å¨å»ç»ï¼å¤æ¨¡æééå¨åèåæ¨¡åä¹è½ææå°å¼å¯¼SAMå©ç¨å¤æ¨¡æä¿¡æ¯ã</li>
</ul>
<p>è¿äºç»æä¸è´è¯æäºMM SAM-adapterå¨æå©åä¸å©æ¡ä»¶ä¸åä¼äºç«äºæ¹æ³ï¼çªæ¾äºå¤æ¨¡æéåºæ§å¨é²æ£åºæ¯çè§£ä¸­çæææ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æå°MM SAM-adapterçä¸ä¸ªå±éæ§æ¯ï¼ç±äºè·¯é¢èåæ¨¡åçéå¶ï¼å®<strong>ç®åä»æ¯æä¸¤ç§è¾å¥æ¨¡æ</strong>ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±å¯¹æ´å¤æ¨¡æçæ¯æï¼</strong> è®¾è®¡ä¸ä¸ªåæ°ä¸é«æçèåæ¨¡åï¼ä»¥éåºæ´å¤æçå¤æ¨¡æåºæ¯ï¼æ¯æè¶è¿ä¸¤ç§è¾å¥æ¨¡æã
*   <strong>åºç¨äºå¶ä»ä»»å¡ï¼</strong> æ¢ç´¢è¯¥æ¡æ¶å¨å¶ä»ä»»å¡ä¸­çæ½åï¼ä¾å¦å¨æ¯åå²ï¼panoptic segmentationï¼ï¼è¿ä¸ºæªæ¥çç ç©¶æä¾äºä¸ä¸ªæåæ¯çæ¹åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation.</li>
<li>We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance.</li>
<li>To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets.</li>
<li>Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10408v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10408v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10312v1'></a></p>
<h2 id="compute-only-16-tokens-in-one-timestep-accelerating-diffusion-transformers-with-cluster-driven-feature-caching"><a href="https://arxiv.org/abs/2509.10312v1">Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching</a></h2>
<p><strong>Authors:</strong> Zhixin Zheng, Xinyu Wang, Chang Zou, Shaobo Wang, Linfeng Zhang</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zhixin Zhengç­äººæ°åçè®ºæâCompute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Cachingâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching</strong></p>
<p><strong>1. æ ¸å¿é®é¢ä¸ç ç©¶å¨æº</strong>
æ©æ£Transformerï¼Diffusion Transformers, DiTsï¼å¨çæé«è´¨éå¾ååè§é¢æ¹é¢è¡¨ç°åºè²ï¼ä½å¶è¿­ä»£å»åªè¿ç¨å¯¼è´å·¨å¤§çè®¡ç®ææ¬ï¼ä¸¥ééå¶äºå¶å®éåºç¨ãç°æçç¹å¾ç¼å­æ¹æ³ä¸»è¦å©ç¨æ©æ£æ¨¡åå¨æ¶é´ç»´åº¦ä¸çç¸ä¼¼æ§æ¥å éï¼ä½å¿½ç¥äºç©ºé´ç»´åº¦ä¸çç¸ä¼¼æ§ãå æ­¤ï¼è®ºææ¨å¨è§£å³DiTsçè®¡ç®æçç¶é¢ï¼ç¹å«æ¯å¨é«å éæ¯ä¸å¦ä½ä¿æçæè´¨éã</p>
<p><strong>2. å³é®åæ°ä¸æ¹æ³è´¡ç®</strong>
æ¬ææåºäº<strong>èç±»é©±å¨ç¹å¾ç¼å­ï¼Cluster-Driven Feature Caching, ClusCaï¼</strong>ï¼ä½ä¸ºç°ææ¶é´ç»´åº¦ç¹å¾ç¼å­æ¹æ³çæ­£äº¤è¡¥åãClusCaçæ ¸å¿åæ°å¨äºåæ¶å©ç¨tokenç<strong>æ¶é´ç¸ä¼¼æ§</strong>å<strong>ç©ºé´ç¸ä¼¼æ§</strong>æ¥å éæ¨çãå·ä½è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>ç©ºé´ç¸ä¼¼æ§åæ</strong>ï¼éè¿å¯¹DiTsä¸­tokenç©ºé´åä½çç³»ç»æ§ç ç©¶ï¼è®ºæåç°åä¸èç±»ä¸­çtokenå¨ç¹å¾åè¿å¨æ¨¡å¼ä¸é½è¡¨ç°åºæ¾èç¸ä¼¼æ§ï¼ä¸èç±»ç»æå¨æ¶é´æ­¥ä¹é´ä¿æç¨³å®ãè¿ä¸ºå©ç¨ç©ºé´ç¸ä¼¼æ§è¿è¡å éæä¾äºçè®ºåºç¡ã</li>
<li><strong>æ¶ç©ºç¹å¾éç¨èå¼</strong>ï¼ClusCaå°æææ¶é´æ­¥ååä¸ºç¼å­å¨æãå¨æ¯ä¸ªå¨æçç¬¬ä¸ä¸ªæ¶é´æ­¥è¿è¡å¨éè®¡ç®ï¼å¹¶å©ç¨K-Meansç®æ³å¯¹tokenè¿è¡ç©ºé´èç±»ãå¨åç»­æ¶é´æ­¥ä¸­ï¼ClusCaä»è®¡ç®æ¯ä¸ªèç±»ä¸­çä¸ä¸ªä»£è¡¨æ§tokenï¼ä¾å¦ï¼éè¿éæºéæ©ï¼ï¼ç¶åéè¿å ææ±åçæ¹å¼å°è¯¥ä»£è¡¨æ§tokençä¿¡æ¯ï¼ç©ºé´éç¨ï¼ä¸åä¸æ¶é´æ­¥çç¼å­ç¹å¾ï¼æ¶é´éç¨ï¼ç»åï¼ä¼ æ­ç»åä¸èç±»ä¸­çææå¶ä»tokenãè¿ç§æ¹æ³è½å¤å°tokenæ°éåå°90%ä»¥ä¸ï¼æ¾èéä½è®¡ç®å¤æåº¦ï¼åæ¶éè¿åééç¨ç­ç¥ç¼è§£äºç¼å­å¯¼è´çè¯¯å·®ç´¯ç§¯ã</li>
<li><strong>æ éé¢å¤è®­ç»</strong>ï¼ClusCaæ¯ä¸ä¸ªå³æå³ç¨çæ¹æ³ï¼å¯ä»¥ç´æ¥åºç¨äºä»»ä½æ©æ£Transformeræ¨¡åï¼æ éè¿è¡é¢å¤çè®­ç»ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æä¸æä¹</strong>
è®ºæå¨DiTãFLUXåHunyuanVideoç­ä¸»æµæ©æ£æ¶æä¸è¿è¡äºå¹¿æ³å®éªï¼éªè¯äºClusCaå¨ææ¬å°å¾ååææ¬å°è§é¢çæä»»å¡ä¸­çæææ§ï¼</p>
<ul>
<li><strong>æ¾èå éä¸é«è´¨éçæ</strong>ï¼ClusCaå¨FLUXæ¨¡åä¸å®ç°äº4.96åçå éï¼åæ¶ImageRewardè¯åè¾¾å°99.49%ï¼è¶è¶åå§æ¨¡å0.51%ãå¨HunyuanVideoä¸ï¼ClusCaå®ç°äº6.21åå éï¼VBenchè¯åè¾¾å°79.60%ã</li>
<li><strong>ä¼è¶çæç-è´¨éæè¡¡</strong>ï¼ä¸ç°æç¹å¾ç¼å­æ¹æ³ï¼å¦FORAãToCaãDuCaãTaylorSeerï¼ç¸æ¯ï¼ClusCaå¨é«å éæ¯ä¸è¡¨ç°åºæ´ä½çFIDåæ´é«ççæè´¨éï¼å°¤å¶æ¯å¨æç«¯å éæ¡ä»¶ä¸ä»è½ä¿ææ§è½ã</li>
<li><strong>å¯è§åéªè¯</strong>ï¼PCAå¯è§åç»ææ¾ç¤ºï¼ClusCaçæçç¹å¾è½¨è¿¹ä¸æªå éçåå§DiTæ¨¡åé«åº¦å¹éï¼å°¤å¶æ¯å¨å»åªè¿ç¨çåæï¼è¿å®ééªè¯äºClusCaå¨ä¿æè®¡ç®æççåæ¶æåæå°åäºè¯¯å·®ç´¯ç§¯ã</li>
<li><strong>èç±»ä¸ä¼ æ­å¼éåæ</strong>ï¼èç±»æä½çè®¡ç®å¼ééå¸¸ä½äºæ»è®¡ç®ææ¬ç5%ï¼ä¼ æ­æºå¶è½ç¶ç¥æå¢å ï¼ä½æ¾èæåäºçæè´¨éã</li>
</ul>
<p><strong>4. è®ºæå±éæ§</strong>
è®ºæä¸­æªæç¡®æåæ¾èçå±éæ§ï¼ä½ä»æ¹æ³æè¿°åå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çèèï¼
*   <strong>Kå¼éæ©</strong>ï¼èç±»æ°éKçè®¾å®éè¦å¹³è¡¡çæè´¨éåè®¡ç®å¼éãè½ç¶è®ºææåºK=16å¨DiT-XL/2ä¸è¡¨ç°è¯å¥½ï¼ä½å¯¹äºä¸åæ¨¡åæä»»å¡ï¼Kå¼çæä¼éæ©å¯è½éè¦è¿ä¸æ­¥æ¢ç´¢ã
*   <strong>ä¼ æ­æ¯çÎ³çæææ§</strong>ï¼ä¼ æ­æ¯çÎ³æ¯ä¸ä¸ªå³é®è¶åæ°ï¼å½±åç©ºé´éç¨åæ¶é´éç¨çå¹³è¡¡ãè¿å¤§æè¿å°çÎ³å¼é½å¯è½å¯¼è´æ§è½ä¸éï¼éè¦ä»ç»è°æ´ã
*   <strong>èç±»ç®æ³çéæ©</strong>ï¼è®ºæéæ©äºK-Meansç®æ³ï¼å å¶ç®åé«æãä½å¯¹äºæäºå¤æçç¹å¾åå¸ï¼å¶ä»æ´é«çº§çèç±»ç®æ³æ¯å¦è½å¸¦æ¥è¿ä¸æ­¥çæ§è½æåå¼å¾æ¢è®¨ã
*   <strong>éç¨æ§</strong>ï¼è½ç¶è®ºæå¼ºè°ClusCaå¯ä»¥åºç¨äºä»»ä½æ©æ£Transformerï¼ä½å¶å¨æ´å¹¿æ³çæ¨¡åæ¶æãæ°æ®éåä»»å¡ä¸çè¡¨ç°ä»éè¿ä¸æ­¥éªè¯ã</p>
<p><strong>5. æ½å¨æªæ¥ç ç©¶æ¹å</strong>
*   <strong>èªéåºèç±»ä¸ä¼ æ­</strong>ï¼æ¢ç´¢æ´æºè½çèªéåºæºå¶ï¼æ ¹æ®ä¸åçæ¶é´æ­¥ãæ¨¡åå±æåå®¹å¤ææ§å¨æè°æ´èç±»æ°éKåä¼ æ­æ¯çÎ³ï¼ä»¥è¿ä¸æ­¥ä¼åæçåè´¨éã
*   <strong>æ´å¤æçç©ºé´ç¸ä¼¼æ§å©ç¨</strong>ï¼ç ç©¶é¤äºK-Meansä¹å¤çå¶ä»èç±»æå¾ç¥ç»ç½ç»æ¹æ³ï¼ä»¥æ´ç²¾ç»å°æætokenä¹é´çç©ºé´å³ç³»ï¼ä»èå®ç°æ´é«æçç¹å¾éç¨ã
*   <strong>ç»åå¶ä»å éææ¯</strong>ï¼å°ClusCaä¸æ­¥æ°ä¼åï¼å¦DPM-Solverï¼ãæ¨¡ååªææéåç­å¶ä»å éææ¯ç¸ç»åï¼æ¢ç´¢å¤ç»´åº¦å éç­ç¥çååæåºã
*   <strong>çè®ºåæä¸æ³åæ§</strong>ï¼å¯¹ClusCaçè¯¯å·®ç´¯ç§¯æºå¶è¿è¡æ´æ·±å¥ççè®ºåæï¼å¹¶ç ç©¶å¶å¨ä¸åæ©æ£æ¨¡ååä½ï¼å¦æ¡ä»¶çæãå¤æ¨¡æçæï¼ä¸çæ³åè½åã
*   <strong>ç¡¬ä»¶ä¼å</strong>ï¼éå¯¹ClusCaçè®¡ç®æ¨¡å¼ï¼èç±»ãéæ©ãä¼ æ­ï¼è¿è¡ç¹å®çç¡¬ä»¶å éä¼åï¼ä»¥è¿ä¸æ­¥æåå®éé¨ç½²æ¶çæ¨çéåº¦ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%.</li>
<li>The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10312v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10312v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10260v1'></a></p>
<h2 id="magicmirror-a-large-scale-dataset-and-benchmark-for-fine-grained-artifacts-assessment-in-text-to-image-generation"><a href="https://arxiv.org/abs/2509.10260v1">MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Jia Wang, Jie Hu, Xiaoqi Ma, Hanghang Ma, Yanbing Zeng, Xiaoming Wei</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä»¥ä¸æ¯Jia Wangç­äººæ°åçè®ºæâMagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generationâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼MagicMirrorï¼ç¨äºææ¬å°å¾åçæä¸­ç»ç²åº¦ä¼ªå½±è¯ä¼°çå¤§è§æ¨¡æ°æ®éååºå</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡ææ¬å°å¾åï¼T2Iï¼çæå¨æä»¤éµå¾ªåç¾å­¦æ¹é¢åå¾äºæ¾èè¿å±ï¼ä½ç©çä¼ªå½±ï¼å¦è§£ååç»æç¼ºé·ï¼çæ®éå­å¨ä¸¥ééä½äºæç¥è´¨éå¹¶éå¶äºåºç¨ãç°æåºåç¼ºä¹ç³»ç»ä¸ç»ç²åº¦çè¯ä¼°æ¡æ¶æ¥è§£å³è¿äºå¤æ ·ä¸å¤æçä¼ªå½±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èå¼å¥äºMagicMirrorï¼ä¸ä¸ªç¨äºä¼ªå½±è¯ä¼°çç»¼åæ¡æ¶ï¼å¶ä¸»è¦è´¡ç®åæ¬ï¼
*   <strong>è¯¦ç»çä¼ªå½±åç±»æ³ï¼</strong> é¦æ¬¡å»ºç«äºçæå¾åä¼ªå½±çè¯¦ç»åç±»æ³ï¼å°ä¼ªå½±åä¸ºå¯¹è±¡è§£åãå±æ§åäº¤äºä¸å¤§ç±»ï¼å¹¶è¿ä¸æ­¥ç»åä¸ºL2åL3çº§å«æ ç­¾ã
*   <strong>MagicData340Kæ°æ®éï¼</strong> åºäºè¯¥åç±»æ³ï¼æå¨æ æ³¨äºMagicData340Kï¼è¿æ¯é¦ä¸ªåå«34ä¸å¼ çæå¾åçãå¸¦æç»ç²åº¦ä¼ªå½±æ ç­¾çå¤§è§æ¨¡äººå·¥æ æ³¨æ°æ®éã
*   <strong>MagicAssessoræ¨¡åï¼</strong> è®­ç»äºä¸ä¸ªä¸é¨çè§è§-è¯­è¨æ¨¡åï¼VLMï¼ï¼MagicAssessorï¼ç¨äºæä¾è¯¦ç»çä¼ªå½±è¯ä¼°åç¸åºæ ç­¾ãä¸ºäºè§£å³ç±»å«ä¸å¹³è¡¡åå¥å±ä½å¼ç­ææï¼ä½èè®¾è®¡äºä¸ç§æ°é¢çæ°æ®éæ ·ç­ç¥åå¤çº§å¥å±ç³»ç»ï¼å¹¶å°å¶åºç¨äºGroup Relative Policy Optimization (GRPO)è®­ç»ã
*   <strong>MagicBenchåºåï¼</strong> å©ç¨MagicAssessoræå»ºäºMagicBenchï¼è¿æ¯ä¸ä¸ªç¨äºè¯ä¼°å½åT2Iæ¨¡åå¾åä¼ªå½±çèªå¨ååºåã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   MagicBenchçè¯ä¼°ç»ææ¾ç¤ºï¼å³ä½¿æ¯GPT-image-1ç­é¡¶çº§T2Iæ¨¡åä¹æ®éå­å¨æ¾èä¼ªå½±ï¼è¿è¡¨æä¼ªå½±åå°æ¯æªæ¥T2Iåå±çå³é®åæ²¿ã
*   MagicAssessorå¨ä¼ªå½±æ£æµä»»å¡ä¸è¡¨ç°åºè²ï¼å¨äºååç±»ä»»å¡ä¸­å®ç°äº0.77çç²¾ç¡®åº¦åçº¦0.7çF1åæ°ï¼è¡¨æå¶ä½ä¸ºå¥å±ä¿¡å·çå¼ºå¤§æ½åã
*   è¯¥æ¨¡åå¨è¯å«äººç±»åå¨ç©è§£åä¼ªå½±æ¹é¢è¡¨ç°çªåºï¼ä½å¨äº¤äºåå¯¹è±¡å½¢æé®é¢ä¸ææç¨å·®ã
*   ä¸ç°ææ¨¡åç¸æ¯ï¼MagicAssessorå¨ææä¸»è¦è¯ä¼°ææ ä¸åæ¾èä¼äºç«äºå¯¹æï¼å¡«è¡¥äºæ§è½ç©ºç½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   MagicAssessorå¨å¤çäº¤äºåå¯¹è±¡å½¢æé®é¢æ¶ææä¸ä½³ãå¯¹äºäº¤äºï¼æ¨¡åé¾ä»¥åºååç´ éå åä½è´¨éå¾ååºåï¼å¯¹äºå¯¹è±¡å½¢æï¼å¯¹è±¡ç±»åçå¤æ ·æ§ååºå¤§æ°éä½¿å¾è¯ä¼°ä»æ ¹æ¬ä¸åå¾å°é¾ã
*   ä¸äºå¤§åVLMæ¨¡åï¼å¦Qwen-VLåInternVLç³»åï¼å¯¹ä¼ªå½±ä¸ææï¼å¬åçéå¸¸ä½ï¼çè³æ´å¤§çæ¬è¡¨ç°æ´å·®ï¼è¿å¯è½ä¸å®ä»¬å¾åäºå°å¾åè§ä¸ºæ­£å¸¸æå³ã
*   GPT-40è¡¨ç°åºä¸ä¸è´çæ§è½ï¼è½ç¶è½è¯å«ææ¾çè§£åç¸å½¢ï¼ä½å¯¹æ´ç»å¾®æé»è¾éè¯¯åä¼å¿½ç¥ï¼è¿å¯è½ä¸å¶æä¾éç¨æè¿°çå¾åæå³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å°MagicMirroræ¡æ¶ç´æ¥æ´åå°æ¨¡åè®­ç»çå½å¨æä¸­ï¼ç¨äºéæçæ§ï¼å¨é¢è®­ç»é¶æ®µè¯ä¼°æ¨¡åæ£æ¥ç¹ï¼åä¸»å¨ä¼åï¼å¨åè®­ç»é¶æ®µä½ä¸ºå¥å±æ¨¡åï¼éè¿RLHFæReFLç´æ¥åå°ä¼ªå½±çæï¼ã
*   æªæ¥çæ¨¡åå¼åéè¦å¨è¿½æ±ç¾å­¦ååå°ä¼ªå½±çå³é®éæ±ä¹é´åå¾å¹³è¡¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment.</li>
<li>To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10260v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10260v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10156v1'></a></p>
<h2 id="layerlock-non-collapsing-representation-learning-with-progressive-freezing"><a href="https://arxiv.org/abs/2509.10156v1">LayerLock: Non-collapsing Representation Learning with Progressive Freezing</a></h2>
<p><strong>Authors:</strong> Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi Sajjadi, Joao Carreira</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Goker Erdoganç­äººæ°åçè®ºæâLayerLock: Non-collapsing Representation Learning with Progressive Freezingâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³èªçç£è§è§è¡¨ç¤ºå­¦ä¹ ä¸­çæçåç¨³å®æ§é®é¢ï¼ç¹å«æ¯è§é¢æ©ç èªç¼ç ï¼MAEï¼æ¨¡åå¨è®­ç»è¿ç¨ä¸­å±æ¶æé¡ºåºä¸ä¸è´ä»¥åæ½å¨é¢æµæ¹æ³ä¸­å¸¸è§çâè¡¨ç¤ºå´©æºâé®é¢ãç ç©¶äººåè§å¯å°ï¼å¨è§é¢MAEæ¨¡åè®­ç»ä¸­ï¼ViTå±æç§æ·±åº¦é¡ºåºæ¶æï¼æµå±åæ¶æï¼æ·±å±åæ¶æãå¦ä½å©ç¨è¿ä¸è§å¯ç»ææ¥å éè®­ç»ãæé«è¡¨ç¤ºå­¦ä¹ çç¨³å®æ§ï¼å¹¶é¿åè¡¨ç¤ºå´©æºæ¯æ ¸å¿é®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
LayerLockæåºäºä¸ä¸ªç®åèææçèªçç£è§è§è¡¨ç¤ºå­¦ä¹ æ¹æ³ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>æ¸è¿å¼å±å»ç»ï¼Progressive Layer Freezingï¼</strong>ï¼å©ç¨ViTå±ææ·±åº¦é¡ºåºæ¶æçè§å¯ï¼LayerLockå¨è®­ç»è¿ç¨ä¸­æ ¹æ®é¢è®¾çæ¶é´è¡¨éæ­¥å»ç»æ¨¡åä¸­çæµå±ï¼ä»èå éæ åMAEçè®­ç»ã
*   <strong>å¨æç®æ é¢æµï¼Dynamically Evolving Prediction Targetï¼</strong>ï¼LayerLockå¨è®­ç»è¿ç¨ä¸­å¨æå°å°é¢æµç®æ ä»æµå±ç¹å¾ï¼å¦åç´ ï¼è¿æ¸¡å°æ´æ·±å±çä¸­é´æ½å¨æ¨¡åæ¿æ´»ãè¿ç»åäºåç´ é¢æµçç¨³å®æ§ï¼é¿åå´©æºï¼åæ½å¨é¢æµå­¦ä¹ æ½è±¡è¯­ä¹ç¹å¾çè½åã
*   <strong>é¿åè¡¨ç¤ºå´©æº</strong>ï¼éè¿æ¸è¿å¼å»ç»åå¨æç®æ é¢æµï¼LayerLockæä¾äºä¸ç§ç®åä¸å¯æ©å±çæ½å¨é¢æµæ¹æ³ï¼ææé¿åäºä¼ ç»æ½å¨é¢æµæ¹æ³ä¸­å¸¸è§çè¡¨ç¤ºå´©æºé®é¢ã
*   <strong>æ°å3Dæè½¬ä½ç½®åµå¥ï¼3D Rotary Positional Embeddingsï¼</strong>ï¼è®ºæå¼å¥äºä¸ç§æ°é¢ç3Dæè½¬ä½ç½®åµå¥æ¹æ³ï¼æ¾èæé«äºææä¸æ¸¸ä»»å¡çæ§è½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>è®­ç»æçæå</strong>ï¼LayerLockéè¿æ¸è¿å¼å»ç»ï¼å¨ä¸æå¤±æ§è½çæåµä¸ï¼æ¾èéä½äºMAEæ¨¡åçæ»è®­ç»ææ¬åå³°å¼åå­ä½¿ç¨éãå¨1Bè®­ç»æ ·æ¬ä¸ï¼FLOPæçæåé«è¾¾19%ã
*   <strong>è¡¨ç¤ºå­¦ä¹ æ§è½æå</strong>ï¼LayerLockå¨åç´ é¢æµï¼å¦4DS MAEï¼åæ½å¨é¢æµï¼å¦V-JEPAï¼ä¸¤ç§èªçç£æ¹æ³ä¸é½åå¾äºä¼äºåºçº¿æ¨¡åçæ§è½ãå¨å¨ä½åç±»ï¼SSv2åKinetics700ï¼åæ·±åº¦ä¼°è®¡ï¼ScanNetï¼ç­è¯­ä¹åä½çº§è§è§ä»»å¡ä¸åææ¾èæ¹è¿ã
*   <strong>ç¨³å®æ§ä¸å¯æ©å±æ§</strong>ï¼LayerLockæ¹æ³å¨è®­ç»å¤§åè§é¢æ¨¡åï¼é«è¾¾4Båæ°ï¼æ¶è¡¨ç°åºé«åº¦ç¨³å®æ§ï¼ä¸ä¸åºç°è¡¨ç¤ºå´©æºé®é¢ã
*   <strong>3Dæè½¬ä½ç½®åµå¥çæææ§</strong>ï¼å®éªè¯æï¼æ°å¼å¥ç3Dæè½¬ä½ç½®åµå¥ç¬ç«äºLayerLockï¼è½ä¸ºåºçº¿æ¨¡ååLayerLockæ¨¡åå¸¦æ¥æ§è½æåï¼ä¾å¦å¨SSv2åç±»ä»»å¡ä¸­æå2.5%ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>æ½å¨æå¤±çæçæè¡¡</strong>ï¼å¨è®¡ç®æ½å¨æå¤±æ¶ï¼ä½¿ç¨å°éï¼ä¾å¦5%ï¼çè¡¥ä¸è½ç¶ä»ä¼äºåºçº¿ï¼ä½ç¸æ¯ä½¿ç¨å¨é¨è¡¥ä¸ï¼å¨æ·±åº¦ä¼°è®¡æ§è½ä¸ç¥æä¸éãè¿è¡¨æå¨æçåæ§è½ä¹é´å­å¨æè¡¡ï¼éè¦è¿ä¸æ­¥æ¢ç´¢ã
*   <strong>å»ç»è°åº¦åæ°æææ§</strong>ï¼å»ç»å¼å§æ­¥æ°ãå»ç»é´éåæ¯æ¬¡å»ç»çå±æ°ï¼layer jumpï¼ç­åæ°å¯¹ä¸æ¸¸ä»»å¡æ§è½ææ¾èå½±åï¼éè¦ä»ç»è°æ´ãä¾å¦ï¼è¿æ©å»ç»æä¸æ¬¡å»ç»è¿å¤å±ä¼å¯¼è´æ§è½ä¸éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>æ©å±å°æ´é¿è§é¢ãæ´é«åè¾¨çåæ´æ·±æ¨¡å</strong>ï¼æ¸è¿å¼å»ç»èççè®¡ç®ååå­ä¸ºå°èªçç£å­¦ä¹ æ©å±å°æ´é¿è§é¢ãæ´å¤§åè¾¨çåæ´æ·±çæ¨¡åå¼è¾äºæ°éå¾ã
*   <strong>æ´ç²¾ç»çæçä¸æ§è½æè¡¡</strong>ï¼è¿ä¸æ­¥æ¢ç´¢æ½å¨æå¤±è®¡ç®ä¸­è¡¥ä¸å­éæ©çæçä¸æ§è½ä¹é´çå³ç³»ã
*   <strong>ä¼åå»ç»è°åº¦</strong>ï¼æ·±å¥ç ç©¶åä¼åå»ç»è°åº¦åæ°ï¼ä»¥å®ç°æä½³æ§è½åæçã
*   <strong>å¤ç®æ é¢æµçæ¢ç´¢</strong>ï¼å°½ç®¡ç®åLayerLockçåç®æ é¢æµæ¹æ³è¡¨ç°è¯å¥½ï¼ä½æªæ¥å¯ä»¥ç»§ç»­æ¢ç´¢å¨è®­ç»è¿ç¨ä¸­åæ¶é¢æµå¤ä¸ªç®æ ï¼åç´ åä¸­é´å±ï¼çæ½å¨çå¤ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10156v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10156v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10105v1'></a></p>
<h2 id="varco-vision-20-technical-report"><a href="https://arxiv.org/abs/2509.10105v1">VARCO-VISION-2.0 Technical Report</a></h2>
<p><strong>Authors:</strong> Young-rok Cha, Jeongho Ju, SunYoung Park, Jong-Hyeon Lee, Younghyun Yu, Youngjune Kim</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâVARCO-VISION-2.0 Technical Reportâçå¨é¢æè¦ï¼ç±Young-rok Chaç­äººæ°åï¼</p>
<p><strong>è®ºææè¦ï¼VARCO-VISION-2.0 ææ¯æ¥å</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æè§è§-è¯­è¨æ¨¡åï¼VLMï¼å¨å¤çå¤å¾åè¾å¥ãé©è¯­æ¬å°åä»»å¡ä»¥åå¨ä¿ææ ¸å¿è¯­è¨è½ååå®å¨æ§çåæ¶æåå¤æ¨¡æå¯¹é½æ¹é¢çå±éæ§ãå·ä½æ¥è¯´ï¼å®è´åäºå¼åä¸ä¸ªå¼æºãåè¯­ï¼é©è¯­åè±è¯­ï¼VLMï¼è½å¤æ´åç¡®ãæ´å¿ å®å°çè§£å¾ååææ¬ï¼å¹¶æ¯æå¸å±æç¥çåå­¦å­ç¬¦è¯å«ï¼OCRï¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
*   <strong>å¤å¾åçè§£æ¯æï¼</strong> VARCO-VISION-2.0 è½å¤åæ¶åæå¤å¼ å¾åï¼ä»èå®ç°æ´å¨é¢ãä¸ä¸ææç¥çå³ç­ï¼ææå¤çææ¡£ãå¾è¡¨åè¡¨æ ¼ç­å¤æè§è§åå®¹ã
*   <strong>é©è¯­è¯­è¨ä¸ä¸åï¼</strong> æ¨¡åå¯¹é©è¯­ççè§£ãä¸ä¸æåæåææ´æ·±å±æ¬¡çææ¡ï¼æ¾èæé«äºé©è¯­ææ¬çæçèªç¶åº¦ãæµçæ§ååç¡®æ§ã
*   <strong>å¸¦ææ¬å®ä½çOCRï¼</strong> æ¨¡åè½å¤è¯å«ææ¬ä½ç½®å¹¶æä¾è¾¹çæ¡ï¼è¿å¯¹äºææ¡£çè§£ãæ çè§£éåç»æåè§è§æ°æ®ç¹å«æç¨ã
*   <strong>å¢å¼ºçå®å¨æ§ï¼</strong> éè¿åå¥½ä¼åï¼preference optimizationï¼ï¼æ¨¡åæ¹è¿äºå¯¹æå®³æé²éª¨åå®¹çå¤çï¼ç¡®ä¿äºæ´å®å¨å¯é çäº¤äºã
*   <strong>åé¶æ®µè¯¾ç¨è®­ç»ç­ç¥ï¼</strong> æ¨¡åéç¨åé¶æ®µè¯¾ç¨è®­ç»ï¼ç»ååå­é«æææ¯ï¼å®ç°äºå¢å¼ºçå¤æ¨¡æå¯¹é½ï¼åæ¶ä¿çäºæ ¸å¿è¯­è¨è½åã
*   <strong>æ¨¡åæ¶æï¼</strong> åºäºLLaVA-OneVisionæ¶æï¼éç¨Qwen3ä½ä¸ºLLMï¼SigLIP2ï¼patch-16éç½®ï¼ä½ä¸ºè§è§ç¼ç å¨ï¼å¹¶éè¿ä¸¤å±MLPè¿æ¥å¨å°å¾åç¹å¾æå½±å°LLMçåµå¥ç©ºé´ã
*   <strong>1.7Bæ¨¡ååå§åç­ç¥ï¼</strong> 1.7Bæ¨¡åçè§è§ç¼ç å¨æéä»ç»è¿ç¬¬ä¸é¶æ®µè®­ç»ç14Bæ¨¡åä¸­åå§åï¼ä¿è¿äºç¥è¯è¿ç§»å¹¶å éäºæ¶æã
*   <strong>æ¨¡ååå¹¶ç­ç¥ï¼éå¯¹14Bæ¨¡åï¼ï¼</strong> éç¨âåå¹¶-è®­ç»-åå¹¶âç­ç¥ï¼éè¿åå¹¶å¤ä¸ªç¬¬ä¸é¶æ®µæ£æ¥ç¹æ¥è·å¾ç¨³å¥çç¬¬åé¶æ®µåå§åå¨ï¼å¹¶å¨ç¬¬åé¶æ®µå®æååæ¬¡åå¹¶æ£æ¥ç¹ä»¥çææç»æ¨¡åï¼ä»èåå°æ£æ¥ç¹æ¹å·®å¹¶èåä¸åæ¨¡å¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>å¼ºå¤§çç©ºé´å®ä½è½åï¼</strong> å¹¿æ³çåºåè¯ä¼°è¡¨æï¼æ¨¡åå¨ç©ºé´å®ä½æ¹é¢è¡¨ç°åºè²ï¼å¹¶å¨é©è¯­åè±è¯­ä¸¤ç§è¯­è¨ä¸é½åå¾äºæç«äºåçç»æã
*   <strong>OpenCompass VLMæè¡æ¦è¡¨ç°ï¼</strong> 14Bæ¨¡åå¨OpenCompass VLMæè¡æ¦ä¸ï¼å¨åç­è§æ¨¡çæ¨¡åä¸­æåç¬¬8ã
*   <strong>OCRæ§è½ï¼</strong> VARCO-VISION-2.0å¨CORDãICDAR2013åICDAR2015ç­OCRåºåæµè¯ä¸­è¡¨ç°åºæ¾èä¼äºæµè¡å¼æºOCRç³»ç»ï¼å¦PaddleOCRåEasyOCRï¼çæ§è½ï¼å¹¶ä¸åä¸ç³»ç»CLOVA OCRå·æç«äºåãè¿è¡¨æå¶å¼ºå¤§çè§è§-ææ¬å¯¹é½è½åã
*   <strong>ææ¬ä¸ç¨ä»»å¡è¡¨ç°ï¼</strong> å¨å°ºå¯¸åè½»éçº§VARCO-VISION-2.0æ¨¡åå¨åç§ææ¬ä¸ç¨ä»»å¡ï¼åæ¬éç¨ç¥è¯ãæä»¤éµå¾ªåå¤è½®æ¨çï¼ä¸ååå¾æç»­é«åï¼è¡¨æå¶è¯­è¨è½åå¨è®­ç»é¶æ®µå¾å°äºææä¿æã
*   <strong>è½»éçº§çæ¬ï¼</strong> é¤äº14Bæ¨¡åå¤ï¼è¿åå¸äºéå¯¹è®¾å¤é¨ç½²ä¼åç1.7Bè½»éçº§çæ¬ï¼ä¸ºå®éåºç¨æä¾äºå®ç¨éæ©ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
*   <strong>æä»¤é²æ£æ§ä¸è¶³ï¼</strong> æ¨¡åçè¾åºå¯¹è¡¨é¢æ ¼å¼ååï¼å¦ç©ºæ ¼ãæ¢è¡ç¬¦ï¼ææï¼è¡¨æè¿åº¦ä¾èµåºå®æç¤ºæ¨¡æ¿åå¯¹å¤æ ·åæä»¤æ ¼å¼çæ¥è§¦ä¸è¶³ã
*   <strong>ç¥è¯åææ¡£çè§£ä¸è¶³ï¼</strong> å°½ç®¡å¨æç¥åç©ºé´æ¨çæ¹é¢è¡¨ç°å¼ºå²ï¼ä½æ¨¡åå¨ç¥è¯å¯éååææ¡£ä¸­å¿ä»»å¡ä¸è¡¨ç°ä¸ä½³ï¼è¿å½å äºç¥è¯æ¥æºçæéçº³å¥åè®­ç»æé´ç¼ºä¹ç¨³å¥çå¸å±æç¥çç£ã
*   <strong>æä»£è½ååå¼±ï¼</strong> ä¸ä¹åçVARCO-VISIONæ¨¡åç¸æ¯ï¼æ¨¡åå¨æä»£ä»»å¡ä¸çæ§è½ææä¸éãè¿å¯è½éå¶äºæ¨¡åå¨æ¶åç²¾ç¡®å¯¹è±¡éæ©æåºäºè§è§ä¸ä¸æçæä»¤éµå¾ªçå®éåºç¨ä¸­çæ§è½ã
*   <strong>æ¨çè½åï¼</strong> å³ä½¿æç¡®æç¤ºéµå¾ªå¤æ­¥æ¨çï¼æ¨¡åä¹å¸¸å¸¸å¯¹å¶åå§é¢æµä¿æèªä¿¡ï¼å¾å°æ¹åè¾åºï¼è¡¨ææ¨çæç¤ºï¼å¦âè®©æä»¬ä¸æ­¥æ­¥æèâï¼å¹¶æªæ¾èæé«åç¡®æ§ã
*   <strong>æåçè§£ï¼</strong> å¨é©è¯­æååºåæµè¯ä¸­ï¼æ¨¡åè¡¨ç°ä»ä¸å¦å¶ä»é¢åæ¨¡åï¼è¿å¯è½ä¸é©è¯­æåç¸å³è®­ç»æ°æ®çç¸å¯¹æéæ§æå³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>å°æ¨¡åè¸é¦ï¼</strong> æ¢ç´¢æå¸-å­¦çè®­ç»æ¹æ¡ï¼å©ç¨å¤§æ¨¡åä½ä¸ºæå¸ï¼å¯¹å°æ¨¡åè¿è¡ç¥è¯è¸é¦ã
*   <strong>æ¨çè½åæ¹è¿ï¼</strong> éè¿å¼ºåå­¦ä¹ æ¿å±æ¨çè¡ä¸ºï¼ä¾å¦é¾å¼æèè¸é¦ååºäºåå¥½çå¾®è°ï¼ä»¥å¢å¼ºå¤æ­¥æ¨çè½åã
*   <strong>é«æä¸ä¸æå¤çï¼</strong> éç¨YaRNç­ææ¯è¿è¡é«æä¸ä¸ææ©å±ï¼å¹¶å©ç¨åºåå¹¶è¡æ§åå°åå­å¼éï¼ä»¥æ¯æé«åè¾¨çå¤å¾åè¾å¥åé¿æ¶åºè§é¢çè§£ã
*   <strong>æ©å±å°è§é¢æ¨¡æåå¶ä»ï¼</strong> å¢å¼ºæ¨¡åå¯¹é¿æ¶åºè§é¢ççè§£è½åï¼æ¯æ3Dèªç±è§è§è§é¢åå¯æ§ç¸æºè½¨è¿¹ï¼å¹¶å¼åæ´åé³é¢åè¯­é³ç­æ¨¡æçå¨æ¨¡ææ¨¡åã
*   <strong>è¿åå·èº«å¤æ¨¡ææºè½ä½ï¼</strong> éæ­¥å¼åè½å¤æç¥åè¡å¨çå·èº«æºè½ä½ï¼åæ¬ä¸GUIäº¤äºåæçºµå±å¹ç¯å¢ã
*   <strong>æ¨¡ååæ°æ®è§æ¨¡æ©å±ï¼</strong> æ©å¤§æ¨¡åå®¹éåæ°æ®éï¼éç¨é«ææ¶æè®¾è®¡ï¼æ©å±é«è´¨éãå¤æ ·åçç£çå¤æ¨¡æè®­ç»è¯­æåºï¼å¹¶éè¿æ¹è¿å¥å±ä¿¡å·åå¯æ©å±ç®æ³ä¼ååå¥½å­¦ä¹ ã</p>
<p>æ»èè¨ä¹ï¼VARCO-VISION-2.0ä»£è¡¨äºåè¯­VLMé¢åçéå¤§è¿å±ï¼ç¹å«æ¯å¨é©è¯­åè±è¯­çè§è§-è¯­è¨çè§£ãOCRåå¤å¾åå¤çæ¹é¢ãå°½ç®¡å­å¨ä¸äºå±éæ§ï¼ä½å¶å¼æºåå¸åå¼ºå¤§çæ§è½ä½¿å¶æä¸ºæå»ºå®ç¨å¤æ¨¡æç³»ç»çæåæ¯çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10105v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10105v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10080v1'></a></p>
<h2 id="bevtraj-map-free-end-to-end-trajectory-prediction-in-birds-eye-view-with-deformable-attention-and-sparse-goal-proposals"><a href="https://arxiv.org/abs/2509.10080v1">BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals</a></h2>
<p><strong>Authors:</strong> Minsang Kong, Myeongjun Kim, Sang Gu Kang, Sang Hun Lee</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV, I.2.9; I.4.8</p>
<p><strong>Abstract:</strong></p>
<p>In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Minsang Kong, Myeongjun Kim, Sang Gu Kang, Sang Hun Leeæ°åçè®ºæâBEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposalsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼BEVTraj: åºäºé¸ç°å¾ãå¯åå½¢æ³¨æåä¸ç¨çç®æ æè®®çæ å°å¾ç«¯å°ç«¯è½¨è¿¹é¢æµ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
èªå¨é©¾é©¶ä¸­çè½¨è¿¹é¢æµå¯¹äºå®å¨é«æçå¯¼èªè³å³éè¦ãç°ææ¹æ³éå¸¸ä¾èµé¢æå»ºçé«æ¸ï¼HDï¼å°å¾æå®æ¶å±é¨å°å¾æå»ºæ¨¡åæ¥æ´åéæç¯å¢ä¿¡æ¯ãç¶èï¼é¢æå»ºçHDå°å¾è¦çèå´æéä¸æ æ³éåºç¬æååï¼èå±é¨å°å¾æå»ºæ¨¡åå¯è½å ä»è¯å«é¢å®ä¹åç´ èéæ¼å³é®åºæ¯ç»èæå¼å¥éè¯¯ï¼ä»èéä½é¢æµæ§è½ãè¿ç¯è®ºææ¨å¨è§£å³è¿äºéå¶ï¼æåºä¸ç§ä¸ä¾èµé¢æå»ºå°å¾ï¼ç´æ¥å©ç¨å®æ¶ä¼ æå¨æ°æ®è¿è¡è½¨è¿¹é¢æµçæ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
BEVTrajæ¡æ¶å¼å¥äºä»¥ä¸å³é®åæ°ï¼
*   <strong>æ å°å¾çé¸ç°å¾ï¼BEVï¼ç©ºé´æä½ï¼</strong> BEVTrajæ¯ä¸ç§æ°é¢çãç®æ é©±å¨çè½¨è¿¹é¢æµæ¡æ¶ï¼å®ç´æ¥å©ç¨ä»åå§ä¼ æå¨æ°æ®æå»ºçBEVè¡¨ç¤ºè¿è¡æä½ï¼ä»èæ¶é¤äºå¯¹é¢æå»ºHDå°å¾çä¾èµãè¿ä½¿å¾ç³»ç»è½å¤éåºç¬æååï¼å¹¶åå°ä¿¡æ¯æå¤±ã
*   <strong>å¯åå½¢æ³¨æåï¼Deformable Attentionï¼ï¼</strong> BEVTrajå©ç¨å¯åå½¢æ³¨æåæºå¶ï¼ä»å¯éçBEVç¹å¾ä¸­é«ææåç¸å³ä¸ä¸æä¿¡æ¯ãè¿ç§æºå¶åè®¸æ¨¡åéæ©æ§å°å³æ³¨BEVç¹å¾å¾ä¸­çå³é®ç©ºé´ä½ç½®ï¼ä»èå¨å¯éååä½çBEVè¡¨ç¤ºä¸­æé«æçåé¢æµåç¡®æ§ã
*   <strong>ç¨çç®æ åéæè®®ï¼Sparse Goal Candidate Proposal, SGCPï¼æ¨¡åï¼</strong> å¼å¥SGCPæ¨¡åä»¥è§£å³ç°æåºäºç®æ çæ¹æ³å¯¹ç®æ åéå¯åº¦ææçé®é¢ãSGCPæ¨¡åè½å¤çæç¨ççç®æ åééï¼è¿äºåééåºäºç®æ æºè½ä½çå¨æç¶æåBEVç¹å¾å¾è¿è¡æ¡ä»¶åï¼ä»èå®ç°å®å¨ç«¯å°ç«¯çé¢æµï¼æ ééæå¤§å¼æå¶ï¼NMSï¼ç­åå¤çæ­¥éª¤ã
*   <strong>è¿­ä»£å¯åå½¢è§£ç å¨ï¼</strong> è¯¥è§£ç å¨éè¿è¿­ä»£ç»åè¿ç¨ï¼åºäºBEVç¹å¾ååºæ¯ä¸ä¸æç¹å¾é¢æµå¹¶æ¹è¿ç®æ æºè½ä½çå¤æ¨¡ææªæ¥è½¨è¿¹ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½å¯ä¸HDå°å¾æ¨¡ååª²ç¾ï¼</strong> å¹¿æ³çå®éªè¡¨æï¼BEVTrajå¨è½¨è¿¹é¢æµæ§è½ä¸è¾¾å°äºä¸æåè¿çåºäºHDå°å¾çæ¨¡åç¸å½çæ°´å¹³ï¼çè³å¨æäºææ ï¼å¦Miss Rateï¼ä¸è¡¨ç°æ´ä¼ãè¿è¯æäºæ å°å¾è½¨è¿¹é¢æµçå¯è¡æ§ã
*   <strong>å¢å¼ºççµæ´»æ§ï¼</strong> éè¿æ¶é¤å¯¹é¢æå»ºå°å¾çä¾èµï¼BEVTrajæä¾äºæ´å¤§ççµæ´»æ§ï¼è½å¤éåºæªæ å°åºååå¨æéè·¯æ¡ä»¶ã
*   <strong>å¯¹å¤æåºæ¯çé²æ£æ§ï¼</strong> BEVTrajå¨æ¥è½¬å¼¯åé®æ¡äº¤åå£ç­å¤ææ¡ä»¶ä¸ï¼è½å¤çæåçä¸ä¸è½¦éå¯¹é½çæªæ¥è½¨è¿¹ï¼è¿å¾çäºå¶ç´æ¥ä»åå§ä¼ æå¨æ°æ®ä¸­æåç»ç²åº¦è§è§çº¿ç´¢çè½åã
*   <strong>ä¸BEVèå¼å¼å®¹ï¼</strong> è¯¥æ¡æ¶ä¸éç¨BEVèå¼çç°ä»£èªå¨é©¾é©¶ç³»ç»å®å¨å¼å®¹ï¼ä¾¿äºéæå°ç°æç®¡éä¸­ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ä¼ æå¨æ°æ®èå´éå¶ï¼</strong> ä¼ æå¨æ°æ®åºæçæç¥èå´éå¶äºBEVTrajçæ§è½ï¼å°¤å¶æ¯å¨éè¦æ´å¹¿éç©ºé´ä¸ä¸æçæåµä¸ãè½ç¶å¯åå½¢æ³¨æåæå©äºéæ©æ§å°å³æ³¨ä¿¡æ¯åºåï¼ä½ä¼ æå¨èå´çç©çéå¶ä»ç¶å­å¨ã
*   <strong>å®æ¶HDå°å¾æå»ºçåç¡®æ§ï¼</strong> è®ºæè½ç¶éç¦»äºå°å¾èå´çå½±åï¼ä½å®éé¨ç½²ä¸­å®æ¶HDå°å¾æå»ºçåç¡®æ§å¯åæ§ä»éèèï¼è¿å¯è½å¨è¦çèå´å¢å æ¶æ¶åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´åBEVç¹å¾çç®æ æ£æµä¸è·è¸ªï¼</strong> å°BEVTrajæ©å±ï¼çº³å¥åºäºBEVç¹å¾çç®æ æ£æµåè·è¸ªæ¨¡åï¼ä»¥å¨ç»ä¸çè¡¨ç¤ºç©ºé´ä¸­å®ç°éæåå¨æåºæ¯ä¿¡æ¯çèåå¤çã
*   <strong>å¢å¼ºBEVç¹å¾çæ¶é´å»ºæ¨¡è½åï¼</strong> éè¿å¾ªç¯æ¶ææè·¨æ«æèåææ¯ï¼è¿ä¸æ­¥æé«BEVç¹å¾çæ¶é´å»ºæ¨¡è½åï¼ä»¥æ´å¥½å°ææé¿æè¿å¨æ¨¡å¼ã
*   <strong>èªéåºBEVç½æ ¼ç»æï¼</strong> ç ç©¶èªéåºBEVç½æ ¼ç»æï¼ä»¥ç¼è§£ç©ºé´éå¶ï¼å®ç°æ´çµæ´»åç»ç²åº¦çè½¨è¿¹é¢æµã
*   <strong>æ©å±å°è¿å¨è§åï¼</strong> å°è¯¥æ¡æ¶èªç¶å°æ©å±å°è¿å¨è§ååºç¨ï¼ä½ä¸ºè¿æ¥æç¥åè§åçç»ä¸è§£å³æ¹æ¡ã
*   <strong>å¶ä»é¢ååºç¨ï¼</strong> å°æå¼åçææ¯åºç¨äºæ´å¹¿æ³çé¢åï¼å¦çæ§ç³»ç»ï¼è½¦è¾åäººç±»æ´»å¨çæµï¼åæºå¨äººææ¯ï¼äººæºäº¤äºåå¨æç¯å¢å¯¼èªï¼ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps.</li>
<li>Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps.</li>
<li>Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10080v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10080v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.10026v1'></a></p>
<h2 id="lav-cot-language-aware-visual-cot-with-multi-aspect-reward-optimization-for-real-world-multilingual-vqa"><a href="https://arxiv.org/abs/2509.10026v1">LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA</a></h2>
<p><strong>Authors:</strong> Jing Huang, Zhiya Tan, Shutao Gong, Fanwei Zeng, Jianshu Li</p>
<p><strong>Published:</strong> 2025-09-12</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to <script type="math/tex">\sim</script>9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2<script type="math/tex">\times</script> larger scales by <script type="math/tex">\sim</script>2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâLaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQAâè®ºæçå¨é¢æè¦ï¼ä½¿ç¨ä¸­æä¹¦åï¼</p>
<p><strong>è®ºææè¦ï¼LaV-CoT: é¢åçå®ä¸çå¤è¯­è¨VQAçè¯­è¨æç¥è§è§CoTä¸å¤æ¹é¢å¥å±ä¼å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
éçå¤§åè§è§è¯­è¨æ¨¡åï¼VLMsï¼å¨å¤è¯­è¨è§è§é®ç­ï¼mVQAï¼æ¹é¢è½åçæåï¼é¾å¼æèï¼CoTï¼æ¨çè¢«è¯æè½å¢å¼ºå¯è§£éæ§åå¤ææ¨çè½åãç¶èï¼ç°ææ¹æ³ä¸»è¦ä¾èµææ¬CoTï¼å¯¹å¤è¯­è¨å¤æ¨¡ææ¨ççæ¯ææéï¼è¿éå¶äºå®ä»¬å¨çå®ä¸çåºç¨ä¸­çé¨ç½²ãå·ä½ææåæ¬ï¼(i) è¯­è¨ä¸ä¸è´æ§ï¼(ii) è§è§-ææ¬éä½ï¼è§è§åå®¹ä¸ç¿»è¯ææ¬çæ¥å°ä¸è¶³ï¼ï¼ä»¥å(iii) æéçå¤è¯­è¨è§è§æ¨çè½åï¼å°¤å¶æ¯å¨éè¦å¤æè·¨è¯­è¨ãå¤æ¨¡ææ¨ççä»»å¡ä¸­ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºè§£å³ä¸è¿°é®é¢ï¼è®ºææåºäº<strong>LaV-CoT</strong>ï¼è¿æ¯é¦ä¸ªç»åå¤æ¹é¢å¥å±ä¼åçè¯­è¨æç¥è§è§CoTæ¡æ¶ãå¶ä¸»è¦åæ°åè´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>å¯è§£éçå¤é¶æ®µæ¨çç®¡éï¼</strong> LaV-CoTå¼å¥äºä¸ä¸ªç±åä¸ªå³é®ç»ä»¶ç»æçå¤é¶æ®µæ¨çç®¡éï¼(1) å¸¦æè¾¹çæ¡ï¼BBoxï¼çææ¬æè¦ï¼(2) è¯­è¨è¯å«ï¼(3) ç©ºé´å¯¹è±¡çº§å¾åæè¿°ï¼ä»¥å(4) éæ­¥é»è¾æ¨çãè¿ç§ç»æåç®¡éæç¡®å°è§£è¦äºè¯­è¨åè§è§æ¨çï¼å®ç°äºç»ç²åº¦çè·¨æ¨¡æå¯¹é½ï¼å¹¶æé«äºå¤è¯­è¨ç¯å¢ä¸çå¯è§£éæ§ã</li>
<li><strong>èªå¨åæ°æ®çææ¹æ³ï¼</strong> éå¯¹é«è´¨éå¤è¯­è¨æ¨çæ°æ®æå»ºçææï¼è®ºæè®¾è®¡äºä¸ç§èªå¨æ°æ®çææ¹æ³ï¼éè¿è¿­ä»£çæãçº æ­£åç»åæ¥åå»ºå¤è¯­è¨CoTæ æ³¨ãè¿ç¡®ä¿äºå¯æ©å±å°çæç»æåãå¯éªè¯çé«è´¨éè®­ç»æ°æ®ï¼åæ¶ææè¯­è¨ä¿çåº¦åå¤æ¨¡ææ¨çè´¨éã</li>
<li><strong>ä¸¤é¶æ®µè®­ç»èå¼ä¸è¯­è¨æç¥ç¾¤ç»ç¸å¯¹ç­ç¥ä¼åï¼GRPOï¼ï¼</strong> LaV-CoTéç¨ä¸¤é¶æ®µè®­ç»èå¼ï¼ç»åäºçç£å¾®è°ï¼SFTï¼åè¯­è¨æç¥ç¾¤ç»ç¸å¯¹ç­ç¥ä¼åï¼GRPOï¼ãGRPOç±å¯éªè¯çå¤æ¹é¢å¥å±æå¯¼ï¼åæ¬è¯­è¨ä¸è´æ§å¥å±ãææ¬æ®µåå¯¹è±¡è®¡æ°å¥å±ãæç»ç­æ¡ç¼è¾è·ç¦»å¥å±åæ ¼å¼å¥å±ï¼ä»èå®ç°ç¨³å®çä¼ååè·¨è¯­è¨ãè·¨æ¨¡æçé²æ£æ¨çã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éªè¯ä¼°è¯æäºLaV-CoTçæææ§ï¼</p>
<ul>
<li>å¨MMMBãMultilingual MMBenchåMTVQAç­å¬å±æ°æ®éä¸ï¼LaV-CoTï¼SFT + GRPOåä½ï¼ç¸æ¯åç­è§æ¨¡çå¼æºåºçº¿æ¨¡åï¼åç¡®çæé«äºçº¦9.5%ï¼çè³è¶è¶äºè§æ¨¡å¤§2åçæ¨¡åçº¦2.6%ã</li>
<li>LaV-CoTå¨ç¹å®å¤è¯­è¨è®¾ç½®ï¼å¦é¿æä¼¯è¯­ãåè³å¶è¯­ãé©è¯­ç­ï¼ä¸ï¼è¡¨ç°ä¼äºGPT-4o-0513åGemini-2.5-flashç­åè¿ä¸ææ¨¡åã</li>
<li>å¨çº¿A/Bæµè¯è¿ä¸æ­¥éªè¯äºè¯¥æ¹æ³å¨çå®ä¸çæ°æ®ä¸çæææ§ï¼æ¾ç¤ºå¶å¨å·¥ä¸é¨ç½²ååä¸åºç¨ä¸­çæ½åï¼æ¾èæé«äºç­æ¡æ¥åçåç¨æ·æ»¡æåº¦ã</li>
<li>GRPOè®­ç»çå¼å¥æ¾èæåäºLaV-CoTçæ§è½ï¼å°¤å¶æ¯å¨è·¨è¯­è¨æ¨çæ¹é¢ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡åå¾äºå¯åçææï¼LaV-CoTä»å­å¨ä¸äºå±éæ§ï¼</p>
<ul>
<li><strong>å¯¹å¤è¯­è¨è¾å¥è´¨éçæææ§ï¼</strong> LaV-CoTçæææ§åå³äºå¤æ¨¡æè¾å¥çè´¨éãå½ææ¡£å¾ååå«å¤§éè·¨èæ¬çè¯­è¨æ··åæ¶ï¼æ¨çç®¡éå¯è½é¾ä»¥ä¿æè¯­è¨ä¸è´æ§åè¯­ä¹ä¿çåº¦ã</li>
<li><strong>ä½èµæºè¯­è¨çè¦çèå´ï¼</strong> å½åè®­ç»ä¸»è¦ä¾èµäºä¾§éäºä¸­é«èµæºè¯­è¨çå¼æºå¤è¯­è¨VQAæ°æ®éãçæ­£ä½èµæºæä¸å¸¸è§è¯­è¨çé«è´¨éæ°æ®éä»ç¶ç¨ç¼ºï¼å¶æ¶éåæå»ºå¯¹äºæ´å¹¿æ³çåå®¹æ§è³å³éè¦ã</li>
<li><strong>å¿«æ¢æ¨çæ´åçæ¢ç´¢æéï¼</strong> LaV-CoTç®åè®¾è®¡ä¸ºæ¢éãå¤æ­¥éª¤æ¨çç®¡éä»¥å¢å¼ºå¯è§£éæ§ãè½ç¶ææï¼ä½å°æªæ´åå¿«éæèç­ç¥ææ··åå¿«æ¢æ¨çæºå¶ï¼è¿å°å¨æªæ¥å·¥ä½ä¸­è¿ä¸æ­¥æ¢ç´¢ä»¥æé«æçåéåºæ§ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥å·¥ä½è®¡åå°LaV-CoTæ©å±å°æ´å¹¿æ³çä½èµæºè¯­è¨åç¹å®é¢ååºç¨ï¼å¹¶è¿ä¸æ­¥æ¢ç´¢åè¿çå¥å±å»ºæ¨¡ï¼ä»¥å¢å¼ºå¤è¯­è¨å¤æ¨¡ææ¨çç³»ç»çé²æ£æ§ååå®¹æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization.</li>
<li>Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.10026v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.10026v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-15 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
