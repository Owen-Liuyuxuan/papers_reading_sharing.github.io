<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-15 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-14/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-16/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-15">Arxiv Computer Vision Papers - 2025-10-15</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-14" class="nav-link">Arxiv 计算机视觉领域每日报告执行摘要 (2025-10-14)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda" class="nav-link">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a>
                </li>
                <li class="nav-item">
                    <a href="#compositional-zero-shot-learning-a-survey" class="nav-link">Compositional Zero-Shot Learning: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation" class="nav-link">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search" class="nav-link">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a>
                </li>
                <li class="nav-item">
                    <a href="#vico-a-training-strategy-towards-semantic-aware-dynamic-high-resolution" class="nav-link">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#what-if-understanding-motion-through-sparse-interactions" class="nav-link">What If : Understanding Motion Through Sparse Interactions</a>
                </li>
                <li class="nav-item">
                    <a href="#flashvsr-towards-real-time-diffusion-based-streaming-video-super-resolution" class="nav-link">FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#omni-captioner-data-pipeline-models-and-benchmark-for-omni-detailed-perception" class="nav-link">Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#sail-embedding-technical-report-omni-modal-embedding-foundation-model" class="nav-link">SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</a>
                </li>
                <li class="nav-item">
                    <a href="#reasoning-in-the-dark-interleaved-vision-text-reasoning-in-latent-space" class="nav-link">Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-15">Arxiv Computer Vision Papers - 2025-10-15</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-14">Arxiv 计算机视觉领域每日报告执行摘要 (2025-10-14)</h2>
<p><strong>概述：</strong></p>
<p>今天的 Arxiv 计算机视觉论文主要围绕<strong>多模态学习、视频理解与处理、以及基础模型</strong>这三大核心主题展开。我们观察到研究人员正积极探索如何将视觉与语言模型结合，以实现更通用、更强大的感知和推理能力，尤其是在复杂场景（如城市监控、视频分割）中。同时，对高效、实时处理的需求也推动了视频超分辨率和动态高分辨率训练策略的进步。</p>
<p><strong>主要趋势与亮点：</strong></p>
<ol>
<li>
<p><strong>多模态与视觉-语言模型 (VLM) 的融合与应用：</strong> 多篇论文聚焦于 VLM 的发展和应用。</p>
<ul>
<li><strong>"Towards General Urban Monitoring with Vision-Language Models"</strong> 提供了 VLM 在城市监控领域的全面综述、评估和未来研究议程，表明 VLM 在实际应用中潜力巨大。</li>
<li><strong>"DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search"</strong> 展示了多模态 LLM 在多模态网络搜索中的应用，预示着搜索范式可能发生变革。</li>
<li><strong>"Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space"</strong> 探索了在潜在空间进行视觉-文本交错推理，为更深层次的多模态理解提供了新思路。</li>
<li><strong>"Omni-Captioner"</strong> 和 <strong>"SAIL-Embedding Technical Report"</strong> 则分别构建了全方位详细感知的基准、模型和数据管道，以及提出了全模态嵌入基础模型，旨在统一不同模态的表示。</li>
</ul>
</li>
<li>
<p><strong>视频理解与处理的复杂性与效率提升：</strong></p>
<ul>
<li><strong>"LSVOS 2025 Challenge Report"</strong> 揭示了复杂视频目标分割的最新进展，强调了该领域持续的挑战和创新。</li>
<li><strong>"FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution"</strong> 解决了视频超分辨率的实时性问题，通过扩散模型实现了高效的流媒体处理，具有重要的实际应用价值。</li>
<li><strong>"ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution"</strong> 提出了一种语义感知的动态高分辨率训练策略，旨在优化视频处理的质量和效率。</li>
<li><strong>"What If : Understanding Motion Through Sparse Interactions"</strong> 则从稀疏交互的角度探索了运动理解，为视频中的行为分析提供了新的视角。</li>
</ul>
</li>
<li>
<p><strong>基础模型与通用能力：</strong></p>
<ul>
<li><strong>"SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model"</strong> 提出了一种全模态嵌入基础模型，旨在为不同模态提供统一的表示，这可能成为未来多模态AI系统的基石。</li>
<li><strong>"Compositional Zero-Shot Learning: A Survey"</strong> 对组合零样本学习进行了全面调查，这对于构建能够泛化到未见过概念的通用AI系统至关重要。</li>
</ul>
</li>
</ol>
<p><strong>特别显著或创新论文：</strong></p>
<ul>
<li><strong>"FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution"</strong>：在实时视频处理方面取得了突破，将扩散模型应用于流媒体超分辨率，具有显著的工程和应用价值。</li>
<li><strong>"SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model"</strong>：提出了全模态嵌入基础模型，其目标是统一所有模态的表示，这可能对未来的多模态AI架构产生深远影响。</li>
<li><strong>"Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space"</strong>：探索了在潜在空间进行更深层次的视觉-文本推理，为克服当前VLM的局限性提供了新的研究方向。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>全模态（Omni-modal）嵌入与基础模型：</strong> 旨在构建能够处理和统一所有模态信息的通用表示。</li>
<li><strong>潜在空间中的多模态推理：</strong> 探索在抽象的潜在空间中进行视觉-文本交错推理，以实现更高级别的理解。</li>
<li><strong>扩散模型在实时视频处理中的应用：</strong> 扩散模型在生成质量上表现出色，其在实时视频超分辨率等任务中的效率优化是重要方向。</li>
<li><strong>语义感知与动态高分辨率处理：</strong> 结合语义信息来优化视频处理的质量和效率。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于忙碌的研究人员，我建议优先阅读以下论文，以获取最前沿和最具影响力的信息：</p>
<ol>
<li><strong>"SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model" (Lin Lin et al.)</strong>：如果您的研究涉及多模态基础模型或通用表示，这篇论文提供了潜在的未来方向。</li>
<li><strong>"FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution" (Junhao Zhuang et al.)</strong>：如果您关注实时视频处理、扩散模型应用或工程优化，这篇论文提供了重要的技术突破。</li>
<li><strong>"Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda" (André Torneiro et al.)</strong>：对于了解VLM在实际应用中的潜力、挑战和未来研究方向，这篇综述是极佳的起点。</li>
<li><strong>"Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space" (Chao Chen et al.)</strong>：如果您对多模态推理的深层机制和潜在空间操作感兴趣，这篇论文提供了创新的思路。</li>
</ol>
<p>这些论文代表了当前计算机视觉和机器学习领域最活跃和最有前景的研究方向，值得深入探讨。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></li>
<li><a href="#2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></li>
<li><a href="#2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></li>
<li><a href="#2510.12801v1">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a></li>
<li><a href="#2510.12793v1">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</a></li>
<li><a href="#2510.12777v1">What If : Understanding Motion Through Sparse Interactions</a></li>
<li><a href="#2510.12747v1">FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</a></li>
<li><a href="#2510.12720v1">Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</a></li>
<li><a href="#2510.12709v1">SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</a></li>
<li><a href="#2510.12603v1">Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.12400v1'></a></p>
<h2 id="towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda"><a href="https://arxiv.org/abs/2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></h2>
<p><strong>Authors:</strong> André Torneiro, Diogo Monteiro, Paulo Novais, Pedro Rangel Henriques, Nuno F. Rodrigues</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Urban monitoring of public infrastructure (such as waste bins, road signs,
vegetation, sidewalks, and construction sites) poses significant challenges due
to the diversity of objects, environments, and contextual conditions involved.
Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.
This raises a critical question: Can machines now "see" like citizens and infer
informed opinions about the condition of urban infrastructure? Vision-Language
Models (VLMs), which integrate visual understanding with natural language
reasoning, have recently demonstrated impressive capabilities in processing
complex visual information, turning them into a promising technology to address
this challenge. This systematic review investigates the role of VLMs in urban
monitoring, with particular emphasis on zero-shot applications. Following the
PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021
and 2025 to address four core research questions: (1) What urban monitoring
tasks have been effectively addressed using VLMs? (2) Which VLM architectures
and frameworks are most commonly used and demonstrate superior performance? (3)
What datasets and resources support this emerging field? (4) How are VLM-based
applications evaluated, and what performance levels have been reported?</p>
<p><strong>Analysis:</strong></p>
<p>以下是André Torneiro等人撰写的论文“Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决城市公共基础设施（如垃圾桶、路标、植被、人行道和建筑工地）监测所面临的重大挑战。传统的物联网传感器和人工检查方法成本高昂、难以扩展，且往往与市民的直观视觉感知不符。因此，核心研究问题是：机器能否像市民一样“看”，并对城市基础设施的状况形成有根据的判断？论文特别关注视觉语言模型（VLMs）在城市监测中的作用，尤其是在零样本（zero-shot）应用方面。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
* <strong>系统性综述与分类：</strong> 论文采用PRISMA方法论，对2021年至2025年间发表的32篇同行评审研究进行了分析。
* <strong>功能性分类法：</strong> 论文引入了一个包含七个领域的VLM城市应用功能性分类法，包括：物体检测与分割、城市规划与土地利用分类、导航与寻路、交通分析与运输、城市场景理解与感知、地理定位与位置查找、城市监控与安全。这为研究人员提供了一个结构化的框架，以理解现有方法并识别特定任务的最合适技术。
* <strong>模型架构与数据集分析：</strong> 论文深入分析了最常用的11种VLM架构及其优缺点，并考察了该领域广泛使用的数据集、评估指标和基准测试方法。</p>
<p><strong>3. 主要结果及其重要性：</strong>
* <strong>VLM在城市监测中的潜力：</strong> VLMs通过整合视觉理解和自然语言推理，在处理复杂视觉信息方面展现出强大能力，被认为是解决城市监测挑战的有前景技术。
* <strong>零样本应用的有效性：</strong> 综述表明，VLMs在零样本应用中表现出色，例如无需专用训练样本即可检测“人行道上溢出的垃圾桶”。
* <strong>性能多样性：</strong> 不同任务、数据集和方法导致性能结果差异显著。例如，物体检测与分割任务中，SAM和Grounding DINO结合取得了高IoU分数；城市规划中，UrbanCLIP在住宅区F1分数达到0.82。
* <strong>数据集使用模式：</strong> 街景图像数据集（如Google Street View、Mapillary Vistas）因其视觉丰富性和与人类中心城市场景的对齐而占据主导地位。合成数据集（如CARLA、SYNTHIA）在模拟稀有或危险条件方面很有价值，但存在真实世界泛化差距。
* <strong>模型架构偏好：</strong> CLIP、Grounding DINO和GPT-3.5是最常用的模型，反映了对模块化、通用骨干网络的强烈偏好，这些骨干网络支持零样本或基于提示的定制。</p>
<p><strong>4. 论文中提及的局限性：</strong>
* <strong>评估实践滞后：</strong> 尽管数据集多样性有所改善，但评估实践未能跟上部署需求。很少有研究对跨城市或多帧泛化进行基准测试，也未能充分考虑操作限制。
* <strong>模态差距与上下文缺失：</strong> 大多数城市VLM管道过度关注静态图像-文本对，忽略了时间序列、深度图、地理定位甚至环境声音等丰富的多模态信号。这限制了模型在自动驾驶、交通预测等动态任务中的性能和推理深度。
* <strong>度量碎片化与评估不足：</strong> 缺乏标准化评估协议，导致性能报告不一致，难以进行直接比较。许多研究省略了基线比较、置信区间和详细的错误分析。
* <strong>对资源密集型架构的过度依赖：</strong> 许多最先进的VLM模型计算成本高昂，不适合实时、移动或嵌入式部署，导致创新与实际应用之间存在差距。
* <strong>伦理盲点与法律疏忽：</strong> 尽管AI日益融入城市环境，但伦理考量（如算法公平性、知情同意、数据来源、隐私保护）在VLM研究中仍被忽视。
* <strong>边缘硬件实验有限：</strong> 很少有研究在边缘硬件上进行模拟或部署，这阻碍了对延迟、热性能和功耗权衡的理解。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
* <strong>SLM-VLM混合架构：</strong> 结合小型语言模型（SLMs）与模块化视觉编码器和解码器，以实现高效的多模态推理，适用于智能手机、无人机和AR/VR设备上的实时、低延迟推理。
* <strong>统一的城市基准：</strong> 开发集成多语言提示、多模态传感器流（图像、视频、音频、LiDAR）和文化多样性地理数据的评估套件，以确保可复现性、跨领域可比性和鲁棒泛化。
* <strong>以部署为中心的设计：</strong> 将硬件限制、延迟要求、热预算和隐私考量等部署约束嵌入模型开发周期。
* <strong>嵌入式伦理与合规性：</strong> 将文化鲁棒性检查、算法公平性评估、数据集同意追踪和偏见审计整合到核心基准测试和评估生命周期中。
* <strong>可复现的开放生态系统：</strong> 培养透明文化，包括版本化数据集、Docker化基线、公共排行榜和共享评估代码。</p>
<p>总而言之，这篇论文全面回顾了VLMs在城市监测中的应用现状，识别了其潜力、现有局限性，并提出了一个多维度的研究议程，旨在推动城市AI系统向更可部署、包容、可解释和符合伦理的方向发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12400v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12400v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11106v1'></a></p>
<h2 id="compositional-zero-shot-learning-a-survey"><a href="https://arxiv.org/abs/2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></h2>
<p><strong>Authors:</strong> Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision
that enables models to recognize unseen combinations of known attributes and
objects during inference, addressing the combinatorial challenge of requiring
training data for every possible composition. This is particularly challenging
because the visual appearance of primitives is highly contextual; for example,
<code>small'' cats appear visually distinct from</code>older'' ones, and <code>wet'' cars
differ significantly from</code>wet'' cats. Effectively modeling this contextuality
and the inherent compositionality is crucial for robust compositional zero-shot
recognition. This paper presents, to our knowledge, the first comprehensive
survey specifically focused on Compositional Zero-Shot Learning. We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement. We provide a detailed comparative analysis of
these methods, highlighting their core advantages and limitations in different
problem settings, such as closed-world and open-world CZSL. Finally, we
identify the most significant open challenges and outline promising future
research directions. This survey aims to serve as a foundational resource to
guide and inspire further advancements in this fascinating and important field.
Papers studied in this survey with their official code are available on our
github: https://github.com/ans92/Compositional-Zero-Shot-Learning</p>
<p><strong>Analysis:</strong></p>
<p>这篇由Ans Munir, Faisal Z. Qureshi, Mohsen Ali和Muhammad Haris Khan撰写的论文“Compositional Zero-Shot Learning: A Survey”对组合式零样本学习（CZSL）领域进行了首次全面综述。</p>
<p><strong>1. 主要问题或研究问题：</strong>
CZSL旨在使计算机视觉模型能够识别已知属性和对象在推理时未曾见过的组合，从而解决为每种可能组合都需要训练数据的组合挑战。核心问题在于，基元（如属性和对象）的视觉外观具有高度情境性，且模型需要有效建模这种情境性和固有的组合性，以实现鲁棒的零样本识别。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要贡献在于提出了首个基于“解耦”原则的CZSL方法综合分类法。该分类法将现有方法分为四大类：
*   <strong>无显式解耦（No Explicit Disentanglement）：</strong> 将属性-对象组合视为单一单元，通过整体嵌入或直接融合机制进行建模。
*   <strong>文本特征解耦（Textual Disentanglement）：</strong> 在语言空间中分离基元的语义嵌入，以实现独立概念表示。
*   <strong>视觉特征解耦（Visual Disentanglement）：</strong> 在图像表示中隔离属性和对象的视觉特征，将其分解为可组合的表示。
*   <strong>跨模态（混合）解耦（Cross-Modal (Hybrid) Disentanglement）：</strong> 同时在视觉和文本空间中解耦基元，并通过跨模态对齐整合互补信息。</p>
<p>在第二层，方法根据其建模属性和处理组合挑战的策略（如基于原型建模、合成嵌入、因果推理等）进一步细分。</p>
<p><strong>3. 主要结果及其意义：</strong>
通过对现有方法的详细比较分析，论文揭示了以下主要趋势和发现：
*   <strong>骨干网络效应：</strong> 基于CLIP编码器的方法（自2023年起）在准确性上显著优于早期基于ResNet编码器的方法，这表明视觉-语言预训练作为标准骨干网络的优势。
*   <strong>解耦策略的有效性：</strong> 视觉解耦方法在闭世界设置中表现出最强的性能，其中基于原型的CLUSPRO方法在多个数据集上取得了最高准确率。
*   <strong>跨模态方法的潜力：</strong> 跨模态解耦方法虽然出现较晚，但在闭世界设置中已能与最佳视觉模型匹敌，显示出巨大的潜力。然而，在开放世界设置中，它们仍落后于顶级的视觉解耦方法。
*   <strong>开放世界挑战：</strong> 在开放世界设置中，模型需要处理所有可能的属性-对象组合（包括不可行的组合），这导致性能显著下降，表明模型在识别和可行性处理方面的挑战。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>无显式解耦方法：</strong> 无法捕捉属性和对象的独特语义或其情境变异性，难以泛化到新组合。
*   <strong>文本解耦方法：</strong> 仅依赖语言空间不足以捕捉属性丰富的视觉变异性，容易忽略图像中实际存在的纠缠。
*   <strong>视觉解耦方法：</strong> 强制严格分离可能过度简化自然依赖关系，丢弃有助于识别的情境线索；实现干净分离在实践中很困难，特别是对于微妙或强情境依赖的属性；其有效性高度依赖于监督质量和训练数据的多样性。
*   <strong>跨模态解耦方法：</strong> 存在架构和计算复杂性高的问题；一致的跨模态接地仍然难以实现，因为视觉基元在外观上差异很大，而文本基元相对稳定。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>建模基元和情境性：</strong> 进一步完善视觉解耦策略，并更加重视开发跨模态框架，以可扩展和鲁棒的方式捕捉情境性。
*   <strong>扩展到开放世界评估：</strong> 开发内在鲁棒于不可行组合的模型，缩小闭世界和开放世界性能之间的差距，而无需显式可行性计算。
*   <strong>泛化到未见基元：</strong> 设计能够动态适应未见对象的属性表示的模型，并探索利用大型语言模型的语义扩展能力进行跨模态解耦。
*   <strong>利用大型多模态模型（LMMs）：</strong> 探索将LMMs与跨模态模型结合，以实现更强的组合泛化，同时建立严格的评估协议来解决数据污染问题，并开发适应策略以学习真正的组合结构而非表面相关性。</p>
<p>总而言之，这篇综述为CZSL领域提供了宝贵的路线图，强调了现有方法的优势和局限性，并为未来的研究指明了方向，以构建更具可扩展性、鲁棒性和透明度的组合推理系统。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11106v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11106v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11063v1'></a></p>
<h2 id="lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation"><a href="https://arxiv.org/abs/2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></h2>
<p><strong>Authors:</strong> Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard <script type="math/tex">{J}</script>, <script type="math/tex">F</script>, and <script type="math/tex">{J\&F}</script> metrics for
VOS and RVOS, while MOSEv2 adopts <script type="math/tex">{J\&\dot{F}}</script> as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation”的摘要，由Chang Liu等人撰写。</p>
<p><strong>论文摘要：LSVOS 2025 挑战赛报告：复杂视频目标分割的最新进展</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该报告旨在概述第七届大规模视频目标分割（LSVOS）挑战赛，该挑战赛致力于解决在非受限、真实世界视频场景中视频目标分割（VOS）的鲁棒性问题。传统的VOS和参照VOS（RVOS）任务在处理复杂场景时仍面临挑战，尤其是在密集小物体、频繁出现/消失事件、严重遮挡、恶劣天气和光照等情况下，现有方法难以保持长期一致性和泛化能力。因此，挑战赛引入了新的MOSEv2赛道，旨在进一步推动VOS研究超越现有基准的局限性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>引入MOSEv2赛道：</strong> LSVOS 2025挑战赛引入了全新的“复杂VOS (MOSEv2)”赛道。MOSEv2数据集在MOSEv1的基础上显著增加了难度，包含更多挑战性且真实的场景，如密集小物体、频繁出现/消失事件、严重遮挡、恶劣天气和光照等，旨在推动长期一致性和泛化能力。
*   <strong>MOSEv2新评估指标：</strong> MOSEv2采用了新的评估指标<script type="math/tex">J\&\dot{F}</script>（区域相似度J和自适应边界精度F的平均值）作为主要排名指标，以更好地评估跨尺度和出现/消失情况下的物体。
*   <strong>强调LLM/MLLM组件和记忆感知传播：</strong> 报告总结了顶级解决方案中新兴的趋势，包括大型语言模型（LLM）/多模态大型语言模型（MLLM）组件日益增长的作用，以及记忆感知传播技术，这些技术对于处理复杂视频场景中的鲁棒性和语言感知分割至关重要。
*   <strong>顶级解决方案的方法论亮点：</strong>
    *   <strong>MOSEv2赛道：</strong> 第一名团队（DSS-Track）采用了基于SAM-2的SeC框架，通过增强概念建模、更大的记忆尺寸（N=22）和概念感知记忆（Concept-aware Memory）来捕获长期跨帧关系和处理场景变化。第二名团队（IXC-Seg）也采用了SeC框架，强调其通过场景自适应激活策略构建目标概念和整合LVLM推理能力。第三名团队（hyu_cvlab）则在Cutie基础上融合SAM2图像编码器以丰富语义特征，并引入运动预测模块（MPM）来估计遮挡下物体位置，以提高时间一致性。
    *   <strong>VOS赛道：</strong> 第一名团队（NJUST-KMG）对SAM2模型进行微调，并采用置信度引导的多模型集成策略，结合像素级检查和投票机制来解决不一致预测。第二名团队（Transsion）基于SAM2框架，通过伪标签增强的域适应和SeC模型的级联推理来提升性能。第三名团队（TS_Video）利用SeC框架，通过概念引导（LVLM）、场景自适应激活和干扰感知记忆策略来增强鲁棒性。
    *   <strong>RVOS赛道：</strong> 第一名团队（SaSaSa2VA）在Sa2VA基础上设计了分割增强策略，包括关键帧压缩（KFC）和缩放[SEG] tokens，以平衡时空效率和MLLM的全局视频理解能力。第二名团队（Transsion）提出了一个包含视频语义匹配（VLC）、关键帧采样器（KFS）和Sa2VA分割模块的框架，以确保文本与视频内容的匹配，并选择信息丰富的关键帧。第三名团队（dytino）的Sa2VA-i模型通过确保推理过程与训练过程一致，并采用均匀帧采样来提高性能。</p>
<p><strong>3. 主要结果和意义：</strong>
*   <strong>MOSEv2的挑战性：</strong> MOSEv2赛道的结果表明，现代VOS方法仍有很大的提升空间，领先方法的得分显著下降，证明了MOSEv2在复杂真实场景下的难度。
*   <strong>LLM/MLLM的崛起：</strong> 挑战赛的顶级解决方案突出显示了LLM/MLLM组件在语言引导视频任务中的有效性，表明它们在视频理解方面具有巨大潜力。
*   <strong>记忆机制的重要性：</strong> 记忆感知传播和长期记忆管理在处理复杂时空场景、物体出现/消失和遮挡方面发挥了关键作用，有助于提高模型的鲁棒性和一致性。
*   <strong>多模型集成和自适应策略：</strong> 许多顶级解决方案采用了多模型集成、伪标签、场景自适应激活和置信度引导的融合策略，以利用不同模型的互补优势，提高整体性能。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>MOSEv2的挑战性：</strong> 尽管取得了进展，但MOSEv2数据集的引入揭示了当前最先进的VOS系统在复杂、真实场景中仍面临显著挑战，尤其是在处理密集小物体、频繁出现/消失、严重遮挡和恶劣天气等情况时。
*   <strong>Sa2VA的局限性（RVOS赛道）：</strong> 原始Sa2VA模型在训练时每视频仅采样五帧，且仅使用一个[SEG] token来传递信息，这限制了MLLM捕获全局视频上下文的能力，并难以适应时间变化。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更深层次的LLM/MLLM集成：</strong> 报告预测，LLM/MLLM的更深层次集成将继续提升性能，尤其是在语言感知视频分割方面。
*   <strong>解决MOSEv2中的失败模式：</strong> 未来的研究应关注MOSEv2挑战赛中识别出的最困难的失败模式和真实世界用例，以进一步推动视频目标分割及相关研究的前沿。
*   <strong>增强长期一致性和泛化能力：</strong> 针对MOSEv2中提出的挑战，需要开发新的算法设计，以提高模型在非受限、真实世界场景中的长期一致性和泛化能力。
*   <strong>改进跨尺度和出现/消失情况下的评估：</strong> MOSEv2引入的<script type="math/tex">J\&\dot{F}</script>指标为未来的研究提供了更精细的评估标准，鼓励开发能够更好地处理这些复杂情况的方法。</p>
<p>总而言之，LSVOS 2025挑战赛报告不仅概述了视频目标分割领域的最新进展，还通过引入MOSEv2数据集和强调LLM/MLLM的作用，为未来的研究指明了方向，旨在实现更具鲁棒性、语言感知且能在野外场景中有效工作的视频分割系统。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11063v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11063v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12801v1'></a></p>
<h2 id="deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search"><a href="https://arxiv.org/abs/2510.12801v1">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a></h2>
<p><strong>Authors:</strong> Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV, cs.IR</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) in real-world applications require
access to external knowledge sources and must remain responsive to the dynamic
and ever-changing real-world information in order to address
information-seeking and knowledge-intensive user queries. Existing approaches,
such as retrieval augmented generation (RAG) methods, search agents, and search
equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and
poorly constructed search queries, which result in inefficiencies and
suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.
Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops
of the input image making the image search more effective, and can iteratively
adapt text search queries based on retrieved information, thereby enabling
self-reflection and self-correction. Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization. For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools. This
dataset contains diverse, multi-hop queries that integrate textual and visual
information, teaching the model when to search, what to search for, which
search tool to use and how to reason over the retrieved information. We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach. Finally, we analyze the results
and provide insights that are valuable for advancing multimodal web-search.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Kartik Narayan等人撰写的论文“DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search”的全面摘要：</p>
<p><strong>1. 论文旨在解决的主要问题或研究问题</strong></p>
<p>当前的多模态大型语言模型（MLLMs）在处理需要外部知识和实时信息的复杂、知识密集型用户查询时面临挑战。现有的方法，如检索增强生成（RAG）和搜索代理，通常存在管道僵化、搜索调用过多以及搜索查询构建不佳等问题，导致效率低下和结果不理想。论文旨在开发一种能够进行按需、多轮网络搜索并动态生成图像和文本搜索查询的MLLM，以克服这些限制。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<ul>
<li><strong>DeepMMSearch-R1模型：</strong> 论文提出了DeepMMSearch-R1，这是第一个能够执行按需、多轮网络搜索并动态生成图像和文本搜索查询的多模态LLM。它能够基于输入图像的相关裁剪区域启动网络搜索，从而提高图像搜索的效率，并能根据检索到的信息迭代调整文本搜索查询，实现自我反思和自我纠正。</li>
<li><strong>两阶段训练流程：</strong> DeepMMSearch-R1的训练采用两阶段管道：首先是冷启动监督微调（SFT）阶段，然后是在线强化学习（RL）优化阶段。</li>
<li><strong>DeepMMSearchVQA数据集：</strong> 为了训练模型，论文引入了一个新颖的多模态VQA数据集DeepMMSearchVQA。该数据集通过自动化管道创建，融合了来自网络搜索工具的真实世界信息，包含多样化、多跳的查询，整合了文本和视觉信息，旨在教会模型何时搜索、搜索什么、使用哪个搜索工具以及如何推理检索到的信息。</li>
<li><strong>多模态搜索工具集成：</strong> DeepMMSearch-R1集成了三个工具：文本搜索工具（用于检索网页和获取最新事实知识）、基于Grounding DINO的图像裁剪工具（用于识别和裁剪图像中最相关的视觉实体）以及图像搜索工具（用于检索视觉相似的图像和上下文信息）。</li>
<li><strong>自我反思和自我纠正：</strong> 模型能够根据检索到的信息迭代地调整文本搜索查询，从而实现自我反思和自我纠正，以更好地应对嘈杂的真实世界网络信息。</li>
</ul>
<p><strong>3. 主要结果及其重要性</strong></p>
<ul>
<li><strong>卓越的性能：</strong> DeepMMSearch-R1在多项知识密集型基准测试中表现出优越性，超越了现有基线，包括GPT-4o，并与GPT-03模型具有竞争力。</li>
<li><strong>裁剪图像搜索和自我反思的有效性：</strong> 实验证明，裁剪图像搜索和自我反思/自我纠正能力显著提升了性能。裁剪图像搜索平均提高了+1.75的性能，有效缓解了背景噪声，提高了搜索效率。</li>
<li><strong>SFT数据平衡的重要性：</strong> 实验表明，SFT数据中搜索所需和搜索无关示例的50:50平衡配置，以及知识分类中均匀采样的示例，能够提供最佳的平均性能。</li>
<li><strong>RL对工具使用的优化：</strong> 强化学习阶段进一步优化了模型的工具选择行为，减少了不必要的调用，使模型在图像搜索和多轮文本搜索方面更加高效和有针对性。</li>
</ul>
<p><strong>4. 论文中提到的局限性</strong></p>
<ul>
<li><strong>现有方法的局限性：</strong> 现有的RAG方法和搜索代理通常存在管道僵化、搜索调用过多和查询构建不佳的问题。</li>
<li><strong>静态训练语料库的局限性：</strong> 现有MLLMs的训练依赖于静态语料库，导致知识过时，难以应对不断变化的实时信息和长尾知识分布。</li>
<li><strong>现有搜索工具的局限性：</strong> 现有搜索工具通常局限于文本搜索，无法进行图像搜索，且在多模态知识密集型问答中的适用性有限。此外，图像搜索工具通常缺乏问题特定的指导或焦点，容易受到背景噪声和无关视觉实体的干扰。</li>
<li><strong>MMSearch-R1的局限性：</strong> 之前的MMSearch-R1模型虽然支持多模态检索，但受限于每个工具只能调用一次，且图像搜索工具无法进行问题特定的裁剪。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>扩展工具多样性：</strong> 未来的工作可以探索扩展工具的多样性，以进一步增强MLLMs的功能。</li>
<li><strong>长上下文推理：</strong> 进一步研究长上下文推理能力，以处理更复杂的查询和信息。</li>
<li><strong>多语言和多模态领域的扩展：</strong> 将训练扩展到更广泛的多语言和多模态领域，以提高模型的普适性。</li>
</ul>
<p>总而言之，DeepMMSearch-R1通过引入按需、多轮网络搜索、动态查询生成和裁剪图像搜索工具，显著提升了MLLMs在知识密集型和信息寻求型视觉问答任务中的能力。其两阶段训练流程和新颖的DeepMMSearchVQA数据集为多模态网络搜索的未来发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.</li>
<li>Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization.</li>
<li>For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools.</li>
<li>We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12801v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12801v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12793v1'></a></p>
<h2 id="vico-a-training-strategy-towards-semantic-aware-dynamic-high-resolution"><a href="https://arxiv.org/abs/2510.12793v1">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</a></h2>
<p><strong>Authors:</strong> Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing Multimodal Large Language Models (MLLMs) suffer from increased
inference costs due to the additional vision tokens introduced by image inputs.
In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens. The key idea behind our
method is to employ multiple MLP connectors, each with a different image
compression ratio, to downsample the vision tokens based on the semantic
complexity of the image. During training, we minimize the KL divergence between
the responses conditioned on different MLP connectors. At inference time, we
introduce an image router, termed Visual Resolution Router (ViR), that
automatically selects the appropriate compression rate for each image patch.
Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.
Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model's perception, reasoning,
and OCR capabilities. We hope this work will contribute to the development of
more efficient MLLMs. The code and models will be released to facilitate future
research.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Long Cui等人撰写的论文“ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
现有的多模态大型语言模型（MLLMs）在处理图像输入时，由于引入了额外的视觉tokens，导致推理成本显著增加。传统的动态高分辨率策略通常根据图像分辨率调整视觉tokens的数量，但未能充分考虑图像内容的语义复杂性，导致在处理语义信息不均的图像时效率低下或性能下降。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
为了解决上述问题，本文提出了<strong>视觉一致性学习（Visual Consistency Learning, ViCO）</strong>，这是一种新颖的训练算法，使模型能够根据图像的语义复杂性，使用不同数量的视觉tokens来表示图像。其核心思想和贡献包括：</p>
<ul>
<li><strong>多重MLP连接器与语义复杂性下采样：</strong> ViCO采用多个MLP连接器，每个连接器对应不同的图像压缩比。模型根据图像的语义复杂性，通过这些连接器对视觉tokens进行下采样。</li>
<li><strong>一致性训练（Consistency Training）：</strong> 在训练阶段，模型通过最小化在不同MLP连接器（即不同压缩率）下响应的KL散度来确保一致性。这使得模型即使在使用高度压缩的视觉tokens时也能生成准确的响应，从而提高了在压缩视觉tokens下的性能和鲁棒性。</li>
<li><strong>视觉分辨率路由器（Visual Resolution Router, ViR）：</strong> 在推理阶段，引入了一个名为ViR的图像路由器。ViR能够自动为每个图像块选择合适的压缩率。与现有基于图像分辨率的动态策略不同，ViR根据图像块的语义复杂性动态调整视觉tokens的数量，从而实现更细粒度的控制。</li>
<li><strong>损失比率（Loss Ratio）作为指导：</strong> ViR的训练通过计算每个图像块的“损失比率”来生成监督信号，该比率量化了压缩对模型输出性能的影响。这使得路由器能够识别哪些图像块可以安全地进行高压缩，哪些需要保留高分辨率以保持关键语义信息。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
实验结果表明，ViCO方法在保持模型感知、推理和OCR能力的同时，能够将视觉tokens的数量减少高达50%。具体来说：</p>
<ul>
<li><strong>性能保持与吞吐量提升：</strong> 在InternVL3.5系列模型（从4B到241B-MoE）上，ViCO模型在各种通用基准测试中平均保留了超过99.6%的原始性能，同时将首个token的吞吐量提高了约1.8倍。</li>
<li><strong>OCR相关任务的鲁棒性：</strong> 在OCRBench、ChartQA和TextVQA等OCR相关基准测试中，ViCO模型在实现高压缩率（例如OCRBench上71%的压缩率）的同时，性能几乎与原始模型保持一致，这表明其自适应路由策略的鲁棒性。</li>
<li><strong>多图像和视频理解：</strong> 在多图像和视频基准测试中，ViCO也实现了显著的tokens压缩，同时保持了高性能，证明了其在处理长视觉序列和需要跨多图像/视频帧理解的任务中的有效性。</li>
<li><strong>优于现有方法：</strong> 与FastV和SparseVLM等现有tokens削减方法相比，ViCO在相似压缩率下表现出更好的性能，尤其是在视觉敏感任务上，因为它能根据语义重要性自适应地调整压缩。</li>
<li><strong>消融研究验证：</strong> 消融实验验证了ViCO中一致性训练和ViR路由器的有效性，特别是patch级别的路由比图像级别的路由能提供更平衡和语义信息更丰富的tokens压缩。</li>
</ul>
<p>这些结果表明，ViCO为开发更高效的MLLMs做出了重要贡献，通过语义感知的方式显著降低了推理成本，同时保持了强大的性能。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中没有明确提及ViCO方法的具体局限性。然而，从其设计和实验设置中可以推断出一些潜在的考虑：</p>
<ul>
<li><strong>计算开销：</strong> 尽管ViCO旨在降低推理成本，但在训练阶段，一致性训练和ViR的训练可能引入额外的计算开销，尤其是在大规模模型和数据集上。</li>
<li><strong>路由器的准确性：</strong> ViR的性能依赖于其准确识别图像块语义复杂性的能力。如果路由器未能准确判断，可能会导致关键信息被过度压缩或不必要地保留冗余信息。</li>
<li><strong>压缩率的粒度：</strong> 目前ViCO采用的是预定义的压缩率（例如256 tokens到64 tokens）。更细粒度或连续的压缩率选择可能会进一步优化性能，但也会增加模型的复杂性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
论文中指出了以下未来研究方向：</p>
<ul>
<li><strong>更高效的MLLMs开发：</strong> 本文希望其工作能促进更高效MLLMs的开发。</li>
<li><strong>自适应视觉表示：</strong> ViCO通过基于视觉语义做出决策，使模型能够有效地将计算集中在图像中最具信息量的区域，这为未来研究自适应视觉表示提供了见解。</li>
<li><strong>代码和模型发布：</strong> 论文承诺将发布代码和模型，以促进未来的研究和复现。</li>
</ul>
<p>总而言之，ViCO通过引入语义感知的动态视觉tokens压缩策略，为解决MLLMs的推理成本问题提供了一个创新且高效的解决方案，为未来更高效、更智能的多模态AI模型奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens.</li>
<li>Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.</li>
<li>Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model's perception, reasoning,
and OCR capabilities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12793v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12793v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12777v1'></a></p>
<h2 id="what-if-understanding-motion-through-sparse-interactions"><a href="https://arxiv.org/abs/2510.12777v1">What If : Understanding Motion Through Sparse Interactions</a></h2>
<p><strong>Authors:</strong> Stefan Andreas Baumann, Nick Stracke, Timy Phan, Björn Ommer</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Understanding the dynamics of a physical scene involves reasoning about the
diverse ways it can potentially change, especially as a result of local
interactions. We present the Flow Poke Transformer (FPT), a novel framework for
directly predicting the distribution of local motion, conditioned on sparse
interactions termed "pokes". Unlike traditional methods that typically only
enable dense sampling of a single realization of scene dynamics, FPT provides
an interpretable directly accessible representation of multi-modal scene
motion, its dependency on physical interactions and the inherent uncertainties
of scene dynamics. We also evaluate our model on several downstream tasks to
enable comparisons with prior methods and highlight the flexibility of our
approach. On dense face motion generation, our generic pre-trained model
surpasses specialized baselines. FPT can be fine-tuned in strongly
out-of-distribution tasks such as synthetic datasets to enable significant
improvements over in-domain methods in articulated object motion estimation.
Additionally, predicting explicit motion distributions directly enables our
method to achieve competitive performance on tasks like moving part
segmentation from pokes which further demonstrates the versatility of our FPT.
Code and models are publicly available at
https://compvis.github.io/flow-poke-transformer.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：What If : Understanding Motion Through Sparse Interactions</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于提出了 <strong>Flow Poke Transformer (FPT)</strong>，一个新颖的框架，能够直接预测局部运动的分布，并以稀疏的“戳动”（pokes）作为条件。与传统方法不同，FPT 提供了一种可解释的、直接可访问的多模态场景运动表示，揭示了运动对物理交互的依赖性以及场景动态固有的不确定性。该模型在多个下游任务中展现出卓越的性能和泛化能力。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>FPT 的关键创新在于其能够：</p>
<ul>
<li><strong>直接预测运动分布 (Directly Predicting Motion Distribution):</strong> 大多数现有方法倾向于生成单一的、密集的运动实现。FPT 则专注于预测运动的<strong>分布</strong>，这使得它能够捕捉场景动态的多模态性和内在不确定性，从而理解“如果……会怎样”的多种可能性。</li>
<li><strong>稀疏交互作为条件 (Conditioned on Sparse Interactions - "Pokes"):</strong> 通过将稀疏的“戳动”作为输入条件，FPT 模拟了人类对物理场景的直观理解方式，即通过局部、有目的的交互来推断整体动态。这比依赖密集、全局的输入更高效、更具解释性。</li>
<li><strong>Flow Poke Transformer 架构 (FPT Architecture):</strong> 尽管摘要没有详细说明架构细节，但“Transformer”的命名暗示了其可能利用了自注意力机制来捕捉长距离依赖和复杂交互，并结合了“Flow”的概念，可能与光流或运动场预测相关。这种组合旨在有效地从稀疏输入中学习复杂的运动模式。</li>
<li><strong>可解释的多模态表示 (Interpretable Multi-modal Representation):</strong> 能够直接访问运动分布，意味着模型不仅能给出“会发生什么”，还能给出“可能发生什么”以及“为什么会发生”，这大大增强了模型的可解释性。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动物理场景理解 (Advancing Physical Scene Understanding):</strong> FPT 能够理解场景动态的多模态性和不确定性，这对于构建更智能、更具鲁士性的机器人和AI系统至关重要，这些系统需要在真实世界中进行规划和交互。</li>
<li><strong>提升交互式AI系统 (Enhancing Interactive AI Systems):</strong> 能够根据稀疏交互预测运动分布，将极大地促进人机交互、虚拟现实/增强现实中的物理模拟以及机器人操作等领域的发展。</li>
<li><strong>泛化能力和少样本学习 (Generalization and Few-shot Learning):</strong> 摘要中提到FPT可以在“强分布外任务”上进行微调，并超越“域内方法”，这表明其具有强大的泛化能力和迁移学习潜力，对于数据稀缺或新颖场景的应用具有重要意义。</li>
<li><strong>新的评估范式 (New Evaluation Paradigms):</strong> 预测运动分布而非单一实现，可能需要新的评估指标和方法来衡量模型对不确定性和多模态性的捕捉能力。</li>
</ul>
<p><strong>4. 相关领域或应用</strong></p>
<ul>
<li><strong>机器人学和操作 (Robotics and Manipulation):</strong> 机器人需要理解物体如何响应推、拉等操作，以便进行有效的抓取、放置和工具使用。FPT 可以帮助机器人预测操作的多种可能结果。</li>
<li><strong>物理模拟和游戏开发 (Physics Simulation and Game Development):</strong> 生成更真实、更具交互性的物理模拟，尤其是在处理复杂、非刚体或可变形物体时。</li>
<li><strong>虚拟现实/增强现实 (VR/AR):</strong> 增强虚拟环境中物体的交互真实感，例如，当用户触摸或推动虚拟物体时，预测其可能的响应。</li>
<li><strong>视频预测和生成 (Video Prediction and Generation):</strong> 预测未来帧中物体的运动，尤其是在存在不确定性或多种可能发展路径的情况下。</li>
<li><strong>异常检测 (Anomaly Detection):</strong> 识别与预测运动分布显著偏离的运动模式，可能指示异常事件。</li>
<li><strong>可变形物体操作 (Deformable Object Manipulation):</strong> 理解布料、绳索、面部等可变形物体在局部交互下的复杂运动。</li>
<li><strong>医学影像分析 (Medical Image Analysis):</strong> 预测组织或器官在外部刺激（如手术器械接触）下的形变。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>“Pokes”的定义和获取 (Definition and Acquisition of "Pokes"):</strong> 摘要中没有详细说明“pokes”是如何定义的，以及在实际应用中如何获取这些稀疏交互信息。这可能是一个挑战，尤其是在没有直接传感器输入的情况下。</li>
<li><strong>计算成本 (Computational Cost):</strong> Transformer 模型通常计算成本较高，尤其是在处理高分辨率或长序列数据时。预测整个运动分布而非单一实现，也可能增加计算复杂性。</li>
<li><strong>可解释性的深度 (Depth of Interpretability):</strong> 尽管摘要声称提供了“可解释的”表示，但这种解释性具体体现在何种程度上，以及是否能提供因果层面的理解，还需要进一步的论文细节来判断。</li>
<li><strong>“稀疏交互”的限制 (Limitations of "Sparse Interactions"):</strong> 某些场景可能需要更密集的输入才能准确预测运动，或者稀疏交互可能无法捕捉所有关键的物理信息。模型在处理这些情况时的表现如何，尚不清楚。</li>
<li><strong>物理定律的显式编码 (Explicit Encoding of Physics Laws):</strong> 摘要没有提及模型是否显式地编码了物理定律，或者是否完全通过数据驱动的方式学习。纯数据驱动的模型在面对极端或未见过的物理条件时，可能表现出局限性。</li>
<li><strong>“下游任务”的范围 (Scope of Downstream Tasks):</strong> 摘要提到了“密集人脸运动生成”、“铰接物体运动估计”和“移动部件分割”，这些任务虽然多样，但仍属于特定范畴。FPT 在更广泛、更复杂的物理推理任务（例如，多物体碰撞、流体动力学）上的表现如何，仍需验证。</li>
</ul>
<hr />
<p>总而言之，这篇论文提出了一种非常有前景的方法，通过关注运动分布和稀疏交互，为物理场景理解和交互式AI系统开辟了新的道路。其在泛化能力和多模态理解方面的优势，使其成为计算机视觉和机器学习领域值得关注的重要工作。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present the Flow Poke Transformer (FPT), a novel framework for
directly predicting the distribution of local motion, conditioned on sparse
interactions termed "pokes".</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12777v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12777v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12747v1'></a></p>
<h2 id="flashvsr-towards-real-time-diffusion-based-streaming-video-super-resolution"><a href="https://arxiv.org/abs/2510.12747v1">FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</a></h2>
<p><strong>Authors:</strong> Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have recently advanced video restoration, but applying them
to real-world video super-resolution (VSR) remains challenging due to high
latency, prohibitive computation, and poor generalization to ultra-high
resolutions. Our goal in this work is to make diffusion-based VSR practical by
achieving efficiency, scalability, and real-time performance. To this end, we
propose FlashVSR, the first diffusion-based one-step streaming framework
towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408
videos on a single A100 GPU by combining three complementary innovations: (i) a
train-friendly three-stage distillation pipeline that enables streaming
super-resolution, (ii) locality-constrained sparse attention that cuts
redundant computation while bridging the train-test resolution gap, and (iii) a
tiny conditional decoder that accelerates reconstruction without sacrificing
quality. To support large-scale training, we also construct VSR-120K, a new
dataset with 120k videos and 180k images. Extensive experiments show that
FlashVSR scales reliably to ultra-high resolutions and achieves
state-of-the-art performance with up to 12x speedup over prior one-step
diffusion VSR models. We will release the code, pretrained models, and dataset
to foster future research in efficient diffusion-based VSR.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</strong></p>
<p><strong>1. 论文主要贡献的简明总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于提出了 FlashVSR，这是首个实现实时性能的扩散模型驱动的单步流式视频超分辨率（VSR）框架。它通过解决现有扩散模型在 VSR 应用中面临的高延迟、计算成本和高分辨率泛化能力差等挑战，使得扩散模型在实际 VSR 场景中变得可行。FlashVSR 在单张 A100 GPU 上能以约 17 FPS 的速度处理 768x1408 视频，并在性能上超越了现有单步扩散 VSR 模型，同时实现了高达 12 倍的加速。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>FlashVSR 的关键创新在于其结合了三个互补的技术点，以实现效率、可扩展性和实时性能：</p>
<ul>
<li><strong>(i) 训练友好的三阶段蒸馏管线 (Train-friendly three-stage distillation pipeline)：</strong> 这使得模型能够进行流式超分辨率处理，是实现实时性能的基础。蒸馏通常用于将大型、复杂的模型（如原始扩散模型）的知识转移到更小、更快的模型中。</li>
<li><strong>(ii) 局部性约束的稀疏注意力 (Locality-constrained sparse attention)：</strong> 这种机制旨在削减冗余计算，同时弥合训练和测试分辨率之间的差距。稀疏注意力通过只关注输入中的关键部分来减少计算量，而局部性约束则可能利用视频帧之间的时空局部性。</li>
<li><strong>(iii) 微型条件解码器 (Tiny conditional decoder)：</strong> 这个小型解码器在不牺牲重建质量的前提下加速了重建过程。这表明它可能是一个轻量级的模块，负责将扩散模型的输出转换为最终的高分辨率视频帧。</li>
</ul>
<p>此外，为了支持大规模训练，作者还构建了一个新的数据集 <strong>VSR-120K</strong>，包含 120k 视频和 180k 图像，这本身也是一个重要的贡献，将促进该领域未来的研究。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>FlashVSR 对计算机视觉领域，特别是视频处理和生成领域，具有显著的潜在影响：</p>
<ul>
<li><strong>推动扩散模型在实时应用中的落地：</strong> 解决了扩散模型在 VSR 领域长期存在的效率瓶颈，使其从理论研究走向实际应用，例如直播、视频会议、游戏等。</li>
<li><strong>设定新的性能基准：</strong> 在实时性和高分辨率 VSR 方面，FlashVSR 实现了 SOTA 性能和显著的加速，为后续研究树立了新的目标。</li>
<li><strong>促进高效扩散模型架构研究：</strong> 其提出的蒸馏管线、稀疏注意力机制和微型解码器等创新，将启发研究者探索更高效的扩散模型设计。</li>
<li><strong>提供大规模数据集：</strong> VSR-120K 数据集的发布将为 VSR 领域的训练和评估提供宝贵的资源，加速该领域的发展。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>实时视频流媒体服务：</strong> 提高低带宽下视频的观看体验，例如在线直播、云游戏。</li>
<li><strong>视频会议/远程协作：</strong> 改善视频通话质量，尤其是在网络条件不佳时。</li>
<li><strong>安防监控：</strong> 提升监控视频的细节，帮助识别目标。</li>
<li><strong>医疗影像：</strong> 对低分辨率医疗视频进行增强，辅助诊断。</li>
<li><strong>虚拟现实/增强现实 (VR/AR)：</strong> 实时生成更高质量的视觉内容，提升沉浸感。</li>
<li><strong>内容创作/后期制作：</strong> 加速视频素材的超分辨率处理，提高工作效率。</li>
<li><strong>边缘计算设备上的视频处理：</strong> 随着模型效率的提升，未来可能在更低功耗的设备上实现高质量 VSR。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>硬件依赖性：</strong> 摘要中提到在“单个 A100 GPU”上运行，虽然 A100 是高性能 GPU，但对于更广泛的消费级硬件或边缘设备，其性能表现仍需进一步验证。实时性能可能仍然受限于高端硬件。</li>
<li><strong>特定分辨率的性能：</strong> 17 FPS 是针对 768x1408 视频而言的。虽然论文声称“可靠地扩展到超高分辨率”，但具体在更高分辨率下的 FPS 表现和质量权衡仍不明确。</li>
<li><strong>“单步”的含义：</strong> 摘要强调“one-step streaming framework”和“prior one-step diffusion VSR models”。这可能意味着它与多步（迭代）扩散模型相比，在生成质量上可能存在一定的权衡，尽管摘要声称“不牺牲质量”。“单步”通常意味着更快的推理速度，但有时会以生成质量的微小下降为代价。</li>
<li><strong>泛化能力：</strong> 尽管摘要提到“弥合训练-测试分辨率差距”，但对于训练数据中未见的极端场景、不同压缩伪影或特定内容（如动画、CG）的泛化能力，摘要中未详细说明。</li>
<li><strong>模型复杂性与部署：</strong> 尽管通过蒸馏和稀疏注意力进行了优化，但扩散模型本身通常仍比传统方法更复杂。其部署和维护成本（即使是优化后的版本）可能仍高于非扩散模型。</li>
</ul>
<hr />
<p>总的来说，FlashVSR 是一项令人兴奋的工作，它在将扩散模型推向实时视频应用方面迈出了重要一步，有望显著加速该领域的研究和实际应用。其方法论上的创新和数据集的发布都将对社区产生积极影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To support large-scale training, we also construct VSR-120K, a new
dataset with 120k videos and 180k images.</li>
<li>Extensive experiments show that
FlashVSR scales reliably to ultra-high resolutions and achieves
state-of-the-art performance with up to 12x speedup over prior one-step
diffusion VSR models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12747v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12747v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12720v1'></a></p>
<h2 id="omni-captioner-data-pipeline-models-and-benchmark-for-omni-detailed-perception"><a href="https://arxiv.org/abs/2510.12720v1">Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</a></h2>
<p><strong>Authors:</strong> Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, Eng Siong Chng, Xie Chen</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CL, cs.CV, cs.MM, cs.SD</p>
<p><strong>Abstract:</strong></p>
<p>Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对Ziyang Ma等人撰写的论文“Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception”的全面摘要：</p>
<p><strong>论文摘要：Omni-Captioner：全方位细节感知的多模态数据管道、模型与基准测试</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前全方位语言模型（OLMs）在处理音频和视频信号时，捕获和准确描述细粒度细节的能力有限的问题。具体而言，研究发现现有OLMs在细节水平和幻觉（hallucination）之间存在固有的“共同增长”现象，即描述越详细，幻觉也越多。此外，缺乏专门用于全方位细节感知的基准测试，使得评估变得不稳定、低效且不可靠。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>Omni-Detective 数据生成管道：</strong> 提出了一种代理式（agentic）数据生成管道，通过集成工具调用（如OCR、ASR、MLLM）和模态特定观察器，自主生成高度详细但幻觉最小的多模态数据。该管道通过迭代查询-观察循环，逐步收集有效且有根据的细节，并交叉验证现有声明，旨在解耦细节获取与幻觉增长。
*   <strong>Audio-Captioner 和 Omni-Captioner 模型：</strong> 基于Omni-Detective生成的数据，训练了两个字幕生成模型：用于纯音频细节感知的Audio-Captioner和用于音视频细节感知的Omni-Captioner。模型采用两阶段课程学习策略，首先冻结视觉编码器以强制音频对齐，然后联合优化两种模态以生成连贯、跨模态和详细的叙述。
*   <strong>Omni-Cloze 基准测试：</strong> 针对全方位细节感知缺乏专用基准的问题，设计了一种新颖的完形填空式（cloze-style）评估方法。Omni-Cloze通过多项选择题形式的完形填空，包含“未给出”（Not Given）选项以区分遗漏和幻觉，确保了详细音频、视觉和音视频字幕评估的稳定性、效率和可靠性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>Audio-Captioner 的卓越性能：</strong> 在级联评估协议下，Audio-Captioner在MMAU和MMAR等所有开源模型中表现最佳，超越了Gemini 2.5 Flash，并与Gemini 2.5 Pro性能相当。
*   <strong>Omni-Captioner 的领先地位：</strong> 在现有详细字幕基准测试中，Omni-Captioner在VDC上取得了新的最先进（state-of-the-art）结果，并在video-SALMONN 2测试集上实现了细节和幻觉之间的最佳权衡。
*   <strong>Omni-Detective 的有效性：</strong> 实验结果表明，Omni-Detective在生成高质量详细字幕方面非常有效，能够将细节-幻觉边界向外推移，在不按比例增加幻觉的情况下提供更丰富的描述。
*   <strong>Omni-Cloze 的优越性：</strong> Omni-Cloze在评估详细字幕方面表现出优越性，并与人类偏好高度一致，其相关系数（r=0.91）高于VDC和video-SALMONN 2。这验证了其评估设计的可靠性和稳定性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   尽管论文明确探讨了详细字幕中的幻觉问题，但其评估方法无法检测所有类型的幻觉。特别是，模型输出与输入完全无关的内容（无关生成）仍然难以可靠测量。
*   存在模型预测的细节实际上存在于音视频输入中，但却缺失于真实参考数据的情况，这使得幻觉评估变得复杂。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   开发更鲁棒的方法来处理幻觉评估中的“无关生成”问题，特别是在模型预测的细节存在于输入但缺失于参考数据的情况。
*   进一步探索和开发能够提供稳定测量并透明反映模型实际能力的新型评估协议。
*   继续推动细粒度多模态感知系统的发展，使其更加可靠和精细。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark.</li>
<li>We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data.</li>
<li>On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset.</li>
<li>Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12720v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12720v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12709v1'></a></p>
<h2 id="sail-embedding-technical-report-omni-modal-embedding-foundation-model"><a href="https://arxiv.org/abs/2510.12709v1">SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</a></h2>
<p><strong>Authors:</strong> Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.IR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the
Douyin-Selected scenario. For the Douyin feed rank model, the match features
produced by SAIL-Embedding yield a +0.08% AUC gain.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Lin Lin等人撰写的论文“SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决现有多模态嵌入模型在实际应用和商业场景中面临的挑战，包括：模态支持有限、训练机制不稳定以及工业领域差距。具体而言，研究问题是如何构建一个能够处理任意模态输入（包括文本、视觉和音频），并能有效支持多模态检索和分类任务的全模态嵌入基础模型，同时确保训练的鲁棒性、可扩展性和泛化能力，并提升在推荐系统中的实际业务价值。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
SAIL-Embedding模型通过以下关键创新和方法论贡献来解决上述问题：</p>
<ul>
<li><strong>全模态支持和统一表示：</strong> SAIL-Embedding是一个全模态嵌入基础模型，能够处理任意组合的视觉、文本和音频模态输入，并将其映射到统一的向量空间，以满足多样化的业务需求。</li>
<li><strong>动态硬负例挖掘（Dynamic Hard Negative Mining）：</strong> 引入动态硬负例挖掘策略，自适应地确定每个数据集的最佳相似度阈值，使模型能够专注于区分具有挑战性的负例，从而增强模型对领域特定知识的理解并减少误分类风险。</li>
<li><strong>自适应多源数据平衡（Adaptive Multi-Source Data Balancing）：</strong> 提出自适应加权框架，直接从数据分布中学习数据集特定的采样权重，而非依赖手动启发式配置，以平衡数据质量和分布多样性，防止过拟合并提高泛化能力。</li>
<li><strong>多阶段训练方案：</strong><ul>
<li><strong>内容感知渐进式训练（Content-Aware Progressive Training）：</strong> 逐步增强嵌入对不同任务的判别能力和处理未见场景的泛化能力，通过利用多样化、语义丰富的数据资源，使模型掌握全面的领域知识。</li>
<li><strong>协作感知推荐增强训练（Collaboration-aware Recommendation Enhancement Training）：</strong> 通过序列到物品（sequence-to-item）和ID到物品（ID-to-item）蒸馏知识，并挖掘用户历史兴趣，使多模态表示适应推荐场景。</li>
</ul>
</li>
<li><strong>随机专业化和数据集驱动的模式匹配（Stochastic Specialization and Dataset-Driven Pattern Matching）：</strong> 提出随机专业化训练策略，通过在每次迭代中随机选择单个数据集进行训练，减少梯度方差，简化迭代逻辑，并提高模型训练的灵活性和泛化能力。数据集驱动的模式匹配则统一了各种对比目标，以处理异构模态可用性和不平衡问题。</li>
<li><strong>架构设计：</strong> 利用大型语言模型（LLM）作为核心推理和集成骨干，并引入视觉感知器（Visual Perceiver）模块进行令牌缩减，以及采用CLAP模型进行音频编码，确保高效处理和多模态融合。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
实验结果表明SAIL-Embedding取得了显著的性能提升：</p>
<ul>
<li><strong>检索任务的SOTA性能：</strong> 在多项基准数据集上，SAIL-Embedding在物品到物品（i2i）和查询到物品（q2i）检索任务中均实现了最先进（SOTA）的性能，超越了CLIP-based和VLM-based模型。</li>
<li><strong>在线实验的业务价值：</strong> 在Douyin推荐系统的在线实验中，SAIL-Embedding显著提升了用户生命周期（LT），这是推荐体验的关键指标。例如，在Douyin-Selected场景中，7天LT增益为+0.158%，14天LT增益为+0.144%。对于Douyin信息流排名模型，SAIL-Embedding生成的匹配特征带来了+0.08%的AUC增益。</li>
<li><strong>模块和组件的必要性：</strong> 广泛的消融研究证实了所提出模块和训练策略的必要性，例如协作感知推荐增强训练显著提高了NMI、Kendall相关性和交集指标，并提升了模型在Gid-i2i基准上的性能。</li>
<li><strong>泛化能力和适应性：</strong> 模型在不同任务和跨领域场景中表现出强大的泛化能力和适应性，这得益于全模态架构、数据集驱动的模式匹配和随机专业化训练。</li>
</ul>
<p><strong>4. 论文中提到的局限性</strong>
论文中没有明确指出当前工作的局限性，但从未来研究方向和对SIDs的讨论中可以推断出一些潜在的改进空间：</p>
<ul>
<li><strong>SIDs与密集嵌入的信息量：</strong> 论文提到SIDs（语义ID）比密集嵌入更容易在基于规则的方法中使用，但密集嵌入通常包含更多信息。如何有效利用密集嵌入的全部信息是一个值得探索的方向。</li>
<li><strong>推荐系统中的VLM集成：</strong> 尽管模型在推荐场景中表现出色，但论文仍将进一步探索如何将视觉语言模型（VLM）更深入地集成到推荐系统中。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
论文提出了以下未来研究方向：</p>
<ul>
<li><strong>增强VLM与推荐系统的集成：</strong> 探索训练与推荐目标对齐的VLM，并构建为推荐量身定制的生成任务，以使模型在早期阶段获得领域特定知识并增强推荐能力。</li>
<li><strong>优化表示学习：</strong> 在表示学习阶段，旨在通过挖掘更多来自推荐信号和行为反馈的配对数据，更好地将模型训练与推荐目标对齐，从而将用户偏好注入多模态表示中。</li>
<li><strong>推荐场景中的硬负例挖掘：</strong> 进一步研究推荐场景中的硬负例挖掘，以提高表示学习的鲁棒性。</li>
<li><strong>扩展到更广泛的下游任务：</strong> 将SAIL-Embedding框架扩展到更广泛的下游任务，如视频理解、个性化内容生成和跨领域知识迁移。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.</li>
<li>Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability.</li>
<li>Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12709v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12709v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12603v1'></a></p>
<h2 id="reasoning-in-the-dark-interleaved-vision-text-reasoning-in-latent-space"><a href="https://arxiv.org/abs/2510.12603v1">Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</a></h2>
<p><strong>Authors:</strong> Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Chao Chen等人撰写的论文“Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space”的全面摘要。</p>
<hr />
<h3 id="reasoning-in-the-dark-interleaved-vision-text-reasoning-in-latent-space_1">论文摘要：Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前多模态大语言模型（MLLMs）在执行复杂推理任务时，通常依赖于显式的、需要大量人工标注的视觉-文本中间推理步骤。这种方法不仅劳动密集，而且引入了显著的推理延迟。论文旨在解决如何提升MLLMs的多模态推理能力，同时减少对显式标注的依赖并提高推理效率的问题。具体来说，研究问题是如何在潜在空间中实现有效的视觉-文本交错推理，以克服现有方法的局限性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了<strong>交错视觉-文本潜在推理（Interleaved Vision-Text Latent Reasoning, IVT-LR）</strong>方法，这是首个在多模态潜在空间中实现完全多模态潜在推理的框架。其主要创新点包括：
*   <strong>潜在空间中的多模态推理：</strong> IVT-LR允许文本和视觉信息在模型的潜在空间中进行推理，无需生成中间的显式文本或图像。每个推理步骤由两部分组成：来自前一步骤隐藏状态的“潜在文本”和一组动态选择的图像嵌入（“潜在视觉”）。
*   <strong>动态视觉焦点：</strong> 潜在视觉部分通过基于注意力分数选择最相关的图像嵌入，实现了对视觉特征的动态聚焦，从而在推理过程中整合关键视觉信息。
*   <strong>渐进式多阶段训练策略：</strong> 为了有效融合潜在文本和潜在视觉组件，论文引入了一种渐进式多阶段训练策略。该策略逐步用潜在推理步骤替代显式CoT（Chain-of-Thought）步骤，并将监督集中在剩余的未来显式步骤和最终答案上，以确保准确推理。这种方法减少了对中间视觉推理步骤显式标注的需求，并提高了推理效率。</p>
<p><strong>3. 主要结果及其意义：</strong>
IVT-LR在M³CoT和ScienceQA等挑战性视觉问答基准上进行了广泛实验，结果显示：
*   <strong>显著的准确性提升：</strong> IVT-LR在准确性方面平均提升了5.45%，在M³CoT和ScienceQA上均超越了所有基线方法，包括最强的Chain-of-Focus，提升幅度在5%到7.5%之间。这表明潜在空间中的跨模态交互更为有效，增强了多模态推理能力。
*   <strong>推理效率大幅提高：</strong> IVT-LR的推理速度比现有方法快5倍以上。它将自回归步骤的数量减少了至少9倍，显著降低了推理延迟，同时保持了高准确性。这得益于在潜在空间中进行推理，避免了生成冗长显式推理过程的需要。
*   <strong>潜在推理的有效性：</strong> 消融研究证实了潜在文本和潜在视觉组件的必要性，两者都对模型性能至关重要。潜在视觉的长度增加会提高准确性，表明更丰富的视觉线索有助于复杂推理。
*   <strong>动态注意力机制：</strong> 潜在推理模式下，注意力比率呈现下降趋势（从潜在视觉转向潜在文本），注意力焦点逐渐集中，这与人类解决问题的过程相似，表明模型能够有效过滤和提炼多模态信息。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>额外令牌的引入：</strong> 潜在视觉的自适应选择不可避免地引入了少量固定的额外令牌。尽管这些令牌在内部处理而非外部生成，确保了最终推理速度在准确性高的设置下仍是最佳，但这仍是一个需要考虑的因素。
*   <strong>训练复杂性：</strong> IVT-LR需要专门的多阶段训练课程，这使其比简单的基于提示的方法更为复杂。然而，论文认为这种复杂性是值得的投资，因为它带来了巨大的准确性和效率提升，且所需的训练资源和时间相对适中。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>动态潜在步骤：</strong> 未来工作可以探索更动态的视觉潜在推理方式，例如根据问题的复杂性自适应地确定最佳潜在步骤数量，而不是依赖固定的阶段数。
*   <strong>更广泛的应用：</strong> 将IVT-LR方法扩展到纯推理之外的更广泛的序列多模态任务，包括规划和动态环境中的复杂决策制定。</p>
<hr />
<p>总而言之，这篇论文为多模态推理领域引入了一种新颖且高效的范式，通过在潜在空间中交错进行视觉和文本推理，显著提升了MLLMs的性能和效率，为未来构建更智能、更具感知力的视觉-语言模型奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency.</li>
<li>To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12603v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12603v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-15 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
