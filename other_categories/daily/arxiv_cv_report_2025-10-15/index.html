<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-15 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-14/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-16/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-15">Arxiv Computer Vision Papers - 2025-10-15</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-14" class="nav-link">Arxiv è®¡ç®æºè§è§é¢åæ¯æ¥æ¥åæ§è¡æè¦ (2025-10-14)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda" class="nav-link">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a>
                </li>
                <li class="nav-item">
                    <a href="#compositional-zero-shot-learning-a-survey" class="nav-link">Compositional Zero-Shot Learning: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation" class="nav-link">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search" class="nav-link">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a>
                </li>
                <li class="nav-item">
                    <a href="#vico-a-training-strategy-towards-semantic-aware-dynamic-high-resolution" class="nav-link">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#what-if-understanding-motion-through-sparse-interactions" class="nav-link">What If : Understanding Motion Through Sparse Interactions</a>
                </li>
                <li class="nav-item">
                    <a href="#flashvsr-towards-real-time-diffusion-based-streaming-video-super-resolution" class="nav-link">FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#omni-captioner-data-pipeline-models-and-benchmark-for-omni-detailed-perception" class="nav-link">Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#sail-embedding-technical-report-omni-modal-embedding-foundation-model" class="nav-link">SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</a>
                </li>
                <li class="nav-item">
                    <a href="#reasoning-in-the-dark-interleaved-vision-text-reasoning-in-latent-space" class="nav-link">Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-15">Arxiv Computer Vision Papers - 2025-10-15</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-14">Arxiv è®¡ç®æºè§è§é¢åæ¯æ¥æ¥åæ§è¡æè¦ (2025-10-14)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»å¤©ç Arxiv è®¡ç®æºè§è§è®ºæä¸»è¦å´ç»<strong>å¤æ¨¡æå­¦ä¹ ãè§é¢çè§£ä¸å¤çãä»¥ååºç¡æ¨¡å</strong>è¿ä¸å¤§æ ¸å¿ä¸»é¢å±å¼ãæä»¬è§å¯å°ç ç©¶äººåæ­£ç§¯ææ¢ç´¢å¦ä½å°è§è§ä¸è¯­è¨æ¨¡åç»åï¼ä»¥å®ç°æ´éç¨ãæ´å¼ºå¤§çæç¥åæ¨çè½åï¼å°¤å¶æ¯å¨å¤æåºæ¯ï¼å¦åå¸çæ§ãè§é¢åå²ï¼ä¸­ãåæ¶ï¼å¯¹é«æãå®æ¶å¤ççéæ±ä¹æ¨å¨äºè§é¢è¶åè¾¨çåå¨æé«åè¾¨çè®­ç»ç­ç¥çè¿æ­¥ã</p>
<p><strong>ä¸»è¦è¶å¿ä¸äº®ç¹ï¼</strong></p>
<ol>
<li>
<p><strong>å¤æ¨¡æä¸è§è§-è¯­è¨æ¨¡å (VLM) çèåä¸åºç¨ï¼</strong> å¤ç¯è®ºæèç¦äº VLM çåå±ååºç¨ã</p>
<ul>
<li><strong>"Towards General Urban Monitoring with Vision-Language Models"</strong> æä¾äº VLM å¨åå¸çæ§é¢åçå¨é¢ç»¼è¿°ãè¯ä¼°åæªæ¥ç ç©¶è®®ç¨ï¼è¡¨æ VLM å¨å®éåºç¨ä¸­æ½åå·¨å¤§ã</li>
<li><strong>"DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search"</strong> å±ç¤ºäºå¤æ¨¡æ LLM å¨å¤æ¨¡æç½ç»æç´¢ä¸­çåºç¨ï¼é¢ç¤ºçæç´¢èå¼å¯è½åçåé©ã</li>
<li><strong>"Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space"</strong> æ¢ç´¢äºå¨æ½å¨ç©ºé´è¿è¡è§è§-ææ¬äº¤éæ¨çï¼ä¸ºæ´æ·±å±æ¬¡çå¤æ¨¡æçè§£æä¾äºæ°æè·¯ã</li>
<li><strong>"Omni-Captioner"</strong> å <strong>"SAIL-Embedding Technical Report"</strong> ååå«æå»ºäºå¨æ¹ä½è¯¦ç»æç¥çåºåãæ¨¡ååæ°æ®ç®¡éï¼ä»¥åæåºäºå¨æ¨¡æåµå¥åºç¡æ¨¡åï¼æ¨å¨ç»ä¸ä¸åæ¨¡æçè¡¨ç¤ºã</li>
</ul>
</li>
<li>
<p><strong>è§é¢çè§£ä¸å¤ççå¤ææ§ä¸æçæåï¼</strong></p>
<ul>
<li><strong>"LSVOS 2025 Challenge Report"</strong> æ­ç¤ºäºå¤æè§é¢ç®æ åå²çææ°è¿å±ï¼å¼ºè°äºè¯¥é¢åæç»­çææååæ°ã</li>
<li><strong>"FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution"</strong> è§£å³äºè§é¢è¶åè¾¨ççå®æ¶æ§é®é¢ï¼éè¿æ©æ£æ¨¡åå®ç°äºé«æçæµåªä½å¤çï¼å·æéè¦çå®éåºç¨ä»·å¼ã</li>
<li><strong>"ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution"</strong> æåºäºä¸ç§è¯­ä¹æç¥çå¨æé«åè¾¨çè®­ç»ç­ç¥ï¼æ¨å¨ä¼åè§é¢å¤ççè´¨éåæçã</li>
<li><strong>"What If : Understanding Motion Through Sparse Interactions"</strong> åä»ç¨çäº¤äºçè§åº¦æ¢ç´¢äºè¿å¨çè§£ï¼ä¸ºè§é¢ä¸­çè¡ä¸ºåææä¾äºæ°çè§è§ã</li>
</ul>
</li>
<li>
<p><strong>åºç¡æ¨¡åä¸éç¨è½åï¼</strong></p>
<ul>
<li><strong>"SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model"</strong> æåºäºä¸ç§å¨æ¨¡æåµå¥åºç¡æ¨¡åï¼æ¨å¨ä¸ºä¸åæ¨¡ææä¾ç»ä¸çè¡¨ç¤ºï¼è¿å¯è½æä¸ºæªæ¥å¤æ¨¡æAIç³»ç»çåºç³ã</li>
<li><strong>"Compositional Zero-Shot Learning: A Survey"</strong> å¯¹ç»åé¶æ ·æ¬å­¦ä¹ è¿è¡äºå¨é¢è°æ¥ï¼è¿å¯¹äºæå»ºè½å¤æ³åå°æªè§è¿æ¦å¿µçéç¨AIç³»ç»è³å³éè¦ã</li>
</ul>
</li>
</ol>
<p><strong>ç¹å«æ¾èæåæ°è®ºæï¼</strong></p>
<ul>
<li><strong>"FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution"</strong>ï¼å¨å®æ¶è§é¢å¤çæ¹é¢åå¾äºçªç ´ï¼å°æ©æ£æ¨¡ååºç¨äºæµåªä½è¶åè¾¨çï¼å·ææ¾èçå·¥ç¨ååºç¨ä»·å¼ã</li>
<li><strong>"SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model"</strong>ï¼æåºäºå¨æ¨¡æåµå¥åºç¡æ¨¡åï¼å¶ç®æ æ¯ç»ä¸æææ¨¡æçè¡¨ç¤ºï¼è¿å¯è½å¯¹æªæ¥çå¤æ¨¡æAIæ¶æäº§çæ·±è¿å½±åã</li>
<li><strong>"Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space"</strong>ï¼æ¢ç´¢äºå¨æ½å¨ç©ºé´è¿è¡æ´æ·±å±æ¬¡çè§è§-ææ¬æ¨çï¼ä¸ºåæå½åVLMçå±éæ§æä¾äºæ°çç ç©¶æ¹åã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>å¨æ¨¡æï¼Omni-modalï¼åµå¥ä¸åºç¡æ¨¡åï¼</strong> æ¨å¨æå»ºè½å¤å¤çåç»ä¸æææ¨¡æä¿¡æ¯çéç¨è¡¨ç¤ºã</li>
<li><strong>æ½å¨ç©ºé´ä¸­çå¤æ¨¡ææ¨çï¼</strong> æ¢ç´¢å¨æ½è±¡çæ½å¨ç©ºé´ä¸­è¿è¡è§è§-ææ¬äº¤éæ¨çï¼ä»¥å®ç°æ´é«çº§å«ççè§£ã</li>
<li><strong>æ©æ£æ¨¡åå¨å®æ¶è§é¢å¤çä¸­çåºç¨ï¼</strong> æ©æ£æ¨¡åå¨çæè´¨éä¸è¡¨ç°åºè²ï¼å¶å¨å®æ¶è§é¢è¶åè¾¨çç­ä»»å¡ä¸­çæçä¼åæ¯éè¦æ¹åã</li>
<li><strong>è¯­ä¹æç¥ä¸å¨æé«åè¾¨çå¤çï¼</strong> ç»åè¯­ä¹ä¿¡æ¯æ¥ä¼åè§é¢å¤ççè´¨éåæçã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå¿ç¢çç ç©¶äººåï¼æå»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼ä»¥è·åæåæ²¿åæå·å½±ååçä¿¡æ¯ï¼</p>
<ol>
<li><strong>"SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model" (Lin Lin et al.)</strong>ï¼å¦ææ¨çç ç©¶æ¶åå¤æ¨¡æåºç¡æ¨¡åæéç¨è¡¨ç¤ºï¼è¿ç¯è®ºææä¾äºæ½å¨çæªæ¥æ¹åã</li>
<li><strong>"FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution" (Junhao Zhuang et al.)</strong>ï¼å¦ææ¨å³æ³¨å®æ¶è§é¢å¤çãæ©æ£æ¨¡ååºç¨æå·¥ç¨ä¼åï¼è¿ç¯è®ºææä¾äºéè¦çææ¯çªç ´ã</li>
<li><strong>"Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda" (AndrÃ© Torneiro et al.)</strong>ï¼å¯¹äºäºè§£VLMå¨å®éåºç¨ä¸­çæ½åãææåæªæ¥ç ç©¶æ¹åï¼è¿ç¯ç»¼è¿°æ¯æä½³çèµ·ç¹ã</li>
<li><strong>"Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space" (Chao Chen et al.)</strong>ï¼å¦ææ¨å¯¹å¤æ¨¡ææ¨ççæ·±å±æºå¶åæ½å¨ç©ºé´æä½æå´è¶£ï¼è¿ç¯è®ºææä¾äºåæ°çæè·¯ã</li>
</ol>
<p>è¿äºè®ºæä»£è¡¨äºå½åè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åææ´»è·åææåæ¯çç ç©¶æ¹åï¼å¼å¾æ·±å¥æ¢è®¨ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></li>
<li><a href="#2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></li>
<li><a href="#2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></li>
<li><a href="#2510.12801v1">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a></li>
<li><a href="#2510.12793v1">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</a></li>
<li><a href="#2510.12777v1">What If : Understanding Motion Through Sparse Interactions</a></li>
<li><a href="#2510.12747v1">FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</a></li>
<li><a href="#2510.12720v1">Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</a></li>
<li><a href="#2510.12709v1">SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</a></li>
<li><a href="#2510.12603v1">Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.12400v1'></a></p>
<h2 id="towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda"><a href="https://arxiv.org/abs/2510.12400v1">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></h2>
<p><strong>Authors:</strong> AndrÃ© Torneiro, Diogo Monteiro, Paulo Novais, Pedro Rangel Henriques, Nuno F. Rodrigues</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Urban monitoring of public infrastructure (such as waste bins, road signs,
vegetation, sidewalks, and construction sites) poses significant challenges due
to the diversity of objects, environments, and contextual conditions involved.
Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.
This raises a critical question: Can machines now "see" like citizens and infer
informed opinions about the condition of urban infrastructure? Vision-Language
Models (VLMs), which integrate visual understanding with natural language
reasoning, have recently demonstrated impressive capabilities in processing
complex visual information, turning them into a promising technology to address
this challenge. This systematic review investigates the role of VLMs in urban
monitoring, with particular emphasis on zero-shot applications. Following the
PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021
and 2025 to address four core research questions: (1) What urban monitoring
tasks have been effectively addressed using VLMs? (2) Which VLM architectures
and frameworks are most commonly used and demonstrate superior performance? (3)
What datasets and resources support this emerging field? (4) How are VLM-based
applications evaluated, and what performance levels have been reported?</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯AndrÃ© Torneiroç­äººæ°åçè®ºæâTowards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agendaâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åå¸å¬å±åºç¡è®¾æ½ï¼å¦åå¾æ¡¶ãè·¯æ ãæ¤è¢«ãäººè¡éåå»ºç­å·¥å°ï¼çæµæé¢ä¸´çéå¤§ææãä¼ ç»çç©èç½ä¼ æå¨åäººå·¥æ£æ¥æ¹æ³ææ¬é«æãé¾ä»¥æ©å±ï¼ä¸å¾å¾ä¸å¸æ°çç´è§è§è§æç¥ä¸ç¬¦ãå æ­¤ï¼æ ¸å¿ç ç©¶é®é¢æ¯ï¼æºå¨è½å¦åå¸æ°ä¸æ ·âçâï¼å¹¶å¯¹åå¸åºç¡è®¾æ½çç¶åµå½¢æææ ¹æ®çå¤æ­ï¼è®ºæç¹å«å³æ³¨è§è§è¯­è¨æ¨¡åï¼VLMsï¼å¨åå¸çæµä¸­çä½ç¨ï¼å°¤å¶æ¯å¨é¶æ ·æ¬ï¼zero-shotï¼åºç¨æ¹é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
* <strong>ç³»ç»æ§ç»¼è¿°ä¸åç±»ï¼</strong> è®ºæéç¨PRISMAæ¹æ³è®ºï¼å¯¹2021å¹´è³2025å¹´é´åè¡¨ç32ç¯åè¡è¯å®¡ç ç©¶è¿è¡äºåæã
* <strong>åè½æ§åç±»æ³ï¼</strong> è®ºæå¼å¥äºä¸ä¸ªåå«ä¸ä¸ªé¢åçVLMåå¸åºç¨åè½æ§åç±»æ³ï¼åæ¬ï¼ç©ä½æ£æµä¸åå²ãåå¸è§åä¸åå°å©ç¨åç±»ãå¯¼èªä¸å¯»è·¯ãäº¤éåæä¸è¿è¾ãåå¸åºæ¯çè§£ä¸æç¥ãå°çå®ä½ä¸ä½ç½®æ¥æ¾ãåå¸çæ§ä¸å®å¨ãè¿ä¸ºç ç©¶äººåæä¾äºä¸ä¸ªç»æåçæ¡æ¶ï¼ä»¥çè§£ç°ææ¹æ³å¹¶è¯å«ç¹å®ä»»å¡çæåéææ¯ã
* <strong>æ¨¡åæ¶æä¸æ°æ®éåæï¼</strong> è®ºææ·±å¥åæäºæå¸¸ç¨ç11ç§VLMæ¶æåå¶ä¼ç¼ºç¹ï¼å¹¶èå¯äºè¯¥é¢åå¹¿æ³ä½¿ç¨çæ°æ®éãè¯ä¼°ææ ååºåæµè¯æ¹æ³ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
* <strong>VLMå¨åå¸çæµä¸­çæ½åï¼</strong> VLMséè¿æ´åè§è§çè§£åèªç¶è¯­è¨æ¨çï¼å¨å¤çå¤æè§è§ä¿¡æ¯æ¹é¢å±ç°åºå¼ºå¤§è½åï¼è¢«è®¤ä¸ºæ¯è§£å³åå¸çæµææçæåæ¯ææ¯ã
* <strong>é¶æ ·æ¬åºç¨çæææ§ï¼</strong> ç»¼è¿°è¡¨æï¼VLMså¨é¶æ ·æ¬åºç¨ä¸­è¡¨ç°åºè²ï¼ä¾å¦æ éä¸ç¨è®­ç»æ ·æ¬å³å¯æ£æµâäººè¡éä¸æº¢åºçåå¾æ¡¶âã
* <strong>æ§è½å¤æ ·æ§ï¼</strong> ä¸åä»»å¡ãæ°æ®éåæ¹æ³å¯¼è´æ§è½ç»æå·®å¼æ¾èãä¾å¦ï¼ç©ä½æ£æµä¸åå²ä»»å¡ä¸­ï¼SAMåGrounding DINOç»ååå¾äºé«IoUåæ°ï¼åå¸è§åä¸­ï¼UrbanCLIPå¨ä½å®åºF1åæ°è¾¾å°0.82ã
* <strong>æ°æ®éä½¿ç¨æ¨¡å¼ï¼</strong> è¡æ¯å¾åæ°æ®éï¼å¦Google Street ViewãMapillary Vistasï¼å å¶è§è§ä¸°å¯æ§åä¸äººç±»ä¸­å¿åå¸åºæ¯çå¯¹é½èå æ®ä¸»å¯¼å°ä½ãåææ°æ®éï¼å¦CARLAãSYNTHIAï¼å¨æ¨¡æç¨ææå±é©æ¡ä»¶æ¹é¢å¾æä»·å¼ï¼ä½å­å¨çå®ä¸çæ³åå·®è·ã
* <strong>æ¨¡åæ¶æåå¥½ï¼</strong> CLIPãGrounding DINOåGPT-3.5æ¯æå¸¸ç¨çæ¨¡åï¼åæ äºå¯¹æ¨¡ååãéç¨éª¨å¹²ç½ç»çå¼ºçåå¥½ï¼è¿äºéª¨å¹²ç½ç»æ¯æé¶æ ·æ¬æåºäºæç¤ºçå®å¶ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
* <strong>è¯ä¼°å®è·µæ»åï¼</strong> å°½ç®¡æ°æ®éå¤æ ·æ§æææ¹åï¼ä½è¯ä¼°å®è·µæªè½è·ä¸é¨ç½²éæ±ãå¾å°æç ç©¶å¯¹è·¨åå¸æå¤å¸§æ³åè¿è¡åºåæµè¯ï¼ä¹æªè½ååèèæä½éå¶ã
* <strong>æ¨¡æå·®è·ä¸ä¸ä¸æç¼ºå¤±ï¼</strong> å¤§å¤æ°åå¸VLMç®¡éè¿åº¦å³æ³¨éæå¾å-ææ¬å¯¹ï¼å¿½ç¥äºæ¶é´åºåãæ·±åº¦å¾ãå°çå®ä½çè³ç¯å¢å£°é³ç­ä¸°å¯çå¤æ¨¡æä¿¡å·ãè¿éå¶äºæ¨¡åå¨èªå¨é©¾é©¶ãäº¤éé¢æµç­å¨æä»»å¡ä¸­çæ§è½åæ¨çæ·±åº¦ã
* <strong>åº¦éç¢çåä¸è¯ä¼°ä¸è¶³ï¼</strong> ç¼ºä¹æ ååè¯ä¼°åè®®ï¼å¯¼è´æ§è½æ¥åä¸ä¸è´ï¼é¾ä»¥è¿è¡ç´æ¥æ¯è¾ãè®¸å¤ç ç©¶çç¥äºåºçº¿æ¯è¾ãç½®ä¿¡åºé´åè¯¦ç»çéè¯¯åæã
* <strong>å¯¹èµæºå¯éåæ¶æçè¿åº¦ä¾èµï¼</strong> è®¸å¤æåè¿çVLMæ¨¡åè®¡ç®ææ¬é«æï¼ä¸éåå®æ¶ãç§»å¨æåµå¥å¼é¨ç½²ï¼å¯¼è´åæ°ä¸å®éåºç¨ä¹é´å­å¨å·®è·ã
* <strong>ä¼¦çç²ç¹ä¸æ³å¾çå¿½ï¼</strong> å°½ç®¡AIæ¥çèå¥åå¸ç¯å¢ï¼ä½ä¼¦çèéï¼å¦ç®æ³å¬å¹³æ§ãç¥æåæãæ°æ®æ¥æºãéç§ä¿æ¤ï¼å¨VLMç ç©¶ä¸­ä»è¢«å¿½è§ã
* <strong>è¾¹ç¼ç¡¬ä»¶å®éªæéï¼</strong> å¾å°æç ç©¶å¨è¾¹ç¼ç¡¬ä»¶ä¸è¿è¡æ¨¡ææé¨ç½²ï¼è¿é»ç¢äºå¯¹å»¶è¿ãç­æ§è½ååèæè¡¡ççè§£ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
* <strong>SLM-VLMæ··åæ¶æï¼</strong> ç»åå°åè¯­è¨æ¨¡åï¼SLMsï¼ä¸æ¨¡ååè§è§ç¼ç å¨åè§£ç å¨ï¼ä»¥å®ç°é«æçå¤æ¨¡ææ¨çï¼éç¨äºæºè½ææºãæ äººæºåAR/VRè®¾å¤ä¸çå®æ¶ãä½å»¶è¿æ¨çã
* <strong>ç»ä¸çåå¸åºåï¼</strong> å¼åéæå¤è¯­è¨æç¤ºãå¤æ¨¡æä¼ æå¨æµï¼å¾åãè§é¢ãé³é¢ãLiDARï¼åæåå¤æ ·æ§å°çæ°æ®çè¯ä¼°å¥ä»¶ï¼ä»¥ç¡®ä¿å¯å¤ç°æ§ãè·¨é¢åå¯æ¯æ§åé²æ£æ³åã
* <strong>ä»¥é¨ç½²ä¸ºä¸­å¿çè®¾è®¡ï¼</strong> å°ç¡¬ä»¶éå¶ãå»¶è¿è¦æ±ãç­é¢ç®åéç§èéç­é¨ç½²çº¦æåµå¥æ¨¡åå¼åå¨æã
* <strong>åµå¥å¼ä¼¦çä¸åè§æ§ï¼</strong> å°æåé²æ£æ§æ£æ¥ãç®æ³å¬å¹³æ§è¯ä¼°ãæ°æ®éåæè¿½è¸ªååè§å®¡è®¡æ´åå°æ ¸å¿åºåæµè¯åè¯ä¼°çå½å¨æä¸­ã
* <strong>å¯å¤ç°çå¼æ¾çæç³»ç»ï¼</strong> å¹å»éææåï¼åæ¬çæ¬åæ°æ®éãDockerååºçº¿ãå¬å±æè¡æ¦åå±äº«è¯ä¼°ä»£ç ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæå¨é¢åé¡¾äºVLMså¨åå¸çæµä¸­çåºç¨ç°ç¶ï¼è¯å«äºå¶æ½åãç°æå±éæ§ï¼å¹¶æåºäºä¸ä¸ªå¤ç»´åº¦çç ç©¶è®®ç¨ï¼æ¨å¨æ¨å¨åå¸AIç³»ç»åæ´å¯é¨ç½²ãåå®¹ãå¯è§£éåç¬¦åä¼¦ççæ¹ååå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12400v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12400v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11106v1'></a></p>
<h2 id="compositional-zero-shot-learning-a-survey"><a href="https://arxiv.org/abs/2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></h2>
<p><strong>Authors:</strong> Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision
that enables models to recognize unseen combinations of known attributes and
objects during inference, addressing the combinatorial challenge of requiring
training data for every possible composition. This is particularly challenging
because the visual appearance of primitives is highly contextual; for example,
<code>small'' cats appear visually distinct from</code>older'' ones, and <code>wet'' cars
differ significantly from</code>wet'' cats. Effectively modeling this contextuality
and the inherent compositionality is crucial for robust compositional zero-shot
recognition. This paper presents, to our knowledge, the first comprehensive
survey specifically focused on Compositional Zero-Shot Learning. We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement. We provide a detailed comparative analysis of
these methods, highlighting their core advantages and limitations in different
problem settings, such as closed-world and open-world CZSL. Finally, we
identify the most significant open challenges and outline promising future
research directions. This survey aims to serve as a foundational resource to
guide and inspire further advancements in this fascinating and important field.
Papers studied in this survey with their official code are available on our
github: https://github.com/ans92/Compositional-Zero-Shot-Learning</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯ç±Ans Munir, Faisal Z. Qureshi, Mohsen AliåMuhammad Haris Khanæ°åçè®ºæâCompositional Zero-Shot Learning: A Surveyâå¯¹ç»åå¼é¶æ ·æ¬å­¦ä¹ ï¼CZSLï¼é¢åè¿è¡äºé¦æ¬¡å¨é¢ç»¼è¿°ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
CZSLæ¨å¨ä½¿è®¡ç®æºè§è§æ¨¡åè½å¤è¯å«å·²ç¥å±æ§åå¯¹è±¡å¨æ¨çæ¶æªæ¾è§è¿çç»åï¼ä»èè§£å³ä¸ºæ¯ç§å¯è½ç»åé½éè¦è®­ç»æ°æ®çç»åææãæ ¸å¿é®é¢å¨äºï¼åºåï¼å¦å±æ§åå¯¹è±¡ï¼çè§è§å¤è§å·æé«åº¦æå¢æ§ï¼ä¸æ¨¡åéè¦ææå»ºæ¨¡è¿ç§æå¢æ§ååºæçç»åæ§ï¼ä»¥å®ç°é²æ£çé¶æ ·æ¬è¯å«ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçä¸»è¦è´¡ç®å¨äºæåºäºé¦ä¸ªåºäºâè§£è¦âååçCZSLæ¹æ³ç»¼ååç±»æ³ãè¯¥åç±»æ³å°ç°ææ¹æ³åä¸ºåå¤§ç±»ï¼
*   <strong>æ æ¾å¼è§£è¦ï¼No Explicit Disentanglementï¼ï¼</strong> å°å±æ§-å¯¹è±¡ç»åè§ä¸ºåä¸ååï¼éè¿æ´ä½åµå¥æç´æ¥èåæºå¶è¿è¡å»ºæ¨¡ã
*   <strong>ææ¬ç¹å¾è§£è¦ï¼Textual Disentanglementï¼ï¼</strong> å¨è¯­è¨ç©ºé´ä¸­åç¦»åºåçè¯­ä¹åµå¥ï¼ä»¥å®ç°ç¬ç«æ¦å¿µè¡¨ç¤ºã
*   <strong>è§è§ç¹å¾è§£è¦ï¼Visual Disentanglementï¼ï¼</strong> å¨å¾åè¡¨ç¤ºä¸­éç¦»å±æ§åå¯¹è±¡çè§è§ç¹å¾ï¼å°å¶åè§£ä¸ºå¯ç»åçè¡¨ç¤ºã
*   <strong>è·¨æ¨¡æï¼æ··åï¼è§£è¦ï¼Cross-Modal (Hybrid) Disentanglementï¼ï¼</strong> åæ¶å¨è§è§åææ¬ç©ºé´ä¸­è§£è¦åºåï¼å¹¶éè¿è·¨æ¨¡æå¯¹é½æ´åäºè¡¥ä¿¡æ¯ã</p>
<p>å¨ç¬¬äºå±ï¼æ¹æ³æ ¹æ®å¶å»ºæ¨¡å±æ§åå¤çç»åææçç­ç¥ï¼å¦åºäºååå»ºæ¨¡ãåæåµå¥ãå ææ¨çç­ï¼è¿ä¸æ­¥ç»åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
éè¿å¯¹ç°ææ¹æ³çè¯¦ç»æ¯è¾åæï¼è®ºææ­ç¤ºäºä»¥ä¸ä¸»è¦è¶å¿ååç°ï¼
*   <strong>éª¨å¹²ç½ç»æåºï¼</strong> åºäºCLIPç¼ç å¨çæ¹æ³ï¼èª2023å¹´èµ·ï¼å¨åç¡®æ§ä¸æ¾èä¼äºæ©æåºäºResNetç¼ç å¨çæ¹æ³ï¼è¿è¡¨æè§è§-è¯­è¨é¢è®­ç»ä½ä¸ºæ åéª¨å¹²ç½ç»çä¼å¿ã
*   <strong>è§£è¦ç­ç¥çæææ§ï¼</strong> è§è§è§£è¦æ¹æ³å¨é­ä¸çè®¾ç½®ä¸­è¡¨ç°åºæå¼ºçæ§è½ï¼å¶ä¸­åºäºååçCLUSPROæ¹æ³å¨å¤ä¸ªæ°æ®éä¸åå¾äºæé«åç¡®çã
*   <strong>è·¨æ¨¡ææ¹æ³çæ½åï¼</strong> è·¨æ¨¡æè§£è¦æ¹æ³è½ç¶åºç°è¾æï¼ä½å¨é­ä¸çè®¾ç½®ä¸­å·²è½ä¸æä½³è§è§æ¨¡åå¹æï¼æ¾ç¤ºåºå·¨å¤§çæ½åãç¶èï¼å¨å¼æ¾ä¸çè®¾ç½®ä¸­ï¼å®ä»¬ä»è½åäºé¡¶çº§çè§è§è§£è¦æ¹æ³ã
*   <strong>å¼æ¾ä¸çææï¼</strong> å¨å¼æ¾ä¸çè®¾ç½®ä¸­ï¼æ¨¡åéè¦å¤çææå¯è½çå±æ§-å¯¹è±¡ç»åï¼åæ¬ä¸å¯è¡çç»åï¼ï¼è¿å¯¼è´æ§è½æ¾èä¸éï¼è¡¨ææ¨¡åå¨è¯å«åå¯è¡æ§å¤çæ¹é¢çææã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æ æ¾å¼è§£è¦æ¹æ³ï¼</strong> æ æ³ææå±æ§åå¯¹è±¡çç¬ç¹è¯­ä¹æå¶æå¢åå¼æ§ï¼é¾ä»¥æ³åå°æ°ç»åã
*   <strong>ææ¬è§£è¦æ¹æ³ï¼</strong> ä»ä¾èµè¯­è¨ç©ºé´ä¸è¶³ä»¥ææå±æ§ä¸°å¯çè§è§åå¼æ§ï¼å®¹æå¿½ç¥å¾åä¸­å®éå­å¨ççº ç¼ ã
*   <strong>è§è§è§£è¦æ¹æ³ï¼</strong> å¼ºå¶ä¸¥æ ¼åç¦»å¯è½è¿åº¦ç®åèªç¶ä¾èµå³ç³»ï¼ä¸¢å¼æå©äºè¯å«çæå¢çº¿ç´¢ï¼å®ç°å¹²ååç¦»å¨å®è·µä¸­å¾å°é¾ï¼ç¹å«æ¯å¯¹äºå¾®å¦æå¼ºæå¢ä¾èµçå±æ§ï¼å¶æææ§é«åº¦ä¾èµäºçç£è´¨éåè®­ç»æ°æ®çå¤æ ·æ§ã
*   <strong>è·¨æ¨¡æè§£è¦æ¹æ³ï¼</strong> å­å¨æ¶æåè®¡ç®å¤ææ§é«çé®é¢ï¼ä¸è´çè·¨æ¨¡ææ¥å°ä»ç¶é¾ä»¥å®ç°ï¼å ä¸ºè§è§åºåå¨å¤è§ä¸å·®å¼å¾å¤§ï¼èææ¬åºåç¸å¯¹ç¨³å®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å»ºæ¨¡åºååæå¢æ§ï¼</strong> è¿ä¸æ­¥å®åè§è§è§£è¦ç­ç¥ï¼å¹¶æ´å éè§å¼åè·¨æ¨¡ææ¡æ¶ï¼ä»¥å¯æ©å±åé²æ£çæ¹å¼æææå¢æ§ã
*   <strong>æ©å±å°å¼æ¾ä¸çè¯ä¼°ï¼</strong> å¼ååå¨é²æ£äºä¸å¯è¡ç»åçæ¨¡åï¼ç¼©å°é­ä¸çåå¼æ¾ä¸çæ§è½ä¹é´çå·®è·ï¼èæ éæ¾å¼å¯è¡æ§è®¡ç®ã
*   <strong>æ³åå°æªè§åºåï¼</strong> è®¾è®¡è½å¤å¨æéåºæªè§å¯¹è±¡çå±æ§è¡¨ç¤ºçæ¨¡åï¼å¹¶æ¢ç´¢å©ç¨å¤§åè¯­è¨æ¨¡åçè¯­ä¹æ©å±è½åè¿è¡è·¨æ¨¡æè§£è¦ã
*   <strong>å©ç¨å¤§åå¤æ¨¡ææ¨¡åï¼LMMsï¼ï¼</strong> æ¢ç´¢å°LMMsä¸è·¨æ¨¡ææ¨¡åç»åï¼ä»¥å®ç°æ´å¼ºçç»åæ³åï¼åæ¶å»ºç«ä¸¥æ ¼çè¯ä¼°åè®®æ¥è§£å³æ°æ®æ±¡æé®é¢ï¼å¹¶å¼åéåºç­ç¥ä»¥å­¦ä¹ çæ­£çç»åç»æèéè¡¨é¢ç¸å³æ§ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºCZSLé¢åæä¾äºå®è´µçè·¯çº¿å¾ï¼å¼ºè°äºç°ææ¹æ³çä¼å¿åå±éæ§ï¼å¹¶ä¸ºæªæ¥çç ç©¶ææäºæ¹åï¼ä»¥æå»ºæ´å·å¯æ©å±æ§ãé²æ£æ§åéæåº¦çç»åæ¨çç³»ç»ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11106v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11106v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11063v1'></a></p>
<h2 id="lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation"><a href="https://arxiv.org/abs/2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></h2>
<p><strong>Authors:</strong> Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard <script type="math/tex">{J}</script>, <script type="math/tex">F</script>, and <script type="math/tex">{J\&F}</script> metrics for
VOS and RVOS, while MOSEv2 adopts <script type="math/tex">{J\&\dot{F}}</script> as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâLSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentationâçæè¦ï¼ç±Chang Liuç­äººæ°åã</p>
<p><strong>è®ºææè¦ï¼LSVOS 2025 ææèµæ¥åï¼å¤æè§é¢ç®æ åå²çææ°è¿å±</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥æ¥åæ¨å¨æ¦è¿°ç¬¬ä¸å±å¤§è§æ¨¡è§é¢ç®æ åå²ï¼LSVOSï¼ææèµï¼è¯¥ææèµè´åäºè§£å³å¨éåéãçå®ä¸çè§é¢åºæ¯ä¸­è§é¢ç®æ åå²ï¼VOSï¼çé²æ£æ§é®é¢ãä¼ ç»çVOSååç§VOSï¼RVOSï¼ä»»å¡å¨å¤çå¤æåºæ¯æ¶ä»é¢ä¸´ææï¼å°¤å¶æ¯å¨å¯éå°ç©ä½ãé¢ç¹åºç°/æ¶å¤±äºä»¶ãä¸¥éé®æ¡ãæ¶å£å¤©æ°ååç§ç­æåµä¸ï¼ç°ææ¹æ³é¾ä»¥ä¿æé¿æä¸è´æ§åæ³åè½åãå æ­¤ï¼ææèµå¼å¥äºæ°çMOSEv2èµéï¼æ¨å¨è¿ä¸æ­¥æ¨å¨VOSç ç©¶è¶è¶ç°æåºåçå±éæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¼å¥MOSEv2èµéï¼</strong> LSVOS 2025ææèµå¼å¥äºå¨æ°çâå¤æVOS (MOSEv2)âèµéãMOSEv2æ°æ®éå¨MOSEv1çåºç¡ä¸æ¾èå¢å äºé¾åº¦ï¼åå«æ´å¤æææ§ä¸çå®çåºæ¯ï¼å¦å¯éå°ç©ä½ãé¢ç¹åºç°/æ¶å¤±äºä»¶ãä¸¥éé®æ¡ãæ¶å£å¤©æ°ååç§ç­ï¼æ¨å¨æ¨å¨é¿æä¸è´æ§åæ³åè½åã
*   <strong>MOSEv2æ°è¯ä¼°ææ ï¼</strong> MOSEv2éç¨äºæ°çè¯ä¼°ææ <script type="math/tex">J\&\dot{F}</script>ï¼åºåç¸ä¼¼åº¦Jåèªéåºè¾¹çç²¾åº¦Fçå¹³åå¼ï¼ä½ä¸ºä¸»è¦æåææ ï¼ä»¥æ´å¥½å°è¯ä¼°è·¨å°ºåº¦ååºç°/æ¶å¤±æåµä¸çç©ä½ã
*   <strong>å¼ºè°LLM/MLLMç»ä»¶åè®°å¿æç¥ä¼ æ­ï¼</strong> æ¥åæ»ç»äºé¡¶çº§è§£å³æ¹æ¡ä¸­æ°å´çè¶å¿ï¼åæ¬å¤§åè¯­è¨æ¨¡åï¼LLMï¼/å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMï¼ç»ä»¶æ¥çå¢é¿çä½ç¨ï¼ä»¥åè®°å¿æç¥ä¼ æ­ææ¯ï¼è¿äºææ¯å¯¹äºå¤çå¤æè§é¢åºæ¯ä¸­çé²æ£æ§åè¯­è¨æç¥åå²è³å³éè¦ã
*   <strong>é¡¶çº§è§£å³æ¹æ¡çæ¹æ³è®ºäº®ç¹ï¼</strong>
    *   <strong>MOSEv2èµéï¼</strong> ç¬¬ä¸åå¢éï¼DSS-Trackï¼éç¨äºåºäºSAM-2çSeCæ¡æ¶ï¼éè¿å¢å¼ºæ¦å¿µå»ºæ¨¡ãæ´å¤§çè®°å¿å°ºå¯¸ï¼N=22ï¼åæ¦å¿µæç¥è®°å¿ï¼Concept-aware Memoryï¼æ¥æè·é¿æè·¨å¸§å³ç³»åå¤çåºæ¯ååãç¬¬äºåå¢éï¼IXC-Segï¼ä¹éç¨äºSeCæ¡æ¶ï¼å¼ºè°å¶éè¿åºæ¯èªéåºæ¿æ´»ç­ç¥æå»ºç®æ æ¦å¿µåæ´åLVLMæ¨çè½åãç¬¬ä¸åå¢éï¼hyu_cvlabï¼åå¨Cutieåºç¡ä¸èåSAM2å¾åç¼ç å¨ä»¥ä¸°å¯è¯­ä¹ç¹å¾ï¼å¹¶å¼å¥è¿å¨é¢æµæ¨¡åï¼MPMï¼æ¥ä¼°è®¡é®æ¡ä¸ç©ä½ä½ç½®ï¼ä»¥æé«æ¶é´ä¸è´æ§ã
    *   <strong>VOSèµéï¼</strong> ç¬¬ä¸åå¢éï¼NJUST-KMGï¼å¯¹SAM2æ¨¡åè¿è¡å¾®è°ï¼å¹¶éç¨ç½®ä¿¡åº¦å¼å¯¼çå¤æ¨¡åéæç­ç¥ï¼ç»ååç´ çº§æ£æ¥åæç¥¨æºå¶æ¥è§£å³ä¸ä¸è´é¢æµãç¬¬äºåå¢éï¼Transsionï¼åºäºSAM2æ¡æ¶ï¼éè¿ä¼ªæ ç­¾å¢å¼ºçåéåºåSeCæ¨¡åççº§èæ¨çæ¥æåæ§è½ãç¬¬ä¸åå¢éï¼TS_Videoï¼å©ç¨SeCæ¡æ¶ï¼éè¿æ¦å¿µå¼å¯¼ï¼LVLMï¼ãåºæ¯èªéåºæ¿æ´»åå¹²æ°æç¥è®°å¿ç­ç¥æ¥å¢å¼ºé²æ£æ§ã
    *   <strong>RVOSèµéï¼</strong> ç¬¬ä¸åå¢éï¼SaSaSa2VAï¼å¨Sa2VAåºç¡ä¸è®¾è®¡äºåå²å¢å¼ºç­ç¥ï¼åæ¬å³é®å¸§åç¼©ï¼KFCï¼åç¼©æ¾[SEG] tokensï¼ä»¥å¹³è¡¡æ¶ç©ºæçåMLLMçå¨å±è§é¢çè§£è½åãç¬¬äºåå¢éï¼Transsionï¼æåºäºä¸ä¸ªåå«è§é¢è¯­ä¹å¹éï¼VLCï¼ãå³é®å¸§éæ ·å¨ï¼KFSï¼åSa2VAåå²æ¨¡åçæ¡æ¶ï¼ä»¥ç¡®ä¿ææ¬ä¸è§é¢åå®¹çå¹éï¼å¹¶éæ©ä¿¡æ¯ä¸°å¯çå³é®å¸§ãç¬¬ä¸åå¢éï¼dytinoï¼çSa2VA-iæ¨¡åéè¿ç¡®ä¿æ¨çè¿ç¨ä¸è®­ç»è¿ç¨ä¸è´ï¼å¹¶éç¨ååå¸§éæ ·æ¥æé«æ§è½ã</p>
<p><strong>3. ä¸»è¦ç»æåæä¹ï¼</strong>
*   <strong>MOSEv2çæææ§ï¼</strong> MOSEv2èµéçç»æè¡¨æï¼ç°ä»£VOSæ¹æ³ä»æå¾å¤§çæåç©ºé´ï¼é¢åæ¹æ³çå¾åæ¾èä¸éï¼è¯æäºMOSEv2å¨å¤æçå®åºæ¯ä¸çé¾åº¦ã
*   <strong>LLM/MLLMçå´èµ·ï¼</strong> ææèµçé¡¶çº§è§£å³æ¹æ¡çªåºæ¾ç¤ºäºLLM/MLLMç»ä»¶å¨è¯­è¨å¼å¯¼è§é¢ä»»å¡ä¸­çæææ§ï¼è¡¨æå®ä»¬å¨è§é¢çè§£æ¹é¢å·æå·¨å¤§æ½åã
*   <strong>è®°å¿æºå¶çéè¦æ§ï¼</strong> è®°å¿æç¥ä¼ æ­åé¿æè®°å¿ç®¡çå¨å¤çå¤ææ¶ç©ºåºæ¯ãç©ä½åºç°/æ¶å¤±åé®æ¡æ¹é¢åæ¥äºå³é®ä½ç¨ï¼æå©äºæé«æ¨¡åçé²æ£æ§åä¸è´æ§ã
*   <strong>å¤æ¨¡åéæåèªéåºç­ç¥ï¼</strong> è®¸å¤é¡¶çº§è§£å³æ¹æ¡éç¨äºå¤æ¨¡åéæãä¼ªæ ç­¾ãåºæ¯èªéåºæ¿æ´»åç½®ä¿¡åº¦å¼å¯¼çèåç­ç¥ï¼ä»¥å©ç¨ä¸åæ¨¡åçäºè¡¥ä¼å¿ï¼æé«æ´ä½æ§è½ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>MOSEv2çæææ§ï¼</strong> å°½ç®¡åå¾äºè¿å±ï¼ä½MOSEv2æ°æ®éçå¼å¥æ­ç¤ºäºå½åæåè¿çVOSç³»ç»å¨å¤æãçå®åºæ¯ä¸­ä»é¢ä¸´æ¾èææï¼å°¤å¶æ¯å¨å¤çå¯éå°ç©ä½ãé¢ç¹åºç°/æ¶å¤±ãä¸¥éé®æ¡åæ¶å£å¤©æ°ç­æåµæ¶ã
*   <strong>Sa2VAçå±éæ§ï¼RVOSèµéï¼ï¼</strong> åå§Sa2VAæ¨¡åå¨è®­ç»æ¶æ¯è§é¢ä»éæ ·äºå¸§ï¼ä¸ä»ä½¿ç¨ä¸ä¸ª[SEG] tokenæ¥ä¼ éä¿¡æ¯ï¼è¿éå¶äºMLLMæè·å¨å±è§é¢ä¸ä¸æçè½åï¼å¹¶é¾ä»¥éåºæ¶é´ååã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´æ·±å±æ¬¡çLLM/MLLMéæï¼</strong> æ¥åé¢æµï¼LLM/MLLMçæ´æ·±å±æ¬¡éæå°ç»§ç»­æåæ§è½ï¼å°¤å¶æ¯å¨è¯­è¨æç¥è§é¢åå²æ¹é¢ã
*   <strong>è§£å³MOSEv2ä¸­çå¤±è´¥æ¨¡å¼ï¼</strong> æªæ¥çç ç©¶åºå³æ³¨MOSEv2ææèµä¸­è¯å«åºçæå°é¾çå¤±è´¥æ¨¡å¼åçå®ä¸çç¨ä¾ï¼ä»¥è¿ä¸æ­¥æ¨å¨è§é¢ç®æ åå²åç¸å³ç ç©¶çåæ²¿ã
*   <strong>å¢å¼ºé¿æä¸è´æ§åæ³åè½åï¼</strong> éå¯¹MOSEv2ä¸­æåºçææï¼éè¦å¼åæ°çç®æ³è®¾è®¡ï¼ä»¥æé«æ¨¡åå¨éåéãçå®ä¸çåºæ¯ä¸­çé¿æä¸è´æ§åæ³åè½åã
*   <strong>æ¹è¿è·¨å°ºåº¦ååºç°/æ¶å¤±æåµä¸çè¯ä¼°ï¼</strong> MOSEv2å¼å¥ç<script type="math/tex">J\&\dot{F}</script>ææ ä¸ºæªæ¥çç ç©¶æä¾äºæ´ç²¾ç»çè¯ä¼°æ åï¼é¼å±å¼åè½å¤æ´å¥½å°å¤çè¿äºå¤ææåµçæ¹æ³ã</p>
<p>æ»èè¨ä¹ï¼LSVOS 2025ææèµæ¥åä¸ä»æ¦è¿°äºè§é¢ç®æ åå²é¢åçææ°è¿å±ï¼è¿éè¿å¼å¥MOSEv2æ°æ®éåå¼ºè°LLM/MLLMçä½ç¨ï¼ä¸ºæªæ¥çç ç©¶ææäºæ¹åï¼æ¨å¨å®ç°æ´å·é²æ£æ§ãè¯­è¨æç¥ä¸è½å¨éå¤åºæ¯ä¸­ææå·¥ä½çè§é¢åå²ç³»ç»ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11063v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11063v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12801v1'></a></p>
<h2 id="deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search"><a href="https://arxiv.org/abs/2510.12801v1">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a></h2>
<p><strong>Authors:</strong> Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV, cs.IR</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) in real-world applications require
access to external knowledge sources and must remain responsive to the dynamic
and ever-changing real-world information in order to address
information-seeking and knowledge-intensive user queries. Existing approaches,
such as retrieval augmented generation (RAG) methods, search agents, and search
equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and
poorly constructed search queries, which result in inefficiencies and
suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.
Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops
of the input image making the image search more effective, and can iteratively
adapt text search queries based on retrieved information, thereby enabling
self-reflection and self-correction. Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization. For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools. This
dataset contains diverse, multi-hop queries that integrate textual and visual
information, teaching the model when to search, what to search for, which
search tool to use and how to reason over the retrieved information. We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach. Finally, we analyze the results
and provide insights that are valuable for advancing multimodal web-search.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Kartik Narayanç­äººæ°åçè®ºæâDeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Searchâçå¨é¢æè¦ï¼</p>
<p><strong>1. è®ºææ¨å¨è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>å½åçå¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨å¤çéè¦å¤é¨ç¥è¯åå®æ¶ä¿¡æ¯çå¤æãç¥è¯å¯éåç¨æ·æ¥è¯¢æ¶é¢ä¸´ææãç°æçæ¹æ³ï¼å¦æ£ç´¢å¢å¼ºçæï¼RAGï¼åæç´¢ä»£çï¼éå¸¸å­å¨ç®¡éåµåãæç´¢è°ç¨è¿å¤ä»¥åæç´¢æ¥è¯¢æå»ºä¸ä½³ç­é®é¢ï¼å¯¼è´æçä½ä¸åç»æä¸çæ³ãè®ºææ¨å¨å¼åä¸ç§è½å¤è¿è¡æéãå¤è½®ç½ç»æç´¢å¹¶å¨æçæå¾ååææ¬æç´¢æ¥è¯¢çMLLMï¼ä»¥åæè¿äºéå¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<ul>
<li><strong>DeepMMSearch-R1æ¨¡åï¼</strong> è®ºææåºäºDeepMMSearch-R1ï¼è¿æ¯ç¬¬ä¸ä¸ªè½å¤æ§è¡æéãå¤è½®ç½ç»æç´¢å¹¶å¨æçæå¾ååææ¬æç´¢æ¥è¯¢çå¤æ¨¡æLLMãå®è½å¤åºäºè¾å¥å¾åçç¸å³è£åªåºåå¯å¨ç½ç»æç´¢ï¼ä»èæé«å¾åæç´¢çæçï¼å¹¶è½æ ¹æ®æ£ç´¢å°çä¿¡æ¯è¿­ä»£è°æ´ææ¬æç´¢æ¥è¯¢ï¼å®ç°èªæåæåèªæçº æ­£ã</li>
<li><strong>ä¸¤é¶æ®µè®­ç»æµç¨ï¼</strong> DeepMMSearch-R1çè®­ç»éç¨ä¸¤é¶æ®µç®¡éï¼é¦åæ¯å·å¯å¨çç£å¾®è°ï¼SFTï¼é¶æ®µï¼ç¶åæ¯å¨çº¿å¼ºåå­¦ä¹ ï¼RLï¼ä¼åé¶æ®µã</li>
<li><strong>DeepMMSearchVQAæ°æ®éï¼</strong> ä¸ºäºè®­ç»æ¨¡åï¼è®ºæå¼å¥äºä¸ä¸ªæ°é¢çå¤æ¨¡æVQAæ°æ®éDeepMMSearchVQAãè¯¥æ°æ®ééè¿èªå¨åç®¡éåå»ºï¼èåäºæ¥èªç½ç»æç´¢å·¥å·ççå®ä¸çä¿¡æ¯ï¼åå«å¤æ ·åãå¤è·³çæ¥è¯¢ï¼æ´åäºææ¬åè§è§ä¿¡æ¯ï¼æ¨å¨æä¼æ¨¡åä½æ¶æç´¢ãæç´¢ä»ä¹ãä½¿ç¨åªä¸ªæç´¢å·¥å·ä»¥åå¦ä½æ¨çæ£ç´¢å°çä¿¡æ¯ã</li>
<li><strong>å¤æ¨¡ææç´¢å·¥å·éæï¼</strong> DeepMMSearch-R1éæäºä¸ä¸ªå·¥å·ï¼ææ¬æç´¢å·¥å·ï¼ç¨äºæ£ç´¢ç½é¡µåè·åææ°äºå®ç¥è¯ï¼ãåºäºGrounding DINOçå¾åè£åªå·¥å·ï¼ç¨äºè¯å«åè£åªå¾åä¸­æç¸å³çè§è§å®ä½ï¼ä»¥åå¾åæç´¢å·¥å·ï¼ç¨äºæ£ç´¢è§è§ç¸ä¼¼çå¾ååä¸ä¸æä¿¡æ¯ï¼ã</li>
<li><strong>èªæåæåèªæçº æ­£ï¼</strong> æ¨¡åè½å¤æ ¹æ®æ£ç´¢å°çä¿¡æ¯è¿­ä»£å°è°æ´ææ¬æç´¢æ¥è¯¢ï¼ä»èå®ç°èªæåæåèªæçº æ­£ï¼ä»¥æ´å¥½å°åºå¯¹åæççå®ä¸çç½ç»ä¿¡æ¯ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§</strong></p>
<ul>
<li><strong>åè¶çæ§è½ï¼</strong> DeepMMSearch-R1å¨å¤é¡¹ç¥è¯å¯éååºåæµè¯ä¸­è¡¨ç°åºä¼è¶æ§ï¼è¶è¶äºç°æåºçº¿ï¼åæ¬GPT-4oï¼å¹¶ä¸GPT-03æ¨¡åå·æç«äºåã</li>
<li><strong>è£åªå¾åæç´¢åèªæåæçæææ§ï¼</strong> å®éªè¯æï¼è£åªå¾åæç´¢åèªæåæ/èªæçº æ­£è½åæ¾èæåäºæ§è½ãè£åªå¾åæç´¢å¹³åæé«äº+1.75çæ§è½ï¼ææç¼è§£äºèæ¯åªå£°ï¼æé«äºæç´¢æçã</li>
<li><strong>SFTæ°æ®å¹³è¡¡çéè¦æ§ï¼</strong> å®éªè¡¨æï¼SFTæ°æ®ä¸­æç´¢æéåæç´¢æ å³ç¤ºä¾ç50:50å¹³è¡¡éç½®ï¼ä»¥åç¥è¯åç±»ä¸­ååéæ ·çç¤ºä¾ï¼è½å¤æä¾æä½³çå¹³åæ§è½ã</li>
<li><strong>RLå¯¹å·¥å·ä½¿ç¨çä¼åï¼</strong> å¼ºåå­¦ä¹ é¶æ®µè¿ä¸æ­¥ä¼åäºæ¨¡åçå·¥å·éæ©è¡ä¸ºï¼åå°äºä¸å¿è¦çè°ç¨ï¼ä½¿æ¨¡åå¨å¾åæç´¢åå¤è½®ææ¬æç´¢æ¹é¢æ´å é«æåæéå¯¹æ§ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong></p>
<ul>
<li><strong>ç°ææ¹æ³çå±éæ§ï¼</strong> ç°æçRAGæ¹æ³åæç´¢ä»£çéå¸¸å­å¨ç®¡éåµåãæç´¢è°ç¨è¿å¤åæ¥è¯¢æå»ºä¸ä½³çé®é¢ã</li>
<li><strong>éæè®­ç»è¯­æåºçå±éæ§ï¼</strong> ç°æMLLMsçè®­ç»ä¾èµäºéæè¯­æåºï¼å¯¼è´ç¥è¯è¿æ¶ï¼é¾ä»¥åºå¯¹ä¸æ­ååçå®æ¶ä¿¡æ¯åé¿å°¾ç¥è¯åå¸ã</li>
<li><strong>ç°ææç´¢å·¥å·çå±éæ§ï¼</strong> ç°ææç´¢å·¥å·éå¸¸å±éäºææ¬æç´¢ï¼æ æ³è¿è¡å¾åæç´¢ï¼ä¸å¨å¤æ¨¡æç¥è¯å¯éåé®ç­ä¸­çéç¨æ§æéãæ­¤å¤ï¼å¾åæç´¢å·¥å·éå¸¸ç¼ºä¹é®é¢ç¹å®çæå¯¼æç¦ç¹ï¼å®¹æåå°èæ¯åªå£°åæ å³è§è§å®ä½çå¹²æ°ã</li>
<li><strong>MMSearch-R1çå±éæ§ï¼</strong> ä¹åçMMSearch-R1æ¨¡åè½ç¶æ¯æå¤æ¨¡ææ£ç´¢ï¼ä½åéäºæ¯ä¸ªå·¥å·åªè½è°ç¨ä¸æ¬¡ï¼ä¸å¾åæç´¢å·¥å·æ æ³è¿è¡é®é¢ç¹å®çè£åªã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<ul>
<li><strong>æ©å±å·¥å·å¤æ ·æ§ï¼</strong> æªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢æ©å±å·¥å·çå¤æ ·æ§ï¼ä»¥è¿ä¸æ­¥å¢å¼ºMLLMsçåè½ã</li>
<li><strong>é¿ä¸ä¸ææ¨çï¼</strong> è¿ä¸æ­¥ç ç©¶é¿ä¸ä¸ææ¨çè½åï¼ä»¥å¤çæ´å¤æçæ¥è¯¢åä¿¡æ¯ã</li>
<li><strong>å¤è¯­è¨åå¤æ¨¡æé¢åçæ©å±ï¼</strong> å°è®­ç»æ©å±å°æ´å¹¿æ³çå¤è¯­è¨åå¤æ¨¡æé¢åï¼ä»¥æé«æ¨¡åçæ®éæ§ã</li>
</ul>
<p>æ»èè¨ä¹ï¼DeepMMSearch-R1éè¿å¼å¥æéãå¤è½®ç½ç»æç´¢ãå¨ææ¥è¯¢çæåè£åªå¾åæç´¢å·¥å·ï¼æ¾èæåäºMLLMså¨ç¥è¯å¯éååä¿¡æ¯å¯»æ±åè§è§é®ç­ä»»å¡ä¸­çè½åãå¶ä¸¤é¶æ®µè®­ç»æµç¨åæ°é¢çDeepMMSearchVQAæ°æ®éä¸ºå¤æ¨¡æç½ç»æç´¢çæªæ¥åå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.</li>
<li>Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization.</li>
<li>For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools.</li>
<li>We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12801v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12801v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12793v1'></a></p>
<h2 id="vico-a-training-strategy-towards-semantic-aware-dynamic-high-resolution"><a href="https://arxiv.org/abs/2510.12793v1">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</a></h2>
<p><strong>Authors:</strong> Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing Multimodal Large Language Models (MLLMs) suffer from increased
inference costs due to the additional vision tokens introduced by image inputs.
In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens. The key idea behind our
method is to employ multiple MLP connectors, each with a different image
compression ratio, to downsample the vision tokens based on the semantic
complexity of the image. During training, we minimize the KL divergence between
the responses conditioned on different MLP connectors. At inference time, we
introduce an image router, termed Visual Resolution Router (ViR), that
automatically selects the appropriate compression rate for each image patch.
Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.
Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model's perception, reasoning,
and OCR capabilities. We hope this work will contribute to the development of
more efficient MLLMs. The code and models will be released to facilitate future
research.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Long Cuiç­äººæ°åçè®ºæâViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolutionâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
ç°æçå¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨å¤çå¾åè¾å¥æ¶ï¼ç±äºå¼å¥äºé¢å¤çè§è§tokensï¼å¯¼è´æ¨çææ¬æ¾èå¢å ãä¼ ç»çå¨æé«åè¾¨çç­ç¥éå¸¸æ ¹æ®å¾ååè¾¨çè°æ´è§è§tokensçæ°éï¼ä½æªè½ååèèå¾ååå®¹çè¯­ä¹å¤ææ§ï¼å¯¼è´å¨å¤çè¯­ä¹ä¿¡æ¯ä¸åçå¾åæ¶æçä½ä¸ææ§è½ä¸éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼æ¬ææåºäº<strong>è§è§ä¸è´æ§å­¦ä¹ ï¼Visual Consistency Learning, ViCOï¼</strong>ï¼è¿æ¯ä¸ç§æ°é¢çè®­ç»ç®æ³ï¼ä½¿æ¨¡åè½å¤æ ¹æ®å¾åçè¯­ä¹å¤ææ§ï¼ä½¿ç¨ä¸åæ°éçè§è§tokensæ¥è¡¨ç¤ºå¾åãå¶æ ¸å¿ææ³åè´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>å¤éMLPè¿æ¥å¨ä¸è¯­ä¹å¤ææ§ä¸éæ ·ï¼</strong> ViCOéç¨å¤ä¸ªMLPè¿æ¥å¨ï¼æ¯ä¸ªè¿æ¥å¨å¯¹åºä¸åçå¾ååç¼©æ¯ãæ¨¡åæ ¹æ®å¾åçè¯­ä¹å¤ææ§ï¼éè¿è¿äºè¿æ¥å¨å¯¹è§è§tokensè¿è¡ä¸éæ ·ã</li>
<li><strong>ä¸è´æ§è®­ç»ï¼Consistency Trainingï¼ï¼</strong> å¨è®­ç»é¶æ®µï¼æ¨¡åéè¿æå°åå¨ä¸åMLPè¿æ¥å¨ï¼å³ä¸ååç¼©çï¼ä¸ååºçKLæ£åº¦æ¥ç¡®ä¿ä¸è´æ§ãè¿ä½¿å¾æ¨¡åå³ä½¿å¨ä½¿ç¨é«åº¦åç¼©çè§è§tokensæ¶ä¹è½çæåç¡®çååºï¼ä»èæé«äºå¨åç¼©è§è§tokensä¸çæ§è½åé²æ£æ§ã</li>
<li><strong>è§è§åè¾¨çè·¯ç±å¨ï¼Visual Resolution Router, ViRï¼ï¼</strong> å¨æ¨çé¶æ®µï¼å¼å¥äºä¸ä¸ªåä¸ºViRçå¾åè·¯ç±å¨ãViRè½å¤èªå¨ä¸ºæ¯ä¸ªå¾ååéæ©åéçåç¼©çãä¸ç°æåºäºå¾ååè¾¨ççå¨æç­ç¥ä¸åï¼ViRæ ¹æ®å¾ååçè¯­ä¹å¤ææ§å¨æè°æ´è§è§tokensçæ°éï¼ä»èå®ç°æ´ç»ç²åº¦çæ§å¶ã</li>
<li><strong>æå¤±æ¯çï¼Loss Ratioï¼ä½ä¸ºæå¯¼ï¼</strong> ViRçè®­ç»éè¿è®¡ç®æ¯ä¸ªå¾ååçâæå¤±æ¯çâæ¥çæçç£ä¿¡å·ï¼è¯¥æ¯çéåäºåç¼©å¯¹æ¨¡åè¾åºæ§è½çå½±åãè¿ä½¿å¾è·¯ç±å¨è½å¤è¯å«åªäºå¾ååå¯ä»¥å®å¨å°è¿è¡é«åç¼©ï¼åªäºéè¦ä¿çé«åè¾¨çä»¥ä¿æå³é®è¯­ä¹ä¿¡æ¯ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
å®éªç»æè¡¨æï¼ViCOæ¹æ³å¨ä¿ææ¨¡åæç¥ãæ¨çåOCRè½åçåæ¶ï¼è½å¤å°è§è§tokensçæ°éåå°é«è¾¾50%ãå·ä½æ¥è¯´ï¼</p>
<ul>
<li><strong>æ§è½ä¿æä¸ååéæåï¼</strong> å¨InternVL3.5ç³»åæ¨¡åï¼ä»4Bå°241B-MoEï¼ä¸ï¼ViCOæ¨¡åå¨åç§éç¨åºåæµè¯ä¸­å¹³åä¿çäºè¶è¿99.6%çåå§æ§è½ï¼åæ¶å°é¦ä¸ªtokençååéæé«äºçº¦1.8åã</li>
<li><strong>OCRç¸å³ä»»å¡çé²æ£æ§ï¼</strong> å¨OCRBenchãChartQAåTextVQAç­OCRç¸å³åºåæµè¯ä¸­ï¼ViCOæ¨¡åå¨å®ç°é«åç¼©çï¼ä¾å¦OCRBenchä¸71%çåç¼©çï¼çåæ¶ï¼æ§è½å ä¹ä¸åå§æ¨¡åä¿æä¸è´ï¼è¿è¡¨æå¶èªéåºè·¯ç±ç­ç¥çé²æ£æ§ã</li>
<li><strong>å¤å¾ååè§é¢çè§£ï¼</strong> å¨å¤å¾ååè§é¢åºåæµè¯ä¸­ï¼ViCOä¹å®ç°äºæ¾èçtokensåç¼©ï¼åæ¶ä¿æäºé«æ§è½ï¼è¯æäºå¶å¨å¤çé¿è§è§åºååéè¦è·¨å¤å¾å/è§é¢å¸§çè§£çä»»å¡ä¸­çæææ§ã</li>
<li><strong>ä¼äºç°ææ¹æ³ï¼</strong> ä¸FastVåSparseVLMç­ç°ætokensååæ¹æ³ç¸æ¯ï¼ViCOå¨ç¸ä¼¼åç¼©çä¸è¡¨ç°åºæ´å¥½çæ§è½ï¼å°¤å¶æ¯å¨è§è§ææä»»å¡ä¸ï¼å ä¸ºå®è½æ ¹æ®è¯­ä¹éè¦æ§èªéåºå°è°æ´åç¼©ã</li>
<li><strong>æ¶èç ç©¶éªè¯ï¼</strong> æ¶èå®éªéªè¯äºViCOä¸­ä¸è´æ§è®­ç»åViRè·¯ç±å¨çæææ§ï¼ç¹å«æ¯patchçº§å«çè·¯ç±æ¯å¾åçº§å«çè·¯ç±è½æä¾æ´å¹³è¡¡åè¯­ä¹ä¿¡æ¯æ´ä¸°å¯çtokensåç¼©ã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼ViCOä¸ºå¼åæ´é«æçMLLMsååºäºéè¦è´¡ç®ï¼éè¿è¯­ä¹æç¥çæ¹å¼æ¾èéä½äºæ¨çææ¬ï¼åæ¶ä¿æäºå¼ºå¤§çæ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
è®ºæä¸­æ²¡ææç¡®æåViCOæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»å¶è®¾è®¡åå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çèèï¼</p>
<ul>
<li><strong>è®¡ç®å¼éï¼</strong> å°½ç®¡ViCOæ¨å¨éä½æ¨çææ¬ï¼ä½å¨è®­ç»é¶æ®µï¼ä¸è´æ§è®­ç»åViRçè®­ç»å¯è½å¼å¥é¢å¤çè®¡ç®å¼éï¼å°¤å¶æ¯å¨å¤§è§æ¨¡æ¨¡ååæ°æ®éä¸ã</li>
<li><strong>è·¯ç±å¨çåç¡®æ§ï¼</strong> ViRçæ§è½ä¾èµäºå¶åç¡®è¯å«å¾ååè¯­ä¹å¤ææ§çè½åãå¦æè·¯ç±å¨æªè½åç¡®å¤æ­ï¼å¯è½ä¼å¯¼è´å³é®ä¿¡æ¯è¢«è¿åº¦åç¼©æä¸å¿è¦å°ä¿çåä½ä¿¡æ¯ã</li>
<li><strong>åç¼©ççç²åº¦ï¼</strong> ç®åViCOéç¨çæ¯é¢å®ä¹çåç¼©çï¼ä¾å¦256 tokenså°64 tokensï¼ãæ´ç»ç²åº¦æè¿ç»­çåç¼©çéæ©å¯è½ä¼è¿ä¸æ­¥ä¼åæ§è½ï¼ä½ä¹ä¼å¢å æ¨¡åçå¤ææ§ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºæä¸­æåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ´é«æçMLLMså¼åï¼</strong> æ¬æå¸æå¶å·¥ä½è½ä¿è¿æ´é«æMLLMsçå¼åã</li>
<li><strong>èªéåºè§è§è¡¨ç¤ºï¼</strong> ViCOéè¿åºäºè§è§è¯­ä¹ååºå³ç­ï¼ä½¿æ¨¡åè½å¤ææå°å°è®¡ç®éä¸­å¨å¾åä¸­æå·ä¿¡æ¯éçåºåï¼è¿ä¸ºæªæ¥ç ç©¶èªéåºè§è§è¡¨ç¤ºæä¾äºè§è§£ã</li>
<li><strong>ä»£ç åæ¨¡ååå¸ï¼</strong> è®ºææ¿è¯ºå°åå¸ä»£ç åæ¨¡åï¼ä»¥ä¿è¿æªæ¥çç ç©¶åå¤ç°ã</li>
</ul>
<p>æ»èè¨ä¹ï¼ViCOéè¿å¼å¥è¯­ä¹æç¥çå¨æè§è§tokensåç¼©ç­ç¥ï¼ä¸ºè§£å³MLLMsçæ¨çææ¬é®é¢æä¾äºä¸ä¸ªåæ°ä¸é«æçè§£å³æ¹æ¡ï¼ä¸ºæªæ¥æ´é«æãæ´æºè½çå¤æ¨¡æAIæ¨¡åå¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens.</li>
<li>Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.</li>
<li>Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model's perception, reasoning,
and OCR capabilities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12793v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12793v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12777v1'></a></p>
<h2 id="what-if-understanding-motion-through-sparse-interactions"><a href="https://arxiv.org/abs/2510.12777v1">What If : Understanding Motion Through Sparse Interactions</a></h2>
<p><strong>Authors:</strong> Stefan Andreas Baumann, Nick Stracke, Timy Phan, BjÃ¶rn Ommer</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Understanding the dynamics of a physical scene involves reasoning about the
diverse ways it can potentially change, especially as a result of local
interactions. We present the Flow Poke Transformer (FPT), a novel framework for
directly predicting the distribution of local motion, conditioned on sparse
interactions termed "pokes". Unlike traditional methods that typically only
enable dense sampling of a single realization of scene dynamics, FPT provides
an interpretable directly accessible representation of multi-modal scene
motion, its dependency on physical interactions and the inherent uncertainties
of scene dynamics. We also evaluate our model on several downstream tasks to
enable comparisons with prior methods and highlight the flexibility of our
approach. On dense face motion generation, our generic pre-trained model
surpasses specialized baselines. FPT can be fine-tuned in strongly
out-of-distribution tasks such as synthetic datasets to enable significant
improvements over in-domain methods in articulated object motion estimation.
Additionally, predicting explicit motion distributions directly enables our
method to achieve competitive performance on tasks like moving part
segmentation from pokes which further demonstrates the versatility of our FPT.
Code and models are publicly available at
https://compvis.github.io/flow-poke-transformer.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼What If : Understanding Motion Through Sparse Interactions</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºæåºäº <strong>Flow Poke Transformer (FPT)</strong>ï¼ä¸ä¸ªæ°é¢çæ¡æ¶ï¼è½å¤ç´æ¥é¢æµå±é¨è¿å¨çåå¸ï¼å¹¶ä»¥ç¨ççâæ³å¨âï¼pokesï¼ä½ä¸ºæ¡ä»¶ãä¸ä¼ ç»æ¹æ³ä¸åï¼FPT æä¾äºä¸ç§å¯è§£éçãç´æ¥å¯è®¿é®çå¤æ¨¡æåºæ¯è¿å¨è¡¨ç¤ºï¼æ­ç¤ºäºè¿å¨å¯¹ç©çäº¤äºçä¾èµæ§ä»¥ååºæ¯å¨æåºæçä¸ç¡®å®æ§ãè¯¥æ¨¡åå¨å¤ä¸ªä¸æ¸¸ä»»å¡ä¸­å±ç°åºåè¶çæ§è½åæ³åè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>FPT çå³é®åæ°å¨äºå¶è½å¤ï¼</p>
<ul>
<li><strong>ç´æ¥é¢æµè¿å¨åå¸ (Directly Predicting Motion Distribution):</strong> å¤§å¤æ°ç°ææ¹æ³å¾åäºçæåä¸çãå¯éçè¿å¨å®ç°ãFPT åä¸æ³¨äºé¢æµè¿å¨ç<strong>åå¸</strong>ï¼è¿ä½¿å¾å®è½å¤ææåºæ¯å¨æçå¤æ¨¡ææ§ååå¨ä¸ç¡®å®æ§ï¼ä»èçè§£âå¦æâ¦â¦ä¼ææ ·âçå¤ç§å¯è½æ§ã</li>
<li><strong>ç¨çäº¤äºä½ä¸ºæ¡ä»¶ (Conditioned on Sparse Interactions - "Pokes"):</strong> éè¿å°ç¨ççâæ³å¨âä½ä¸ºè¾å¥æ¡ä»¶ï¼FPT æ¨¡æäºäººç±»å¯¹ç©çåºæ¯çç´è§çè§£æ¹å¼ï¼å³éè¿å±é¨ãæç®ççäº¤äºæ¥æ¨æ­æ´ä½å¨æãè¿æ¯ä¾èµå¯éãå¨å±çè¾å¥æ´é«æãæ´å·è§£éæ§ã</li>
<li><strong>Flow Poke Transformer æ¶æ (FPT Architecture):</strong> å°½ç®¡æè¦æ²¡æè¯¦ç»è¯´ææ¶æç»èï¼ä½âTransformerâçå½åæç¤ºäºå¶å¯è½å©ç¨äºèªæ³¨æåæºå¶æ¥ææé¿è·ç¦»ä¾èµåå¤æäº¤äºï¼å¹¶ç»åäºâFlowâçæ¦å¿µï¼å¯è½ä¸åæµæè¿å¨åºé¢æµç¸å³ãè¿ç§ç»åæ¨å¨ææå°ä»ç¨çè¾å¥ä¸­å­¦ä¹ å¤æçè¿å¨æ¨¡å¼ã</li>
<li><strong>å¯è§£éçå¤æ¨¡æè¡¨ç¤º (Interpretable Multi-modal Representation):</strong> è½å¤ç´æ¥è®¿é®è¿å¨åå¸ï¼æå³çæ¨¡åä¸ä»è½ç»åºâä¼åçä»ä¹âï¼è¿è½ç»åºâå¯è½åçä»ä¹âä»¥åâä¸ºä»ä¹ä¼åçâï¼è¿å¤§å¤§å¢å¼ºäºæ¨¡åçå¯è§£éæ§ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨ç©çåºæ¯çè§£ (Advancing Physical Scene Understanding):</strong> FPT è½å¤çè§£åºæ¯å¨æçå¤æ¨¡ææ§åä¸ç¡®å®æ§ï¼è¿å¯¹äºæå»ºæ´æºè½ãæ´å·é²å£«æ§çæºå¨äººåAIç³»ç»è³å³éè¦ï¼è¿äºç³»ç»éè¦å¨çå®ä¸çä¸­è¿è¡è§ååäº¤äºã</li>
<li><strong>æåäº¤äºå¼AIç³»ç» (Enhancing Interactive AI Systems):</strong> è½å¤æ ¹æ®ç¨çäº¤äºé¢æµè¿å¨åå¸ï¼å°æå¤§å°ä¿è¿äººæºäº¤äºãèæç°å®/å¢å¼ºç°å®ä¸­çç©çæ¨¡æä»¥åæºå¨äººæä½ç­é¢åçåå±ã</li>
<li><strong>æ³åè½ååå°æ ·æ¬å­¦ä¹  (Generalization and Few-shot Learning):</strong> æè¦ä¸­æå°FPTå¯ä»¥å¨âå¼ºåå¸å¤ä»»å¡âä¸è¿è¡å¾®è°ï¼å¹¶è¶è¶âååæ¹æ³âï¼è¿è¡¨æå¶å·æå¼ºå¤§çæ³åè½ååè¿ç§»å­¦ä¹ æ½åï¼å¯¹äºæ°æ®ç¨ç¼ºææ°é¢åºæ¯çåºç¨å·æéè¦æä¹ã</li>
<li><strong>æ°çè¯ä¼°èå¼ (New Evaluation Paradigms):</strong> é¢æµè¿å¨åå¸èéåä¸å®ç°ï¼å¯è½éè¦æ°çè¯ä¼°ææ åæ¹æ³æ¥è¡¡éæ¨¡åå¯¹ä¸ç¡®å®æ§åå¤æ¨¡ææ§çææè½åã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>æºå¨äººå­¦åæä½ (Robotics and Manipulation):</strong> æºå¨äººéè¦çè§£ç©ä½å¦ä½ååºæ¨ãæç­æä½ï¼ä»¥ä¾¿è¿è¡ææçæåãæ¾ç½®åå·¥å·ä½¿ç¨ãFPT å¯ä»¥å¸®å©æºå¨äººé¢æµæä½çå¤ç§å¯è½ç»æã</li>
<li><strong>ç©çæ¨¡æåæ¸¸æå¼å (Physics Simulation and Game Development):</strong> çææ´çå®ãæ´å·äº¤äºæ§çç©çæ¨¡æï¼å°¤å¶æ¯å¨å¤çå¤æãéåä½æå¯åå½¢ç©ä½æ¶ã</li>
<li><strong>èæç°å®/å¢å¼ºç°å® (VR/AR):</strong> å¢å¼ºèæç¯å¢ä¸­ç©ä½çäº¤äºçå®æï¼ä¾å¦ï¼å½ç¨æ·è§¦æ¸ææ¨å¨èæç©ä½æ¶ï¼é¢æµå¶å¯è½çååºã</li>
<li><strong>è§é¢é¢æµåçæ (Video Prediction and Generation):</strong> é¢æµæªæ¥å¸§ä¸­ç©ä½çè¿å¨ï¼å°¤å¶æ¯å¨å­å¨ä¸ç¡®å®æ§æå¤ç§å¯è½åå±è·¯å¾çæåµä¸ã</li>
<li><strong>å¼å¸¸æ£æµ (Anomaly Detection):</strong> è¯å«ä¸é¢æµè¿å¨åå¸æ¾èåç¦»çè¿å¨æ¨¡å¼ï¼å¯è½æç¤ºå¼å¸¸äºä»¶ã</li>
<li><strong>å¯åå½¢ç©ä½æä½ (Deformable Object Manipulation):</strong> çè§£å¸æãç»³ç´¢ãé¢é¨ç­å¯åå½¢ç©ä½å¨å±é¨äº¤äºä¸çå¤æè¿å¨ã</li>
<li><strong>å»å­¦å½±ååæ (Medical Image Analysis):</strong> é¢æµç»ç»æå¨å®å¨å¤é¨åºæ¿ï¼å¦ææ¯å¨æ¢°æ¥è§¦ï¼ä¸çå½¢åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>âPokesâçå®ä¹åè·å (Definition and Acquisition of "Pokes"):</strong> æè¦ä¸­æ²¡æè¯¦ç»è¯´æâpokesâæ¯å¦ä½å®ä¹çï¼ä»¥åå¨å®éåºç¨ä¸­å¦ä½è·åè¿äºç¨çäº¤äºä¿¡æ¯ãè¿å¯è½æ¯ä¸ä¸ªææï¼å°¤å¶æ¯å¨æ²¡æç´æ¥ä¼ æå¨è¾å¥çæåµä¸ã</li>
<li><strong>è®¡ç®ææ¬ (Computational Cost):</strong> Transformer æ¨¡åéå¸¸è®¡ç®ææ¬è¾é«ï¼å°¤å¶æ¯å¨å¤çé«åè¾¨çæé¿åºåæ°æ®æ¶ãé¢æµæ´ä¸ªè¿å¨åå¸èéåä¸å®ç°ï¼ä¹å¯è½å¢å è®¡ç®å¤ææ§ã</li>
<li><strong>å¯è§£éæ§çæ·±åº¦ (Depth of Interpretability):</strong> å°½ç®¡æè¦å£°ç§°æä¾äºâå¯è§£éçâè¡¨ç¤ºï¼ä½è¿ç§è§£éæ§å·ä½ä½ç°å¨ä½ç§ç¨åº¦ä¸ï¼ä»¥åæ¯å¦è½æä¾å æå±é¢ççè§£ï¼è¿éè¦è¿ä¸æ­¥çè®ºæç»èæ¥å¤æ­ã</li>
<li><strong>âç¨çäº¤äºâçéå¶ (Limitations of "Sparse Interactions"):</strong> æäºåºæ¯å¯è½éè¦æ´å¯éçè¾å¥æè½åç¡®é¢æµè¿å¨ï¼æèç¨çäº¤äºå¯è½æ æ³ææææå³é®çç©çä¿¡æ¯ãæ¨¡åå¨å¤çè¿äºæåµæ¶çè¡¨ç°å¦ä½ï¼å°ä¸æ¸æ¥ã</li>
<li><strong>ç©çå®å¾çæ¾å¼ç¼ç  (Explicit Encoding of Physics Laws):</strong> æè¦æ²¡ææåæ¨¡åæ¯å¦æ¾å¼å°ç¼ç äºç©çå®å¾ï¼æèæ¯å¦å®å¨éè¿æ°æ®é©±å¨çæ¹å¼å­¦ä¹ ãçº¯æ°æ®é©±å¨çæ¨¡åå¨é¢å¯¹æç«¯ææªè§è¿çç©çæ¡ä»¶æ¶ï¼å¯è½è¡¨ç°åºå±éæ§ã</li>
<li><strong>âä¸æ¸¸ä»»å¡âçèå´ (Scope of Downstream Tasks):</strong> æè¦æå°äºâå¯éäººè¸è¿å¨çæâãâé°æ¥ç©ä½è¿å¨ä¼°è®¡âåâç§»å¨é¨ä»¶åå²âï¼è¿äºä»»å¡è½ç¶å¤æ ·ï¼ä½ä»å±äºç¹å®èç´ãFPT å¨æ´å¹¿æ³ãæ´å¤æçç©çæ¨çä»»å¡ï¼ä¾å¦ï¼å¤ç©ä½ç¢°æãæµä½å¨åå­¦ï¼ä¸çè¡¨ç°å¦ä½ï¼ä»ééªè¯ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºææåºäºä¸ç§éå¸¸æåæ¯çæ¹æ³ï¼éè¿å³æ³¨è¿å¨åå¸åç¨çäº¤äºï¼ä¸ºç©çåºæ¯çè§£åäº¤äºå¼AIç³»ç»å¼è¾äºæ°çéè·¯ãå¶å¨æ³åè½ååå¤æ¨¡æçè§£æ¹é¢çä¼å¿ï¼ä½¿å¶æä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå¼å¾å³æ³¨çéè¦å·¥ä½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present the Flow Poke Transformer (FPT), a novel framework for
directly predicting the distribution of local motion, conditioned on sparse
interactions termed "pokes".</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12777v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12777v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12747v1'></a></p>
<h2 id="flashvsr-towards-real-time-diffusion-based-streaming-video-super-resolution"><a href="https://arxiv.org/abs/2510.12747v1">FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</a></h2>
<p><strong>Authors:</strong> Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have recently advanced video restoration, but applying them
to real-world video super-resolution (VSR) remains challenging due to high
latency, prohibitive computation, and poor generalization to ultra-high
resolutions. Our goal in this work is to make diffusion-based VSR practical by
achieving efficiency, scalability, and real-time performance. To this end, we
propose FlashVSR, the first diffusion-based one-step streaming framework
towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408
videos on a single A100 GPU by combining three complementary innovations: (i) a
train-friendly three-stage distillation pipeline that enables streaming
super-resolution, (ii) locality-constrained sparse attention that cuts
redundant computation while bridging the train-test resolution gap, and (iii) a
tiny conditional decoder that accelerates reconstruction without sacrificing
quality. To support large-scale training, we also construct VSR-120K, a new
dataset with 120k videos and 180k images. Extensive experiments show that
FlashVSR scales reliably to ultra-high resolutions and achieves
state-of-the-art performance with up to 12x speedup over prior one-step
diffusion VSR models. We will release the code, pretrained models, and dataset
to foster future research in efficient diffusion-based VSR.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºæåºäº FlashVSRï¼è¿æ¯é¦ä¸ªå®ç°å®æ¶æ§è½çæ©æ£æ¨¡åé©±å¨çåæ­¥æµå¼è§é¢è¶åè¾¨çï¼VSRï¼æ¡æ¶ãå®éè¿è§£å³ç°ææ©æ£æ¨¡åå¨ VSR åºç¨ä¸­é¢ä¸´çé«å»¶è¿ãè®¡ç®ææ¬åé«åè¾¨çæ³åè½åå·®ç­ææï¼ä½¿å¾æ©æ£æ¨¡åå¨å®é VSR åºæ¯ä¸­åå¾å¯è¡ãFlashVSR å¨åå¼  A100 GPU ä¸è½ä»¥çº¦ 17 FPS çéåº¦å¤ç 768x1408 è§é¢ï¼å¹¶å¨æ§è½ä¸è¶è¶äºç°æåæ­¥æ©æ£ VSR æ¨¡åï¼åæ¶å®ç°äºé«è¾¾ 12 åçå éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>FlashVSR çå³é®åæ°å¨äºå¶ç»åäºä¸ä¸ªäºè¡¥çææ¯ç¹ï¼ä»¥å®ç°æçãå¯æ©å±æ§åå®æ¶æ§è½ï¼</p>
<ul>
<li><strong>(i) è®­ç»åå¥½çä¸é¶æ®µè¸é¦ç®¡çº¿ (Train-friendly three-stage distillation pipeline)ï¼</strong> è¿ä½¿å¾æ¨¡åè½å¤è¿è¡æµå¼è¶åè¾¨çå¤çï¼æ¯å®ç°å®æ¶æ§è½çåºç¡ãè¸é¦éå¸¸ç¨äºå°å¤§åãå¤æçæ¨¡åï¼å¦åå§æ©æ£æ¨¡åï¼çç¥è¯è½¬ç§»å°æ´å°ãæ´å¿«çæ¨¡åä¸­ã</li>
<li><strong>(ii) å±é¨æ§çº¦æçç¨çæ³¨æå (Locality-constrained sparse attention)ï¼</strong> è¿ç§æºå¶æ¨å¨åååä½è®¡ç®ï¼åæ¶å¼¥åè®­ç»åæµè¯åè¾¨çä¹é´çå·®è·ãç¨çæ³¨æåéè¿åªå³æ³¨è¾å¥ä¸­çå³é®é¨åæ¥åå°è®¡ç®éï¼èå±é¨æ§çº¦æåå¯è½å©ç¨è§é¢å¸§ä¹é´çæ¶ç©ºå±é¨æ§ã</li>
<li><strong>(iii) å¾®åæ¡ä»¶è§£ç å¨ (Tiny conditional decoder)ï¼</strong> è¿ä¸ªå°åè§£ç å¨å¨ä¸çºç²éå»ºè´¨éçåæä¸å éäºéå»ºè¿ç¨ãè¿è¡¨æå®å¯è½æ¯ä¸ä¸ªè½»éçº§çæ¨¡åï¼è´è´£å°æ©æ£æ¨¡åçè¾åºè½¬æ¢ä¸ºæç»çé«åè¾¨çè§é¢å¸§ã</li>
</ul>
<p>æ­¤å¤ï¼ä¸ºäºæ¯æå¤§è§æ¨¡è®­ç»ï¼ä½èè¿æå»ºäºä¸ä¸ªæ°çæ°æ®é <strong>VSR-120K</strong>ï¼åå« 120k è§é¢å 180k å¾åï¼è¿æ¬èº«ä¹æ¯ä¸ä¸ªéè¦çè´¡ç®ï¼å°ä¿è¿è¯¥é¢åæªæ¥çç ç©¶ã</p>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<p>FlashVSR å¯¹è®¡ç®æºè§è§é¢åï¼ç¹å«æ¯è§é¢å¤çåçæé¢åï¼å·ææ¾èçæ½å¨å½±åï¼</p>
<ul>
<li><strong>æ¨å¨æ©æ£æ¨¡åå¨å®æ¶åºç¨ä¸­çè½å°ï¼</strong> è§£å³äºæ©æ£æ¨¡åå¨ VSR é¢åé¿æå­å¨çæçç¶é¢ï¼ä½¿å¶ä»çè®ºç ç©¶èµ°åå®éåºç¨ï¼ä¾å¦ç´æ­ãè§é¢ä¼è®®ãæ¸¸æç­ã</li>
<li><strong>è®¾å®æ°çæ§è½åºåï¼</strong> å¨å®æ¶æ§åé«åè¾¨ç VSR æ¹é¢ï¼FlashVSR å®ç°äº SOTA æ§è½åæ¾èçå éï¼ä¸ºåç»­ç ç©¶æ ç«äºæ°çç®æ ã</li>
<li><strong>ä¿è¿é«ææ©æ£æ¨¡åæ¶æç ç©¶ï¼</strong> å¶æåºçè¸é¦ç®¡çº¿ãç¨çæ³¨æåæºå¶åå¾®åè§£ç å¨ç­åæ°ï¼å°å¯åç ç©¶èæ¢ç´¢æ´é«æçæ©æ£æ¨¡åè®¾è®¡ã</li>
<li><strong>æä¾å¤§è§æ¨¡æ°æ®éï¼</strong> VSR-120K æ°æ®éçåå¸å°ä¸º VSR é¢åçè®­ç»åè¯ä¼°æä¾å®è´µçèµæºï¼å éè¯¥é¢åçåå±ã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å®æ¶è§é¢æµåªä½æå¡ï¼</strong> æé«ä½å¸¦å®½ä¸è§é¢çè§çä½éªï¼ä¾å¦å¨çº¿ç´æ­ãäºæ¸¸æã</li>
<li><strong>è§é¢ä¼è®®/è¿ç¨åä½ï¼</strong> æ¹åè§é¢éè¯è´¨éï¼å°¤å¶æ¯å¨ç½ç»æ¡ä»¶ä¸ä½³æ¶ã</li>
<li><strong>å®é²çæ§ï¼</strong> æåçæ§è§é¢çç»èï¼å¸®å©è¯å«ç®æ ã</li>
<li><strong>å»çå½±åï¼</strong> å¯¹ä½åè¾¨çå»çè§é¢è¿è¡å¢å¼ºï¼è¾å©è¯æ­ã</li>
<li><strong>èæç°å®/å¢å¼ºç°å® (VR/AR)ï¼</strong> å®æ¶çææ´é«è´¨éçè§è§åå®¹ï¼æåæ²æµ¸æã</li>
<li><strong>åå®¹åä½/åæå¶ä½ï¼</strong> å éè§é¢ç´ æçè¶åè¾¨çå¤çï¼æé«å·¥ä½æçã</li>
<li><strong>è¾¹ç¼è®¡ç®è®¾å¤ä¸çè§é¢å¤çï¼</strong> éçæ¨¡åæççæåï¼æªæ¥å¯è½å¨æ´ä½åèçè®¾å¤ä¸å®ç°é«è´¨é VSRã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>ç¡¬ä»¶ä¾èµæ§ï¼</strong> æè¦ä¸­æå°å¨âåä¸ª A100 GPUâä¸è¿è¡ï¼è½ç¶ A100 æ¯é«æ§è½ GPUï¼ä½å¯¹äºæ´å¹¿æ³çæ¶è´¹çº§ç¡¬ä»¶æè¾¹ç¼è®¾å¤ï¼å¶æ§è½è¡¨ç°ä»éè¿ä¸æ­¥éªè¯ãå®æ¶æ§è½å¯è½ä»ç¶åéäºé«ç«¯ç¡¬ä»¶ã</li>
<li><strong>ç¹å®åè¾¨ççæ§è½ï¼</strong> 17 FPS æ¯éå¯¹ 768x1408 è§é¢èè¨çãè½ç¶è®ºæå£°ç§°âå¯é å°æ©å±å°è¶é«åè¾¨çâï¼ä½å·ä½å¨æ´é«åè¾¨çä¸ç FPS è¡¨ç°åè´¨éæè¡¡ä»ä¸æç¡®ã</li>
<li><strong>âåæ­¥âçå«ä¹ï¼</strong> æè¦å¼ºè°âone-step streaming frameworkâåâprior one-step diffusion VSR modelsâãè¿å¯è½æå³çå®ä¸å¤æ­¥ï¼è¿­ä»£ï¼æ©æ£æ¨¡åç¸æ¯ï¼å¨çæè´¨éä¸å¯è½å­å¨ä¸å®çæè¡¡ï¼å°½ç®¡æè¦å£°ç§°âä¸çºç²è´¨éâãâåæ­¥âéå¸¸æå³çæ´å¿«çæ¨çéåº¦ï¼ä½ææ¶ä¼ä»¥çæè´¨éçå¾®å°ä¸éä¸ºä»£ä»·ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡æè¦æå°âå¼¥åè®­ç»-æµè¯åè¾¨çå·®è·âï¼ä½å¯¹äºè®­ç»æ°æ®ä¸­æªè§çæç«¯åºæ¯ãä¸ååç¼©ä¼ªå½±æç¹å®åå®¹ï¼å¦å¨ç»ãCGï¼çæ³åè½åï¼æè¦ä¸­æªè¯¦ç»è¯´æã</li>
<li><strong>æ¨¡åå¤ææ§ä¸é¨ç½²ï¼</strong> å°½ç®¡éè¿è¸é¦åç¨çæ³¨æåè¿è¡äºä¼åï¼ä½æ©æ£æ¨¡åæ¬èº«éå¸¸ä»æ¯ä¼ ç»æ¹æ³æ´å¤æãå¶é¨ç½²åç»´æ¤ææ¬ï¼å³ä½¿æ¯ä¼ååççæ¬ï¼å¯è½ä»é«äºéæ©æ£æ¨¡åã</li>
</ul>
<hr />
<p>æ»çæ¥è¯´ï¼FlashVSR æ¯ä¸é¡¹ä»¤äººå´å¥çå·¥ä½ï¼å®å¨å°æ©æ£æ¨¡åæ¨åå®æ¶è§é¢åºç¨æ¹é¢è¿åºäºéè¦ä¸æ­¥ï¼æææ¾èå éè¯¥é¢åçç ç©¶åå®éåºç¨ãå¶æ¹æ³è®ºä¸çåæ°åæ°æ®éçåå¸é½å°å¯¹ç¤¾åºäº§çç§¯æå½±åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To support large-scale training, we also construct VSR-120K, a new
dataset with 120k videos and 180k images.</li>
<li>Extensive experiments show that
FlashVSR scales reliably to ultra-high resolutions and achieves
state-of-the-art performance with up to 12x speedup over prior one-step
diffusion VSR models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12747v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12747v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12720v1'></a></p>
<h2 id="omni-captioner-data-pipeline-models-and-benchmark-for-omni-detailed-perception"><a href="https://arxiv.org/abs/2510.12720v1">Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</a></h2>
<p><strong>Authors:</strong> Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, Eng Siong Chng, Xie Chen</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CL, cs.CV, cs.MM, cs.SD</p>
<p><strong>Abstract:</strong></p>
<p>Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯å¯¹Ziyang Maç­äººæ°åçè®ºæâOmni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perceptionâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼Omni-Captionerï¼å¨æ¹ä½ç»èæç¥çå¤æ¨¡ææ°æ®ç®¡éãæ¨¡åä¸åºåæµè¯</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå¨æ¹ä½è¯­è¨æ¨¡åï¼OLMsï¼å¨å¤çé³é¢åè§é¢ä¿¡å·æ¶ï¼æè·ååç¡®æè¿°ç»ç²åº¦ç»èçè½åæéçé®é¢ãå·ä½èè¨ï¼ç ç©¶åç°ç°æOLMså¨ç»èæ°´å¹³åå¹»è§ï¼hallucinationï¼ä¹é´å­å¨åºæçâå±åå¢é¿âç°è±¡ï¼å³æè¿°è¶è¯¦ç»ï¼å¹»è§ä¹è¶å¤ãæ­¤å¤ï¼ç¼ºä¹ä¸é¨ç¨äºå¨æ¹ä½ç»èæç¥çåºåæµè¯ï¼ä½¿å¾è¯ä¼°åå¾ä¸ç¨³å®ãä½æä¸ä¸å¯é ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>Omni-Detective æ°æ®çæç®¡éï¼</strong> æåºäºä¸ç§ä»£çå¼ï¼agenticï¼æ°æ®çæç®¡éï¼éè¿éæå·¥å·è°ç¨ï¼å¦OCRãASRãMLLMï¼åæ¨¡æç¹å®è§å¯å¨ï¼èªä¸»çæé«åº¦è¯¦ç»ä½å¹»è§æå°çå¤æ¨¡ææ°æ®ãè¯¥ç®¡ééè¿è¿­ä»£æ¥è¯¢-è§å¯å¾ªç¯ï¼éæ­¥æ¶éææä¸ææ ¹æ®çç»èï¼å¹¶äº¤åéªè¯ç°æå£°æï¼æ¨å¨è§£è¦ç»èè·åä¸å¹»è§å¢é¿ã
*   <strong>Audio-Captioner å Omni-Captioner æ¨¡åï¼</strong> åºäºOmni-Detectiveçæçæ°æ®ï¼è®­ç»äºä¸¤ä¸ªå­å¹çææ¨¡åï¼ç¨äºçº¯é³é¢ç»èæç¥çAudio-Captioneråç¨äºé³è§é¢ç»èæç¥çOmni-Captionerãæ¨¡åéç¨ä¸¤é¶æ®µè¯¾ç¨å­¦ä¹ ç­ç¥ï¼é¦åå»ç»è§è§ç¼ç å¨ä»¥å¼ºå¶é³é¢å¯¹é½ï¼ç¶åèåä¼åä¸¤ç§æ¨¡æä»¥çæè¿è´¯ãè·¨æ¨¡æåè¯¦ç»çåè¿°ã
*   <strong>Omni-Cloze åºåæµè¯ï¼</strong> éå¯¹å¨æ¹ä½ç»èæç¥ç¼ºä¹ä¸ç¨åºåçé®é¢ï¼è®¾è®¡äºä¸ç§æ°é¢çå®å½¢å¡«ç©ºå¼ï¼cloze-styleï¼è¯ä¼°æ¹æ³ãOmni-Clozeéè¿å¤é¡¹éæ©é¢å½¢å¼çå®å½¢å¡«ç©ºï¼åå«âæªç»åºâï¼Not Givenï¼éé¡¹ä»¥åºåéæ¼åå¹»è§ï¼ç¡®ä¿äºè¯¦ç»é³é¢ãè§è§åé³è§é¢å­å¹è¯ä¼°çç¨³å®æ§ãæçåå¯é æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>Audio-Captioner çåè¶æ§è½ï¼</strong> å¨çº§èè¯ä¼°åè®®ä¸ï¼Audio-Captionerå¨MMAUåMMARç­ææå¼æºæ¨¡åä¸­è¡¨ç°æä½³ï¼è¶è¶äºGemini 2.5 Flashï¼å¹¶ä¸Gemini 2.5 Proæ§è½ç¸å½ã
*   <strong>Omni-Captioner çé¢åå°ä½ï¼</strong> å¨ç°æè¯¦ç»å­å¹åºåæµè¯ä¸­ï¼Omni-Captionerå¨VDCä¸åå¾äºæ°çæåè¿ï¼state-of-the-artï¼ç»æï¼å¹¶å¨video-SALMONN 2æµè¯éä¸å®ç°äºç»èåå¹»è§ä¹é´çæä½³æè¡¡ã
*   <strong>Omni-Detective çæææ§ï¼</strong> å®éªç»æè¡¨æï¼Omni-Detectiveå¨çæé«è´¨éè¯¦ç»å­å¹æ¹é¢éå¸¸ææï¼è½å¤å°ç»è-å¹»è§è¾¹çåå¤æ¨ç§»ï¼å¨ä¸ææ¯ä¾å¢å å¹»è§çæåµä¸æä¾æ´ä¸°å¯çæè¿°ã
*   <strong>Omni-Cloze çä¼è¶æ§ï¼</strong> Omni-Clozeå¨è¯ä¼°è¯¦ç»å­å¹æ¹é¢è¡¨ç°åºä¼è¶æ§ï¼å¹¶ä¸äººç±»åå¥½é«åº¦ä¸è´ï¼å¶ç¸å³ç³»æ°ï¼r=0.91ï¼é«äºVDCåvideo-SALMONN 2ãè¿éªè¯äºå¶è¯ä¼°è®¾è®¡çå¯é æ§åç¨³å®æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   å°½ç®¡è®ºææç¡®æ¢è®¨äºè¯¦ç»å­å¹ä¸­çå¹»è§é®é¢ï¼ä½å¶è¯ä¼°æ¹æ³æ æ³æ£æµææç±»åçå¹»è§ãç¹å«æ¯ï¼æ¨¡åè¾åºä¸è¾å¥å®å¨æ å³çåå®¹ï¼æ å³çæï¼ä»ç¶é¾ä»¥å¯é æµéã
*   å­å¨æ¨¡åé¢æµçç»èå®éä¸å­å¨äºé³è§é¢è¾å¥ä¸­ï¼ä½å´ç¼ºå¤±äºçå®åèæ°æ®çæåµï¼è¿ä½¿å¾å¹»è§è¯ä¼°åå¾å¤æã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å¼åæ´é²æ£çæ¹æ³æ¥å¤çå¹»è§è¯ä¼°ä¸­çâæ å³çæâé®é¢ï¼ç¹å«æ¯å¨æ¨¡åé¢æµçç»èå­å¨äºè¾å¥ä½ç¼ºå¤±äºåèæ°æ®çæåµã
*   è¿ä¸æ­¥æ¢ç´¢åå¼åè½å¤æä¾ç¨³å®æµéå¹¶éæåæ æ¨¡åå®éè½åçæ°åè¯ä¼°åè®®ã
*   ç»§ç»­æ¨å¨ç»ç²åº¦å¤æ¨¡ææç¥ç³»ç»çåå±ï¼ä½¿å¶æ´å å¯é åç²¾ç»ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark.</li>
<li>We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data.</li>
<li>On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset.</li>
<li>Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12720v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12720v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12709v1'></a></p>
<h2 id="sail-embedding-technical-report-omni-modal-embedding-foundation-model"><a href="https://arxiv.org/abs/2510.12709v1">SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</a></h2>
<p><strong>Authors:</strong> Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.IR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the
Douyin-Selected scenario. For the Douyin feed rank model, the match features
produced by SAIL-Embedding yield a +0.08% AUC gain.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Lin Linç­äººæ°åçè®ºæâSAIL-Embedding Technical Report: Omni-modal Embedding Foundation Modelâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æå¤æ¨¡æåµå¥æ¨¡åå¨å®éåºç¨ååä¸åºæ¯ä¸­é¢ä¸´çææï¼åæ¬ï¼æ¨¡ææ¯ææéãè®­ç»æºå¶ä¸ç¨³å®ä»¥åå·¥ä¸é¢åå·®è·ãå·ä½èè¨ï¼ç ç©¶é®é¢æ¯å¦ä½æå»ºä¸ä¸ªè½å¤å¤çä»»ææ¨¡æè¾å¥ï¼åæ¬ææ¬ãè§è§åé³é¢ï¼ï¼å¹¶è½æææ¯æå¤æ¨¡ææ£ç´¢ååç±»ä»»å¡çå¨æ¨¡æåµå¥åºç¡æ¨¡åï¼åæ¶ç¡®ä¿è®­ç»çé²æ£æ§ãå¯æ©å±æ§åæ³åè½åï¼å¹¶æåå¨æ¨èç³»ç»ä¸­çå®éä¸å¡ä»·å¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
SAIL-Embeddingæ¨¡åéè¿ä»¥ä¸å³é®åæ°åæ¹æ³è®ºè´¡ç®æ¥è§£å³ä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>å¨æ¨¡ææ¯æåç»ä¸è¡¨ç¤ºï¼</strong> SAIL-Embeddingæ¯ä¸ä¸ªå¨æ¨¡æåµå¥åºç¡æ¨¡åï¼è½å¤å¤çä»»æç»åçè§è§ãææ¬åé³é¢æ¨¡æè¾å¥ï¼å¹¶å°å¶æ å°å°ç»ä¸çåéç©ºé´ï¼ä»¥æ»¡è¶³å¤æ ·åçä¸å¡éæ±ã</li>
<li><strong>å¨æç¡¬è´ä¾ææï¼Dynamic Hard Negative Miningï¼ï¼</strong> å¼å¥å¨æç¡¬è´ä¾ææç­ç¥ï¼èªéåºå°ç¡®å®æ¯ä¸ªæ°æ®éçæä½³ç¸ä¼¼åº¦éå¼ï¼ä½¿æ¨¡åè½å¤ä¸æ³¨äºåºåå·ææææ§çè´ä¾ï¼ä»èå¢å¼ºæ¨¡åå¯¹é¢åç¹å®ç¥è¯ççè§£å¹¶åå°è¯¯åç±»é£é©ã</li>
<li><strong>èªéåºå¤æºæ°æ®å¹³è¡¡ï¼Adaptive Multi-Source Data Balancingï¼ï¼</strong> æåºèªéåºå ææ¡æ¶ï¼ç´æ¥ä»æ°æ®åå¸ä¸­å­¦ä¹ æ°æ®éç¹å®çéæ ·æéï¼èéä¾èµæå¨å¯åå¼éç½®ï¼ä»¥å¹³è¡¡æ°æ®è´¨éååå¸å¤æ ·æ§ï¼é²æ­¢è¿æåå¹¶æé«æ³åè½åã</li>
<li><strong>å¤é¶æ®µè®­ç»æ¹æ¡ï¼</strong><ul>
<li><strong>åå®¹æç¥æ¸è¿å¼è®­ç»ï¼Content-Aware Progressive Trainingï¼ï¼</strong> éæ­¥å¢å¼ºåµå¥å¯¹ä¸åä»»å¡çå¤å«è½ååå¤çæªè§åºæ¯çæ³åè½åï¼éè¿å©ç¨å¤æ ·åãè¯­ä¹ä¸°å¯çæ°æ®èµæºï¼ä½¿æ¨¡åææ¡å¨é¢çé¢åç¥è¯ã</li>
<li><strong>åä½æç¥æ¨èå¢å¼ºè®­ç»ï¼Collaboration-aware Recommendation Enhancement Trainingï¼ï¼</strong> éè¿åºåå°ç©åï¼sequence-to-itemï¼åIDå°ç©åï¼ID-to-itemï¼è¸é¦ç¥è¯ï¼å¹¶ææç¨æ·åå²å´è¶£ï¼ä½¿å¤æ¨¡æè¡¨ç¤ºéåºæ¨èåºæ¯ã</li>
</ul>
</li>
<li><strong>éæºä¸ä¸ååæ°æ®éé©±å¨çæ¨¡å¼å¹éï¼Stochastic Specialization and Dataset-Driven Pattern Matchingï¼ï¼</strong> æåºéæºä¸ä¸åè®­ç»ç­ç¥ï¼éè¿å¨æ¯æ¬¡è¿­ä»£ä¸­éæºéæ©åä¸ªæ°æ®éè¿è¡è®­ç»ï¼åå°æ¢¯åº¦æ¹å·®ï¼ç®åè¿­ä»£é»è¾ï¼å¹¶æé«æ¨¡åè®­ç»ççµæ´»æ§åæ³åè½åãæ°æ®éé©±å¨çæ¨¡å¼å¹éåç»ä¸äºåç§å¯¹æ¯ç®æ ï¼ä»¥å¤çå¼ææ¨¡æå¯ç¨æ§åä¸å¹³è¡¡é®é¢ã</li>
<li><strong>æ¶æè®¾è®¡ï¼</strong> å©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMï¼ä½ä¸ºæ ¸å¿æ¨çåéæéª¨å¹²ï¼å¹¶å¼å¥è§è§æç¥å¨ï¼Visual Perceiverï¼æ¨¡åè¿è¡ä»¤çç¼©åï¼ä»¥åéç¨CLAPæ¨¡åè¿è¡é³é¢ç¼ç ï¼ç¡®ä¿é«æå¤çåå¤æ¨¡æèåã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
å®éªç»æè¡¨æSAIL-Embeddingåå¾äºæ¾èçæ§è½æåï¼</p>
<ul>
<li><strong>æ£ç´¢ä»»å¡çSOTAæ§è½ï¼</strong> å¨å¤é¡¹åºåæ°æ®éä¸ï¼SAIL-Embeddingå¨ç©åå°ç©åï¼i2iï¼åæ¥è¯¢å°ç©åï¼q2iï¼æ£ç´¢ä»»å¡ä¸­åå®ç°äºæåè¿ï¼SOTAï¼çæ§è½ï¼è¶è¶äºCLIP-basedåVLM-basedæ¨¡åã</li>
<li><strong>å¨çº¿å®éªçä¸å¡ä»·å¼ï¼</strong> å¨Douyinæ¨èç³»ç»çå¨çº¿å®éªä¸­ï¼SAIL-Embeddingæ¾èæåäºç¨æ·çå½å¨æï¼LTï¼ï¼è¿æ¯æ¨èä½éªçå³é®ææ ãä¾å¦ï¼å¨Douyin-Selectedåºæ¯ä¸­ï¼7å¤©LTå¢çä¸º+0.158%ï¼14å¤©LTå¢çä¸º+0.144%ãå¯¹äºDouyinä¿¡æ¯æµæåæ¨¡åï¼SAIL-Embeddingçæçå¹éç¹å¾å¸¦æ¥äº+0.08%çAUCå¢çã</li>
<li><strong>æ¨¡ååç»ä»¶çå¿è¦æ§ï¼</strong> å¹¿æ³çæ¶èç ç©¶è¯å®äºææåºæ¨¡ååè®­ç»ç­ç¥çå¿è¦æ§ï¼ä¾å¦åä½æç¥æ¨èå¢å¼ºè®­ç»æ¾èæé«äºNMIãKendallç¸å³æ§åäº¤éææ ï¼å¹¶æåäºæ¨¡åå¨Gid-i2iåºåä¸çæ§è½ã</li>
<li><strong>æ³åè½ååéåºæ§ï¼</strong> æ¨¡åå¨ä¸åä»»å¡åè·¨é¢ååºæ¯ä¸­è¡¨ç°åºå¼ºå¤§çæ³åè½ååéåºæ§ï¼è¿å¾çäºå¨æ¨¡ææ¶æãæ°æ®éé©±å¨çæ¨¡å¼å¹éåéæºä¸ä¸åè®­ç»ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
è®ºæä¸­æ²¡ææç¡®æåºå½åå·¥ä½çå±éæ§ï¼ä½ä»æªæ¥ç ç©¶æ¹ååå¯¹SIDsçè®¨è®ºä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹è¿ç©ºé´ï¼</p>
<ul>
<li><strong>SIDsä¸å¯éåµå¥çä¿¡æ¯éï¼</strong> è®ºææå°SIDsï¼è¯­ä¹IDï¼æ¯å¯éåµå¥æ´å®¹æå¨åºäºè§åçæ¹æ³ä¸­ä½¿ç¨ï¼ä½å¯éåµå¥éå¸¸åå«æ´å¤ä¿¡æ¯ãå¦ä½ææå©ç¨å¯éåµå¥çå¨é¨ä¿¡æ¯æ¯ä¸ä¸ªå¼å¾æ¢ç´¢çæ¹åã</li>
<li><strong>æ¨èç³»ç»ä¸­çVLMéæï¼</strong> å°½ç®¡æ¨¡åå¨æ¨èåºæ¯ä¸­è¡¨ç°åºè²ï¼ä½è®ºæä»å°è¿ä¸æ­¥æ¢ç´¢å¦ä½å°è§è§è¯­è¨æ¨¡åï¼VLMï¼æ´æ·±å¥å°éæå°æ¨èç³»ç»ä¸­ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>å¢å¼ºVLMä¸æ¨èç³»ç»çéæï¼</strong> æ¢ç´¢è®­ç»ä¸æ¨èç®æ å¯¹é½çVLMï¼å¹¶æå»ºä¸ºæ¨èéèº«å®å¶ççæä»»å¡ï¼ä»¥ä½¿æ¨¡åå¨æ©æé¶æ®µè·å¾é¢åç¹å®ç¥è¯å¹¶å¢å¼ºæ¨èè½åã</li>
<li><strong>ä¼åè¡¨ç¤ºå­¦ä¹ ï¼</strong> å¨è¡¨ç¤ºå­¦ä¹ é¶æ®µï¼æ¨å¨éè¿æææ´å¤æ¥èªæ¨èä¿¡å·åè¡ä¸ºåé¦çéå¯¹æ°æ®ï¼æ´å¥½å°å°æ¨¡åè®­ç»ä¸æ¨èç®æ å¯¹é½ï¼ä»èå°ç¨æ·åå¥½æ³¨å¥å¤æ¨¡æè¡¨ç¤ºä¸­ã</li>
<li><strong>æ¨èåºæ¯ä¸­çç¡¬è´ä¾ææï¼</strong> è¿ä¸æ­¥ç ç©¶æ¨èåºæ¯ä¸­çç¡¬è´ä¾ææï¼ä»¥æé«è¡¨ç¤ºå­¦ä¹ çé²æ£æ§ã</li>
<li><strong>æ©å±å°æ´å¹¿æ³çä¸æ¸¸ä»»å¡ï¼</strong> å°SAIL-Embeddingæ¡æ¶æ©å±å°æ´å¹¿æ³çä¸æ¸¸ä»»å¡ï¼å¦è§é¢çè§£ãä¸ªæ§ååå®¹çæåè·¨é¢åç¥è¯è¿ç§»ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.</li>
<li>Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability.</li>
<li>Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12709v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12709v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.12603v1'></a></p>
<h2 id="reasoning-in-the-dark-interleaved-vision-text-reasoning-in-latent-space"><a href="https://arxiv.org/abs/2510.12603v1">Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</a></h2>
<p><strong>Authors:</strong> Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie</p>
<p><strong>Published:</strong> 2025-10-14</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chao Chenç­äººæ°åçè®ºæâReasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Spaceâçå¨é¢æè¦ã</p>
<hr />
<h3 id="reasoning-in-the-dark-interleaved-vision-text-reasoning-in-latent-space_1">è®ºææè¦ï¼Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼å¨æ§è¡å¤ææ¨çä»»å¡æ¶ï¼éå¸¸ä¾èµäºæ¾å¼çãéè¦å¤§éäººå·¥æ æ³¨çè§è§-ææ¬ä¸­é´æ¨çæ­¥éª¤ãè¿ç§æ¹æ³ä¸ä»å³å¨å¯éï¼èä¸å¼å¥äºæ¾èçæ¨çå»¶è¿ãè®ºææ¨å¨è§£å³å¦ä½æåMLLMsçå¤æ¨¡ææ¨çè½åï¼åæ¶åå°å¯¹æ¾å¼æ æ³¨çä¾èµå¹¶æé«æ¨çæççé®é¢ãå·ä½æ¥è¯´ï¼ç ç©¶é®é¢æ¯å¦ä½å¨æ½å¨ç©ºé´ä¸­å®ç°ææçè§è§-ææ¬äº¤éæ¨çï¼ä»¥åæç°ææ¹æ³çå±éæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäº<strong>äº¤éè§è§-ææ¬æ½å¨æ¨çï¼Interleaved Vision-Text Latent Reasoning, IVT-LRï¼</strong>æ¹æ³ï¼è¿æ¯é¦ä¸ªå¨å¤æ¨¡ææ½å¨ç©ºé´ä¸­å®ç°å®å¨å¤æ¨¡ææ½å¨æ¨ççæ¡æ¶ãå¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>æ½å¨ç©ºé´ä¸­çå¤æ¨¡ææ¨çï¼</strong> IVT-LRåè®¸ææ¬åè§è§ä¿¡æ¯å¨æ¨¡åçæ½å¨ç©ºé´ä¸­è¿è¡æ¨çï¼æ éçæä¸­é´çæ¾å¼ææ¬æå¾åãæ¯ä¸ªæ¨çæ­¥éª¤ç±ä¸¤é¨åç»æï¼æ¥èªåä¸æ­¥éª¤éèç¶æçâæ½å¨ææ¬âåä¸ç»å¨æéæ©çå¾ååµå¥ï¼âæ½å¨è§è§âï¼ã
*   <strong>å¨æè§è§ç¦ç¹ï¼</strong> æ½å¨è§è§é¨åéè¿åºäºæ³¨æååæ°éæ©æç¸å³çå¾ååµå¥ï¼å®ç°äºå¯¹è§è§ç¹å¾çå¨æèç¦ï¼ä»èå¨æ¨çè¿ç¨ä¸­æ´åå³é®è§è§ä¿¡æ¯ã
*   <strong>æ¸è¿å¼å¤é¶æ®µè®­ç»ç­ç¥ï¼</strong> ä¸ºäºææèåæ½å¨ææ¬åæ½å¨è§è§ç»ä»¶ï¼è®ºæå¼å¥äºä¸ç§æ¸è¿å¼å¤é¶æ®µè®­ç»ç­ç¥ãè¯¥ç­ç¥éæ­¥ç¨æ½å¨æ¨çæ­¥éª¤æ¿ä»£æ¾å¼CoTï¼Chain-of-Thoughtï¼æ­¥éª¤ï¼å¹¶å°çç£éä¸­å¨å©ä½çæªæ¥æ¾å¼æ­¥éª¤åæç»ç­æ¡ä¸ï¼ä»¥ç¡®ä¿åç¡®æ¨çãè¿ç§æ¹æ³åå°äºå¯¹ä¸­é´è§è§æ¨çæ­¥éª¤æ¾å¼æ æ³¨çéæ±ï¼å¹¶æé«äºæ¨çæçã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
IVT-LRå¨MÂ³CoTåScienceQAç­æææ§è§è§é®ç­åºåä¸è¿è¡äºå¹¿æ³å®éªï¼ç»ææ¾ç¤ºï¼
*   <strong>æ¾èçåç¡®æ§æåï¼</strong> IVT-LRå¨åç¡®æ§æ¹é¢å¹³åæåäº5.45%ï¼å¨MÂ³CoTåScienceQAä¸åè¶è¶äºææåºçº¿æ¹æ³ï¼åæ¬æå¼ºçChain-of-Focusï¼æåå¹åº¦å¨5%å°7.5%ä¹é´ãè¿è¡¨ææ½å¨ç©ºé´ä¸­çè·¨æ¨¡æäº¤äºæ´ä¸ºææï¼å¢å¼ºäºå¤æ¨¡ææ¨çè½åã
*   <strong>æ¨çæçå¤§å¹æé«ï¼</strong> IVT-LRçæ¨çéåº¦æ¯ç°ææ¹æ³å¿«5åä»¥ä¸ãå®å°èªåå½æ­¥éª¤çæ°éåå°äºè³å°9åï¼æ¾èéä½äºæ¨çå»¶è¿ï¼åæ¶ä¿æäºé«åç¡®æ§ãè¿å¾çäºå¨æ½å¨ç©ºé´ä¸­è¿è¡æ¨çï¼é¿åäºçæåé¿æ¾å¼æ¨çè¿ç¨çéè¦ã
*   <strong>æ½å¨æ¨ççæææ§ï¼</strong> æ¶èç ç©¶è¯å®äºæ½å¨ææ¬åæ½å¨è§è§ç»ä»¶çå¿è¦æ§ï¼ä¸¤èé½å¯¹æ¨¡åæ§è½è³å³éè¦ãæ½å¨è§è§çé¿åº¦å¢å ä¼æé«åç¡®æ§ï¼è¡¨ææ´ä¸°å¯çè§è§çº¿ç´¢æå©äºå¤ææ¨çã
*   <strong>å¨ææ³¨æåæºå¶ï¼</strong> æ½å¨æ¨çæ¨¡å¼ä¸ï¼æ³¨æåæ¯çåç°ä¸éè¶å¿ï¼ä»æ½å¨è§è§è½¬åæ½å¨ææ¬ï¼ï¼æ³¨æåç¦ç¹éæ¸éä¸­ï¼è¿ä¸äººç±»è§£å³é®é¢çè¿ç¨ç¸ä¼¼ï¼è¡¨ææ¨¡åè½å¤ææè¿æ»¤åæç¼å¤æ¨¡æä¿¡æ¯ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é¢å¤ä»¤ççå¼å¥ï¼</strong> æ½å¨è§è§çèªéåºéæ©ä¸å¯é¿åå°å¼å¥äºå°éåºå®çé¢å¤ä»¤çãå°½ç®¡è¿äºä»¤çå¨åé¨å¤çèéå¤é¨çæï¼ç¡®ä¿äºæç»æ¨çéåº¦å¨åç¡®æ§é«çè®¾ç½®ä¸ä»æ¯æä½³ï¼ä½è¿ä»æ¯ä¸ä¸ªéè¦èèçå ç´ ã
*   <strong>è®­ç»å¤ææ§ï¼</strong> IVT-LRéè¦ä¸é¨çå¤é¶æ®µè®­ç»è¯¾ç¨ï¼è¿ä½¿å¶æ¯ç®åçåºäºæç¤ºçæ¹æ³æ´ä¸ºå¤æãç¶èï¼è®ºæè®¤ä¸ºè¿ç§å¤ææ§æ¯å¼å¾çæèµï¼å ä¸ºå®å¸¦æ¥äºå·¨å¤§çåç¡®æ§åæçæåï¼ä¸æéçè®­ç»èµæºåæ¶é´ç¸å¯¹éä¸­ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨ææ½å¨æ­¥éª¤ï¼</strong> æªæ¥å·¥ä½å¯ä»¥æ¢ç´¢æ´å¨æçè§è§æ½å¨æ¨çæ¹å¼ï¼ä¾å¦æ ¹æ®é®é¢çå¤ææ§èªéåºå°ç¡®å®æä½³æ½å¨æ­¥éª¤æ°éï¼èä¸æ¯ä¾èµåºå®çé¶æ®µæ°ã
*   <strong>æ´å¹¿æ³çåºç¨ï¼</strong> å°IVT-LRæ¹æ³æ©å±å°çº¯æ¨çä¹å¤çæ´å¹¿æ³çåºåå¤æ¨¡æä»»å¡ï¼åæ¬è§ååå¨æç¯å¢ä¸­çå¤æå³ç­å¶å®ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºå¤æ¨¡ææ¨çé¢åå¼å¥äºä¸ç§æ°é¢ä¸é«æçèå¼ï¼éè¿å¨æ½å¨ç©ºé´ä¸­äº¤éè¿è¡è§è§åææ¬æ¨çï¼æ¾èæåäºMLLMsçæ§è½åæçï¼ä¸ºæªæ¥æå»ºæ´æºè½ãæ´å·æç¥åçè§è§-è¯­è¨æ¨¡åå¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency.</li>
<li>To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.12603v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.12603v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-15 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
