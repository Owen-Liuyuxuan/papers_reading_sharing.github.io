<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-24 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-23/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-26/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-24">Arxiv Computer Vision Papers - 2025-09-24</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#xai-cv-an-overview-of-explainable-artificial-intelligence-in-computer-vision" class="nav-link">xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#qwen3-omni-technical-report" class="nav-link">Qwen3-Omni Technical Report</a>
                </li>
                <li class="nav-item">
                    <a href="#automatic-intermodal-loading-unit-identification-using-computer-vision-a-scoping-review" class="nav-link">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a>
                </li>
                <li class="nav-item">
                    <a href="#volsplat-rethinking-feed-forward-3d-gaussian-splatting-with-voxel-aligned-prediction" class="nav-link">VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</a>
                </li>
                <li class="nav-item">
                    <a href="#overlaybench-a-benchmark-for-layout-to-image-generation-with-dense-overlaps" class="nav-link">OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps</a>
                </li>
                <li class="nav-item">
                    <a href="#adversarially-refined-vq-gan-with-dense-motion-tokenization-for-spatio-temporal-heatmaps" class="nav-link">Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps</a>
                </li>
                <li class="nav-item">
                    <a href="#convis-bench-estimating-video-similarity-through-semantic-concepts" class="nav-link">ConViS-Bench: Estimating Video Similarity Through Semantic Concepts</a>
                </li>
                <li class="nav-item">
                    <a href="#vision-free-retrieval-rethinking-multimodal-search-with-textual-scene-descriptions" class="nav-link">Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions</a>
                </li>
                <li class="nav-item">
                    <a href="#rose-robust-self-supervised-stereo-matching-under-adverse-weather-conditions" class="nav-link">RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions</a>
                </li>
                <li class="nav-item">
                    <a href="#kamera-enhancing-aerial-surveys-of-ice-associated-seals-in-arctic-environments" class="nav-link">KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-24">Arxiv Computer Vision Papers - 2025-09-24</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹æ¨æä¾ç Arxiv è®¡ç®æºè§è§è®ºæåè¡¨çæ¯æ¥æ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>æ¯æ¥ Arxiv è®¡ç®æºè§è§è®ºææ§è¡æè¦ (2025-09-23)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong></p>
<p>ä»å¤©çè®ºææ¶µçäºè®¡ç®æºè§è§é¢åçå¤ä¸ªåæ²¿æ¹åï¼åç°åºä»¥ä¸å ä¸ªä¸»è¦è¶å¿ï¼</p>
<ol>
<li><strong>å¤æ¨¡æä¸å¤§æ¨¡åæç»­æ¼è¿ï¼</strong> ä»¥ Qwen3-Omni ä¸ºä»£è¡¨çå¤§åå¤æ¨¡ææ¨¡åå¨éç¨è½åä¸æç»­çªç ´ï¼é¢ç¤ºçæ´å¼ºå¤§çæç¥ä¸çè§£è½åã</li>
<li><strong>å¯è§£éæ§ä¸é²æ£æ§æ¥çåéè§ï¼</strong> éç AI åºç¨çæ·±å¥ï¼å¯¹æ¨¡åå³ç­ççè§£ï¼XAI-CVï¼åå¨å¤æç¯å¢ï¼å¦æ¶å£å¤©æ°ï¼ä¸çé²æ£æ§ï¼RoSeï¼æä¸ºå³é®ç ç©¶ç¹ã</li>
<li><strong>3D è§è§ä¸æ°é¢è¡¨ç¤ºæ¹æ³ï¼</strong> 3D Gaussian Splatting åå¶åä½ï¼VolSplatï¼ç»§ç»­æ¯ç­é¨æ¹åï¼æ¢ç´¢æ´é«æãé«è´¨éç 3D éå»ºä¸æ¸²æã</li>
<li><strong>ç¹å®åºç¨åºæ¯çæ·±åº¦æ¢ç´¢ï¼</strong> è®ºææ¶µçäºä»å·¥ä¸èªå¨åï¼Intermodal Loading Unit Identificationï¼ãåæåå®¹çæï¼Layout-to-Imageï¼ãè§é¢åæï¼Video Similarityï¼å°ç¯å¢çæµï¼Arctic Seals Surveyï¼ç­å¤ä¸ªåç´é¢åï¼æ¾ç¤ºåºè®¡ç®æºè§è§ææ¯å¨è§£å³å®éé®é¢ä¸­çå¹¿æ³åºç¨ã</li>
<li><strong>åºåæµè¯ä¸è¯ä¼°ï¼</strong> å¤ä¸ªæ°åºåï¼OverLayBench, ConViS-Benchï¼çæåºï¼åæ äºç¤¾åºå¯¹æ´å¨é¢ãæ´å·æææ§çè¯ä¼°æ åçéæ±ã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>Qwen3-Omni Technical Report (Jin Xu et al.):</strong> è¿ç¯ææ¯æ¥åæ çæ¯ä»æ¥æåå³æ³¨çè®ºæä¹ä¸ãä½ä¸ºå¤§åå¤æ¨¡ææ¨¡åçææ°è¿å±ï¼å®å¯è½å¨éç¨è§è§çè§£ãè·¨æ¨¡æäº¤äºåå¤ä»»å¡å¤çæ¹é¢å¸¦æ¥æ¾èæåï¼å¯¹æ´ä¸ªé¢åå·ææ·±è¿å½±åã</li>
<li><strong>VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction (Weijie Wang et al.):</strong> å¨ 3D Gaussian Splatting é¢åï¼è¿ç¯è®ºæéè¿å¼å¥ Voxel-Aligned Prediction éæ°æèååä¼ æ­ï¼ææå¨æ¸²æè´¨éåæçä¸åå¾æ°ççªç ´ï¼æ¯ 3D è§è§ç ç©¶çéè¦è¿å±ã</li>
<li><strong>xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision (Nguyen Van Tu et al.):</strong> è¿ç¯ç»¼è¿°è®ºæå¯¹äºçè§£å½åè®¡ç®æºè§è§é¢åå¯è§£é AI çç°ç¶ãææåæªæ¥æ¹åè³å³éè¦ãå¨ AI ä¼¦çåä¿¡ä»»æ¥çéè¦çèæ¯ä¸ï¼å¶ä»·å¼ä¸è¨èå»ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>å¤æ¨¡æå¤§æ¨¡åä¸éç¨æºè½ï¼</strong> Qwen3-Omni çåºç°è¿ä¸æ­¥å·©åºäºå¤æ¨¡æå¤§æ¨¡åä½ä¸ºæªæ¥ AI æ ¸å¿çåå±æ¹åï¼å¶éç¨æ§å°æ¯æªæ¥ç ç©¶çéç¹ã</li>
<li><strong>é«æä¸é«è´¨éç 3D è¡¨ç¤ºä¸æ¸²æï¼</strong> VolSplat ç­å·¥ä½è¡¨æï¼å¨ 3D Gaussian Splatting çåºç¡ä¸ï¼å¦ä½è¿ä¸æ­¥ä¼åå¶ç»æãæé«æçåæ¸²æè´¨éä»æ¯æ´»è·çç ç©¶é¢åã</li>
<li><strong>ææ¬é©±å¨çè§è§æ£ç´¢ä¸çæï¼</strong> "Vision-Free Retrieval" å "Layout-to-Image Generation" å¼ºè°äºææ¬ä½ä¸ºæ ¸å¿åªä»å¨è§è§ä»»å¡ä¸­çå¼ºå¤§æ½åï¼é¢ç¤ºçæ´æºè½ãæ´çµæ´»çè·¨æ¨¡æäº¤äºã</li>
<li><strong>æ¶å£ç¯å¢ä¸çé²æ£æ§ï¼</strong> RoSe ä¸æ³¨äºæ¶å£å¤©æ°ä¸çç«ä½å¹éï¼å¸æ¾äºå¨éçæ³æ¡ä»¶ä¸ä¿ææ¨¡åæ§è½çéè¦æ§ï¼è¿å¯¹äºèªå¨é©¾é©¶ãæºå¨äººç­å®éåºç¨è³å³éè¦ã</li>
</ul>
<p><strong>å»ºè®®éè¯»çè®ºæï¼</strong></p>
<p>å¯¹äºå¸æå¨é¢äºè§£ææ°è¿å±çç ç©¶äººåï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ol>
<li><strong>Qwen3-Omni Technical Report (Jin Xu et al.):</strong> äºè§£å¤§åå¤æ¨¡ææ¨¡åçææ°è½ååææ¯ç»èã</li>
<li><strong>xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision (Nguyen Van Tu et al.):</strong> ææ¡å¯è§£é AI å¨è®¡ç®æºè§è§ä¸­çå¨è²åæªæ¥è¶å¿ã</li>
<li><strong>VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction (Weijie Wang et al.):</strong> å¦ææ¨å³æ³¨ 3D è§è§åæ°é¢çåºæ¯è¡¨ç¤ºæ¹æ³ã</li>
<li><strong>RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions (Yun Wang et al.):</strong> å¦ææ¨å¯¹æ¶å£ç¯å¢ä¸çé²æ£æ§è§è§æç¥æå´è¶£ï¼ç¹å«æ¯èªå¨é©¾é©¶ææºå¨äººé¢åã</li>
<li><strong>OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps (Bingnan Li et al.):</strong> å¦ææ¨ä»äºçæå¼ AI æå¾ååæé¢åï¼è¿ä¸ªæ°åºåå¯è½æä¾æ°çç ç©¶æ¹åã</li>
</ol>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éç­éåºæç¸å³çè®ºæãè¯¦ç»åå®¹ä»éæ¥éåæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.18913v1">xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision</a></li>
<li><a href="#2509.17765v1">Qwen3-Omni Technical Report</a></li>
<li><a href="#2509.17707v1">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></li>
<li><a href="#2509.19297v1">VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</a></li>
<li><a href="#2509.19282v1">OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps</a></li>
<li><a href="#2509.19252v1">Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps</a></li>
<li><a href="#2509.19245v1">ConViS-Bench: Estimating Video Similarity Through Semantic Concepts</a></li>
<li><a href="#2509.19203v1">Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions</a></li>
<li><a href="#2509.19165v1">RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions</a></li>
<li><a href="#2509.19129v1">KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.18913v1'></a></p>
<h2 id="xai-cv-an-overview-of-explainable-artificial-intelligence-in-computer-vision"><a href="https://arxiv.org/abs/2509.18913v1">xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision</a></h2>
<p><strong>Authors:</strong> Nguyen Van Tu, Pham Nguyen Hai Long, Vo Hoai Viet</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâxAI-CV: An Overview of Explainable Artificial Intelligence in Computer Visionâçå¨é¢æè¦ï¼ç¨ä¸­æåæï¼</p>
<p><strong>è®ºææè¦ï¼xAI-CVï¼è®¡ç®æºè§è§ä¸­å¯è§£éäººå·¥æºè½çæ¦è¿°</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
æ·±åº¦å­¦ä¹ æ¨¡åå¨è®¡ç®æºè§è§ä»»å¡ä¸­åå¾äºæåè¿çæ§è½ï¼ä½å¶âé»ç®±âç¹æ§ä½¿å¾å³ç­è¿ç¨é¾ä»¥è§£éï¼è¿å¨å³é®åºç¨ä¸­å¼åäºå¯¹å¯é æ§çæå¿§ãæ¬ææ¨å¨è§£å³è¿ä¸ææï¼éè¿å¯¹å¯è§£éäººå·¥æºè½ï¼xAIï¼å¨è§è§æç¥ä»»å¡ä¸­çåç§ä»£è¡¨æ§æ¹æ³è¿è¡ç³»ç»æ§æ¦è¿°ï¼ä¸ºäººç±»çè§£AIæ¨¡åå¦ä½å¤çåå³ç­æä¾æ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬æçä¸»è¦è´¡ç®å¨äºå¯¹xAIé¢ååç§ä¸»è¦æ¹æ³çç»æååæåç»¼åï¼éæäºå®ä»¬çåºå±æºå¶ãä¼å¿ãå±éæ§ä»¥åè¯ä¼°ææ ãè¿åç§æ¹æ³æ¯ï¼
*   <strong>æ¾èæ§å¾ï¼Saliency Mapsï¼ï¼</strong> åç­âæ¨¡åçåªéï¼âçé®é¢ï¼éè¿ç­åå¾çªåºå¾åä¸­éè¦çåç´ çº§åºåã
*   <strong>æ¦å¿µç¶é¢æ¨¡åï¼Concept Bottleneck Models, CBMï¼ï¼</strong> åç­âæ¨¡åå¨æèä»ä¹ï¼âçé®é¢ï¼éè¿å¼ºå¶æ¨¡åéè¿äººç±»å¯çè§£çé«çº§æ¦å¿µè¿è¡æ¨çã
*   <strong>åºäºååçæ¹æ³ï¼Prototype-based methodsï¼ï¼</strong> åç­âè¿ä¸ªæ°æ®ååªä¸ªæ¡ä¾ï¼âçé®é¢ï¼éè¿ä¸ä»æ°æ®éä¸­å­¦ä¹ å°çååè¿è¡æ¯è¾æ¥è§£éé¢æµã
*   <strong>æ··åæ¹æ³ï¼Hybrid approachesï¼ï¼</strong> ç»åä¸è¿°æ¹æ³ï¼ä»¥å©ç¨åèªçä¼å¿ï¼æä¾æ´å¨é¢ãå¯é åçµæ´»çè§£éã</p>
<p>è®ºæè¯¦ç»åæäºæ¯ç§æ¹æ³çæ¼è¿é¶æ®µãå®éåºç¨ä»¥åè¯ä¼°ææ ï¼åæ¬AOPCãçµãå é¤/æå¥ãæåæ¸¸æãä»»å¡åç¡®æ§ãæ¦å¿µåç¡®æ§ãå¯å¹²é¢æ§æµè¯ãæ¾èæ§å¾å¯¹é½ãå®éæ£æ¥ãCGIMãCEMãCLMä»¥åTCAVåæ°ç­ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èæ§å¾</strong>ä½ä¸ºåºç¡å·¥å·ï¼è½å¤è¯å«å¯¹é¢æµå½±åæå¤§çå¾ååºåï¼æ¯è°è¯åæ£æµèåç¹å¾çè¯æ­æ­¥éª¤ã
*   <strong>æ¦å¿µç¶é¢æ¨¡å</strong>å¼å¥äºå¹²é¢è½åï¼åè®¸äººç±»ä¸æ¨¡åäº¤äºãçº æ­£éè¯¯å¹¶æµè¯å æåè®¾ï¼è¿å¯¹äºå¨ä¸ä¸é¢åå»ºç«ä¿¡ä»»è³å³éè¦ã
*   <strong>åºäºååçæ¹æ³</strong>éè¿ååç¤ºä¾è¿è¡è§£éï¼æ­ç¤ºäºæ¨¡åå­¦ä¹ å°çæ°æ®åå¸å¹¶è¯å«è¾¹çæ¡ä¾ï¼å¶æ¨çæ¹å¼æ´æ¥è¿äººç±»æç»´ã
*   <strong>æ··åæ¹æ³</strong>éè¿ç»åä¸åææ¯çä¼å¿ï¼æä¾äºæ´å¨é¢çè§£éï¼ä¾å¦å°æ¾èæ§å¾çç©ºé´å®ä½è½åä¸CBMçè¯­ä¹æ¨çç¸ç»åï¼åæ¶åç­âåªéâåâä¸ºä»ä¹âçé®é¢ï¼ä»èæé«äºæ´ä½å¯é æ§ã</p>
<p>è¿äºæ¹æ³å±åä¸ºçè§£æ¨¡åè¡ä¸ºæä¾äºå¤æ¹é¢çå·¥å·åï¼æ å¿çxAIé¢åä»åç­âæ¨¡åçåªéï¼âçåºæ¬é®é¢ï¼åå±å°è§£å³âæ¨¡åç¨ä»ä¹æ¦å¿µè¿è¡æ¨çï¼âç­æ´å¤ææ¥è¯¢çæ¼è¿ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡xAIåå¾äºæ¾èè¿å±ï¼ä½ä»é¢ä¸´ç³»ç»æ§ææï¼
*   <strong>è§£éçå¿ å®æ§åé²æ£æ§ï¼</strong> è§£éæ¯å¦çæ­£åæ äºæ¨¡åçåé¨æ¨çè¿ç¨ä»æ¯ä¸ä¸ªä¸»è¦é®é¢ãæ¾èæ§å¾å¯è½ä¸ç¨³å®ï¼CBMä¹å¯è½å­¦ä¹ âæ·å¾âèéå¿ å®å°ä»£è¡¨äººç±»å®ä¹çæ¦å¿µã
*   <strong>è¯ä¼°åæ ååé®é¢ï¼</strong> ç¼ºä¹æ ååææ ååºåæ°æ®éï¼ä½¿å¾éåâå¥½âçè§£éåå¾å°é¾ï¼é¾ä»¥å¬å¹³æ¯è¾ä¸åæ¹æ³ã
*   <strong>å¯¹äººç±»çä¾èµåå¯æ©å±æ§ï¼</strong> CBMååºäºååçæ¹æ³éå¸¸éè¦å¤§éçäººå·¥åä¸æ¥å®ä¹æ¦å¿µæè§£éååï¼è¿éå¶äºå®ä»¬çå¯æ©å±æ§ã
*   <strong>æ¾èæ§å¾çå±éæ§ï¼</strong> ç»æä¸ç¨³å®ãéåå°é¾ãå­å¨è¯¯è§£é£é©ãç¼ºä¹æ¦å¿µå±é¢è§£éã
*   <strong>CBMçå±éæ§ï¼</strong> æ¦å¿µå®ä¹è´æéãå­å¨æ·å¾å­¦ä¹ é£é©ãå¯æ©å±æ§ææã
*   <strong>åºäºååæ¹æ³çå±éæ§ï¼</strong> éè¦ä¿®æ¹æ¨¡åæ¶æãé¾ä»¥å­¦ä¹ ææä¹çååãè®¡ç®ææ¬é«ã
*   <strong>æ··åæ¹æ³çå±éæ§ï¼</strong> å®ç°ãè®¡ç®åè§£éç»æçå¤ææ§é«ãæ ååè¯ä¼°å°é¾ãéè¦ç¨æ·æå¥å¤§éç²¾åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ç³»ç»æ§ææï¼æªæ¥çç ç©¶æ¹ååæ¬ï¼
*   <strong>ä»åºäºç¸å³æ§çè§£éè½¬åå ææ¨çï¼</strong> æªæ¥çæ¹æ³éè¦è½å¤çæåäºå®è§£éï¼ä»¥æ¥æé©±å¨æ¨¡åå³ç­ççæ­£å æå ç´ ã
*   <strong>å»ºç«å¨é¢åæ ååçè¯ä¼°æ¡æ¶ï¼</strong> éè¦åå«å¿ å®æ§åé²æ£æ§çè®¡ç®åº¦éï¼å¹¶ç»åäººæºäº¤äºç ç©¶æ¥è¯ä¼°è§£éå¯¹æç»ç¨æ·çæç¨æ§åæææ§ã
*   <strong>å¼åè½å¤èªå¨åç°ææä¹æ¦å¿µåååçæ¹æ³ï¼</strong> æ¨å¨æé«å¯æ©å±æ§å¹¶åå°å¯¹é¢åä¸å®¶çä¾èµã
*   <strong>æå»ºååæ§çæ··åæ¨¡åï¼</strong> è½å¤åè°ç©ºé´åç¡®æ§ä¸è¯­ä¹ä¸°å¯æ§ï¼å°xAIä»å®éªå®¤è¯æ­å·¥å·è½¬åä¸ºçå®ä¸çå³é®AIç³»ç»ä¸­ä¸å¯æç¼ºçä¼ä¼´ã</p>
<p>æ»ä¹ï¼æ¬ææä¾äºä¸ä¸ªæ¡æ¶ï¼å¸®å©ç ç©¶äººååå®è·µèé©¾é©­å½åçxAIæ ¼å±ï¼å¹¶ååºææºçå³ç­ï¼æ¨å¨xAIåæå»ºçæ­£å¼å¾ä¿¡èµçç³»ç»åå±ï¼ä»¥å®ç°AIå¨å³é®é¢åçå®å¨åè´è´£ä»»é¨ç½²ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.18913v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.18913v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17765v1'></a></p>
<h2 id="qwen3-omni-technical-report"><a href="https://arxiv.org/abs/2509.17765v1">Qwen3-Omni Technical Report</a></h2>
<p><strong>Authors:</strong> Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV, eess.AS</p>
<p><strong>Abstract:</strong></p>
<p>We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾ãQwen3-Omni Technical Reportãçå¨é¢æè¦ã</p>
<hr />
<h3 id="qwen3-omni-technical-report_1">ãQwen3-Omni Technical Reportãæè¦</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå¤æ¨¡æå¤§æ¨¡åæ®éå­å¨çæ¨¡æé´æ§è½æè¡¡é®é¢ï¼å³å¨æåæä¸æ¨¡ææ§è½æ¶ï¼å¾å¾ä¼´éçå¶ä»æ¨¡æçæ§è½ä¸éãQwen3-Omniè´åäºæå»ºä¸ä¸ªåä¸çå¤æ¨¡ææ¨¡åï¼è½å¤å¨ææ¬ãå¾åãé³é¢åè§é¢ç­å¤ç§æ¨¡æä¸åæ¶ä¿ææåè¿çæ§è½ï¼ä¸ä¸åçä»»ä½æ§è½éåï¼å¹¶æ¾èå¢å¼ºè·¨æ¨¡ææ¨çåäº¤äºè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
Qwen3-Omniå¨Qwen2.5-OmniçThinker-Talkeræ¶æåºç¡ä¸è¿è¡äºäºé¡¹å³é®åçº§ï¼å®ç°äºå¶åè¶æ§è½ï¼
*   <strong>MoEæ¶æåçº§ï¼</strong> ThinkeråTalkerååçº§ä¸ºMoEï¼Mixture-of-Expertsï¼æ¶æï¼ä»¥æ¯æé«å¹¶ååå¿«éæ¨çã
*   <strong>AuTé³é¢ç¼ç å¨ï¼</strong> å¼å¥äºä»å¤´è®­ç»çAuTï¼Audio Transformerï¼ç¼ç å¨ï¼åä»£äºWhisperé³é¢ç¼ç å¨ï¼å¨2000ä¸å°æ¶ççç£é³é¢æ°æ®ä¸è®­ç»ï¼çææ´å¼ºçéç¨é³é¢è¡¨ç¤ºï¼å¹¶éç¨åçº§çªå£æ³¨æåæºå¶å®ç°å®æ¶é¢å¡«åç¼å­ã
*   <strong>å¤ç æ¬è¯­é³çæï¼</strong> éç¨å¤ç æ¬è¡¨ç¤ºåå¤è½¨ç¼è§£ç å»ºæ¨¡ï¼Talkeréè¿MTPæ¨¡åèªåå½é¢æµå¤ä¸ªç æ¬å±ï¼ä»¥å¿ å®å°å»ºæ¨¡å¤æ ·åçå£°é³ãå¯è¯­è¨çº¿ç´¢åå£°å­¦ç°è±¡ã
*   <strong>è½»éçº§ConvNetæ³¢å½¢åæï¼</strong> æ³¢å½¢çæé¶æ®µç¨è½»éçº§å æConvNetåä»£äºè®¡ç®å¯éåçåçº§æ©æ£æ¨¡åï¼DiTï¼ï¼å®ç°äºä»ç¬¬ä¸ä¸ªç¼è§£ç å¸§å¼å§çæµå¼åæï¼æ¾èéä½äºæ¨çå»¶è¿åè®¡ç®ææ¬ã
*   <strong>ä½å»¶è¿æµå¼äº¤äºï¼</strong> è¾å¥åè¾åºé³é¢ç çéä½è³12.5 Hzï¼è¾åºç¼è§£ç å¨æ¯æåå¸§å³æ¶è¯­é³åæï¼å¨å·å¯å¨è®¾ç½®ä¸å®ç°äº234æ¯«ç§ççè®ºç«¯å°ç«¯é¦åå»¶è¿ã
*   <strong>æ¾å¼å¤æ¨¡ææ¨çï¼</strong> å¼å¥äºä¸ä¸ªâThinkingâæ¨¡åï¼è½å¤æ¾å¼å°å¯¹æ¥èªä»»ä½æ¨¡æçè¾å¥è¿è¡æ¨çï¼è¿ä¸æ­¥å¢å¼ºäºå¤æ¨¡ææ¨çè½åã
*   <strong>é³é¢å­å¹æ¨¡åï¼</strong> éè¿å¾®è°Qwen3-Omni-30B-A3Bï¼å¾å°äºQwen3-Omni-30B-A3B-Captionerï¼è½å¤ä¸ºä»»æé³é¢è¾å¥çæè¯¦ç»ãä½å¹»è§çå­å¹ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>SOTAæ§è½ï¼</strong> Qwen3-Omniå¨36ä¸ªé³é¢åè§å¬åºåæµè¯ä¸­ï¼å¨32ä¸ªåºåæµè¯ä¸å®ç°äºå¼æºSOTAï¼å¹¶å¨22ä¸ªåºåæµè¯ä¸åå¾äºæ»ä½SOTAï¼è¶è¶äºGemini-2.5-ProãSeed-ASRåGPT-4o-Transcribeç­å¼ºå¤§çé­æºæ¨¡åã
*   <strong>æ¨¡æé´æ éåï¼</strong> é¦æ¬¡è¯æäºéè¿éæå¤æ¨¡æè®­ç»ï¼å¯ä»¥å¨æææ¨¡æä¸å®ç°æ§è½åç­ï¼å³æ²¡ææ¨¡æç¹å®çæ§è½ä¸éï¼åæ¶æ¾èå¢å¼ºäºè§é¢çè§£ç­è·¨æ¨¡æè½åã
*   <strong>å¹¿æ³çè¯­è¨æ¯æï¼</strong> æ¯æ119ç§ææ¬äº¤äºè¯­è¨ï¼19ç§è¯­é³çè§£è¯­è¨å10ç§è¯­é³çæè¯­è¨ã
*   <strong>å®æ¶äº¤äºè½åï¼</strong> è½å¤å¤çé¿è¾¾40åéçé³é¢å½é³ï¼å®ç°ASRåå£è¯­çè§£ï¼å¹¶æ¯æéè¿ç¨æ·å®ä¹çç³»ç»æç¤ºè¿è¡ç»ç²åº¦çå¯¹è¯è¯­æ°åè§è²å®å¶ã
*   <strong>å¼æ¾æ§ï¼</strong> Qwen3-Omni-30B-A3BãQwen3-Omni-30B-A3B-ThinkingåQwen3-Omni-30B-A3B-Captioneråå¨Apache 2.0è®¸å¯ä¸å¬å¼åå¸ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é¿è§é¢åºåæµè¯æ§è½æ¬¡ä¼ï¼</strong> å½åæ¨¡åå¨é¿è§é¢åºåæµè¯ä¸çæ§è½ä»ææåç©ºé´ï¼è¿ä¸»è¦æºäºä¸¤ä¸ªæ¶æéå¶ï¼ä½ç½®å¤æ¨è½åæéååéçä¸ä¸æé¿åº¦ã
*   <strong>è®¡ç®ææ¬é«æï¼</strong> ç±äºå®éªææ¬é«æï¼æªè½å¯¹æææ¨¡åè§æ¨¡è¿è¡å¨é¢çæ§è½è¯ä¼°ã
*   <strong>è¯­è¨è½åæåä¸ææ¾ï¼</strong> ç»éªè¡¨æï¼æ·»å è§è§æé³é¢ä¿¡å·å¹¶æªå¨è¯­è¨è½åä¸å¸¦æ¥å¯è¡¡éçæåã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤æ¬å£°å¨ASRï¼</strong> è¿ä¸æ­¥æ¯æå¤æ¬å£°å¨èªå¨è¯­é³è¯å«ã
*   <strong>è§é¢OCRï¼</strong> å¢å¼ºè§é¢ä¸­çåå­¦å­ç¬¦è¯å«è½åã
*   <strong>è§å¬ä¸»å¨å­¦ä¹ ï¼</strong> æ¢ç´¢è§å¬é¢åçä¸»å¨å­¦ä¹ æºå¶ã
*   <strong>åºäºä»£ççå·¥ä½æµåå½æ°è°ç¨ï¼</strong> å¢å¼ºå¯¹åºäºä»£ççå·¥ä½æµåå½æ°è°ç¨çæ¯æã
*   <strong>è§£å³é¿è§é¢çè§£çå±éæ§ï¼</strong> æ¹è¿æ¶æä»¥æåæ¨¡åå¨é¿è§é¢åºåæµè¯ä¸çæ§è½ï¼ç¹å«æ¯è§£å³ä½ç½®å¤æ¨åä¸ä¸æé¿åº¦çéå¶ã</p>
<hr />
<p>æ»èè¨ä¹ï¼Qwen3-Omniä»£è¡¨äºå¤æ¨¡æAIé¢åçä¸ä¸ªéè¦éç¨ç¢ï¼å®é¦æ¬¡å¨ä¸çºç²ä»»ä½åæ¨¡ææ§è½çåæä¸ï¼å®ç°äºææ¬ãå¾åãé³é¢åè§é¢çç»ä¸SOTAè¡¨ç°ãå¶åæ°çThinker-Talker MoEæ¶æãAuTç¼ç å¨ãå¤ç æ¬è¯­é³çæåè½»éçº§ConvNetï¼å±åæå»ºäºä¸ä¸ªé«æãä½å»¶è¿ä¸åè½å¼ºå¤§çå¤æ¨¡æç³»ç»ï¼ä¸ºæªæ¥çå¤æ¨¡æç ç©¶ååºç¨å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts.</li>
<li>To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17765v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17765v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.17707v1'></a></p>
<h2 id="automatic-intermodal-loading-unit-identification-using-computer-vision-a-scoping-review"><a href="https://arxiv.org/abs/2509.17707v1">Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review</a></h2>
<p><strong>Authors:</strong> Emre GÃ¼lsoylu, Alhassan Abdelhalim, Derya Kara Boztas, Ole Grasse, Carlos Jahn, Simone Frintrop, Janick Edinger</p>
<p><strong>Published:</strong> 2025-09-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºEmre GÃ¼lsoyluç­äººæ°åçè®ºæâAutomatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Reviewâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºæé¢ç®ï¼</strong> ä½¿ç¨è®¡ç®æºè§è§çèªå¨å¤å¼èè¿è£è½½ååè¯å«ï¼ä¸é¡¹èå´çå®ç»¼è¿°</p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤å¼èè¿è£è½½ååï¼ILUï¼å¦éè£ç®±ãåæè½¦åå¯æ¢ç®±ä½ï¼å¨æ¸¯å£åç å¤´è¿è¡é«æãé²æ£è¯å«çå³é®ç¶é¢ãå°½ç®¡ILUçæ ååå½»åºæ¹åäºå¨çè´¸æï¼ä½å¶è¯å«è¿ç¨ä»é¢ä¸´ææãè¯¥ç»¼è¿°éè¿åé¡¾è¿å»35å¹´ï¼1990-2025ï¼åºäºè®¡ç®æºè§è§ï¼CVï¼çè§£å³æ¹æ¡ï¼æ¢è®¨äºè¯¥é¢åçåå±ãå½åææ¯ç¶åµãææä»¥åæªæ¥çç ç©¶æ¹åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¨é¢ç»¼è¿°ï¼</strong> æ¬æå¯¹63é¡¹å®è¯ç ç©¶è¿è¡äºç³»ç»æ§ç»¼è¿°ï¼æ¶µçäºä»æ©ææ°å­å¾åå¤çï¼DIPï¼åä¼ ç»æºå¨å­¦ä¹ ï¼MLï¼å°å½åæ·±åº¦å­¦ä¹ ï¼DLï¼ææ¯çæ¼åã
*   <strong>æ¯è¯­æ ååï¼</strong> è®ºæéæ°è¯ä¼°å¹¶æä¾äºILUè¯å«é¢åä¸­ä½¿ç¨çæ¯è¯­çæ¸æ°å®ä¹ï¼åæ¬DIPãMLãDLãå¯¹è±¡æ£æµãå®ä¾åå²ãå­ç¬¦åå²ãå­ç¬¦æ£æµãåºæ¯ææ¬æ£æµãåºæ¯ææ¬è¯å«åææ¬è¯å«ï¼Text Spottingï¼ã
*   <strong>æ¹æ³è®ºæ¼ååæï¼</strong> è¯¦ç»åæäºILUè¯å«æ¹æ³éæ¶é´æ¨ç§»çæ¼åï¼æåºä»å­ç¬¦çº§è¯å«ååºæ¯ææ¬è¯å«çè½¬åï¼ä»¥ååºå®æåå¤´åç§»å¨æåå¤´ï¼å¦æ äººæºãå°é¢è½¦è¾ï¼çè½¬åã
*   <strong>æ°æ®éç¹æ§æ»ç»ï¼</strong> æ»ç»äºç°ææ°æ®éçééè®¾ç½®ï¼åºå®/ç§»å¨æåå¤´ï¼ãå¤æ ·æ§ï¼çå®ä¸ç/åæ§ç¯å¢ãå¤©æ°ãåç§ãæåç­ï¼åå¯ç¨æ§ï¼å¼ºè°äºå¬å±åºåæ°æ®éçç¼ºä¹ã
*   <strong>è¯ä¼°ææ åæï¼</strong> è®¨è®ºäºILUè¯å«ä¸­å¸¸ç¨çè¯ä¼°ææ ï¼å¹¶å¼ºè°äºç«¯å°ç«¯åç¡®çä½ä¸ºæä¸¥æ ¼åæç¸å³çææ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ææ¯æ¼åï¼</strong> ILUè¯å«é¢åå·²ä»DIPåä¼ ç»MLæ¹æ³åå±å°DLææ¯çæ¾èä¸»å¯¼ï¼å°¤å¶æ¯å¨2016-2020å¹´ä¹åï¼DLæ¹æ³å¨å¤çå¤ææ¡ä»¶ä¸çé²æ£æ§è¡¨ç°åºä¼è¶æ§ã
*   <strong>å°åéä¸­ï¼</strong> äºæ´²å°åºå¨ILUè¯å«ç ç©¶ä¸­å æ®ä¸»å¯¼å°ä½ï¼79.71%çåºçç©ï¼ï¼è¿åæ äºè¯¥å°åºä½ä¸ºå¨çè´¸ææ¢çº½å¯¹é«æç©æµçè¿«åéæ±åç åæå¥ã
*   <strong>èµéæ¥æºï¼</strong> å¬å±èµéæ¯ä¸»è¦æ¯ææ¥æºï¼38.10%ï¼ï¼è¡¨ææ¿åºå¯¹åºç¡è®¾æ½åç»æµå¢é¿çéè§ã
*   <strong>æ°æ®ééå¶ï¼</strong> ç»å¤§å¤æ°æ°æ®éï¼85.71%ï¼ä¸å¬å¼å¯ç¨ï¼å¯¼è´ç ç©¶ç»æçå¯éå¤æ§åæ¯è¾æ§å·®ï¼ç«¯å°ç«¯åç¡®çæ¥åèå´ä»5%å°96%ä¸ç­ï¼å·®å¼å·¨å¤§ã
*   <strong>ææè½¬åï¼</strong> è¯å«ä»»å¡ä»å­ç¬¦çº§ææ¬è¯å«è½¬ååºæ¯ææ¬è¯å«ï¼å¹¶æ´åç§»å¨æåå¤´ï¼å¦æ äººæºãå°é¢è½¦è¾ï¼ï¼å¼å¥äºæ°çå¤ææ§ï¼å¦å¨æåºæ¯çæ§åå§¿æä¼°è®¡ã
*   <strong>åºçç©åå¸ï¼</strong> è¶è¿ä¸åçè®ºæåè¡¨å¨æªæåçæåæä¼è®®ä¸ï¼è¿è¡¨æè¯¥é¢åéè¦æ´å¼ºè°æ°é¢è´¡ç®ãå¯æ¯æ§åç»æå¯éå¤æ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>ç¼ºä¹å¬å¼åºåæ°æ®éï¼</strong> è¿æ¯è¯¥é¢åç ç©¶åå¼åçæå¤§éç¢ï¼å¯¼è´æ¨¡åè®­ç»æ°æ®ä¸è¶³ï¼ç»æå¯æ¯æ§å·®ï¼ä¸é¾ä»¥è¿è¡å¬å¹³çæ¹æ³æ¯è¾ã
*   <strong>æ¯è¯­ä¸æ ååï¼</strong> ç°ææç®ä¸­æ¯è¯­ä½¿ç¨ä¸ä¸è´ï¼å½±åäºç ç©¶çæ¸æ°åº¦åæ£ç´¢æçã
*   <strong>æ°æ®å¤æ ·æ§ä¸è¶³ï¼</strong> å°½ç®¡ä¸äºæ°æ®éè¯å¾åå«å¤æ ·åçæ¡ä»¶ï¼ä½æ¥èªå¤ä¸ªå°ç¹ååºæ¯çå¾åæ°æ®éä»ç¶ç¨ç¼ºï¼éå¶äºæ¨¡åçæ³åè½åã
*   <strong>DLæ¨¡åè®­ç»æ°æ®éå°ï¼</strong> ç°ææ°æ®éçè§æ¨¡å¯¹äºDLæ¨¡åçé²æ£è®­ç»æ¥è¯´éå¸¸å¤ªå°ï¼è¿«ä½¿ç ç©¶äººåéåºéç¨ææ¬æ£æµåè¯å«æ¨¡åã
*   <strong>æ§è½è¯ä¼°å·®å¼å¤§ï¼</strong> ç±äºæ°æ®éä¸åï¼æ¥åçç«¯å°ç«¯åç¡®çå·®å¼å·¨å¤§ï¼é¾ä»¥å¯¹æ¹æ³è¿è¡æææ¯è¾ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ ååæ¯è¯­ï¼</strong> å»ºç«ç»ä¸çæ¯è¯­ä½ç³»ï¼ä»¥æé«ç ç©¶çæ¸æ°åº¦åå¯æ¯æ§ã
*   <strong>å¼æ¾è·åæ°æ®éåå±äº«ä»£ç ï¼</strong> å¼ååå»ºå¬å¼å¯ç¨çåºåæ°æ®éåå±äº«æºä»£ç ï¼ä»¥ä¿è¿ç ç©¶çå¯éå¤æ§ãå¬å¹³æ¯è¾åé¢åè¿æ­¥ã
*   <strong>æ ä¸ä¸æææ¬è¯å«ï¼</strong> éå¯¹ISO6346ä»£ç çæ ä¸ä¸æææ¬è¯å«è¿è¡ä¼åï¼å ä¸ºè¿äºä»£ç ç¼ºä¹èªç¶è¯­è¨ä¸ä¸æï¼ç°æä¾èµè¯­è¨æ¨¡åçåºæ¯ææ¬è¯å«æ¨¡åå¯è½æçä½ä¸ã
*   <strong>ç§»å¨æåå¤´éæï¼</strong> è¿ä¸æ­¥ç ç©¶åå¼åéç¨äºç§»å¨æåå¤´ï¼å¦æ äººæºãå°é¢è½¦è¾ï¼çILUè¯å«ç³»ç»ï¼ä»¥å®ç°å¨æç»ç«¯çæ§ï¼å¹¶è§£å³å§¿æä¼°è®¡ç­æ°ä»»å¡ã
*   <strong>å®æ¶å¤çä¼åï¼</strong> å³æ³¨ç®æ³ä¼åï¼ä½¿å¶è½å¨ç§»å¨è®¾å¤ä¸å®æ¶è¿è¡ï¼å¹¶éè¿è§é¢æµåææé«ç³»ç»æçã
*   <strong>åææ°æ®çæï¼</strong> æ¢ç´¢ä½¿ç¨æ¸¸æå¼æç­æ¹æ³çæåææ°æ®ï¼ä»¥å¼¥è¡¥çå®ä¸çæ°æ®éçä¸è¶³ã
*   <strong>å¾åè´¨éæåï¼</strong> éç¨å¾åå¢å¼ºææ¯ï¼å¦éåªãGANsï¼ååå¤çæ­¥éª¤æ¥å¤çä½åç§ãé®æ¡ãæåç­æææ§æ¡ä»¶ã
*   <strong>ç½ç»æ¶æä¼åï¼</strong> æ¹è¿DLç½ç»æ¶æï¼å¢å æåéï¼å¹¶å¼å¥æ³¨æåæºå¶ä»¥æé«è¯å«åç¡®çã
*   <strong>å¤è§è§è¯å«ï¼</strong> å©ç¨ä»ä¸åè§åº¦æè·çå¤ä¸ªå¾åè¿è¡å¤è§è§è¯å«ï¼ä»¥åºå¯¹éåIDä»£ç çææã
*   <strong>å¬ç§åä½ï¼</strong> é¼å±å¬å±é¨é¨åç§è¥ä¼ä¸ä¹é´çåä½ï¼å±åèµå©åå¼åILUè¯å«ææ¯ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºè®¡ç®æºè§è§å¨ILUè¯å«é¢åçåºç¨æä¾äºå¨é¢çè§è§ï¼å¼ºè°äºDLææ¯çå´èµ·ãäºæ´²å¨è¯¥é¢åçé¢å¯¼å°ä½ï¼å¹¶æç¡®æåºäºå½åé¢ä¸´çæ°æ®éåæ ååææï¼ä¸ºæªæ¥çç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring.</li>
<li>To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.17707v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.17707v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.19297v1'></a></p>
<h2 id="volsplat-rethinking-feed-forward-3d-gaussian-splatting-with-voxel-aligned-prediction"><a href="https://arxiv.org/abs/2509.19297v1">VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</a></h2>
<p><strong>Authors:</strong> Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Weijie Wangç­äººæ°åçè®ºæâVolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Predictionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼VolSplatï¼éæ°æèåå3Dé«æ¯æ³¼æºä¸ä½ç´ å¯¹é½é¢æµ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åå3Dé«æ¯æ³¼æºï¼3DGSï¼æ¹æ³ä¸­åç´ å¯¹é½èå¼çåºæå±éæ§ãç°ææ¹æ³å°æ¯ä¸ª2Dåç´ æ å°å°ä¸ä¸ª3Dé«æ¯ï¼å¯¼è´éå»ºç3Dæ¨¡åä¸¥éä¾èµè¾å¥è§å¾æ°éãè§å¾åç½®çå¯åº¦åå¸ï¼å¹¶å¨æºè§å¾å­å¨é®æ¡æä½çº¹çæ¶å¼å¥å¯¹é½è¯¯å·®ãè¿äºéå¶å¯¼è´å ä½ç²¾åº¦ä¸è¶³ãæµ®ç¹é®é¢ä»¥åæ æ³æ ¹æ®åºæ¯å¤ææ§èªéåºæ§å¶é«æ¯å¯åº¦ï¼ä»èå½±åæ°é¢è§å¾æ¸²æçè´¨éåè¡¨ç¤ºçé²æ£æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
VolSplatå¼å¥äºä¸ç§æ°é¢çå¤è§å¾ååèå¼ï¼ç¨<strong>ä½ç´ å¯¹é½é«æ¯</strong>åä»£äºä¼ ç»çåç´ å¯¹é½ãå¶æ ¸å¿åæ°åè´¡ç®åæ¬ï¼
*   <strong>ä½ç´ å¯¹é½é¢æµï¼</strong> VolSplatç´æ¥ä»é¢æµç3Dä½ç´ ç½æ ¼ä¸­é¢æµé«æ¯ï¼ä»èåæäºåç´ å¯¹é½å¯¹æåºéç2Dç¹å¾å¹éçä¾èµï¼ç¡®ä¿äºé²æ£çå¤è§å¾ä¸è´æ§ã
*   <strong>èªéåºé«æ¯å¯åº¦æ§å¶ï¼</strong> è¯¥æ¹æ³è½å¤æ ¹æ®3Dåºæ¯çå¤ææ§èªéåºæ§å¶é«æ¯å¯åº¦ï¼ä»èçææ´å¿ å®çé«æ¯ç¹äºãæ¹è¿çå ä½ä¸è´æ§å¹¶å¢å¼ºæ°é¢è§å¾çæ¸²æè´¨éã
*   <strong>3Dç¹å¾æå»ºä¸ç²¾ç¼ï¼</strong> é¦åï¼éè¿Transformerç½ç»åå¹³é¢æ«ææå»ºæ¯è§å¾ææ¬ä½ï¼å¹¶ä½¿ç¨æ·±åº¦é¢æµæ¨¡åä¼°è®¡æ·±åº¦å¾ãç¶åï¼å°2Dç¹å¾åæå½±å°3Dç©ºé´å½¢æä½ç´ ç¹å¾ç½æ ¼ãæ¥çï¼ä½¿ç¨ç¨ç3D U-Netè§£ç å¨å¯¹è¿äºç¹å¾è¿è¡ç²¾ç¼ï¼ä»¥é¢æµæ¯ä¸ªå æ®ä½ç´ ç3Dé«æ¯åæ°ãè¿ç§æ®å·®ç²¾ç¼æ¶ææå©äºå­¦ä¹ æ ¡æ­£é¡¹ï¼åæ¶ä¿æç²ç²åº¦ä½ç´ ä¿¡æ¯ã
*   <strong>è§£è¦3Dè¡¨ç¤ºä¸2Dåç´ ç½æ ¼ï¼</strong> éè¿ä½ç´ å¯¹é½ï¼è¯¥æ¹æ³å°3Dè¡¨ç¤ºä»2Dåç´ ç½æ ¼ççº¦æä¸­è§£è¦åºæ¥ï¼è§£å³äºé«æ¯å¯åº¦ä¸è¾å¥å¾ååè¾¨çåæ§è¦åçé®é¢ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> å¨RealEstate10KåScanNetç­å¹¿æ³ä½¿ç¨çåºåæµè¯ä¸­ï¼VolSplatå®ç°äºæåè¿çæ§è½ï¼çæäºæ´åçãè§å¾ä¸è´çé«æ¯éå»ºã
*   <strong>æ´å°çæµ®ç¹åä¼ªå½±ï¼</strong> æ¸²æå¾åå¨ç©ä½è¾¹çå¤åºæ¬æ²¡æç«äºæ¹æ³ä¸­å¸¸è§çæµ®ç¹åä¼ªå½±ï¼è¿ç´æ¥å½å äºæ¨¡åè§£å³3Dç¹å¾è¡¨ç¤ºä¸­å¤è§å¾å¯¹é½é®é¢çè½åã
*   <strong>åè¶çæ³åè½åï¼</strong> å¨è·¨æ°æ®éæ³åå®éªï¼ä¾å¦å¨ACIDæ°æ®éä¸è¿è¡é¶æ ·æ¬è¿ç§»ï¼ä¸­ï¼VolSplatè¡¨ç°åºæ¾èæ´é«çæ§è½ï¼è¿è¡¨æå¶ä½ç´ å¯¹é½æ¡æ¶å·æåºæçé²æ£æ§ã
*   <strong>é«æçé«æ¯å¯åº¦ç®¡çï¼</strong> ä¸åç´ å¯¹é½æ¹æ³ç¸æ¯ï¼VolSplatè½å¤æ ¹æ®åºæ¯å¤ææ§èªéåºå°åéé«æ¯ï¼é¿åäºç®ååºåçè¿åº¦å¯éååå¤æå ä½åºåçä¸è¶³ãå®éå¸¸ä»¥æ´é«æãæ´ç´§åçé«æ¯éå®ç°åè¶çæ¸²æè´¨éã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºVolSplatçå±éæ§ï¼ä½ä»å¶æ¹æ³æè¿°åæ¶èç ç©¶ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çèèå ç´ ï¼
*   <strong>ä½ç´ å¤§å°éæ©ï¼</strong> ä½ç´ å¤§å°æ¯ä¸ä¸ªå³é®çè¶åæ°ï¼å®å¨å ä½è¡¨ç¤ºçä¿çåº¦åè®¡ç®èµæºæ¶èä¹é´å­å¨æè¡¡ãè½ç¶è®ºæéè¿å®éªæ¾å°äºä¸ä¸ªææçå¹³è¡¡ç¹ï¼ä½å¯¹äºç¹å®åºç¨ææç«¯åºæ¯ï¼ä½ç´ å¤§å°çéæ©å¯è½ä»éä»ç»è°æ´ã
*   <strong>è®¡ç®èµæºï¼</strong> å°½ç®¡ç¨ç3D U-Netæé«äºæçï¼ä½å¤ç3Dä½ç´ ç½æ ¼ï¼å°¤å¶æ¯å¯¹äºéå¸¸ç²¾ç»çä½ç´ å¤§å°ï¼ä»ç¶å¯è½éè¦æ¾èçåå­åå¤çæ¶é´ï¼è¿å¯è½éå¶äºå¶å¨èµæºåéè®¾å¤ä¸çåºç¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´é«æçä½ç´ è¡¨ç¤ºï¼</strong> è¿ä¸æ­¥æ¢ç´¢æ´é«æçç¨çä½ç´ æ°æ®ç»ææèªéåºä½ç´ ç½æ ¼ï¼ä»¥å¨ä¿æå ä½ç»èçåæ¶åå°åå­åè®¡ç®å¼éã
*   <strong>å¨æåºæ¯åå®æ¶åºç¨ï¼</strong> å°VolSplatæ©å±å°å¨æåºæ¯ï¼å¹¶è¿ä¸æ­¥ä¼åå¶æ¨çéåº¦ï¼ä»¥æ¯ææ´ä¸¥æ ¼çå®æ¶åºç¨ï¼ä¾å¦æºå¨äººå¯¼èªåAR/VRã
*   <strong>å¤æ¨¡æèåï¼</strong> æ¢ç´¢å°VolSplatä¸æ´å¤æ¨¡æè¾å¥ï¼å¦LiDARç¹äºãè¯­ä¹ä¿¡æ¯ç­ï¼èåï¼ä»¥å¢å¼º3Déå»ºçé²æ£æ§åç»èã
*   <strong>æ´å¹¿æ³çæ³åï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æé«æ¨¡åå¨æ´å¤æ ·åãæ´å·æææ§çåºæ¯ï¼ä¾å¦æç«¯åç§ãåå°è¡¨é¢ç­ï¼ä¸­çæ³åè½åã
*   <strong>ä¸ä¸æ¸¸ä»»å¡çéæï¼</strong> å°VolSplatçæçé²æ£3Dè¡¨ç¤ºä¸åç§ä¸æ¸¸ä»»å¡ï¼å¦ç©ä½æ£æµãè¯­ä¹åå²ãè·¯å¾è§åç­ï¼æ´ç´§å¯å°éæã</p>
<hr />
<p>æ»èè¨ä¹ï¼VolSplatéè¿å¼å¥ä½ç´ å¯¹é½é¢æµèå¼ï¼å¯¹åå3DGSé¢åååºäºéå¤§è´¡ç®ãå®ææå°è§£å³äºç°æåç´ å¯¹é½æ¹æ³çå³é®å±éæ§ï¼æä¾äºæ´é²æ£ãæ´ä¸è´ä¸èªéåºç3Dåºæ¯éå»ºãè¿é¡¹å·¥ä½ä¸ºæªæ¥å¨æ´å¹¿æ³ç¤¾åºä¸­è¿è¡åå3Déå»ºçç ç©¶å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis.</li>
<li>To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians.</li>
<li>Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality.</li>
<li>Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions.</li>
<li>In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.19297v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.19297v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.19282v1'></a></p>
<h2 id="overlaybench-a-benchmark-for-layout-to-image-generation-with-dense-overlaps"><a href="https://arxiv.org/abs/2509.19282v1">OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps</a></h2>
<p><strong>Authors:</strong> Bingnan Li, Chen-Yu Wang, Haiyang Xu, Xiang Zhang, Ethan Armand, Divyansh Srivastava, Xiaojun Shan, Zeyuan Chen, Jianwen Xie, Zhuowen Tu</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Bingnan Liç­äººæ°åçè®ºæâOverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlapsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="overlaybench">è®ºææè¦ï¼OverLayBench: ç¨äºå¯ééå å¸å±å°å¾åçæçåºå</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡å¸å±å°å¾åï¼L2Iï¼çæåå¾äºæ¾èè¿å±ï¼ä½ç°ææ¹æ³å¨å¤çåå«æ¾èéå è¾¹çæ¡çå¸å±æ¶ä»é¢ä¸´ææãå·ä½æ¥è¯´ï¼è®ºææåºä¸¤ä¸ªä¸»è¦é®é¢ï¼(1) å¤§é¢ç§¯éå åºåï¼ä»¥å (2) è¯­ä¹åºååº¦æä½çéå å®ä¾ãè¿äºå ç´ å¯¼è´çæå¾åçè´¨éä¸éï¼è¡¨ç°ä¸ºç©ä½èåãç©ºé´æ¨¡ç³åè§è§å¤±çç­ä¼ªå½±ãå æ­¤ï¼è¯¥ç ç©¶æ¨å¨ç³»ç»å°è¯ä¼°åæ¹è¿L2Iæ¨¡åå¨å¤æéå åºæ¯ä¸çæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>OverLayScoreï¼</strong> è®ºæå¼å¥äºä¸ä¸ªæ°é¢çåº¦éæ åOverLayScoreï¼ç¨äºéåéå è¾¹çæ¡çå¤ææ§ãå®éè¿è®¡ç®ææå®ä¾å¯¹çIoUï¼äº¤å¹¶æ¯ï¼ä¹åï¼å¹¶æ ¹æ®CLIPåµå¥çè¯­ä¹ç¸ä¼¼æ§è¿è¡å æï¼ä»èææå¸å±ä¸­ç©ºé´åè¯­ä¹éå çé¾åº¦ãé«OverLayScoreå¼è¡¨ç¤ºçæé¾åº¦æ´å¤§ã
*   <strong>OverLayBenchåºåï¼</strong> ä¸ºäºè§£å³ç°æåºåååç®åå¸å±çé®é¢ï¼è®ºææåºäºOverLayBenchï¼è¿æ¯ä¸ä¸ªæ°çL2Içæåºåãå®å·æé«è´¨éçæ æ³¨ï¼åæ¬è¯¦ç»çå¾ååå¯éå®ä¾æè¿°ï¼ä»¥åè·¨OverLayScoreä¸åé¾åº¦çº§å«ï¼ç®åãå¸¸è§ãå¤æï¼çå¹³è¡¡åå¸ï¼æ¨å¨æ´ä¸¥æ ¼å°è¯ä¼°æ¨¡åå¨å¤æåéå å¸å±ä¸çé²æ£æ§ã
*   <strong>CreatiLayout-AMæ¨¡åï¼</strong> ä½ä¸ºæ¹è¿å¤æéå æ§è½çåæ­¥å°è¯ï¼è®ºææåºäºCreatiLayout-AMãè¿æ¯ä¸ä¸ªå¨ç²¾å¿ç­åçéæ¨¡æï¼amodalï¼æ©ç æ°æ®éä¸è¿è¡å¾®è°çæ¨¡åï¼éè¿å¼å¥ä¸¤ä¸ªé¢å¤çæå¤±é¡¹ï¼LtokenåLpixelï¼æ¥é¼å±æ¨¡åæ³¨æåå¾ä¸éæ¨¡ææ©ç ä¹é´çå¯¹é½ï¼ä»èç¼è§£å®ä¾é®æ¡å¼èµ·ççæä¼ªå½±ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>OverLayScoreçæææ§ï¼</strong> å®éªç»æè¡¨æï¼éçOverLayScoreçå¢å ï¼ç°æL2Iæ¨¡åççæè´¨éï¼mIoUï¼æç»­ä¸éï¼éªè¯äºOverLayScoreè½ææåæ éå å¸å±ççæé¾åº¦ã
*   <strong>ç°æåºåçå±éæ§ï¼</strong> åæåç°ï¼COCOãLayoutSAMåHiCoç­ç°æL2Iåºåå¨OverLayScoreåå¸ä¸å­å¨ä¸¥éåå·®ï¼å¤§å¤æ°æ ·æ¬å±äºä½é¾åº¦èå´ï¼éå¶äºå®ä»¬å¨å¤æåºæ¯ä¸è¯ä¼°æ¨¡åæ§è½çæææ§ã
*   <strong>OverLayBenchçä¼å¿ï¼</strong> OverLayBenchéè¿æä¾æ´å¹³è¡¡çé¾åº¦åå¸ï¼è½å¤å¯¹L2Iæ¨¡åå¨ç©ºé´åè¯­ä¹å¤æå¸å±ä¸çé²æ£æ§è¿è¡æ´ä¸¥æ ¼çè¯ä¼°ã
*   <strong>CreatiLayout-AMçæ§è½æåï¼</strong> CreatiLayout-AMå¨OverLayBenchçç®ååå¸¸è§é¾åº¦çº§å«ä¸ä¼äºåå§CreatiLayoutï¼ç¹å«æ¯å¨O-mIoUï¼éå åºåmIoUï¼ä¸åå¾äºæ¾èæåï¼+15.90%å+5.42%ï¼ï¼éªè¯äºéæ¨¡ææ©ç çç£å¨æ¹åéå è¾¹çæ¡ä¸çL2Içæè´¨éæ¹é¢çæææ§ãå¨å¤æé¾åº¦çº§å«ä¸ï¼å¶æ§è½ä¹ä¿æäºç«äºåã
*   <strong>éè¯¯æ¨¡å¼åæï¼</strong> è®ºæè¯¦ç»åæäºç°æL2Iæ¨¡åå¨éå åºæ¯ä¸­å¸¸è§çäºç§å¤±è´¥æ¨¡å¼ï¼ä¸æ­£ç¡®çç©ä½æ°éãç©ä½èåãç©ä½å¤±çãä¸æ­£ç¡®çç±»å«åè¾¹çæ¡éä½ï¼ä¸ºæªæ¥çç ç©¶ææäºæ¹åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸»è¦å³æ³¨ç°æL2Iæ¨¡åå¨å¤çéå å¸å±æ¶çä¸è¶³ï¼å¹¶æåºäºæ°çåºåååæ­¥è§£å³æ¹æ¡ãè½ç¶CreatiLayout-AMå¨ç®ååå¸¸è§éå åºæ¯ä¸­è¡¨ç°è¯å¥½ï¼ä½å¨âå¤æâéå åºæ¯ä¸­ï¼å¶æ§è½æåç¸å¯¹è¾å°ï¼çè³å¨æäºææ ä¸ç¥æä¸éãè¿è¡¨æï¼å½è®­ç»éä¸æµè¯éçåå¸å·®å¼è¾å¤§æ¶ï¼æ¨¡åä»é¢ä¸´ææãæ­¤å¤ï¼CreatiLayout-AMçéæ¨¡ææ©ç çç£æ¹æ³æ¯åæ­¥æ¢ç´¢ï¼å¯è½è¿æè¿ä¸æ­¥ä¼åçç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¼ºå¤§çéæ¨¡ææ©ç çç£ï¼</strong> è¿ä¸æ­¥æ¢ç´¢æ´åè¿çéæ¨¡ææ©ç çç£æ¹æ³ï¼ä»¥å¨æ´å¤æçéå åºæ¯ä¸­å®ç°æ´å¤§çæ§è½æåã
*   <strong>æåæ¨¡åå¯¹åå¸åç§»çé²æ£æ§ï¼</strong> ç ç©¶å¦ä½ä½¿L2Iæ¨¡åå¨è®­ç»æ°æ®ä¸å®éå¤æåºæ¯å­å¨åå¸å·®å¼æ¶ï¼ä»è½ä¿æç¨³å®ççæè´¨éã
*   <strong>æ´ç²¾ç»çç©ºé´æ¨çåç»åçè§£ï¼</strong> é¼å±å¼åå·ææ´å¼ºç©ºé´æ¨çåç»åçè§£è½åçæ°æ¹æ³ï¼ä»¥æ´å¥½å°å¤çå¯ééå åºæ¯ä¸­çç©ä½äº¤äºã
*   <strong>å¤æ¨¡æèåï¼</strong> ç»åå¶ä»è¾å©æ¨¡æï¼å¦æ·±åº¦å¾ãè¾¹ç¼å¾ï¼å¯è½æå©äºæ¨¡åæ´å¥½å°çè§£ä¸ç»´åºæ¯åç©ä½é®æ¡å³ç³»ã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºå¸å±å°å¾åçæé¢åï¼ç¹å«æ¯å¤çå¯ééå åºæ¯ï¼æä¾äºä¸ä¸ªéè¦çåºååæ°çç ç©¶æ¹åãéè¿å¼å¥OverLayScoreåOverLayBenchï¼ä½èä»¬ä¸ºæªæ¥æ´é²æ£ãæ´çå®çå¾åçæå¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality.</li>
<li>To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes.</li>
<li>To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore.</li>
<li>Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.19282v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.19282v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.19252v1'></a></p>
<h2 id="adversarially-refined-vq-gan-with-dense-motion-tokenization-for-spatio-temporal-heatmaps"><a href="https://arxiv.org/abs/2509.19252v1">Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps</a></h2>
<p><strong>Authors:</strong> Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâAdversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmapsâè®ºæçå¨é¢æè¦ï¼ç±Gabriel Maldonadoç­äººæ°åï¼</p>
<p><strong>è®ºææè¦ï¼å¯¹ææ§ç²¾ç¼VQ-GANä¸å¯éè¿å¨æ è®°åç¨äºæ¶ç©ºç­å¾</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®¡ç®æºè§è§é¢åä¸­çè§£è¿ç»­äººä½è¿å¨çæ ¸å¿ææãäººä½è¿å¨æ°æ®å·æé«ç»´åº¦ååºæçåä½æ§ï¼è¿ä½¿å¾é«æçåç¼©åè¡¨ç¤ºå¯¹äºåæå¤æçè¿å¨å¨æè³å³éè¦ãå·ä½æ¥è¯´ï¼ç°ææ¹æ³å¨åç¼©æ¶ç©ºç­å¾æ¶ï¼å¾å¾é¾ä»¥å¨ä¿æè¿å¨ä¿çåº¦ï¼ä¾å¦é¿åè¿å¨æ¨¡ç³åæ¶é´éä½ç­éå»ºä¼ªå½±ï¼çåæ¶å®ç°é«æåç¼©ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä½èå¼å¥äºä¸ä¸ªæ°é¢ç<strong>å¯¹ææ§ç²¾ç¼VQ-GANæ¡æ¶</strong>ï¼å¹¶ç»å<strong>å¯éè¿å¨æ è®°å</strong>æ¥åç¼©æ¶ç©ºç­å¾ãå¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>å¯¹ææ§ç²¾ç¼VQ-GANæ¡æ¶ï¼</strong> è¿æ¯é¦ä¸ªç¨äºäººä½è¿å¨ç¼ç çVQ-GANæ¡æ¶ï¼å®å°2Då3Dæ¶ç©ºç­å¾åºåç¦»æ£åä¸ºç´§åçæ½å¨æ è®°ãéè¿å¼å¥å¯¹ææ§è®­ç»ç®æ ï¼è¯¥æ¡æ¶è½å¤ç¡®ä¿æ¶é´è¿è´¯æ§ï¼æææ¶é¤éå¯¹ææ§åºçº¿ä¸­å¸¸è§çè¿å¨æ¨¡ç³åéä½ä¼ªå½±ã
*   <strong>å¯éè¿å¨æ è®°åååæï¼</strong> æåºäºä¸ç§æ°é¢çå¯éæ è®°åç­ç¥ï¼è½å¤æè·ç¨çè¡¨ç¤ºä¸­å¸¸å¸¸ä¸¢å¤±çç»å¾®è¿å¨æ¨¡å¼ãéè¿ç³»ç»åæåç¼©å å­åè¯æ±éå¤§å°çä½ç¨ï¼è®ºææ·±å¥æ¢è®¨äºè¿å¨çåå¨å¤ææ§ã
*   <strong>ä¼è¶çåç¼©åä¿çåº¦æ§è½ï¼</strong> è¯¥æ¡æ¶å¨è¿å¨åç¼©æ¹é¢å»ºç«äºæ°çææ¯æ°´å¹³ï¼å¶å¯¹ææ§å¢å¼ºçç¦»æ£åµå¥å¨éå»ºè´¨éåæ¶é´ç¨³å®æ§æ¹é¢åä¼äºdVAEæ¨¡åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½ä¼è¶æ§ï¼</strong> å¨CMU Panopticæ°æ®éä¸çå®éªè¡¨æï¼è¯¥æ¹æ³å¨SSIMï¼ç»æç¸ä¼¼æ§ææ°ï¼æ¹é¢æ¯dVAEåºçº¿é«åº9.31%ï¼å¹¶å°æ¶é´ä¸ç¨³å®æ§éä½äº37.1%ãè¿è¯å®äºå¯¹ææ§ç®æ å¨æ¶é¤æ¶é´ä¼ªå½±åä¿æè¿å¨ç²¾ç»è½¨è¿¹æ¹é¢çå³é®ä½ç¨ã
*   <strong>è¿å¨å¤ææ§åæï¼</strong> å¯éæ è®°åç­ç¥æ­ç¤ºäº2Dè¿å¨å¯ä»¥éè¿ç´§åç128ä¸ªæ è®°è¯æ±è¡¨è¿è¡æä½³è¡¨ç¤ºï¼è3Dè¿å¨çå¤ææ§éè¦æ´å¤§ç1024ä¸ªæ è®°ç æ¬æè½å®ç°å¿ å®éå»ºãè¿ä¸åç°ä¸ºè®¾è®¡é«æãç»´åº¦æç¥çåç¼©æ¨¡åæä¾äºååæ§æå¯¼ã
*   <strong>å®éé¨ç½²å¯è¡æ§ï¼</strong> è®ºæç»æè¡¨æï¼è¯¥æ¡æ¶å¨å®éåç¼©çä¸ï¼ä¾å¦F16åç¼©æ¶2Dè¿å¨SSIMè¾¾å°95.4%ï¼3Dè¿å¨SSIMè¾¾å°91.2%ï¼ä»è½ä¿æé«ä¿çåº¦ï¼è¯æäºä½¿ç¨ç¦»æ£æ è®°åå¤çé«è¦æ±è¿å¨åæä»»å¡çå¯è¡æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææ¿è®¤ï¼å°½ç®¡å¶æ¶æå»ºç«äºæ°çææ¯æ°´å¹³ï¼ä½æ´å·è¡¨ç°åçç¼ç å¨ï¼å¦åºäºTransformerçè®¾è®¡ï¼ä»£è¡¨äºæªæ¥å¢å¼ºçæåæ¯çéå¾ãè¿æç¤ºäºå½åæ¨¡åå¯è½å°æªååå©ç¨ææåè¿çç¼ç ææ¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ä¸æ¸¸åºç¨ï¼</strong> æ¨¡åçæçç´§åä¸è¯­ä¹ä¸°å¯çè¿å¨æ è®°å¯ä»¥ä½ä¸ºæªæ¥ä¸æ¸¸ä»»å¡ï¼å¦å¨ä½åç±»ãè¿å¨é¢æµåå¼å¸¸æ£æµï¼çåºç¡éª¨å¹²ãéè¿ç¨è¿å¨æ è®°åºåæ¿æ¢åå§é«ç»´è§é¢æ°æ®ï¼åç±»å¨å¯ä»¥å¨æ´å°ä½ä¿¡æ¯ä¸°å¯çè¾å¥ä¸è¿è¡ï¼ä»èå¯è½å®ç°æ´å¿«çæ¨çåæ´å¥½çæ³åã
*   <strong>å¼å¸¸æ£æµï¼</strong> éè¿å¨å¤§éæ­£å¸¸äººä½è¡ä¸ºæ°æ®éä¸è¿è¡é¢è®­ç»ï¼å­¦ä¹ å°çè¿å¨è¯æ±è¡¨å¯ç¨äºè¯å«å¼å¸¸æä¸å¯»å¸¸çè¿å¨åºåã
*   <strong>æ´åè¿çç¼ç å¨ï¼</strong> æ¢ç´¢å°Transformerç­æ´å·è¡¨ç°åçç¼ç å¨éæå°VQ-GANæ¡æ¶ä¸­ï¼ä»¥è¿ä¸æ­¥æåæ§è½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion.</li>
<li>Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines.</li>
<li>Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.</li>
<li>Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.19252v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.19252v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.19245v1'></a></p>
<h2 id="convis-bench-estimating-video-similarity-through-semantic-concepts"><a href="https://arxiv.org/abs/2509.19245v1">ConViS-Bench: Estimating Video Similarity Through Semantic Concepts</a></h2>
<p><strong>Authors:</strong> Benedetta Liberatori, Alessandro Conti, Lorenzo Vaquero, Yiming Wang, Elisa Ricci, Paolo Rota</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Benedetta Liberatoriç­äººæ°åçè®ºæâConViS-Bench: Estimating Video Similarity Through Semantic Conceptsâçå¨é¢æè¦ï¼</p>
<p><strong>1. è®ºæä¸»è¦è§£å³çé®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³è§é¢ç¸ä¼¼æ§è¯ä¼°ä¸­é¿æå­å¨çé®é¢ãä¼ ç»æ¹æ³éå¸¸ä¾èµäºå®½æ³çå¨å±ç¸ä¼¼åº¦åæ°ï¼è¿ä½¿å¾æ¨¡åé¾ä»¥åäººç±»ä¸æ ·æ ¹æ®è§é¢çä¸åè¯­ä¹æ¦å¿µï¼å¦å¨ä½ãå°ç¹ãä¸»ä½ç­ï¼è¿è¡ç»è´çæ¯è¾ãå·ä½æ¥è¯´ï¼è®ºææåºäºä¸ä¸ªæ ¸å¿é®é¢ï¼å¦ä½éåè§é¢å¨ç¹å®è¯­ä¹æ¦å¿µä¸çç¸ä¼¼æ§ï¼å¹¶ä¸ºè§é¢çè§£æ¨¡åæä¾ä¸ä¸ªè½å¤åäººç±»ä¸æ ·è¿è¡æ¦å¿µåæ¨ççåºåã</p>
<p><strong>2. ä¸»è¦åæ°ç¹ææ¹æ³å­¦è´¡ç®</strong>
*   <strong>å¼å¥ConViSä»»å¡ï¼</strong> è®ºææåºäºâåºäºæ¦å¿µçè§é¢ç¸ä¼¼æ§ä¼°è®¡âï¼ConViSï¼è¿ä¸æ°ä»»å¡ãå®è¶è¶äºä¼ ç»çå¨å±è¯åï¼éè¿è®¡ç®è·¨é¢å®ä¹è¯­ä¹æ¦å¿µï¼å¦ä¸»è¦å¨ä½ãä¸»ä½ãç©ä½ãå°ç¹åå¨ä½é¡ºåºï¼çå¯è§£éç¸ä¼¼åº¦åæ°æ¥æ¯è¾è§é¢å¯¹ãè¿ç§æ¹æ³åè®¸ç¨æ·æ ¹æ®èªç¶è¯­è¨å®ä¹çè¯­ä¹ç»´åº¦è¿è¡çµæ´»çæ¯è¾ï¼å¹¶å¯èåä¸ºæ´ä½ç¸ä¼¼åº¦åæ°ã
*   <strong>åå¸ConViS-Benchåºåæ°æ®éï¼</strong> ä¸ºäºæ¯æConViSä»»å¡ï¼è®ºææå»ºå¹¶åå¸äºConViS-Benchï¼è¿æ¯ä¸ä¸ªåå«610å¯¹è§é¢å¯¹çæ°åºåæ°æ®éãè¿äºè§é¢å¯¹ç»è¿äººå·¥æ æ³¨ï¼åå«æ¦å¿µçº§å«çç¸ä¼¼åº¦åæ°ï¼1å°5åï¼ä»¥åå¯¹ç¸ä¼¼ç¹åå·®å¼ç¹çèªç±ææ¬æè¿°ãè¯¥æ°æ®éæ¶µçäº16ä¸ªä¸åçé¢åï¼æ¯ç°æåºåæ´å¹¿æ³ï¼å¹¶ä¸è§é¢å¹³åæ¶é¿æ´é¿ï¼æä¾äºä¸°å¯çè¯­ä¹åè§è§å¤æ ·æ§ã
*   <strong>å¯¹LMMsè¿è¡å¹¿æ³åºåæµè¯ï¼</strong> è®ºæå¯¹å¤ç§æåè¿çå¤§åå¤æ¨¡ææ¨¡åï¼LMMsï¼å¨ConViSä»»å¡ä¸çæ§è½è¿è¡äºè¯ä¼°ï¼åæ¬mPLUG-Owl3ãLLaVAç³»åãQwen-VLç³»åãInternVLç³»ååGemini 2.0-Flashãè¿æä¾äºå³äºè¿äºæ¨¡åä¸äººç±»å¤æ­ä¸è´æ§çæ·±å¥è§è§£ï¼å¹¶æ­ç¤ºäºå®ä»¬å¨æ¦å¿µçè§£æ¹é¢çä¼å¿åå±éæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>LMMsæ§è½å·®å¼æ¾èï¼</strong> è¯ä¼°ç»ææ¾ç¤ºï¼LMMså¨ConViSä»»å¡ä¸è¡¨ç°åºæ¾èçæ§è½å·®å¼ãè¾å¤§çæ¨¡åéå¸¸ä¼äºè¾å°çæ¨¡åï¼å¶ä¸­LLaVA-OV-7Bå¨æ´ä½ç¸å³æ§æ¹é¢è¡¨ç°æä½³ã
*   <strong>æ¦å¿µæææ§ï¼</strong> æäºæ¦å¿µï¼å¦âå¨ä½é¡ºåºâï¼å¯¹æææ¨¡åæ¥è¯´é½æ´å·æææ§ï¼èå¶ä»æ¦å¿µï¼å¦âä¸»è¦ç©ä½âæâå°ç¹âï¼åç¸å¯¹å®¹æãè¿è¡¨æç°ææ¨¡åå¨å»ºæ¨¡ç©ºé´ä¸ä¸æåæ¶é´ç»ææ¹é¢å­å¨ä¸è¶³ã
*   <strong>æ¶é´ä¸ä¸æçéè¦æ§ï¼</strong> æªå¨FineVideoä¸é¢è®­ç»çæ¨¡åï¼å¦LLaVA-OV/LLaVA-Video/Qwen2.5-VLï¼å¨è¾å¥å¸§æ°åå°æ¶ï¼æ§è½æ¾èä¸éï¼è¡¨æå®ä»¬å¯¹ä¸°å¯çæ¶é´ä¸ä¸æçä¾èµæ§ãèInternVLç³»åæ¨¡åï¼å¨FineVideoä¸é¢è®­ç»è¿ï¼è¡¨ç°åºç¸å¯¹å¹³ç¨³çæ§è½è¶å¿ï¼å¯è½å­å¨è®°å¿æåºã
*   <strong>å¨å±è¡¨ç¤ºçå±éæ§ï¼</strong> å¯¹è®¡ç®å¨å±è§é¢ç¸ä¼¼åº¦åæ°çæ¨¡åè¿è¡åæåç°ï¼è§é¢å°è§é¢çæ¹æ³æ´ä¾§éäºâå°ç¹âæ¦å¿µï¼èææ¬å°ææ¬çæ¹æ³åæé¿æè·ä¸âå¨ä½âç¸å³çæ¦å¿µãè·¨æ¨¡ææ¹æ³ï¼å¦VQAScoreï¼å¨æææ¦å¿µä¸åå¾äºæé«çå¹³åç¸å³æ§ï¼ä½æ²¡æåä¸æ¨¡åå¨æææ¦å¿µä¸é½è¡¨ç°æä½³ã
*   <strong>æ¦å¿µæ¡ä»¶è§é¢æ£ç´¢ï¼</strong> å¨æ¦å¿µæ¡ä»¶è§é¢æ£ç´¢ä»»å¡ä¸­ï¼LMMsçR@1åæ°æ®éé«äºP@1ï¼è¡¨æå®ä»¬å¨æ£ç´¢ç¸å³è§é¢æ¹é¢è¡¨ç°è¯å¥½ï¼ä½ä¹äº§çè¾å¤çåé³æ§ãè½ç¶LMMsè¡¨ç°åºä¸å®çè½åï¼ä½æ¦å¿µæ¡ä»¶è§é¢å°è§é¢æ£ç´¢ä»æ¯å¶å±éæ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>æ¦å¿µéçèå´ï¼</strong> å½åçConViSæ¦å¿µéæ¯éç¨æ§çï¼å¯è½æ æ³æè·ææç¹å®é¢åæç»ç²åº¦çè§é¢ç¸ä¼¼æ§æ¹é¢ã
*   <strong>æ°æ®éè§æ¨¡ï¼</strong> ConViS-Benchç®ååå«ç¸å¯¹éåº¦çè§é¢å¯¹æ°éãè½ç¶è®ºæä¼åèèäºæ æ³¨è´¨éèéæ°éï¼ä½è§æ¨¡çéå¶å¯è½ä¼å½±åæ´å¹¿æ³çè¯ä¼°ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>æ©å±æ¦å¿µåç±»ï¼</strong> å°ConViSæ¡æ¶æ©å±å°åå«æ´å¤ç¹å®é¢åæç»ç²åº¦çæ¦å¿µï¼ä»¥å®ç°æ´ä¸°å¯çä¸ä¸ååæã
*   <strong>æ©åæ°æ®éï¼</strong> å¨ä¿æè´¨éæ åçåæä¸ï¼æ©åConViS-Benchæ°æ®éçè§æ¨¡ï¼ä»¥æ¯ææ´å¹¿æ³çè¯ä¼°åç ç©¶ã
*   <strong>æ¹è¿æ¨¡åçæ¶é´çè§£ï¼</strong> è¿ä¸æ­¥ç ç©¶åå¼åè½å¤æ´å¥½å°çè§£è§é¢æ¶é´ç»æåå¨ä½é¡ºåºçæ¨¡åã
*   <strong>å¼åæ´å·è§£éæ§åå¯æ§æ§çè§é¢çè§£ç³»ç»ï¼</strong> ConViSä¸ºæå»ºæ´å·è§£éæ§ãå¯æ§æ§åç¨æ·å¯¹é½çè§é¢çè§£ç³»ç»æä¾äºéå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks.</li>
<li>We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts.</li>
<li>ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval.</li>
<li>To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.</li>
<li>Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.19245v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.19245v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.19203v1'></a></p>
<h2 id="vision-free-retrieval-rethinking-multimodal-search-with-textual-scene-descriptions"><a href="https://arxiv.org/abs/2509.19203v1">Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions</a></h2>
<p><strong>Authors:</strong> Ioanna Ntinou, Alexandros Xenos, Yassine Ouali, Adrian Bulat, Georgios Tzimiropoulos</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâVision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptionsâè®ºæçå¨é¢æè¦ï¼ç±Ioanna Ntinou, Alexandros Xenos, Yassine Ouali, Adrian BulatåGeorgios Tzimiropoulosæ°åã</p>
<p><strong>è®ºææè¦ï¼Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå¯¹æ¯è®­ç»çè§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¦CLIPï¼å¨æ£ç´¢ä»»å¡ä¸­å­å¨çå ä¸ªå³é®å±éæ§ãè¿äºé®é¢åæ¬ï¼
*   <strong>æµå±è¯­è¨çè§£åâè¯è¢âè¡ä¸ºï¼</strong> VLMså¾å¾ç¼ºä¹å¯¹è¯­è¨ç»æçæ·±å¥çè§£ï¼å¯¼è´å¶å¨å¤çå¤ææç»åæ§æ¥è¯¢æ¶è¡¨ç°ä¸ä½³ã
*   <strong>æ¨¡æé¸¿æ²ï¼Modality Gapï¼ï¼</strong> åç¼ç å¨è®¾è®¡ï¼å¾ååææ¬åå«ç¼ç ï¼å¨ä¸åæ¨¡æçåµå¥ç©ºé´ä¹é´é æäºä¸ä¸è´æ§ï¼å½±åäºæ¨¡åçå¬å¹³æ§åç»åè½åã
*   <strong>è®¡ç®ææ¬åéç§é®é¢ï¼</strong> è®­ç»VLMséè¦å¤§éç½ç»æ¶éçæ°æ®ï¼è¿ä¸ä»è®¡ç®æè´µï¼è¿å¼åäºæ¾èçéç§æå¿§ã
*   <strong>ç°æåºåçä¸è¶³ï¼</strong> ç°æçææ¬-å¾åæ£ç´¢åºåï¼å¦Flickr30kåCOCOï¼ä¸»è¦ä¾èµé¿å¥æè¿°ï¼æªè½ååè¯ä¼°æ¨¡åå¨ç­çãç»åæ§æ¥è¯¢ä¸çæ§è½ã</p>
<p>ä¸ºäºè§£å³è¿äºé®é¢ï¼è®ºæææäºå¨æ£ç´¢ä»»å¡ä¸­è§è§ç¼ç å¨çå¿è¦æ§ï¼å¹¶æåºäºä¸ç§æ°çâæ è§è§âæ£ç´¢èå¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿åæ°å¨äºå¼å¥äºä¸ä¸ªåä¸º<strong>LexiCLIP</strong>çâæ è§è§ãåç¼ç å¨æ£ç´¢ç®¡éâï¼å¶ä¸»è¦è´¡ç®åæ¬ï¼
*   <strong>èå¼è½¬åï¼ä»ææ¬-å¾åå°ææ¬-ææ¬æ£ç´¢ï¼</strong> è®ºææ¾å¼äºä¼ ç»çææ¬-å¾åæ£ç´¢ï¼è½¬èéç¨ææ¬-ææ¬èå¼ãéè¿å¤§åè§è§-è¯­è¨æ¨¡åï¼VLLMï¼çæç»æåçå¾åæè¿°ï¼å°å¾ååå®¹å®å¨è½¬æ¢ä¸ºææ¬ï¼ä»èä½¿è¯­è¨æ¨¡åè½å¤çº¯ç²¹éè¿ææ¬æ¨çè§è§åå®¹ã
*   <strong>å¾åå°ææ¬è½¬æ¢ç®¡éï¼</strong> æåºäºä¸ç§é²æ£ãæååä¸ç²¾å¿è®¾è®¡çç®¡éï¼ç¨äºå°ä¸°å¯çè§è§ä¿¡æ¯åç¡®å°è½¬æ¢ä¸ºææ¬æè¿°ãè¿åæ¬çæè¯¦ç»çåºæ¯æè¿°åç»æåçå¯¹è±¡æ³¨éï¼åå«å¯¹è±¡ç±»å«ãå±æ§ãå¨ä½ãä½ç½®ç­ï¼ï¼å¹¶éç¨JSONæ ¼å¼ä»¥ç¡®ä¿è¾åºçè¿è´¯æ§åç»ç»æ§ã
*   <strong>åç¼ç å¨æ¶æï¼</strong> éè¿å°å¾åè½¬æ¢ä¸ºææ¬ï¼LexiCLIPè½å¤å©ç¨å±äº«çåç¼ç å¨æ¶æï¼æ¾èåå°äºæ¨¡æé¸¿æ²ï¼å¹¶å©ç¨é¢è®­ç»è¯­è¨æ¨¡åçä¸°å¯è¯­è¨ç¥è¯ã
*   <strong>è½»éçº§æ ¡åï¼</strong> è¯¥æ¹æ³ä»éå¨å°éGPUä¸è¿è¡æ°å°æ¶çæ ¡åï¼å³å¯å®ç°æ¾èæ§è½æåï¼é¿åäºæè´µçä»å¤´å¼å§è®­ç»ã
*   <strong>éç§åå¥½ï¼</strong> ç¨ææ¬æè¿°æ¿ä»£åå§å¾åï¼æä¾äºä¸ç§æ´æ³¨ééç§çæ£ç´¢æ¿ä»£æ¹æ¡ï¼å ä¸ºå¤§å¤æ°èº«ä»½ç¸å³ä¿¡æ¯ï¼å¦é¢é¨ãç§äººæ¿é´ï¼å¨è½¬æ¢è¿ç¨ä¸­è¢«ç§»é¤ã
*   <strong>æ°åºåæ°æ®éï¼</strong> ä¸ºäºæ´å¨é¢å°è¯ä¼°æ¨¡åå¨ç­çãç»åæ§æ¥è¯¢ä¸çæ³åè½åï¼è®ºæåå¸äºä¸¤ä¸ªæ°çåºåæ°æ®éï¼<strong>subFlickr</strong>å<strong>subCOCO</strong>ï¼å®ä»¬ä»Flickr30kåCOCOä¸­æåï¼åå«å¤æ ·åçç­ç»åæ§æ¥è¯¢ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¨¡æé¸¿æ²æ¾èåå°ï¼</strong> LexiCLIPéè¿å¶åç¼ç å¨è®¾è®¡ï¼æ¾èç¼©å°äºææ¬åå¾ååµå¥ä¹é´çæ¨¡æé¸¿æ²ï¼æé«äºæ¨¡åå¨è·¨æ¨¡ææ£ç´¢ä¸­çå¯¹é½è½åã
*   <strong>æ¹è¿çç»åæ§åæ§è½ï¼</strong> å¨ç­åé¿æ é¢æ¥è¯¢ä¸ï¼LexiCLIPè¡¨ç°åºæ´å¥½çç»åæ§åæ§è½ãå¨SugarCrepeåSugarCrepe++ç­ç»åæ§åºåæµè¯ä¸­ï¼å³ä½¿æ¯0.3Båæ°çå°æ¨¡åï¼ä¹è¾¾å°äºæåè¿çé¶æ ·æ¬æ§è½ï¼çè³è¶è¶äºä¼ ç»çãåæ°éæ´å¤§çå¤æ¨¡ææ¨¡åã
*   <strong>é¶æ ·æ¬åå¾®è°æ§è½ï¼</strong> å¨Flickr30kåCOCOç­æ ååºåä¸ï¼LexiCLIPå¨é¶æ ·æ¬è®¾ç½®ä¸è¡¨ç°åºè²ï¼ç»è¿å°éææ¬æ°æ®å¾®è°åï¼æ§è½è¿ä¸æ­¥æåï¼çè³è¶è¶äºOpenCLIPç­å¤§åæ¨¡åã
*   <strong>é¿ææ¬æ£ç´¢è½åï¼</strong> å©ç¨é¢è®­ç»è¯­è¨æ¨¡åå¤çéç¨ææ¬çè½åï¼LexiCLIPå¨é¿ææ¬æ£ç´¢ä»»å¡ï¼å¦Urbanlkæ°æ®éï¼ä¸è¡¨ç°ä¼å¼ï¼è¶è¶äºææå¶ä»CLIPåä½åä¸é¨ä¸ºé¿æ é¢å¾®è°çæ¨¡åã
*   <strong>å¯¹VLLMéæ©çé²æ£æ§ï¼</strong> è®ºæåæäºä¸åVLLMæ¶æåå¤§å°å¯¹æ§è½çå½±åï¼åç°InternVL-2.5-8B-MPOè¡¨ç°æä½³ï¼ä¸LexiCLIPå¯¹å¾åæè¿°çæå¨çè§æ¨¡å·æé²æ£æ§ï¼å³ä½¿æ¯è¾å°ç1Bæ¨¡åä¹è½å®ç°é«ææ§è½ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>å¯¹VLLMçå¼ºä¾èµï¼</strong> è¯¥æ¹æ³ä¸¥éä¾èµVLLMçæå¾åæè¿°ï¼å¯è½å¯¼è´æäºè§è§ç»èï¼å°¤å¶æ¯æ¥æ¤åºæ¯æå°ç©ä½ï¼çä¸¢å¤±ã
*   <strong>VLLMåå·®åå¹»è§çç»§æ¿ï¼</strong> VLLMå¯è½å­å¨åå·®æå¹»è§ï¼çæçæè¿°å¯è½ä¼ç»§æ¿è¿äºéè¯¯ï¼å¹¶ä¼ æ­å°æ£ç´¢è¿ç¨ä¸­ã
*   <strong>é¢å¤çè®¡ç®å¼éï¼</strong> ä½¿ç¨VLLMä½ä¸ºå¾åæè¿°å¨å¼å¥äºé¢å¤çè®¡ç®å¼éï¼å°½ç®¡è¿ä¸æ­¥éª¤æ¯ç¦»çº¿æ§è¡çï¼ä¸æ£ç´¢æ¬èº«ä»ç¶é«æï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ç¼è§£VLLMå±éæ§ï¼</strong> æªæ¥çå·¥ä½å°å°è¯éç¨è¿æ»¤æ¹æ³ãéæå¤ä¸ªæè¿°å¨ï¼ensemble captionersï¼ï¼çè³å¼å¥äººå·¥éªè¯ï¼ä»¥åè½»VLLMçåå·®åå¹»è§é®é¢ã
*   <strong>ä¼åè®¡ç®æçï¼</strong> éè¿ä¼åå®ç°ï¼å¦v1lmé¡¹ç®ï¼åéåææ¯ï¼è¿ä¸æ­¥éä½VLLMçæå¾åæè¿°çé¢å¤çææ¬ã
*   <strong>æ¢ç´¢æ´å°ççæå¨ï¼</strong> è®ºææåºï¼ä½¿ç¨è¾å°ççæå¨ï¼å¦2B InternVLï¼ä¹è½è·å¾ç¸ä¼¼çæ£ç´¢æ§è½ï¼è¿ä¸ºéä½é¢å¤çææ¬æä¾äºæ¹åã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥æ è§è§ãåç¼ç å¨çLexiCLIPæ¡æ¶ï¼å¯¹å¤æ¨¡ææ£ç´¢é¢åæåºäºæ ¹æ¬æ§çææååæ°ãå®ä¸ä»è§£å³äºç°æVLMsçæ¨¡æé¸¿æ²ãæµå±è¯­è¨çè§£åéç§é®é¢ï¼è¿å¨å¤ä¸ªæ£ç´¢åç»åæ§åºåä¸å®ç°äºæåè¿çæ§è½ï¼ä¸ºæªæ¥æ´é«æãæ´éç§ãæ´å·ç»åæ§çå¤æ¨¡ææç´¢å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval.</li>
<li>Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.19203v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.19203v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.19165v1'></a></p>
<h2 id="rose-robust-self-supervised-stereo-matching-under-adverse-weather-conditions"><a href="https://arxiv.org/abs/2509.19165v1">RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions</a></h2>
<p><strong>Authors:</strong> Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, Dapeng Oliver Wu</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yun Wangç­äººæ°åçè®ºæâRoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditionsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºæé¢ç®ï¼</strong> RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions</p>
<p><strong>ä½èï¼</strong> Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, Dapeng Oliver Wu</p>
<p><strong>æè¦ï¼</strong></p>
<ol>
<li>
<p><strong>ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
    è¯¥è®ºææ¨å¨è§£å³èªçç£ç«ä½å¹éæ¹æ³å¨æ¶å£å¤©æ°æ¡ä»¶ï¼å¦å¤æãé¨å¤©åé¾å¤©ï¼ä¸æ§è½æ¾èä¸éçé®é¢ãä½èè¯å«åºå¯¼è´æ§è½ä¸éçä¸¤ä¸ªä¸»è¦å¼±ç¹ï¼</p>
<ul>
<li>æ¶å£å¤©æ°å¼å¥çåªå£°åè½è§åº¦éä½ï¼ä½¿å¾åºäºCNNçç¹å¾æåå¨é¾ä»¥å¤çåå°åæ çº¹çåºåç­éååºåã</li>
<li>è¿äºéååºåä¼ç ´ååç¡®çåç´ å¯¹åºå³ç³»ï¼å¯¼è´åºäºååº¦ä¸è´æ§åè®¾ççç£ä¿¡å·å¤±æã</li>
</ul>
</li>
<li>
<p><strong>å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
    ä¸ºäºè§£å³ä¸è¿°ææï¼ä½èæåºäºRoSeï¼Robust Self-supervised Stereo Matchingï¼ï¼ä¸ä¸ªä¸¤é¶æ®µçèªçç£è®­ç»èå¼ï¼å¶æ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>é²æ£ç¹å¾æåå¨ï¼</strong> æåºå°è§è§åºç¡æ¨¡åï¼å¦SAMåDAMv2ï¼çé²æ£åéªæ³¨å¥å°CNN-basedç¹å¾æåå¨ä¸­ï¼ä»¥æ¹åæ¶å£å¤©æ°ä¸çç¹å¾è¡¨ç¤ºãæ­¤å¤ï¼å¼å¥äº<strong>ææ¶å£å¤©æ°ç¹å¾å¢å¼ºæ¨¡åï¼AFEMï¼</strong>ï¼éè¿å¨ç©ºé´ãééåé¢çåå¤çï¼ææåç¦»ä¸éåç¸å³çåªå£°åææä¹çåºæ¯ç¹å¾ï¼çæéçº§ä¸åçç¹å¾ã</li>
<li><strong>åºæ¯å¯¹åºåéªæå»ºé²æ£çç£ä¿¡å·ï¼</strong> ä¸åä»ä»ä¾èµååº¦ä¸è´æ§åè®¾ï¼èæ¯å¼å¥åºæ¯å¯¹åºåéªæ¥æå»ºé²æ£ççç£ä¿¡å·ãå·ä½åæ³æ¯ï¼<ul>
<li>åå»ºåæç«ä½æ°æ®éï¼åå«å·æçå®å¤©æ°éåçæ¸æ°åæ¶å£å¾åå¯¹ï¼è¿äºå¾åå¯¹ä¿æç¸åçè¯­ä¹ä¸ä¸æåè§å·®ï¼ä»èä¿çåºæ¯å¯¹åºå±æ§ã</li>
<li><strong>ä¸¤é¶æ®µèªçç£è®­ç»èå¼ï¼</strong><ul>
<li><strong>ç¬¬ä¸æ­¥ï¼é²æ£èªçç£åºæ¯å¯¹åºå­¦ä¹ ï¼</strong> å¼å¥ä¸¤ä¸ªåæ¯åå«å¤çæ¸æ°åæ¶å£å¤©æ°æ¡ä»¶ä¸çå¾åå¯¹ãå©ç¨åºæ¯å¯¹åºåéªï¼æåºäº<strong>ç¹å¾ä¸è´æ§æå¤±ï¼Feature Consistency Lossï¼</strong>å<strong>è§å·®ä¸è´æ§æå¤±ï¼Disparity Consistency Lossï¼</strong>ï¼ç¡®ä¿æ¸æ°åæ¶å£å¤©æ°æ¡ä»¶ä¸å­¦ä¹ å°çç¹å¾åè§å·®å¼ä¿æä¸è´ï¼ä»èå­¦ä¹ å°åå¤©æ°éåå½±åè¾å°çæ½å¨ç©ºé´ã</li>
<li><strong>ç¬¬äºæ­¥ï¼æ¶å£å¤©æ°è¸é¦ï¼</strong> å©ç¨ç¬¬ä¸æ­¥ä¸­å»ç»æ¨¡åçæçæ¸æ°å¾åå¯¹çé«è´¨éä¼ªæ ç­¾ä½ä¸ºçç£ä¿¡å·ï¼ææç¼è§£ååº¦ä¸è´æ§åè®¾å¨é®æ¡åºåç­çæåºåçå¤±æé®é¢ã</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>ä¸»è¦ç»æåå¶æä¹ï¼</strong></p>
<ul>
<li>å¹¿æ³çå®éªè¯æäºRoSeè§£å³æ¹æ¡çæææ§åéç¨æ§ãå¨DrivingStereoåMS2ç­åå«æ¶å£å¤©æ°æ¡ä»¶çæ°æ®éä¸ï¼RoSeçæ§è½ä¼äºç°æçæåè¿èªçç£æ¹æ³ã</li>
<li>å¨KITTI 2012åKITTI 2015åºåæµè¯ä¸ï¼RoSeä¹åå¾äºæå·ç«äºåçæ§è½ï¼å¹¶ä¸å¨æ ååæææ§æ¡ä»¶ä¸ï¼å¶æ¡æ¶æ¾èä¼äºä¹åçèªçç£å·¥ä½ã</li>
<li>å®æ§åå®éç»æè¡¨æï¼RoSeå¨åç§æ¶å£å¤©æ°æ¡ä»¶ä¸ï¼å¦å¤é´ãé¨å¤©ãé¾å¤©ï¼é½è½è¿è¡é²æ£çè§å·®ä¼°è®¡ï¼å°¤å¶å¨æææ§åºåè¡¨ç°åºè²ã</li>
<li>æ¨¡åæçæ¹é¢ï¼RoSeå¨åå­æ¶èåæ¨çæ¶é´ä¸è¡¨ç°åçã</li>
</ul>
</li>
<li>
<p><strong>è®ºæä¸­æåçå±éæ§ï¼</strong></p>
<ul>
<li>RoSeé«åº¦ä¾èµäºå¾åå°å¾åç¿»è¯æ¨¡åçæçåææ¶å£å¤©æ°å¯¹ççå®æ§åä¸è´æ§ãç¿»è¯ä¸­çä¸åç¡®æä¸ä¸è´ï¼å¦å ä½å¤±çæè¯­ä¹ä¸å¹éï¼å¯è½åçç£ä¿¡å·å¼å¥åªå£°ï¼ä»èå¯è½éä½æ´ä½æ§è½ã</li>
<li>å¨ä¸¥éçå¾åéåæ¡ä»¶ä¸ï¼RoSeå¯è½ä¼äº§çä¸å¯é çè§å·®ä¼°è®¡ï¼è¿å¨èªå¨é©¾é©¶ç­å®å¨å³é®åºç¨ä¸­ææé£é©ï¼å ä¸ºåç¡®çæ·±åº¦æç¥å¯¹äºå¯¼èªåé¿éè³å³éè¦ã</li>
</ul>
</li>
<li>
<p><strong>æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
    è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶å±éæ§å¯ä»¥æ¨æ­åºï¼</p>
<ul>
<li>è¿ä¸æ­¥æåå¾åå°å¾åç¿»è¯æ¨¡åççå®æ§åä¸è´æ§ï¼ä»¥çææ´é«è´¨éçæ¶å£å¤©æ°åææ°æ®ã</li>
<li>ç ç©¶å¨æç«¯æ¶å£å¾åéåæ¡ä»¶ä¸æé«è§å·®ä¼°è®¡å¯é æ§çæ¹æ³ï¼ä»¥æ»¡è¶³å®å¨å³é®åºç¨çéæ±ã</li>
<li>æ¢ç´¢å¦ä½å°RoSeçé²æ£æ§æ©å±å°æ´å¤æ ·åçæ¶å£å¤©æ°ç±»åææ´å¤æçåºæ¯ã</li>
</ul>
</li>
</ol>
<hr />
<p>æ»èè¨ä¹ï¼RoSeè®ºæéè¿å¼å¥è§è§åºç¡æ¨¡åçé²æ£åéªååºæ¯å¯¹åºåéªï¼å¹¶è®¾è®¡äºä¸ä¸ªä¸¤é¶æ®µçèªçç£è®­ç»èå¼ï¼æåè§£å³äºèªçç£ç«ä½å¹éå¨æ¶å£å¤©æ°ä¸æ§è½ä¸éçé®é¢ãå¶æ ¸å¿è´¡ç®å¨äºéè¿éçº§ä¸åç¹å¾æååé²æ£çç£ä¿¡å·æå»ºï¼æ¾èæåäºæ¨¡åå¨æææ§ç¯å¢ä¸­çé²æ£æ§åæ³åè½åï¼ä¸ºèªå¨é©¾é©¶ç­å®éåºç¨æä¾äºæ´å¯é çæ·±åº¦æç¥è§£å³æ¹æ¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions.</li>
<li>With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation.</li>
<li>Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.19165v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.19165v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.19129v1'></a></p>
<h2 id="kamera-enhancing-aerial-surveys-of-ice-associated-seals-in-arctic-environments"><a href="https://arxiv.org/abs/2509.19129v1">KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments</a></h2>
<p><strong>Authors:</strong> Adam Romlein, Benjamin X. Hou, Yuval Boss, Cynthia L. Christman, Stacie Koslovsky, Erin E. Moreland, Jason Parham, Anthony Hoogs</p>
<p><strong>Published:</strong> 2025-09-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Adam Romleinç­äººæ°åçè®ºæâKAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environmentsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼KAMERA: æååæç¯å¢ä¸­å°ç¸å³æµ·è±¹èªç©ºæµéçæç</strong></p>
<p>è¿ç¯è®ºæä»ç»äºKAMERAï¼Knowledge-guided Image Acquisition ManagER and Archiverï¼ï¼ä¸ä¸ªæ¨å¨æ¾èæååæå°åºå°ç¸å³æµ·è±¹ååæçèªç©ºæµéæçååç¡®æ§çç»¼åç³»ç»ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ä¼ ç»çå°ç¸å³æµ·è±¹èªç©ºæµéæ¹æ³é¢ä¸´å¤éææï¼æ°æ®å¤çæ¶é´é¿ï¼éå¸¸éè¦æ°æçè³æ°å¹´ï¼ï¼ç³»ç»ç»ä»¶ï¼ç¡¬ä»¶åè½¯ä»¶ï¼ä¹é´è¦åæ¾æ£å¯¼è´æ°æ®åæ­¥åå¯¹é½å°é¾ï¼ä»¥åä¸æè½¯ä»¶éå¶äºç§å­¦ç¤¾åºçå¹¿æ³åºç¨ãè¿äºé®é¢é»ç¢äºå¯¹æµ·è±¹ç§ç¾¤çåæ¶çæµåç®¡çï¼å°¤å¶æ¯å¨æµ·å°æ æ¯å°ä¸æ­ååçèæ¯ä¸ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
KAMERAç³»ç»éè¿ä»¥ä¸åæ°è§£å³äºä¸è¿°é®é¢ï¼
*   <strong>å¤ç¸æºãå¤åè°±åæ­¥ï¼</strong> KAMERAå®ç°äºå¤ç¸æºãå¤åè°±å¾åééçç²¾ç¡®åæ­¥ï¼æææ°æ®é½éè¿åä¸å¤é¨æ¶é´èå²æ¶éï¼å¹¶èåå°å¸¦æè¯¦ç»åæ°æ®çä¸ä¸ªå­å¨ä½ç½®ãè¿ä½¿å¾è½å¤å©ç¨ä¸ååè°±ï¼RGBãIRãUVï¼çæ°æ®è¿è¡æ´åç¡®çç©ä½æ£æµã
*   <strong>å®æ¶æ£æµï¼</strong> ç³»ç»å©ç¨æºè½½GPUå¯¹åæ­¥å¾åè¿è¡å®æ¶åæï¼è½å¤å®æ¶è¯å«æµ·è±¹ååæçï¼å¹¶æ ¹æ®æ£æµç»æå³å®æ¯å¦å­æ¡£å¾åï¼ä»èå¤§å¹åå°äºéè¦å­å¨åå¤ççç©ºç½å¾åæ°æ®éã
*   <strong>ç²¾ç¡®æµç»ï¼</strong> ææå¾ååå¨ç©æ£æµç»æé½è¢«æ å°å°ä¸çå¹³é¢ä¸ï¼ä»¥å®ç°ç²¾ç¡®çæµéåºåä¼°ç®åå¿«éè¯ä¼°æµéç»æãè¿éè¿ä¸¥æ ¼çç¸æºæ ¡åï¼åæ¬åååå¤åï¼ä»¥åä¸æ¯æ§å¯¼èªç³»ç»ï¼INSï¼çå¯¹é½æ¥å®ç°ã
*   <strong>å¼æ¾æºä»£ç ï¼</strong> KAMERAçææè½¯ä»¶ãæ¨¡åååçå¾é½æ¯å®å¨å¼æºçï¼Apache LicenseåCC BY 4.0ï¼ï¼ä¿è¿äºç§å­¦ç¤¾åºçåä½åå¤ç¨ã
*   <strong>ä¸¤é¶æ®µæ£æµç®¡çº¿ï¼éå¯¹2021å¹´æµéï¼ï¼</strong> ç»åçº¢å¤ï¼IRï¼ç­ç¹æ£æµåå½©è²å¾åä¸­çç©ç§åç±»æ¨¡åãIRæ¨¡åé¦åè¯å«ç­ç¹ï¼ç¶åè£åªåºç¸åºçé«åè¾¨çå½©è²å¾ååºåï¼åç±ç©ç§ç¹å¼æ§å½©è²æ£æµå¨è¿è¡åç±»ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ°æ®å¤çæ¶é´å¤§å¹åå°ï¼</strong> KAMERAç³»ç»å°æ°æ®éå¤çæ¶é´æ¯ä»¥åçæ¹æ³åå°äºé«è¾¾80%ï¼ä¾å¦ï¼ä¸2016å¹´æ¥ç§å¥æµ·çæµéç¸æ¯ï¼2021å¹´ååç¦ç¹æµ·çæµéæ¶é´ä»6ä¸ªæç¼©ç­å°5å¨ã
*   <strong>é«ç²¾åº¦æ£æµæ¨¡åï¼</strong> è®ºæå±ç¤ºäºIRç­ç¹æ£æµæ¨¡åãä¸¤ç§æµ·è±¹ç©ç§åç±»æ¨¡åååæçæ£æµæ¨¡åçéªè¯ç»æãIRç­ç¹æ£æµåæµ·è±¹æ¨¡åå¨å¬åçä¸è¡¨ç°åºè²ï¼åè¾¾å°0.93ææ´é«ï¼ï¼è¿å¯¹äºæ°æ®æ¶éå³ç­è³å³éè¦ã
*   <strong>å®éåºç¨ï¼</strong> KAMERAå·²æååºç¨äºé¿ææ¯å ç½ä»¤æµ·ãæ¥ç§å¥æµ·ååç¦ç¹æµ·çèªç©ºæµéï¼æ¶éäºæ°ç¾ä¸å¾åæ ·æ¬ã
*   <strong>ç¨æ·åå¥½çé¢ï¼</strong> å¼åäºå¾å½¢ç¨æ·çé¢ï¼GUIï¼ï¼æ¯æå®æ¶ç¸æºæ§å¶ãç³»ç»çæ§åé£è¡åæ°æ®è¯ä¼°ï¼å³ä½¿éææ¯ç¨æ·ä¹è½æææä½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ ¡åæ¼ç§»ï¼</strong> ç¸æºæ ¡åä¼éæ¶é´æ¼ç§»ï¼éè¦å®ææ£æ¥åè°æ´ã
*   <strong>SIFTç¹å¾çå±éæ§ï¼</strong> å¨å¤åè°±å¹éä¸­ï¼SIFTç¹å¾å¨ç¨çç¹å¾åºååä¸ååè°±ä¹é´å¹éææä¸ä½³ã
*   <strong>æ¨¡åæ³åè½åï¼</strong> 2025å¹´æµéçåæ­¥è¯ä¼°æ¾ç¤ºï¼IRç­ç¹æ¨¡åå¨æ°ç­åä»ªåæ´æ°çNUCæ ¡åä¸æ§è½ææä¸éï¼è¡¨ææ¨¡åå¨ä¸åç¡¬ä»¶åå¾ååä¹é´çæ³åè½åæå¾æé«ã
*   <strong>æ°æ®ä¸å¹³è¡¡ï¼</strong> è®­ç»æ°æ®ä¸­æµ·è±¹ç±»å«ä¹é´ä»¥åä¸åæçæ ç­¾ä¹é´å­å¨ä¸¥éä¸å¹³è¡¡ï¼éè¦éç¨Focal Lossç­ææ¯æ¥è§£å³ã
*   <strong>UVæ¨¡åå¼åï¼</strong> ç®åå°æªè®­ç»UVæ¨¡åï¼ä»å¨å¼åä¸­ï¼ä¸»è¦ç¨äºåæçåç½è²å¹¼æµ·è±¹çæ£æµã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¹è¿æ ¡åæ¹æ³ï¼</strong> æ¢ç´¢åºäºæ·±åº¦å­¦ä¹ çç¹å¾åå³é®ç¹ï¼å¦SuperPointãSuperGlueåALIKEDï¼æ¥æ¹è¿å¤åè°±æ ¡åã
*   <strong>å¢å¼ºæ¨¡åé²æ£æ§ï¼</strong> è¿ä¸æ­¥åæåæ¹è¿æ¨¡åï¼ä»¥æé«å¶å¨ä¸åç¡¬ä»¶åå¾ååä¸çæ³åè½åï¼ä¾å¦å©ç¨2025å¹´æ¶éçæ°æ®è®­ç»æ°çIR-RGBæ¨¡åï¼å¹¶æ¢ç´¢YOLOv9ç­æ´ç°ä»£çæ¶æã
*   <strong>UVæ¨¡åå¼åï¼</strong> å®æå¹¶é¨ç½²ç¨äºåæçåç½è²å¹¼æµ·è±¹æ£æµçUVæ¨¡åã
*   <strong>æ´å¹¿æ³çåºç¨ï¼</strong> å°KAMERAç³»ç»æ¨å¹¿å°å¶ä»éçå¨ç©è°æ¥ï¼ä¾å¦é©¼é¹¿ãå¤§è±¡ææµ·é¾ã</p>
<hr />
<p>æ»èè¨ä¹ï¼KAMERAç³»ç»ä»£è¡¨äºèªç©ºæµéé¢åçä¸ä¸ªéå¤§è¿æ­¥ï¼éè¿å¶å¤ç¸æºãå¤åè°±åæ­¥ãå®æ¶æ£æµåç²¾ç¡®æµç»è½åï¼æ¾èæåäºæ°æ®æ¶éååæçæçä¸åç¡®æ§ãå¶å¼æ¾æºä»£ç çç¹æ§æææ¨å¨ç§å­¦ç¤¾åºå¨éçå¨ç©çæµåä¿æ¤æ¹é¢çè¿ä¸æ­¥åæ°ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.19129v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.19129v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-24 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
