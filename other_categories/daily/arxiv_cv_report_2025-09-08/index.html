<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-08 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-07/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-09/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-08">Arxiv Computer Vision Papers - 2025-09-08</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#wint3r-window-based-streaming-reconstruction-with-camera-token-pool" class="nav-link">WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</a>
                </li>
                <li class="nav-item">
                    <a href="#latticeworld-a-multimodal-large-language-model-empowered-framework-for-interactive-complex-world-generation" class="nav-link">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#enhancing-3d-point-cloud-classification-with-modelnet-r-and-point-skipnet" class="nav-link">Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</a>
                </li>
                <li class="nav-item">
                    <a href="#sgs-3d-high-fidelity-3d-instance-segmentation-via-reliable-semantic-mask-splitting-and-growing" class="nav-link">SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</a>
                </li>
                <li class="nav-item">
                    <a href="#robust-experts-the-effect-of-adversarial-training-on-cnns-with-sparse-mixture-of-experts-layers" class="nav-link">Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers</a>
                </li>
                <li class="nav-item">
                    <a href="#a-biologically-inspired-separable-learning-vision-model-for-real-time-traffic-object-perception-in-dark" class="nav-link">A biologically inspired separable learning vision model for real-time traffic object perception in Dark</a>
                </li>
                <li class="nav-item">
                    <a href="#propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granularity-discrimination" class="nav-link">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a>
                </li>
                <li class="nav-item">
                    <a href="#extracting-uncertainty-estimates-from-mixtures-of-experts-for-semantic-segmentation" class="nav-link">Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#flowseek-optical-flow-made-easier-with-depth-foundation-models-and-motion-bases" class="nav-link">FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</a>
                </li>
                <li class="nav-item">
                    <a href="#cogitao-a-visual-reasoning-framework-to-study-compositionality-generalization" class="nav-link">COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-08">Arxiv Computer Vision Papers - 2025-09-08</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份为忙碌的研究人员准备的 Arxiv 计算机视觉领域最新论文的执行摘要：</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025-09-05)</strong></p>
<p><strong>概述与主要趋势：</strong></p>
<p>今日 Arxiv 计算机视觉论文呈现出多模态融合、3D 视觉的持续深化、模型鲁棒性与不确定性估计的关注，以及对高效和通用视觉系统构建的探索。具体而言，我们看到了将大型语言模型（LLM）引入复杂世界生成和视觉推理的趋势，3D 点云处理在分类和分割方面的新进展，以及在对抗性训练和不确定性量化方面对模型可靠性的重视。此外，对实时、特定场景（如暗光交通）和高效流式处理的关注也表明了领域向实际应用落地的努力。</p>
<p><strong>特别重要或创新的论文：</strong></p>
<ol>
<li><strong>"LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation" (Yinglin Duan et al.)</strong>: 这篇论文通过将多模态大型语言模型（MLLM）引入交互式复杂世界生成，展示了LLM在视觉领域超越传统识别任务的巨大潜力，预示着AI内容生成和虚拟环境构建的新范式。</li>
<li><strong>"FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases" (Matteo Poggi, Fabio Tosi)</strong>: 该工作利用深度基础模型和运动基底简化光流估计，可能为光流领域带来更高效、更鲁棒的解决方案，尤其是在利用预训练模型知识方面具有创新性。</li>
<li><strong>"COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization" (Yassine Taoudi-Benchekroun et al.)</strong>: 这篇论文提出了一个视觉推理框架，旨在深入研究组合性和泛化能力，对于理解和提升AI模型的认知能力具有重要理论和实践意义。</li>
</ol>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>LLM/MLLM 与视觉任务的深度融合</strong>: 不仅仅是图像描述，而是利用LLM的推理和生成能力进行更复杂的视觉任务（如世界生成、交互）。</li>
<li><strong>基础模型在特定视觉任务中的应用</strong>: 利用预训练的深度基础模型（如深度估计）来辅助其他视觉任务（如光流），提高效率和性能。</li>
<li><strong>模型鲁棒性与不确定性估计的系统性研究</strong>: 针对对抗性攻击和模型预测不确定性，通过混合专家模型（MoE）等结构进行量化和提升。</li>
<li><strong>高效流式处理与实时感知</strong>: 针对视频流和特定应用场景（如自动驾驶）的实时、高效算法设计。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对多模态AI和生成式AI感兴趣的</strong>:<ul>
<li><strong>"LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation"</strong> (Yinglin Duan et al.)</li>
<li><strong>"COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization"</strong> (Yassine Taoudi-Benchekroun et al.)</li>
</ul>
</li>
<li><strong>对3D视觉和点云处理感兴趣的</strong>:<ul>
<li><strong>"SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing"</strong> (Chaolei Wang et al.)</li>
<li><strong>"Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet"</strong> (Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari)</li>
</ul>
</li>
<li><strong>对模型鲁棒性、不确定性估计和对抗性训练感兴趣的</strong>:<ul>
<li><strong>"Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers"</strong> (Svetlana Pavlitska et al.)</li>
<li><strong>"Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation"</strong> (Svetlana Pavlitska et al.)</li>
</ul>
</li>
<li><strong>对光流和运动估计感兴趣的</strong>:<ul>
<li><strong>"FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases"</strong> (Matteo Poggi, Fabio Tosi)</li>
</ul>
</li>
<li><strong>对实时系统和特定应用（如自动驾驶）感兴趣的</strong>:<ul>
<li><strong>"WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool"</strong> (Zizun Li et al.)</li>
<li><strong>"A biologically inspired separable learning vision model for real-time traffic object perception in Dark"</strong> (Hulin Li et al.)</li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要旨在帮助您快速把握今日Arxiv计算机视觉领域的关键进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.05296v1">WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</a></li>
<li><a href="#2509.05263v1">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></li>
<li><a href="#2509.05198v1">Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</a></li>
<li><a href="#2509.05144v1">SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</a></li>
<li><a href="#2509.05086v1">Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers</a></li>
<li><a href="#2509.05012v1">A biologically inspired separable learning vision model for real-time traffic object perception in Dark</a></li>
<li><a href="#2509.04833v1">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a></li>
<li><a href="#2509.04816v1">Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</a></li>
<li><a href="#2509.05297v1">FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</a></li>
<li><a href="#2509.05249v1">COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.05296v1'></a></p>
<h2 id="wint3r-window-based-streaming-reconstruction-with-camera-token-pool"><a href="https://arxiv.org/abs/2509.05296v1">WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</a></h2>
<p><strong>Authors:</strong> Zizun Li, Jianjun Zhou, Yifan Wang, Haoyu Guo, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Junyi Chen, Chunhua Shen, Tong He</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zizun Li等人撰写的论文“WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool”的全面摘要。</p>
<hr />
<p><strong>论文摘要：WinT3R: 基于窗口的带相机令牌池的流式重建</strong></p>
<p><strong>1. 解决的主要问题或研究问题</strong>
该论文旨在解决实时3D重建领域中长期存在的挑战，即重建质量和实时性能之间的权衡。传统的在线重建方法往往难以同时实现高精度的几何预测和快速的重建速度。具体来说，现有方法在处理图像流时，由于帧间信息交互不足或全局信息利用效率低下，导致重建质量次优，尤其是在相机姿态估计方面。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
WinT3R模型引入了两项核心创新来解决上述问题：</p>
<ul>
<li><strong>滑动窗口机制 (Sliding Window Mechanism):</strong> 为了确保窗口内帧之间以及相邻窗口之间有足够的信息交换，WinT3R采用了一种滑动窗口策略。与以往方法仅通过状态令牌间接共享信息不同，该机制允许窗口内的图像令牌直接相互作用，从而在不引入大量计算开销的情况下显著提高几何预测的质量。窗口大小设置为4，步长为2，确保了相邻窗口之间有一半的帧重叠，进一步增强了连续性。</li>
<li><strong>相机令牌池 (Camera Token Pool):</strong> 为了提高相机姿态估计的可靠性而不牺牲效率，WinT3R为每个相机帧维护一个紧凑的相机令牌表示，并将其存储在一个可扩展的全局相机令牌池中。在预测新到达帧的相机参数时，模型会利用池中所有历史相机令牌作为全局信息。这种紧凑的表示方式（每个相机帧一个1536维的令牌）大大减少了存储开销和计算成本，同时通过全局视角增强了相机姿态估计的鲁棒性。</li>
</ul>
<p>此外，论文还设计了一个带有滑动窗口掩码注意力机制的相机头部，以更好地利用紧凑的相机令牌进行预测，并采用轻量级卷积头部来预测局部点云图，避免了计算昂贵的DPT头部和可能引入网格状伪影的线性头部。</p>
<p><strong>3. 主要结果及其意义</strong>
WinT3R在多个数据集上进行了广泛的实验，并取得了最先进的性能：</p>
<ul>
<li><strong>3D重建质量:</strong> 在DTU、ETH3D、7-Scenes和NRGBD等数据集上，WinT3R在准确性、完整性和总体Chamfer距离方面均优于其他在线重建方法，实现了高质量的几何重建。</li>
<li><strong>相机姿态估计:</strong> 在Tanks and Temples、CO3Dv2和7-Scenes数据集上，WinT3R在相对旋转准确性（RRA）、相对平移准确性（RTA）和AUC@30等指标上表现出色，证明了其相机姿态估计的可靠性。</li>
<li><strong>重建速度:</strong> WinT3R实现了17 FPS的实时性能，是迄今为止最快的在线重建方法之一，在NVIDIA A800 GPU上运行。</li>
<li><strong>视频深度估计:</strong> 在Sintel、BONN和KITTI数据集上的实验表明，WinT3R在视频深度估计方面也达到了可比或更优的性能。</li>
</ul>
<p>这些结果表明，WinT3R成功地解决了在线3D重建中质量与速度的权衡问题，为实时、高精度的3D重建任务树立了新的标杆。</p>
<p><strong>4. 论文中提到的局限性</strong>
论文中没有明确指出当前WinT3R模型的具体局限性。然而，从其设计和实验设置中可以推断出一些潜在的方面：</p>
<ul>
<li><strong>窗口大小和步长的选择:</strong> 论文提到为了平衡实时性和性能，选择了特定的窗口大小（4）和步长（2）。这些参数的选择可能对不同场景或数据流的性能有影响，其泛化能力可能需要进一步探索。</li>
<li><strong>预训练权重依赖:</strong> 模型初始化使用了DUSt3R的预训练权重，这表明其性能可能部分依赖于强大的预训练模型，对于从头开始训练的性能可能有所不同。</li>
<li><strong>动态场景和无纹理区域:</strong> 尽管论文提到传统SfM方法在动态场景和无纹理区域面临挑战，但WinT3R在这些特定挑战性场景下的鲁棒性未被详细讨论。虽然其设计理念（如全局相机令牌池）有助于提高鲁棒性，但具体表现仍有待深入分析。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
基于论文的贡献和潜在局限性，未来研究可以探索以下方向：</p>
<ul>
<li><strong>自适应窗口机制:</strong> 开发一种能够根据场景复杂性、运动速度或计算资源动态调整窗口大小和步长的机制，以进一步优化性能和质量。</li>
<li><strong>更高效的全局信息融合:</strong> 探索除了相机令牌池之外，更丰富或更细粒度的全局信息表示和融合方法，以进一步提高在复杂或大规模场景中的重建精度和鲁棒性。</li>
<li><strong>鲁棒性提升:</strong> 针对极端动态场景、低纹理环境或光照剧烈变化的场景，进一步提升模型的鲁棒性。</li>
<li><strong>多模态融合:</strong> 结合其他传感器数据（如IMU、激光雷达）来增强重建的准确性和鲁棒性，尤其是在GPS信号不可用或视觉信息受限的环境中。</li>
<li><strong>模型泛化能力:</strong> 进一步研究模型在更广泛、更多样化的“野外”场景中的泛化能力，减少对特定数据集训练的依赖。</li>
<li><strong>实时性能优化:</strong> 尽管已达到17 FPS，但对于某些对延迟要求极高的应用（如AR/VR），仍有进一步优化实时性能的空间，例如通过模型剪枝、量化或硬件加速。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps.</li>
<li>These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05296v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05296v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05263v1'></a></p>
<h2 id="latticeworld-a-multimodal-large-language-model-empowered-framework-for-interactive-complex-world-generation"><a href="https://arxiv.org/abs/2509.05263v1">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></h2>
<p><strong>Authors:</strong> Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Hao Jiang, Kang Chen, Shuang Qiu</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
<script type="math/tex">90\times</script> increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation”论文的摘要，内容基于您提供的PDF全文：</p>
<p><strong>论文摘要：LatticeWorld：一个由多模态大型语言模型驱动的交互式复杂世界生成框架</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决复杂三维世界模型生成中的挑战。传统的虚拟三维场景创建依赖于手动建模，效率低下且难以扩展。尽管现代机器学习方法（特别是生成模型）能够根据用户指令创建虚拟世界，但现有方法在交互性、物理准确性、多模态输入处理以及工业级生产效率方面仍存在局限性。具体来说，论文致力于开发一个能够简化三维环境工业生产流程、实现高保真物理模拟、多智能体交互和实时渲染的框架。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
LatticeWorld提出了一个简单而有效的3D世界生成框架，其核心创新包括：
*   <strong>多模态LLM与工业级渲染引擎的集成：</strong> LatticeWorld将轻量级LLM（如LLaMA-2-7B）与工业级渲染引擎（如Unreal Engine 5）相结合，以生成动态环境。这与Blender等平台不同，UE5提供了更真实的物理模拟、原生多智能体交互和实时渲染。
*   <strong>可解释的中间表示：</strong> 框架接受文本描述和视觉指令（如高度图或草图）作为多模态输入，并通过LLM生成场景布局的符号表示（矩阵），以及环境配置。这种符号表示具有出色的可解释性和语义精度。
*   <strong>三阶段训练方案：</strong> 针对可变高度场景生成，论文提出了一个三阶段训练方案：CLIP微调（用于地形理解）、持续预训练（用于特征对齐）和端到端微调（用于布局生成），以有效整合视觉信息。
*   <strong>分层属性转换框架：</strong> 针对环境配置，论文设计了一个分层属性系统，将粗粒度属性（如地形类型、季节、天气）与细粒度属性（如密度、旋转、位置等）关联起来，简化了复杂环境配置的生成和管理。
*   <strong>多模态数据集构建：</strong> 论文构建了新的多模态数据集（基于LoveDA和Wild数据集），包含草图、布局语义分割、高度图、文本描述和环境配置，并利用GPT-4o进行数据标注和提示工程，确保标注效率和准确性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的生成准确性和视觉保真度：</strong> 实验结果表明，LatticeWorld在场景布局生成和视觉保真度方面优于现有方法（如GPT-4o、Claude 3.7 Sonnet、DeepSeek-R1、Qwen2-VL-Max）。
*   <strong>显著提升工业生产效率：</strong> 与传统手动生产方法相比，LatticeWorld将工业生产效率提高了90倍以上，同时保持了高创意质量。例如，总生产时间从55天缩短到不到0.6天。
*   <strong>支持动态交互式环境：</strong> 框架能够构建包含动态智能体的多智能体交互环境，支持智能体参数（类型、数量、状态、位置）的有效配置，并能实现基于预定义规则的对抗性行为。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>智能体策略简单：</strong> 目前，对抗性智能体遵循简单的策略（例如，当主智能体接近时进行攻击），未来需要实现更多样化的对抗性行为。
*   <strong>主玩家控制限制：</strong> 当前框架仅限于控制单个主玩家，未来可扩展到控制多个主玩家。
*   <strong>主智能体身体部位控制：</strong> 主智能体的身体部位无法独立控制，需要更精细的模型来实现对特定部位的控制。
*   <strong>资产库多样性：</strong> 资产库有待扩展，以包含更多对象和交互元素，从而生成更多样化的虚拟世界。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   实现更复杂和多样化的对抗性智能体行为策略。
*   扩展框架功能以支持多个主玩家的控制。
*   通过更复杂的建模实现对主智能体特定身体部位的精细控制。
*   扩展资产库，增加更多对象和交互元素，以生成更多样化的虚拟世界。
*   丰富数据集，支持更广泛的描述风格，包括更口语化和对话式的语言。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Moreover, LatticeWorld achieves over a
<script type="math/tex">90\times</script> increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods.</li>
<li>Our demo
video is available at https://youtu.be/8VWZXpERR18</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05263v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05263v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05198v1'></a></p>
<h2 id="enhancing-3d-point-cloud-classification-with-modelnet-r-and-point-skipnet"><a href="https://arxiv.org/abs/2509.05198v1">Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</a></h2>
<p><strong>Authors:</strong> Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet”论文的全面摘要：</p>
<p><strong>论文摘要：增强3D点云分类：ModelNet-R与Point-SkipNet</strong></p>
<p>这篇论文由Mohammad Saeid、Amir Salarpour和Pedram MohajerAnsari撰写，旨在解决3D点云分类领域中两个关键问题：现有数据集的质量问题和高性能模型计算效率低下的问题。</p>
<p><strong>1. 主要问题或研究问题：</strong>
3D点云分类在自动驾驶、机器人和增强现实等应用中至关重要。然而，广泛使用的ModelNet40数据集存在显著缺陷，包括标签不一致、包含2D数据、尺寸不匹配以及类别区分不足，这些问题严重阻碍了模型性能。此外，许多最先进的点云分类模型（特别是基于Transformer的架构）计算成本高昂，不适用于资源受限的环境。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
论文提出了两项主要创新：
*   <strong>ModelNet-R数据集：</strong> 这是ModelNet40数据集的精细化版本，旨在解决原始数据集中的标签不一致、低质量（2D）数据、尺寸不匹配和模糊的类别定义问题。通过人工检查、专家交叉引用和混淆矩阵分析，对数据集进行了系统性修正，移除了歧义样本，并调整了类别定义（例如，区分“花盆”和“花瓶”）。
*   <strong>Point-SkipNet模型：</strong> 这是一种轻量级的基于图的神经网络架构，专为高效准确的3D点云分类而设计。它利用了高效采样（最远点采样）、邻域分组（球查询）和跳跃连接（将池化特征与中心点连接）来捕获局部几何特征并减少计算开销。其模块化设计使其适用于各种3D视觉任务。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据集质量的重要性：</strong> 实验结果表明，在ModelNet-R上训练的模型性能显著优于在原始ModelNet上训练的模型。所有测试模型在ModelNet-R上的整体准确率（OA）和平均类别准确率（mAcc）均有所提高，这强调了高质量数据集在提升分类准确性方面的关键作用。
*   <strong>Point-SkipNet的卓越性能和效率：</strong> Point-SkipNet在ModelNet-R上实现了最先进的准确率（94.33% OA和92.93% mAcc），同时参数数量显著低于许多现有模型（1.47M），使其非常适合资源受限的环境（如移动设备和嵌入式系统）。
*   <strong>架构设计选择：</strong> 消融研究表明，旋转增强对学习旋转不变性至关重要，而基于拼接的跳跃连接在保留更丰富信息方面优于基于加法的跳跃连接，从而带来更好的分类性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据集精细化范围有限：</strong> ModelNet-R的精细化目前仅应用于ModelNet40的40个类别中的5个，未来需要扩展到所有类别以确保数据集的整体一致性。
*   <strong>尺寸信息丢失：</strong> 数据归一化移除了与尺寸相关的信息，未来的研究应探索保留这些信息的技术，例如结合尺寸比率。
*   <strong>模型验证范围：</strong> Point-SkipNet仅在ModelNet和ModelNet-R上进行了测试。需要对更多样化的数据集进行进一步验证，并使用ModelNet-R重新评估现有模型以进行公平比较。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   将数据集精细化过程扩展到所有ModelNet40类别，并整合真实世界的噪声数据集以增强泛化能力。
*   探索先进的归一化技术，以保留与尺寸相关的信息。
*   在ScanObjectNN和ShapeNet等多样化3D基准数据集上验证Point-SkipNet的性能。
*   通过强调高效模型设计和数据集完整性，为机器人、自动驾驶和增强现实领域中更准确、计算效率更高的3D分类模型铺平道路。</p>
<p>总而言之，这篇论文通过引入ModelNet-R和Point-SkipNet，在解决3D点云分类的数据质量和模型效率方面取得了重要进展，为该领域未来的研究奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05198v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05198v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05144v1'></a></p>
<h2 id="sgs-3d-high-fidelity-3d-instance-segmentation-via-reliable-semantic-mask-splitting-and-growing"><a href="https://arxiv.org/abs/2509.05144v1">SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</a></h2>
<p><strong>Authors:</strong> Chaolei Wang, Yang Luo, Jing Du, Siyu Chen, Yiping Chen, Ting Han</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Accurate 3D instance segmentation is crucial for high-quality scene
understanding in the 3D vision domain. However, 3D instance segmentation based
on 2D-to-3D lifting approaches struggle to produce precise instance-level
segmentation, due to accumulated errors introduced during the lifting process
from ambiguous semantic guidance and insufficient depth constraints. To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.
Unlike existing approaches that directly rely on raw lifted masks and sacrifice
segmentation accuracy, SGS-3D serves as a training-free refinement method that
jointly fuses semantic and geometric information, enabling effective
cooperation between the two levels of representation. Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances. For the geometric refinement, we construct fine-grained object
instances by exploiting both spatial continuity and high-level features,
particularly in the case of semantic ambiguity between distinct objects.
Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that
SGS-3D substantially improves segmentation accuracy and robustness against
inaccurate masks from pre-trained models, yielding high-fidelity object
instances while maintaining strong generalization across diverse indoor and
outdoor environments. Code is available in the supplementary materials.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Chaolei Wang等人撰写的论文“SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing”的全面摘要。</p>
<hr />
<h3 id="sgs-3d-3d">SGS-3D: 通过可靠语义掩码分裂与增长实现高保真3D实例分割</h3>
<p><strong>1. 论文解决的主要问题或研究问题：</strong>
该论文旨在解决3D实例分割领域的一个核心挑战：如何从2D图像数据中准确、高保真地生成3D实例分割结果。现有的2D到3D提升方法（即从2D语义掩码推断3D实例）通常由于语义指导模糊和深度约束不足，导致累积误差，从而产生不精确的实例级分割，尤其是在相邻但独立的物体之间。这种不准确性在缺乏深度信息的复杂户外场景中尤为突出。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
SGS-3D提出了一种新颖的、无需训练的“分裂-然后-增长”（split-then-grow）框架，通过联合融合语义和几何信息来克服2D到3D提升过程中的误差积累，从而实现高保真3D实例分割。其主要创新包括：</p>
<ul>
<li><strong>遮挡感知点-图像映射（Occlusion-Aware Point-Image Mapping）：</strong> 论文引入了一种鲁棒且高效的映射策略，无需依赖真实的深度图，通过Z-buffering直接从点云和相机参数计算可见性。这确保了准确的掩码到点对应关系，即使在纹理缺失或高反射表面上也能保持有效。</li>
<li><strong>共现掩码过滤（Co-occurrence Mask Filtering）：</strong> 为了解决2D掩码预测中的歧义和不一致性，SGS-3D利用3D几何基元的共现信息来过滤和修剪模糊的2D掩码。通过计算掩码的跨视图一致性分数，该机制能够识别并移除不一致的掩码，从而确保更可靠的语义一致性。</li>
<li><strong>空间连续性分裂（Spatial Continuity Splitting）：</strong> 针对语义相似但空间上分离的物体可能被错误分组的问题，论文在密度空间中对3D语义掩码应用HDBSCAN聚类算法。这会将初始的3D语义掩码分裂成更精细的、纯粹的语义-几何种子，从而在几何上细化语义指导并保持语义质量。</li>
<li><strong>特征引导增长（Feature-Guided Growing）：</strong> 为了将分裂后的语义-几何种子组装成完整的对象实例，论文引入了一个高维特征引导的增长过程。通过统一的亲和力分数（结合语义相似性和空间重叠），该过程智能地将碎片化的实例聚合为完整的对象，特别擅长处理交织的场景。</li>
<li><strong>多视图渐进式合并（Multi-View Progressive Merging）：</strong> 针对单视图增长的局限性，论文采用渐进式多视图合并策略，通过逐步放宽3D空间重叠要求，将来自不同视图的实例提案整合为最终的、完整的3D对象实例。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
SGS-3D在ScanNet200、ScanNet++（室内环境）和KITTI-360（室外环境）等多个基准数据集上取得了最先进的性能。</p>
<ul>
<li><strong>显著的分割精度提升：</strong> 在KITTI-360数据集上，SGS-3D的mAP比次优竞争者SAI3D高出16.4%，验证了其遮挡感知映射在鲁棒户外场景理解中的关键作用。在室内场景中，SGS-3D也持续超越了Open3DIS和SAM2Object等现有SOTA方法。</li>
<li><strong>强大的泛化能力：</strong> SGS-3D作为一种无需训练的方法，在不同数据集和场景中表现出稳定的性能，即使在没有真实深度信息的情况下也能实现高保真对象实例，这凸显了其强大的零样本泛化能力，避免了监督方法固有的数据集特定过拟合问题。</li>
<li><strong>效率和鲁棒性：</strong> 论文展示了SGS-3D在效率和准确性方面的卓越表现，例如在ScanNet200上使用更少的图像（2.5% vs. 10%）却实现了更高的准确性，并显著减少了过分割现象。此外，SGS-3D对模拟遮挡表现出高度的鲁棒性，即使在50%的前景掩码被遮挡的情况下也能保持竞争力。</li>
<li><strong>开放集场景理解应用：</strong> SGS-3D生成的高质量、类别无关的实例提案为开放集场景理解提供了强大基础，可无缝扩展到开放词汇3D分割和文本驱动的3D对象搜索等应用。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中未明确提及当前方法的具体局限性，但从未来研究方向可以推断出一些隐含的限制：</p>
<ul>
<li><strong>实时性能：</strong> 论文提到未来方向包括“优化其多阶段设计以实现实时性能”，这暗示当前方法可能尚未达到实时处理的速度。</li>
<li><strong>动态场景处理：</strong> 论文指出未来方向包括“将我们的框架扩展到动态场景”，这意味着当前方法主要针对静态场景，对动态场景的处理能力可能有限。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
*   将框架扩展到动态场景，通过整合时间信息来提高性能。
*   优化多阶段设计，以实现实时性能。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.</li>
<li>Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05144v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05144v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05086v1'></a></p>
<h2 id="robust-experts-the-effect-of-adversarial-training-on-cnns-with-sparse-mixture-of-experts-layers"><a href="https://arxiv.org/abs/2509.05086v1">Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers</a></h2>
<p><strong>Authors:</strong> Svetlana Pavlitska, Haixi Fan, Konstantin Ditschuneit, J. Marius Zöllner</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Svetlana Pavlitska等人撰写的论文“Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers”的全面摘要。</p>
<hr />
<h3 id="cnns">论文摘要：鲁棒专家：对抗训练对带有稀疏专家混合层CNNs的影响</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决卷积神经网络（CNNs）在面对对抗性攻击时鲁棒性不足的问题。传统的鲁棒化方法通常资源密集，因此作者探索了使用稀疏专家混合（MoE）层来提高CNNs的对抗鲁棒性，同时避免额外的推理成本。具体来说，他们研究了MoE层在对抗训练背景下对模型鲁棒性的影响，以及路由机制和专家利用模式如何影响这一过程。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>引入MoE层以增强鲁棒性：</strong> 作者提出通过替换ResNet架构中选定的残差块（BlockMoE）或卷积层（ConvMoE）来集成稀疏MoE层，以增加模型容量，同时保持推理效率。
*   <strong>MoE层架构设计：</strong> 论文定义了两种结构不同的MoE层变体：BlockMoE（替换整个残差块）和ConvMoE（替换单个卷积层），旨在实现相似的计算复杂度但结构不同，以促进专家专业化和鲁棒性。
*   <strong>门控网络（Gate）设计：</strong> 采用了两种门控网络：GAP-FC（全局平均池化-全连接）和Conv-GAP（卷积-全局平均池化），用于激活和加权不同的专家。
*   <strong>负载均衡损失分析：</strong> 论文分析了熵损失（Entropy Loss）和Switch Loss在防止路由崩溃和鼓励专家多样化方面的作用，并发现Switch Loss在对抗训练中可能导致意外的鲁棒性提升。
*   <strong>个体专家行为分析：</strong> 深入研究了路由崩溃、专家利用模式以及固定专家推理对鲁棒性和专业化的影响。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>MoE层与对抗训练的结合显著提升鲁棒性：</strong> 在CIFAR-100数据集上，将单个MoE层插入ResNet架构的更深层阶段，并结合对抗训练，能够持续提高模型在PGD和AutoPGD攻击下的鲁棒性，同时保持或略微提升干净准确率。ResNet-50模型中的效果更为显著，表明更深层模型受益于输入依赖的专家路由。
*   <strong>BlockMoE优于ConvMoE：</strong> BlockMoE层在对抗训练下表现出更一致的鲁棒性和干净准确率提升，这可能因为残差块等粗粒度模块能够学习更具语义意义和独立的表示。
*   <strong>Switch Loss的意外鲁棒性：</strong> 令人惊讶的是，当使用Switch Loss进行负载均衡时，路由倾向于集中到一小部分过度使用的专家。这种集中训练无意中使这些路径更具鲁棒性，导致某些个体专家在鲁棒性方面甚至优于整个门控MoE模型，这表明通过专业化可以出现鲁棒的子路径。
*   <strong>熵损失促进专家多样性：</strong> 熵损失在对抗训练中表现出更好的鲁棒性，鼓励更有效的专家专业化和路由，从而在不牺牲干净准确率的情况下提高鲁棒性。
*   <strong>专家数量的影响：</strong> 增加专家数量通常能在一定程度上（通常是8-16个专家）提高干净和对抗准确率，但超出此范围性能会趋于平稳或下降。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>鲁棒性提升主要限于对抗训练场景：</strong> 在正常训练下，稀疏MoE层几乎没有带来改进。
*   <strong>门控机制的局限性：</strong> 某些个体专家在隔离状态下表现优于整个MoE模型，这表明门控机制可能未能始终利用最鲁棒的计算路径。
*   <strong>专家数量与专业化：</strong> 增加专家数量可能会降低个体专家专业化程度，暗示路由粒度和有效专家分化之间存在权衡。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>鲁棒性感知门控策略：</strong> 开发能够优先将输入路由到弹性专家的门控策略。
*   <strong>结合彩票假说（Lottery Ticket Hypothesis）：</strong> 利用彩票假说来指导鲁棒子网络的识别和优化。
*   <strong>集成对抗信号：</strong> 将对抗信号整合到路由目标中，或在推理时动态调整专家选择，以进一步提高性能。</p>
<hr />
<p>这篇论文为在CNNs中利用稀疏MoE层提高对抗鲁棒性提供了一个新颖的视角，特别是通过深入分析负载均衡损失和个体专家行为，揭示了鲁棒子路径的出现，为未来鲁棒深度学习架构设计提供了有价值的见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization.</li>
<li>Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05086v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05086v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05012v1'></a></p>
<h2 id="a-biologically-inspired-separable-learning-vision-model-for-real-time-traffic-object-perception-in-dark"><a href="https://arxiv.org/abs/2509.05012v1">A biologically inspired separable learning vision model for real-time traffic object perception in Dark</a></h2>
<p><strong>Authors:</strong> Hulin Li, Qiliang Ren, Jun Li, Hanbing Wei, Zheng Liu, Linfang Fan</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Fast and accurate object perception in low-light traffic scenes has attracted
increasing attention. However, due to severe illumination degradation and the
lack of reliable visual cues, existing perception models and methods struggle
to quickly adapt to and accurately predict in low-light environments. Moreover,
there is the absence of available large-scale benchmark specifically focused on
low-light traffic scenes. To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation. We further propose the Separable
Learning Vision Model (SLVM), a biologically inspired framework designed to
enhance perception under adverse lighting. SLVM integrates four key components:
a light-adaptive pupillary mechanism for illumination-sensitive feature
extraction, a feature-level separable learning strategy for efficient
representation, task-specific decoupled branches for multi-task separable
learning, and a spatial misalignment-aware fusion module for precise
multi-feature alignment. Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead. Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end
trained SLVM surpasses Swin Transformer+EnlightenGAN and
ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key
metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage
points. The Dark-traffic dataset and complete code is released at
https://github.com/alanli1997/slvm.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Hulin Li等人撰写的论文“A biologically inspired separable learning vision model for real-time traffic object perception in Dark”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：基于生物启发的暗光交通目标感知可分离学习视觉模型</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决在低光照交通场景中实时、准确地感知目标（包括目标检测、实例分割和光流估计）的挑战。现有模型由于严重的照明退化和可靠视觉线索的缺乏，难以快速适应和准确预测。此外，缺乏专门针对低光照交通场景的大规模基准数据集也是一个关键问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，论文提出了以下创新：</p>
<ul>
<li><strong>Dark-traffic 数据集：</strong> 构建了一个物理接地（physically grounded）的照明退化方法，并创建了迄今为止最大的、密集标注的低光照交通场景数据集Dark-traffic，支持目标检测、实例分割和光流估计。</li>
<li><strong>可分离学习视觉模型（SLVM）：</strong> 提出了一种受生物学启发的框架，用于增强恶劣光照下的感知能力。SLVM包含四个核心组件：<ul>
<li><strong>光适应瞳孔机制（LAPM）：</strong> 模拟生物瞳孔的快速反应，用于提取对光照敏感的特征，以补偿光照损失并提取纹理感知特征。</li>
<li><strong>特征级可分离学习策略（FSLConv）：</strong> 在特征层面解耦纹理特征，实现更高效的学习和更丰富的表示多样性。</li>
<li><strong>任务特定解耦分支：</strong> 采用独立的、任务特定的分支来学习光照感知和语义特征，增强跨感知任务的适应性。</li>
<li><strong>空间错位感知融合模块（SNI-r）：</strong> 一种特征级插值方法，用于精确的多特征对齐，解决多尺度融合中的错位问题。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
广泛的实验证明SLVM在降低计算开销的同时，实现了最先进的性能：</p>
<ul>
<li>在Dark-traffic数据集上，SLVM在检测任务中超越RT-DETR 11.2个百分点，在实例分割中超越YOLOv12 6.1个百分点，并将基线模型的端点误差（EPE）降低了12.37%。</li>
<li>在LIS基准测试中，端到端训练的SLVM在关键指标上平均超越Swin Transformer+EnlightenGAN和ConvNeXt-T+EnlightenGAN 11个百分点，并超越Mask RCNN（带光照增强）3.1个百分点。</li>
<li>这些结果凸显了SLVM在低光照条件下强大的泛化能力、鲁棒性和计算效率，无需显式增强或去噪技术。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文也讨论了当前方法的局限性：</p>
<ul>
<li><strong>动态感知局限：</strong> SLVM尚未将静态目标感知扩展到更广泛的动态感知领域，即无法感知真实世界中的物体运动和全局动态。目前的“伪动态”感知依赖于逐帧检测和事后关联，缺乏真实的运动信息。</li>
<li><strong>实际应用中的权衡：</strong> 尽管SLVM设计用于实时部署并实现了低延迟，但在实际应用中仍存在权衡。例如，特征分解策略在处理大规模、高分辨率输入时可能会引入轻微的延迟开销，并且模块化架构需要仔细调度并行硬件才能充分发挥其效率优势。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了未来的研究方向：</p>
<ul>
<li>开发一个能够实时检测和分割低光照交通目标，并估计其在空间中的真实运动的实时感知框架。</li>
<li>实现实时目标感知与实时光流或场景流估计的紧密耦合，以实现对动态场景的物理接地理解。</li>
<li>进一步优化模块化架构和并行硬件调度，以在延迟敏感或资源受限的边缘设备上实现更高效的部署。</li>
</ul>
<hr />
<p>这份摘要突出了该论文在解决低光照交通场景感知问题上的创新性，特别是其生物启发式设计、新数据集的构建以及在多任务上的卓越性能。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation.</li>
<li>Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead.</li>
<li>Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05012v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05012v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04833v1'></a></p>
<h2 id="propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granularity-discrimination"><a href="https://arxiv.org/abs/2509.04833v1">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a></h2>
<p><strong>Authors:</strong> Ming Dai, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Ming Dai等人撰写的论文“PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination”的全面摘要。</p>
<hr />
<h3 id="propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granularity-discrimination_1">论文《PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination》全面摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
视觉定位（Visual Grounding, VG）旨在根据自然语言查询在图像中定位和分割对象。传统基于提案的两阶段框架因其效率低下和计算复杂性高而逐渐被端到端直接引用范式取代。然而，这些直接引用方法过度依赖被引用目标的监督，忽略了突出潜在目标的益处，并且通常未能整合多粒度判别，这在复杂场景中对鲁棒的对象识别至关重要。此外，在广义视觉定位（Generalized Visual Grounding, GVG）任务中，模型需要处理零个或多个被引用对象的情况，并准确识别所有与引用表达式对应的位置，同时考虑目标可能不存在的可能性。现有方法在处理这些挑战时存在局限性，尤其是在对象存在判别方面。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
PropVG 提出了一种端到端、基于提案的框架，旨在克服上述局限性，其主要创新包括：</p>
<ul>
<li><strong>端到端、无检测器（Detector-Free）的提案生成与引用对象理解集成：</strong> PropVG 是首个无缝集成前景对象提案生成与引用对象理解的端到端、基于提案的框架，无需额外的预训练检测器。这克服了传统基于提案方法性能不佳和推理速度慢的缺点。</li>
<li><strong>基于对比的引用评分（Contrastive-based Refer Scoring, CRS）模块：</strong> 该模块利用对比学习在句子和单词级别自适应地平衡和整合贡献，以增强模型理解和区分被引用对象的能力，从而精确评估提案的引用相关性。</li>
<li><strong>多粒度目标判别（Multi-granularity Target Discrimination, MTD）模块：</strong> 该模块融合了对象级和语义级信息，通过引入分数先验交叉注意力机制，将先验分数分布信息整合到注意力图中，并直接注入预测的引用和分割分数，以确保目标存在性在多个预测之间保持一致性，从而提高对不存在目标的识别能力。</li>
<li><strong>多任务协同框架：</strong> PropVG 采用多任务协同框架，同时处理检测、引用、全局分割和目标存在性判别任务，增强了模型的通用性和实际适用性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
PropVG 在多个基准测试上取得了显著的性能提升，证明了其有效性：</p>
<ul>
<li><strong>经典视觉定位（REC/RES）：</strong> 在RefCOCO/+/g数据集上，PropVG 在REC任务中超越了包括OneRef在内的先进方法，平均性能提升0.5%至1.2%，且推理速度比传统基于提案模型快4倍。在RES任务中，mIoU平均提升1.4%至4.0%。</li>
<li><strong>广义视觉定位（GREC/GRES）：</strong> 在gRefCOCO、Ref-ZOM和R-RefCOCO/+/g数据集上，PropVG 在所有指标上均表现优异。例如，在gRefCOCO的GRES任务中，gloU指标相比SOTA方法HDC有显著提升（+5.0%至+2.0%）。在Ref-ZOM上，准确率、oIoU和mIoU分别提升4.8%、2.6%和2.2%，甚至超越了基于MLLM的模型。</li>
<li><strong>消融研究：</strong> CRS模块通过句子和单词级别的对比学习，将F1分数和N-acc分别提高了1.8%和3.4%。MTD模块通过整合对象级和语义级特征，将F1分数和N-acc分别提高了1.5%和2.8%。这些模块的结合进一步提升了整体性能。</li>
</ul>
<p>这些结果表明，PropVG 成功地克服了传统基于提案方法的局限性，并在复杂和广义的视觉定位场景中实现了鲁棒且高效的对象识别。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中并未明确指出 PropVG 模型的具体局限性。然而，从其设计和实验设置中可以推断出一些潜在的考虑：</p>
<ul>
<li><strong>计算资源需求：</strong> 尽管 PropVG 相比传统两阶段方法提高了效率，但作为端到端、多任务的Transformer-based模型，其训练和推理可能仍需要较高的计算资源，尤其是在处理大规模数据集和高分辨率图像时。</li>
<li><strong>模型复杂性：</strong> 整合了多模态编码器（BEiT-3）、SimFPN、UNet解码器、多尺度可变形解码器、CRS和MTD等多个复杂模块，模型的整体架构较为复杂，可能增加了调试和维护的难度。</li>
<li><strong>超参数敏感性：</strong> 论文中对K值和损失权重进行了消融研究，表明模型性能可能对这些超参数的选择敏感，需要仔细调优以达到最佳性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于本论文的贡献和现有技术发展，未来研究可以探索以下方向：</p>
<ul>
<li><strong>进一步提升效率和可扩展性：</strong> 尽管 PropVG 已经比传统方法更高效，但仍可探索更轻量级的模型架构、更高效的注意力机制或知识蒸馏等技术，以进一步降低计算成本，使其适用于资源受限的设备或更大规模的数据集。</li>
<li><strong>多模态融合的深度探索：</strong> 论文利用BEiT-3进行视觉-语言编码和融合，未来可以探索更先进的多模态融合策略，例如结合更强大的大型语言模型（LLMs）或视觉-语言模型（VLMs），以增强模型对复杂语义和上下文关系的理解能力。</li>
<li><strong>零样本/少样本学习：</strong> 鉴于广义视觉定位任务中可能存在稀有或未见过的对象，探索零样本（zero-shot）或少样本（few-shot）学习能力，使模型能够泛化到新类别或新场景，将是一个重要的研究方向。</li>
<li><strong>实时应用：</strong> 进一步优化模型的推理速度，使其能够满足实时视觉定位应用的需求，例如在自动驾驶、机器人导航或增强现实等领域。</li>
<li><strong>跨领域泛化能力：</strong> 评估和提升 PropVG 在不同领域（如医学图像、卫星图像等）的泛化能力，可能需要引入领域适应或迁移学习技术。</li>
</ul>
<hr />
<p>这份摘要旨在全面概括论文的核心内容，并从专业角度分析其贡献和潜在发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors.</li>
<li>Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04833v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04833v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04816v1'></a></p>
<h2 id="extracting-uncertainty-estimates-from-mixtures-of-experts-for-semantic-segmentation"><a href="https://arxiv.org/abs/2509.04816v1">Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</a></h2>
<p><strong>Authors:</strong> Svetlana Pavlitska, Beyza Keskin, Alwin Faßbender, Christian Hubschneider, J. Marius Zöllner</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Svetlana Pavlitska等人撰写的论文“Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：从专家混合模型中提取语义分割的不确定性估计</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决计算机视觉模型，特别是语义分割模型在安全关键应用中，如何准确且良好校准地估计预测不确定性的问题。传统的集成方法（如模型集成或MC Dropout）虽然可以量化不确定性，但计算成本较高。作者探索了专家混合（MoE）模型作为一种更高效的替代方案，以在不修改其架构的情况下，从MoE中提取可靠的不确定性估计。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>无架构修改的不确定性提取：</strong> 论文的核心创新在于，它提出并证明了可以从标准、未经修改的MoE模型中提取出良好校准的预测不确定性估计，而无需引入额外的架构修改或显式不确定性建模。这保留了MoE的效率和与现有预训练模型的兼容性。
*   <strong>三种预测不确定性估计方法：</strong> 作者研究了三种从MoE中提取预测不确定性的方法：
    *   <strong>预测熵（PE）：</strong> 通过对MoE输出概率分布的香农熵进行测量。
    *   <strong>互信息（MI）：</strong> 量化模型参数不确定性引起的认知不确定性。
    *   <strong>专家方差（EV）：</strong> 基于MoE中各专家预测相对于最终MoE输出的变异性。
*   <strong>路由不确定性评估：</strong> 引入了“门控熵”（gate entropy）来量化路由不确定性，反映门控网络在选择专家时的置信度，从而提供模型内部决策过程的额外信号。
*   <strong>两种MoE输出聚合方法：</strong> 针对PE和MI的计算，提出了“堆叠（stacked）”和“加权（weighted）”两种方法来聚合专家输出。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>OOD数据上的优越性：</strong> 在A2D2数据集上，MoE在条件正确性指标方面（如p(accurate certain)和p(uncertain inaccurate)）比集成方法和MC Dropout在域外（OOD）数据上产生了更可靠的不确定性估计。这表明MoE在处理数据偏移时具有更强的鲁棒性。
*   <strong>路由不确定性校准：</strong> 简单的门控机制比更复杂的类别门控机制能更好地校准路由不确定性估计。
*   <strong>专家数量对校准的影响：</strong> 在Cityscapes数据集上的实验表明，增加专家数量可以进一步增强不确定性校准，尤其是在负对数似然（NLL）方面。
*   <strong>预测不确定性和路由不确定性的分离潜力：</strong> 通过预测熵和门控熵对不确定性组件进行分离，为解耦认知不确定性和偶然不确定性提供了潜力。
*   <strong>MoE作为OOD检测方法：</strong> 论文观察到的MoE在OOD数据上的优越结果表明，MoE可以作为一种有效的OOD或异常检测方法。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>加权与堆叠方法的差异：</strong> 加权和堆叠方法在计算PE和MI方面的差异相对较小，这可能限制了它们在某些场景下的区分能力。
*   <strong>简单门控与类别门控的权衡：</strong> 简单门控机制虽然在路由不确定性校准方面表现更好，但类别门控（在增加卷积层时）在分割精度上有所提升，这表明在不同目标之间存在权衡。
*   <strong>专家数量增加的非线性影响：</strong> 尽管增加专家数量可以略微提高不确定性估计（如NLL），但它并不总是能直接改善所有校准指标，甚至可能略微下降。这可能需要更复杂的门控或正则化策略来充分利用额外的专家。
*   <strong>MoE在标准校准指标上的表现：</strong> 在数据偏移下，MoE在ECE和Brier分数等标准校准指标上并不总是优于集成方法，尽管它在条件正确性指标上表现出色。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>自适应专家选择：</strong> 探索更智能的自适应专家选择机制，以进一步优化MoE的性能和不确定性估计。
*   <strong>更深层的门控策略：</strong> 研究更复杂的门控网络架构和策略，以更好地利用专家多样性，提高不确定性鲁棒性和互操作性。
*   <strong>更丰富的专家多样性：</strong> 探索如何通过增加专家多样性来进一步提升MoE的不确定性估计能力。
*   <strong>OOD或异常检测：</strong> 进一步研究MoE作为OOD或异常检测方法的潜力，并开发专门的评估协议。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications.</li>
<li>Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04816v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04816v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05297v1'></a></p>
<h2 id="flowseek-optical-flow-made-easier-with-depth-foundation-models-and-motion-bases"><a href="https://arxiv.org/abs/2509.05297v1">FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</a></h2>
<p><strong>Authors:</strong> Matteo Poggi, Fabio Tosi</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training. FlowSeek marries the latest advances on the
design space of optical flow networks with cutting-edge single-image depth
foundation models and classical low-dimensional motion parametrization,
implementing a compact, yet accurate architecture. FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Matteo Poggi和Fabio Tosi撰写的论文“FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases”的全面摘要。</p>
<hr />
<h3 id="flowseek">FlowSeek: 利用深度基础模型和运动基简化光流估计</h3>
<p><strong>1. 论文解决的主要问题或研究问题</strong></p>
<p>该论文旨在解决当前光流估计领域面临的两个核心挑战：
*   <strong>高硬件资源需求：</strong> 现有最先进的光流模型通常需要大量的计算资源（多块高端GPU）进行训练，这使得研究对资源有限的团队而言难以企及。
*   <strong>泛化能力不足：</strong> 尽管性能强大，但许多光流模型在跨不同数据集（即零样本泛化）时表现不佳，难以保持细节的精细度。</p>
<p>FlowSeek致力于在保持高精度的同时，显著降低训练所需的硬件成本，并提高模型在不同场景下的泛化能力。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>FlowSeek的核心创新在于其独特的架构，它巧妙地融合了三个看似独立的领域：
*   <strong>光流网络设计的前沿进展：</strong> FlowSeek以SEA-RAFT [81]等迭代细化架构为基础，利用其在光流估计中的有效性。
*   <strong>单图像深度基础模型（Depth Foundation Models）：</strong> 论文首次将预训练的深度基础模型（如Depth Anything v2 [93]）集成到光流估计框架中。这些模型能够提供丰富的语义和几何先验知识，通过提取深度图和深度特征来增强光流骨干网络的特征表示和上下文信息。
*   <strong>经典的低维运动参数化（Motion Bases）：</strong> 论文引入了基于3D运动自由度的低维运动基（Bmotion），这些运动基能够为光流模型提供一个初始的、几何一致的运动猜测，尤其适用于刚性运动场景。</p>
<p>通过将这些组件结合，FlowSeek实现了以下关键方法论贡献：
*   <strong>特征增强：</strong> 将深度基础模型提取的深度图和深度特征与光流骨干网络的特征进行拼接，并通过一个浅层BottNeck网络进行处理，以生成更丰富的特征用于相关性计算。
*   <strong>上下文和隐藏状态增强：</strong> 深度图也被送入ContextNet，与图像一起提取更强的上下文特征和初始隐藏状态，以指导迭代光流估计过程。
*   <strong>运动基集成：</strong> 运动基通过一个BasesNet模块提取出密集的运动特征，并与原始上下文和隐藏状态特征进行拼接，为迭代光流估计提供几何先验。
*   <strong>紧凑且高效的架构：</strong> 这种集成允许FlowSeek在单个消费级GPU上进行训练，显著降低了硬件预算。</p>
<p><strong>3. 主要结果及其意义</strong></p>
<p>FlowSeek在多个标准数据集上取得了显著的性能提升，证明了其方法的有效性：
*   <strong>硬件效率：</strong> FlowSeek仅需单个消费级GPU进行训练，相比大多数最新方法所需的硬件预算降低了约8倍，这使得先进的光流研究更具可及性。
*   <strong>卓越的零样本泛化能力：</strong> 在Sintel Final和KITTI数据集上，FlowSeek相比之前的最先进方法SEA-RAFT，实现了10%和15%的相对改进。在Spring和LayeredFlow数据集上，FlowSeek也表现出优越的性能，尤其是在LayeredFlow数据集的透明和反射区域，FlowSeek在大多数指标上取得了显著的改进。
*   <strong>精度和细节：</strong> 尽管硬件预算较低，FlowSeek仍能实现最先进的精度，并能恢复更精细的细节，减少伪影。
*   <strong>设计通用性：</strong> 论文通过将FlowSeek与不同的光流骨干网络（如CRAFT和FlowFormer）以及不同的深度基础模型（如DPT、Depth Anything v1和v2）结合，证明了其设计方案的通用性。</p>
<p>这些结果的意义在于，FlowSeek证明了通过智能地重用现有预训练模型（如深度基础模型）和结合经典计算机视觉技术，可以在不依赖大量硬件资源的情况下，推动计算机视觉领域（特别是光流估计）的进步。</p>
<p><strong>4. 论文中提及的局限性</strong></p>
<p>论文中提到了FlowSeek的局限性：
*   <strong>对预训练基础模型的依赖：</strong> FlowSeek的训练效率得益于大型、预训练的深度基础模型，这些模型本身可能是在高硬件预算下通过大规模网络数据训练的。因此，FlowSeek的低硬件需求是建立在这些现有模型的基础之上的，而不是从零开始。
*   <strong>在某些复杂场景下的性能权衡：</strong> 在某些消融实验中，FlowSeek的更大变体（M和L）在KITTI数据集上表现出轻微的精度下降，这可能与在单个GPU上训练这些复杂模型时缺乏强大的预训练有关。
*   <strong>运动基的假设：</strong> 运动基主要适用于刚性运动，对于非刚性或复杂运动场景，其初始猜测的准确性可能受限。</p>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>论文提出了以下未来研究方向：
*   <strong>进一步利用更准确的基础模型：</strong> 随着未来更准确的深度基础模型的出现，FlowSeek的性能有望进一步提升。
*   <strong>解决训练数据瓶颈：</strong> 训练数据仍然是光流模型的一个重要瓶颈。未来的研究将致力于模拟深度估计文献中成功的策略，以解决这一问题，例如通过合成数据生成或知识蒸馏。
*   <strong>探索其他计算机视觉任务：</strong> FlowSeek的成功经验可以启发其他计算机视觉领域，设计出在最小硬件预算下可训练的新模型。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training.</li>
<li>FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05297v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05297v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05249v1'></a></p>
<h2 id="cogitao-a-visual-reasoning-framework-to-study-compositionality-generalization"><a href="https://arxiv.org/abs/2509.05249v1">COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</a></h2>
<p><strong>Authors:</strong> Yassine Taoudi-Benchekroun, Klim Troyan, Pascal Sager, Stefan Gerber, Lukas Tuggener, Benjamin Grewe</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Yassine Taoudi-Benchekroun等人撰写的论文“COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization”的摘要：</p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前最先进的机器学习模型在组合性（compositionality）和泛化（generalization）方面的持续局限性。尽管人类智能能够轻松地组合已学习的概念并应用于新颖情境，但机器模型在视觉领域中，尤其是在面对熟悉元素的新颖组合时，往往难以实现这种能力。研究问题是：如何系统地研究和评估视觉模型在组合性和泛化方面的能力，并揭示它们在这些方面的不足？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文引入了<strong>COGITAO</strong>，这是一个模块化且可扩展的数据生成框架和基准，用于系统地研究视觉领域中的组合性和泛化。其主要创新包括：
*   <strong>规则生成任务：</strong> COGITAO受ARC-AGI问题设置的启发，构建了基于规则的任务，这些任务将一系列变换应用于网格环境中的对象。
*   <strong>丰富的变换集和组合性：</strong> 框架支持对28种可互操作的原子变换进行任意深度的组合，从而能够创建数百万个独特的任务规则，其数量远超现有数据集。
*   <strong>灵活的参数控制：</strong> 对网格参数化和对象属性（如大小、对称性、连通性、颜色、形状等）进行广泛控制，使得任务难度范围广泛，并能生成几乎无限的样本。
*   <strong>系统性评估基准：</strong> COGITAO不仅是一个生成器，更是一个评估模型在不同泛化设置（如组合泛化和环境泛化）下表现的基准。
*   <strong>开源性：</strong> COGITAO的代码和数据集完全开源，以促进该领域的持续研究。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文提供了使用最先进视觉模型（Vanilla ViT、Grid ViT和LLaDA）进行的基线实验。主要结果和意义如下：
*   <strong>模型在域内表现良好，但泛化能力差：</strong> 尽管在训练数据（域内）上表现强劲，但模型在泛化到熟悉元素的新颖组合时（域外）表现出一致的失败。例如，在组合泛化研究中，某些模型的域外准确率甚至低至5.1%或6.4%。
*   <strong>Grid-ViT的优势：</strong> Grid-ViT（一种针对网格结构任务定制的ViT变体）在大多数实验设置中实现了最强的域内性能，并在某些域外场景中表现出优于Vanilla ViT的性能，这表明其引入的归纳偏置对网格关系结构任务有效。
*   <strong>LLaDA的潜力：</strong> LLaDA（一种基于扩散的语言模型）在某些配置中实现了最高的域外准确率，甚至在某些情况下与Grid-ViT相当或略优，这暗示了其在系统泛化方面的潜力。
*   <strong>确认组合性泛化挑战：</strong> 实验结果证实了COGITAO基准的挑战性，并强调了需要能够处理组合性和系统泛化的架构。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>抽象性和合成性：</strong> COGITAO是抽象和合成的，缺乏真实世界视觉数据的基础。虽然许多任务对人类来说概念上很简单，但深度较大的变换序列可能在认知上要求很高。
*   <strong>变换结构差异：</strong> 尽管任务空间很大，但一些变换在结构或复杂性上没有显著差异。
*   <strong>与视觉复杂性的混淆：</strong> 现有视觉基准往往将视觉复杂性与关系结构混淆，从而阻碍了对真正的组合泛化的关注。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展架构：</strong> 未来工作可以探索额外的架构，以进一步提高性能。
*   <strong>上下文学习：</strong> 将框架扩展到上下文学习，例如通过提供演示示例，可以评估模型在已知受益于更长上下文的设置中的泛化能力。
*   <strong>课程学习：</strong> 由于其可控的环境和可调节的难度，COGITAO非常适合课程学习，逐步增加任务复杂性以指导学习。
*   <strong>内部模型表示分析：</strong> 分析在COGITAO上训练的内部模型表示，可能揭示模型如何发展以对象为中心或以变换为中心的抽象。
*   <strong>更详尽的实验：</strong> 扩展实验，涵盖COGITAO支持的所有变换，以提供更全面的模型行为理解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models.</li>
<li>To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains.</li>
<li>We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05249v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05249v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-08 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
