<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-08 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-07/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-09/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-08">Arxiv Computer Vision Papers - 2025-09-08</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#wint3r-window-based-streaming-reconstruction-with-camera-token-pool" class="nav-link">WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</a>
                </li>
                <li class="nav-item">
                    <a href="#latticeworld-a-multimodal-large-language-model-empowered-framework-for-interactive-complex-world-generation" class="nav-link">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#enhancing-3d-point-cloud-classification-with-modelnet-r-and-point-skipnet" class="nav-link">Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</a>
                </li>
                <li class="nav-item">
                    <a href="#sgs-3d-high-fidelity-3d-instance-segmentation-via-reliable-semantic-mask-splitting-and-growing" class="nav-link">SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</a>
                </li>
                <li class="nav-item">
                    <a href="#robust-experts-the-effect-of-adversarial-training-on-cnns-with-sparse-mixture-of-experts-layers" class="nav-link">Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers</a>
                </li>
                <li class="nav-item">
                    <a href="#a-biologically-inspired-separable-learning-vision-model-for-real-time-traffic-object-perception-in-dark" class="nav-link">A biologically inspired separable learning vision model for real-time traffic object perception in Dark</a>
                </li>
                <li class="nav-item">
                    <a href="#propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granularity-discrimination" class="nav-link">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a>
                </li>
                <li class="nav-item">
                    <a href="#extracting-uncertainty-estimates-from-mixtures-of-experts-for-semantic-segmentation" class="nav-link">Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#flowseek-optical-flow-made-easier-with-depth-foundation-models-and-motion-bases" class="nav-link">FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</a>
                </li>
                <li class="nav-item">
                    <a href="#cogitao-a-visual-reasoning-framework-to-study-compositionality-generalization" class="nav-link">COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-08">Arxiv Computer Vision Papers - 2025-09-08</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½ä¸ºå¿ç¢çç ç©¶äººååå¤ç Arxiv è®¡ç®æºè§è§é¢åææ°è®ºæçæ§è¡æè¦ï¼</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-05)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong></p>
<p>ä»æ¥ Arxiv è®¡ç®æºè§è§è®ºæåç°åºå¤æ¨¡æèåã3D è§è§çæç»­æ·±åãæ¨¡åé²æ£æ§ä¸ä¸ç¡®å®æ§ä¼°è®¡çå³æ³¨ï¼ä»¥åå¯¹é«æåéç¨è§è§ç³»ç»æå»ºçæ¢ç´¢ãå·ä½èè¨ï¼æä»¬çå°äºå°å¤§åè¯­è¨æ¨¡åï¼LLMï¼å¼å¥å¤æä¸ççæåè§è§æ¨ççè¶å¿ï¼3D ç¹äºå¤çå¨åç±»ååå²æ¹é¢çæ°è¿å±ï¼ä»¥åå¨å¯¹ææ§è®­ç»åä¸ç¡®å®æ§éåæ¹é¢å¯¹æ¨¡åå¯é æ§çéè§ãæ­¤å¤ï¼å¯¹å®æ¶ãç¹å®åºæ¯ï¼å¦æåäº¤éï¼åé«ææµå¼å¤ççå³æ³¨ä¹è¡¨æäºé¢ååå®éåºç¨è½å°çåªåã</p>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ol>
<li><strong>"LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation" (Yinglin Duan et al.)</strong>: è¿ç¯è®ºæéè¿å°å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMï¼å¼å¥äº¤äºå¼å¤æä¸ççæï¼å±ç¤ºäºLLMå¨è§è§é¢åè¶è¶ä¼ ç»è¯å«ä»»å¡çå·¨å¤§æ½åï¼é¢ç¤ºçAIåå®¹çæåèæç¯å¢æå»ºçæ°èå¼ã</li>
<li><strong>"FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases" (Matteo Poggi, Fabio Tosi)</strong>: è¯¥å·¥ä½å©ç¨æ·±åº¦åºç¡æ¨¡ååè¿å¨åºåºç®ååæµä¼°è®¡ï¼å¯è½ä¸ºåæµé¢åå¸¦æ¥æ´é«æãæ´é²æ£çè§£å³æ¹æ¡ï¼å°¤å¶æ¯å¨å©ç¨é¢è®­ç»æ¨¡åç¥è¯æ¹é¢å·æåæ°æ§ã</li>
<li><strong>"COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization" (Yassine Taoudi-Benchekroun et al.)</strong>: è¿ç¯è®ºææåºäºä¸ä¸ªè§è§æ¨çæ¡æ¶ï¼æ¨å¨æ·±å¥ç ç©¶ç»åæ§åæ³åè½åï¼å¯¹äºçè§£åæåAIæ¨¡åçè®¤ç¥è½åå·æéè¦çè®ºåå®è·µæä¹ã</li>
</ol>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>LLM/MLLM ä¸è§è§ä»»å¡çæ·±åº¦èå</strong>: ä¸ä»ä»æ¯å¾åæè¿°ï¼èæ¯å©ç¨LLMçæ¨çåçæè½åè¿è¡æ´å¤æçè§è§ä»»å¡ï¼å¦ä¸ççæãäº¤äºï¼ã</li>
<li><strong>åºç¡æ¨¡åå¨ç¹å®è§è§ä»»å¡ä¸­çåºç¨</strong>: å©ç¨é¢è®­ç»çæ·±åº¦åºç¡æ¨¡åï¼å¦æ·±åº¦ä¼°è®¡ï¼æ¥è¾å©å¶ä»è§è§ä»»å¡ï¼å¦åæµï¼ï¼æé«æçåæ§è½ã</li>
<li><strong>æ¨¡åé²æ£æ§ä¸ä¸ç¡®å®æ§ä¼°è®¡çç³»ç»æ§ç ç©¶</strong>: éå¯¹å¯¹ææ§æ»å»åæ¨¡åé¢æµä¸ç¡®å®æ§ï¼éè¿æ··åä¸å®¶æ¨¡åï¼MoEï¼ç­ç»æè¿è¡éååæåã</li>
<li><strong>é«ææµå¼å¤çä¸å®æ¶æç¥</strong>: éå¯¹è§é¢æµåç¹å®åºç¨åºæ¯ï¼å¦èªå¨é©¾é©¶ï¼çå®æ¶ãé«æç®æ³è®¾è®¡ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹å¤æ¨¡æAIåçæå¼AIæå´è¶£ç</strong>:<ul>
<li><strong>"LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation"</strong> (Yinglin Duan et al.)</li>
<li><strong>"COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization"</strong> (Yassine Taoudi-Benchekroun et al.)</li>
</ul>
</li>
<li><strong>å¯¹3Dè§è§åç¹äºå¤çæå´è¶£ç</strong>:<ul>
<li><strong>"SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing"</strong> (Chaolei Wang et al.)</li>
<li><strong>"Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet"</strong> (Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari)</li>
</ul>
</li>
<li><strong>å¯¹æ¨¡åé²æ£æ§ãä¸ç¡®å®æ§ä¼°è®¡åå¯¹ææ§è®­ç»æå´è¶£ç</strong>:<ul>
<li><strong>"Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers"</strong> (Svetlana Pavlitska et al.)</li>
<li><strong>"Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation"</strong> (Svetlana Pavlitska et al.)</li>
</ul>
</li>
<li><strong>å¯¹åæµåè¿å¨ä¼°è®¡æå´è¶£ç</strong>:<ul>
<li><strong>"FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases"</strong> (Matteo Poggi, Fabio Tosi)</li>
</ul>
</li>
<li><strong>å¯¹å®æ¶ç³»ç»åç¹å®åºç¨ï¼å¦èªå¨é©¾é©¶ï¼æå´è¶£ç</strong>:<ul>
<li><strong>"WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool"</strong> (Zizun Li et al.)</li>
<li><strong>"A biologically inspired separable learning vision model for real-time traffic object perception in Dark"</strong> (Hulin Li et al.)</li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥Arxivè®¡ç®æºè§è§é¢åçå³é®è¿å±ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.05296v1">WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</a></li>
<li><a href="#2509.05263v1">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></li>
<li><a href="#2509.05198v1">Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</a></li>
<li><a href="#2509.05144v1">SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</a></li>
<li><a href="#2509.05086v1">Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers</a></li>
<li><a href="#2509.05012v1">A biologically inspired separable learning vision model for real-time traffic object perception in Dark</a></li>
<li><a href="#2509.04833v1">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a></li>
<li><a href="#2509.04816v1">Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</a></li>
<li><a href="#2509.05297v1">FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</a></li>
<li><a href="#2509.05249v1">COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.05296v1'></a></p>
<h2 id="wint3r-window-based-streaming-reconstruction-with-camera-token-pool"><a href="https://arxiv.org/abs/2509.05296v1">WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</a></h2>
<p><strong>Authors:</strong> Zizun Li, Jianjun Zhou, Yifan Wang, Haoyu Guo, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Junyi Chen, Chunhua Shen, Tong He</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zizun Liç­äººæ°åçè®ºæâWinT3R: Window-Based Streaming Reconstruction with Camera Token Poolâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼WinT3R: åºäºçªå£çå¸¦ç¸æºä»¤çæ± çæµå¼éå»º</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³å®æ¶3Déå»ºé¢åä¸­é¿æå­å¨çææï¼å³éå»ºè´¨éåå®æ¶æ§è½ä¹é´çæè¡¡ãä¼ ç»çå¨çº¿éå»ºæ¹æ³å¾å¾é¾ä»¥åæ¶å®ç°é«ç²¾åº¦çå ä½é¢æµåå¿«éçéå»ºéåº¦ãå·ä½æ¥è¯´ï¼ç°ææ¹æ³å¨å¤çå¾åæµæ¶ï¼ç±äºå¸§é´ä¿¡æ¯äº¤äºä¸è¶³æå¨å±ä¿¡æ¯å©ç¨æçä½ä¸ï¼å¯¼è´éå»ºè´¨éæ¬¡ä¼ï¼å°¤å¶æ¯å¨ç¸æºå§¿æä¼°è®¡æ¹é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
WinT3Ræ¨¡åå¼å¥äºä¸¤é¡¹æ ¸å¿åæ°æ¥è§£å³ä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>æ»å¨çªå£æºå¶ (Sliding Window Mechanism):</strong> ä¸ºäºç¡®ä¿çªå£åå¸§ä¹é´ä»¥åç¸é»çªå£ä¹é´æè¶³å¤çä¿¡æ¯äº¤æ¢ï¼WinT3Réç¨äºä¸ç§æ»å¨çªå£ç­ç¥ãä¸ä»¥å¾æ¹æ³ä»éè¿ç¶æä»¤çé´æ¥å±äº«ä¿¡æ¯ä¸åï¼è¯¥æºå¶åè®¸çªå£åçå¾åä»¤çç´æ¥ç¸äºä½ç¨ï¼ä»èå¨ä¸å¼å¥å¤§éè®¡ç®å¼éçæåµä¸æ¾èæé«å ä½é¢æµçè´¨éãçªå£å¤§å°è®¾ç½®ä¸º4ï¼æ­¥é¿ä¸º2ï¼ç¡®ä¿äºç¸é»çªå£ä¹é´æä¸åçå¸§éå ï¼è¿ä¸æ­¥å¢å¼ºäºè¿ç»­æ§ã</li>
<li><strong>ç¸æºä»¤çæ±  (Camera Token Pool):</strong> ä¸ºäºæé«ç¸æºå§¿æä¼°è®¡çå¯é æ§èä¸çºç²æçï¼WinT3Rä¸ºæ¯ä¸ªç¸æºå¸§ç»´æ¤ä¸ä¸ªç´§åçç¸æºä»¤çè¡¨ç¤ºï¼å¹¶å°å¶å­å¨å¨ä¸ä¸ªå¯æ©å±çå¨å±ç¸æºä»¤çæ± ä¸­ãå¨é¢æµæ°å°è¾¾å¸§çç¸æºåæ°æ¶ï¼æ¨¡åä¼å©ç¨æ± ä¸­ææåå²ç¸æºä»¤çä½ä¸ºå¨å±ä¿¡æ¯ãè¿ç§ç´§åçè¡¨ç¤ºæ¹å¼ï¼æ¯ä¸ªç¸æºå¸§ä¸ä¸ª1536ç»´çä»¤çï¼å¤§å¤§åå°äºå­å¨å¼éåè®¡ç®ææ¬ï¼åæ¶éè¿å¨å±è§è§å¢å¼ºäºç¸æºå§¿æä¼°è®¡çé²æ£æ§ã</li>
</ul>
<p>æ­¤å¤ï¼è®ºæè¿è®¾è®¡äºä¸ä¸ªå¸¦ææ»å¨çªå£æ©ç æ³¨æåæºå¶çç¸æºå¤´é¨ï¼ä»¥æ´å¥½å°å©ç¨ç´§åçç¸æºä»¤çè¿è¡é¢æµï¼å¹¶éç¨è½»éçº§å·ç§¯å¤´é¨æ¥é¢æµå±é¨ç¹äºå¾ï¼é¿åäºè®¡ç®æè´µçDPTå¤´é¨åå¯è½å¼å¥ç½æ ¼ç¶ä¼ªå½±ççº¿æ§å¤´é¨ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
WinT3Rå¨å¤ä¸ªæ°æ®éä¸è¿è¡äºå¹¿æ³çå®éªï¼å¹¶åå¾äºæåè¿çæ§è½ï¼</p>
<ul>
<li><strong>3Déå»ºè´¨é:</strong> å¨DTUãETH3Dã7-ScenesåNRGBDç­æ°æ®éä¸ï¼WinT3Rå¨åç¡®æ§ãå®æ´æ§åæ»ä½Chamferè·ç¦»æ¹é¢åä¼äºå¶ä»å¨çº¿éå»ºæ¹æ³ï¼å®ç°äºé«è´¨éçå ä½éå»ºã</li>
<li><strong>ç¸æºå§¿æä¼°è®¡:</strong> å¨Tanks and TemplesãCO3Dv2å7-Scenesæ°æ®éä¸ï¼WinT3Rå¨ç¸å¯¹æè½¬åç¡®æ§ï¼RRAï¼ãç¸å¯¹å¹³ç§»åç¡®æ§ï¼RTAï¼åAUC@30ç­ææ ä¸è¡¨ç°åºè²ï¼è¯æäºå¶ç¸æºå§¿æä¼°è®¡çå¯é æ§ã</li>
<li><strong>éå»ºéåº¦:</strong> WinT3Rå®ç°äº17 FPSçå®æ¶æ§è½ï¼æ¯è¿ä»ä¸ºæ­¢æå¿«çå¨çº¿éå»ºæ¹æ³ä¹ä¸ï¼å¨NVIDIA A800 GPUä¸è¿è¡ã</li>
<li><strong>è§é¢æ·±åº¦ä¼°è®¡:</strong> å¨SintelãBONNåKITTIæ°æ®éä¸çå®éªè¡¨æï¼WinT3Rå¨è§é¢æ·±åº¦ä¼°è®¡æ¹é¢ä¹è¾¾å°äºå¯æ¯ææ´ä¼çæ§è½ã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼WinT3Ræåå°è§£å³äºå¨çº¿3Déå»ºä¸­è´¨éä¸éåº¦çæè¡¡é®é¢ï¼ä¸ºå®æ¶ãé«ç²¾åº¦ç3Déå»ºä»»å¡æ ç«äºæ°çæ æã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
è®ºæä¸­æ²¡ææç¡®æåºå½åWinT3Ræ¨¡åçå·ä½å±éæ§ãç¶èï¼ä»å¶è®¾è®¡åå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼</p>
<ul>
<li><strong>çªå£å¤§å°åæ­¥é¿çéæ©:</strong> è®ºææå°ä¸ºäºå¹³è¡¡å®æ¶æ§åæ§è½ï¼éæ©äºç¹å®ççªå£å¤§å°ï¼4ï¼åæ­¥é¿ï¼2ï¼ãè¿äºåæ°çéæ©å¯è½å¯¹ä¸ååºæ¯ææ°æ®æµçæ§è½æå½±åï¼å¶æ³åè½åå¯è½éè¦è¿ä¸æ­¥æ¢ç´¢ã</li>
<li><strong>é¢è®­ç»æéä¾èµ:</strong> æ¨¡ååå§åä½¿ç¨äºDUSt3Rçé¢è®­ç»æéï¼è¿è¡¨æå¶æ§è½å¯è½é¨åä¾èµäºå¼ºå¤§çé¢è®­ç»æ¨¡åï¼å¯¹äºä»å¤´å¼å§è®­ç»çæ§è½å¯è½ææä¸åã</li>
<li><strong>å¨æåºæ¯åæ çº¹çåºå:</strong> å°½ç®¡è®ºææå°ä¼ ç»SfMæ¹æ³å¨å¨æåºæ¯åæ çº¹çåºåé¢ä¸´ææï¼ä½WinT3Rå¨è¿äºç¹å®æææ§åºæ¯ä¸çé²æ£æ§æªè¢«è¯¦ç»è®¨è®ºãè½ç¶å¶è®¾è®¡çå¿µï¼å¦å¨å±ç¸æºä»¤çæ± ï¼æå©äºæé«é²æ£æ§ï¼ä½å·ä½è¡¨ç°ä»æå¾æ·±å¥åæã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
åºäºè®ºæçè´¡ç®åæ½å¨å±éæ§ï¼æªæ¥ç ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸æ¹åï¼</p>
<ul>
<li><strong>èªéåºçªå£æºå¶:</strong> å¼åä¸ç§è½å¤æ ¹æ®åºæ¯å¤ææ§ãè¿å¨éåº¦æè®¡ç®èµæºå¨æè°æ´çªå£å¤§å°åæ­¥é¿çæºå¶ï¼ä»¥è¿ä¸æ­¥ä¼åæ§è½åè´¨éã</li>
<li><strong>æ´é«æçå¨å±ä¿¡æ¯èå:</strong> æ¢ç´¢é¤äºç¸æºä»¤çæ± ä¹å¤ï¼æ´ä¸°å¯ææ´ç»ç²åº¦çå¨å±ä¿¡æ¯è¡¨ç¤ºåèåæ¹æ³ï¼ä»¥è¿ä¸æ­¥æé«å¨å¤ææå¤§è§æ¨¡åºæ¯ä¸­çéå»ºç²¾åº¦åé²æ£æ§ã</li>
<li><strong>é²æ£æ§æå:</strong> éå¯¹æç«¯å¨æåºæ¯ãä½çº¹çç¯å¢æåç§å§çååçåºæ¯ï¼è¿ä¸æ­¥æåæ¨¡åçé²æ£æ§ã</li>
<li><strong>å¤æ¨¡æèå:</strong> ç»åå¶ä»ä¼ æå¨æ°æ®ï¼å¦IMUãæ¿åé·è¾¾ï¼æ¥å¢å¼ºéå»ºçåç¡®æ§åé²æ£æ§ï¼å°¤å¶æ¯å¨GPSä¿¡å·ä¸å¯ç¨æè§è§ä¿¡æ¯åéçç¯å¢ä¸­ã</li>
<li><strong>æ¨¡åæ³åè½å:</strong> è¿ä¸æ­¥ç ç©¶æ¨¡åå¨æ´å¹¿æ³ãæ´å¤æ ·åçâéå¤âåºæ¯ä¸­çæ³åè½åï¼åå°å¯¹ç¹å®æ°æ®éè®­ç»çä¾èµã</li>
<li><strong>å®æ¶æ§è½ä¼å:</strong> å°½ç®¡å·²è¾¾å°17 FPSï¼ä½å¯¹äºæäºå¯¹å»¶è¿è¦æ±æé«çåºç¨ï¼å¦AR/VRï¼ï¼ä»æè¿ä¸æ­¥ä¼åå®æ¶æ§è½çç©ºé´ï¼ä¾å¦éè¿æ¨¡ååªæãéåæç¡¬ä»¶å éã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps.</li>
<li>These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05296v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05296v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05263v1'></a></p>
<h2 id="latticeworld-a-multimodal-large-language-model-empowered-framework-for-interactive-complex-world-generation"><a href="https://arxiv.org/abs/2509.05263v1">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></h2>
<p><strong>Authors:</strong> Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Hao Jiang, Kang Chen, Shuang Qiu</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.AI, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
<script type="math/tex">90\times</script> increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâLatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generationâè®ºæçæè¦ï¼åå®¹åºäºæ¨æä¾çPDFå¨æï¼</p>
<p><strong>è®ºææè¦ï¼LatticeWorldï¼ä¸ä¸ªç±å¤æ¨¡æå¤§åè¯­è¨æ¨¡åé©±å¨çäº¤äºå¼å¤æä¸ççææ¡æ¶</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤æä¸ç»´ä¸çæ¨¡åçæä¸­çææãä¼ ç»çèæä¸ç»´åºæ¯åå»ºä¾èµäºæå¨å»ºæ¨¡ï¼æçä½ä¸ä¸é¾ä»¥æ©å±ãå°½ç®¡ç°ä»£æºå¨å­¦ä¹ æ¹æ³ï¼ç¹å«æ¯çææ¨¡åï¼è½å¤æ ¹æ®ç¨æ·æä»¤åå»ºèæä¸çï¼ä½ç°ææ¹æ³å¨äº¤äºæ§ãç©çåç¡®æ§ãå¤æ¨¡æè¾å¥å¤çä»¥åå·¥ä¸çº§çäº§æçæ¹é¢ä»å­å¨å±éæ§ãå·ä½æ¥è¯´ï¼è®ºæè´åäºå¼åä¸ä¸ªè½å¤ç®åä¸ç»´ç¯å¢å·¥ä¸çäº§æµç¨ãå®ç°é«ä¿çç©çæ¨¡æãå¤æºè½ä½äº¤äºåå®æ¶æ¸²æçæ¡æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
LatticeWorldæåºäºä¸ä¸ªç®åèææç3Dä¸ççææ¡æ¶ï¼å¶æ ¸å¿åæ°åæ¬ï¼
*   <strong>å¤æ¨¡æLLMä¸å·¥ä¸çº§æ¸²æå¼æçéæï¼</strong> LatticeWorldå°è½»éçº§LLMï¼å¦LLaMA-2-7Bï¼ä¸å·¥ä¸çº§æ¸²æå¼æï¼å¦Unreal Engine 5ï¼ç¸ç»åï¼ä»¥çæå¨æç¯å¢ãè¿ä¸Blenderç­å¹³å°ä¸åï¼UE5æä¾äºæ´çå®çç©çæ¨¡æãåçå¤æºè½ä½äº¤äºåå®æ¶æ¸²æã
*   <strong>å¯è§£éçä¸­é´è¡¨ç¤ºï¼</strong> æ¡æ¶æ¥åææ¬æè¿°åè§è§æä»¤ï¼å¦é«åº¦å¾æèå¾ï¼ä½ä¸ºå¤æ¨¡æè¾å¥ï¼å¹¶éè¿LLMçæåºæ¯å¸å±çç¬¦å·è¡¨ç¤ºï¼ç©éµï¼ï¼ä»¥åç¯å¢éç½®ãè¿ç§ç¬¦å·è¡¨ç¤ºå·æåºè²çå¯è§£éæ§åè¯­ä¹ç²¾åº¦ã
*   <strong>ä¸é¶æ®µè®­ç»æ¹æ¡ï¼</strong> éå¯¹å¯åé«åº¦åºæ¯çæï¼è®ºææåºäºä¸ä¸ªä¸é¶æ®µè®­ç»æ¹æ¡ï¼CLIPå¾®è°ï¼ç¨äºå°å½¢çè§£ï¼ãæç»­é¢è®­ç»ï¼ç¨äºç¹å¾å¯¹é½ï¼åç«¯å°ç«¯å¾®è°ï¼ç¨äºå¸å±çæï¼ï¼ä»¥æææ´åè§è§ä¿¡æ¯ã
*   <strong>åå±å±æ§è½¬æ¢æ¡æ¶ï¼</strong> éå¯¹ç¯å¢éç½®ï¼è®ºæè®¾è®¡äºä¸ä¸ªåå±å±æ§ç³»ç»ï¼å°ç²ç²åº¦å±æ§ï¼å¦å°å½¢ç±»åãå­£èãå¤©æ°ï¼ä¸ç»ç²åº¦å±æ§ï¼å¦å¯åº¦ãæè½¬ãä½ç½®ç­ï¼å³èèµ·æ¥ï¼ç®åäºå¤æç¯å¢éç½®ççæåç®¡çã
*   <strong>å¤æ¨¡ææ°æ®éæå»ºï¼</strong> è®ºææå»ºäºæ°çå¤æ¨¡ææ°æ®éï¼åºäºLoveDAåWildæ°æ®éï¼ï¼åå«èå¾ãå¸å±è¯­ä¹åå²ãé«åº¦å¾ãææ¬æè¿°åç¯å¢éç½®ï¼å¹¶å©ç¨GPT-4oè¿è¡æ°æ®æ æ³¨åæç¤ºå·¥ç¨ï¼ç¡®ä¿æ æ³¨æçååç¡®æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶ççæåç¡®æ§åè§è§ä¿çåº¦ï¼</strong> å®éªç»æè¡¨æï¼LatticeWorldå¨åºæ¯å¸å±çæåè§è§ä¿çåº¦æ¹é¢ä¼äºç°ææ¹æ³ï¼å¦GPT-4oãClaude 3.7 SonnetãDeepSeek-R1ãQwen2-VL-Maxï¼ã
*   <strong>æ¾èæåå·¥ä¸çäº§æçï¼</strong> ä¸ä¼ ç»æå¨çäº§æ¹æ³ç¸æ¯ï¼LatticeWorldå°å·¥ä¸çäº§æçæé«äº90åä»¥ä¸ï¼åæ¶ä¿æäºé«åæè´¨éãä¾å¦ï¼æ»çäº§æ¶é´ä»55å¤©ç¼©ç­å°ä¸å°0.6å¤©ã
*   <strong>æ¯æå¨æäº¤äºå¼ç¯å¢ï¼</strong> æ¡æ¶è½å¤æå»ºåå«å¨ææºè½ä½çå¤æºè½ä½äº¤äºç¯å¢ï¼æ¯ææºè½ä½åæ°ï¼ç±»åãæ°éãç¶æãä½ç½®ï¼çææéç½®ï¼å¹¶è½å®ç°åºäºé¢å®ä¹è§åçå¯¹ææ§è¡ä¸ºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æºè½ä½ç­ç¥ç®åï¼</strong> ç®åï¼å¯¹ææ§æºè½ä½éµå¾ªç®åçç­ç¥ï¼ä¾å¦ï¼å½ä¸»æºè½ä½æ¥è¿æ¶è¿è¡æ»å»ï¼ï¼æªæ¥éè¦å®ç°æ´å¤æ ·åçå¯¹ææ§è¡ä¸ºã
*   <strong>ä¸»ç©å®¶æ§å¶éå¶ï¼</strong> å½åæ¡æ¶ä»éäºæ§å¶åä¸ªä¸»ç©å®¶ï¼æªæ¥å¯æ©å±å°æ§å¶å¤ä¸ªä¸»ç©å®¶ã
*   <strong>ä¸»æºè½ä½èº«ä½é¨ä½æ§å¶ï¼</strong> ä¸»æºè½ä½çèº«ä½é¨ä½æ æ³ç¬ç«æ§å¶ï¼éè¦æ´ç²¾ç»çæ¨¡åæ¥å®ç°å¯¹ç¹å®é¨ä½çæ§å¶ã
*   <strong>èµäº§åºå¤æ ·æ§ï¼</strong> èµäº§åºæå¾æ©å±ï¼ä»¥åå«æ´å¤å¯¹è±¡åäº¤äºåç´ ï¼ä»èçææ´å¤æ ·åçèæä¸çã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å®ç°æ´å¤æåå¤æ ·åçå¯¹ææ§æºè½ä½è¡ä¸ºç­ç¥ã
*   æ©å±æ¡æ¶åè½ä»¥æ¯æå¤ä¸ªä¸»ç©å®¶çæ§å¶ã
*   éè¿æ´å¤æçå»ºæ¨¡å®ç°å¯¹ä¸»æºè½ä½ç¹å®èº«ä½é¨ä½çç²¾ç»æ§å¶ã
*   æ©å±èµäº§åºï¼å¢å æ´å¤å¯¹è±¡åäº¤äºåç´ ï¼ä»¥çææ´å¤æ ·åçèæä¸çã
*   ä¸°å¯æ°æ®éï¼æ¯ææ´å¹¿æ³çæè¿°é£æ ¼ï¼åæ¬æ´å£è¯­ååå¯¹è¯å¼çè¯­è¨ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Moreover, LatticeWorld achieves over a
<script type="math/tex">90\times</script> increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods.</li>
<li>Our demo
video is available at https://youtu.be/8VWZXpERR18</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05263v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05263v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05198v1'></a></p>
<h2 id="enhancing-3d-point-cloud-classification-with-modelnet-r-and-point-skipnet"><a href="https://arxiv.org/abs/2509.05198v1">Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</a></h2>
<p><strong>Authors:</strong> Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâEnhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNetâè®ºæçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼å¢å¼º3Dç¹äºåç±»ï¼ModelNet-Rä¸Point-SkipNet</strong></p>
<p>è¿ç¯è®ºæç±Mohammad SaeidãAmir SalarpouråPedram MohajerAnsariæ°åï¼æ¨å¨è§£å³3Dç¹äºåç±»é¢åä¸­ä¸¤ä¸ªå³é®é®é¢ï¼ç°ææ°æ®éçè´¨éé®é¢åé«æ§è½æ¨¡åè®¡ç®æçä½ä¸çé®é¢ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
3Dç¹äºåç±»å¨èªå¨é©¾é©¶ãæºå¨äººåå¢å¼ºç°å®ç­åºç¨ä¸­è³å³éè¦ãç¶èï¼å¹¿æ³ä½¿ç¨çModelNet40æ°æ®éå­å¨æ¾èç¼ºé·ï¼åæ¬æ ç­¾ä¸ä¸è´ãåå«2Dæ°æ®ãå°ºå¯¸ä¸å¹éä»¥åç±»å«åºåä¸è¶³ï¼è¿äºé®é¢ä¸¥éé»ç¢äºæ¨¡åæ§è½ãæ­¤å¤ï¼è®¸å¤æåè¿çç¹äºåç±»æ¨¡åï¼ç¹å«æ¯åºäºTransformerçæ¶æï¼è®¡ç®ææ¬é«æï¼ä¸éç¨äºèµæºåéçç¯å¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
è®ºææåºäºä¸¤é¡¹ä¸»è¦åæ°ï¼
*   <strong>ModelNet-Ræ°æ®éï¼</strong> è¿æ¯ModelNet40æ°æ®éçç²¾ç»åçæ¬ï¼æ¨å¨è§£å³åå§æ°æ®éä¸­çæ ç­¾ä¸ä¸è´ãä½è´¨éï¼2Dï¼æ°æ®ãå°ºå¯¸ä¸å¹éåæ¨¡ç³çç±»å«å®ä¹é®é¢ãéè¿äººå·¥æ£æ¥ãä¸å®¶äº¤åå¼ç¨åæ··æ·ç©éµåæï¼å¯¹æ°æ®éè¿è¡äºç³»ç»æ§ä¿®æ­£ï¼ç§»é¤äºæ­§ä¹æ ·æ¬ï¼å¹¶è°æ´äºç±»å«å®ä¹ï¼ä¾å¦ï¼åºåâè±çâåâè±ç¶âï¼ã
*   <strong>Point-SkipNetæ¨¡åï¼</strong> è¿æ¯ä¸ç§è½»éçº§çåºäºå¾çç¥ç»ç½ç»æ¶æï¼ä¸ä¸ºé«æåç¡®ç3Dç¹äºåç±»èè®¾è®¡ãå®å©ç¨äºé«æéæ ·ï¼æè¿ç¹éæ ·ï¼ãé»ååç»ï¼çæ¥è¯¢ï¼åè·³è·è¿æ¥ï¼å°æ± åç¹å¾ä¸ä¸­å¿ç¹è¿æ¥ï¼æ¥æè·å±é¨å ä½ç¹å¾å¹¶åå°è®¡ç®å¼éãå¶æ¨¡ååè®¾è®¡ä½¿å¶éç¨äºåç§3Dè§è§ä»»å¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ°æ®éè´¨éçéè¦æ§ï¼</strong> å®éªç»æè¡¨æï¼å¨ModelNet-Rä¸è®­ç»çæ¨¡åæ§è½æ¾èä¼äºå¨åå§ModelNetä¸è®­ç»çæ¨¡åãæææµè¯æ¨¡åå¨ModelNet-Rä¸çæ´ä½åç¡®çï¼OAï¼åå¹³åç±»å«åç¡®çï¼mAccï¼åæææé«ï¼è¿å¼ºè°äºé«è´¨éæ°æ®éå¨æååç±»åç¡®æ§æ¹é¢çå³é®ä½ç¨ã
*   <strong>Point-SkipNetçåè¶æ§è½åæçï¼</strong> Point-SkipNetå¨ModelNet-Rä¸å®ç°äºæåè¿çåç¡®çï¼94.33% OAå92.93% mAccï¼ï¼åæ¶åæ°æ°éæ¾èä½äºè®¸å¤ç°ææ¨¡åï¼1.47Mï¼ï¼ä½¿å¶éå¸¸éåèµæºåéçç¯å¢ï¼å¦ç§»å¨è®¾å¤ååµå¥å¼ç³»ç»ï¼ã
*   <strong>æ¶æè®¾è®¡éæ©ï¼</strong> æ¶èç ç©¶è¡¨æï¼æè½¬å¢å¼ºå¯¹å­¦ä¹ æè½¬ä¸åæ§è³å³éè¦ï¼èåºäºæ¼æ¥çè·³è·è¿æ¥å¨ä¿çæ´ä¸°å¯ä¿¡æ¯æ¹é¢ä¼äºåºäºå æ³çè·³è·è¿æ¥ï¼ä»èå¸¦æ¥æ´å¥½çåç±»æ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®éç²¾ç»åèå´æéï¼</strong> ModelNet-Rçç²¾ç»åç®åä»åºç¨äºModelNet40ç40ä¸ªç±»å«ä¸­ç5ä¸ªï¼æªæ¥éè¦æ©å±å°ææç±»å«ä»¥ç¡®ä¿æ°æ®éçæ´ä½ä¸è´æ§ã
*   <strong>å°ºå¯¸ä¿¡æ¯ä¸¢å¤±ï¼</strong> æ°æ®å½ä¸åç§»é¤äºä¸å°ºå¯¸ç¸å³çä¿¡æ¯ï¼æªæ¥çç ç©¶åºæ¢ç´¢ä¿çè¿äºä¿¡æ¯çææ¯ï¼ä¾å¦ç»åå°ºå¯¸æ¯çã
*   <strong>æ¨¡åéªè¯èå´ï¼</strong> Point-SkipNetä»å¨ModelNetåModelNet-Rä¸è¿è¡äºæµè¯ãéè¦å¯¹æ´å¤æ ·åçæ°æ®éè¿è¡è¿ä¸æ­¥éªè¯ï¼å¹¶ä½¿ç¨ModelNet-Réæ°è¯ä¼°ç°ææ¨¡åä»¥è¿è¡å¬å¹³æ¯è¾ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å°æ°æ®éç²¾ç»åè¿ç¨æ©å±å°ææModelNet40ç±»å«ï¼å¹¶æ´åçå®ä¸ççåªå£°æ°æ®éä»¥å¢å¼ºæ³åè½åã
*   æ¢ç´¢åè¿çå½ä¸åææ¯ï¼ä»¥ä¿çä¸å°ºå¯¸ç¸å³çä¿¡æ¯ã
*   å¨ScanObjectNNåShapeNetç­å¤æ ·å3Dåºåæ°æ®éä¸éªè¯Point-SkipNetçæ§è½ã
*   éè¿å¼ºè°é«ææ¨¡åè®¾è®¡åæ°æ®éå®æ´æ§ï¼ä¸ºæºå¨äººãèªå¨é©¾é©¶åå¢å¼ºç°å®é¢åä¸­æ´åç¡®ãè®¡ç®æçæ´é«ç3Dåç±»æ¨¡åéºå¹³éè·¯ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥ModelNet-RåPoint-SkipNetï¼å¨è§£å³3Dç¹äºåç±»çæ°æ®è´¨éåæ¨¡åæçæ¹é¢åå¾äºéè¦è¿å±ï¼ä¸ºè¯¥é¢åæªæ¥çç ç©¶å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05198v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05198v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05144v1'></a></p>
<h2 id="sgs-3d-high-fidelity-3d-instance-segmentation-via-reliable-semantic-mask-splitting-and-growing"><a href="https://arxiv.org/abs/2509.05144v1">SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</a></h2>
<p><strong>Authors:</strong> Chaolei Wang, Yang Luo, Jing Du, Siyu Chen, Yiping Chen, Ting Han</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Accurate 3D instance segmentation is crucial for high-quality scene
understanding in the 3D vision domain. However, 3D instance segmentation based
on 2D-to-3D lifting approaches struggle to produce precise instance-level
segmentation, due to accumulated errors introduced during the lifting process
from ambiguous semantic guidance and insufficient depth constraints. To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.
Unlike existing approaches that directly rely on raw lifted masks and sacrifice
segmentation accuracy, SGS-3D serves as a training-free refinement method that
jointly fuses semantic and geometric information, enabling effective
cooperation between the two levels of representation. Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances. For the geometric refinement, we construct fine-grained object
instances by exploiting both spatial continuity and high-level features,
particularly in the case of semantic ambiguity between distinct objects.
Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that
SGS-3D substantially improves segmentation accuracy and robustness against
inaccurate masks from pre-trained models, yielding high-fidelity object
instances while maintaining strong generalization across diverse indoor and
outdoor environments. Code is available in the supplementary materials.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chaolei Wangç­äººæ°åçè®ºæâSGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="sgs-3d-3d">SGS-3D: éè¿å¯é è¯­ä¹æ©ç åè£ä¸å¢é¿å®ç°é«ä¿ç3Då®ä¾åå²</h3>
<p><strong>1. è®ºæè§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³3Då®ä¾åå²é¢åçä¸ä¸ªæ ¸å¿ææï¼å¦ä½ä»2Då¾åæ°æ®ä¸­åç¡®ãé«ä¿çå°çæ3Då®ä¾åå²ç»æãç°æç2Då°3Dæåæ¹æ³ï¼å³ä»2Dè¯­ä¹æ©ç æ¨æ­3Då®ä¾ï¼éå¸¸ç±äºè¯­ä¹æå¯¼æ¨¡ç³åæ·±åº¦çº¦æä¸è¶³ï¼å¯¼è´ç´¯ç§¯è¯¯å·®ï¼ä»èäº§çä¸ç²¾ç¡®çå®ä¾çº§åå²ï¼å°¤å¶æ¯å¨ç¸é»ä½ç¬ç«çç©ä½ä¹é´ãè¿ç§ä¸åç¡®æ§å¨ç¼ºä¹æ·±åº¦ä¿¡æ¯çå¤ææ·å¤åºæ¯ä¸­å°¤ä¸ºçªåºã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
SGS-3Dæåºäºä¸ç§æ°é¢çãæ éè®­ç»çâåè£-ç¶å-å¢é¿âï¼split-then-growï¼æ¡æ¶ï¼éè¿èåèåè¯­ä¹åå ä½ä¿¡æ¯æ¥åæ2Då°3Dæåè¿ç¨ä¸­çè¯¯å·®ç§¯ç´¯ï¼ä»èå®ç°é«ä¿ç3Då®ä¾åå²ãå¶ä¸»è¦åæ°åæ¬ï¼</p>
<ul>
<li><strong>é®æ¡æç¥ç¹-å¾åæ å°ï¼Occlusion-Aware Point-Image Mappingï¼ï¼</strong> è®ºæå¼å¥äºä¸ç§é²æ£ä¸é«æçæ å°ç­ç¥ï¼æ éä¾èµçå®çæ·±åº¦å¾ï¼éè¿Z-bufferingç´æ¥ä»ç¹äºåç¸æºåæ°è®¡ç®å¯è§æ§ãè¿ç¡®ä¿äºåç¡®çæ©ç å°ç¹å¯¹åºå³ç³»ï¼å³ä½¿å¨çº¹çç¼ºå¤±æé«åå°è¡¨é¢ä¸ä¹è½ä¿æææã</li>
<li><strong>å±ç°æ©ç è¿æ»¤ï¼Co-occurrence Mask Filteringï¼ï¼</strong> ä¸ºäºè§£å³2Dæ©ç é¢æµä¸­çæ­§ä¹åä¸ä¸è´æ§ï¼SGS-3Då©ç¨3Då ä½åºåçå±ç°ä¿¡æ¯æ¥è¿æ»¤åä¿®åªæ¨¡ç³ç2Dæ©ç ãéè¿è®¡ç®æ©ç çè·¨è§å¾ä¸è´æ§åæ°ï¼è¯¥æºå¶è½å¤è¯å«å¹¶ç§»é¤ä¸ä¸è´çæ©ç ï¼ä»èç¡®ä¿æ´å¯é çè¯­ä¹ä¸è´æ§ã</li>
<li><strong>ç©ºé´è¿ç»­æ§åè£ï¼Spatial Continuity Splittingï¼ï¼</strong> éå¯¹è¯­ä¹ç¸ä¼¼ä½ç©ºé´ä¸åç¦»çç©ä½å¯è½è¢«éè¯¯åç»çé®é¢ï¼è®ºæå¨å¯åº¦ç©ºé´ä¸­å¯¹3Dè¯­ä¹æ©ç åºç¨HDBSCANèç±»ç®æ³ãè¿ä¼å°åå§ç3Dè¯­ä¹æ©ç åè£ææ´ç²¾ç»çãçº¯ç²¹çè¯­ä¹-å ä½ç§å­ï¼ä»èå¨å ä½ä¸ç»åè¯­ä¹æå¯¼å¹¶ä¿æè¯­ä¹è´¨éã</li>
<li><strong>ç¹å¾å¼å¯¼å¢é¿ï¼Feature-Guided Growingï¼ï¼</strong> ä¸ºäºå°åè£åçè¯­ä¹-å ä½ç§å­ç»è£æå®æ´çå¯¹è±¡å®ä¾ï¼è®ºæå¼å¥äºä¸ä¸ªé«ç»´ç¹å¾å¼å¯¼çå¢é¿è¿ç¨ãéè¿ç»ä¸çäº²åååæ°ï¼ç»åè¯­ä¹ç¸ä¼¼æ§åç©ºé´éå ï¼ï¼è¯¥è¿ç¨æºè½å°å°ç¢çåçå®ä¾èåä¸ºå®æ´çå¯¹è±¡ï¼ç¹å«æé¿å¤çäº¤ç»çåºæ¯ã</li>
<li><strong>å¤è§å¾æ¸è¿å¼åå¹¶ï¼Multi-View Progressive Mergingï¼ï¼</strong> éå¯¹åè§å¾å¢é¿çå±éæ§ï¼è®ºæéç¨æ¸è¿å¼å¤è§å¾åå¹¶ç­ç¥ï¼éè¿éæ­¥æ¾å®½3Dç©ºé´éå è¦æ±ï¼å°æ¥èªä¸åè§å¾çå®ä¾ææ¡æ´åä¸ºæç»çãå®æ´ç3Då¯¹è±¡å®ä¾ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
SGS-3Då¨ScanNet200ãScanNet++ï¼å®¤åç¯å¢ï¼åKITTI-360ï¼å®¤å¤ç¯å¢ï¼ç­å¤ä¸ªåºåæ°æ®éä¸åå¾äºæåè¿çæ§è½ã</p>
<ul>
<li><strong>æ¾èçåå²ç²¾åº¦æåï¼</strong> å¨KITTI-360æ°æ®éä¸ï¼SGS-3DçmAPæ¯æ¬¡ä¼ç«äºèSAI3Dé«åº16.4%ï¼éªè¯äºå¶é®æ¡æç¥æ å°å¨é²æ£æ·å¤åºæ¯çè§£ä¸­çå³é®ä½ç¨ãå¨å®¤ååºæ¯ä¸­ï¼SGS-3Dä¹æç»­è¶è¶äºOpen3DISåSAM2Objectç­ç°æSOTAæ¹æ³ã</li>
<li><strong>å¼ºå¤§çæ³åè½åï¼</strong> SGS-3Dä½ä¸ºä¸ç§æ éè®­ç»çæ¹æ³ï¼å¨ä¸åæ°æ®éååºæ¯ä¸­è¡¨ç°åºç¨³å®çæ§è½ï¼å³ä½¿å¨æ²¡æçå®æ·±åº¦ä¿¡æ¯çæåµä¸ä¹è½å®ç°é«ä¿çå¯¹è±¡å®ä¾ï¼è¿å¸æ¾äºå¶å¼ºå¤§çé¶æ ·æ¬æ³åè½åï¼é¿åäºçç£æ¹æ³åºæçæ°æ®éç¹å®è¿æåé®é¢ã</li>
<li><strong>æçåé²æ£æ§ï¼</strong> è®ºæå±ç¤ºäºSGS-3Då¨æçååç¡®æ§æ¹é¢çåè¶è¡¨ç°ï¼ä¾å¦å¨ScanNet200ä¸ä½¿ç¨æ´å°çå¾åï¼2.5% vs. 10%ï¼å´å®ç°äºæ´é«çåç¡®æ§ï¼å¹¶æ¾èåå°äºè¿åå²ç°è±¡ãæ­¤å¤ï¼SGS-3Då¯¹æ¨¡æé®æ¡è¡¨ç°åºé«åº¦çé²æ£æ§ï¼å³ä½¿å¨50%çåæ¯æ©ç è¢«é®æ¡çæåµä¸ä¹è½ä¿æç«äºåã</li>
<li><strong>å¼æ¾éåºæ¯çè§£åºç¨ï¼</strong> SGS-3Dçæçé«è´¨éãç±»å«æ å³çå®ä¾ææ¡ä¸ºå¼æ¾éåºæ¯çè§£æä¾äºå¼ºå¤§åºç¡ï¼å¯æ ç¼æ©å±å°å¼æ¾è¯æ±3Dåå²åææ¬é©±å¨ç3Då¯¹è±¡æç´¢ç­åºç¨ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æªæç¡®æåå½åæ¹æ³çå·ä½å±éæ§ï¼ä½ä»æªæ¥ç ç©¶æ¹åå¯ä»¥æ¨æ­åºä¸äºéå«çéå¶ï¼</p>
<ul>
<li><strong>å®æ¶æ§è½ï¼</strong> è®ºææå°æªæ¥æ¹ååæ¬âä¼åå¶å¤é¶æ®µè®¾è®¡ä»¥å®ç°å®æ¶æ§è½âï¼è¿æç¤ºå½åæ¹æ³å¯è½å°æªè¾¾å°å®æ¶å¤ççéåº¦ã</li>
<li><strong>å¨æåºæ¯å¤çï¼</strong> è®ºææåºæªæ¥æ¹ååæ¬âå°æä»¬çæ¡æ¶æ©å±å°å¨æåºæ¯âï¼è¿æå³çå½åæ¹æ³ä¸»è¦éå¯¹éæåºæ¯ï¼å¯¹å¨æåºæ¯çå¤çè½åå¯è½æéã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å°æ¡æ¶æ©å±å°å¨æåºæ¯ï¼éè¿æ´åæ¶é´ä¿¡æ¯æ¥æé«æ§è½ã
*   ä¼åå¤é¶æ®µè®¾è®¡ï¼ä»¥å®ç°å®æ¶æ§è½ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.</li>
<li>Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05144v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05144v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05086v1'></a></p>
<h2 id="robust-experts-the-effect-of-adversarial-training-on-cnns-with-sparse-mixture-of-experts-layers"><a href="https://arxiv.org/abs/2509.05086v1">Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers</a></h2>
<p><strong>Authors:</strong> Svetlana Pavlitska, Haixi Fan, Konstantin Ditschuneit, J. Marius ZÃ¶llner</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Svetlana Pavlitskaç­äººæ°åçè®ºæâRobust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layersâçå¨é¢æè¦ã</p>
<hr />
<h3 id="cnns">è®ºææè¦ï¼é²æ£ä¸å®¶ï¼å¯¹æè®­ç»å¯¹å¸¦æç¨çä¸å®¶æ··åå±CNNsçå½±å</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å·ç§¯ç¥ç»ç½ç»ï¼CNNsï¼å¨é¢å¯¹å¯¹ææ§æ»å»æ¶é²æ£æ§ä¸è¶³çé®é¢ãä¼ ç»çé²æ£åæ¹æ³éå¸¸èµæºå¯éï¼å æ­¤ä½èæ¢ç´¢äºä½¿ç¨ç¨çä¸å®¶æ··åï¼MoEï¼å±æ¥æé«CNNsçå¯¹æé²æ£æ§ï¼åæ¶é¿åé¢å¤çæ¨çææ¬ãå·ä½æ¥è¯´ï¼ä»ä»¬ç ç©¶äºMoEå±å¨å¯¹æè®­ç»èæ¯ä¸å¯¹æ¨¡åé²æ£æ§çå½±åï¼ä»¥åè·¯ç±æºå¶åä¸å®¶å©ç¨æ¨¡å¼å¦ä½å½±åè¿ä¸è¿ç¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¼å¥MoEå±ä»¥å¢å¼ºé²æ£æ§ï¼</strong> ä½èæåºéè¿æ¿æ¢ResNetæ¶æä¸­éå®çæ®å·®åï¼BlockMoEï¼æå·ç§¯å±ï¼ConvMoEï¼æ¥éæç¨çMoEå±ï¼ä»¥å¢å æ¨¡åå®¹éï¼åæ¶ä¿ææ¨çæçã
*   <strong>MoEå±æ¶æè®¾è®¡ï¼</strong> è®ºæå®ä¹äºä¸¤ç§ç»æä¸åçMoEå±åä½ï¼BlockMoEï¼æ¿æ¢æ´ä¸ªæ®å·®åï¼åConvMoEï¼æ¿æ¢åä¸ªå·ç§¯å±ï¼ï¼æ¨å¨å®ç°ç¸ä¼¼çè®¡ç®å¤æåº¦ä½ç»æä¸åï¼ä»¥ä¿è¿ä¸å®¶ä¸ä¸ååé²æ£æ§ã
*   <strong>é¨æ§ç½ç»ï¼Gateï¼è®¾è®¡ï¼</strong> éç¨äºä¸¤ç§é¨æ§ç½ç»ï¼GAP-FCï¼å¨å±å¹³åæ± å-å¨è¿æ¥ï¼åConv-GAPï¼å·ç§¯-å¨å±å¹³åæ± åï¼ï¼ç¨äºæ¿æ´»åå æä¸åçä¸å®¶ã
*   <strong>è´è½½åè¡¡æå¤±åæï¼</strong> è®ºæåæäºçµæå¤±ï¼Entropy Lossï¼åSwitch Losså¨é²æ­¢è·¯ç±å´©æºåé¼å±ä¸å®¶å¤æ ·åæ¹é¢çä½ç¨ï¼å¹¶åç°Switch Losså¨å¯¹æè®­ç»ä¸­å¯è½å¯¼è´æå¤çé²æ£æ§æåã
*   <strong>ä¸ªä½ä¸å®¶è¡ä¸ºåæï¼</strong> æ·±å¥ç ç©¶äºè·¯ç±å´©æºãä¸å®¶å©ç¨æ¨¡å¼ä»¥ååºå®ä¸å®¶æ¨çå¯¹é²æ£æ§åä¸ä¸åçå½±åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>MoEå±ä¸å¯¹æè®­ç»çç»åæ¾èæåé²æ£æ§ï¼</strong> å¨CIFAR-100æ°æ®éä¸ï¼å°åä¸ªMoEå±æå¥ResNetæ¶æçæ´æ·±å±é¶æ®µï¼å¹¶ç»åå¯¹æè®­ç»ï¼è½å¤æç»­æé«æ¨¡åå¨PGDåAutoPGDæ»å»ä¸çé²æ£æ§ï¼åæ¶ä¿ææç¥å¾®æåå¹²ååç¡®çãResNet-50æ¨¡åä¸­çæææ´ä¸ºæ¾èï¼è¡¨ææ´æ·±å±æ¨¡ååçäºè¾å¥ä¾èµçä¸å®¶è·¯ç±ã
*   <strong>BlockMoEä¼äºConvMoEï¼</strong> BlockMoEå±å¨å¯¹æè®­ç»ä¸è¡¨ç°åºæ´ä¸è´çé²æ£æ§åå¹²ååç¡®çæåï¼è¿å¯è½å ä¸ºæ®å·®åç­ç²ç²åº¦æ¨¡åè½å¤å­¦ä¹ æ´å·è¯­ä¹æä¹åç¬ç«çè¡¨ç¤ºã
*   <strong>Switch Lossçæå¤é²æ£æ§ï¼</strong> ä»¤äººæè®¶çæ¯ï¼å½ä½¿ç¨Switch Lossè¿è¡è´è½½åè¡¡æ¶ï¼è·¯ç±å¾åäºéä¸­å°ä¸å°é¨åè¿åº¦ä½¿ç¨çä¸å®¶ãè¿ç§éä¸­è®­ç»æ æä¸­ä½¿è¿äºè·¯å¾æ´å·é²æ£æ§ï¼å¯¼è´æäºä¸ªä½ä¸å®¶å¨é²æ£æ§æ¹é¢çè³ä¼äºæ´ä¸ªé¨æ§MoEæ¨¡åï¼è¿è¡¨æéè¿ä¸ä¸åå¯ä»¥åºç°é²æ£çå­è·¯å¾ã
*   <strong>çµæå¤±ä¿è¿ä¸å®¶å¤æ ·æ§ï¼</strong> çµæå¤±å¨å¯¹æè®­ç»ä¸­è¡¨ç°åºæ´å¥½çé²æ£æ§ï¼é¼å±æ´ææçä¸å®¶ä¸ä¸ååè·¯ç±ï¼ä»èå¨ä¸çºç²å¹²ååç¡®ççæåµä¸æé«é²æ£æ§ã
*   <strong>ä¸å®¶æ°éçå½±åï¼</strong> å¢å ä¸å®¶æ°ééå¸¸è½å¨ä¸å®ç¨åº¦ä¸ï¼éå¸¸æ¯8-16ä¸ªä¸å®¶ï¼æé«å¹²ååå¯¹æåç¡®çï¼ä½è¶åºæ­¤èå´æ§è½ä¼è¶äºå¹³ç¨³æä¸éã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é²æ£æ§æåä¸»è¦éäºå¯¹æè®­ç»åºæ¯ï¼</strong> å¨æ­£å¸¸è®­ç»ä¸ï¼ç¨çMoEå±å ä¹æ²¡æå¸¦æ¥æ¹è¿ã
*   <strong>é¨æ§æºå¶çå±éæ§ï¼</strong> æäºä¸ªä½ä¸å®¶å¨éç¦»ç¶æä¸è¡¨ç°ä¼äºæ´ä¸ªMoEæ¨¡åï¼è¿è¡¨æé¨æ§æºå¶å¯è½æªè½å§ç»å©ç¨æé²æ£çè®¡ç®è·¯å¾ã
*   <strong>ä¸å®¶æ°éä¸ä¸ä¸åï¼</strong> å¢å ä¸å®¶æ°éå¯è½ä¼éä½ä¸ªä½ä¸å®¶ä¸ä¸åç¨åº¦ï¼æç¤ºè·¯ç±ç²åº¦åææä¸å®¶ååä¹é´å­å¨æè¡¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>é²æ£æ§æç¥é¨æ§ç­ç¥ï¼</strong> å¼åè½å¤ä¼åå°è¾å¥è·¯ç±å°å¼¹æ§ä¸å®¶çé¨æ§ç­ç¥ã
*   <strong>ç»åå½©ç¥¨åè¯´ï¼Lottery Ticket Hypothesisï¼ï¼</strong> å©ç¨å½©ç¥¨åè¯´æ¥æå¯¼é²æ£å­ç½ç»çè¯å«åä¼åã
*   <strong>éæå¯¹æä¿¡å·ï¼</strong> å°å¯¹æä¿¡å·æ´åå°è·¯ç±ç®æ ä¸­ï¼æå¨æ¨çæ¶å¨æè°æ´ä¸å®¶éæ©ï¼ä»¥è¿ä¸æ­¥æé«æ§è½ã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºå¨CNNsä¸­å©ç¨ç¨çMoEå±æé«å¯¹æé²æ£æ§æä¾äºä¸ä¸ªæ°é¢çè§è§ï¼ç¹å«æ¯éè¿æ·±å¥åæè´è½½åè¡¡æå¤±åä¸ªä½ä¸å®¶è¡ä¸ºï¼æ­ç¤ºäºé²æ£å­è·¯å¾çåºç°ï¼ä¸ºæªæ¥é²æ£æ·±åº¦å­¦ä¹ æ¶æè®¾è®¡æä¾äºæä»·å¼çè§è§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization.</li>
<li>Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05086v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05086v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05012v1'></a></p>
<h2 id="a-biologically-inspired-separable-learning-vision-model-for-real-time-traffic-object-perception-in-dark"><a href="https://arxiv.org/abs/2509.05012v1">A biologically inspired separable learning vision model for real-time traffic object perception in Dark</a></h2>
<p><strong>Authors:</strong> Hulin Li, Qiliang Ren, Jun Li, Hanbing Wei, Zheng Liu, Linfang Fan</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Fast and accurate object perception in low-light traffic scenes has attracted
increasing attention. However, due to severe illumination degradation and the
lack of reliable visual cues, existing perception models and methods struggle
to quickly adapt to and accurately predict in low-light environments. Moreover,
there is the absence of available large-scale benchmark specifically focused on
low-light traffic scenes. To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation. We further propose the Separable
Learning Vision Model (SLVM), a biologically inspired framework designed to
enhance perception under adverse lighting. SLVM integrates four key components:
a light-adaptive pupillary mechanism for illumination-sensitive feature
extraction, a feature-level separable learning strategy for efficient
representation, task-specific decoupled branches for multi-task separable
learning, and a spatial misalignment-aware fusion module for precise
multi-feature alignment. Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead. Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end
trained SLVM surpasses Swin Transformer+EnlightenGAN and
ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key
metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage
points. The Dark-traffic dataset and complete code is released at
https://github.com/alanli1997/slvm.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Hulin Liç­äººæ°åçè®ºæâA biologically inspired separable learning vision model for real-time traffic object perception in Darkâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼åºäºçç©å¯åçæåäº¤éç®æ æç¥å¯åç¦»å­¦ä¹ è§è§æ¨¡å</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³å¨ä½åç§äº¤éåºæ¯ä¸­å®æ¶ãåç¡®å°æç¥ç®æ ï¼åæ¬ç®æ æ£æµãå®ä¾åå²ååæµä¼°è®¡ï¼çææãç°ææ¨¡åç±äºä¸¥éçç§æéååå¯é è§è§çº¿ç´¢çç¼ºä¹ï¼é¾ä»¥å¿«ééåºååç¡®é¢æµãæ­¤å¤ï¼ç¼ºä¹ä¸é¨éå¯¹ä½åç§äº¤éåºæ¯çå¤§è§æ¨¡åºåæ°æ®éä¹æ¯ä¸ä¸ªå³é®é®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼è®ºææåºäºä»¥ä¸åæ°ï¼</p>
<ul>
<li><strong>Dark-traffic æ°æ®éï¼</strong> æå»ºäºä¸ä¸ªç©çæ¥å°ï¼physically groundedï¼çç§æéåæ¹æ³ï¼å¹¶åå»ºäºè¿ä»ä¸ºæ­¢æå¤§çãå¯éæ æ³¨çä½åç§äº¤éåºæ¯æ°æ®éDark-trafficï¼æ¯æç®æ æ£æµãå®ä¾åå²ååæµä¼°è®¡ã</li>
<li><strong>å¯åç¦»å­¦ä¹ è§è§æ¨¡åï¼SLVMï¼ï¼</strong> æåºäºä¸ç§åçç©å­¦å¯åçæ¡æ¶ï¼ç¨äºå¢å¼ºæ¶å£åç§ä¸çæç¥è½åãSLVMåå«åä¸ªæ ¸å¿ç»ä»¶ï¼<ul>
<li><strong>åéåºç³å­æºå¶ï¼LAPMï¼ï¼</strong> æ¨¡æçç©ç³å­çå¿«éååºï¼ç¨äºæåå¯¹åç§ææçç¹å¾ï¼ä»¥è¡¥å¿åç§æå¤±å¹¶æåçº¹çæç¥ç¹å¾ã</li>
<li><strong>ç¹å¾çº§å¯åç¦»å­¦ä¹ ç­ç¥ï¼FSLConvï¼ï¼</strong> å¨ç¹å¾å±é¢è§£è¦çº¹çç¹å¾ï¼å®ç°æ´é«æçå­¦ä¹ åæ´ä¸°å¯çè¡¨ç¤ºå¤æ ·æ§ã</li>
<li><strong>ä»»å¡ç¹å®è§£è¦åæ¯ï¼</strong> éç¨ç¬ç«çãä»»å¡ç¹å®çåæ¯æ¥å­¦ä¹ åç§æç¥åè¯­ä¹ç¹å¾ï¼å¢å¼ºè·¨æç¥ä»»å¡çéåºæ§ã</li>
<li><strong>ç©ºé´éä½æç¥èåæ¨¡åï¼SNI-rï¼ï¼</strong> ä¸ç§ç¹å¾çº§æå¼æ¹æ³ï¼ç¨äºç²¾ç¡®çå¤ç¹å¾å¯¹é½ï¼è§£å³å¤å°ºåº¦èåä¸­çéä½é®é¢ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
å¹¿æ³çå®éªè¯æSLVMå¨éä½è®¡ç®å¼éçåæ¶ï¼å®ç°äºæåè¿çæ§è½ï¼</p>
<ul>
<li>å¨Dark-trafficæ°æ®éä¸ï¼SLVMå¨æ£æµä»»å¡ä¸­è¶è¶RT-DETR 11.2ä¸ªç¾åç¹ï¼å¨å®ä¾åå²ä¸­è¶è¶YOLOv12 6.1ä¸ªç¾åç¹ï¼å¹¶å°åºçº¿æ¨¡åçç«¯ç¹è¯¯å·®ï¼EPEï¼éä½äº12.37%ã</li>
<li>å¨LISåºåæµè¯ä¸­ï¼ç«¯å°ç«¯è®­ç»çSLVMå¨å³é®ææ ä¸å¹³åè¶è¶Swin Transformer+EnlightenGANåConvNeXt-T+EnlightenGAN 11ä¸ªç¾åç¹ï¼å¹¶è¶è¶Mask RCNNï¼å¸¦åç§å¢å¼ºï¼3.1ä¸ªç¾åç¹ã</li>
<li>è¿äºç»æå¸æ¾äºSLVMå¨ä½åç§æ¡ä»¶ä¸å¼ºå¤§çæ³åè½åãé²æ£æ§åè®¡ç®æçï¼æ éæ¾å¼å¢å¼ºæå»åªææ¯ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹è®¨è®ºäºå½åæ¹æ³çå±éæ§ï¼</p>
<ul>
<li><strong>å¨ææç¥å±éï¼</strong> SLVMå°æªå°éæç®æ æç¥æ©å±å°æ´å¹¿æ³çå¨ææç¥é¢åï¼å³æ æ³æç¥çå®ä¸çä¸­çç©ä½è¿å¨åå¨å±å¨æãç®åçâä¼ªå¨æâæç¥ä¾èµäºéå¸§æ£æµåäºåå³èï¼ç¼ºä¹çå®çè¿å¨ä¿¡æ¯ã</li>
<li><strong>å®éåºç¨ä¸­çæè¡¡ï¼</strong> å°½ç®¡SLVMè®¾è®¡ç¨äºå®æ¶é¨ç½²å¹¶å®ç°äºä½å»¶è¿ï¼ä½å¨å®éåºç¨ä¸­ä»å­å¨æè¡¡ãä¾å¦ï¼ç¹å¾åè§£ç­ç¥å¨å¤çå¤§è§æ¨¡ãé«åè¾¨çè¾å¥æ¶å¯è½ä¼å¼å¥è½»å¾®çå»¶è¿å¼éï¼å¹¶ä¸æ¨¡ååæ¶æéè¦ä»ç»è°åº¦å¹¶è¡ç¡¬ä»¶æè½åååæ¥å¶æçä¼å¿ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºæªæ¥çç ç©¶æ¹åï¼</p>
<ul>
<li>å¼åä¸ä¸ªè½å¤å®æ¶æ£æµååå²ä½åç§äº¤éç®æ ï¼å¹¶ä¼°è®¡å¶å¨ç©ºé´ä¸­ççå®è¿å¨çå®æ¶æç¥æ¡æ¶ã</li>
<li>å®ç°å®æ¶ç®æ æç¥ä¸å®æ¶åæµæåºæ¯æµä¼°è®¡çç´§å¯è¦åï¼ä»¥å®ç°å¯¹å¨æåºæ¯çç©çæ¥å°çè§£ã</li>
<li>è¿ä¸æ­¥ä¼åæ¨¡ååæ¶æåå¹¶è¡ç¡¬ä»¶è°åº¦ï¼ä»¥å¨å»¶è¿æææèµæºåéçè¾¹ç¼è®¾å¤ä¸å®ç°æ´é«æçé¨ç½²ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦çªåºäºè¯¥è®ºæå¨è§£å³ä½åç§äº¤éåºæ¯æç¥é®é¢ä¸çåæ°æ§ï¼ç¹å«æ¯å¶çç©å¯åå¼è®¾è®¡ãæ°æ°æ®éçæå»ºä»¥åå¨å¤ä»»å¡ä¸çåè¶æ§è½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation.</li>
<li>Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead.</li>
<li>Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05012v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05012v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04833v1'></a></p>
<h2 id="propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granularity-discrimination"><a href="https://arxiv.org/abs/2509.04833v1">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a></h2>
<p><strong>Authors:</strong> Ming Dai, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Ming Daiç­äººæ°åçè®ºæâPropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discriminationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="propvg-end-to-end-proposal-driven-visual-grounding-with-multi-granularity-discrimination_1">è®ºæãPropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discriminationãå¨é¢æè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è§è§å®ä½ï¼Visual Grounding, VGï¼æ¨å¨æ ¹æ®èªç¶è¯­è¨æ¥è¯¢å¨å¾åä¸­å®ä½ååå²å¯¹è±¡ãä¼ ç»åºäºææ¡çä¸¤é¶æ®µæ¡æ¶å å¶æçä½ä¸åè®¡ç®å¤ææ§é«èéæ¸è¢«ç«¯å°ç«¯ç´æ¥å¼ç¨èå¼åä»£ãç¶èï¼è¿äºç´æ¥å¼ç¨æ¹æ³è¿åº¦ä¾èµè¢«å¼ç¨ç®æ ççç£ï¼å¿½ç¥äºçªåºæ½å¨ç®æ ççå¤ï¼å¹¶ä¸éå¸¸æªè½æ´åå¤ç²åº¦å¤å«ï¼è¿å¨å¤æåºæ¯ä¸­å¯¹é²æ£çå¯¹è±¡è¯å«è³å³éè¦ãæ­¤å¤ï¼å¨å¹¿ä¹è§è§å®ä½ï¼Generalized Visual Grounding, GVGï¼ä»»å¡ä¸­ï¼æ¨¡åéè¦å¤çé¶ä¸ªæå¤ä¸ªè¢«å¼ç¨å¯¹è±¡çæåµï¼å¹¶åç¡®è¯å«ææä¸å¼ç¨è¡¨è¾¾å¼å¯¹åºçä½ç½®ï¼åæ¶èèç®æ å¯è½ä¸å­å¨çå¯è½æ§ãç°ææ¹æ³å¨å¤çè¿äºæææ¶å­å¨å±éæ§ï¼å°¤å¶æ¯å¨å¯¹è±¡å­å¨å¤å«æ¹é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
PropVG æåºäºä¸ç§ç«¯å°ç«¯ãåºäºææ¡çæ¡æ¶ï¼æ¨å¨åæä¸è¿°å±éæ§ï¼å¶ä¸»è¦åæ°åæ¬ï¼</p>
<ul>
<li><strong>ç«¯å°ç«¯ãæ æ£æµå¨ï¼Detector-Freeï¼çææ¡çæä¸å¼ç¨å¯¹è±¡çè§£éæï¼</strong> PropVG æ¯é¦ä¸ªæ ç¼éæåæ¯å¯¹è±¡ææ¡çæä¸å¼ç¨å¯¹è±¡çè§£çç«¯å°ç«¯ãåºäºææ¡çæ¡æ¶ï¼æ éé¢å¤çé¢è®­ç»æ£æµå¨ãè¿åæäºä¼ ç»åºäºææ¡æ¹æ³æ§è½ä¸ä½³åæ¨çéåº¦æ¢çç¼ºç¹ã</li>
<li><strong>åºäºå¯¹æ¯çå¼ç¨è¯åï¼Contrastive-based Refer Scoring, CRSï¼æ¨¡åï¼</strong> è¯¥æ¨¡åå©ç¨å¯¹æ¯å­¦ä¹ å¨å¥å­ååè¯çº§å«èªéåºå°å¹³è¡¡åæ´åè´¡ç®ï¼ä»¥å¢å¼ºæ¨¡åçè§£ååºåè¢«å¼ç¨å¯¹è±¡çè½åï¼ä»èç²¾ç¡®è¯ä¼°ææ¡çå¼ç¨ç¸å³æ§ã</li>
<li><strong>å¤ç²åº¦ç®æ å¤å«ï¼Multi-granularity Target Discrimination, MTDï¼æ¨¡åï¼</strong> è¯¥æ¨¡åèåäºå¯¹è±¡çº§åè¯­ä¹çº§ä¿¡æ¯ï¼éè¿å¼å¥åæ°åéªäº¤åæ³¨æåæºå¶ï¼å°åéªåæ°åå¸ä¿¡æ¯æ´åå°æ³¨æåå¾ä¸­ï¼å¹¶ç´æ¥æ³¨å¥é¢æµçå¼ç¨ååå²åæ°ï¼ä»¥ç¡®ä¿ç®æ å­å¨æ§å¨å¤ä¸ªé¢æµä¹é´ä¿æä¸è´æ§ï¼ä»èæé«å¯¹ä¸å­å¨ç®æ çè¯å«è½åã</li>
<li><strong>å¤ä»»å¡ååæ¡æ¶ï¼</strong> PropVG éç¨å¤ä»»å¡ååæ¡æ¶ï¼åæ¶å¤çæ£æµãå¼ç¨ãå¨å±åå²åç®æ å­å¨æ§å¤å«ä»»å¡ï¼å¢å¼ºäºæ¨¡åçéç¨æ§åå®ééç¨æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
PropVG å¨å¤ä¸ªåºåæµè¯ä¸åå¾äºæ¾èçæ§è½æåï¼è¯æäºå¶æææ§ï¼</p>
<ul>
<li><strong>ç»å¸è§è§å®ä½ï¼REC/RESï¼ï¼</strong> å¨RefCOCO/+/gæ°æ®éä¸ï¼PropVG å¨RECä»»å¡ä¸­è¶è¶äºåæ¬OneRefå¨åçåè¿æ¹æ³ï¼å¹³åæ§è½æå0.5%è³1.2%ï¼ä¸æ¨çéåº¦æ¯ä¼ ç»åºäºææ¡æ¨¡åå¿«4åãå¨RESä»»å¡ä¸­ï¼mIoUå¹³åæå1.4%è³4.0%ã</li>
<li><strong>å¹¿ä¹è§è§å®ä½ï¼GREC/GRESï¼ï¼</strong> å¨gRefCOCOãRef-ZOMåR-RefCOCO/+/gæ°æ®éä¸ï¼PropVG å¨ææææ ä¸åè¡¨ç°ä¼å¼ãä¾å¦ï¼å¨gRefCOCOçGRESä»»å¡ä¸­ï¼gloUææ ç¸æ¯SOTAæ¹æ³HDCææ¾èæåï¼+5.0%è³+2.0%ï¼ãå¨Ref-ZOMä¸ï¼åç¡®çãoIoUåmIoUåå«æå4.8%ã2.6%å2.2%ï¼çè³è¶è¶äºåºäºMLLMçæ¨¡åã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> CRSæ¨¡åéè¿å¥å­ååè¯çº§å«çå¯¹æ¯å­¦ä¹ ï¼å°F1åæ°åN-accåå«æé«äº1.8%å3.4%ãMTDæ¨¡åéè¿æ´åå¯¹è±¡çº§åè¯­ä¹çº§ç¹å¾ï¼å°F1åæ°åN-accåå«æé«äº1.5%å2.8%ãè¿äºæ¨¡åçç»åè¿ä¸æ­¥æåäºæ´ä½æ§è½ã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼PropVG æåå°åæäºä¼ ç»åºäºææ¡æ¹æ³çå±éæ§ï¼å¹¶å¨å¤æåå¹¿ä¹çè§è§å®ä½åºæ¯ä¸­å®ç°äºé²æ£ä¸é«æçå¯¹è±¡è¯å«ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­å¹¶æªæç¡®æåº PropVG æ¨¡åçå·ä½å±éæ§ãç¶èï¼ä»å¶è®¾è®¡åå®éªè®¾ç½®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çèèï¼</p>
<ul>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> å°½ç®¡ PropVG ç¸æ¯ä¼ ç»ä¸¤é¶æ®µæ¹æ³æé«äºæçï¼ä½ä½ä¸ºç«¯å°ç«¯ãå¤ä»»å¡çTransformer-basedæ¨¡åï¼å¶è®­ç»åæ¨çå¯è½ä»éè¦è¾é«çè®¡ç®èµæºï¼å°¤å¶æ¯å¨å¤çå¤§è§æ¨¡æ°æ®éåé«åè¾¨çå¾åæ¶ã</li>
<li><strong>æ¨¡åå¤ææ§ï¼</strong> æ´åäºå¤æ¨¡æç¼ç å¨ï¼BEiT-3ï¼ãSimFPNãUNetè§£ç å¨ãå¤å°ºåº¦å¯åå½¢è§£ç å¨ãCRSåMTDç­å¤ä¸ªå¤ææ¨¡åï¼æ¨¡åçæ´ä½æ¶æè¾ä¸ºå¤æï¼å¯è½å¢å äºè°è¯åç»´æ¤çé¾åº¦ã</li>
<li><strong>è¶åæ°æææ§ï¼</strong> è®ºæä¸­å¯¹Kå¼åæå¤±æéè¿è¡äºæ¶èç ç©¶ï¼è¡¨ææ¨¡åæ§è½å¯è½å¯¹è¿äºè¶åæ°çéæ©ææï¼éè¦ä»ç»è°ä¼ä»¥è¾¾å°æä½³æ§è½ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºæ¬è®ºæçè´¡ç®åç°æææ¯åå±ï¼æªæ¥ç ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸æ¹åï¼</p>
<ul>
<li><strong>è¿ä¸æ­¥æåæçåå¯æ©å±æ§ï¼</strong> å°½ç®¡ PropVG å·²ç»æ¯ä¼ ç»æ¹æ³æ´é«æï¼ä½ä»å¯æ¢ç´¢æ´è½»éçº§çæ¨¡åæ¶æãæ´é«æçæ³¨æåæºå¶æç¥è¯è¸é¦ç­ææ¯ï¼ä»¥è¿ä¸æ­¥éä½è®¡ç®ææ¬ï¼ä½¿å¶éç¨äºèµæºåéçè®¾å¤ææ´å¤§è§æ¨¡çæ°æ®éã</li>
<li><strong>å¤æ¨¡æèåçæ·±åº¦æ¢ç´¢ï¼</strong> è®ºæå©ç¨BEiT-3è¿è¡è§è§-è¯­è¨ç¼ç åèåï¼æªæ¥å¯ä»¥æ¢ç´¢æ´åè¿çå¤æ¨¡æèåç­ç¥ï¼ä¾å¦ç»åæ´å¼ºå¤§çå¤§åè¯­è¨æ¨¡åï¼LLMsï¼æè§è§-è¯­è¨æ¨¡åï¼VLMsï¼ï¼ä»¥å¢å¼ºæ¨¡åå¯¹å¤æè¯­ä¹åä¸ä¸æå³ç³»ççè§£è½åã</li>
<li><strong>é¶æ ·æ¬/å°æ ·æ¬å­¦ä¹ ï¼</strong> é´äºå¹¿ä¹è§è§å®ä½ä»»å¡ä¸­å¯è½å­å¨ç¨æææªè§è¿çå¯¹è±¡ï¼æ¢ç´¢é¶æ ·æ¬ï¼zero-shotï¼æå°æ ·æ¬ï¼few-shotï¼å­¦ä¹ è½åï¼ä½¿æ¨¡åè½å¤æ³åå°æ°ç±»å«ææ°åºæ¯ï¼å°æ¯ä¸ä¸ªéè¦çç ç©¶æ¹åã</li>
<li><strong>å®æ¶åºç¨ï¼</strong> è¿ä¸æ­¥ä¼åæ¨¡åçæ¨çéåº¦ï¼ä½¿å¶è½å¤æ»¡è¶³å®æ¶è§è§å®ä½åºç¨çéæ±ï¼ä¾å¦å¨èªå¨é©¾é©¶ãæºå¨äººå¯¼èªæå¢å¼ºç°å®ç­é¢åã</li>
<li><strong>è·¨é¢åæ³åè½åï¼</strong> è¯ä¼°åæå PropVG å¨ä¸åé¢åï¼å¦å»å­¦å¾åãå«æå¾åç­ï¼çæ³åè½åï¼å¯è½éè¦å¼å¥é¢åéåºæè¿ç§»å­¦ä¹ ææ¯ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¨é¢æ¦æ¬è®ºæçæ ¸å¿åå®¹ï¼å¹¶ä»ä¸ä¸è§åº¦åæå¶è´¡ç®åæ½å¨åå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors.</li>
<li>Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04833v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04833v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04816v1'></a></p>
<h2 id="extracting-uncertainty-estimates-from-mixtures-of-experts-for-semantic-segmentation"><a href="https://arxiv.org/abs/2509.04816v1">Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation</a></h2>
<p><strong>Authors:</strong> Svetlana Pavlitska, Beyza Keskin, Alwin FaÃbender, Christian Hubschneider, J. Marius ZÃ¶llner</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Svetlana Pavlitskaç­äººæ°åçè®ºæâExtracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼ä»ä¸å®¶æ··åæ¨¡åä¸­æåè¯­ä¹åå²çä¸ç¡®å®æ§ä¼°è®¡</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®¡ç®æºè§è§æ¨¡åï¼ç¹å«æ¯è¯­ä¹åå²æ¨¡åå¨å®å¨å³é®åºç¨ä¸­ï¼å¦ä½åç¡®ä¸è¯å¥½æ ¡åå°ä¼°è®¡é¢æµä¸ç¡®å®æ§çé®é¢ãä¼ ç»çéææ¹æ³ï¼å¦æ¨¡åéææMC Dropoutï¼è½ç¶å¯ä»¥éåä¸ç¡®å®æ§ï¼ä½è®¡ç®ææ¬è¾é«ãä½èæ¢ç´¢äºä¸å®¶æ··åï¼MoEï¼æ¨¡åä½ä¸ºä¸ç§æ´é«æçæ¿ä»£æ¹æ¡ï¼ä»¥å¨ä¸ä¿®æ¹å¶æ¶æçæåµä¸ï¼ä»MoEä¸­æåå¯é çä¸ç¡®å®æ§ä¼°è®¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>æ æ¶æä¿®æ¹çä¸ç¡®å®æ§æåï¼</strong> è®ºæçæ ¸å¿åæ°å¨äºï¼å®æåºå¹¶è¯æäºå¯ä»¥ä»æ åãæªç»ä¿®æ¹çMoEæ¨¡åä¸­æååºè¯å¥½æ ¡åçé¢æµä¸ç¡®å®æ§ä¼°è®¡ï¼èæ éå¼å¥é¢å¤çæ¶æä¿®æ¹ææ¾å¼ä¸ç¡®å®æ§å»ºæ¨¡ãè¿ä¿çäºMoEçæçåä¸ç°æé¢è®­ç»æ¨¡åçå¼å®¹æ§ã
*   <strong>ä¸ç§é¢æµä¸ç¡®å®æ§ä¼°è®¡æ¹æ³ï¼</strong> ä½èç ç©¶äºä¸ç§ä»MoEä¸­æåé¢æµä¸ç¡®å®æ§çæ¹æ³ï¼
    *   <strong>é¢æµçµï¼PEï¼ï¼</strong> éè¿å¯¹MoEè¾åºæ¦çåå¸çé¦åçµè¿è¡æµéã
    *   <strong>äºä¿¡æ¯ï¼MIï¼ï¼</strong> éåæ¨¡ååæ°ä¸ç¡®å®æ§å¼èµ·çè®¤ç¥ä¸ç¡®å®æ§ã
    *   <strong>ä¸å®¶æ¹å·®ï¼EVï¼ï¼</strong> åºäºMoEä¸­åä¸å®¶é¢æµç¸å¯¹äºæç»MoEè¾åºçåå¼æ§ã
*   <strong>è·¯ç±ä¸ç¡®å®æ§è¯ä¼°ï¼</strong> å¼å¥äºâé¨æ§çµâï¼gate entropyï¼æ¥éåè·¯ç±ä¸ç¡®å®æ§ï¼åæ é¨æ§ç½ç»å¨éæ©ä¸å®¶æ¶çç½®ä¿¡åº¦ï¼ä»èæä¾æ¨¡ååé¨å³ç­è¿ç¨çé¢å¤ä¿¡å·ã
*   <strong>ä¸¤ç§MoEè¾åºèåæ¹æ³ï¼</strong> éå¯¹PEåMIçè®¡ç®ï¼æåºäºâå å ï¼stackedï¼âåâå æï¼weightedï¼âä¸¤ç§æ¹æ³æ¥èåä¸å®¶è¾åºã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>OODæ°æ®ä¸çä¼è¶æ§ï¼</strong> å¨A2D2æ°æ®éä¸ï¼MoEå¨æ¡ä»¶æ­£ç¡®æ§ææ æ¹é¢ï¼å¦p(accurate certain)åp(uncertain inaccurate)ï¼æ¯éææ¹æ³åMC Dropoutå¨åå¤ï¼OODï¼æ°æ®ä¸äº§çäºæ´å¯é çä¸ç¡®å®æ§ä¼°è®¡ãè¿è¡¨æMoEå¨å¤çæ°æ®åç§»æ¶å·ææ´å¼ºçé²æ£æ§ã
*   <strong>è·¯ç±ä¸ç¡®å®æ§æ ¡åï¼</strong> ç®åçé¨æ§æºå¶æ¯æ´å¤æçç±»å«é¨æ§æºå¶è½æ´å¥½å°æ ¡åè·¯ç±ä¸ç¡®å®æ§ä¼°è®¡ã
*   <strong>ä¸å®¶æ°éå¯¹æ ¡åçå½±åï¼</strong> å¨Cityscapesæ°æ®éä¸çå®éªè¡¨æï¼å¢å ä¸å®¶æ°éå¯ä»¥è¿ä¸æ­¥å¢å¼ºä¸ç¡®å®æ§æ ¡åï¼å°¤å¶æ¯å¨è´å¯¹æ°ä¼¼ç¶ï¼NLLï¼æ¹é¢ã
*   <strong>é¢æµä¸ç¡®å®æ§åè·¯ç±ä¸ç¡®å®æ§çåç¦»æ½åï¼</strong> éè¿é¢æµçµåé¨æ§çµå¯¹ä¸ç¡®å®æ§ç»ä»¶è¿è¡åç¦»ï¼ä¸ºè§£è¦è®¤ç¥ä¸ç¡®å®æ§åå¶ç¶ä¸ç¡®å®æ§æä¾äºæ½åã
*   <strong>MoEä½ä¸ºOODæ£æµæ¹æ³ï¼</strong> è®ºæè§å¯å°çMoEå¨OODæ°æ®ä¸çä¼è¶ç»æè¡¨æï¼MoEå¯ä»¥ä½ä¸ºä¸ç§ææçOODæå¼å¸¸æ£æµæ¹æ³ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>å æä¸å å æ¹æ³çå·®å¼ï¼</strong> å æåå å æ¹æ³å¨è®¡ç®PEåMIæ¹é¢çå·®å¼ç¸å¯¹è¾å°ï¼è¿å¯è½éå¶äºå®ä»¬å¨æäºåºæ¯ä¸çåºåè½åã
*   <strong>ç®åé¨æ§ä¸ç±»å«é¨æ§çæè¡¡ï¼</strong> ç®åé¨æ§æºå¶è½ç¶å¨è·¯ç±ä¸ç¡®å®æ§æ ¡åæ¹é¢è¡¨ç°æ´å¥½ï¼ä½ç±»å«é¨æ§ï¼å¨å¢å å·ç§¯å±æ¶ï¼å¨åå²ç²¾åº¦ä¸æææåï¼è¿è¡¨æå¨ä¸åç®æ ä¹é´å­å¨æè¡¡ã
*   <strong>ä¸å®¶æ°éå¢å çéçº¿æ§å½±åï¼</strong> å°½ç®¡å¢å ä¸å®¶æ°éå¯ä»¥ç¥å¾®æé«ä¸ç¡®å®æ§ä¼°è®¡ï¼å¦NLLï¼ï¼ä½å®å¹¶ä¸æ»æ¯è½ç´æ¥æ¹åæææ ¡åææ ï¼çè³å¯è½ç¥å¾®ä¸éãè¿å¯è½éè¦æ´å¤æçé¨æ§ææ­£ååç­ç¥æ¥ååå©ç¨é¢å¤çä¸å®¶ã
*   <strong>MoEå¨æ åæ ¡åææ ä¸çè¡¨ç°ï¼</strong> å¨æ°æ®åç§»ä¸ï¼MoEå¨ECEåBrieråæ°ç­æ åæ ¡åææ ä¸å¹¶ä¸æ»æ¯ä¼äºéææ¹æ³ï¼å°½ç®¡å®å¨æ¡ä»¶æ­£ç¡®æ§ææ ä¸è¡¨ç°åºè²ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>èªéåºä¸å®¶éæ©ï¼</strong> æ¢ç´¢æ´æºè½çèªéåºä¸å®¶éæ©æºå¶ï¼ä»¥è¿ä¸æ­¥ä¼åMoEçæ§è½åä¸ç¡®å®æ§ä¼°è®¡ã
*   <strong>æ´æ·±å±çé¨æ§ç­ç¥ï¼</strong> ç ç©¶æ´å¤æçé¨æ§ç½ç»æ¶æåç­ç¥ï¼ä»¥æ´å¥½å°å©ç¨ä¸å®¶å¤æ ·æ§ï¼æé«ä¸ç¡®å®æ§é²æ£æ§åäºæä½æ§ã
*   <strong>æ´ä¸°å¯çä¸å®¶å¤æ ·æ§ï¼</strong> æ¢ç´¢å¦ä½éè¿å¢å ä¸å®¶å¤æ ·æ§æ¥è¿ä¸æ­¥æåMoEçä¸ç¡®å®æ§ä¼°è®¡è½åã
*   <strong>OODæå¼å¸¸æ£æµï¼</strong> è¿ä¸æ­¥ç ç©¶MoEä½ä¸ºOODæå¼å¸¸æ£æµæ¹æ³çæ½åï¼å¹¶å¼åä¸é¨çè¯ä¼°åè®®ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications.</li>
<li>Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.04816v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04816v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05297v1'></a></p>
<h2 id="flowseek-optical-flow-made-easier-with-depth-foundation-models-and-motion-bases"><a href="https://arxiv.org/abs/2509.05297v1">FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</a></h2>
<p><strong>Authors:</strong> Matteo Poggi, Fabio Tosi</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training. FlowSeek marries the latest advances on the
design space of optical flow networks with cutting-edge single-image depth
foundation models and classical low-dimensional motion parametrization,
implementing a compact, yet accurate architecture. FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Matteo PoggiåFabio Tosiæ°åçè®ºæâFlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Basesâçå¨é¢æè¦ã</p>
<hr />
<h3 id="flowseek">FlowSeek: å©ç¨æ·±åº¦åºç¡æ¨¡ååè¿å¨åºç®ååæµä¼°è®¡</h3>
<p><strong>1. è®ºæè§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>è¯¥è®ºææ¨å¨è§£å³å½ååæµä¼°è®¡é¢åé¢ä¸´çä¸¤ä¸ªæ ¸å¿ææï¼
*   <strong>é«ç¡¬ä»¶èµæºéæ±ï¼</strong> ç°ææåè¿çåæµæ¨¡åéå¸¸éè¦å¤§éçè®¡ç®èµæºï¼å¤åé«ç«¯GPUï¼è¿è¡è®­ç»ï¼è¿ä½¿å¾ç ç©¶å¯¹èµæºæéçå¢éèè¨é¾ä»¥ä¼åã
*   <strong>æ³åè½åä¸è¶³ï¼</strong> å°½ç®¡æ§è½å¼ºå¤§ï¼ä½è®¸å¤åæµæ¨¡åå¨è·¨ä¸åæ°æ®éï¼å³é¶æ ·æ¬æ³åï¼æ¶è¡¨ç°ä¸ä½³ï¼é¾ä»¥ä¿æç»èçç²¾ç»åº¦ã</p>
<p>FlowSeekè´åäºå¨ä¿æé«ç²¾åº¦çåæ¶ï¼æ¾èéä½è®­ç»æéçç¡¬ä»¶ææ¬ï¼å¹¶æé«æ¨¡åå¨ä¸ååºæ¯ä¸çæ³åè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<p>FlowSeekçæ ¸å¿åæ°å¨äºå¶ç¬ç¹çæ¶æï¼å®å·§å¦å°èåäºä¸ä¸ªçä¼¼ç¬ç«çé¢åï¼
*   <strong>åæµç½ç»è®¾è®¡çåæ²¿è¿å±ï¼</strong> FlowSeekä»¥SEA-RAFT [81]ç­è¿­ä»£ç»åæ¶æä¸ºåºç¡ï¼å©ç¨å¶å¨åæµä¼°è®¡ä¸­çæææ§ã
*   <strong>åå¾åæ·±åº¦åºç¡æ¨¡åï¼Depth Foundation Modelsï¼ï¼</strong> è®ºæé¦æ¬¡å°é¢è®­ç»çæ·±åº¦åºç¡æ¨¡åï¼å¦Depth Anything v2 [93]ï¼éæå°åæµä¼°è®¡æ¡æ¶ä¸­ãè¿äºæ¨¡åè½å¤æä¾ä¸°å¯çè¯­ä¹åå ä½åéªç¥è¯ï¼éè¿æåæ·±åº¦å¾åæ·±åº¦ç¹å¾æ¥å¢å¼ºåæµéª¨å¹²ç½ç»çç¹å¾è¡¨ç¤ºåä¸ä¸æä¿¡æ¯ã
*   <strong>ç»å¸çä½ç»´è¿å¨åæ°åï¼Motion Basesï¼ï¼</strong> è®ºæå¼å¥äºåºäº3Dè¿å¨èªç±åº¦çä½ç»´è¿å¨åºï¼Bmotionï¼ï¼è¿äºè¿å¨åºè½å¤ä¸ºåæµæ¨¡åæä¾ä¸ä¸ªåå§çãå ä½ä¸è´çè¿å¨çæµï¼å°¤å¶éç¨äºåæ§è¿å¨åºæ¯ã</p>
<p>éè¿å°è¿äºç»ä»¶ç»åï¼FlowSeekå®ç°äºä»¥ä¸å³é®æ¹æ³è®ºè´¡ç®ï¼
*   <strong>ç¹å¾å¢å¼ºï¼</strong> å°æ·±åº¦åºç¡æ¨¡åæåçæ·±åº¦å¾åæ·±åº¦ç¹å¾ä¸åæµéª¨å¹²ç½ç»çç¹å¾è¿è¡æ¼æ¥ï¼å¹¶éè¿ä¸ä¸ªæµå±BottNeckç½ç»è¿è¡å¤çï¼ä»¥çææ´ä¸°å¯çç¹å¾ç¨äºç¸å³æ§è®¡ç®ã
*   <strong>ä¸ä¸æåéèç¶æå¢å¼ºï¼</strong> æ·±åº¦å¾ä¹è¢«éå¥ContextNetï¼ä¸å¾åä¸èµ·æåæ´å¼ºçä¸ä¸æç¹å¾ååå§éèç¶æï¼ä»¥æå¯¼è¿­ä»£åæµä¼°è®¡è¿ç¨ã
*   <strong>è¿å¨åºéæï¼</strong> è¿å¨åºéè¿ä¸ä¸ªBasesNetæ¨¡åæååºå¯éçè¿å¨ç¹å¾ï¼å¹¶ä¸åå§ä¸ä¸æåéèç¶æç¹å¾è¿è¡æ¼æ¥ï¼ä¸ºè¿­ä»£åæµä¼°è®¡æä¾å ä½åéªã
*   <strong>ç´§åä¸é«æçæ¶æï¼</strong> è¿ç§éæåè®¸FlowSeekå¨åä¸ªæ¶è´¹çº§GPUä¸è¿è¡è®­ç»ï¼æ¾èéä½äºç¡¬ä»¶é¢ç®ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<p>FlowSeekå¨å¤ä¸ªæ åæ°æ®éä¸åå¾äºæ¾èçæ§è½æåï¼è¯æäºå¶æ¹æ³çæææ§ï¼
*   <strong>ç¡¬ä»¶æçï¼</strong> FlowSeekä»éåä¸ªæ¶è´¹çº§GPUè¿è¡è®­ç»ï¼ç¸æ¯å¤§å¤æ°ææ°æ¹æ³æéçç¡¬ä»¶é¢ç®éä½äºçº¦8åï¼è¿ä½¿å¾åè¿çåæµç ç©¶æ´å·å¯åæ§ã
*   <strong>åè¶çé¶æ ·æ¬æ³åè½åï¼</strong> å¨Sintel FinalåKITTIæ°æ®éä¸ï¼FlowSeekç¸æ¯ä¹åçæåè¿æ¹æ³SEA-RAFTï¼å®ç°äº10%å15%çç¸å¯¹æ¹è¿ãå¨SpringåLayeredFlowæ°æ®éä¸ï¼FlowSeekä¹è¡¨ç°åºä¼è¶çæ§è½ï¼å°¤å¶æ¯å¨LayeredFlowæ°æ®éçéæååå°åºåï¼FlowSeekå¨å¤§å¤æ°ææ ä¸åå¾äºæ¾èçæ¹è¿ã
*   <strong>ç²¾åº¦åç»èï¼</strong> å°½ç®¡ç¡¬ä»¶é¢ç®è¾ä½ï¼FlowSeekä»è½å®ç°æåè¿çç²¾åº¦ï¼å¹¶è½æ¢å¤æ´ç²¾ç»çç»èï¼åå°ä¼ªå½±ã
*   <strong>è®¾è®¡éç¨æ§ï¼</strong> è®ºæéè¿å°FlowSeekä¸ä¸åçåæµéª¨å¹²ç½ç»ï¼å¦CRAFTåFlowFormerï¼ä»¥åä¸åçæ·±åº¦åºç¡æ¨¡åï¼å¦DPTãDepth Anything v1åv2ï¼ç»åï¼è¯æäºå¶è®¾è®¡æ¹æ¡çéç¨æ§ã</p>
<p>è¿äºç»æçæä¹å¨äºï¼FlowSeekè¯æäºéè¿æºè½å°éç¨ç°æé¢è®­ç»æ¨¡åï¼å¦æ·±åº¦åºç¡æ¨¡åï¼åç»åç»å¸è®¡ç®æºè§è§ææ¯ï¼å¯ä»¥å¨ä¸ä¾èµå¤§éç¡¬ä»¶èµæºçæåµä¸ï¼æ¨å¨è®¡ç®æºè§è§é¢åï¼ç¹å«æ¯åæµä¼°è®¡ï¼çè¿æ­¥ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<p>è®ºæä¸­æå°äºFlowSeekçå±éæ§ï¼
*   <strong>å¯¹é¢è®­ç»åºç¡æ¨¡åçä¾èµï¼</strong> FlowSeekçè®­ç»æçå¾çäºå¤§åãé¢è®­ç»çæ·±åº¦åºç¡æ¨¡åï¼è¿äºæ¨¡åæ¬èº«å¯è½æ¯å¨é«ç¡¬ä»¶é¢ç®ä¸éè¿å¤§è§æ¨¡ç½ç»æ°æ®è®­ç»çãå æ­¤ï¼FlowSeekçä½ç¡¬ä»¶éæ±æ¯å»ºç«å¨è¿äºç°ææ¨¡åçåºç¡ä¹ä¸çï¼èä¸æ¯ä»é¶å¼å§ã
*   <strong>å¨æäºå¤æåºæ¯ä¸çæ§è½æè¡¡ï¼</strong> å¨æäºæ¶èå®éªä¸­ï¼FlowSeekçæ´å¤§åä½ï¼MåLï¼å¨KITTIæ°æ®éä¸è¡¨ç°åºè½»å¾®çç²¾åº¦ä¸éï¼è¿å¯è½ä¸å¨åä¸ªGPUä¸è®­ç»è¿äºå¤ææ¨¡åæ¶ç¼ºä¹å¼ºå¤§çé¢è®­ç»æå³ã
*   <strong>è¿å¨åºçåè®¾ï¼</strong> è¿å¨åºä¸»è¦éç¨äºåæ§è¿å¨ï¼å¯¹äºéåæ§æå¤æè¿å¨åºæ¯ï¼å¶åå§çæµçåç¡®æ§å¯è½åéã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<p>è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>è¿ä¸æ­¥å©ç¨æ´åç¡®çåºç¡æ¨¡åï¼</strong> éçæªæ¥æ´åç¡®çæ·±åº¦åºç¡æ¨¡åçåºç°ï¼FlowSeekçæ§è½ææè¿ä¸æ­¥æåã
*   <strong>è§£å³è®­ç»æ°æ®ç¶é¢ï¼</strong> è®­ç»æ°æ®ä»ç¶æ¯åæµæ¨¡åçä¸ä¸ªéè¦ç¶é¢ãæªæ¥çç ç©¶å°è´åäºæ¨¡ææ·±åº¦ä¼°è®¡æç®ä¸­æåçç­ç¥ï¼ä»¥è§£å³è¿ä¸é®é¢ï¼ä¾å¦éè¿åææ°æ®çææç¥è¯è¸é¦ã
*   <strong>æ¢ç´¢å¶ä»è®¡ç®æºè§è§ä»»å¡ï¼</strong> FlowSeekçæåç»éªå¯ä»¥å¯åå¶ä»è®¡ç®æºè§è§é¢åï¼è®¾è®¡åºå¨æå°ç¡¬ä»¶é¢ç®ä¸å¯è®­ç»çæ°æ¨¡åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training.</li>
<li>FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05297v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05297v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.05249v1'></a></p>
<h2 id="cogitao-a-visual-reasoning-framework-to-study-compositionality-generalization"><a href="https://arxiv.org/abs/2509.05249v1">COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</a></h2>
<p><strong>Authors:</strong> Yassine Taoudi-Benchekroun, Klim Troyan, Pascal Sager, Stefan Gerber, Lukas Tuggener, Benjamin Grewe</p>
<p><strong>Published:</strong> 2025-09-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Yassine Taoudi-Benchekrounç­äººæ°åçè®ºæâCOGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalizationâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åæåè¿çæºå¨å­¦ä¹ æ¨¡åå¨ç»åæ§ï¼compositionalityï¼åæ³åï¼generalizationï¼æ¹é¢çæç»­å±éæ§ãå°½ç®¡äººç±»æºè½è½å¤è½»æ¾å°ç»åå·²å­¦ä¹ çæ¦å¿µå¹¶åºç¨äºæ°é¢æå¢ï¼ä½æºå¨æ¨¡åå¨è§è§é¢åä¸­ï¼å°¤å¶æ¯å¨é¢å¯¹çæåç´ çæ°é¢ç»åæ¶ï¼å¾å¾é¾ä»¥å®ç°è¿ç§è½åãç ç©¶é®é¢æ¯ï¼å¦ä½ç³»ç»å°ç ç©¶åè¯ä¼°è§è§æ¨¡åå¨ç»åæ§åæ³åæ¹é¢çè½åï¼å¹¶æ­ç¤ºå®ä»¬å¨è¿äºæ¹é¢çä¸è¶³ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæå¼å¥äº<strong>COGITAO</strong>ï¼è¿æ¯ä¸ä¸ªæ¨¡ååä¸å¯æ©å±çæ°æ®çææ¡æ¶ååºåï¼ç¨äºç³»ç»å°ç ç©¶è§è§é¢åä¸­çç»åæ§åæ³åãå¶ä¸»è¦åæ°åæ¬ï¼
*   <strong>è§åçæä»»å¡ï¼</strong> COGITAOåARC-AGIé®é¢è®¾ç½®çå¯åï¼æå»ºäºåºäºè§åçä»»å¡ï¼è¿äºä»»å¡å°ä¸ç³»ååæ¢åºç¨äºç½æ ¼ç¯å¢ä¸­çå¯¹è±¡ã
*   <strong>ä¸°å¯çåæ¢éåç»åæ§ï¼</strong> æ¡æ¶æ¯æå¯¹28ç§å¯äºæä½çåå­åæ¢è¿è¡ä»»ææ·±åº¦çç»åï¼ä»èè½å¤åå»ºæ°ç¾ä¸ä¸ªç¬ç¹çä»»å¡è§åï¼å¶æ°éè¿è¶ç°ææ°æ®éã
*   <strong>çµæ´»çåæ°æ§å¶ï¼</strong> å¯¹ç½æ ¼åæ°ååå¯¹è±¡å±æ§ï¼å¦å¤§å°ãå¯¹ç§°æ§ãè¿éæ§ãé¢è²ãå½¢ç¶ç­ï¼è¿è¡å¹¿æ³æ§å¶ï¼ä½¿å¾ä»»å¡é¾åº¦èå´å¹¿æ³ï¼å¹¶è½çæå ä¹æ éçæ ·æ¬ã
*   <strong>ç³»ç»æ§è¯ä¼°åºåï¼</strong> COGITAOä¸ä»æ¯ä¸ä¸ªçæå¨ï¼æ´æ¯ä¸ä¸ªè¯ä¼°æ¨¡åå¨ä¸åæ³åè®¾ç½®ï¼å¦ç»åæ³ååç¯å¢æ³åï¼ä¸è¡¨ç°çåºåã
*   <strong>å¼æºæ§ï¼</strong> COGITAOçä»£ç åæ°æ®éå®å¨å¼æºï¼ä»¥ä¿è¿è¯¥é¢åçæç»­ç ç©¶ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºææä¾äºä½¿ç¨æåè¿è§è§æ¨¡åï¼Vanilla ViTãGrid ViTåLLaDAï¼è¿è¡çåºçº¿å®éªãä¸»è¦ç»æåæä¹å¦ä¸ï¼
*   <strong>æ¨¡åå¨ååè¡¨ç°è¯å¥½ï¼ä½æ³åè½åå·®ï¼</strong> å°½ç®¡å¨è®­ç»æ°æ®ï¼ååï¼ä¸è¡¨ç°å¼ºå²ï¼ä½æ¨¡åå¨æ³åå°çæåç´ çæ°é¢ç»åæ¶ï¼åå¤ï¼è¡¨ç°åºä¸è´çå¤±è´¥ãä¾å¦ï¼å¨ç»åæ³åç ç©¶ä¸­ï¼æäºæ¨¡åçåå¤åç¡®ççè³ä½è³5.1%æ6.4%ã
*   <strong>Grid-ViTçä¼å¿ï¼</strong> Grid-ViTï¼ä¸ç§éå¯¹ç½æ ¼ç»æä»»å¡å®å¶çViTåä½ï¼å¨å¤§å¤æ°å®éªè®¾ç½®ä¸­å®ç°äºæå¼ºçååæ§è½ï¼å¹¶å¨æäºåå¤åºæ¯ä¸­è¡¨ç°åºä¼äºVanilla ViTçæ§è½ï¼è¿è¡¨æå¶å¼å¥çå½çº³åç½®å¯¹ç½æ ¼å³ç³»ç»æä»»å¡ææã
*   <strong>LLaDAçæ½åï¼</strong> LLaDAï¼ä¸ç§åºäºæ©æ£çè¯­è¨æ¨¡åï¼å¨æäºéç½®ä¸­å®ç°äºæé«çåå¤åç¡®çï¼çè³å¨æäºæåµä¸ä¸Grid-ViTç¸å½æç¥ä¼ï¼è¿æç¤ºäºå¶å¨ç³»ç»æ³åæ¹é¢çæ½åã
*   <strong>ç¡®è®¤ç»åæ§æ³åææï¼</strong> å®éªç»æè¯å®äºCOGITAOåºåçæææ§ï¼å¹¶å¼ºè°äºéè¦è½å¤å¤çç»åæ§åç³»ç»æ³åçæ¶æã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ½è±¡æ§ååææ§ï¼</strong> COGITAOæ¯æ½è±¡ååæçï¼ç¼ºä¹çå®ä¸çè§è§æ°æ®çåºç¡ãè½ç¶è®¸å¤ä»»å¡å¯¹äººç±»æ¥è¯´æ¦å¿µä¸å¾ç®åï¼ä½æ·±åº¦è¾å¤§çåæ¢åºåå¯è½å¨è®¤ç¥ä¸è¦æ±å¾é«ã
*   <strong>åæ¢ç»æå·®å¼ï¼</strong> å°½ç®¡ä»»å¡ç©ºé´å¾å¤§ï¼ä½ä¸äºåæ¢å¨ç»ææå¤ææ§ä¸æ²¡ææ¾èå·®å¼ã
*   <strong>ä¸è§è§å¤ææ§çæ··æ·ï¼</strong> ç°æè§è§åºåå¾å¾å°è§è§å¤ææ§ä¸å³ç³»ç»ææ··æ·ï¼ä»èé»ç¢äºå¯¹çæ­£çç»åæ³åçå³æ³¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±æ¶æï¼</strong> æªæ¥å·¥ä½å¯ä»¥æ¢ç´¢é¢å¤çæ¶æï¼ä»¥è¿ä¸æ­¥æé«æ§è½ã
*   <strong>ä¸ä¸æå­¦ä¹ ï¼</strong> å°æ¡æ¶æ©å±å°ä¸ä¸æå­¦ä¹ ï¼ä¾å¦éè¿æä¾æ¼ç¤ºç¤ºä¾ï¼å¯ä»¥è¯ä¼°æ¨¡åå¨å·²ç¥åçäºæ´é¿ä¸ä¸æçè®¾ç½®ä¸­çæ³åè½åã
*   <strong>è¯¾ç¨å­¦ä¹ ï¼</strong> ç±äºå¶å¯æ§çç¯å¢åå¯è°èçé¾åº¦ï¼COGITAOéå¸¸éåè¯¾ç¨å­¦ä¹ ï¼éæ­¥å¢å ä»»å¡å¤ææ§ä»¥æå¯¼å­¦ä¹ ã
*   <strong>åé¨æ¨¡åè¡¨ç¤ºåæï¼</strong> åæå¨COGITAOä¸è®­ç»çåé¨æ¨¡åè¡¨ç¤ºï¼å¯è½æ­ç¤ºæ¨¡åå¦ä½åå±ä»¥å¯¹è±¡ä¸ºä¸­å¿æä»¥åæ¢ä¸ºä¸­å¿çæ½è±¡ã
*   <strong>æ´è¯¦å°½çå®éªï¼</strong> æ©å±å®éªï¼æ¶µçCOGITAOæ¯æçææåæ¢ï¼ä»¥æä¾æ´å¨é¢çæ¨¡åè¡ä¸ºçè§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models.</li>
<li>To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains.</li>
<li>We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.05249v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.05249v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-08 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
