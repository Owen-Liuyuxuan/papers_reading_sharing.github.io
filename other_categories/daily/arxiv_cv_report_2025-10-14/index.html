<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-14 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-13/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-15/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-14">Arxiv Computer Vision Papers - 2025-10-14</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-13" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-13)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#compositional-zero-shot-learning-a-survey" class="nav-link">Compositional Zero-Shot Learning: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation" class="nav-link">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#a-survey-on-agentic-multimodal-large-language-models" class="nav-link">A Survey on Agentic Multimodal Large Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#image-to-video-transfer-learning-based-on-image-language-foundation-models-a-comprehensive-survey" class="nav-link">Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#ev4dgs-novel-view-rendering-of-non-rigid-objects-from-monocular-event-streams" class="nav-link">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a>
                </li>
                <li class="nav-item">
                    <a href="#dit360-high-fidelity-panoramic-image-generation-via-hybrid-training" class="nav-link">DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</a>
                </li>
                <li class="nav-item">
                    <a href="#scaling-language-centric-omnimodal-representation-learning" class="nav-link">Scaling Language-Centric Omnimodal Representation Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#infinihuman-infinite-3d-human-creation-with-precise-control" class="nav-link">InfiniHuman: Infinite 3D Human Creation with Precise Control</a>
                </li>
                <li class="nav-item">
                    <a href="#physic-physically-plausible-3d-human-scene-interaction-and-contact-from-a-single-image" class="nav-link">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a>
                </li>
                <li class="nav-item">
                    <a href="#expvid-a-benchmark-for-experiment-video-understanding-reasoning" class="nav-link">ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-14">Arxiv Computer Vision Papers - 2025-10-14</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-13">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-13)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»å¤©ç Arxiv è®¡ç®æºè§è§è®ºæä¸»è¦å´ç»<strong>å¤æ¨¡æå­¦ä¹ ã3D åå®¹çæä¸çè§£ä»¥åè§é¢åæ</strong>å±å¼ãæ¾èè¶å¿åæ¬å©ç¨å¤§åè¯­è¨æ¨¡å (LLMs) å¢å¼ºè§è§ä»»å¡ï¼ä»¥åå¨å¤æåºæ¯ä¸å¯¹ 3D äººä½ãç©ä½åè§é¢è¿è¡æ´ç²¾ç»çå»ºæ¨¡åçæã</p>
<p><strong>ä¸»è¦ä¸»é¢åè¶å¿ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æä¸ LLM èåï¼</strong> å¤ç¯è®ºææ¢è®¨äºå°è§è§ä¸è¯­è¨æ¨¡åç»åï¼ä»¥å®ç°æ´å¼ºå¤§çé¶æ ·æ¬å­¦ä¹ ãä»£çå¼å¤æ¨¡æäº¤äºåå¨æ¨¡æè¡¨ç¤ºå­¦ä¹ ãè¿è¡¨æ LLMs å¨è®¡ç®æºè§è§é¢åçæ¸éæ¥çå æ·±ï¼æä¸ºè§£å³å¤ææ¨çåæ³åé®é¢çå³é®ã</li>
<li><strong>3D åå®¹çæä¸çè§£ï¼</strong> 3D äººä½ãåºæ¯åéåæ§ç©ä½ççæä¸äº¤äºæ¯å¦ä¸ä¸ªçªåºä¸»é¢ãä»åç®äºä»¶æµçææ°è§è§ï¼å°é«ä¿çå¨æ¯å¾åçæï¼åå°ç²¾ç¡®æ§å¶ç 3D äººä½åå»ºåç©çå¯ä¿¡ç 3D äººæºäº¤äºï¼é½æ¾ç¤ºåºè¯¥é¢åå¨çå®æåå¯æ§æ§æ¹é¢çæ¾èè¿æ­¥ã</li>
<li><strong>è§é¢çè§£ä¸åå²ï¼</strong> è§é¢åæä»ç¶æ¯æ´»è·çç ç©¶é¢åï¼ç¹å«æ¯å¨å¤æè§é¢å¯¹è±¡åå²åå®éªè§é¢çè§£ä¸æ¨çæ¹é¢ãè¿åæ äºå¯¹å¨æåºæ¯åæ¶é´åºåæ°æ®æ´æ·±å±æ¬¡çè§£çéæ±ã</li>
<li><strong>é¶æ ·æ¬ä¸æ³åè½åï¼</strong> é¶æ ·æ¬å­¦ä¹ ä½ä¸ºä¸ç§éè¦çæ³åèå¼ï¼å¨å¤æ¨¡æèæ¯ä¸å¾å°äºå¹¿æ³å³æ³¨ï¼æ¨å¨ä½¿æ¨¡åè½å¤å¤çæªè§è¿çæ°æ®ç±»å«ã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"A Survey on Agentic Multimodal Large Language Models" (Huanjin Yao et al.)ï¼</strong> è¿ç¯ç»¼è¿°éå¸¸åæ¶ï¼å ä¸ºå®æ·±å¥æ¢è®¨äºä»£çå¼å¤æ¨¡æ LLMsï¼è¿æ¯ä¸ä¸ªæ°å´ä¸æå·æ½åçç ç©¶æ¹åï¼é¢ç¤ºçæªæ¥ AI ç³»ç»å°å·å¤æ´å¼ºçèªä¸»å³ç­åå¤æ¨¡æäº¤äºè½åã</li>
<li><strong>"InfiniHuman: Infinite 3D Human Creation with Precise Control" (Yuxuan Xue et al.)ï¼</strong> è¿ç¯è®ºæå¨ 3D äººä½çææ¹é¢åå¾äºæ¾èçªç ´ï¼å¼ºè°äºâæ éâåâç²¾ç¡®æ§å¶âï¼è¿å¯¹äºèæç°å®ãæ¸¸æãçµå½±å¶ä½ä»¥åæ°å­äººé¢åå·æå·¨å¤§åºç¨ä»·å¼ã</li>
<li><strong>"PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image" (Pradyumna Yalandur Muralidhar et al.)ï¼</strong> ä»åå¼ å¾åæ¨æ­ç©çå¯ä¿¡ç 3D äººæºäº¤äºåæ¥è§¦æ¯ä¸ä¸ªæå·æææ§çä»»å¡ï¼è¯¥å·¥ä½å¨çå®æåç©çä¸è´æ§æ¹é¢è¿åºäºéè¦ä¸æ­¥ï¼å¯¹æºå¨äººãäººæºäº¤äºååºæ¯çè§£è³å³éè¦ã</li>
<li><strong>"Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams" (Takuya Nakabayashi et al.)ï¼</strong> å©ç¨åç®äºä»¶æµè¿è¡éåæ§ç©ä½çæ°è§è§æ¸²æï¼å±ç¤ºäºäºä»¶ç¸æºå¨å¤çé«éè¿å¨åä½åç§æ¡ä»¶ä¸çç¬ç¹ä¼å¿ï¼ä¸ºå¨æåºæ¯éå»ºæä¾äºæ°æè·¯ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>ä»£çå¼å¤æ¨¡æ LLMsï¼</strong> å° LLMs æåå°å·æä»£çè½åï¼ä½¿å¶è½å¤è§åãæ§è¡ååæå¤æ¨¡æä»»å¡ï¼æ¯æªæ¥ AI åå±çéè¦æ¹åã</li>
<li><strong>äºä»¶ç¸æºå¨ 3D éå»ºä¸­çåºç¨ï¼</strong> äºä»¶æµæ°æ®å¨å¤çé«ééåæ§ç©ä½åå¨æåºæ¯æ¹é¢å±ç°åºå·¨å¤§æ½åã</li>
<li><strong>é«ä¿çãå¯æ§ç 3D åå®¹çæï¼</strong> å°¤å¶æ¯å¨ 3D äººä½åå¨æ¯å¾åæ¹é¢ï¼å¼ºè°äºå¯¹çæåå®¹ç»èåå±æ§çç²¾ç»æ§å¶ã</li>
<li><strong>ç©çå¯ä¿¡ç 3D äº¤äºå»ºæ¨¡ï¼</strong> ä¸ä»ä»æ¯å ä½ä¸çå¹éï¼æ´è¦èèç©çè§å¾åæ¥è§¦åå­¦ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºå¸ææ·±å¥äºè§£åæ²¿è¿å±çç ç©¶äººåï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>"A Survey on Agentic Multimodal Large Language Models" (Huanjin Yao et al.)ï¼</strong> äºè§£è¯¥é¢åææ°ä¸æå·æ½åçæ¹åã</li>
<li><strong>"InfiniHuman: Infinite 3D Human Creation with Precise Control" (Yuxuan Xue et al.)ï¼</strong> å¦ææ¨å¯¹ 3D äººä½çæåæ°å­äººææ¯æå´è¶£ã</li>
<li><strong>"PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image" (Pradyumna Yalandur Muralidhar et al.)ï¼</strong> å¦ææ¨å³æ³¨ 3D åºæ¯çè§£ãäººæºäº¤äºææºå¨äººé¢åã</li>
<li><strong>"Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams" (Takuya Nakabayashi et al.)ï¼</strong> å¦ææ¨å¯¹äºä»¶ç¸æºåå¨æ 3D éå»ºæå´è¶£ã</li>
<li><strong>"Scaling Language-Centric Omnimodal Representation Learning" (Chenghao Xiao et al.)ï¼</strong> å¦ææ¨å³æ³¨å¤æ¨¡æåºç¡æ¨¡ååéç¨è¡¨ç¤ºå­¦ä¹ ã</li>
</ul>
<p>è¿ä»½æè¦æ¨å¨å¸®å©æ¨å¿«éææ¡ä»æ¥ Arxiv è®¡ç®æºè§è§é¢åçå³é®åå±ï¼å¹¶ä¸ºæ¨çè¿ä¸æ­¥ç ç©¶æä¾æ¹åã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></li>
<li><a href="#2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></li>
<li><a href="#2510.10991v1">A Survey on Agentic Multimodal Large Language Models</a></li>
<li><a href="#2510.10671v1">Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</a></li>
<li><a href="#2510.11717v1">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a></li>
<li><a href="#2510.11712v1">DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</a></li>
<li><a href="#2510.11693v1">Scaling Language-Centric Omnimodal Representation Learning</a></li>
<li><a href="#2510.11650v1">InfiniHuman: Infinite 3D Human Creation with Precise Control</a></li>
<li><a href="#2510.11649v1">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a></li>
<li><a href="#2510.11606v1">ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.11106v1'></a></p>
<h2 id="compositional-zero-shot-learning-a-survey"><a href="https://arxiv.org/abs/2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></h2>
<p><strong>Authors:</strong> Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision
that enables models to recognize unseen combinations of known attributes and
objects during inference, addressing the combinatorial challenge of requiring
training data for every possible composition. This is particularly challenging
because the visual appearance of primitives is highly contextual; for example,
<code>small'' cats appear visually distinct from</code>older'' ones, and <code>wet'' cars
differ significantly from</code>wet'' cats. Effectively modeling this contextuality
and the inherent compositionality is crucial for robust compositional zero-shot
recognition. This paper presents, to our knowledge, the first comprehensive
survey specifically focused on Compositional Zero-Shot Learning. We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement. We provide a detailed comparative analysis of
these methods, highlighting their core advantages and limitations in different
problem settings, such as closed-world and open-world CZSL. Finally, we
identify the most significant open challenges and outline promising future
research directions. This survey aims to serve as a foundational resource to
guide and inspire further advancements in this fascinating and important field.
Papers studied in this survey with their official code are available on our
github: https://github.com/ans92/Compositional-Zero-Shot-Learning</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯ç±Ans Munirç­äººæ°åçè®ºæãCompositional Zero-Shot Learning: A Surveyãå¨é¢æ¦è¿°äºç»åå¼é¶æ ·æ¬å­¦ä¹ ï¼CZSLï¼é¢åï¼è¯¥é¢åæ¨å¨ä½¿è®¡ç®æºè§è§æ¨¡åè½å¤è¯å«å·²ç¥å±æ§åå¯¹è±¡çæ°é¢ç»åï¼èæ ééå¯¹æ¯ç§ç»åè¿è¡æ¾å¼è®­ç»ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
CZSLçæ ¸å¿ææå¨äºï¼æ¨¡åéè¦å¨æ¨çæ¶è¯å«è®­ç»ä¸­æªæ¾è§è¿çå±æ§ä¸å¯¹è±¡çç»åãè¿å°¤å¶å°é¾ï¼å ä¸ºåºåï¼å¦å±æ§åå¯¹è±¡ï¼çè§è§å¤è§å·æé«åº¦è¯­å¢ä¾èµæ§ï¼ä¾å¦ï¼âå°âç«ä¸âèâç«çè§è§å·®å¼ï¼ä»¥åâæ¹¿âè½¦ä¸âæ¹¿âç«çæ¾èä¸åï¼ãææå°å»ºæ¨¡è¿ç§è¯­å¢æ§ååºæçç»åæ§å¯¹äºé²æ£çç»åå¼é¶æ ·æ¬è¯å«è³å³éè¦ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæé¦æ¬¡æåºäºä¸ä¸ªå¨é¢çCZSLæ¹æ³åç±»æ³ï¼å¶æ ¸å¿ææ³æ¯âè§£è¦âï¼disentanglementï¼ãåç±»æ³å°ç°ææ¹æ³åä¸ºåå¤§ç±»ï¼
*   <strong>æ æ¾å¼è§£è¦ï¼No Explicit Disentanglementï¼ï¼</strong> å°å±æ§-å¯¹è±¡ç»åè§ä¸ºåä¸ååï¼éè¿æ´ä½åµå¥æç´æ¥èåæºå¶è¿è¡å»ºæ¨¡ã
*   <strong>ææ¬è§£è¦ï¼Textual Disentanglementï¼ï¼</strong> å¨è¯­è¨ç©ºé´ä¸­åç¦»åºåçè¯­ä¹åµå¥ï¼éè¿ç¬ç«æ¦å¿µè¡¨ç¤ºå®ç°ç³»ç»ç»åã
*   <strong>è§è§è§£è¦ï¼Visual Disentanglementï¼ï¼</strong> å¨è§è§ç¹å¾ç©ºé´ä¸­åç¦»å±æ§åå¯¹è±¡çè§è§ç¹å¾ï¼å°è¿äºåºåè§£è¦ä¸ºå¯ç»åçè¡¨ç¤ºã
*   <strong>è·¨æ¨¡æï¼æ··åï¼è§£è¦ï¼Cross-Modal (Hybrid) Disentanglementï¼ï¼</strong> åæ¶å¨è§è§åææ¬ç©ºé´ä¸­è§£è¦åºåï¼å¹¶éè¿è·¨æ¨¡æå¯¹é½æ´åäºè¡¥ä¿¡æ¯ã</p>
<p>å¨ç¬¬äºå±ï¼æ¹æ³æ ¹æ®å¶å»ºæ¨¡å±æ§åå¤çç»åææçç­ç¥è¿ä¸æ­¥åç±»ï¼åæ¬åºäºååå»ºæ¨¡ãåæåµå¥ãå ææ¨çåçº¦æé©±å¨ç­ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>éª¨å¹²ç½ç»æåºï¼</strong> æ©æä½¿ç¨ResNetç¼ç å¨çæ¹æ³æ§è½è¾ä½ï¼èåºäºCLIPç¼ç å¨ï¼ä»2023å¹´å¼å§ï¼çææ°æ¹æ³å¨åç¡®æ§ä¸æ¾èæé«ï¼è¡¨æè§è§-è¯­è¨é¢è®­ç»ä½ä¸ºæ åéª¨å¹²ç½ç»çä¼å¿ã
*   <strong>è§£è¦ç­ç¥çæææ§ï¼</strong>
    *   <strong>æ æ¾å¼è§£è¦</strong> æ¹æ³éå¸¸å¤äºæ§è½è°±çä½ç«¯ï¼å¸æ¾äºå°ç»åè§ä¸ºæ´ä½çå±éæ§ã
    *   <strong>ææ¬è§£è¦</strong> æ¹æ³æ¯æ è§£è¦åºçº¿æææ¹è¿ï¼ä½åéäºä»ä¾èµè¯­è¨ï¼æ æ³ææè§è§å±æ§çåå¼æ§ã
    *   <strong>è§è§è§£è¦</strong> æ¯ææ´»è·ä¸æå·ç«äºåçæ¹æ³ç±»å«ï¼å¨é­ç¯è®¾ç½®ä¸­è¡¨ç°åºæ¾èä¼å¿ï¼çè³å¨æäºæåµä¸è¶è¶äºæ··åæ¹æ³ã
    *   <strong>è·¨æ¨¡æè§£è¦</strong> è½ç¶èµ·æ­¥è¾æï¼ä¸»è¦å¨2024å¹´åºç°ï¼ï¼ä½å¨é­ç¯è®¾ç½®ä¸­æ¾ç¤ºåºæå¼ºçæ½åï¼å¶ç¼ç å¨å¢å¼ºæ¹æ³ï¼å¦CAILAåTroikaï¼å¨å¤æ ·åæ°æ®éä¸å·²è½ä¸è§è§åºçº¿å¹æã</p>
<ul>
<li><strong>å¼æ¾ä¸çè®¾ç½®ï¼</strong> å¨å¼æ¾ä¸çè®¾ç½®ä¸­ï¼æ§è½è¶å¿ä¸é­ç¯è®¾ç½®ææä¸åãæ è§£è¦æ¹æ³ä»ç¶æ¯æå¼±çï¼ä½åå§çº§å«ç­ç¥ï¼å¦KG-SPï¼ææ¶è½ä¸è¯­å¢æç¥æ¨¡åå¹æãææ¬è§£è¦ç»§ç»­æä¾å¢çãè§è§è§£è¦ä»ç¶æ¯æææçæ¹æ³ï¼å¶ä¸­å¢å¼ºåï¼Retrievalï¼ãååä¸­å¿åï¼CLUSPROï¼åæ¡ä»¶å±æ§æ¨¡åï¼CDS-CZSLï¼è¡¨ç°å¼ºå²ãè·¨æ¨¡ææ¹æ³å¨å¼æ¾ä¸çè¯ä¼°ä¸­è½åäºé¡¶çº§è§è§æ¹æ³ï¼è¡¨æå¨æ©å±çæ ç­¾ç©ºé´ä¸ï¼ä»è§è§åºç¡å¯è½æ¯èåè§è§-ææ¬å¯¹é½æ´å¯é ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æ æ¾å¼è§£è¦ï¼</strong> æ æ³ææå±æ§åå¯¹è±¡çç¬ç¹è¯­ä¹æå¶è¯­å¢åå¼æ§ï¼é¾ä»¥æ³åå°æ°é¢ç»åã
*   <strong>ææ¬è§£è¦ï¼</strong> æ æ³ææå±æ§ä¸°å¯çè§è§åå¼æ§ï¼å¿½ç¥äºå¾åä¸­å®éå­å¨ççº ç¼ ã
*   <strong>è§è§è§£è¦ï¼</strong> å¼ºå¶ä¸¥æ ¼åç¦»å¯è½è¿åº¦ç®åèªç¶ä¾èµå³ç³»ï¼ä¸¢å¼æå©äºè¯å«çè¯­å¢çº¿ç´¢ãå®ç°å¹²ååç¦»å¨å®è·µä¸­ä¹å¾å°é¾ï¼ç¹å«æ¯å¯¹äºå¾®å¦æå¼ºè¯­å¢ä¾èµçå±æ§ã
*   <strong>è·¨æ¨¡æè§£è¦ï¼</strong> å­å¨æ¶æåè®¡ç®å¤ææ§ï¼å¯¹é½ä¸¤ç§æ¨¡æéå¸¸éè¦é¢å¤çæ¨¡åãä¸è´çè·¨æ¨¡æåºç¡ä»ç¶é¾ä»¥å®ç°ï¼å ä¸ºè§è§åºåå¨å¤è§ä¸å·®å¼å¾å¤§ï¼èææ¬åºåç¸å¯¹ç¨³å®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å»ºæ¨¡åºååè¯­å¢æ§ï¼</strong> è¿ä¸æ­¥å®åè§è§è§£è¦ç­ç¥ï¼å¹¶æ´å éè§å¼åè·¨æ¨¡ææ¡æ¶ï¼ä»¥å¯æ©å±åé²æ£çæ¹å¼ææè¯­å¢æ§ã
*   <strong>æ©å±å°å¼æ¾ä¸çè¯ä¼°ï¼</strong> å¼ååå¨é²æ£äºä¸å¯è¡ç»åçæ¨¡åï¼ç¼©å°é­ç¯åå¼æ¾ä¸çæ§è½ä¹é´çå·®è·ï¼èæ éæ¾å¼å¯è¡æ§è®¡ç®ã
*   <strong>æ³åå°æªè§åºåï¼</strong> è®¾è®¡è½å¤å¨æéåºæªè§å¯¹è±¡å±æ§è¡¨ç¤ºçæ¨¡åï¼å¹¶æ¢ç´¢è·¨æ¨¡æè§£è¦ä»¥å©ç¨è¯­è¨æ¨¡åçè¯­ä¹æ©å±è½åã
*   <strong>å©ç¨å¤§åå¤æ¨¡ææ¨¡åï¼LMMsï¼ï¼</strong> å»ºç«ä¸¥æ ¼çè¯ä¼°åè®®ä»¥è§£å³æ°æ®æ±¡æé®é¢ï¼å¹¶å¼åå©ç¨LMMsæ¨çè½ååæ¶çº¦æå¶å­¦ä¹ çå®ç»åç»æèéè¡¨é¢ç¸å³æ§çéåºç­ç¥ã</p>
<p>æ»ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºCZSLé¢åæä¾äºä¸ä¸ªç»æåçåææ¡æ¶ï¼ç³»ç»å°åé¡¾äºç°ææ¹æ³ï¼å¹¶æåºäºæªæ¥ç ç©¶çå³é®æ¹åï¼æ¨å¨æ¨å¨è¯¥é¢åå®ç°æ´é²æ£ãå¯æ©å±åéæçç»åå¼è¯å«ç³»ç»ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11106v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11106v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11063v1'></a></p>
<h2 id="lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation"><a href="https://arxiv.org/abs/2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></h2>
<p><strong>Authors:</strong> Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard <script type="math/tex">{J}</script>, <script type="math/tex">F</script>, and <script type="math/tex">{J\&F}</script> metrics for
VOS and RVOS, while MOSEv2 adopts <script type="math/tex">{J\&\dot{F}}</script> as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâLSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentationâçæè¦ï¼ç±Chang Liuç­æ°åï¼åå®¹åºäºæ¨æä¾çPDFå¨æï¼</p>
<p><strong>è®ºææè¦ï¼LSVOS 2025 ææèµæ¥åï¼å¤æè§é¢ç®æ åå²çææ°è¿å±</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥æ¥åæ¨å¨æ¦è¿°ç¬¬ä¸å±å¤§è§æ¨¡è§é¢ç®æ åå²ï¼LSVOSï¼ææèµï¼è¯¥ææèµä¸ICCV 2025åæä¸¾è¡ãæ ¸å¿é®é¢æ¯æ¨å¨è§é¢ç®æ åå²ï¼VOSï¼åæä»£è§é¢ç®æ åå²ï¼RVOSï¼å¨æ´å¤æãæ´çå®çè§é¢åºæ¯ä¸­å®ç°é²æ£æ§ãé¿æä¸è´æ§åæ³åè½åï¼è¶è¶ç°æç­å±åºåçå±éãç¹å«æ¯ï¼æ°å¼å¥çå¤æVOS (MOSEv2) èµéæ¨å¨è§£å³ç°æVOSæ¹æ³å¨å¤çå¯éå°ç©ä½ãé¢ç¹åºç°/æ¶å¤±äºä»¶ãä¸¥éé®æ¡ãæ¶å£å¤©æ°ååç§ç­æç«¯çå®ä¸çæ¡ä»¶ä¸çä¸è¶³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¼å¥MOSEv2æ°æ®éåèµéï¼</strong> MOSEv2æ¯MOSEv1çç»§ä»»èï¼æ¾èå¢å äºé¾åº¦ï¼åå«æ´å¤æææ§åºæ¯ï¼å¦å¯éå°ç©ä½ãé¢ç¹åºç°/æ¶å¤±ãä¸¥éé®æ¡ãæ¶å£å¤©æ°ååç§ç­ï¼æ¨å¨æ¨å¨é¿æä¸è´æ§åæ³åè½åã
*   <strong>MOSEv2çæ°è¯ä¼°ææ ï¼</strong> ä¸ºäºæ´åç¡®å°è¯ä¼°è·¨å¯¹è±¡å°ºåº¦åæ¶å¤±æåµä¸çæ§è½ï¼MOSEv2å¼å¥äºæ°çææ ï¼åæ¬èªéåºè¾¹çç²¾åº¦ï¼<script type="math/tex">\dot{F}</script>ï¼åä»¥<script type="math/tex">\text{J\&}\dot{F}</script>ä½ä¸ºä¸»è¦æåææ ã
*   <strong>é¡¶å°è§£å³æ¹æ¡çææ¯è¶å¿ï¼</strong>
    *   <strong>MOSEv2èµéï¼</strong> é¢åçè§£å³æ¹æ¡ï¼å¦DSS-TrackçSeCæ¡æ¶ï¼å©ç¨SAM-2ï¼Segment Anything Model 2ï¼åInternVL-2.5-4Bç­åºç¡æ¨¡åï¼éè¿å¢å¼ºçæ¦å¿µå»ºæ¨¡ãæ´å¤§çè®°å¿å°ºå¯¸ï¼N=22ï¼æ¥æè·é¿æè·¨å¸§å³ç³»ï¼å¹¶ç»åæ¦å¿µæç¥è®°å¿ï¼Concept-aware Memoryï¼ååºæ¯èªéåºæ¿æ´»ç­ç¥æ¥å¤çå¤ææ¶ç©ºåºæ¯ã
    *   <strong>VOSèµéï¼</strong> é¡¶å°æ¹æ³ï¼å¦NJUST-KMGï¼åºäºSAM2æ¨¡åï¼éè¿ç½®ä¿¡åº¦å¼å¯¼çå¤æ¨¡åéæç­ç¥ï¼ç»åSAM2LongãCutieãLiVOSãXMemç­ï¼æ¥å¢å¼ºé²æ£æ§ï¼å¹¶éç¨å¤ä»»å¡æå¤±å½æ°è¿è¡è®­ç»ï¼ä»¥åºå¯¹å¤æåºæ¯ã
    *   <strong>RVOSèµéï¼</strong> é¢åæ¹æ³ï¼å¦SaSaSa2VAï¼ç»åå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼åSAM2ï¼éè¿å³é®å¸§åç¼©ï¼KFCï¼åæ©å±[SEG] tokenæ°éæ¥å¤çè§é¢åææ¬æä»¤ï¼ä»¥æè·å¨å±è§é¢ä¸ä¸æåå¤çå¤æ ·åçæ¶é´ååãè¿å¼å¥äºè§é¢è¯­ä¹å¹éæ¨¡åï¼VLCï¼ä»¥éªè¯è§é¢-ææ¬å¯¹åºå³ç³»ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>MOSEv2çæææ§ï¼</strong> MOSEv2èµéçç»æè¡¨æï¼å³ä½¿æ¯é¡¶å°æ¹æ³ï¼å¶<script type="math/tex">\text{J\&}\dot{F}</script>åæ°ä¹ç¸å¯¹è¾ä½ï¼ç¬¬ä¸åDSS-Trackä¸º39.89%ï¼ï¼è¿çªåºæ¾ç¤ºäºç°ä»£VOSæ¹æ³å¨å¤æçå®åºæ¯ä¸­ä»ææ¾èæåç©ºé´ã
*   <strong>LLM/MLLMç»ä»¶çæ¥çéè¦æ§ï¼</strong> æ¥åå¼ºè°ï¼å¤§è¯­è¨æ¨¡åï¼LLMï¼åå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼å·²æä¸ºè®¸å¤ç®¡éä¸­çé»è®¤ç»ä»¶ï¼å°¤å¶æ¯å¨è¯­è¨å¼å¯¼çè§é¢ä»»å¡ä¸­ï¼è¿å¸æ¾äºå®ä»¬å¨è§é¢çè§£æ¹é¢çæ½åã
*   <strong>è®°å¿æç¥ä¼ æ­çéè¦æ§ï¼</strong> é¡¶å°è§£å³æ¹æ¡æ®ééç¨äºå¢å¼ºçè®°å¿æºå¶ï¼æ è®ºæ¯ç¨äºæè·é¿æè·¨å¸§å³ç³»ï¼è¿æ¯ç¨äºå¤çå¯¹è±¡åºç°/æ¶å¤±ï¼é½å¼ºè°äºè®°å¿ç®¡çå¨è§é¢åå²ä¸­çå³é®ä½ç¨ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>MOSEv2çæææ§ï¼</strong> å°½ç®¡åå¾äºè¿å±ï¼ä½MOSEv2èµéçç»æè¡¨æï¼ç°ææåè¿ç³»ç»å¨å¤æãçå®çåºæ¯ä¸­ä»ç¶é¢ä¸´ææï¼å¯¼è´æ§è½æ¾èä¸éã
*   <strong>Sa2VAçå±éæ§ï¼</strong> å¨RVOSèµéä¸­ï¼Sa2VAæ¨¡åå¨è®­ç»æ¶ä»éæ ·5å¸§ï¼å¹¶ä¸ä½¿ç¨åä¸ª[SEG] tokenæ¥ä¼ éä¿¡æ¯ï¼è¿éå¶äºMLLMæè·å¨å±è§é¢ä¸ä¸æçè½åï¼å¹¶ä¸é¾ä»¥éåºå¯¹è±¡ä½ç½®ãå½¢ç¶åå¤è§çé¢ç¹ååã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´æ·±å±æ¬¡çLLM/MLLMéæï¼</strong> æ¥åé¢æµï¼LLM/MLLMçæ´æ·±å±æ¬¡éæå°ç»§ç»­æåæ§è½ï¼å°¤å¶æ¯å¨è¯­è¨æç¥è§é¢åå²æ¹é¢ã
*   <strong>è§£å³æå°é¾çå¤±è´¥æ¨¡å¼ï¼</strong> æªæ¥çç ç©¶å°éç¹å³æ³¨éè¿æ¬æ¬¡ææèµç»æåçå®ä¸çç¨ä¾è¯å«åºçæå°é¾çå¤±è´¥æ¨¡å¼ï¼ä»¥è¿ä¸æ­¥æ¨å¨è§é¢ç®æ åå²åç¸å³ç ç©¶çåæ²¿ã</p>
<p>æ»èè¨ä¹ï¼LSVOS 2025ææèµæ¥åä¸ä»å±ç¤ºäºè§é¢ç®æ åå²é¢åçææ°è¿å±ï¼è¿éè¿å¼å¥MOSEv2æ°æ®éåæ°çè¯ä¼°ææ ï¼ä»¥åé¡¶å°è§£å³æ¹æ¡ä¸­æ¶ç°åºçLLM/MLLMåè®°å¿æç¥ä¼ æ­ç­è¶å¿ï¼ä¸ºè¯¥é¢åçæªæ¥ç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11063v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11063v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.10991v1'></a></p>
<h2 id="a-survey-on-agentic-multimodal-large-language-models"><a href="https://arxiv.org/abs/2510.10991v1">A Survey on Agentic Multimodal Large Language Models</a></h2>
<p><strong>Authors:</strong> Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, Dacheng Tao</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>With the recent emergence of revolutionary autonomous agentic systems,
research community is witnessing a significant shift from traditional static,
passive, and domain-specific AI agents toward more dynamic, proactive, and
generalizable agentic AI. Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we
explore the emerging paradigm of agentic MLLMs, delineating their conceptual
foundations and distinguishing characteristics from conventional MLLM-based
agents. We establish a conceptual framework that organizes agentic MLLMs along
three fundamental dimensions: (i) Agentic internal intelligence functions as
the system's commander, enabling accurate long-horizon planning through
reasoning, reflection, and memory; (ii) Agentic external tool invocation,
whereby models proactively use various external tools to extend their
problem-solving capabilities beyond their intrinsic knowledge; and (iii)
Agentic environment interaction further situates models within virtual or
physical environments, allowing them to take actions, adapt strategies, and
sustain goal-directed behavior in dynamic real-world scenarios. To further
accelerate research in this area for the community, we compile open-source
training frameworks, training and evaluation datasets for developing agentic
MLLMs. Finally, we review the downstream applications of agentic MLLMs and
outline future research directions for this rapidly evolving field. To
continuously track developments in this rapidly evolving field, we will also
actively update a public repository at
https://github.com/HJYao00/Awesome-Agentic-MLLMs.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Huanjin Yaoç­äººæ°åçè®ºæâA Survey on Agentic Multimodal Large Language Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="a-survey-on-agentic-multimodal-large-language-models_1">è®ºæãA Survey on Agentic Multimodal Large Language Modelsãå¨é¢æè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åäººå·¥æºè½é¢åä»ä¼ ç»éæãè¢«å¨ãé¢åä¸ç¨åAIæºè½ä½åæ´å¨æãä¸»å¨ãéç¨åAgentic AIæºè½ä½è½¬åçè¶å¿ãå·ä½æ¥è¯´ï¼å®å³æ³¨Agenticå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼Agentic MLLMsï¼è¿ä¸æ°å´èå¼ï¼å¹¶è¯å¾æä¾ä¸ä¸ªå¨é¢çç»¼è¿°ï¼ä»¥éæå¶æ¦å¿µåºç¡ãä¸ä¼ ç»MLLMæºè½ä½çåºå«ãå³é®è½åãè®­ç»ä¸è¯ä¼°èµæºï¼ä»¥åæªæ¥çç ç©¶æ¹åãæ ¸å¿é®é¢æ¯ï¼å¦ä½ç³»ç»å°çè§£åæ¨å¨Agentic MLLMsçåå±ï¼ä½¿å¶è½å¤èªä¸»å°è¿è¡é¿å¨æè§åãä¸»å¨ä½¿ç¨å·¥å·å¹¶ä¸å¨æç¯å¢äº¤äºï¼ä»èå®ç°æ´æ¥è¿éç¨äººå·¥æºè½ï¼AGIï¼çç®æ ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçä¸»è¦è´¡ç®å¨äºæå»ºäºä¸ä¸ªå¨é¢çæ¦å¿µæ¡æ¶ï¼å°Agentic MLLMsçè½åç»ç»æä¸ä¸ªåºæ¬ç»´åº¦ï¼</p>
<ul>
<li><strong>Agentic åé¨æºè½ï¼Agentic Internal Intelligenceï¼ï¼</strong> ä½ä¸ºç³»ç»çâææ¥å®âï¼éè¿æ¨çãåæåè®°å¿å®ç°åç¡®çé¿å¨æè§åãè®ºæè¯¦ç»æ¢è®¨äºåºäºæç¤ºãSFTåRLçæ¨çæ¹æ³ï¼ä»¥åæ¾å¼åéå¼åææºå¶ï¼å¹¶åºåäºä¸ä¸æè®°å¿åå¤é¨è®°å¿ç³»ç»ï¼åæ¬å¯åå¼é©±å¨åæ¨çé©±å¨ï¼ã</li>
<li><strong>Agentic å¤é¨å·¥å·è°ç¨ï¼Agentic External Tool Invocationï¼ï¼</strong> æ¨¡åä¸»å¨å©ç¨åç§å¤é¨å·¥å·ï¼å¦ä¿¡æ¯æç´¢ãä»£ç æ§è¡ãè§è§å¤çï¼æ¥æ©å±å¶è¶è¶åå¨ç¥è¯çé®é¢è§£å³è½åãè®ºæåç±»è®¨è®ºäºAgenticæç´¢ãAgenticç¼ç åAgenticè§è§å¤çï¼åæ¬å¾åè£åªãå¾åæä½åå¾åçæï¼ç­å·¥å·çä½¿ç¨ã</li>
<li><strong>Agentic ç¯å¢äº¤äºï¼Agentic Environment Interactionï¼ï¼</strong> æ¨¡åè½å¤ä¸èææç©çç¯å¢è¿è¡äº¤äºï¼ä»èéåè¡å¨ãè°æ´ç­ç¥å¹¶å¨å¨æçå®ä¸çåºæ¯ä¸­ç»´æç®æ å¯¼åçè¡ä¸ºãè¿åæ¬èæGUIæºè½ä½ï¼éè¿ç¦»çº¿æ¼ç¤ºæå¨çº¿äº¤äºå­¦ä¹ ï¼åç©çå·èº«AIï¼æ¶åå·èº«æç¥ãè§åãå¯¼èªåæä½ï¼ã</li>
</ul>
<p>æ­¤å¤ï¼è®ºæè¿ï¼
*   <strong>å½¢å¼åäºAgentic MLLMsä¸ä¼ ç»MLLMæºè½ä½çåºå«ï¼</strong> å¼ºè°äºAgentic MLLMsçå¨æå·¥ä½æµãä¸»å¨è¡å¨æ§è¡åè·¨é¢åæ³åè½åã
*   <strong>ç³»ç»æ¢³çäºè®­ç»ä¸è¯ä¼°èµæºï¼</strong> æ´çäºå¼æºè®­ç»æ¡æ¶ï¼åæ¬Agentic CPT/SFTåRLæ¡æ¶ï¼ä»¥åç¨äºå¼ååè¯ä¼°Agentic MLLMsçè®­ç»åè¯ä¼°æ°æ®éã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥è®ºææ¬èº«æ¯ä¸ç¯ç»¼è¿°ï¼å æ­¤å¶âç»æâä½ç°å¨å¯¹ç°æç ç©¶çå¨é¢æ¢³çåæ´å¯ä¸ï¼èéå®éªç»æãä¸»è¦æä¹åæ¬ï¼</p>
<ul>
<li><strong>æä¾ç»ä¸çè§è§ï¼</strong> é¦æ¬¡å¨é¢å°å¯¹Agentic MLLMsé¢åè¿è¡äºç³»ç»æ§åç±»åæ»ç»ï¼ä¸ºç ç©¶äººåæä¾äºä¸ä¸ªæ¸æ°ãå¨é¢çé¢åæ¦è§åæ¦å¿µæ¡æ¶ã</li>
<li><strong>æ­ç¤ºåå±è¶å¿ï¼</strong> å¼ºè°äºAgentic MLLMså¨èªä¸»å³ç­ãä¸»å¨è§åãå·¥å·ä½¿ç¨åç¯å¢äº¤äºæ¹é¢çä¼å¿ï¼é¢ç¤ºäºAIæºè½ä½åæ´éç¨ãæ´æºè½æ¹ååå±çè¶å¿ã</li>
<li><strong>ä¿è¿ç¤¾åºåå±ï¼</strong> æ±ç¼äºå¤§éçå¼æºèµæºï¼è®­ç»æ¡æ¶ãæ°æ®éï¼ï¼ä¸ºç ç©¶äººåè¿å¥åå éè¯¥é¢åçç ç©¶æä¾äºå®è´µçèµ·ç¹ã</li>
<li><strong>çªåºåºç¨æ½åï¼</strong> å±ç¤ºäºAgentic MLLMså¨æ·±åº¦ç ç©¶ãå·èº«AIãå»çä¿å¥ãGUIæºè½ä½ãèªå¨é©¾é©¶åæ¨èç³»ç»ç­å¹¿æ³ä¸æ¸¸åºç¨ä¸­çå·¨å¤§æ½åï¼è¡¨æå¶è½å¤å¤çå¤æççå®ä¸çåºæ¯ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æç¡®æåºäºAgentic MLLMså½åé¢ä¸´çææåå±éæ§ï¼</p>
<ul>
<li><strong>è¡å¨ç©ºé´åéï¼</strong> ç°ææ¨¡åçè¡å¨ç©ºé´åå¯è®¿é®å·¥å·èå´éå¸¸åéäºåä¸ç±»åï¼ç¼ºä¹æ´å¹¿æ³çå¤é¨å·¥å·åæå¡éæã</li>
<li><strong>æçé®é¢ï¼</strong> å¤è½®æ¨çåå¤é¨å·¥å·è°ç¨ç­è¿­ä»£è¿ç¨æ¾èå¢å äºè®¡ç®åæ¨çå¼éï¼å¯¼è´è®­ç»åæ¨çæçä½ä¸ï¼é¾ä»¥æ»¡è¶³å®æ¶åºç¨åå¤§è§æ¨¡é¨ç½²çéæ±ã</li>
<li><strong>é¿å¨æè®°å¿çå±éæ§ï¼</strong> å½åç³»ç»çè®°å¿ææé¿åº¦é«åº¦åéï¼é¾ä»¥å¨æ´é¿çæ¶é´è·¨åº¦åç»´æè¿è´¯çç¥è¯ï¼ä¸å¤æ¨¡æè®°å¿ç®¡çæ¹é¢çç ç©¶ä¸è¶³ã</li>
<li><strong>è®­ç»ä¸è¯ä¼°æ°æ®ç¨ç¼ºï¼</strong> ä¸é¨ä¸ºAgenticè¡ä¸ºè®¾è®¡çè®­ç»æ°æ®éä»ç¶ç¨ç¼ºï¼å°¤å¶æ¯å¨å¤æ¨¡æé¢åç¼ºä¹è¶³å¤çæ¢ç´¢ãç°æè¯ä¼°åºåä¸»è¦å³æ³¨Agenticè¡ä¸ºçç¹å®æ¹é¢ï¼èè®°å¿å©ç¨ãè·¨å·¥å·è°ç¨åè°ç­è½åä»ç¼ºä¹ææçè¯ä¼°æ°æ®éã</li>
<li><strong>å®å¨é®é¢ï¼</strong> éçAgentic MLLMså¨è§åãå·¥å·è°ç¨åç¯å¢äº¤äºæ¹é¢æ¥çèªä¸»ï¼ç¡®ä¿å¶å®å¨æ§ï¼å¦é¿åæå¤åæãå¤çä¸æ­£ç¡®ææå®³ä¿¡æ¯ãé²æ­¢è¡ä¸ºä¸ç¨³å®ï¼æä¸ºä¸ä¸ªå³é®çç ç©¶ä¼åäºé¡¹ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ´ä¸°å¯çè¡å¨ç©ºé´ï¼</strong> å¼åè½å¤æ ç¼éææ´å¤å¤é¨å·¥å·åæå¡ï¼å¦æ°æ®åæå¹³å°ãä»¿çç¯å¢ãå¤æ¨¡æä¼ æå¨ãäº¤äºå¼APIï¼çAgentic MLLMsã</li>
<li><strong>æé«æçï¼</strong> ä¸æ³¨äºæåAgentic MLLMsçè®¡ç®æçï¼å éè®­ç»åæ¨çè¿ç¨ï¼åæ¶ä¸çºç²æ§è½ï¼ä»¥æ¯æå®æ¶åºç¨åå¤§è§æ¨¡é¨ç½²ã</li>
<li><strong>é¿å¨æAgenticè®°å¿ï¼</strong> è®¾è®¡æä¹æ§è®°å¿æ¶æï¼ä½¿æ¨¡åè½å¤è·¨é¿æ¶é´è·¨åº¦ç§¯ç´¯ãç»ç»åæ£ç´¢ç¥è¯ï¼å¹¶è½å¤å¤çæµ·éçå¤æ¨¡ææ°æ®æµï¼å®ç°ä¸ªæ§åãæç»­åä½åèªéåºé®é¢è§£å³ã</li>
<li><strong>Agenticè®­ç»ä¸è¯ä¼°æ°æ®éï¼</strong> å¼åææä¸é«æçæ¹æ³æ¥åæé«è´¨éçå¤æ¨¡æAgenticè½¨è¿¹æ°æ®ãåæ¶ï¼æå»ºæ´å¨é¢çè¯ä¼°åºåï¼ä»¥è¯ä¼°è®°å¿å©ç¨ãè·¨å·¥å·è°ç¨åè°ä»¥åè¡å¨æ§è¡çåç¡®æ§ç­å³é®Agenticè½åã</li>
<li><strong>Agentic MLLMsçå®å¨æ§ï¼</strong> ç»åä¸¥æ ¼çåºåæµè¯ãå¯¹ææ§ååæµè¯åè§èæ¡æ¶çéæï¼ç¡®ä¿Agentic MLLMså¨æ´å¹¿æ³çèªä¸»æ§æ¹é¢ä¿æå¯é ãå¯æ§å¹¶ä¸äººç±»æå¾å¯¹é½ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.10991v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.10991v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.10671v1'></a></p>
<h2 id="image-to-video-transfer-learning-based-on-image-language-foundation-models-a-comprehensive-survey"><a href="https://arxiv.org/abs/2510.10671v1">Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</a></h2>
<p><strong>Authors:</strong> Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai</p>
<p><strong>Published:</strong> 2025-10-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Image-Language Foundation Models (ILFM) have demonstrated remarkable success
in image-text understanding/generation tasks, providing transferable multimodal
representations that generalize across diverse downstream image-based tasks.
The advancement of video-text research has spurred growing interest in
extending image-based models to the video domain. This paradigm, known as
image-to-video transfer learning, succeeds in alleviating the substantial data
and computational requirements associated with training video-language
foundation models from scratch for video-text learning. This survey provides
the first comprehensive review of this emerging field, which begins by
summarizing the widely used ILFM and their capabilities. We then systematically
classify existing image-to-video transfer learning strategies into two
categories: frozen features and modified features, depending on whether the
original representations from ILFM are preserved or undergo modifications.
Building upon the task-specific nature of image-to-video transfer, this survey
methodically elaborates these strategies and details their applications across
a spectrum of video-text learning tasks, ranging from fine-grained (e.g.,
spatio-temporal video grounding) to coarse-grained (e.g., video question
answering). We further present a detailed experimental analysis to investigate
the efficacy of different image-to-video transfer learning paradigms on a range
of downstream video understanding tasks. Finally, we identify prevailing
challenges and highlight promising directions for future research. By offering
a comprehensive and structured overview, this survey aims to establish a
structured roadmap for advancing video-text learning based on existing ILFM,
and to inspire future research directions in this rapidly evolving domain.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºJinxuan Liç­äººæ°åçè®ºæâImage-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Surveyâçå¨é¢æè¦ï¼åå®¹åºäºæ¨æä¾çæè¦åPDFå¨æï¼</p>
<p><strong>è®ºæå¨é¢æè¦ï¼Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ (Main Problem or Research Question Addressed)</strong></p>
<p>è¯¥è®ºæä¸»è¦è§£å³çé®é¢æ¯ï¼å¦ä½ææå°å°å¾å-è¯­è¨åºç¡æ¨¡åï¼Image-Language Foundation Models, ILFMï¼çå¼ºå¤§è½åï¼è¿ç§»å°è§é¢çè§£ä»»å¡ä¸­ï¼ä»¥åºå¯¹è§é¢æ°æ®ç¹æçæ¶åºç»´åº¦ææï¼å¹¶ç¼è§£ä»å¤´è®­ç»è§é¢-è¯­è¨åºç¡æ¨¡åæéçå¤§éæ°æ®åè®¡ç®èµæºãå·ä½æ¥è¯´ï¼å®æ¨å¨ä¸ºåºäºç°æILFMçè§é¢-ææ¬å­¦ä¹ å»ºç«ä¸ä¸ªç»æåçè·¯çº¿å¾ï¼å¹¶æ¿åè¯¥é¢åæªæ¥çç ç©¶æ¹åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç® (Key Innovations or Methodological Contributions)</strong></p>
<p>è¯¥è®ºæçä¸»è¦è´¡ç®å¨äºå¯¹å¾åå°è§é¢è¿ç§»å­¦ä¹ è¿ä¸æ°å´é¢åè¿è¡äºé¦æ¬¡å¨é¢ä¸ç»æåçç»¼è¿°ãå¶å³é®åæ°åæ¹æ³è®ºè´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>ç³»ç»æ§åç±»è¿ç§»ç­ç¥ï¼</strong> è®ºæå°ç°æçå¾åå°è§é¢è¿ç§»å­¦ä¹ ç­ç¥ç³»ç»å°åä¸ºä¸¤å¤§ç±»ï¼<ul>
<li><strong>å»ç»ç¹å¾ï¼Frozen Featuresï¼ï¼</strong> ä¿æILFMçåå§è¡¨ç¤ºä¸åï¼éè¿ç¥è¯è¸é¦ãåç½ç»å¾®è°ï¼Post-Network Tuningï¼åä¾§è°ï¼Side-Tuningï¼ç­æ¹æ³è¿è¡è¿ç§»ã</li>
<li><strong>ä¿®æ¹ç¹å¾ï¼Modified Featuresï¼ï¼</strong> å¯¹ILFMçåå§è¡¨ç¤ºè¿è¡ä¿®æ¹ï¼åæ¬å¨å¾®è°ï¼Full Fine-Tuningï¼ãé¨åå¾®è°ï¼Partial Tuningï¼ãä½¿ç¨é¢å¤æ¨¡åå¾®è°ï¼Fine-Tuning with Extra Modelsï¼ãåºäºééå¨å¾®è°ï¼Fine-Tuning with Adapterï¼ãLoRAå¾®è°ï¼Fine-Tuning with LoRAï¼åæç¤ºå¾®è°ï¼Prompt Tuningï¼ã</li>
</ul>
</li>
<li><strong>ä»»å¡ç¹å®åºç¨éè¿°ï¼</strong> è®ºææ ¹æ®ä»»å¡çç²åº¦ï¼ä»ç»ç²åº¦å°ç²ç²åº¦ï¼è¯¦ç»éè¿°äºè¿äºè¿ç§»ç­ç¥å¨åç§è§é¢-ææ¬å­¦ä¹ ä»»å¡ä¸­çåºç¨ï¼ä¾å¦ï¼<ul>
<li><strong>ç»ç²åº¦ä»»å¡ï¼</strong> æ¶ç©ºè§é¢å®ä½ï¼Spatio-Temporal Video Grounding, STVGï¼ãå¼æ¾è¯æ±å¤ç®æ è·è¸ªï¼Open Vocabulary Multi-Object Tracking, OV-MOTï¼ãè§é¢å®ä¾åå²ï¼Video Instance Segmentation, OV-VISï¼ç­ï¼è¿äºä»»å¡éè¦ç²¾ç¡®çç©ºé´åºååæ¶é´é´éå®ä½ã</li>
<li><strong>ç²ç²åº¦ä»»å¡ï¼</strong> è§é¢-ææ¬æ£ç´¢ï¼Video-Text Retrieval, VTRï¼ãè§é¢å¨ä½è¯å«ï¼Video Action Recognition, VARï¼ãè§é¢é®ç­ï¼Video Question Answering, VideoQAï¼ãè§é¢å­å¹çæï¼Video Captioningï¼ç­ï¼è¿äºä»»å¡æ´ä¾§éäºå¯¹è§é¢äºä»¶çæ´ä½çè§£ã</li>
</ul>
</li>
<li><strong>è¯¦ç»çå®éªåæï¼</strong> è®ºææä¾äºè¯¦ç»çå®éªåæï¼éè¿æ¯è¾ä¸åå¾åå°è§é¢è¿ç§»å­¦ä¹ èå¼å¨åç§ä¸æ¸¸è§é¢çè§£ä»»å¡ä¸çææï¼éªè¯äºä¸åç­ç¥çæææ§ãä¾å¦ï¼å¨TVGä»»å¡ä¸­ï¼R2-tuningï¼ä¾§è°ï¼å¨CLIPåºç¡ä¸è¡¨ç°æä½³ï¼èNumProï¼é¢å¤æ¨¡åå¾®è°ï¼å¨LLaVAåºç¡ä¸è¡¨ç°çªåºãå¨VTRä»»å¡ä¸­ï¼LoRAå¾®è°å¨CLIPåºç¡ä¸åå¾äºæä½³æ§è½ãå¨VARä»»å¡ä¸­ï¼ééå¨ç­ç¥è¡¨ç°ä¼äºä¾§è°ãå¨VideoQAä»»å¡ä¸­ï¼VideoDistillï¼ç¥è¯è¸é¦ï¼å¨CLIPåºç¡ä¸è¡¨ç°ä¼å¼ï¼èBIMBAï¼åç½ç»å¾®è°ï¼å¨LLaVAåºç¡ä¸æ¾èæåäºæ§è½ã</li>
<li><strong>ç»æåè·¯çº¿å¾ï¼</strong> éè¿æä¾å¨é¢ä¸ç»æåçæ¦è¿°ï¼è®ºææ¨å¨ä¸ºåºäºç°æILFMçè§é¢-ææ¬å­¦ä¹ å»ºç«ä¸ä¸ªç»æåçè·¯çº¿å¾ï¼å¹¶æ¿åè¯¥é¢åæªæ¥çç ç©¶æ¹åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ (Main Results and Their Significance)</strong></p>
<p>è®ºæçå®éªåææ­ç¤ºäºä»¥ä¸ä¸»è¦ç»æåå¶æä¹ï¼</p>
<ul>
<li><strong>è¿ç§»å­¦ä¹ çæææ§ï¼</strong> å¾åå°è§é¢è¿ç§»å­¦ä¹ èå¼å¨ç¼è§£ä»å¤´è®­ç»è§é¢-è¯­è¨åºç¡æ¨¡åæéçå¤§éæ°æ®åè®¡ç®èµæºæ¹é¢åå¾äºæåã</li>
<li><strong>ç­ç¥ä¸ä»»å¡çå¹éæ§ï¼</strong> æ²¡æåä¸çè¿ç§»ç­ç¥å¨ææä»»å¡ä¸é½è¡¨ç°æä½³ãä¸åä»»å¡å¯¹æ¨¡åè½åæä¸åè¦æ±ï¼å æ­¤éè¦éæ©æåéçè¿ç§»èå¼ãä¾å¦ï¼ç»ç²åº¦ä»»å¡ï¼å¦TVGï¼éå¸¸åçäºè½å¤ç²¾ç¡®å»ºæ¨¡æ¶ç©ºå³ç³»çç­ç¥ï¼èç²ç²åº¦ä»»å¡ï¼å¦VideoQAï¼å¯è½æ´ä¾§éäºå¤æ¨¡æç¥è¯çè§£çé²æ£æ§ã</li>
<li><strong>åºç¡æ¨¡åçéæ©ï¼</strong> ä¸åçILFMï¼å¦CLIPãMDETRãGroundingDINOãBLIPãLLaVAï¼å·æä¸åçä¼å¿ãä¾å¦ï¼MDETRåGroundingDINOå¨ç»ç²åº¦è§è§-ææ¬å¯¹åºæ¹é¢è¡¨ç°æ´å¼ºï¼èCLIPåBLIPæ´æé¿å¤çæ½è±¡æé«çº§è§è§-ææ¬å¯¹åºãLLaVAéè¿å©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMï¼å¼¥åäºè¿ä¸¤ç§æç«¯æåµã</li>
<li><strong>LLMçæ½åï¼</strong> å°å¤§åè¯­è¨æ¨¡åï¼LLMï¼ä½ä¸ºåºç¡æ¨¡åè¿è¡è¿ç§»ï¼å¨æäºä»»å¡ï¼å¦VideoQAï¼ä¸­æ¾ç¤ºåºæ¾èçæ§è½æåï¼è¿è¡¨æLLMå¼ºå¤§çè¯­ä¹æ¨çè½åå¯¹è§é¢çè§£è³å³éè¦ã</li>
<li><strong>åæ°æççéè¦æ§ï¼</strong> ééå¨ãLoRAåæç¤ºå¾®è°ç­åæ°é«æçå¾®è°æ¹æ³å¨èµæºåéçç¯å¢ä¸å·æå¾é«çå®ç¨æ§åå¯æ©å±æ§ï¼åæ¶ä¹è½ä¿æILFMçå¼ºå¤§è½åã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ (Limitations Mentioned in the Paper)</strong></p>
<p>è®ºæä¸­æåçç°ææ¹æ³çå±éæ§åæ¬ï¼</p>
<ul>
<li><strong>å»ç»ç¹å¾çå±éæ§ï¼</strong> å»ç»ç¹å¾æ¹æ³ï¼å¦ç¥è¯è¸é¦ãåç½ç»å¾®è°ãä¾§è°ï¼çæ§è½å¯è½åéäºéæå¾åçº§ç¹å¾åºæçè¡¨ç¤ºè½åï¼ä»¥åå®ä»¬ä¸è§é¢ç¹å®ä»»å¡è¦æ±çä¸å®ç¾å¯¹é½ï¼å°¤å¶æ¯å¨æ³åå°æ°åºæ¯æ¶ãå®ä»¬å¯è½å¼å¥ILFMçéè¯¯ç¥è¯ï¼ä»èéå¶è·¨æ¨¡æè§é¢-ææ¬çè§£çæææ§ã</li>
<li><strong>å¨å¾®è°çè®¡ç®ææ¬ï¼</strong> å¨å¾®è°è½ç¶ç®åææï¼ä½éè¦å¤§è§æ¨¡ãå®ä¹è¯å¥½ä¸æ æ³¨çæ°æ®ï¼ä»¥åå¤§éçè®¡ç®èµæºï¼å¨å®è·µä¸­éç¨æ§è¾ä½ã</li>
<li><strong>é¢å¤æ¨¡åçå¤ææ§ï¼</strong> è½ç¶é¢å¤æ¨¡åå¾®è°å¯ä»¥æææ³¨å¥è§é¢ç¹å®çç»æä¿¡æ¯ï¼ä½ä¹ä¼å¢å è®¡ç®ææ¬ã</li>
<li><strong>LoRAåæç¤ºå¾®è°çå±éæ§ï¼</strong> LoRAæ¬èº«æ æ³å»ºæ¨¡æ¶åºä¿¡æ¯ï¼éè¦ä¾èµå¤é¨æ¶åºå»ºæ¨¡æ¨¡åæç½ç»ãæç¤ºå¾®è°è½ç¶è®¡ç®ææ¬ä½ï¼ä½å¶å¯¹å»ç»éª¨å¹²ç½ç»çä¾èµå¯è½ä¼éå¶ç»ç²åº¦æ¶åºæ¨çè½åã</li>
<li><strong>ç°æè§£å³æ¹æ¡çç¢çåï¼</strong> å½åç ç©¶ä¸­ï¼éå¯¹ä¸åè§é¢ä»»å¡å¾å¾éè¦éæ©ä¸åçåºç¡æ¨¡ååè®¾è®¡ç¬ç¹çå¾®è°ç­ç¥ï¼å¯¼è´è§£å³æ¹æ¡ç¢çåï¼ç¼ºä¹ç»ä¸æ§åéç¨æ§ã</li>
<li><strong>æ¶åºç»´åº¦å»ºæ¨¡çææï¼</strong> è§é¢åå«å¤æçæ¶ç©ºå¨æï¼åæ¬è¿å¨ãäºä»¶è¿ç¨åå æäº¤äºï¼è¿ä½¿å¾ä»å¤çéæç©ºé´ä¿¡æ¯å°ææå»ºæ¨¡è¿äºå¤ææ¶ç©ºå³ç³»éè¦æ ¹æ¬æ§çæ¶æèå¼è½¬åã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å (Potential Future Research Directions)</strong></p>
<p>è®ºææåºäºä»¥ä¸å ä¸ªæåæ¯çæªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>ç»ä¸è¿ç§»å­¦ä¹ èå¼ï¼</strong> å¼åä¸ä¸ªç»ä¸çè¿ç§»å­¦ä¹ æ¡æ¶ï¼ä½¿åä¸ªILFMè½å¤åæ¶ææå°éåºå¤ä¸ªè§é¢-è¯­è¨ä»»å¡ï¼ä»èå®ç°æ´éç¨ãæ´é«æçè§é¢æºè½ç³»ç»ãè¿å¯è½æ¶åæ¢ç´¢åºäºæç¤ºçå­¦ä¹ æè®¾è®¡åæ°é«æçç»ä¸ééå¨/è¶ç½ç»ã</li>
<li><strong>å¤åºç¡æ¨¡ååä½ï¼</strong> æææ´åå¤ä¸ªé¢è®­ç»çåºç¡æ¨¡åï¼æ¯ä¸ªæ¨¡åä¸æ³¨äºä¸åçæ¨¡ææè½åï¼ä»¥è§£å³åä¸ªè§é¢-è¯­è¨ä»»å¡ãç ç©¶å¯ä»¥éä¸­äºèåæºå¶ãç¥è¯è¸é¦åè·¨æ¨¡åæ³¨æåï¼ä»¥å©ç¨äºè¡¥ä¼å¿å¹¶éä½è®¡ç®ææ¬ã</li>
<li><strong>é«çº§åæ¨¡æèåæ¹æ³ï¼</strong> è¿ä¸æ­¥ç ç©¶æ´å¨æãé«æçèåææ¯ï¼å¦è·¨æ¨¡æTransformerãåºäºå¾çå¯¹é½æè½»éçº§ç¹å¾äº¤äºæ¨¡åï¼ä»¥æ¹åè§è§åè¯­è¨ç¹å¾å¨æ¶ç©ºç»´åº¦ä¸çå¯¹é½åèåï¼è¿å¯¹äºç»ç²åº¦çè§£åé¿ç¯è§é¢æ¨çè³å³éè¦ã</li>
<li><strong>è§é¢ç¼è¾åçæï¼</strong> æ©å±å¾åå°è§é¢è¿ç§»å­¦ä¹ ææ¯ï¼ä»¥æ¯æè§é¢ç¼è¾åçæä»»å¡ãè¿éè¦ææå»ºæ¨¡æ¶åºè¿è´¯æ§åå¨æè¿å¨ï¼ä¾å¦éè¿éææ¶åºæ³¨æåãå©ç¨è¾å©æ¨¡æï¼å¦æ·±åº¦å¾ãåæµï¼æå¼åæ°çæ©æ£æ¨¡åæ¶æã</li>
<li><strong>å¼æ¾è¯æ±è½åï¼</strong> è¿ä¸æ­¥å¢å¼ºæ¨¡åå¨å¼æ¾è¯æ±åºæ¯ä¸çæ³åè½åï¼ä½¿å¶è½å¤å¤çè®­ç»ä¸­æªè§è¿çå¯¹è±¡ç±»å«åäºä»¶ã</li>
</ul>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºå¾åå°è§é¢è¿ç§»å­¦ä¹ é¢åæä¾äºä¸ä¸ªå¨é¢çææ¯èå¾ï¼ä¸ä»æ»ç»äºç°ææ¹æ³ï¼è¿æåºäºæªæ¥çåå±æ¹åï¼å¯¹äºæ¨å¨è§é¢-ææ¬çè§£é¢åçåå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Finally, we identify prevailing
challenges and highlight promising directions for future research.</li>
<li>By offering
a comprehensive and structured overview, this survey aims to establish a
structured roadmap for advancing video-text learning based on existing ILFM,
and to inspire future research directions in this rapidly evolving domain.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.10671v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.10671v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11717v1'></a></p>
<h2 id="ev4dgs-novel-view-rendering-of-non-rigid-objects-from-monocular-event-streams"><a href="https://arxiv.org/abs/2510.11717v1">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a></h2>
<p><strong>Authors:</strong> Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanikæ°åçè®ºæâEv4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streamsâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Ev4DGS: ä»åç®äºä»¶æµæ¸²æéåæ§ç©ä½çæ°è§è§</strong></p>
<p>è¿ç¯è®ºæãEv4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streamsãé¦æ¬¡æåºäºä¸ç§ä»åç®äºä»¶æµä¸­æ¸²æéåæ§åå½¢ç©ä½æ°è§è§çæ¹æ³ãä¼ ç»ä¸ï¼äºä»¶ç¸æºå¨åæ§åºæ¯çæ°è§è§åææ¹é¢å·²æ¾ç¤ºåºä¼å¿ï¼ä½å¯¹äºéåæ§ç©ä½ï¼ç°ææ¹æ³éå¸¸ä»ä¾èµäºç¨ççRGBè¾å¥ï¼è¿éå¶äºå¶å¨çº¯äºä»¶æµåºæ¯ä¸çåºç¨ãEv4DGSæ¨å¨è§£å³è¿ä¸æææ§é®é¢ï¼æ¢ç´¢æ¯å¦ä»å­äºä»¶æµå°±è½å­¦ä¹ å°éåæ§ç©ä½çæ¨¡åã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶ä¸»è¦è§£å³çæ ¸å¿é®é¢æ¯ï¼å¦ä½ä»ä½¿ç¨åç®äºä»¶æµï¼å®ç°å¯¹éåæ§åå½¢ç©ä½çé«è´¨éæ°è§è§æ¸²æï¼ç°ææ¹æ³å¨å¤çéåæ§ç©ä½æ¶ï¼éå¸¸éè¦ç»åRGBå¾åï¼èçº¯äºä»¶æµçè§£å³æ¹æ¡ä»æ¯æªç¥çãè¿ç¯è®ºææ¨å¨å¡«è¡¥è¿ä¸ç©ºç½ï¼å¹¶è¯æä»éè¿äºä»¶æ°æ®è¿è¡éåæ§3Déå»ºåæ°è§è§åææ¯å¯è¡çã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
Ev4DGSå¼å¥äºä»¥ä¸å³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼
*   <strong>é¦ä¸ªçº¯äºä»¶æµéåæ§æ°è§è§æ¸²ææ¹æ³ï¼</strong> Ev4DGSæ¯ç¬¬ä¸ä¸ªè½å¤ä»åç®äºä»¶æµä¸­æ¸²æéåæ§åå½¢ç©ä½ï¼ä»¥RGBæç°åº¦å¾åå½¢å¼ï¼æ°è§è§çæ¹æ³ï¼æ éä»»ä½RGBè¾å¥ã
*   <strong>ä¸¤é¶æ®µè®­ç»ç­ç¥ï¼</strong> ä¸ºäºè§£å³ä»åç®è§æµä¸­éå»ºå¯åå½¢ç©ä½çé«åº¦çæé®é¢ï¼Ev4DGSéç¨ä¸¤é¶æ®µè®­ç»ã
    *   <strong>ç²ç¥åå½¢æ¨¡åï¼Coarse Stageï¼ï¼</strong> ç¬¬ä¸é¶æ®µè®­ç»ä¸ä¸ªç²ç¥ç3Dåå½¢æ¨¡åï¼è¯¥æ¨¡åå°éåæ§ç©ä½å½¢ç¶è¡¨ç¤ºä¸ºä¸ç»æ¶é´ä¾èµçç²ç¥ç¹äºï¼å¹¶è½ææåºæ¯ä¸­çå¤§å°ºåº¦åå½¢ãå®éè¿å­¦ä¹ ä½ç§©åºç¹äºççº¿æ§ç»åæ¥è¡¨ç¤ºç©ä½éæ¶é´æ¼åçç¶æï¼å¹¶å©ç¨ä»äºä»¶çæçäºå¼æ©ç è¿è¡çç£ã
    *   <strong>ç²¾ç»3Dé«æ¯æºå°è¡¨ç¤ºï¼Fine Stageï¼ï¼</strong> ç¬¬äºé¶æ®µå¨æ­¤ç²ç¥æ¨¡åçåºç¡ä¸ï¼å©ç¨4Dé«æ¯æºå°ï¼3DGSçæ³åï¼æ¥è¡¨ç¤ºç©ä½çç²¾ç»å¤è§ï¼å®ç°å¨ææ°è§è§åæãé«æ¯åæ°éè¿ç²ç¥åå½¢æ¨¡åé©±å¨ï¼å¹¶å±äº«æ¶é´ä¿¡æ¯ã
*   <strong>äºä»¶æå¤±åè½®å»æå¤±ï¼</strong> éå¯¹äºä»¶æ°æ®çç¹æ§ï¼è®ºæè®¾è®¡äºç¹å®çæå¤±å½æ°ï¼
    *   <strong>äºä»¶æå¤±ï¼Event Lossï¼ï¼</strong> å°ä¼°è®¡æ¨¡åçè¾åºä¸2Däºä»¶è§æµç©ºé´å³èèµ·æ¥ï¼éè¿æ¯è¾æ¸²æå¾åçäº®åº¦ååä¸äºä»¶æµä¸­ç´¯ç§¯çäº®åº¦å·®å¼æ¥ä¼å3Dé«æ¯åæ°ã
    *   <strong>è½®å»æå¤±ï¼Silhouette Lossï¼ï¼</strong> å©ç¨ä»äºä»¶æµçæçäºå¼æ©ç ï¼æå¶èæ¯ä¸­ä¸å¿è¦çé«æ¯ç¹ï¼ä»èæé«å¾åè´¨éã
*   <strong>èªçç£å­¦ä¹ ï¼</strong> æ´ä¸ªæ¡æ¶éè¿äºä»¶æµåç¸æºè·è¸ªä¿¡æ¯å®ç°èªçç£å­¦ä¹ ï¼æ éé¢å¤è¾å¥ã
*   <strong>æ°æ°æ®éï¼</strong> è®ºæåå»ºäºæ°çåæåçå®æ°æ®éï¼ä»¥è¯ä¼°Ev4DGSå¨éåæ§ç©ä½ä¸çæ§è½ï¼å¡«è¡¥äºç°ææ°æ®éçä¸è¶³ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ä¼è¶çæ§è½ï¼</strong> Ev4DGSå¨åæåçå®æ°æ®éä¸åè¡¨ç°åºåè¶çæ§è½ï¼å¨PSNRææ ä¸å¹³åæ¯ç°æåºçº¿æ¹æ³ï¼å¦3DGSåD3DGSï¼é«åºçº¦10%ï¼åæ¶å¨SSIMä¸ä¹å·æç«äºåã
*   <strong>é«è´¨éæ°è§è§æ¸²æï¼</strong> å®éªç»æè¡¨æï¼Ev4DGSè½å¤çæé«è´¨éãç©ºé´åæ¶é´è¿è´¯çæ°è§è§æ¸²æï¼åç¡®ææåå½¢ç©ä½çå¤è§åå½¢ç¶ï¼èç«äºæ¹æ³å¾å¾ä¼ä¸¢å¤±ç©ä½åé¨çç»èæåºç°æ¨¡ç³ã
*   <strong>çº¯äºä»¶æµçå¯è¡æ§ï¼</strong> è®ºææåè¯æäºä»ä½¿ç¨äºä»¶æµè¿è¡éåæ§3Déå»ºåæ°è§è§åæçå¯è¡æ§ï¼é¿åäºä¸­é´å¸§éå»ºå¸¦æ¥çä¿¡æ¯æå¤±åè¯¯å·®ã
*   <strong>å¯¹æ©ç è´¨éçæææ§ï¼</strong> æ¶èç ç©¶æ¾ç¤ºï¼æ¨¡åæ§è½å¯¹äºå¼æ©ç çè´¨éææãéè¿Snakeç®æ³ç´æ¥ä»äºä»¶æµçææ©ç æ¯éè¿E2VID+SAMä»éå»ºå¾åçææ©ç æææ´å¥½ï¼ä½ä»ä¸å¦ä½¿ç¨çå®ï¼GTï¼äºå¼æ©ç ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¤è§ä¼ªå½±ï¼</strong> å°½ç®¡Ev4DGSåå¾äºæé«çåç¡®æ§ï¼ä½æ¨¡åçä¸ä¸ªå±éæ§æ¯æ¸²æçé«æ¯ç¹å¯è½å¯¼è´å¤è§ä¸åºç°ä¼ªå½±ï¼å¦åä¸ªææç»æ¸²æé«æ¯ç¹ï¼ã
*   <strong>å¯¹æ©ç è´¨éçä¾èµï¼</strong> æ¨¡åçæ§è½å¨ä¸å®ç¨åº¦ä¸ä¾èµäºäºå¼æ©ç çè´¨éãå¦æè½ç´æ¥ä»äºä»¶æµçææ´åç¡®çäºå¼æ©ç ï¼æ§è½ææè¿ä¸æ­¥æåã
*   <strong>ä¼åé¾åº¦ï¼</strong> å¢å åºåéKçæ°éè½ç¶è½å¢å¼ºæ¨¡åçè¿å¨è¡¨è¾¾è½åï¼ä½ä¹ä¼å¢å ä¼åçé¾åº¦ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¹è¿æ©ç çæï¼</strong> æ¢ç´¢æ´åç¡®ãæ´é²æ£çç´æ¥ä»äºä»¶æµçæäºå¼æ©ç çæ¹æ³ï¼ä»¥è¿ä¸æ­¥æåEv4DGSçæ§è½ã
*   <strong>åå°å¤è§ä¼ªå½±ï¼</strong> ç ç©¶å¦ä½ä¼åé«æ¯æºå°è¡¨ç¤ºææ¸²æè¿ç¨ï¼ä»¥åå°æ¸²æå¾åä¸­çå¤è§ä¼ªå½±ï¼æé«è§è§çå®æã
*   <strong>æ´å¤æçåå½¢ï¼</strong> æ¢ç´¢Ev4DGSå¨æ´å¤æãæ´å¤§èå´çéåæ§åå½¢åºæ¯ä¸­çåºç¨åæ¹è¿ã
*   <strong>å®æ¶æ§è½ä¼åï¼</strong> å°½ç®¡è®ºææå°äºå¨éå»ºæ¶é´çº¦ä¸º4å°æ¶ï¼ä½æªæ¥å¯ä»¥ç ç©¶å¦ä½è¿ä¸æ­¥ä¼åè®­ç»åæ¸²æè¿ç¨ï¼ä»¥å®ç°æ´æ¥è¿å®æ¶çæ§è½ã
*   <strong>å¤æ¨¡æèåï¼</strong> è½ç¶Ev4DGSä¸æ³¨äºçº¯äºä»¶æµï¼ä½æªæ¥ä¹å¯ä»¥æ¢ç´¢ä¸å°éRGBè¾å¥æå¶ä»ä¼ æå¨æ°æ®çæ´ææèåï¼ä»¥å¨ç¹å®åºç¨ä¸­è¿ä¸æ­¥æåæ§è½ã</p>
<hr />
<p>æ»èè¨ä¹ï¼Ev4DGSæ¯è®¡ç®æºè§è§é¢åçä¸é¡¹éè¦è¿å±ï¼å®ä¸ºä»åç®äºä»¶æµä¸­è¿è¡éåæ§ç©ä½çæ°è§è§æ¸²æå¼è¾äºæ°éå¾ï¼å±ç¤ºäºäºä»¶ç¸æºå¨å¤çå¨æåºæ¯æ¹é¢çå·¨å¤§æ½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature.</li>
<li>This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams.</li>
<li>Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events.</li>
<li>We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11717v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11717v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11712v1'></a></p>
<h2 id="dit360-high-fidelity-panoramic-image-generation-via-hybrid-training"><a href="https://arxiv.org/abs/2510.11712v1">DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</a></h2>
<p><strong>Authors:</strong> Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this work, we propose DiT360, a DiT-based framework that performs hybrid
training on perspective and panoramic data for panoramic image generation. For
the issues of maintaining geometric fidelity and photorealism in generation
quality, we attribute the main reason to the lack of large-scale, high-quality,
real-world panoramic data, where such a data-centric view differs from prior
methods that focus on model design. Basically, DiT360 has several key modules
for inter-domain transformation and intra-domain augmentation, applied at both
the pre-VAE image level and the post-VAE token level. At the image level, we
incorporate cross-domain knowledge through perspective image guidance and
panoramic refinement, which enhance perceptual quality while regularizing
diversity and photorealism. At the token level, hybrid supervision is applied
across multiple modules, which include circular padding for boundary
continuity, yaw loss for rotational robustness, and cube loss for distortion
awareness. Extensive experiments on text-to-panorama, inpainting, and
outpainting tasks demonstrate that our method achieves better boundary
consistency and image fidelity across eleven quantitative metrics. Our code is
available at https://github.com/Insta360-Research-Team/DiT360.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡è¯¦ç»åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>DiT360 æåºäºä¸ç§åºäº DiT çå¨æ¯å¾åçææ¡æ¶ï¼éè¿å¨éè§æ°æ®åå¨æ¯æ°æ®ä¸è¿è¡æ··åè®­ç»æ¥è§£å³å¨æ¯å¾åçæä¸­çå ä½ä¿çåº¦åçå®æé®é¢ãè¯¥æ¹æ³çæ ¸å¿å¨äºå¶æ°æ®ä¸­å¿è§è§ï¼éè¿å¨å¾åå token çº§å«å¼å¥è·¨åè½¬æ¢åååå¢å¼ºæ¨¡åï¼æ¾èæåäºçæè´¨éãè¾¹çä¸è´æ§åå¾åä¿çåº¦ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶<strong>æ··åè®­ç»èå¼åæ°æ®ä¸­å¿è§è§</strong>ï¼è¿ä¸ä»¥å¾ä¸æ³¨äºæ¨¡åè®¾è®¡çæ¹æ³å½¢æå¯¹æ¯ãå·ä½æ¹æ³è®ºåæ¬ï¼</p>
<ul>
<li><strong>æ··åè®­ç» (Hybrid Training)ï¼</strong> å¨éè§æ°æ®åå¨æ¯æ°æ®ä¸åæ¶è¿è¡è®­ç»ï¼å©ç¨éè§æ°æ®ä¸°å¯çç»èåçå®ææ¥å¼¥è¡¥å¨æ¯æ°æ®ç¨ç¼ºçä¸è¶³ã</li>
<li><strong>å¤å±æ¬¡è·¨åç¥è¯èåï¼</strong><ul>
<li><strong>å¾åçº§å« (Pre-VAE Image Level)ï¼</strong> å¼å¥éè§å¾åå¼å¯¼ (perspective image guidance) åå¨æ¯ç²¾ç¼ (panoramic refinement)ï¼ä»¥å¢å¼ºæç¥è´¨éï¼åæ¶è§èå¤æ ·æ§åçå®æãè¿è¡¨ææ¨¡åå¨ç¼ç å¨ä¹åå°±å©ç¨äºéè§å¾åçç»æä¿¡æ¯ã</li>
<li><strong>Token çº§å« (Post-VAE Token Level)ï¼</strong> å¨ VAE ç¼ç åçæ½å¨ç©ºé´ (token level) åºç¨æ··åçç£ï¼åæ¬ï¼<ul>
<li><strong>å¾ªç¯å¡«å (Circular Padding)ï¼</strong> ç¡®ä¿å¨æ¯å¾åå·¦å³è¾¹ççè¿ç»­æ§ï¼è¿æ¯å¨æ¯å¾ç¹æçææã</li>
<li><strong>åèªæå¤± (Yaw Loss)ï¼</strong> å¢å¼ºæè½¬é²æ£æ§ï¼ä½¿çæåå®¹å¨ä¸åè§è§ä¸ä¿æä¸è´ã</li>
<li><strong>ç«æ¹ä½æå¤± (Cube Loss)ï¼</strong> æé«å¯¹å¨æ¯å¾ååºæç¸åçæç¥åå¤çè½åï¼å¯è½éè¿å°å¨æ¯å¾æå½±å°ç«æ¹ä½è´´å¾æ¥è®¡ç®æå¤±ã</li>
</ul>
</li>
</ul>
</li>
<li><strong>æ°æ®ä¸­å¿è§è§ï¼</strong> å¼ºè°ç¼ºä¹å¤§è§æ¨¡ãé«è´¨éãçå®ä¸çå¨æ¯æ°æ®æ¯çæè´¨éé®é¢çæ ¹æ¬åå ï¼å¹¶éè¿ä¸è¿°æ··åè®­ç»åå¤å±æ¬¡ç­ç¥æ¥ææå©ç¨ç°ææ°æ®ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>DiT360 ææå¯¹è®¡ç®æºè§è§é¢åäº§çä»¥ä¸æ½å¨å½±åï¼</p>
<ul>
<li><strong>æåå¨æ¯å¾åçæè´¨éæ åï¼</strong> éè¿è§£å³å ä½ä¿çåº¦åçå®æçæ ¸å¿é®é¢ï¼DiT360 å¯è½æä¸ºå¨æ¯å¾åçæé¢åçæ°åºçº¿ï¼æ¨å¨æ´é«è´¨éççæç»æã</li>
<li><strong>å¯åæ°çæ°æ®å©ç¨ç­ç¥ï¼</strong> å¶æ··åè®­ç»åæ°æ®ä¸­å¿è§è§ä¸ºå¤çç¹å®é¢åæ°æ®ç¨ç¼ºé®é¢æä¾äºæ°çæè·¯ï¼å°¤å¶æ¯å¨éè¦è·¨åç¥è¯è¿ç§»ççæä»»å¡ä¸­ã</li>
<li><strong>æ¨å¨å¨æ¯åå®¹åä½ååºç¨ï¼</strong> æ´é«è´¨éçå¨æ¯çæè½åå°ç´æ¥èµè½èæç°å® (VR)ãå¢å¼ºç°å® (AR)ãåå®å®ã360Â° è§é¢å¶ä½ãæ¸¸æç¯å¢çæç­é¢åï¼éä½åå®¹åä½é¨æ§ã</li>
<li><strong>ä¸º DiT æ¶æçåºç¨æä¾æ°æ¹åï¼</strong> å±ç¤ºäº DiT æ¶æå¨å¤çå¤æå ä½åææç»ææ°æ®ï¼å¦å¨æ¯å¾ï¼æ¹é¢çæ½åï¼å¯è½æ¿å DiT å¨å¶ä»éæ åå¾åæ ¼å¼çæä»»å¡ä¸­çåºç¨ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨</strong></p>
<p>ä»¥ä¸é¢åæåºç¨å°ä»è¿é¡¹ç ç©¶ä¸­åçï¼</p>
<ul>
<li><strong>èæç°å® (VR) åå¢å¼ºç°å® (AR)ï¼</strong> çæé¼çç 360Â° ç¯å¢åçº¹çï¼ç¨äºæ²æµ¸å¼ä½éªã</li>
<li><strong>åå®å® (Metaverse)ï¼</strong> å¿«éåå»ºåå¡«åèæä¸ççå¨æ¯èæ¯ååºæ¯ã</li>
<li><strong>360Â° è§é¢åå¾åç¼è¾ï¼</strong> ææ¬å°å¨æ¯å¾çæãå¨æ¯å¾ä¿®å¤ (inpainting) åæ©å± (outpainting) å°æå¤§å°ç®å 360Â° åªä½çåæå¶ä½ã</li>
<li><strong>æ¸¸æå¼åï¼</strong> èªå¨çææ¸¸æåºæ¯çèæ¯å¤©ç©ºç (skybox) æç¯å¢è´´å¾ã</li>
<li><strong>æºå¨äººåèªå¨é©¾é©¶ï¼</strong> çæå¤æ ·åçå¨æ¯ç¯å¢æ°æ®ç¨äºè®­ç»åæµè¯æç¥ç³»ç»ï¼å°¤å¶æ¯å¨æ°æ®ééå°é¾çåºæ¯ã</li>
<li><strong>å»ºç­åå®¤åè®¾è®¡ï¼</strong> å¿«éçæä¸åè®¾è®¡æ¹æ¡çå¨æ¯æ¸²æå¾ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<p>å°½ç®¡æè¦å¼ºè°äºæ¾èçè¿æ­¥ï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨å±éæ§ï¼</p>
<ul>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> æ··åè®­ç»ï¼å°¤å¶æ¯å¨å¾åå token çº§å«é½è¿è¡å¤ææä½ï¼å¯è½éè¦å¤§éçè®¡ç®èµæºï¼GPU åå­åè®¡ç®æ¶é´ï¼ï¼è¿å¯¹äºä¸ªäººç ç©¶èæå°åå¢éå¯è½æ¯ä¸ä¸ªææã</li>
<li><strong>æ°æ®ä¾èµæ§ï¼</strong> å°½ç®¡å¼ºè°äºæ°æ®ä¸­å¿è§è§ï¼ä½å¶æ§è½ä»å¯è½é«åº¦ä¾èµäºæä½¿ç¨çéè§æ°æ®åå°éå¨æ¯æ°æ®çè´¨éåå¤æ ·æ§ãå¦æéè§æ°æ®æ¬èº«å­å¨åå·®æä¸è¶³ï¼å¯è½ä¼å½±åæç»çæè´¨éã</li>
<li><strong>æ³åè½åï¼</strong> æè¦ä¸­æªæåæ¨¡åå¨å¤çæç«¯å¤æåºæ¯ãé«åº¦å¨æåå®¹æç¹å®é£æ ¼å¨æ¯å¾æ¶çæ³åè½åãä¾å¦ï¼å¨çæå·æå¤æå ä½ç»ææç²¾ç»çº¹ççå¨æ¯å¾æ¶ï¼æ¯å¦è½å§ç»ä¿æé«ä¿çåº¦ã</li>
<li><strong>âç¼ºä¹å¤§è§æ¨¡ãé«è´¨éãçå®ä¸çå¨æ¯æ°æ®âçæ ¹æ¬é®é¢ï¼</strong> å°½ç®¡ DiT360 æ¨å¨ç¼è§£è¿ä¸é®é¢ï¼ä½å®å¹¶æªä»æ ¹æ¬ä¸è§£å³å¨æ¯æ°æ®ç¨ç¼ºçææãå¦ææªæ¥æå¤§éé«è´¨éå¨æ¯æ°æ®å¯ç¨ï¼å¶æ··åè®­ç»ç­ç¥å¯è½éè¦è°æ´æä¼åã</li>
<li><strong>ç¹å®ç¸åå¤çï¼</strong> å°½ç®¡æå°äºâç«æ¹ä½æå¤±âæ¥å¤çç¸åï¼ä½å¨æ¯å¾çç¸åæ¯å¤æçï¼å°¤å¶æ¯å¨æç¹éè¿ãæè¦ä¸­æªè¯¦ç»è¯´æå¶å¯¹ææç±»åç¸åï¼å¦æç¹æä¼¸ï¼çå¤çææã</li>
<li><strong>è¯ä¼°ææ çå±éæ§ï¼</strong> æè¦æå°âåä¸ä¸ªå®éææ âï¼ä½è¿äºææ æ¯å¦è½å®å¨ææå¨æ¯å¾åçè§è§è´¨éãå ä½åç¡®æ§åç¨æ·ä½éªï¼ä»éå¨è®ºææ­£æä¸­è¯¦ç»æ¢è®¨ãä¾å¦ï¼äººç±»æç¥è¯ä¼°ï¼user studyï¼éå¸¸æ¯ä¸å¯æç¼ºçã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼DiT360 æåºäºä¸ç§æ°é¢ä¸å®ç¨çå¨æ¯å¾åçææ¹æ³ï¼éè¿å¶ç¬ç¹çæ··åè®­ç»åå¤å±æ¬¡å¤çç­ç¥ï¼ææå¨è§£å³å¨æ¯å¾çæä¸­çæ ¸å¿æææ¹é¢åå¾æ¾èè¿å±ãå¶æ°æ®ä¸­å¿è§è§åå¯¹ DiT æ¶æçåæ°åºç¨ï¼ä½¿å¶æä¸ºè®¡ç®æºè§è§é¢åä¸ä¸ªå¼å¾å³æ³¨çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose DiT360, a DiT-based framework that performs hybrid
training on perspective and panoramic data for panoramic image generation.</li>
<li>Extensive experiments on text-to-panorama, inpainting, and
outpainting tasks demonstrate that our method achieves better boundary
consistency and image fidelity across eleven quantitative metrics.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11693v1'></a></p>
<h2 id="scaling-language-centric-omnimodal-representation-learning"><a href="https://arxiv.org/abs/2510.11693v1">Scaling Language-Centric Omnimodal Representation Learning</a></h2>
<p><strong>Authors:</strong> Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chenghao Xiaoç­äººæ°åçè®ºæâScaling Language-Centric Omnimodal Representation Learningâçå¨é¢æè¦ã</p>
<hr />
<h3 id="scaling-language-centric-omnimodal-representation-learning_1">è®ºææè¦ï¼Scaling Language-Centric Omnimodal Representation Learning</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>è¯¥è®ºææ¨å¨è§£å³ä¸ä¸ªæ ¸å¿é®é¢ï¼å°½ç®¡åºäºå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼å¹¶ç»åå¯¹æ¯å­¦ä¹ ï¼CLï¼çåµå¥æ¹æ³å¨å¤æ¨¡æä»»å¡ä¸­è¡¨ç°åºè²ï¼ä½å¶ä¼è¶æ§çæ·±å±åå å°æªè¢«ååæ¢ç´¢ãå·ä½æ¥è¯´ï¼ç ç©¶èå¸æçè§£MLLMå¨çæå¼é¢è®­ç»è¿ç¨ä¸­å¦ä½å®ç°éå¼è·¨æ¨¡æå¯¹é½ï¼ä»¥åè¿ç§å¯¹é½å¦ä½å½±åå¶è¡¨å¾è½åï¼å¹¶ä¸ºåç»­çå¯¹æ¯å­¦ä¹ æä¾åºç¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<ul>
<li><strong>MLLMä¸­æ½å¨è·¨æ¨¡æå¯¹é½çåç°ä¸åæï¼</strong> è®ºæéè¿å¯¹åæ¨¡æï¼ææ¬ãå¾åãé³é¢ãè§é¢ï¼åµå¥ç©ºé´çååå¼æ§ï¼anisotropyï¼åæ ¸çº§ç¸ä¼¼æ§ï¼kernel-level similarityï¼ç»æè¿è¡å®è¯åæï¼åç°MLLMå¨çæå¼é¢è®­ç»ä¸­å·²å®ç°éå¼è·¨æ¨¡æå¯¹é½ãææ¬æ¨¡æçå¯¹æ¯å­¦ä¹ ä¸ä»æé«äºææ¬åµå¥çå¯åºåæ§ï¼è¿æ³åæ§å°å¢å¼ºäºéææ¬æ¨¡æåµå¥çå¯åºåæ§ï¼ä½¿å¶æ´å·åååæ§ã</li>
<li><strong>æåºè¯­è¨ä¸­å¿å¨æ¨¡æåµå¥æ¡æ¶ï¼LCO-EMBï¼ï¼</strong> åºäºä¸è¿°åç°ï¼è®ºææåºäºLCO-EMBæ¡æ¶ãè¯¥æ¡æ¶å©ç¨è¯­è¨ä¸­å¿éå¯¹æ°æ®è¿è¡é«æçå¯¹æ¯å­¦ä¹ å¾®è°ï¼å°å¯¹æ¯å­¦ä¹ è§ä¸ºè½»éçº§çç²¾ç¼é¶æ®µï¼èéä»å¤´å¼å§çå¯¹é½æºå¶ãLCO-EMBéè¿LoRAï¼Low-Rank Adaptationï¼ææ¯å¯¹MLLMè¿è¡è¡¨å¾æ¿æ´»ï¼æ¨å¨æå°åæ°å¨é¢è®­ç»ççæè½ååæ½å¨è·¨æ¨¡æå¯¹é½ã</li>
<li><strong>è¯å«å¹¶çè®ºè§£éçæ-è¡¨å¾ç¼©æ¾å®å¾ï¼GRSLï¼ï¼</strong> è®ºæéè¿å®éªè§å¯å°ï¼MLLMçè¡¨å¾è½åï¼éè¿å¯¹æ¯å­¦ä¹ ç²¾ç¼åï¼ä¸å¶çæè½ååæ­£ç¸å³ãç ç©¶èè¿ä¸æ­¥æä¾äºGRSLççè®ºè§£éï¼éè¿PAC-è´å¶æ¯æ³åçéï¼å½¢å¼åå°å°MLLMççæè´¨éä¸å¶è¡¨å¾æ§è½çä¸éèç³»èµ·æ¥ï¼è¡¨æçæè½åè¶å¼ºï¼è¡¨å¾æ½åè¶å¤§ã</li>
<li><strong>å¼å¥SeaDocåºåä»»å¡ï¼</strong> ä¸ºäºéªè¯GRSLï¼è®ºæå¼å¥äºSeaDocï¼è¿æ¯ä¸ä¸ªå·ææææ§çä½èµæºè§è§ææ¡£æ£ç´¢ä»»å¡ï¼ç¨äºè¯ä¼°MLLMå¨è·¨è¯­è¨å¤æ¨¡æææ¡£çè§£æ¹é¢çè¡¨å¾è½åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<ul>
<li><strong>LCO-EMBçåè¶æ§è½ï¼</strong> LCO-EMBå¨MIEB-Liteåºåæµè¯ä¸­ï¼å³ä½¿ä»ä½¿ç¨å°éï¼çº¦0.37Mï¼è®­ç»å¯¹ï¼ä¹æ¾èä¼äºç°æçå¤æ¨¡æåµå¥æ¨¡åï¼å¹¶å¨å¤ç§æ¨¡æä»»å¡ä¸­è¾¾å°æåè¿çæ§è½ãè¿è¡¨æMLLMçåå¨è·¨æ¨¡æå¯¹é½è½åæ¯å¶æ§è½ä¼å¿çå³é®ã</li>
<li><strong>LoRAçæææ§ï¼</strong> LoRAä½ä¸ºä¸ç§åæ°é«æçå¾®è°æ¹æ³ï¼å¨ä¿ææ¨¡åçæè½ååæ½å¨è·¨æ¨¡æå¯¹é½çåæ¶ï¼æææåäºè¡¨å¾è½åï¼ä¼äºä¼ ç»çCLIPé£æ ¼å¯¹æ¯å­¦ä¹ åå¨éå¾®è°ã</li>
<li><strong>GRSLçå®è¯éªè¯ï¼</strong> å®éªç»æä¸è´è¡¨æï¼åºçº¿çææ§è½ä¸å¯¹æ¯å­¦ä¹ åçè¡¨å¾æ§è½ä¹é´å­å¨æ­£ç¸å³ãè¿è¯å®äºGRSLï¼å¹¶æåºéè¿æåMLLMççæè½åæ¥å¢å¼ºå¶å¤æ¨¡æè¡¨å¾è´¨éæ¯ä¸ç§ææèå¼ã</li>
<li><strong>SeaDocä»»å¡çéªè¯ï¼</strong> å¨SeaDocä»»å¡ä¸ï¼æç»­ççæå¼é¢è®­ç»ï¼å°¤å¶æ¯å¨é«åè¾¨çåç»åéç¨å¾åæ æ³¨æ°æ®çæåµä¸ï¼è½è¿ä¸æ­¥æåæ¨¡åçåµå¥è½åï¼æ¯æäºGRSLçè§ç¹ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<ul>
<li><strong>è®¡ç®ææ¬ï¼</strong> è®ºææåºï¼è½ç¶å¯ä»¥èåè®­ç»çææå¤±åå¯¹æ¯æå¤±ä»¥åæ¶ä¿ææ¨¡åç¥è¯åå¢å¼ºè¡¨å¾è½åï¼ä½è¿ç§æ¹æ³è®¡ç®ææ¬è¾é«ã</li>
<li><strong>LoRAè¶åæ°çä¼åï¼</strong> è®ºææå°ï¼LoRAçrank (r) å alpha (a) è¶åæ°æ²¡æä¸ä¸ªå¨å±æä¼è®¾ç½®ï¼å¶æä½³å¼å¯è½å æ¨¡åå¤§å°èå¼ï¼éè¦å¨å¼å¥æ°ç¥è¯åä¿®æ¹é¢è®­ç»æ¨¡åæéä¹é´åå¾å¹³è¡¡ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<ul>
<li><strong>èåçæä¸å¯¹æ¯å­¦ä¹ ï¼</strong> é´äºèåè®­ç»çææå¤±åå¯¹æ¯æå¤±çè®¡ç®ææ¬è¾é«ï¼æªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢æ´é«æçæ¹æ³æ¥å®ç°è¿ä¸ç®æ ï¼ä»¥è¿ä¸æ­¥æåå¨æ¨¡æè¡¨å¾å­¦ä¹ ã</li>
<li><strong>LoRAè¶åæ°çå¨é¢åæï¼</strong> å¯¹LoRAè¶åæ°è¿è¡æ´å¨é¢çå®è¯åæåçè®ºç ç©¶ï¼ä»¥éåå¶ä¸æ¨¡åæ§è½ä¹é´çå³ç³»ï¼å¹¶æ¾å°æ´éç¨çä¼åç­ç¥ã</li>
<li><strong>GRSLçè¿ä¸æ­¥æ¢ç´¢ï¼</strong> æ·±å¥ç ç©¶GRSLï¼æ¢ç´¢å¦ä½éè¿æ¹è¿çæè½åæ¥ç³»ç»æ§å°æåè¡¨å¾è´¨éï¼è¿å¯è½æ¶åæ°ççæå¼é¢è®­ç»èå¼ææ¨¡åæ¶æã</li>
</ul>
<hr />
<p>è¿ç¯è®ºæä¸ºçè§£MLLMå¨å¤æ¨¡æåµå¥é¢åçæåæä¾äºæ·±å»è§è§£ï¼å¹¶æåºäºä¸ä¸ªæ°é¢çæ¡æ¶åéè¦çç¼©æ¾å®å¾ï¼ä¸ºæªæ¥å¤æ¨¡æè¡¨å¾å­¦ä¹ çç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb.</li>
<li>Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11693v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11693v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11650v1'></a></p>
<h2 id="infinihuman-infinite-3d-human-creation-with-precise-control"><a href="https://arxiv.org/abs/2510.11650v1">InfiniHuman: Infinite 3D Human Creation with Precise Control</a></h2>
<p><strong>Authors:</strong> Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating realistic and controllable 3D human avatars is a long-standing
challenge, particularly when covering broad attribute ranges such as ethnicity,
age, clothing styles, and detailed body shapes. Capturing and annotating
large-scale human datasets for training generative models is prohibitively
expensive and limited in scale and diversity. The central question we address
in this paper is: Can existing foundation models be distilled to generate
theoretically unbounded, richly annotated 3D human data? We introduce
InfiniHuman, a framework that synergistically distills these models to produce
richly annotated human data at minimal cost and with theoretically unlimited
scalability. We propose InfiniHumanData, a fully automatic pipeline that
leverages vision-language and image generation models to create a large-scale
multi-modal dataset. User study shows our automatically generated identities
are undistinguishable from scan renderings. InfiniHumanData contains 111K
identities spanning unprecedented diversity. Each identity is annotated with
multi-granularity text descriptions, multi-view RGB images, detailed clothing
images, and SMPL body-shape parameters. Building on this dataset, we propose
InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body
shape, and clothing assets. InfiniHumanGen enables fast, realistic, and
precisely controllable avatar generation. Extensive experiments demonstrate
significant improvements over state-of-the-art methods in visual quality,
generation speed, and controllability. Our approach enables high-quality avatar
generation with fine-grained control at effectively unbounded scale through a
practical and affordable solution. We will publicly release the automatic data
generation pipeline, the comprehensive InfiniHumanData dataset, and the
InfiniHumanGen models at https://yuxuan-xue.com/infini-human.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼InfiniHuman: Infinite 3D Human Creation with Precise Control</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæçæ ¸å¿è´¡ç®å¨äºæåºäºä¸ç§åä¸º InfiniHuman çæ¡æ¶ï¼è¯¥æ¡æ¶éè¿è¸é¦ç°æåºç¡æ¨¡åï¼ä»¥æä½çææ¬åçè®ºä¸æ éçå¯æ©å±æ§çæäºå¤§è§æ¨¡ãå¤æ ·åä¸å¯å«æ æ³¨ç 3D äººä½æ°æ®ï¼InfiniHumanDataï¼ãå¨æ­¤åºç¡ä¸ï¼å®è¿å¼å¥äºä¸ä¸ªæ©æ£æ¨¡å InfiniHumanGenï¼è½å¤å®ç°å¿«éãé¼çä¸ç²¾ç¡®å¯æ§ç 3D äººä½åèº«çæï¼æ¾èè¶è¶äºç°æææ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶âæ°æ®è¸é¦âåâæ°æ®é©±å¨çæâçæ¹æ³è®ºï¼</p>
<ul>
<li><strong>æ°æ®è¸é¦ä¸èªå¨æ æ³¨ (InfiniHumanData):</strong> æ ¸å¿ææ³æ¯å©ç¨ç°æçè§è§-è¯­è¨æ¨¡ååå¾åçææ¨¡åï¼ä»¥å¨èªå¨çæ¹å¼çæå¤§è§æ¨¡ãå¤æ¨¡æç 3D äººä½æ°æ®éãè¿è§£å³äºä¼ ç»æ¹æ³ä¸­æ°æ®ééåæ æ³¨ææ¬é«æãå¤æ ·æ§åéçé®é¢ãéè¿è¿ç§æ¹å¼ï¼ä»ä»¬è½å¤çæåå« 111K èº«ä»½çæ°æ®éï¼æ¶µçäºåææªæçå¤æ ·æ§ï¼å¹¶ä¸ºæ¯ä¸ªèº«ä»½æä¾äºå¤ç²åº¦ææ¬æè¿°ãå¤è§è§ RGB å¾åãè¯¦ç»æè£å¾åå SMPL èº«ä½å½¢ç¶åæ°ãç¨æ·ç ç©¶è¡¨æå¶èªå¨çæçäººä½ä¸æ«ææ¸²æå¾æ å¼ï¼è¿è¯æäºæ°æ®è´¨éã</li>
<li><strong>æ©æ£æ¨¡åé©±å¨çç²¾ç¡®æ§å¶çæ (InfiniHumanGen):</strong> å¨ InfiniHumanData çåºç¡ä¸ï¼ä»ä»¬å¼åäºä¸ä¸ªåºäºæ©æ£ççæç®¡é InfiniHumanGenãè¿ä¸ªæ¨¡åè½å¤ä»¥ææ¬ãèº«ä½å½¢ç¶åæè£èµäº§ä¸ºæ¡ä»¶ï¼å®ç°å¯¹ 3D äººä½åèº«çå¿«éãé¼çä¸ç²¾ç¡®çæ§å¶çæãè¿ä½¿å¾ç¨æ·å¯ä»¥æ ¹æ®å·ä½éæ±ï¼éè¿å¤æ¨¡æè¾å¥æ¥å®å¶åèº«ã</li>
</ul>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<p>InfiniHuman å¯¹è®¡ç®æºè§è§åå¾å½¢å­¦é¢åå·ææ·±è¿çæ½å¨å½±åï¼</p>
<ul>
<li><strong>æç ´æ°æ®ç¶é¢ï¼</strong> è§£å³äº 3D äººä½çæé¢åé¿æå­å¨çæ°æ®ç¨ç¼ºåæ æ³¨ææ¬é«æçé®é¢ï¼ä¸ºè®­ç»æ´å¼ºå¤§ççææ¨¡åæä¾äºâæ éâçæ°æ®æºã</li>
<li><strong>æåçæè´¨éä¸æ§å¶åï¼</strong> æ¾èæé«äº 3D äººä½åèº«çæçè§è§çå®æãéåº¦åç²¾ç»æ§å¶è½åï¼ä½¿å¾åå»ºé«åº¦å®å¶åçèæäººåå¾æ´å å¯è¡ã</li>
<li><strong>æ¨å¨åºç¡æ¨¡ååºç¨ï¼</strong> å±ç¤ºäºå¦ä½å·§å¦å°å©ç¨åè¸é¦ç°æçå¤§ååºç¡æ¨¡åï¼å¦è§è§-è¯­è¨æ¨¡ååå¾åçææ¨¡åï¼æ¥è§£å³ç¹å®é¢åçå¤æé®é¢ï¼ä¸ºå¶ä»é¢åçæ°æ®çææä¾äºæ°çèå¼ã</li>
<li><strong>éä½ææ¯é¨æ§ï¼</strong> éè¿æä¾èªå¨æ°æ®çæç®¡éãæ°æ®éåæ¨¡åï¼éä½äºç ç©¶äººååå¼åèè¿å¥ 3D äººä½çæé¢åçé¨æ§ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>èæç°å® (VR) åå¢å¼ºç°å® (AR):</strong> åå»ºé«åº¦é¼çåå¯å®å¶çèæåèº«ï¼ç¨äºç¤¾äº¤ VRãæ¸¸æãèæè¯ç©¿ç­ã</li>
<li><strong>çµå½±ãå¨ç»åæ¸¸æäº§ä¸ï¼</strong> å¿«éçæå¤§éå¤æ ·åçè§è²æ¨¡åï¼å¤§å¹ç¼©ç­å¶ä½å¨æåææ¬ã</li>
<li><strong>æ°å­æ¶å°åèæè¯ç©¿ï¼</strong> çæå·æä¸åä½åãè¤è²åæè£é£æ ¼çèææ¨¡ç¹ï¼ç¨äºæè£è®¾è®¡ãå±ç¤ºåå¨çº¿è´­ç©ä½éªã</li>
<li><strong>äººä½å§¿æä¼°è®¡åå¨ä½ææï¼</strong> çæå¤æ ·åçäººä½æ°æ®å¯ä»¥ä½ä¸ºè®­ç»æ°æ®ï¼æé«è¿äºä»»å¡çé²æ£æ§ã</li>
<li><strong>å»çåå¥åº·é¢åï¼</strong> çæä¸åä½ååå¹´é¾æ®µçäººä½æ¨¡åï¼ç¨äºå»å­¦æ¨¡æãåº·å¤è®­ç»æäººä½å·¥ç¨å­¦ç ç©¶ã</li>
<li><strong>äººæºäº¤äº (HCI):</strong> åå»ºæ´å·è¡¨ç°ååä¸ªæ§åçèæå©æææºå¨äººå½¢è±¡ã</li>
<li><strong>è®¡ç®æºå¾å½¢å­¦ç ç©¶ï¼</strong> ä¸º 3D å»ºæ¨¡ãæ¸²æãå¨ç»åè§è²ç»å®ç­ç ç©¶æä¾é«è´¨éçåºåæ°æ®åçæå·¥å·ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<p>å°½ç®¡æè¦å±ç¤ºäºä»¤äººå°è±¡æ·±å»çææï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨çå±éæ§ï¼</p>
<ul>
<li><strong>âçè®ºä¸æ éâä¸å®éè®¡ç®èµæºï¼</strong> å°½ç®¡æ°æ®çææ¯âçè®ºä¸æ éâçï¼ä½å®éçæåå­å¨å¦æ­¤å¤§è§æ¨¡çæ°æ®ä»éè¦å¤§éçè®¡ç®èµæºåå­å¨ç©ºé´ã</li>
<li><strong>åºç¡æ¨¡åçä¾èµæ§ï¼</strong> InfiniHuman çæ§è½å¨å¾å¤§ç¨åº¦ä¸ä¾èµäºæè¸é¦çè§è§-è¯­è¨æ¨¡ååå¾åçææ¨¡åçè´¨éåè½åãå¦æè¿äºåºç¡æ¨¡åå­å¨åå·®æå±éæ§ï¼å¯è½ä¼ä¼ éå°çæçæ°æ®åæ¨¡åä¸­ã</li>
<li><strong>â undistinguishable from scan renderingsâçèå´ï¼</strong> ç¨æ·ç ç©¶è¡¨æèªå¨çæçäººä½ä¸æ«ææ¸²æå¾âæ å¼âï¼ä½è¿éå¸¸æ¯å¨ç¹å®æ¡ä»¶ä¸ï¼ä¾å¦ï¼ç¹å®è§è§ãåç§ãåè¾¨çï¼è¿è¡çãå¨æç«¯ç¹åãå¤æåç§æé«ç²¾åº¦ç©çæ¨¡æç­åºæ¯ä¸ï¼æ¯å¦ä»è½ä¿æè¿ç§âæ å¼âçæ°´å¹³ï¼éè¦è¿ä¸æ­¥éªè¯ã</li>
<li><strong>SMPL æ¨¡åçå±éæ§ï¼</strong> æè¦æå°ä½¿ç¨ SMPL èº«ä½å½¢ç¶åæ°ãSMPL æ¨¡åè½ç¶å¹¿æ³ä½¿ç¨ï¼ä½å®æ¯ä¸ä¸ªåæ°åæ¨¡åï¼å¯è½æ æ³ææå°ææç»å¾®çäººä½è§£åå­¦ç»èæéæ åä½åã</li>
<li><strong>æè£èµäº§çæ¥æºåå¤æ ·æ§ï¼</strong> æè¦æå°âdetailed clothing imagesâåâclothing assetsâãè¿äºæè£èµäº§çæ¥æºãå¤æ ·æ§åè´¨éå°ç´æ¥å½±åçæç»æççå®æåå¯æ§æ§ãå¦ææè£èµäº§æ¬èº«æéï¼é£ä¹çæçå¤æ ·æ§ä¹ä¼åéã</li>
<li><strong>å®æ¶æ§æäº¤äºæ§ï¼</strong> æè¦å¼ºè°äºâfastâçæï¼ä½å¹¶æªæç¡®è¯´ææ¯å¦æ¯æå®æ¶äº¤äºå¼çæï¼è¿å¯¹äºæäºåºç¨ï¼å¦ VR/ARï¼è³å³éè¦ã</li>
</ul>
<hr />
<p>æ»çæ¥è¯´ï¼InfiniHuman æ¯ä¸é¡¹å·æå¼åæ§çå·¥ä½ï¼å®éè¿å·§å¦å°å©ç¨ç°æ AI åºç¡æ¨¡åï¼ä¸º 3D äººä½çæé¢åå¸¦æ¥äºé©å½æ§çæ°æ®çæåæ¨¡åè®­ç»èå¼ï¼ææå¨å¤ä¸ªåºç¨é¢åäº§çæ·±è¿å½±åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce
InfiniHuman, a framework that synergistically distills these models to produce
richly annotated human data at minimal cost and with theoretically unlimited
scalability.</li>
<li>We propose InfiniHumanData, a fully automatic pipeline that
leverages vision-language and image generation models to create a large-scale
multi-modal dataset.</li>
<li>Building on this dataset, we propose
InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body
shape, and clothing assets.</li>
<li>Extensive experiments demonstrate
significant improvements over state-of-the-art methods in visual quality,
generation speed, and controllability.</li>
<li>Our approach enables high-quality avatar
generation with fine-grained control at effectively unbounded scale through a
practical and affordable solution.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11650v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11650v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11649v1'></a></p>
<h2 id="physic-physically-plausible-3d-human-scene-interaction-and-contact-from-a-single-image"><a href="https://arxiv.org/abs/2510.11649v1">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a></h2>
<p><strong>Authors:</strong> Pradyumna Yalandur Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing metrically accurate humans and their surrounding scenes from a
single image is crucial for virtual reality, robotics, and comprehensive 3D
scene understanding. However, existing methods struggle with depth ambiguity,
occlusions, and physically inconsistent contacts. To address these challenges,
we introduce PhySIC, a framework for physically plausible Human-Scene
Interaction and Contact reconstruction. PhySIC recovers metrically consistent
SMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within
a shared coordinate frame from a single RGB image. Starting from coarse
monocular depth and body estimates, PhySIC performs occlusion-aware inpainting,
fuses visible depth with unscaled geometry for a robust metric scaffold, and
synthesizes missing support surfaces like floors. A confidence-weighted
optimization refines body pose, camera parameters, and global scale by jointly
enforcing depth alignment, contact priors, interpenetration avoidance, and 2D
reprojection consistency. Explicit occlusion masking safeguards invisible
regions against implausible configurations. PhySIC is efficient, requiring only
9 seconds for joint human-scene optimization and under 27 seconds end-to-end.
It naturally handles multiple humans, enabling reconstruction of diverse
interactions. Empirically, PhySIC outperforms single-image baselines, reducing
mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,
and improving contact F1 from 0.09 to 0.51. Qualitative results show realistic
foot-floor interactions, natural seating, and plausible reconstructions of
heavily occluded furniture. By converting a single image into a physically
plausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.
Our implementation is publicly available at https://yuxuan-xue.com/physic.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (2-3 å¥è¯)</strong></p>
<p>PhySIC æåºäºä¸ç§ä»åå¼  RGB å¾åéå»ºç©çä¸åççä¸ç»´äººä½-åºæ¯äº¤äºåæ¥è§¦çæ¡æ¶ãå®è§£å³äºç°ææ¹æ³å¨æ·±åº¦æ¨¡ç³ãé®æ¡åç©çä¸ä¸è´æ¥è§¦æ¹é¢çææï¼éè¿èåä¼åäººä½ç½æ ¼ãåºæ¯è¡¨é¢åé¡¶ç¹çº§æ¥è§¦å¾ï¼å®ç°äºåº¦éä¸è´ä¸ç©çå¯ä¿¡çéå»ºãè¯¥æ¹æ³æ¾èæé«äºéå»ºç²¾åº¦ï¼å°¤å¶æ¯å¨åºæ¯å ä½ãäººä½å§¿æåæ¥è§¦æ£æµæ¹é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>PhySIC çå³é®åæ°å¨äºå¶å¤é¶æ®µãèåä¼åçæ¹æ³ï¼ç¹å«å¼ºè°äºç©çåçæ§ï¼</p>
<ul>
<li><strong>é²æ£çåº¦éæ¯æ¶æå»ºï¼</strong> ä»ç²ç¥çåç®æ·±åº¦åäººä½ä¼°è®¡å¼å§ï¼éè¿é®æ¡æç¥ä¿®å¤ãå¯è§æ·±åº¦ä¸æªç¼©æ¾å ä½çèåï¼ä»¥åç¼ºå¤±æ¯æé¢ï¼å¦å°æ¿ï¼çåæï¼æå»ºäºä¸ä¸ªç¨³å¥çåº¦éæ¯æ¶ãè¿ææå°è§£å³äºåç®æ·±åº¦ä¼°è®¡çå°ºåº¦æ¨¡ç³æ§é®é¢ã</li>
<li><strong>ç½®ä¿¡åº¦å æçèåä¼åï¼</strong> å¼å¥äºä¸ä¸ªç½®ä¿¡åº¦å æçä¼åæ¡æ¶ï¼åæ¶ç²¾ç¼äººä½å§¿æãç¸æºåæ°åå¨å±å°ºåº¦ãè¿ä¸ªä¼åå¨éè¿èåå¼ºå¶æ§è¡ä»¥ä¸çº¦ææ¥ç¡®ä¿ç©çåçæ§ï¼<ul>
<li><strong>æ·±åº¦å¯¹é½ï¼</strong> ç¡®ä¿éå»ºç»æä¸åå§æ·±åº¦ä¼°è®¡ä¸è´ã</li>
<li><strong>æ¥è§¦åéªï¼</strong> é¼å±äººä½ä¸åºæ¯ä¹é´åçåççæ¥è§¦ï¼ä¾å¦ï¼èè¸©å¨å°é¢ä¸ï¼äººåå¨æ¤å­ä¸ï¼ã</li>
<li><strong>äºç©¿é¿åï¼</strong> æç¡®é²æ­¢äººä½ä¸åºæ¯ä¹é´åçä¸èªç¶çç©¿éã</li>
<li><strong>2D éæå½±ä¸è´æ§ï¼</strong> ç¡®ä¿ 3D éå»ºç»æå¨å¾åå¹³é¢ä¸çæå½±ä¸åå§ 2D å¾åä¸è´ã</li>
</ul>
</li>
<li><strong>æ¾å¼é®æ¡æ©ç ï¼</strong> ä½¿ç¨æ¾å¼é®æ¡æ©ç æ¥ä¿æ¤ä¸å¯è§åºåï¼é²æ­¢å¶è¢«ä¼åå°ä¸åççéç½®ä¸­ï¼è¿å¯¹äºå¤çå¤æé®æ¡åºæ¯è³å³éè¦ã</li>
<li><strong>é¡¶ç¹çº§æ¥è§¦å¾ï¼</strong> ä¸ä»éå»ºäººä½ååºæ¯ï¼è¿è¾åºç²¾ç»çé¡¶ç¹çº§æ¥è§¦å¾ï¼è¿å¯¹äºçè§£äº¤äºç»èéå¸¸æä»·å¼ã</li>
</ul>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<p>PhySIC å¯¹è®¡ç®æºè§è§é¢åå·ææ¾èçæ½å¨å½±åï¼</p>
<ul>
<li><strong>æ¨å¨åç® 3D éå»ºçè¾¹çï¼</strong> æ¾èæé«äºä»åå¼ å¾åè¿è¡äººä½-åºæ¯èå 3D éå»ºçç²¾åº¦åç©çåçæ§ï¼å°¤å¶æ¯å¨å¤çå¤æäº¤äºåé®æ¡æ¹é¢ã</li>
<li><strong>æ´å¯é ç 3D åºæ¯çè§£ï¼</strong> æä¾äºæ´åç¡®ãæ´å¯ä¿¡ç 3D äººä½ååºæ¯è¡¨ç¤ºï¼è¿å¯¹äºéè¦çè§£åºæ¯ä¸­ç©ä½åä»£çä¹é´å³ç³»çä¸æ¸¸ä»»å¡è³å³éè¦ã</li>
<li><strong>ä¸ºä¸æ¸¸åºç¨æä¾åºç¡ï¼</strong> å¶è¾åºçåº¦éä¸è´çäººä½ç½æ ¼ãåºæ¯è¡¨é¢åæ¥è§¦å¾ï¼å¯ä»¥ä½ä¸ºèæç°å®ãæºå¨äººãå¨ç»åäººæºäº¤äºç­é¢åæ´é«çº§åºç¨çåºç¡ã</li>
<li><strong>æçä¸å¯æ©å±æ§ï¼</strong> ç¸å¯¹é«æçä¼åæ¶é´ï¼9 ç§ç¨äºèåä¼åï¼27 ç§ç«¯å°ç«¯ï¼ä»¥åå¤çå¤äººçè½åï¼ä½¿å¶å¨å®éåºç¨ä¸­æ´å·å¸å¼åã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>èæç°å® (VR) åå¢å¼ºç°å® (AR)ï¼</strong> åå»ºæ´é¼çãäº¤äºæ§æ´å¼ºçèæç¯å¢ï¼ä¾å¦ï¼å°çå®ä¸ççäººä½ååºæ¯æ ç¼éæå°èæä¸çä¸­ã</li>
<li><strong>æºå¨äººå­¦ï¼</strong> å¸®å©æºå¨äººæ´å¥½å°çè§£å¶æä½ç¯å¢ä¸­çäººç±»åç©ä½ï¼ä»èå®ç°æ´å®å¨ãæ´èªç¶çåä½åå¯¼èªãä¾å¦ï¼æºå¨äººå¯ä»¥å©ç¨è¿äºä¿¡æ¯æ¥é¢æµäººç±»çæå¾æé¿åç¢°æã</li>
<li><strong>3D åå®¹åä½åå¨ç»ï¼</strong> èªå¨åä» 2D å¾åçæ 3D åºæ¯åè§è²å§¿æçè¿ç¨ï¼æå¤§å°ç®åäºå¨ç»å¸å 3D èºæ¯å®¶çå·¥ä½æµç¨ã</li>
<li><strong>äººæºäº¤äº (HCI)ï¼</strong> æ´å¥½å°çè§£ç¨æ·å¨ç©çç©ºé´ä¸­çè¡ä¸ºåæå¾ï¼ä»èè®¾è®¡æ´ç´è§ãæ´ååºå¼çäººæºçé¢ã</li>
<li><strong>æºè½çæ§åå®å¨ï¼</strong> åæçæ§è§é¢ä¸­çäººç±»æ´»å¨åäº¤äºï¼ä¾å¦æ£æµå¼å¸¸è¡ä¸ºæçè§£äººç¾¤å¨æã</li>
<li><strong>äººä½å§¿æä¼°è®¡åå½¢ç¶éå»ºï¼</strong> ä¸ºè¿äºä»»å¡æä¾æ´å¼ºçåºæ¯ä¸ä¸æåç©ççº¦æï¼ä»èæé«é²æ£æ§ååç¡®æ§ã</li>
</ul>
<p><strong>5. å¯ä»¥ä»æè¦ä¸­æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<p>å°½ç®¡ PhySIC åå¾äºæ¾èè¿å±ï¼ä½æè¦ä¸­ä»å¯æ¨æ­åºä¸äºæ½å¨å±éæ§ï¼</p>
<ul>
<li><strong>å¯¹åå§ä¼°è®¡çä¾èµï¼</strong> æè¦æå°âStarting from coarse monocular depth and body estimatesâï¼è¿æå³çè¯¥æ¹æ³å¯è½å¨ä¸å®ç¨åº¦ä¸ä¾èµäºè¿äºåå§ä¼°è®¡çè´¨éãå¦æåå§ä¼°è®¡éå¸¸å·®ï¼ä¼åè¿ç¨å¯è½é¾ä»¥æ¶æå°æä¼è§£ã</li>
<li><strong>å¤æåºæ¯çæ³åè½åï¼</strong> å°½ç®¡å®å¤çäºé®æ¡åå¤äººï¼ä½å¯¹äºæç«¯å¤æãé«åº¦æä¹±æåå«å¤§ééåæ§ç©ä½çåºæ¯ï¼å¶æ§è½å¯è½ä»æå¾è¿ä¸æ­¥éªè¯ãä¾å¦ï¼å¯¹äºéå¸¸è§çç©ä½å½¢ç¶æä¸å¸¸è§çäº¤äºæ¨¡å¼ï¼æ¥è§¦åéªçæææ§å¯è½åå°éå¶ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> å°½ç®¡æè¦ç§°å¶âé«æâï¼ä½ 27 ç§çç«¯å°ç«¯æ¶é´å¯¹äºæäºå®æ¶åºç¨ï¼å¦é«å¸§ç VR/ARï¼å¯è½ä»ç¶è¿é¿ã</li>
<li><strong>çº¹çåæè´¨éå»ºï¼</strong> æè¦ä¸»è¦å³æ³¨å ä½å½¢ç¶åæ¥è§¦ï¼å¹¶æªæåå¯¹åºæ¯çº¹çãæè´¨æåç§çéå»ºãè¿å¯è½æå³çè¾åºç 3D æ¨¡åå¨è§è§çå®ææ¹é¢ä»éè¿ä¸æ­¥å¤çã</li>
<li><strong>å¨æåºæ¯ï¼</strong> æè¦ä¸­æ²¡ææç¡®è¯´æå¶å¤çå¨æåºæ¯æè§é¢åºåçè½åãä»åå¼ å¾åéå»ºéå¸¸åå®åºæ¯æ¯éæçï¼æèè³å°å¨å¾åæè·ç¬é´æ¯éæçã</li>
<li><strong>âç©çåçæ§âçå®ä¹ï¼</strong> å°½ç®¡å¼ºè°äºç©çåçæ§ï¼ä½å¶å·ä½å®ç°ï¼ä¾å¦ï¼æ¥è§¦åéªåäºç©¿é¿åçç²¾ç¡®å»ºæ¨¡ï¼å¯è½ä»æ¯ç®åçãä¾å¦ï¼å®å¯è½ä¸èèæ©æ¦ãå¼¹æ§å½¢åç­æ´å¤æçç©çå±æ§ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼PhySIC æ¯ä¸é¡¹ä»¤äººå´å¥çç ç©¶ï¼å®éè¿å¼å¥ä¸å¥æ°é¢çèåä¼åç­ç¥åç©ççº¦æï¼æ¾èæåäºä»åå¼ å¾åè¿è¡ 3D äººä½-åºæ¯éå»ºçè´¨éåå®ç¨æ§ãå¶å¯¹ç©çåçæ§çå¼ºè°æ¯å¶æ ¸å¿ä¼å¿ï¼ææå¨å¤ä¸ªåºç¨é¢åäº§çæ·±è¿å½±åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges,
we introduce PhySIC, a framework for physically plausible Human-Scene
Interaction and Contact reconstruction.</li>
<li>Empirically, PhySIC outperforms single-image baselines, reducing
mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,
and improving contact F1 from 0.09 to 0.51.</li>
<li>Qualitative results show realistic
foot-floor interactions, natural seating, and plausible reconstructions of
heavily occluded furniture.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11649v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11649v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11606v1'></a></p>
<h2 id="expvid-a-benchmark-for-experiment-video-understanding-reasoning"><a href="https://arxiv.org/abs/2510.11606v1">ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</a></h2>
<p><strong>Authors:</strong> Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) hold promise for accelerating
scientific discovery by interpreting complex experimental procedures. However,
their true capabilities are poorly understood, as existing benchmarks neglect
the fine-grained and long-horizon nature of authentic laboratory work,
especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the
first benchmark designed to systematically evaluate MLLMs on scientific
experiment videos. Curated from peer-reviewed video publications, ExpVid
features a new three-level task hierarchy that mirrors the scientific process:
(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural
Understanding of step order and completeness; and (3) Scientific Reasoning that
connects the full experiment to its published conclusions. Our vision-centric
annotation pipeline, combining automated generation with multi-disciplinary
expert validation, ensures that tasks require visual grounding. We evaluate 19
leading MLLMs on ExpVid and find that while they excel at coarse-grained
recognition, they struggle with disambiguating fine details, tracking state
changes over time, and linking experimental procedures to scientific outcomes.
Our results reveal a notable performance gap between proprietary and
open-source models, particularly in high-order reasoning. ExpVid not only
provides a diagnostic tool but also charts a roadmap for developing MLLMs
capable of becoming trustworthy partners in scientific experimentation.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºYicheng Xuç­äººæ°åçè®ºæâExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoningâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼ExpVidï¼ä¸ä¸ªç¨äºå®éªè§é¢çè§£ä¸æ¨ççåºå</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æåºåå¨è¯ä¼°å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼çè§£ç§å­¦å®éªè§é¢æ¹é¢çä¸è¶³ãç°æåºåæªè½ååææçå®å®éªå®¤å·¥ä½ï¼ç¹å«æ¯æ¹¿å®éªå®¤ç¯å¢ï¼ä¸­ç»ç²åº¦ãé¿æ¶ç¨çå¤ææ§ï¼å¯¼è´å¯¹MLLMså¨å éç§å­¦åç°æ¹é¢ççå®è½åçè§£ä¸è¶³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¼å¥ExpVidåºåï¼</strong> é¦æ¬¡ç³»ç»æ§å°è¯ä¼°MLLMså¨ç§å­¦å®éªè§é¢ä¸çè¡¨ç°ï¼ç¹å«å³æ³¨æ¹¿å®éªå®¤ç¯å¢ã
*   <strong>ä¸çº§ä»»å¡å±æ¬¡ç»æï¼</strong> ExpVidè®¾è®¡äºä¸ä¸ªæ¨¡ä»¿ç§å­¦æµç¨çä¸çº§ä»»å¡å±æ¬¡ç»æï¼
    *   <strong>ä¸çº§ï¼ç»ç²åº¦æç¥</strong>ï¼å·¥å·ãææåå¨ä½çè¯å«ï¼ã
    *   <strong>äºçº§ï¼ç¨åºçè§£</strong>ï¼æ­¥éª¤é¡ºåºåå®æ´æ§ççè§£ï¼ã
    *   <strong>ä¸çº§ï¼ç§å­¦æ¨ç</strong>ï¼å°æ´ä¸ªå®éªä¸å·²åè¡¨çç»è®ºèç³»èµ·æ¥ï¼ã
*   <strong>ä»¥è§è§ä¸ºä¸­å¿çæ æ³¨æµç¨ï¼</strong> ç»åèªå¨åçæåå¤å­¦ç§ä¸å®¶éªè¯ï¼ç¡®ä¿ä»»å¡éè¦è§è§åºç¡ï¼é¿åä»ä¾èµææ¬æèæ¯ç¥è¯ã
*   <strong>å¹¿æ³çæ¨¡åè¯ä¼°ï¼</strong> å¨ExpVidä¸è¯ä¼°äº19ä¸ªä¸»æµMLLMsï¼åæ¬å¼æºåä¸ææ¨¡åï¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>MLLMså¨ç²ç²åº¦è¯å«æ¹é¢è¡¨ç°è¯å¥½ï¼</strong> æ¨¡åå¨è¯å«ç²ç²åº¦ä¿¡æ¯æ¹é¢è¡¨ç°åºè²ã
*   <strong>MLLMså¨ç»ç²åº¦çè§£åé«é¶æ¨çæ¹é¢å­å¨ææï¼</strong> æ¨¡åå¨ä»¥ä¸æ¹é¢è¡¨ç°ä¸ä½³ï¼
    *   åºåç»å¾®ç»èã
    *   è·è¸ªéæ¶é´ååçå®éªç¶æã
    *   å°å®éªè¿ç¨ä¸ç§å­¦ç»æèç³»èµ·æ¥ã
*   <strong>ä¸ææ¨¡åä¸å¼æºæ¨¡åä¹é´çæ¾èæ§è½å·®è·ï¼</strong> å°¤å¶æ¯å¨é«é¶æ¨çä»»å¡ä¸­ï¼ä¸ææ¨¡åçè¡¨ç°ææ¾ä¼äºå¼æºæ¨¡åã
*   <strong>è¯æ­å·¥å·åè·¯çº¿å¾ï¼</strong> ExpVidä¸ä»æä¾äºä¸ä¸ªè¯æ­å·¥å·æ¥è¯å«MLLMsçå¼±ç¹ï¼ä¹ä¸ºå¼åè½å¤æä¸ºç§å­¦å®éªä¸­å¼å¾ä¿¡èµçåä½ä¼ä¼´çMLLMsææäºæ¹åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é¢åè¦çèå´æéï¼</strong> ExpVidç®åä¸»è¦å³æ³¨æ¹¿å®éªå®¤å®éªï¼å°æªæ¶µçææç§å­¦æ¢ç©¶é¢åï¼ä¾å¦ç©çå­¦ï¼æ¶åç¬ç¹å®éªè®¾å¤åæ½è±¡ç°è±¡ï¼æçº¯è®¡ç®å®éªã
*   <strong>æ¨çä»»å¡çæ·±åº¦ï¼</strong> ä¸çº§æ¨çä»»å¡è¯ä¼°çæ¯ç»æï¼ä½æªè½æ·±å¥éæå°å®éªä¸ç»è®ºèç³»èµ·æ¥çåºå±æ¨çè¿ç¨ï¼ä¾å¦ï¼æç»´é¾ï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±é¢åè¦çï¼</strong> å°åºåæ©å±å°ç©çå­¦ãè®¡ç®ç§å­¦åå¤§åå·¥ç¨æµè¯ç­æ´å¤ç§å­¦é¢åã
*   <strong>æ·±åæ¨çè¯ä¼°ï¼</strong> å¼åè½å¤è¯ä¼°MLLMså¨å®éªä¸­æ´æ·±å±æ¬¡æ¨çè¿ç¨ï¼å¦å æé¾ãåè®¾æ£éªï¼çæºå¶ã
*   <strong>æåå¼æºMLLMsçé«é¶æ¨çè½åï¼</strong> ç¼©å°å¼æºæ¨¡åä¸ä¸ææ¨¡åå¨å¤ææ¨çä»»å¡ä¸çæ§è½å·®è·ã
*   <strong>ä¼åæ¨¡åå¯¹é¿æ¶ç¨è§é¢æ°æ®çå©ç¨ï¼</strong> è§£å³æ¨¡åå¨å¤çåä½è¾å¥æ¶å¯è½åºç°çé¥±åæå¹²æ°é®é¢ï¼ä»¥æ´å¥½å°å©ç¨æ©å±çæ¶é´ä¸ä¸æã</p>
<p>è¿ç¯è®ºæéè¿å¼å¥ExpVidåºåï¼ä¸ºè¯ä¼°åæ¨å¨MLLMså¨çè§£çå®ç§å­¦å®éªè§é¢æ¹é¢çè½åæä¾äºéè¦è´¡ç®ï¼ä¸ºæªæ¥å¼åæ´æºè½ãæ´å¯é çç§å­¦AIå©æå¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce ExpVid, the
first benchmark designed to systematically evaluate MLLMs on scientific
experiment videos.</li>
<li>Curated from peer-reviewed video publications, ExpVid
features a new three-level task hierarchy that mirrors the scientific process:
(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural
Understanding of step order and completeness; and (3) Scientific Reasoning that
connects the full experiment to its published conclusions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11606v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11606v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-14 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
