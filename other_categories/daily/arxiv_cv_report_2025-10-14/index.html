<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-14 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-13/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-15/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-14">Arxiv Computer Vision Papers - 2025-10-14</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-10-13" class="nav-link">Arxiv 计算机视觉每日报告执行摘要 (2025-10-13)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#compositional-zero-shot-learning-a-survey" class="nav-link">Compositional Zero-Shot Learning: A Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation" class="nav-link">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#a-survey-on-agentic-multimodal-large-language-models" class="nav-link">A Survey on Agentic Multimodal Large Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#image-to-video-transfer-learning-based-on-image-language-foundation-models-a-comprehensive-survey" class="nav-link">Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</a>
                </li>
                <li class="nav-item">
                    <a href="#ev4dgs-novel-view-rendering-of-non-rigid-objects-from-monocular-event-streams" class="nav-link">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a>
                </li>
                <li class="nav-item">
                    <a href="#dit360-high-fidelity-panoramic-image-generation-via-hybrid-training" class="nav-link">DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</a>
                </li>
                <li class="nav-item">
                    <a href="#scaling-language-centric-omnimodal-representation-learning" class="nav-link">Scaling Language-Centric Omnimodal Representation Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#infinihuman-infinite-3d-human-creation-with-precise-control" class="nav-link">InfiniHuman: Infinite 3D Human Creation with Precise Control</a>
                </li>
                <li class="nav-item">
                    <a href="#physic-physically-plausible-3d-human-scene-interaction-and-contact-from-a-single-image" class="nav-link">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a>
                </li>
                <li class="nav-item">
                    <a href="#expvid-a-benchmark-for-experiment-video-understanding-reasoning" class="nav-link">ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-14">Arxiv Computer Vision Papers - 2025-10-14</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-10-13">Arxiv 计算机视觉每日报告执行摘要 (2025-10-13)</h2>
<p><strong>概述：</strong></p>
<p>今天的 Arxiv 计算机视觉论文主要围绕<strong>多模态学习、3D 内容生成与理解以及视频分析</strong>展开。显著趋势包括利用大型语言模型 (LLMs) 增强视觉任务，以及在复杂场景下对 3D 人体、物体和视频进行更精细的建模和生成。</p>
<p><strong>主要主题和趋势：</strong></p>
<ol>
<li><strong>多模态与 LLM 融合：</strong> 多篇论文探讨了将视觉与语言模型结合，以实现更强大的零样本学习、代理式多模态交互和全模态表示学习。这表明 LLMs 在计算机视觉领域的渗透日益加深，成为解决复杂推理和泛化问题的关键。</li>
<li><strong>3D 内容生成与理解：</strong> 3D 人体、场景和非刚性物体的生成与交互是另一个突出主题。从单目事件流生成新视角，到高保真全景图像生成，再到精确控制的 3D 人体创建和物理可信的 3D 人机交互，都显示出该领域在真实感和可控性方面的显著进步。</li>
<li><strong>视频理解与分割：</strong> 视频分析仍然是活跃的研究领域，特别是在复杂视频对象分割和实验视频理解与推理方面。这反映了对动态场景和时间序列数据更深层次理解的需求。</li>
<li><strong>零样本与泛化能力：</strong> 零样本学习作为一种重要的泛化范式，在多模态背景下得到了广泛关注，旨在使模型能够处理未见过的数据类别。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"A Survey on Agentic Multimodal Large Language Models" (Huanjin Yao et al.)：</strong> 这篇综述非常及时，因为它深入探讨了代理式多模态 LLMs，这是一个新兴且极具潜力的研究方向，预示着未来 AI 系统将具备更强的自主决策和多模态交互能力。</li>
<li><strong>"InfiniHuman: Infinite 3D Human Creation with Precise Control" (Yuxuan Xue et al.)：</strong> 这篇论文在 3D 人体生成方面取得了显著突破，强调了“无限”和“精确控制”，这对于虚拟现实、游戏、电影制作以及数字人领域具有巨大应用价值。</li>
<li><strong>"PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image" (Pradyumna Yalandur Muralidhar et al.)：</strong> 从单张图像推断物理可信的 3D 人机交互和接触是一个极具挑战性的任务，该工作在真实感和物理一致性方面迈出了重要一步，对机器人、人机交互和场景理解至关重要。</li>
<li><strong>"Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams" (Takuya Nakabayashi et al.)：</strong> 利用单目事件流进行非刚性物体的新视角渲染，展示了事件相机在处理高速运动和低光照条件下的独特优势，为动态场景重建提供了新思路。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>代理式多模态 LLMs：</strong> 将 LLMs 提升到具有代理能力，使其能够规划、执行和反思多模态任务，是未来 AI 发展的重要方向。</li>
<li><strong>事件相机在 3D 重建中的应用：</strong> 事件流数据在处理高速非刚性物体和动态场景方面展现出巨大潜力。</li>
<li><strong>高保真、可控的 3D 内容生成：</strong> 尤其是在 3D 人体和全景图像方面，强调了对生成内容细节和属性的精细控制。</li>
<li><strong>物理可信的 3D 交互建模：</strong> 不仅仅是几何上的匹配，更要考虑物理规律和接触力学。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>对于希望深入了解前沿进展的研究人员，建议优先阅读以下论文：</p>
<ul>
<li><strong>"A Survey on Agentic Multimodal Large Language Models" (Huanjin Yao et al.)：</strong> 了解该领域最新且最具潜力的方向。</li>
<li><strong>"InfiniHuman: Infinite 3D Human Creation with Precise Control" (Yuxuan Xue et al.)：</strong> 如果您对 3D 人体生成和数字人技术感兴趣。</li>
<li><strong>"PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image" (Pradyumna Yalandur Muralidhar et al.)：</strong> 如果您关注 3D 场景理解、人机交互或机器人领域。</li>
<li><strong>"Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams" (Takuya Nakabayashi et al.)：</strong> 如果您对事件相机和动态 3D 重建感兴趣。</li>
<li><strong>"Scaling Language-Centric Omnimodal Representation Learning" (Chenghao Xiao et al.)：</strong> 如果您关注多模态基础模型和通用表示学习。</li>
</ul>
<p>这份摘要旨在帮助您快速把握今日 Arxiv 计算机视觉领域的关键发展，并为您的进一步研究提供方向。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></li>
<li><a href="#2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></li>
<li><a href="#2510.10991v1">A Survey on Agentic Multimodal Large Language Models</a></li>
<li><a href="#2510.10671v1">Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</a></li>
<li><a href="#2510.11717v1">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a></li>
<li><a href="#2510.11712v1">DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</a></li>
<li><a href="#2510.11693v1">Scaling Language-Centric Omnimodal Representation Learning</a></li>
<li><a href="#2510.11650v1">InfiniHuman: Infinite 3D Human Creation with Precise Control</a></li>
<li><a href="#2510.11649v1">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a></li>
<li><a href="#2510.11606v1">ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.11106v1'></a></p>
<h2 id="compositional-zero-shot-learning-a-survey"><a href="https://arxiv.org/abs/2510.11106v1">Compositional Zero-Shot Learning: A Survey</a></h2>
<p><strong>Authors:</strong> Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision
that enables models to recognize unseen combinations of known attributes and
objects during inference, addressing the combinatorial challenge of requiring
training data for every possible composition. This is particularly challenging
because the visual appearance of primitives is highly contextual; for example,
<code>small'' cats appear visually distinct from</code>older'' ones, and <code>wet'' cars
differ significantly from</code>wet'' cats. Effectively modeling this contextuality
and the inherent compositionality is crucial for robust compositional zero-shot
recognition. This paper presents, to our knowledge, the first comprehensive
survey specifically focused on Compositional Zero-Shot Learning. We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement. We provide a detailed comparative analysis of
these methods, highlighting their core advantages and limitations in different
problem settings, such as closed-world and open-world CZSL. Finally, we
identify the most significant open challenges and outline promising future
research directions. This survey aims to serve as a foundational resource to
guide and inspire further advancements in this fascinating and important field.
Papers studied in this survey with their official code are available on our
github: https://github.com/ans92/Compositional-Zero-Shot-Learning</p>
<p><strong>Analysis:</strong></p>
<p>这篇由Ans Munir等人撰写的论文《Compositional Zero-Shot Learning: A Survey》全面概述了组合式零样本学习（CZSL）领域，该领域旨在使计算机视觉模型能够识别已知属性和对象的新颖组合，而无需针对每种组合进行显式训练。</p>
<p><strong>1. 主要问题或研究问题：</strong>
CZSL的核心挑战在于，模型需要在推理时识别训练中未曾见过的属性与对象的组合。这尤其困难，因为基元（如属性和对象）的视觉外观具有高度语境依赖性（例如，“小”猫与“老”猫的视觉差异，以及“湿”车与“湿”猫的显著不同）。有效地建模这种语境性和固有的组合性对于鲁棒的组合式零样本识别至关重要。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文首次提出了一个全面的CZSL方法分类法，其核心思想是“解耦”（disentanglement）。分类法将现有方法分为四大类：
*   <strong>无显式解耦（No Explicit Disentanglement）：</strong> 将属性-对象组合视为单一单元，通过整体嵌入或直接融合机制进行建模。
*   <strong>文本解耦（Textual Disentanglement）：</strong> 在语言空间中分离基元的语义嵌入，通过独立概念表示实现系统组合。
*   <strong>视觉解耦（Visual Disentanglement）：</strong> 在视觉特征空间中分离属性和对象的视觉特征，将这些基元解耦为可组合的表示。
*   <strong>跨模态（混合）解耦（Cross-Modal (Hybrid) Disentanglement）：</strong> 同时在视觉和文本空间中解耦基元，并通过跨模态对齐整合互补信息。</p>
<p>在第二层，方法根据其建模属性和处理组合挑战的策略进一步分类，包括基于原型建模、合成嵌入、因果推理和约束驱动等。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>骨干网络效应：</strong> 早期使用ResNet编码器的方法性能较低，而基于CLIP编码器（从2023年开始）的最新方法在准确性上显著提高，表明视觉-语言预训练作为标准骨干网络的优势。
*   <strong>解耦策略的有效性：</strong>
    *   <strong>无显式解耦</strong> 方法通常处于性能谱的低端，凸显了将组合视为整体的局限性。
    *   <strong>文本解耦</strong> 方法比无解耦基线有所改进，但受限于仅依赖语言，无法捕捉视觉属性的变异性。
    *   <strong>视觉解耦</strong> 是最活跃且最具竞争力的方法类别，在闭环设置中表现出显著优势，甚至在某些情况下超越了混合方法。
    *   <strong>跨模态解耦</strong> 虽然起步较晚（主要在2024年出现），但在闭环设置中显示出最强的潜力，其编码器增强方法（如CAILA和Troika）在多样化数据集上已能与视觉基线匹敌。</p>
<ul>
<li><strong>开放世界设置：</strong> 在开放世界设置中，性能趋势与闭环设置有所不同。无解耦方法仍然是最弱的，但原始级别策略（如KG-SP）有时能与语境感知模型匹敌。文本解耦继续提供增益。视觉解耦仍然是最有效的方法，其中增强型（Retrieval）、原型中心型（CLUSPRO）和条件属性模型（CDS-CZSL）表现强劲。跨模态方法在开放世界评估中落后于顶级视觉方法，表明在扩展的标签空间下，仅视觉基础可能比联合视觉-文本对齐更可靠。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>无显式解耦：</strong> 无法捕捉属性和对象的独特语义或其语境变异性，难以泛化到新颖组合。
*   <strong>文本解耦：</strong> 无法捕捉属性丰富的视觉变异性，忽略了图像中实际存在的纠缠。
*   <strong>视觉解耦：</strong> 强制严格分离可能过度简化自然依赖关系，丢弃有助于识别的语境线索。实现干净分离在实践中也很困难，特别是对于微妙或强语境依赖的属性。
*   <strong>跨模态解耦：</strong> 存在架构和计算复杂性，对齐两种模态通常需要额外的模块。一致的跨模态基础仍然难以实现，因为视觉基元在外观上差异很大，而文本基元相对稳定。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>建模基元和语境性：</strong> 进一步完善视觉解耦策略，并更加重视开发跨模态框架，以可扩展和鲁棒的方式捕捉语境性。
*   <strong>扩展到开放世界评估：</strong> 开发内在鲁棒于不可行组合的模型，缩小闭环和开放世界性能之间的差距，而无需显式可行性计算。
*   <strong>泛化到未见基元：</strong> 设计能够动态适应未见对象属性表示的模型，并探索跨模态解耦以利用语言模型的语义扩展能力。
*   <strong>利用大型多模态模型（LMMs）：</strong> 建立严格的评估协议以解决数据污染问题，并开发利用LMMs推理能力同时约束其学习真实组合结构而非表面相关性的适应策略。</p>
<p>总之，这篇综述为CZSL领域提供了一个结构化的分析框架，系统地回顾了现有方法，并指出了未来研究的关键方向，旨在推动该领域实现更鲁棒、可扩展和透明的组合式识别系统。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11106v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11106v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11063v1'></a></p>
<h2 id="lsvos-2025-challenge-report-recent-advances-in-complex-video-object-segmentation"><a href="https://arxiv.org/abs/2510.11063v1">LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</a></h2>
<p><strong>Authors:</strong> Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard <script type="math/tex">{J}</script>, <script type="math/tex">F</script>, and <script type="math/tex">{J\&F}</script> metrics for
VOS and RVOS, while MOSEv2 adopts <script type="math/tex">{J\&\dot{F}}</script> as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation”的摘要，由Chang Liu等撰写，内容基于您提供的PDF全文：</p>
<p><strong>论文摘要：LSVOS 2025 挑战赛报告：复杂视频目标分割的最新进展</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该报告旨在概述第七届大规模视频目标分割（LSVOS）挑战赛，该挑战赛与ICCV 2025同期举行。核心问题是推动视频目标分割（VOS）和指代视频目标分割（RVOS）在更复杂、更真实的视频场景中实现鲁棒性、长期一致性和泛化能力，超越现有策展基准的局限。特别是，新引入的复杂VOS (MOSEv2) 赛道旨在解决现有VOS方法在处理密集小物体、频繁出现/消失事件、严重遮挡、恶劣天气和光照等极端真实世界条件下的不足。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>引入MOSEv2数据集和赛道：</strong> MOSEv2是MOSEv1的继任者，显著增加了难度，包含更多挑战性场景，如密集小物体、频繁出现/消失、严重遮挡、恶劣天气和光照等，旨在推动长期一致性和泛化能力。
*   <strong>MOSEv2的新评估指标：</strong> 为了更准确地评估跨对象尺度和消失情况下的性能，MOSEv2引入了新的指标，包括自适应边界精度（<script type="math/tex">\dot{F}</script>）和以<script type="math/tex">\text{J\&}\dot{F}</script>作为主要排名指标。
*   <strong>顶尖解决方案的技术趋势：</strong>
    *   <strong>MOSEv2赛道：</strong> 领先的解决方案（如DSS-Track的SeC框架）利用SAM-2（Segment Anything Model 2）和InternVL-2.5-4B等基础模型，通过增强的概念建模、更大的记忆尺寸（N=22）来捕获长期跨帧关系，并结合概念感知记忆（Concept-aware Memory）和场景自适应激活策略来处理复杂时空场景。
    *   <strong>VOS赛道：</strong> 顶尖方法（如NJUST-KMG）基于SAM2模型，通过置信度引导的多模型集成策略（结合SAM2Long、Cutie、LiVOS、XMem等）来增强鲁棒性，并采用多任务损失函数进行训练，以应对复杂场景。
    *   <strong>RVOS赛道：</strong> 领先方法（如SaSaSa2VA）结合多模态大语言模型（MLLM）和SAM2，通过关键帧压缩（KFC）和扩展[SEG] token数量来处理视频和文本指令，以捕获全局视频上下文和处理多样化的时间变化。还引入了视频语义匹配模块（VLC）以验证视频-文本对应关系。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>MOSEv2的挑战性：</strong> MOSEv2赛道的结果表明，即使是顶尖方法，其<script type="math/tex">\text{J\&}\dot{F}</script>分数也相对较低（第一名DSS-Track为39.89%），这突出显示了现代VOS方法在复杂真实场景中仍有显著提升空间。
*   <strong>LLM/MLLM组件的日益重要性：</strong> 报告强调，大语言模型（LLM）和多模态大语言模型（MLLM）已成为许多管道中的默认组件，尤其是在语言引导的视频任务中，这凸显了它们在视频理解方面的潜力。
*   <strong>记忆感知传播的重要性：</strong> 顶尖解决方案普遍采用了增强的记忆机制，无论是用于捕获长期跨帧关系，还是用于处理对象出现/消失，都强调了记忆管理在视频分割中的关键作用。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>MOSEv2的挑战性：</strong> 尽管取得了进展，但MOSEv2赛道的结果表明，现有最先进系统在复杂、真实的场景中仍然面临挑战，导致性能显著下降。
*   <strong>Sa2VA的局限性：</strong> 在RVOS赛道中，Sa2VA模型在训练时仅采样5帧，并且使用单个[SEG] token来传递信息，这限制了MLLM捕获全局视频上下文的能力，并且难以适应对象位置、形状和外观的频繁变化。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更深层次的LLM/MLLM集成：</strong> 报告预测，LLM/MLLM的更深层次集成将继续提升性能，尤其是在语言感知视频分割方面。
*   <strong>解决最困难的失败模式：</strong> 未来的研究将重点关注通过本次挑战赛结果和真实世界用例识别出的最困难的失败模式，以进一步推动视频目标分割及相关研究的前沿。</p>
<p>总而言之，LSVOS 2025挑战赛报告不仅展示了视频目标分割领域的最新进展，还通过引入MOSEv2数据集和新的评估指标，以及顶尖解决方案中涌现出的LLM/MLLM和记忆感知传播等趋势，为该领域的未来研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11063v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11063v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.10991v1'></a></p>
<h2 id="a-survey-on-agentic-multimodal-large-language-models"><a href="https://arxiv.org/abs/2510.10991v1">A Survey on Agentic Multimodal Large Language Models</a></h2>
<p><strong>Authors:</strong> Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, Dacheng Tao</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>With the recent emergence of revolutionary autonomous agentic systems,
research community is witnessing a significant shift from traditional static,
passive, and domain-specific AI agents toward more dynamic, proactive, and
generalizable agentic AI. Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we
explore the emerging paradigm of agentic MLLMs, delineating their conceptual
foundations and distinguishing characteristics from conventional MLLM-based
agents. We establish a conceptual framework that organizes agentic MLLMs along
three fundamental dimensions: (i) Agentic internal intelligence functions as
the system's commander, enabling accurate long-horizon planning through
reasoning, reflection, and memory; (ii) Agentic external tool invocation,
whereby models proactively use various external tools to extend their
problem-solving capabilities beyond their intrinsic knowledge; and (iii)
Agentic environment interaction further situates models within virtual or
physical environments, allowing them to take actions, adapt strategies, and
sustain goal-directed behavior in dynamic real-world scenarios. To further
accelerate research in this area for the community, we compile open-source
training frameworks, training and evaluation datasets for developing agentic
MLLMs. Finally, we review the downstream applications of agentic MLLMs and
outline future research directions for this rapidly evolving field. To
continuously track developments in this rapidly evolving field, we will also
actively update a public repository at
https://github.com/HJYao00/Awesome-Agentic-MLLMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Huanjin Yao等人撰写的论文“A Survey on Agentic Multimodal Large Language Models”的全面摘要。</p>
<hr />
<h3 id="a-survey-on-agentic-multimodal-large-language-models_1">论文《A Survey on Agentic Multimodal Large Language Models》全面摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前人工智能领域从传统静态、被动、领域专用型AI智能体向更动态、主动、通用型Agentic AI智能体转变的趋势。具体来说，它关注Agentic多模态大语言模型（Agentic MLLMs）这一新兴范式，并试图提供一个全面的综述，以阐明其概念基础、与传统MLLM智能体的区别、关键能力、训练与评估资源，以及未来的研究方向。核心问题是：如何系统地理解和推动Agentic MLLMs的发展，使其能够自主地进行长周期规划、主动使用工具并与动态环境交互，从而实现更接近通用人工智能（AGI）的目标。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文的主要贡献在于构建了一个全面的概念框架，将Agentic MLLMs的能力组织成三个基本维度：</p>
<ul>
<li><strong>Agentic 内部智能（Agentic Internal Intelligence）：</strong> 作为系统的“指挥官”，通过推理、反思和记忆实现准确的长周期规划。论文详细探讨了基于提示、SFT和RL的推理方法，以及显式和隐式反思机制，并区分了上下文记忆和外部记忆系统（包括启发式驱动和推理驱动）。</li>
<li><strong>Agentic 外部工具调用（Agentic External Tool Invocation）：</strong> 模型主动利用各种外部工具（如信息搜索、代码执行、视觉处理）来扩展其超越内在知识的问题解决能力。论文分类讨论了Agentic搜索、Agentic编码和Agentic视觉处理（包括图像裁剪、图像操作和图像生成）等工具的使用。</li>
<li><strong>Agentic 环境交互（Agentic Environment Interaction）：</strong> 模型能够与虚拟或物理环境进行交互，从而采取行动、调整策略并在动态真实世界场景中维持目标导向的行为。这包括虚拟GUI智能体（通过离线演示或在线交互学习）和物理具身AI（涉及具身感知、规划、导航和操作）。</li>
</ul>
<p>此外，论文还：
*   <strong>形式化了Agentic MLLMs与传统MLLM智能体的区别：</strong> 强调了Agentic MLLMs的动态工作流、主动行动执行和跨领域泛化能力。
*   <strong>系统梳理了训练与评估资源：</strong> 整理了开源训练框架（包括Agentic CPT/SFT和RL框架）以及用于开发和评估Agentic MLLMs的训练和评估数据集。</p>
<p><strong>3. 主要结果及其意义：</strong>
该论文本身是一篇综述，因此其“结果”体现在对现有研究的全面梳理和洞察上，而非实验结果。主要意义包括：</p>
<ul>
<li><strong>提供统一的视角：</strong> 首次全面地对Agentic MLLMs领域进行了系统性分类和总结，为研究人员提供了一个清晰、全面的领域概览和概念框架。</li>
<li><strong>揭示发展趋势：</strong> 强调了Agentic MLLMs在自主决策、主动规划、工具使用和环境交互方面的优势，预示了AI智能体向更通用、更智能方向发展的趋势。</li>
<li><strong>促进社区发展：</strong> 汇编了大量的开源资源（训练框架、数据集），为研究人员进入和加速该领域的研究提供了宝贵的起点。</li>
<li><strong>突出应用潜力：</strong> 展示了Agentic MLLMs在深度研究、具身AI、医疗保健、GUI智能体、自动驾驶和推荐系统等广泛下游应用中的巨大潜力，表明其能够处理复杂的真实世界场景。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中明确指出了Agentic MLLMs当前面临的挑战和局限性：</p>
<ul>
<li><strong>行动空间受限：</strong> 现有模型的行动空间和可访问工具范围通常受限于单一类型，缺乏更广泛的外部工具和服务集成。</li>
<li><strong>效率问题：</strong> 多轮推理和外部工具调用等迭代过程显著增加了计算和推理开销，导致训练和推理效率低下，难以满足实时应用和大规模部署的需求。</li>
<li><strong>长周期记忆的局限性：</strong> 当前系统的记忆有效长度高度受限，难以在更长的时间跨度内维持连贯的知识，且多模态记忆管理方面的研究不足。</li>
<li><strong>训练与评估数据稀缺：</strong> 专门为Agentic行为设计的训练数据集仍然稀缺，尤其是在多模态领域缺乏足够的探索。现有评估基准主要关注Agentic行为的特定方面，而记忆利用、跨工具调用协调等能力仍缺乏有效的评估数据集。</li>
<li><strong>安全问题：</strong> 随着Agentic MLLMs在规划、工具调用和环境交互方面日益自主，确保其安全性（如避免意外后果、处理不正确或有害信息、防止行为不稳定）成为一个关键的研究优先事项。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>更丰富的行动空间：</strong> 开发能够无缝集成更多外部工具和服务（如数据分析平台、仿真环境、多模态传感器、交互式API）的Agentic MLLMs。</li>
<li><strong>提高效率：</strong> 专注于提升Agentic MLLMs的计算效率，加速训练和推理过程，同时不牺牲性能，以支持实时应用和大规模部署。</li>
<li><strong>长周期Agentic记忆：</strong> 设计持久性记忆架构，使模型能够跨长时间跨度积累、组织和检索知识，并能够处理海量的多模态数据流，实现个性化、持续协作和自适应问题解决。</li>
<li><strong>Agentic训练与评估数据集：</strong> 开发有效且高效的方法来合成高质量的多模态Agentic轨迹数据。同时，构建更全面的评估基准，以评估记忆利用、跨工具调用协调以及行动执行的准确性等关键Agentic能力。</li>
<li><strong>Agentic MLLMs的安全性：</strong> 结合严格的基准测试、对抗性压力测试和规范框架的集成，确保Agentic MLLMs在更广泛的自主性方面保持可靠、可控并与人类意图对齐。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.10991v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.10991v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.10671v1'></a></p>
<h2 id="image-to-video-transfer-learning-based-on-image-language-foundation-models-a-comprehensive-survey"><a href="https://arxiv.org/abs/2510.10671v1">Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</a></h2>
<p><strong>Authors:</strong> Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai</p>
<p><strong>Published:</strong> 2025-10-12</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Image-Language Foundation Models (ILFM) have demonstrated remarkable success
in image-text understanding/generation tasks, providing transferable multimodal
representations that generalize across diverse downstream image-based tasks.
The advancement of video-text research has spurred growing interest in
extending image-based models to the video domain. This paradigm, known as
image-to-video transfer learning, succeeds in alleviating the substantial data
and computational requirements associated with training video-language
foundation models from scratch for video-text learning. This survey provides
the first comprehensive review of this emerging field, which begins by
summarizing the widely used ILFM and their capabilities. We then systematically
classify existing image-to-video transfer learning strategies into two
categories: frozen features and modified features, depending on whether the
original representations from ILFM are preserved or undergo modifications.
Building upon the task-specific nature of image-to-video transfer, this survey
methodically elaborates these strategies and details their applications across
a spectrum of video-text learning tasks, ranging from fine-grained (e.g.,
spatio-temporal video grounding) to coarse-grained (e.g., video question
answering). We further present a detailed experimental analysis to investigate
the efficacy of different image-to-video transfer learning paradigms on a range
of downstream video understanding tasks. Finally, we identify prevailing
challenges and highlight promising directions for future research. By offering
a comprehensive and structured overview, this survey aims to establish a
structured roadmap for advancing video-text learning based on existing ILFM,
and to inspire future research directions in this rapidly evolving domain.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Jinxuan Li等人撰写的论文“Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey”的全面摘要，内容基于您提供的摘要和PDF全文：</p>
<p><strong>论文全面摘要：Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</strong></p>
<p><strong>1. 主要问题或研究问题 (Main Problem or Research Question Addressed)</strong></p>
<p>该论文主要解决的问题是：如何有效地将图像-语言基础模型（Image-Language Foundation Models, ILFM）的强大能力，迁移到视频理解任务中，以应对视频数据特有的时序维度挑战，并缓解从头训练视频-语言基础模型所需的大量数据和计算资源。具体来说，它旨在为基于现有ILFM的视频-文本学习建立一个结构化的路线图，并激发该领域未来的研究方向。</p>
<p><strong>2. 关键创新或方法论贡献 (Key Innovations or Methodological Contributions)</strong></p>
<p>该论文的主要贡献在于对图像到视频迁移学习这一新兴领域进行了首次全面且结构化的综述。其关键创新和方法论贡献包括：</p>
<ul>
<li><strong>系统性分类迁移策略：</strong> 论文将现有的图像到视频迁移学习策略系统地分为两大类：<ul>
<li><strong>冻结特征（Frozen Features）：</strong> 保持ILFM的原始表示不变，通过知识蒸馏、后网络微调（Post-Network Tuning）和侧调（Side-Tuning）等方法进行迁移。</li>
<li><strong>修改特征（Modified Features）：</strong> 对ILFM的原始表示进行修改，包括全微调（Full Fine-Tuning）、部分微调（Partial Tuning）、使用额外模型微调（Fine-Tuning with Extra Models）、基于适配器微调（Fine-Tuning with Adapter）、LoRA微调（Fine-Tuning with LoRA）和提示微调（Prompt Tuning）。</li>
</ul>
</li>
<li><strong>任务特定应用阐述：</strong> 论文根据任务的粒度（从细粒度到粗粒度）详细阐述了这些迁移策略在各种视频-文本学习任务中的应用，例如：<ul>
<li><strong>细粒度任务：</strong> 时空视频定位（Spatio-Temporal Video Grounding, STVG）、开放词汇多目标跟踪（Open Vocabulary Multi-Object Tracking, OV-MOT）、视频实例分割（Video Instance Segmentation, OV-VIS）等，这些任务需要精确的空间区域和时间间隔定位。</li>
<li><strong>粗粒度任务：</strong> 视频-文本检索（Video-Text Retrieval, VTR）、视频动作识别（Video Action Recognition, VAR）、视频问答（Video Question Answering, VideoQA）、视频字幕生成（Video Captioning）等，这些任务更侧重于对视频事件的整体理解。</li>
</ul>
</li>
<li><strong>详细的实验分析：</strong> 论文提供了详细的实验分析，通过比较不同图像到视频迁移学习范式在各种下游视频理解任务上的效果，验证了不同策略的有效性。例如，在TVG任务中，R2-tuning（侧调）在CLIP基础上表现最佳，而NumPro（额外模型微调）在LLaVA基础上表现突出。在VTR任务中，LoRA微调在CLIP基础上取得了最佳性能。在VAR任务中，适配器策略表现优于侧调。在VideoQA任务中，VideoDistill（知识蒸馏）在CLIP基础上表现优异，而BIMBA（后网络微调）在LLaVA基础上显著提升了性能。</li>
<li><strong>结构化路线图：</strong> 通过提供全面且结构化的概述，论文旨在为基于现有ILFM的视频-文本学习建立一个结构化的路线图，并激发该领域未来的研究方向。</li>
</ul>
<p><strong>3. 主要结果及其意义 (Main Results and Their Significance)</strong></p>
<p>论文的实验分析揭示了以下主要结果及其意义：</p>
<ul>
<li><strong>迁移学习的有效性：</strong> 图像到视频迁移学习范式在缓解从头训练视频-语言基础模型所需的大量数据和计算资源方面取得了成功。</li>
<li><strong>策略与任务的匹配性：</strong> 没有单一的迁移策略在所有任务上都表现最佳。不同任务对模型能力有不同要求，因此需要选择最合适的迁移范式。例如，细粒度任务（如TVG）通常受益于能够精确建模时空关系的策略，而粗粒度任务（如VideoQA）可能更侧重于多模态知识理解的鲁棒性。</li>
<li><strong>基础模型的选择：</strong> 不同的ILFM（如CLIP、MDETR、GroundingDINO、BLIP、LLaVA）具有不同的优势。例如，MDETR和GroundingDINO在细粒度视觉-文本对应方面表现更强，而CLIP和BLIP更擅长处理抽象或高级视觉-文本对应。LLaVA通过利用大型语言模型（LLM）弥合了这两种极端情况。</li>
<li><strong>LLM的潜力：</strong> 将大型语言模型（LLM）作为基础模型进行迁移，在某些任务（如VideoQA）中显示出显著的性能提升，这表明LLM强大的语义推理能力对视频理解至关重要。</li>
<li><strong>参数效率的重要性：</strong> 适配器、LoRA和提示微调等参数高效的微调方法在资源受限的环境下具有很高的实用性和可扩展性，同时也能保持ILFM的强大能力。</li>
</ul>
<p><strong>4. 论文中提及的局限性 (Limitations Mentioned in the Paper)</strong></p>
<p>论文中提及的现有方法的局限性包括：</p>
<ul>
<li><strong>冻结特征的局限性：</strong> 冻结特征方法（如知识蒸馏、后网络微调、侧调）的性能可能受限于静态图像级特征固有的表示能力，以及它们与视频特定任务要求的不完美对齐，尤其是在泛化到新场景时。它们可能引入ILFM的错误知识，从而限制跨模态视频-文本理解的有效性。</li>
<li><strong>全微调的计算成本：</strong> 全微调虽然简单有效，但需要大规模、定义良好且标注的数据，以及大量的计算资源，在实践中适用性较低。</li>
<li><strong>额外模型的复杂性：</strong> 虽然额外模型微调可以有效注入视频特定的结构信息，但也会增加计算成本。</li>
<li><strong>LoRA和提示微调的局限性：</strong> LoRA本身无法建模时序信息，需要依赖外部时序建模模块或网络。提示微调虽然计算成本低，但其对冻结骨干网络的依赖可能会限制细粒度时序推理能力。</li>
<li><strong>现有解决方案的碎片化：</strong> 当前研究中，针对不同视频任务往往需要选择不同的基础模型和设计独特的微调策略，导致解决方案碎片化，缺乏统一性和通用性。</li>
<li><strong>时序维度建模的挑战：</strong> 视频包含复杂的时空动态，包括运动、事件进程和因果交互，这使得从处理静态空间信息到有效建模这些复杂时空关系需要根本性的架构范式转变。</li>
</ul>
<p><strong>5. 潜在的未来研究方向 (Potential Future Research Directions)</strong></p>
<p>论文提出了以下几个有前景的未来研究方向：</p>
<ul>
<li><strong>统一迁移学习范式：</strong> 开发一个统一的迁移学习框架，使单个ILFM能够同时有效地适应多个视频-语言任务，从而实现更通用、更高效的视频智能系统。这可能涉及探索基于提示的学习或设计参数高效的统一适配器/超网络。</li>
<li><strong>多基础模型协作：</strong> 有效整合多个预训练的基础模型，每个模型专注于不同的模态或能力，以解决单个视频-语言任务。研究可以集中于融合机制、知识蒸馏和跨模型注意力，以利用互补优势并降低计算成本。</li>
<li><strong>高级双模态融合方法：</strong> 进一步研究更动态、高效的融合技术，如跨模态Transformer、基于图的对齐或轻量级特征交互模块，以改善视觉和语言特征在时空维度上的对齐和融合，这对于细粒度理解和长篇视频推理至关重要。</li>
<li><strong>视频编辑和生成：</strong> 扩展图像到视频迁移学习技术，以支持视频编辑和生成任务。这需要有效建模时序连贯性和动态运动，例如通过集成时序注意力、利用辅助模态（如深度图、光流）或开发新的扩散模型架构。</li>
<li><strong>开放词汇能力：</strong> 进一步增强模型在开放词汇场景下的泛化能力，使其能够处理训练中未见过的对象类别和事件。</li>
</ul>
<p>总而言之，这篇综述为图像到视频迁移学习领域提供了一个全面的技术蓝图，不仅总结了现有方法，还指出了未来的发展方向，对于推动视频-文本理解领域的发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Finally, we identify prevailing
challenges and highlight promising directions for future research.</li>
<li>By offering
a comprehensive and structured overview, this survey aims to establish a
structured roadmap for advancing video-text learning based on existing ILFM,
and to inspire future research directions in this rapidly evolving domain.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.10671v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.10671v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11717v1'></a></p>
<h2 id="ev4dgs-novel-view-rendering-of-non-rigid-objects-from-monocular-event-streams"><a href="https://arxiv.org/abs/2510.11717v1">Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams</a></h2>
<p><strong>Authors:</strong> Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik撰写的论文“Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Ev4DGS: 从单目事件流渲染非刚性物体的新视角</strong></p>
<p>这篇论文《Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams》首次提出了一种从单目事件流中渲染非刚性变形物体新视角的方法。传统上，事件相机在刚性场景的新视角合成方面已显示出优势，但对于非刚性物体，现有方法通常仍依赖于稀疏的RGB输入，这限制了其在纯事件流场景下的应用。Ev4DGS旨在解决这一挑战性问题，探索是否仅凭事件流就能学习到非刚性物体的模型。</p>
<p><strong>1. 主要问题或研究问题：</strong>
该研究主要解决的核心问题是：如何仅使用单目事件流，实现对非刚性变形物体的高质量新视角渲染？现有方法在处理非刚性物体时，通常需要结合RGB图像，而纯事件流的解决方案仍是未知的。这篇论文旨在填补这一空白，并证明仅通过事件数据进行非刚性3D重建和新视角合成是可行的。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
Ev4DGS引入了以下关键创新和方法论贡献：
*   <strong>首个纯事件流非刚性新视角渲染方法：</strong> Ev4DGS是第一个能够从单目事件流中渲染非刚性变形物体（以RGB或灰度图像形式）新视角的方法，无需任何RGB输入。
*   <strong>两阶段训练策略：</strong> 为了解决从单目观测中重建可变形物体的高度病态问题，Ev4DGS采用两阶段训练。
    *   <strong>粗略变形模型（Coarse Stage）：</strong> 第一阶段训练一个粗略的3D变形模型，该模型将非刚性物体形状表示为一组时间依赖的粗略点云，并能捕捉场景中的大尺度变形。它通过学习低秩基点云的线性组合来表示物体随时间演化的状态，并利用从事件生成的二值掩码进行监督。
    *   <strong>精细3D高斯溅射表示（Fine Stage）：</strong> 第二阶段在此粗略模型的基础上，利用4D高斯溅射（3DGS的泛化）来表示物体的精细外观，实现动态新视角合成。高斯参数通过粗略变形模型驱动，并共享时间信息。
*   <strong>事件损失和轮廓损失：</strong> 针对事件数据的特性，论文设计了特定的损失函数：
    *   <strong>事件损失（Event Loss）：</strong> 将估计模型的输出与2D事件观测空间关联起来，通过比较渲染图像的亮度变化与事件流中累积的亮度差异来优化3D高斯参数。
    *   <strong>轮廓损失（Silhouette Loss）：</strong> 利用从事件流生成的二值掩码，抑制背景中不必要的高斯点，从而提高图像质量。
*   <strong>自监督学习：</strong> 整个框架通过事件流和相机跟踪信息实现自监督学习，无需额外输入。
*   <strong>新数据集：</strong> 论文创建了新的合成和真实数据集，以评估Ev4DGS在非刚性物体上的性能，填补了现有数据集的不足。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>优越的性能：</strong> Ev4DGS在合成和真实数据集上均表现出卓越的性能，在PSNR指标上平均比现有基线方法（如3DGS和D3DGS）高出约10%，同时在SSIM上也具有竞争力。
*   <strong>高质量新视角渲染：</strong> 实验结果表明，Ev4DGS能够生成高质量、空间和时间连贯的新视角渲染，准确捕捉变形物体的外观和形状，而竞争方法往往会丢失物体内部的细节或出现模糊。
*   <strong>纯事件流的可行性：</strong> 论文成功证明了仅使用事件流进行非刚性3D重建和新视角合成的可行性，避免了中间帧重建带来的信息损失和误差。
*   <strong>对掩码质量的敏感性：</strong> 消融研究显示，模型性能对二值掩码的质量敏感。通过Snake算法直接从事件流生成掩码比通过E2VID+SAM从重建图像生成掩码效果更好，但仍不如使用真实（GT）二值掩码。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>外观伪影：</strong> 尽管Ev4DGS取得了最高的准确性，但模型的一个局限性是渲染的高斯点可能导致外观上出现伪影（如单个或成组渲染高斯点）。
*   <strong>对掩码质量的依赖：</strong> 模型的性能在一定程度上依赖于二值掩码的质量。如果能直接从事件流生成更准确的二值掩码，性能有望进一步提升。
*   <strong>优化难度：</strong> 增加基向量K的数量虽然能增强模型的运动表达能力，但也会增加优化的难度。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进掩码生成：</strong> 探索更准确、更鲁棒的直接从事件流生成二值掩码的方法，以进一步提升Ev4DGS的性能。
*   <strong>减少外观伪影：</strong> 研究如何优化高斯溅射表示或渲染过程，以减少渲染图像中的外观伪影，提高视觉真实感。
*   <strong>更复杂的变形：</strong> 探索Ev4DGS在更复杂、更大范围的非刚性变形场景中的应用和改进。
*   <strong>实时性能优化：</strong> 尽管论文提到了全重建时间约为4小时，但未来可以研究如何进一步优化训练和渲染过程，以实现更接近实时的性能。
*   <strong>多模态融合：</strong> 虽然Ev4DGS专注于纯事件流，但未来也可以探索与少量RGB输入或其他传感器数据的更有效融合，以在特定应用中进一步提升性能。</p>
<hr />
<p>总而言之，Ev4DGS是计算机视觉领域的一项重要进展，它为从单目事件流中进行非刚性物体的新视角渲染开辟了新途径，展示了事件相机在处理动态场景方面的巨大潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature.</li>
<li>This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams.</li>
<li>Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events.</li>
<li>We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11717v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11717v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11712v1'></a></p>
<h2 id="dit360-high-fidelity-panoramic-image-generation-via-hybrid-training"><a href="https://arxiv.org/abs/2510.11712v1">DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</a></h2>
<p><strong>Authors:</strong> Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this work, we propose DiT360, a DiT-based framework that performs hybrid
training on perspective and panoramic data for panoramic image generation. For
the issues of maintaining geometric fidelity and photorealism in generation
quality, we attribute the main reason to the lack of large-scale, high-quality,
real-world panoramic data, where such a data-centric view differs from prior
methods that focus on model design. Basically, DiT360 has several key modules
for inter-domain transformation and intra-domain augmentation, applied at both
the pre-VAE image level and the post-VAE token level. At the image level, we
incorporate cross-domain knowledge through perspective image guidance and
panoramic refinement, which enhance perceptual quality while regularizing
diversity and photorealism. At the token level, hybrid supervision is applied
across multiple modules, which include circular padding for boundary
continuity, yaw loss for rotational robustness, and cube loss for distortion
awareness. Extensive experiments on text-to-panorama, inpainting, and
outpainting tasks demonstrate that our method achieves better boundary
consistency and image fidelity across eleven quantitative metrics. Our code is
available at https://github.com/Insta360-Research-Team/DiT360.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<p><strong>论文摘要分析：DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>DiT360 提出了一种基于 DiT 的全景图像生成框架，通过在透视数据和全景数据上进行混合训练来解决全景图像生成中的几何保真度和真实感问题。该方法的核心在于其数据中心视角，通过在图像和 token 级别引入跨域转换和域内增强模块，显著提升了生成质量、边界一致性和图像保真度。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>该论文的关键创新在于其<strong>混合训练范式和数据中心视角</strong>，这与以往专注于模型设计的方法形成对比。具体方法论包括：</p>
<ul>
<li><strong>混合训练 (Hybrid Training)：</strong> 在透视数据和全景数据上同时进行训练，利用透视数据丰富的细节和真实感来弥补全景数据稀缺的不足。</li>
<li><strong>多层次跨域知识融合：</strong><ul>
<li><strong>图像级别 (Pre-VAE Image Level)：</strong> 引入透视图像引导 (perspective image guidance) 和全景精炼 (panoramic refinement)，以增强感知质量，同时规范多样性和真实感。这表明模型在编码器之前就利用了透视图像的结构信息。</li>
<li><strong>Token 级别 (Post-VAE Token Level)：</strong> 在 VAE 编码后的潜在空间 (token level) 应用混合监督，包括：<ul>
<li><strong>循环填充 (Circular Padding)：</strong> 确保全景图像左右边界的连续性，这是全景图特有的挑战。</li>
<li><strong>偏航损失 (Yaw Loss)：</strong> 增强旋转鲁棒性，使生成内容在不同视角下保持一致。</li>
<li><strong>立方体损失 (Cube Loss)：</strong> 提高对全景图像固有畸变的感知和处理能力，可能通过将全景图投影到立方体贴图来计算损失。</li>
</ul>
</li>
</ul>
</li>
<li><strong>数据中心视角：</strong> 强调缺乏大规模、高质量、真实世界全景数据是生成质量问题的根本原因，并通过上述混合训练和多层次策略来有效利用现有数据。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>DiT360 有望对计算机视觉领域产生以下潜在影响：</p>
<ul>
<li><strong>提升全景图像生成质量标准：</strong> 通过解决几何保真度和真实感的核心问题，DiT360 可能成为全景图像生成领域的新基线，推动更高质量的生成结果。</li>
<li><strong>启发新的数据利用策略：</strong> 其混合训练和数据中心视角为处理特定领域数据稀缺问题提供了新的思路，尤其是在需要跨域知识迁移的生成任务中。</li>
<li><strong>推动全景内容创作和应用：</strong> 更高质量的全景生成能力将直接赋能虚拟现实 (VR)、增强现实 (AR)、元宇宙、360° 视频制作、游戏环境生成等领域，降低内容创作门槛。</li>
<li><strong>为 DiT 架构的应用提供新方向：</strong> 展示了 DiT 架构在处理复杂几何和拓扑结构数据（如全景图）方面的潜力，可能激发 DiT 在其他非标准图像格式生成任务中的应用。</li>
</ul>
<p><strong>4. 相关领域或应用</strong></p>
<p>以下领域或应用将从这项研究中受益：</p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 生成逼真的 360° 环境和纹理，用于沉浸式体验。</li>
<li><strong>元宇宙 (Metaverse)：</strong> 快速创建和填充虚拟世界的全景背景和场景。</li>
<li><strong>360° 视频和图像编辑：</strong> 文本到全景图生成、全景图修复 (inpainting) 和扩展 (outpainting) 将极大地简化 360° 媒体的后期制作。</li>
<li><strong>游戏开发：</strong> 自动生成游戏场景的背景天空盒 (skybox) 或环境贴图。</li>
<li><strong>机器人和自动驾驶：</strong> 生成多样化的全景环境数据用于训练和测试感知系统，尤其是在数据采集困难的场景。</li>
<li><strong>建筑和室内设计：</strong> 快速生成不同设计方案的全景渲染图。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<p>尽管摘要强调了显著的进步，但仍可推断出一些潜在局限性：</p>
<ul>
<li><strong>计算资源需求：</strong> 混合训练，尤其是在图像和 token 级别都进行复杂操作，可能需要大量的计算资源（GPU 内存和计算时间），这对于个人研究者或小型团队可能是一个挑战。</li>
<li><strong>数据依赖性：</strong> 尽管强调了数据中心视角，但其性能仍可能高度依赖于所使用的透视数据和少量全景数据的质量和多样性。如果透视数据本身存在偏差或不足，可能会影响最终生成质量。</li>
<li><strong>泛化能力：</strong> 摘要中未提及模型在处理极端复杂场景、高度动态内容或特定风格全景图时的泛化能力。例如，在生成具有复杂几何结构或精细纹理的全景图时，是否能始终保持高保真度。</li>
<li><strong>“缺乏大规模、高质量、真实世界全景数据”的根本问题：</strong> 尽管 DiT360 旨在缓解这一问题，但它并未从根本上解决全景数据稀缺的挑战。如果未来有大量高质量全景数据可用，其混合训练策略可能需要调整或优化。</li>
<li><strong>特定畸变处理：</strong> 尽管提到了“立方体损失”来处理畸变，但全景图的畸变是复杂的，尤其是在极点附近。摘要中未详细说明其对所有类型畸变（如极点拉伸）的处理效果。</li>
<li><strong>评估指标的局限性：</strong> 摘要提到“十一个定量指标”，但这些指标是否能完全捕捉全景图像的视觉质量、几何准确性和用户体验，仍需在论文正文中详细探讨。例如，人类感知评估（user study）通常是不可或缺的。</li>
</ul>
<hr />
<p>总而言之，DiT360 提出了一种新颖且实用的全景图像生成方法，通过其独特的混合训练和多层次处理策略，有望在解决全景图生成中的核心挑战方面取得显著进展。其数据中心视角和对 DiT 架构的创新应用，使其成为计算机视觉领域一个值得关注的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose DiT360, a DiT-based framework that performs hybrid
training on perspective and panoramic data for panoramic image generation.</li>
<li>Extensive experiments on text-to-panorama, inpainting, and
outpainting tasks demonstrate that our method achieves better boundary
consistency and image fidelity across eleven quantitative metrics.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11693v1'></a></p>
<h2 id="scaling-language-centric-omnimodal-representation-learning"><a href="https://arxiv.org/abs/2510.11693v1">Scaling Language-Centric Omnimodal Representation Learning</a></h2>
<p><strong>Authors:</strong> Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Chenghao Xiao等人撰写的论文“Scaling Language-Centric Omnimodal Representation Learning”的全面摘要。</p>
<hr />
<h3 id="scaling-language-centric-omnimodal-representation-learning_1">论文摘要：Scaling Language-Centric Omnimodal Representation Learning</h3>
<p><strong>1. 主要问题或研究问题</strong></p>
<p>该论文旨在解决一个核心问题：尽管基于多模态大语言模型（MLLM）并结合对比学习（CL）的嵌入方法在多模态任务中表现出色，但其优越性的深层原因尚未被充分探索。具体来说，研究者希望理解MLLM在生成式预训练过程中如何实现隐式跨模态对齐，以及这种对齐如何影响其表征能力，并为后续的对比学习提供基础。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<ul>
<li><strong>MLLM中潜在跨模态对齐的发现与分析：</strong> 论文通过对各模态（文本、图像、音频、视频）嵌入空间的各向异性（anisotropy）和核级相似性（kernel-level similarity）结构进行实证分析，发现MLLM在生成式预训练中已实现隐式跨模态对齐。文本模态的对比学习不仅提高了文本嵌入的可区分性，还泛化性地增强了非文本模态嵌入的可区分性，使其更具各向同性。</li>
<li><strong>提出语言中心全模态嵌入框架（LCO-EMB）：</strong> 基于上述发现，论文提出了LCO-EMB框架。该框架利用语言中心配对数据进行高效的对比学习微调，将对比学习视为轻量级的精炼阶段，而非从头开始的对齐机制。LCO-EMB通过LoRA（Low-Rank Adaptation）技术对MLLM进行表征激活，旨在最小化扰动预训练的生成能力和潜在跨模态对齐。</li>
<li><strong>识别并理论解释生成-表征缩放定律（GRSL）：</strong> 论文通过实验观察到，MLLM的表征能力（通过对比学习精炼后）与其生成能力呈正相关。研究者进一步提供了GRSL的理论解释，通过PAC-贝叶斯泛化界限，形式化地将MLLM的生成质量与其表征性能的上限联系起来，表明生成能力越强，表征潜力越大。</li>
<li><strong>引入SeaDoc基准任务：</strong> 为了验证GRSL，论文引入了SeaDoc，这是一个具有挑战性的低资源视觉文档检索任务，用于评估MLLM在跨语言多模态文档理解方面的表征能力。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<ul>
<li><strong>LCO-EMB的卓越性能：</strong> LCO-EMB在MIEB-Lite基准测试中，即使仅使用少量（约0.37M）训练对，也显著优于现有的多模态嵌入模型，并在多种模态任务中达到最先进的性能。这表明MLLM的内在跨模态对齐能力是其性能优势的关键。</li>
<li><strong>LoRA的有效性：</strong> LoRA作为一种参数高效的微调方法，在保持模型生成能力和潜在跨模态对齐的同时，有效提升了表征能力，优于传统的CLIP风格对比学习和全量微调。</li>
<li><strong>GRSL的实证验证：</strong> 实验结果一致表明，基线生成性能与对比学习后的表征性能之间存在正相关。这证实了GRSL，并提出通过提升MLLM的生成能力来增强其多模态表征质量是一种有效范式。</li>
<li><strong>SeaDoc任务的验证：</strong> 在SeaDoc任务上，持续的生成式预训练（尤其是在高分辨率和结合通用图像标注数据的情况下）能进一步提升模型的嵌入能力，支持了GRSL的观点。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 论文指出，虽然可以联合训练生成损失和对比损失以同时保持模型知识和增强表征能力，但这种方法计算成本较高。</li>
<li><strong>LoRA超参数的优化：</strong> 论文提到，LoRA的rank (r) 和 alpha (a) 超参数没有一个全局最优设置，其最佳值可能因模型大小而异，需要在引入新知识和修改预训练模型权重之间取得平衡。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>联合生成与对比学习：</strong> 鉴于联合训练生成损失和对比损失的计算成本较高，未来的工作可以探索更高效的方法来实现这一目标，以进一步提升全模态表征学习。</li>
<li><strong>LoRA超参数的全面分析：</strong> 对LoRA超参数进行更全面的实证分析和理论研究，以量化其与模型性能之间的关系，并找到更通用的优化策略。</li>
<li><strong>GRSL的进一步探索：</strong> 深入研究GRSL，探索如何通过改进生成能力来系统性地提升表征质量，这可能涉及新的生成式预训练范式或模型架构。</li>
</ul>
<hr />
<p>这篇论文为理解MLLM在多模态嵌入领域的成功提供了深刻见解，并提出了一个新颖的框架和重要的缩放定律，为未来多模态表征学习的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb.</li>
<li>Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11693v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11693v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11650v1'></a></p>
<h2 id="infinihuman-infinite-3d-human-creation-with-precise-control"><a href="https://arxiv.org/abs/2510.11650v1">InfiniHuman: Infinite 3D Human Creation with Precise Control</a></h2>
<p><strong>Authors:</strong> Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating realistic and controllable 3D human avatars is a long-standing
challenge, particularly when covering broad attribute ranges such as ethnicity,
age, clothing styles, and detailed body shapes. Capturing and annotating
large-scale human datasets for training generative models is prohibitively
expensive and limited in scale and diversity. The central question we address
in this paper is: Can existing foundation models be distilled to generate
theoretically unbounded, richly annotated 3D human data? We introduce
InfiniHuman, a framework that synergistically distills these models to produce
richly annotated human data at minimal cost and with theoretically unlimited
scalability. We propose InfiniHumanData, a fully automatic pipeline that
leverages vision-language and image generation models to create a large-scale
multi-modal dataset. User study shows our automatically generated identities
are undistinguishable from scan renderings. InfiniHumanData contains 111K
identities spanning unprecedented diversity. Each identity is annotated with
multi-granularity text descriptions, multi-view RGB images, detailed clothing
images, and SMPL body-shape parameters. Building on this dataset, we propose
InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body
shape, and clothing assets. InfiniHumanGen enables fast, realistic, and
precisely controllable avatar generation. Extensive experiments demonstrate
significant improvements over state-of-the-art methods in visual quality,
generation speed, and controllability. Our approach enables high-quality avatar
generation with fine-grained control at effectively unbounded scale through a
practical and affordable solution. We will publicly release the automatic data
generation pipeline, the comprehensive InfiniHumanData dataset, and the
InfiniHumanGen models at https://yuxuan-xue.com/infini-human.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：InfiniHuman: Infinite 3D Human Creation with Precise Control</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于提出了一种名为 InfiniHuman 的框架，该框架通过蒸馏现有基础模型，以极低的成本和理论上无限的可扩展性生成了大规模、多样化且富含标注的 3D 人体数据（InfiniHumanData）。在此基础上，它还引入了一个扩散模型 InfiniHumanGen，能够实现快速、逼真且精确可控的 3D 人体化身生成，显著超越了现有技术。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>该论文的关键创新在于其“数据蒸馏”和“数据驱动生成”的方法论：</p>
<ul>
<li><strong>数据蒸馏与自动标注 (InfiniHumanData):</strong> 核心思想是利用现有的视觉-语言模型和图像生成模型，以全自动的方式生成大规模、多模态的 3D 人体数据集。这解决了传统方法中数据采集和标注成本高昂、多样性受限的问题。通过这种方式，他们能够生成包含 111K 身份的数据集，涵盖了前所未有的多样性，并为每个身份提供了多粒度文本描述、多视角 RGB 图像、详细服装图像和 SMPL 身体形状参数。用户研究表明其自动生成的人体与扫描渲染图无异，这证明了数据质量。</li>
<li><strong>扩散模型驱动的精确控制生成 (InfiniHumanGen):</strong> 在 InfiniHumanData 的基础上，他们开发了一个基于扩散的生成管道 InfiniHumanGen。这个模型能够以文本、身体形状和服装资产为条件，实现对 3D 人体化身的快速、逼真且精确的控制生成。这使得用户可以根据具体需求，通过多模态输入来定制化身。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>InfiniHuman 对计算机视觉和图形学领域具有深远的潜在影响：</p>
<ul>
<li><strong>打破数据瓶颈：</strong> 解决了 3D 人体生成领域长期存在的数据稀缺和标注成本高昂的问题，为训练更强大的生成模型提供了“无限”的数据源。</li>
<li><strong>提升生成质量与控制力：</strong> 显著提高了 3D 人体化身生成的视觉真实感、速度和精细控制能力，使得创建高度定制化的虚拟人变得更加可行。</li>
<li><strong>推动基础模型应用：</strong> 展示了如何巧妙地利用和蒸馏现有的大型基础模型（如视觉-语言模型和图像生成模型）来解决特定领域的复杂问题，为其他领域的数据生成提供了新的范式。</li>
<li><strong>降低技术门槛：</strong> 通过提供自动数据生成管道、数据集和模型，降低了研究人员和开发者进入 3D 人体生成领域的门槛。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 创建高度逼真和可定制的虚拟化身，用于社交 VR、游戏、虚拟试穿等。</li>
<li><strong>电影、动画和游戏产业：</strong> 快速生成大量多样化的角色模型，大幅缩短制作周期和成本。</li>
<li><strong>数字时尚和虚拟试穿：</strong> 生成具有不同体型、肤色和服装风格的虚拟模特，用于服装设计、展示和在线购物体验。</li>
<li><strong>人体姿态估计和动作捕捉：</strong> 生成多样化的人体数据可以作为训练数据，提高这些任务的鲁棒性。</li>
<li><strong>医疗和健康领域：</strong> 生成不同体型和年龄段的人体模型，用于医学模拟、康复训练或人体工程学研究。</li>
<li><strong>人机交互 (HCI):</strong> 创建更具表现力和个性化的虚拟助手或机器人形象。</li>
<li><strong>计算机图形学研究：</strong> 为 3D 建模、渲染、动画和角色绑定等研究提供高质量的基准数据和生成工具。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<p>尽管摘要展示了令人印象深刻的成果，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>“理论上无限”与实际计算资源：</strong> 尽管数据生成是“理论上无限”的，但实际生成和存储如此大规模的数据仍需要大量的计算资源和存储空间。</li>
<li><strong>基础模型的依赖性：</strong> InfiniHuman 的性能在很大程度上依赖于所蒸馏的视觉-语言模型和图像生成模型的质量和能力。如果这些基础模型存在偏差或局限性，可能会传递到生成的数据和模型中。</li>
<li><strong>“ undistinguishable from scan renderings”的范围：</strong> 用户研究表明自动生成的人体与扫描渲染图“无异”，但这通常是在特定条件下（例如，特定视角、光照、分辨率）进行的。在极端特写、复杂光照或高精度物理模拟等场景下，是否仍能保持这种“无异”的水平，需要进一步验证。</li>
<li><strong>SMPL 模型的局限性：</strong> 摘要提到使用 SMPL 身体形状参数。SMPL 模型虽然广泛使用，但它是一个参数化模型，可能无法捕捉到所有细微的人体解剖学细节或非标准体型。</li>
<li><strong>服装资产的来源和多样性：</strong> 摘要提到“detailed clothing images”和“clothing assets”。这些服装资产的来源、多样性和质量将直接影响生成结果的真实感和可控性。如果服装资产本身有限，那么生成的多样性也会受限。</li>
<li><strong>实时性或交互性：</strong> 摘要强调了“fast”生成，但并未明确说明是否支持实时交互式生成，这对于某些应用（如 VR/AR）至关重要。</li>
</ul>
<hr />
<p>总的来说，InfiniHuman 是一项具有开创性的工作，它通过巧妙地利用现有 AI 基础模型，为 3D 人体生成领域带来了革命性的数据生成和模型训练范式，有望在多个应用领域产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce
InfiniHuman, a framework that synergistically distills these models to produce
richly annotated human data at minimal cost and with theoretically unlimited
scalability.</li>
<li>We propose InfiniHumanData, a fully automatic pipeline that
leverages vision-language and image generation models to create a large-scale
multi-modal dataset.</li>
<li>Building on this dataset, we propose
InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body
shape, and clothing assets.</li>
<li>Extensive experiments demonstrate
significant improvements over state-of-the-art methods in visual quality,
generation speed, and controllability.</li>
<li>Our approach enables high-quality avatar
generation with fine-grained control at effectively unbounded scale through a
practical and affordable solution.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11650v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11650v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11649v1'></a></p>
<h2 id="physic-physically-plausible-3d-human-scene-interaction-and-contact-from-a-single-image"><a href="https://arxiv.org/abs/2510.11649v1">PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</a></h2>
<p><strong>Authors:</strong> Pradyumna Yalandur Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing metrically accurate humans and their surrounding scenes from a
single image is crucial for virtual reality, robotics, and comprehensive 3D
scene understanding. However, existing methods struggle with depth ambiguity,
occlusions, and physically inconsistent contacts. To address these challenges,
we introduce PhySIC, a framework for physically plausible Human-Scene
Interaction and Contact reconstruction. PhySIC recovers metrically consistent
SMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within
a shared coordinate frame from a single RGB image. Starting from coarse
monocular depth and body estimates, PhySIC performs occlusion-aware inpainting,
fuses visible depth with unscaled geometry for a robust metric scaffold, and
synthesizes missing support surfaces like floors. A confidence-weighted
optimization refines body pose, camera parameters, and global scale by jointly
enforcing depth alignment, contact priors, interpenetration avoidance, and 2D
reprojection consistency. Explicit occlusion masking safeguards invisible
regions against implausible configurations. PhySIC is efficient, requiring only
9 seconds for joint human-scene optimization and under 27 seconds end-to-end.
It naturally handles multiple humans, enabling reconstruction of diverse
interactions. Empirically, PhySIC outperforms single-image baselines, reducing
mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,
and improving contact F1 from 0.09 to 0.51. Qualitative results show realistic
foot-floor interactions, natural seating, and plausible reconstructions of
heavily occluded furniture. By converting a single image into a physically
plausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.
Our implementation is publicly available at https://yuxuan-xue.com/physic.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</strong></p>
<p><strong>1. 论文主要贡献的简明总结 (2-3 句话)</strong></p>
<p>PhySIC 提出了一种从单张 RGB 图像重建物理上合理的三维人体-场景交互和接触的框架。它解决了现有方法在深度模糊、遮挡和物理不一致接触方面的挑战，通过联合优化人体网格、场景表面和顶点级接触图，实现了度量一致且物理可信的重建。该方法显著提高了重建精度，尤其是在场景几何、人体姿态和接触检测方面。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>PhySIC 的关键创新在于其多阶段、联合优化的方法，特别强调了物理合理性：</p>
<ul>
<li><strong>鲁棒的度量支架构建：</strong> 从粗略的单目深度和人体估计开始，通过遮挡感知修复、可见深度与未缩放几何的融合，以及缺失支撑面（如地板）的合成，构建了一个稳健的度量支架。这有效地解决了单目深度估计的尺度模糊性问题。</li>
<li><strong>置信度加权的联合优化：</strong> 引入了一个置信度加权的优化框架，同时精炼人体姿态、相机参数和全局尺度。这个优化器通过联合强制执行以下约束来确保物理合理性：<ul>
<li><strong>深度对齐：</strong> 确保重建结果与原始深度估计一致。</li>
<li><strong>接触先验：</strong> 鼓励人体与场景之间发生合理的接触（例如，脚踩在地面上，人坐在椅子上）。</li>
<li><strong>互穿避免：</strong> 明确防止人体与场景之间发生不自然的穿透。</li>
<li><strong>2D 重投影一致性：</strong> 确保 3D 重建结果在图像平面上的投影与原始 2D 图像一致。</li>
</ul>
</li>
<li><strong>显式遮挡掩码：</strong> 使用显式遮挡掩码来保护不可见区域，防止其被优化到不合理的配置中，这对于处理复杂遮挡场景至关重要。</li>
<li><strong>顶点级接触图：</strong> 不仅重建人体和场景，还输出精细的顶点级接触图，这对于理解交互细节非常有价值。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>PhySIC 对计算机视觉领域具有显著的潜在影响：</p>
<ul>
<li><strong>推动单目 3D 重建的边界：</strong> 显著提高了从单张图像进行人体-场景联合 3D 重建的精度和物理合理性，尤其是在处理复杂交互和遮挡方面。</li>
<li><strong>更可靠的 3D 场景理解：</strong> 提供了更准确、更可信的 3D 人体和场景表示，这对于需要理解场景中物体和代理之间关系的下游任务至关重要。</li>
<li><strong>为下游应用提供基础：</strong> 其输出的度量一致的人体网格、场景表面和接触图，可以作为虚拟现实、机器人、动画和人机交互等领域更高级应用的基础。</li>
<li><strong>效率与可扩展性：</strong> 相对高效的优化时间（9 秒用于联合优化，27 秒端到端）以及处理多人的能力，使其在实际应用中更具吸引力。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 创建更逼真、交互性更强的虚拟环境，例如，将真实世界的人体和场景无缝集成到虚拟世界中。</li>
<li><strong>机器人学：</strong> 帮助机器人更好地理解其操作环境中的人类和物体，从而实现更安全、更自然的协作和导航。例如，机器人可以利用这些信息来预测人类的意图或避免碰撞。</li>
<li><strong>3D 内容创作和动画：</strong> 自动化从 2D 图像生成 3D 场景和角色姿态的过程，极大地简化了动画师和 3D 艺术家的工作流程。</li>
<li><strong>人机交互 (HCI)：</strong> 更好地理解用户在物理空间中的行为和意图，从而设计更直观、更响应式的人机界面。</li>
<li><strong>智能监控和安全：</strong> 分析监控视频中的人类活动和交互，例如检测异常行为或理解人群动态。</li>
<li><strong>人体姿态估计和形状重建：</strong> 为这些任务提供更强的场景上下文和物理约束，从而提高鲁棒性和准确性。</li>
</ul>
<p><strong>5. 可以从摘要中推断出的任何局限性</strong></p>
<p>尽管 PhySIC 取得了显著进展，但摘要中仍可推断出一些潜在局限性：</p>
<ul>
<li><strong>对初始估计的依赖：</strong> 摘要提到“Starting from coarse monocular depth and body estimates”，这意味着该方法可能在一定程度上依赖于这些初始估计的质量。如果初始估计非常差，优化过程可能难以收敛到最优解。</li>
<li><strong>复杂场景的泛化能力：</strong> 尽管它处理了遮挡和多人，但对于极端复杂、高度杂乱或包含大量非刚性物体的场景，其性能可能仍有待进一步验证。例如，对于非常规的物体形状或不常见的交互模式，接触先验的有效性可能受到限制。</li>
<li><strong>计算成本：</strong> 尽管摘要称其“高效”，但 27 秒的端到端时间对于某些实时应用（如高帧率 VR/AR）可能仍然过长。</li>
<li><strong>纹理和材质重建：</strong> 摘要主要关注几何形状和接触，并未提及对场景纹理、材质或光照的重建。这可能意味着输出的 3D 模型在视觉真实感方面仍需进一步处理。</li>
<li><strong>动态场景：</strong> 摘要中没有明确说明其处理动态场景或视频序列的能力。从单张图像重建通常假定场景是静态的，或者至少在图像捕获瞬间是静态的。</li>
<li><strong>“物理合理性”的定义：</strong> 尽管强调了物理合理性，但其具体实现（例如，接触先验和互穿避免的精确建模）可能仍是简化的。例如，它可能不考虑摩擦、弹性形变等更复杂的物理属性。</li>
</ul>
<hr />
<p>总而言之，PhySIC 是一项令人兴奋的研究，它通过引入一套新颖的联合优化策略和物理约束，显著提升了从单张图像进行 3D 人体-场景重建的质量和实用性。其对物理合理性的强调是其核心优势，有望在多个应用领域产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges,
we introduce PhySIC, a framework for physically plausible Human-Scene
Interaction and Contact reconstruction.</li>
<li>Empirically, PhySIC outperforms single-image baselines, reducing
mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,
and improving contact F1 from 0.09 to 0.51.</li>
<li>Qualitative results show realistic
foot-floor interactions, natural seating, and plausible reconstructions of
heavily occluded furniture.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11649v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11649v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.11606v1'></a></p>
<h2 id="expvid-a-benchmark-for-experiment-video-understanding-reasoning"><a href="https://arxiv.org/abs/2510.11606v1">ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</a></h2>
<p><strong>Authors:</strong> Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang</p>
<p><strong>Published:</strong> 2025-10-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) hold promise for accelerating
scientific discovery by interpreting complex experimental procedures. However,
their true capabilities are poorly understood, as existing benchmarks neglect
the fine-grained and long-horizon nature of authentic laboratory work,
especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the
first benchmark designed to systematically evaluate MLLMs on scientific
experiment videos. Curated from peer-reviewed video publications, ExpVid
features a new three-level task hierarchy that mirrors the scientific process:
(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural
Understanding of step order and completeness; and (3) Scientific Reasoning that
connects the full experiment to its published conclusions. Our vision-centric
annotation pipeline, combining automated generation with multi-disciplinary
expert validation, ensures that tasks require visual grounding. We evaluate 19
leading MLLMs on ExpVid and find that while they excel at coarse-grained
recognition, they struggle with disambiguating fine details, tracking state
changes over time, and linking experimental procedures to scientific outcomes.
Our results reveal a notable performance gap between proprietary and
open-source models, particularly in high-order reasoning. ExpVid not only
provides a diagnostic tool but also charts a roadmap for developing MLLMs
capable of becoming trustworthy partners in scientific experimentation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Yicheng Xu等人撰写的论文“ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning”的全面摘要：</p>
<p><strong>论文摘要：ExpVid：一个用于实验视频理解与推理的基准</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现有基准在评估多模态大型语言模型（MLLMs）理解科学实验视频方面的不足。现有基准未能充分捕捉真实实验室工作（特别是湿实验室环境）中细粒度、长时程的复杂性，导致对MLLMs在加速科学发现方面的真实能力理解不足。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>引入ExpVid基准：</strong> 首次系统性地评估MLLMs在科学实验视频上的表现，特别关注湿实验室环境。
*   <strong>三级任务层次结构：</strong> ExpVid设计了一个模仿科学流程的三级任务层次结构：
    *   <strong>一级：细粒度感知</strong>（工具、材料和动作的识别）。
    *   <strong>二级：程序理解</strong>（步骤顺序和完整性的理解）。
    *   <strong>三级：科学推理</strong>（将整个实验与已发表的结论联系起来）。
*   <strong>以视觉为中心的标注流程：</strong> 结合自动化生成和多学科专家验证，确保任务需要视觉基础，避免仅依赖文本或背景知识。
*   <strong>广泛的模型评估：</strong> 在ExpVid上评估了19个主流MLLMs（包括开源和专有模型）。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>MLLMs在粗粒度识别方面表现良好：</strong> 模型在识别粗粒度信息方面表现出色。
*   <strong>MLLMs在细粒度理解和高阶推理方面存在挑战：</strong> 模型在以下方面表现不佳：
    *   区分细微细节。
    *   跟踪随时间变化的实验状态。
    *   将实验过程与科学结果联系起来。
*   <strong>专有模型与开源模型之间的显著性能差距：</strong> 尤其是在高阶推理任务中，专有模型的表现明显优于开源模型。
*   <strong>诊断工具和路线图：</strong> ExpVid不仅提供了一个诊断工具来识别MLLMs的弱点，也为开发能够成为科学实验中值得信赖的合作伙伴的MLLMs指明了方向。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>领域覆盖范围有限：</strong> ExpVid目前主要关注湿实验室实验，尚未涵盖所有科学探究领域，例如物理学（涉及独特实验设备和抽象现象）或纯计算实验。
*   <strong>推理任务的深度：</strong> 三级推理任务评估的是结果，但未能深入阐明将实验与结论联系起来的底层推理过程（例如，思维链）。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展领域覆盖：</strong> 将基准扩展到物理学、计算科学和大型工程测试等更多科学领域。
*   <strong>深化推理评估：</strong> 开发能够评估MLLMs在实验中更深层次推理过程（如因果链、假设检验）的机制。
*   <strong>提升开源MLLMs的高阶推理能力：</strong> 缩小开源模型与专有模型在复杂推理任务上的性能差距。
*   <strong>优化模型对长时程视频数据的利用：</strong> 解决模型在处理冗余输入时可能出现的饱和或干扰问题，以更好地利用扩展的时间上下文。</p>
<p>这篇论文通过引入ExpVid基准，为评估和推动MLLMs在理解真实科学实验视频方面的能力提供了重要贡献，为未来开发更智能、更可靠的科学AI助手奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce ExpVid, the
first benchmark designed to systematically evaluate MLLMs on scientific
experiment videos.</li>
<li>Curated from peer-reviewed video publications, ExpVid
features a new three-level task hierarchy that mirrors the scientific process:
(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural
Understanding of step order and completeness; and (3) Scientific Reasoning that
connects the full experiment to its published conclusions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.11606v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.11606v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-14 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
