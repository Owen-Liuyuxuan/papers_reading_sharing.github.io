<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-19 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-18/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-19">Arxiv Computer Vision Papers - 2025-11-19</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#arc-is-a-vision-problem" class="nav-link">ARC Is a Vision Problem!</a>
                </li>
                <li class="nav-item">
                    <a href="#unigen-15-enhancing-image-generation-and-editing-through-reward-unification-in-reinforcement-learning" class="nav-link">UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#_06-a-vla-that-learns-from-experience" class="nav-link">π^{*}_{0.6}: a VLA That Learns From Experience</a>
                </li>
                <li class="nav-item">
                    <a href="#co-me-confidence-guided-token-merging-for-visual-geometric-transformers" class="nav-link">Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#zero-shot-synthetic-video-realism-enhancement-via-structure-aware-denoising" class="nav-link">Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-as-self-distillation-end-to-end-latent-diffusion-in-one-model" class="nav-link">Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</a>
                </li>
                <li class="nav-item">
                    <a href="#freeswim-revisiting-sliding-window-attention-mechanisms-for-training-free-ultra-high-resolution-video-generation" class="nav-link">FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#attention-via-synaptic-plasticity-is-all-you-need-a-biologically-inspired-spiking-neuromorphic-transformer" class="nav-link">Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#nora-15-a-vision-language-action-model-trained-using-world-model-and-action-based-preference-rewards" class="nav-link">NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</a>
                </li>
                <li class="nav-item">
                    <a href="#sparsesurf-sparse-view-3d-gaussian-splatting-for-surface-reconstruction" class="nav-link">SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-19">Arxiv Computer Vision Papers - 2025-11-19</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年11月18日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年11月18日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>多模态理解与生成</strong>、<strong>高效模型架构</strong>以及<strong>生物启发式方法</strong>的探索。特别值得注意的是，<strong>视觉-语言-动作（VLA）模型</strong>的进一步发展，以及在<strong>图像和视频生成</strong>领域对<strong>强化学习和扩散模型</strong>的深入应用。同时，<strong>高效的注意力机制</strong>和<strong>稀疏表示</strong>在处理高分辨率数据和三维重建方面展现出巨大潜力。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>UniGen-1.5</strong> 和 <strong>NORA-1.5</strong> 显著推动了<strong>多模态模型</strong>的能力，通过奖励机制的统一和世界模型/动作偏好奖励的引入，提升了图像生成、编辑以及视觉-语言-动作的协同能力。</li>
<li><strong>FreeSwim</strong> 在<strong>超高分辨率视频生成</strong>方面取得了突破，通过重新审视滑动窗口注意力机制，实现了训练无关（training-free）的生成，为处理大规模视频数据提供了新思路。</li>
<li><strong>Diffusion As Self-Distillation</strong> 提出了一种新颖的<strong>端到端潜在扩散模型</strong>，将扩散过程视为一种自蒸馏，简化了训练流程并可能提升性能。</li>
<li><strong>Attention via Synaptic Plasticity is All You Need</strong> 引入了<strong>生物启发的脉冲神经形态 Transformer</strong>，将突触可塑性应用于注意力机制，为开发更节能、更类脑的计算模型提供了方向。</li>
<li><strong>SparseSurf</strong> 在<strong>三维表面重建</strong>领域提出了<strong>稀疏视图 3D 高斯泼溅（Gaussian Splatting）</strong>方法，有望在数据量受限的情况下实现高质量的三维重建。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>强化学习在生成模型中的应用深化：</strong> 通过设计更精细的奖励函数（如 UniGen-1.5 的奖励统一，NORA-1.5 的世界模型/动作偏好奖励），以指导和优化图像、视频的生成和编辑过程。</li>
<li><strong>高效注意力机制的探索：</strong> 针对高分辨率和长序列数据，如 FreeSwim 中的滑动窗口注意力，以及 Co-Me 中的置信度引导的 Token 合并，旨在降低计算复杂度并提升效率。</li>
<li><strong>生物启发式计算模型：</strong> 将神经科学的原理（如突触可塑性）融入 Transformer 架构，探索更节能、更具生物学合理性的模型设计。</li>
<li><strong>零样本（Zero-shot）能力增强：</strong> 如 Zero-shot Synthetic Video Realism Enhancement，通过结构感知去噪等技术，在无需特定训练数据的情况下提升生成内容的质量和真实感。</li>
<li><strong>三维视觉的稀疏化与高效表示：</strong> SparseSurf 展示了利用稀疏视图和高效表示方法（如 3D Gaussian Splatting）进行三维重建的潜力。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>为了快速了解当前研究热点和潜在突破，建议重点阅读以下论文：</p>
<ol>
<li><strong>UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</strong> (多模态生成与RL应用)</li>
<li><strong>FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</strong> (高分辨率视频生成与高效注意力)</li>
<li><strong>NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</strong> (多模态VLA模型前沿)</li>
<li><strong>Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</strong> (扩散模型的新型训练范式)</li>
<li><strong>SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</strong> (三维重建的效率与稀疏化)</li>
</ol>
<hr />
<p>这份摘要旨在为忙碌的研究人员提供一个快速了解最新进展的窗口，并指明了值得深入研究的方向。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.14761v1">ARC Is a Vision Problem!</a></li>
<li><a href="#2511.14760v1">UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</a></li>
<li><a href="#2511.14759v1"><script type="math/tex">π^{*}_{0.6}</script>: a VLA That Learns From Experience</a></li>
<li><a href="#2511.14751v1">Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers</a></li>
<li><a href="#2511.14719v1">Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</a></li>
<li><a href="#2511.14716v1">Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</a></li>
<li><a href="#2511.14712v1">FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</a></li>
<li><a href="#2511.14691v1">Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</a></li>
<li><a href="#2511.14659v1">NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</a></li>
<li><a href="#2511.14633v1">SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.14761v1'></a></p>
<h2 id="arc-is-a-vision-problem"><a href="https://arxiv.org/abs/2511.14761v1">ARC Is a Vision Problem!</a></h2>
<p><strong>Authors:</strong> Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Yeyin Eva Zhu, Jacob Andreas, Kaiming He</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：ARC Is a Vision Problem!</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>这篇论文的核心贡献在于，它首次将抽象推理能力测试基准 ARC (Abstraction and Reasoning Corpus) 视为一个纯粹的视觉问题，并提出了一种新颖的视觉范式来解决它。通过将 ARC 任务转化为图像到图像的翻译问题，并利用标准的视觉模型（如 Vision Transformer），作者在仅使用 ARC 数据从头训练的情况下，取得了显著优于现有方法的性能，并接近了人类平均水平。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>将 ARC 视为视觉问题（Vision Paradigm）：</strong> 这是最核心的创新。以往的研究多将其视为语言或序列推理问题，而本文作者认为 ARC 的任务本质上是视觉的，因为输入和输出都是网格状的视觉模式。</li>
<li><strong>“画布”表示（"Canvas" Representation）：</strong> 作者将输入数据表示在一个可以被视为自然图像的“画布”上。这种表示方式使得标准的计算机视觉模型能够直接处理 ARC 的输入，而无需复杂的预处理或特征工程。</li>
<li><strong>应用标准视觉架构（Standard Vision Architectures）：</strong> 利用了如 Vision Transformer (ViT) 这样的成熟的视觉模型，直接进行图像到图像的映射。这表明了 ARC 任务的视觉本质，以及现有视觉模型在处理这类抽象推理任务上的潜力。</li>
<li><strong>从头训练（Trained from Scratch）和测试时训练（Test-Time Training）：</strong> 模型仅在 ARC 数据集上从头开始训练，并且利用了测试时训练来进一步提升泛化能力。这强调了模型学习 ARC 任务内在规律的能力，而非依赖于预训练的通用模型。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>重新定义 ARC 的研究方向：</strong> 这项工作可能促使研究社区重新审视 ARC 的本质，并鼓励更多地从视觉角度来探索抽象推理。</li>
<li><strong>推动视觉模型在抽象推理上的应用：</strong> 证明了强大的视觉模型（如 ViT）不仅能处理感知任务，还能在需要高度抽象和推理的任务上取得优异成绩，这为视觉模型开辟了新的应用领域。</li>
<li><strong>为通用人工智能（AGI）研究提供新思路：</strong> 抽象推理是 AGI 的关键组成部分。将视觉能力与抽象推理结合，可能为构建更具通用性的人工智能系统提供新的路径。</li>
<li><strong>挑战现有 LLM 的主导地位：</strong> 在 ARC 这一特定基准上，作者的模型能够与领先的 LLM 相媲美，这表明在某些需要视觉理解和推理的任务上，专门设计的视觉模型可能比通用的 LLM 更具优势。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>程序合成（Program Synthesis）：</strong> ARC 的任务本质上是学习一个隐含的程序来生成输出。视觉模型通过学习图像转换规则，可能为程序合成提供新的视角，特别是针对视觉领域的程序。</li>
<li><strong>机器人感知与规划（Robotics Perception and Planning）：</strong> 机器人需要在复杂环境中理解视觉信息并进行推理以完成任务。ARC 中的抽象推理能力对于提升机器人的智能水平至关重要。</li>
<li><strong>教育与智能辅导系统（Education and Intelligent Tutoring Systems）：</strong> 能够理解和生成视觉模式并进行推理的系统，可以用于开发更具交互性和个性化的教育工具，例如自动生成练习题或提供视觉化的解释。</li>
<li><strong>创意生成（Creative Generation）：</strong> 类似 ARC 中的模式转换和抽象能力，可以应用于艺术、设计等领域的创意生成，例如根据用户输入的视觉风格生成新的图像。</li>
<li><strong>科学发现与数据分析（Scientific Discovery and Data Analysis）：</strong> 在科学研究中，识别数据中的模式、进行抽象和预测是关键。视觉模型在 ARC 中的成功可能启发其在科学数据可视化和模式识别方面的应用。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>ARC 上的性能提升，但并非绝对最优：</strong> 虽然作者声称其模型“ substantially outperforming existing methods that are also trained from scratch” 并且“competitive with those of leading LLMs and close the gap to average human performance”，但它并没有声称超越所有 LLM 或达到人类的最高水平。这意味着在某些方面，LLM 可能仍然具有优势，或者人类的最高水平仍有差距。</li>
<li><strong>对 ARC 数据集的依赖性：</strong> 模型是从头开始在 ARC 数据集上训练的。其泛化能力主要体现在对 ARC 中未见过任务的泛化，但其在其他完全不同领域的泛化能力尚未在摘要中体现。</li>
<li><strong>“测试时训练”的成本：</strong> 虽然测试时训练有助于提高性能，但它也可能增加推理成本和时间，尤其是在需要快速响应的应用场景中。</li>
<li><strong>模型的可解释性：</strong> 摘要中没有提及模型的解释性。虽然 ViT 已经取得了很好的性能，但其内部决策过程的可解释性仍然是一个挑战，尤其是在需要理解推理过程的场景下。</li>
<li><strong>“视觉问题”的定义：</strong> 虽然作者将其定义为视觉问题，但 ARC 本身也包含抽象和逻辑推理的成分。如何精确界定“视觉问题”的范畴，以及模型在多大程度上真正解决了“抽象推理”而非仅仅是模式匹配，可能需要更深入的分析。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文的价值在于它提供了一个全新的视角来解决 ARC 这一重要的抽象推理基准。通过将 ARC 任务“视觉化”，并成功应用了成熟的视觉模型，作者不仅在技术上取得了显著进展，也为计算机视觉和人工智能领域的研究开辟了新的方向。它有力地证明了视觉模型在处理需要抽象推理的任务上的潜力，并可能引发对人工智能通用能力研究的新一轮思考。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch.</li>
<li>Our results are competitive with those of leading LLMs and close the gap to average human performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14761v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14761v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14760v1'></a></p>
<h2 id="unigen-15-enhancing-image-generation-and-editing-through-reward-unification-in-reinforcement-learning"><a href="https://arxiv.org/abs/2511.14760v1">UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Rui Tian, Mingfei Gao, Haiming Gang, Jiasen Lu, Zhe Gan, Yinfei Yang, Zuxuan Wu, Afshin Dehghan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning”的全面中文摘要：</p>
<p><strong>论文摘要：UniGen-1.5：通过强化学习中的奖励统一增强图像生成与编辑</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前统一多模态大语言模型（MLLM）在图像理解、生成和编辑能力上的不足，特别是如何有效地提升图像编辑的精细化控制能力，并实现生成与编辑任务的协同优化。现有的模型在图像编辑方面往往面临指令理解困难、编辑效果不佳等问题，同时缺乏将生成与编辑任务统一起来进行端到端优化的有效方法。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
UniGen-1.5 在 UniGen 的基础上进行了多方面的增强，其核心创新包括：</p>
<ul>
<li><strong>增强的模型架构与训练流程：</strong> 改进了模型架构和训练管线，以增强图像理解和生成能力，并解锁强大的图像编辑能力。</li>
<li><strong>统一的强化学习（RL）策略：</strong> 提出了一种统一的 RL 策略，通过共享的奖励模型，同时优化图像生成和图像编辑任务。这种方法将图像编辑任务重新表述为通用的图像生成任务，并与文本到图像生成任务一起进行训练，从而利用稳定的文本到图像奖励模型来共同提升两者性能。</li>
<li><strong>轻量级的编辑指令对齐（Edit Instruction Alignment）阶段：</strong> 引入了一个轻量级的 Post-SFT（Supervised Fine-Tuning）阶段，旨在显著提升模型对编辑指令的理解能力。该阶段通过将条件图像和编辑指令作为输入，优化模型生成目标图像的语义描述，从而为 RL 训练提供更准确的信号。</li>
<li><strong>多任务联合训练：</strong> 在预训练和监督微调阶段，UniGen-1.5 实现了图像理解、文本到图像生成和图像编辑的联合优化。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
实验结果表明，UniGen-1.5 在图像理解和生成方面表现出竞争力。具体而言：</p>
<ul>
<li><strong>图像编辑：</strong> 在 ImgEdit 基准上取得了 4.31 的整体得分，超越了许多最新的开源模型，并达到了与 GPT-Image-1 等专有模型相当的性能。</li>
<li><strong>图像生成：</strong> 在 GenEval 和 DPG-Bench 基准上分别取得了 0.89 和 86.83 的得分，显著优于 BAGEL 等最先进的模型。</li>
<li><strong>图像理解：</strong> 在图像理解任务上也取得了良好的表现，与同等规模的 SOTA 模型相当。</li>
</ul>
<p>这些结果表明，UniGen-1.5 在统一多模态模型领域取得了显著的进展，尤其是在图像编辑的精细化控制和生成与编辑任务的协同优化方面。它为未来统一多模态模型的研究奠定了坚实的基础。</p>
<p><strong>4. 局限性：</strong>
论文中提到了 UniGen-1.5 的两个主要局限性：</p>
<ul>
<li><strong>文本渲染能力不足：</strong> 模型在准确渲染文本字符方面存在不足，这归因于其轻量级的离散式解码器难以精确控制文本所需的精细结构细节。</li>
<li><strong>视觉一致性问题：</strong> 在图像编辑任务中，模型仍然存在视觉不一致的问题，例如在猫的毛发纹理和形状变化，以及鸟类羽毛颜色差异等方面。</li>
</ul>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>集成扩散模型：</strong> 建议将扩散模型集成到框架中，以解决文本渲染能力不足的问题，从而更好地处理需要精细结构细节的生成任务。</li>
<li><strong>开发专用奖励模型：</strong> 提出需要开发专门的奖励模型来强制执行视觉一致性，以解决图像编辑中的视觉不一致问题。</li>
</ul>
<p>总而言之，UniGen-1.5 通过创新的统一 RL 策略和编辑指令对齐阶段，显著提升了统一多模态模型在图像生成和编辑方面的能力，尤其是在精细化控制和任务协同优化方面取得了突破性进展。尽管存在一些局限性，但其研究成果为未来更强大的多模态模型发展提供了有价值的见解和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing.</li>
<li>Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models.</li>
<li>To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training.</li>
<li>Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance.</li>
<li>Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14760v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14760v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14759v1'></a></p>
<h2 id="_06-a-vla-that-learns-from-experience"><a href="https://arxiv.org/abs/2511.14759v1"><script type="math/tex">π^{*}_{0.6}</script>: a VLA That Learns From Experience</a></h2>
<p><strong>Authors:</strong> Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, Zhiyuan Zhou</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call <script type="math/tex">π^{*}_{0.6}</script>, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the <script type="math/tex">π^{*}_{0.6}</script> model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本研究提出了一种名为 RECAP 的新颖方法，用于通过强化学习（RL）来提升视觉-语言-动作（VLA）模型的真实世界部署能力。RECAP 能够整合异构数据（演示、在线收集数据、专家干预），并通过优势条件策略（advantage conditioning）实现 VLA 的自适应学习。研究成果表明，经过 RECAP 方法训练的 VLA 模型 <script type="math/tex">π^{*}_{0.6}</script> 在折叠衣物、组装盒子和制作浓缩咖啡等复杂任务上表现出色，显著提高了任务吞吐量并降低了失败率。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li>
<p><strong>RECAP 方法论：</strong> 这是论文的核心创新。RECAP 是一种通用的 RL 训练方法，专门针对 VLA 模型设计。其关键在于：</p>
<ul>
<li><strong>优势条件策略（Advantage Conditioning）：</strong> 允许模型根据不同数据源（如演示、在线数据、专家干预）带来的“优势”（即相对于当前策略的改进程度）来调整其学习策略。这使得模型能够更有效地利用不同质量和来源的数据。</li>
<li><strong>异构数据整合：</strong> RECAP 能够无缝地整合多种类型的数据，包括：<ul>
<li><strong>演示数据（Demonstrations）：</strong> 预先录制的成功执行示例。</li>
<li><strong>在线收集数据（On-policy data collection）：</strong> 模型在实际执行过程中收集的数据。</li>
<li><strong>专家远程干预（Expert teleoperated interventions）：</strong> 在模型自主执行过程中，专家通过远程控制进行纠正或指导。这种混合数据策略能够克服单一数据源的局限性，加速学习过程。</li>
</ul>
</li>
<li><strong>离线 RL 预训练 + 在线强化：</strong> 首先使用离线 RL 对通用 VLA 模型 <script type="math/tex">π^{*}_{0.6}</script> 进行预训练，使其具备基础能力，然后通过在机器人上进行数据收集和 RL 训练来进一步优化和专业化模型，以适应特定下游任务。</li>
</ul>
</li>
<li>
<p><strong><script type="math/tex">π^{*}_{0.6}</script> 模型：</strong> 这是论文中用于展示 RECAP 方法的 VLA 模型。其名称中的 "<script type="math/tex">π^{*}_{0.6}</script>" 可能暗示了某种性能指标或训练阶段，但具体含义需要查阅论文原文才能确定。重点在于，这个模型通过 RECAP 方法得到了显著提升。</p>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>加速真实世界机器人部署：</strong> RECAP 方法有望显著缩短机器人学习新技能所需的时间和数据量，降低部署成本和技术门槛。</li>
<li><strong>提升 VLA 模型泛化能力和鲁棒性：</strong> 通过整合多种数据源和利用优势条件策略，VLA 模型能够更好地适应复杂多变的真实世界环境，并处理更广泛的任务。</li>
<li><strong>推动人机协作在机器人学习中的应用：</strong> RECAP 方法明确纳入了专家远程干预，这强调了在机器人自主学习过程中，人类专家的指导和纠正的重要性，为更高效的人机协作机器人学习模式提供了范例。</li>
<li><strong>为其他领域提供通用方法论：</strong> RECAP 的核心思想（优势条件策略、异构数据整合）可能可以推广到其他需要从经验中学习的 RL 应用中。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>家庭服务机器人：</strong> 如摘要中提到的折叠衣物、制作咖啡等任务，直接指向了家庭服务机器人领域。</li>
<li><strong>工业自动化和装配：</strong> 组装盒子的任务表明该方法在需要精细操作和多步骤序列的任务中具有潜力。</li>
<li><strong>自动驾驶：</strong> 尽管摘要未直接提及，但 VLA 模型在理解环境、规划动作方面与自动驾驶有共通之处，RECAP 的学习范式可能有助于提升自动驾驶系统的鲁棒性。</li>
<li><strong>医疗机器人：</strong> 需要精确操作和对环境感知能力强的医疗机器人，也可以借鉴这种学习方法。</li>
<li><strong>虚拟现实/增强现实中的交互：</strong> VLA 模型在理解用户意图和执行动作方面，与 VR/AR 中的交互应用息息相关。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<ul>
<li><strong>计算和数据需求：</strong> 尽管 RECAP 旨在提高效率，但“离线 RL 预训练”和“在机器人上进行数据收集”仍然可能需要大量的计算资源和高质量的初始数据。</li>
<li><strong>专家干预的成本和可扩展性：</strong> 专家远程干预虽然有效，但其成本较高，并且在需要大规模部署时，如何有效且经济地提供专家干预是一个挑战。</li>
<li><strong>“优势条件”的实现细节：</strong> 摘要中提到了“优势条件”，但具体的实现方式、如何量化优势以及如何将其有效应用于策略更新，这些技术细节需要查阅论文原文才能了解其可行性和局限性。</li>
<li><strong><script type="math/tex">π^{*}_{0.6}</script> 的具体能力边界：</strong> 摘要中列举的任务是成功的例子，但模型在其他更复杂或未提及的任务上的表现如何，以及其泛化能力的具体边界，目前尚不清楚。</li>
<li><strong>“真实家庭”的定义：</strong> 摘要提到在“真实家庭”中进行部署，但“真实家庭”的复杂性和多样性程度，以及模型在不同家庭环境中的表现一致性，仍需进一步考察。</li>
</ul>
<p><strong>对计算机视觉领域的趣味性或重要性：</strong></p>
<p>这篇论文对计算机视觉领域具有重要意义，主要体现在以下几个方面：</p>
<ul>
<li><strong>视觉理解与动作生成的深度融合：</strong> VLA 模型本身就是将视觉信息（理解环境）、语言指令（理解任务目标）和动作输出（执行任务）紧密结合的代表。RECAP 方法通过 RL 进一步强化了这种融合，使得模型能够从视觉输入中学习到更精细、更具适应性的动作策略。</li>
<li><strong>从感知到行动的闭环学习：</strong> 论文强调了在真实世界部署中通过 RL 进行“自适应学习”和“自我改进”。这对于计算机视觉而言，意味着模型不再仅仅是静态地识别或理解，而是能够通过与环境的交互，不断优化其视觉感知能力，以更好地服务于下游的动作执行。</li>
<li><strong>利用异构数据提升视觉模型的泛化能力：</strong> 传统上，视觉模型可能依赖于大规模标注数据集。RECAP 提出的整合演示、在线数据和专家干预的方法，为如何利用更丰富、更动态的数据源来提升视觉模型的泛化性和鲁棒性提供了新的思路。特别是专家干预，可以看作是一种“弱监督”或“纠错式”的视觉信息注入，能够帮助模型学习到人类的精细判断和操作技巧。</li>
<li><strong>解决“现实世界鸿沟”：</strong> 计算机视觉研究中一个长期存在的挑战是“现实世界鸿沟”（Sim-to-Real Gap），即在模拟环境中训练的模型在真实环境中表现不佳。RECAP 直接聚焦于真实世界部署中的 RL 训练，通过在真实机器人上收集数据和进行优化，有助于弥合这一鸿沟，使视觉模型在实际应用中更加可靠。</li>
<li><strong>为具身智能（Embodied AI）提供关键技术：</strong> 具身智能是当前人工智能领域的热点，其核心在于让智能体（如机器人）能够通过感知和行动与物理世界进行交互。VLA 模型是实现具身智能的关键组成部分，而 RECAP 方法论则为训练更强大、更具适应性的具身智能体提供了重要的技术支撑。</li>
</ul>
<p>总而言之，这篇论文通过提出 RECAP 方法，展示了如何利用 RL 和异构数据来显著提升 VLA 模型在真实世界任务中的性能，这对计算机视觉在机器人、自动化等领域的实际应用具有深远的推动作用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning.</li>
<li>Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution.</li>
<li>We show that the <script type="math/tex">π^{*}_{0.6}</script> model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14759v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14759v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14751v1'></a></p>
<h2 id="co-me-confidence-guided-token-merging-for-visual-geometric-transformers"><a href="https://arxiv.org/abs/2511.14751v1">Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers</a></h2>
<p><strong>Authors:</strong> Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to <script type="math/tex">11.3\times</script> and <script type="math/tex">7.2\times</script> speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers (Co-Me：置信度引导的 Token 合并用于视觉几何 Transformer)</p>
<p><strong>作者：</strong> Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
视觉几何 Transformer（如 VGGT 和 MapAnything）在 3D 重建和场景理解任务中取得了显著进展，但其计算成本高昂，特别是 Transformer 模型中注意力机制与输入序列长度呈二次方复杂度，严重限制了其在资源受限环境下的实时部署。现有加速方法（如 Token 剪枝）可能导致关键几何信息丢失，而基于相似度的 Token 合并则效果有限。因此，研究如何高效地加速视觉几何 Transformer，同时保持其几何理解能力和重建精度，是一个关键的研究问题。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
本文提出了 <strong>Co-Me (Confidence-Guided Token Merging)</strong>，一种无需重新训练或微调基础模型的加速机制。其核心创新在于：</p>
<ul>
<li><strong>置信度引导的 Token 合并：</strong> Co-Me 引入了一个轻量级的置信度预测器，该预测器通过蒸馏（distillation）自冻结的视觉几何 Transformer 的中间层特征，能够预测每个 Token 的不确定性（即置信度）。</li>
<li><strong>选择性合并低置信度 Token：</strong> 基于预测的置信度，Co-Me 能够识别并选择性地合并低置信度的 Token。这种策略旨在保留高置信度区域的关键几何信息，同时大幅减少计算量。</li>
<li><strong>自监督置信度蒸馏：</strong> 置信度预测器采用自监督方式进行训练，仅需学习 Token 置信度的相对排序，而无需依赖地面真值标签。</li>
<li><strong>高效的合并与分割机制：</strong> Co-Me 设计了高效的 Token 合并（Merge）和分割（Split）算子，并利用优化的 CUDA 内核，最小化了合并操作带来的运行时开销。</li>
<li><strong>注意力偏置校正：</strong> 为了解决 Token 合并可能导致的注意力权重分布失真问题，引入了注意力偏置校正机制，以恢复合并后的注意力分布与原始分布的一致性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
Co-Me 在多种视觉几何 Transformer 模型（VGGT、StreamVGGT、MapAnything）上进行了广泛评估，并在多个下游任务（如单目和多视图深度估计、姿态估计、点云重建）上取得了显著成果：</p>
<ul>
<li><strong>大幅加速：</strong> Co-Me 能够实现显著的推理加速，例如在 VGGT 模型上，当序列长度为 512 帧时，加速比可达 <strong>11.3 倍</strong>，甚至在更高合并率下可达 <strong>26.65 倍</strong>。对于 MapAnything，也实现了 <strong>7.2 倍</strong> 的加速。</li>
<li><strong>保持精度：</strong> 在实现加速的同时，Co-Me 能够保持与原始模型相当的性能，仅有微小的精度下降，尤其是在高置信度区域的几何细节上。</li>
<li><strong>通用性与兼容性：</strong> Co-Me 是一种即插即用的加速模块，可以无缝应用于现有的视觉几何 Transformer 模型，无需修改其架构或进行重新训练。</li>
<li><strong>边缘设备部署：</strong> Co-Me 能够将视觉几何 Transformer 部署到边缘设备（如 NVIDIA Jetson Thor），实现近乎实时的 3D 感知和重建，例如在边缘设备上实现了 3.5 FPS 的更新率，比原始模型快 1.5 倍。</li>
<li><strong>优于其他方法：</strong> 相较于基于相似度的 Token 合并方法，Co-Me 在速度-精度权衡上表现更优。</li>
</ul>
<p><strong>意义：</strong> Co-Me 的提出使得原本计算量巨大的视觉几何 Transformer 能够满足实时性要求，为机器人导航、增强现实等需要快速 3D 感知的应用场景提供了可行方案，是视觉几何领域的一项重要进展。</p>
<p><strong>4. 提及的局限性：</strong>
论文中也提及了一些局限性：</p>
<ul>
<li><strong>细小结构的处理：</strong> 在某些情况下，Co-Me 在处理非常细小或细长的结构时，可能会导致轻微的几何失真（如图 12 所示），这主要是由于低置信度区域的局部分辨率丢失所致。</li>
<li><strong>合并率的权衡：</strong> 合并率（merge ratio）的选择需要在速度和精度之间进行权衡。过高的合并率虽然能带来更大的加速，但也可能导致更明显的精度下降。</li>
<li><strong>特定场景下的性能差异：</strong> 在 KITTI 数据集等图像 Token 空间重叠度较低的场景下，Token 合并可能导致更大的信息损失，加速效果相对较弱。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文最后也展望了一些未来的研究方向：</p>
<ul>
<li><strong>支持非均匀批处理：</strong> 支持不同样本具有不同合并率的非均匀批处理，以实现更精细的加速控制。</li>
<li><strong>流式输入中的时间维度合并：</strong> 将 Token 合并应用于流式输入的时间维度，进一步提升流式 Transformer 的效率。</li>
<li><strong>训练阶段的应用：</strong> 将 Co-Me 应用于训练阶段，以提高训练效率。</li>
<li><strong>更复杂的合并策略：</strong> 探索更复杂的合并策略，例如基于注意力机制的动态合并，以进一步提升性能。</li>
</ul>
<p>总而言之，Co-Me 是一种创新且实用的加速技术，它通过引入置信度引导的 Token 合并，有效解决了视觉几何 Transformer 的计算瓶颈问题，为实现实时、高效的 3D 感知和重建开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14751v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14751v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14719v1'></a></p>
<h2 id="zero-shot-synthetic-video-realism-enhancement-via-structure-aware-denoising"><a href="https://arxiv.org/abs/2511.14719v1">Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</a></h2>
<p><strong>Authors:</strong> Yifan Wang, Liya Ji, Zhanghan Ke, Harry Yang, Ser-Nam Lim, Qifeng Chen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising”的全面中文摘要，重点关注其研究问题、方法创新、主要结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</p>
<p><strong>作者：</strong> Yifan Wang, Liya Ji, Zhanghan Ke, Harry Yang, Ser-Nam Lim, Qifeng Chen</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/研究目标：</strong>
该论文旨在解决合成视频与真实世界视频之间的“域差距”（domain gap）问题，具体而言，是提升模拟器生成的合成视频的真实感，使其达到照片级逼真（photorealistic）的水平。研究的核心挑战在于，在增强真实感的同时，必须精确地保留原始合成视频在空间和时间维度上的多层次结构信息，特别是对于自动驾驶场景中至关重要的安全关键对象（如交通信号灯、路标）的语义和结构一致性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
该研究提出了一种“零样本”（zero-shot）的真实感增强框架，该框架基于预训练的扩散视频基础模型，无需进行额外的微调。其核心创新在于：</p>
<ul>
<li><strong>结构感知去噪（Structure-aware Denoising）：</strong> 引入了一种有效的修改，使得生成/去噪过程能够以从合成视频中估计出的结构感知信息（如深度图、语义图、边缘图）作为条件。这些结构信息是通过一个辅助模型提取的，而非直接从模拟器中获取，这确保了增强后的视频在结构和语义层面与原始视频保持一致。</li>
<li><strong>零样本反演与生成框架（Zero-shot Inversion-and-Generation Framework）：</strong> 借鉴了DDIM Inversion技术，首先将合成视频反演到一个初始的、与视频内容和运动紧密相关的潜在表示（latent representation）。然后，利用这个内容感知的潜在表示作为去噪过程的起点，而不是随机噪声，从而将生成过程锚定在源视频的语义上。</li>
<li><strong>基于ControlNet的条件生成：</strong> 利用ControlNet技术，将提取的结构信息（深度、语义、边缘图）作为条件注入到扩散模型的去噪过程中，以指导生成过程，确保结构的一致性。</li>
<li><strong>Classifier-Free Guidance (CFG) 的应用：</strong> 在结构感知去噪阶段，利用CFG来选择性地修改视觉风格，从而消除合成视频中常见的、不真实的计算机生成纹理，并将整体风格导向模型学习到的真实世界美学。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该方法在实验中取得了显著的成果：</p>
<ul>
<li><strong>优于现有基线：</strong> 在结构一致性方面，该方法优于现有的基线方法，同时保持了最先进的照片级逼真质量。</li>
<li><strong>保持结构和语义一致性：</strong> 成功地在增强真实感的同时，保留了原始视频的关键结构和语义信息，尤其是在处理小物体（如交通信号灯、路标）时表现出色，避免了颜色失真、模糊或形状变形等问题。</li>
<li><strong>提升视频质量：</strong> 相比于逐帧生成的方法，该方法生成的视频具有更好的时间一致性和整体质量。</li>
<li><strong>量化评估：</strong> 通过GPT-40进行主观评估，以及使用LPIPS、VBench、DINO和CLIP等客观指标，证明了其在照片级逼真度、视频质量和对象一致性方面的优势。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>固定推理窗口：</strong> 该方法受限于基础模型的固定推理窗口（121帧），对于更长的视频需要采用分块处理，这可能在分块边界引入时间不连续性。
*   <strong>对文本提示的敏感性：</strong> 作为零样本模型，它对与源视频冲突的文本提示可能较为敏感，有时会产生微小的视觉伪影。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>下游任务验证：</strong> 验证通过该方法增强的合成数据在训练自动驾驶模型时的实际效用，以证明其能否有效弥合合成与真实之间的差距。
*   <strong>处理更长视频：</strong> 探索更有效的技术来处理长视频，以避免时间不连续性。
*   <strong>鲁棒性提升：</strong> 进一步提高模型对不匹配文本提示的鲁棒性，减少视觉伪影的产生。</p>
<p>总而言之，这篇论文提出了一种创新的零样本视频真实感增强方法，通过结构感知去噪和反演-生成范式，在保持关键结构和语义信息的同时，显著提升了合成视频的照片级逼真度，为自动驾驶等领域的数据生成提供了有价值的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion.</li>
<li>Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14719v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14719v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14716v1'></a></p>
<h2 id="diffusion-as-self-distillation-end-to-end-latent-diffusion-in-one-model"><a href="https://arxiv.org/abs/2511.14716v1">Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</a></h2>
<p><strong>Authors:</strong> Xiyuan Wang, Muhan Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet <script type="math/tex">256\times 256</script> conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model (扩散作为自蒸馏：单一模型中的端到端潜在扩散)</p>
<p><strong>作者：</strong> Xiyuan Wang, Muhan Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
标准潜在扩散模型（LDMs）通常采用由编码器、解码器和扩散网络组成的复杂三部分模块化架构，并分阶段训练。这种设计存在计算效率低下、性能次优以及难以与主流的单一网络视觉基础模型统一等问题。本文旨在解决的核心问题是：<strong>能否将编码器、解码器和扩散模型统一到一个单一的、可端到端训练的网络中，从而简化生成流程并提高效率？</strong></p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
作者首先发现，直接将LDM的编码器、解码器和扩散模型进行联合端到端训练会导致灾难性的“潜在空间坍塌”（latent collapse），即扩散训练目标干扰了网络学习良好潜在表示的能力。通过将扩散模型类比为基于自蒸馏（Self-Distillation, SD）的无监督学习方法，作者深入分析了潜在空间坍塌的两个根本原因：
*   <strong>潜在方差抑制（Latent Variance Suppression）：</strong> L2损失项隐式地包含了对潜在表示方差的惩罚，迫使编码器最小化方差，导致潜在向量聚集，引发坍塌。
*   <strong>秩区分能力失效（Failure of Rank Differentiation）：</strong> 标准扩散模型的目标（预测速度）输出高秩信号，这与自蒸馏中要求预测器作为低秩滤波器以避免坍塌的稳定性条件相悖。</p>
<p>基于这些洞察，作者提出了<strong>扩散作为自蒸馏（Diffusion as Self-Distillation, DSD）</strong>框架，通过以下两个关键技术创新来解决坍塌问题：
*   <strong>解耦（Decoupling）：</strong> 通过在目标干净潜在表示上应用stop-gradient（sg）操作符，消除了对潜在方差的梯度惩罚，保护了潜在表示的表达能力。
*   <strong>损失变换（Loss Transformation）：</strong> 分析证明，预测速度的损失在数学上等价于预测干净潜在表示的损失。通过将目标从预测速度转变为预测去噪后的图像潜在表示，迫使扩散模型充当低秩滤波器，从而激活了自蒸馏的稳定秩区分机制。</p>
<p>此外，DSD框架还引入了<strong>EMA更新目标编码器</strong>、<strong>数据增强</strong>以及<strong>辅助损失</strong>（如ViT层对齐、表示级自蒸馏和分类损失）来进一步提升训练稳定性和生成质量。</p>
<p><strong>3. 主要结果及其意义：</strong>
DSD框架成功实现了编码器、解码器和扩散模型在单一Vision Transformer（ViT）骨干网络中的统一，并实现了稳定的端到端训练。
*   <strong>性能优越：</strong> 在ImageNet 256x256条件生成任务上，DSD取得了出色的性能。例如，DSD-B模型仅用2.05亿参数，在无分类器引导（classifier-free guidance）的情况下，取得了FID=4.25的优异成绩，甚至超越了参数量高达7亿的现有先进模型。
*   <strong>参数效率高：</strong> DSD模型在参数量远小于许多基线模型的情况下，取得了相当甚至更优的性能，证明了其高度的参数效率。
*   <strong>可扩展性好：</strong> 实验结果表明，DSD框架具有良好的可扩展性，随着模型尺寸的增加，生成性能显著提升。
*   <strong>统一性：</strong> DSD实现了扩散模型与主流单一网络视觉基础模型的统一，为构建更高效、更通用的视觉模型提供了新的方向。</p>
<p><strong>4. 论文提及的局限性：</strong>
*   <strong>计算资源限制：</strong> 由于计算资源限制，作者未能将DSD扩展到与基线模型相当的更大模型尺寸。
*   <strong>未验证无监督学习能力：</strong> 论文未进行实验来验证DSD作为无监督学习方法的有效性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
虽然论文未明确提出未来研究方向，但其研究成果暗示了以下潜在方向：
*   <strong>更大规模的模型探索：</strong> 进一步扩展DSD模型至更大规模，以验证其在更大模型尺寸下的性能和可扩展性。
*   <strong>作为通用无监督学习方法的研究：</strong> 深入研究DSD在纯粹无监督学习任务上的表现和潜力。
*   <strong>与其他基础模型的融合：</strong> 探索将DSD的统一框架与更多类型的视觉基础模型（如多模态模型）相结合的可能性。
*   <strong>更高效的训练策略：</strong> 进一步优化DSD的训练过程，以缩短训练时间或进一步降低计算成本。</p>
<p>总而言之，这篇论文通过创新的“扩散作为自蒸馏”框架，成功解决了潜在扩散模型在端到端训练中的关键挑战——潜在空间坍塌，实现了模型架构的统一和性能的显著提升，为未来高效、强大的生成模型设计开辟了新途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method.</li>
<li>Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14716v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14716v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14712v1'></a></p>
<h2 id="freeswim-revisiting-sliding-window-attention-mechanisms-for-training-free-ultra-high-resolution-video-generation"><a href="https://arxiv.org/abs/2511.14712v1">FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</a></h2>
<p><strong>Authors:</strong> Yunfeng Wu, Jiayi Song, Zhenxiong Tan, Zihao He, Songhua Liu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本论文提出了一种名为FreeSwim的训练无关（training-free）方法，旨在解决现有Transformer视频生成模型在处理超高分辨率视频时面临的计算成本过高问题。其核心贡献在于引入了一种创新的“内向滑动窗口注意力机制”，并结合了“交叉注意力覆盖策略”和“交叉注意力缓存”，从而在不进行任何额外训练的情况下，实现超高分辨率视频的生成，并保持了视觉细节和全局一致性。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>训练无关（Training-Free）的超高分辨率视频生成：</strong> 这是最核心的创新点。利用已在原生尺度下预训练的视频Diffusion Transformer，通过巧妙的设计，直接用于生成更高分辨率的视频，避免了昂贵的端到端训练。</li>
<li><strong>内向滑动窗口注意力机制（Inward Sliding Window Attention）：</strong><ul>
<li><strong>核心观察：</strong> 论文基于一个关键观察，即为了保持视觉保真度和细节，每个查询（query）token需要保持其在训练时的感受野（receptive field）。</li>
<li><strong>挑战与解决方案：</strong> 传统的局部窗口注意力容易导致内容重复和全局不连贯。FreeSwim通过“内向”的设计（具体实现细节可能在论文正文中，但从摘要推测，可能是在窗口内进行注意力计算，并且窗口会向内移动或扩展以覆盖更多信息）来尝试解决这个问题。</li>
</ul>
</li>
<li><strong>双路径流水线（Dual-Path Pipeline）：</strong><ul>
<li><strong>交叉注意力覆盖策略（Cross-Attention Override Strategy）：</strong> 为了克服局部窗口注意力的局限性，论文设计了一个双路径流水线。一个路径使用窗口注意力，另一个路径则利用“交叉注意力覆盖策略”。这个策略使得局部注意力生成的内容能够被一个具有“完整感受野”的另一分支所指导，从而确保了全局的一致性。</li>
<li><strong>交叉注意力缓存策略（Cross-Attention Caching Strategy）：</strong> 为了提高效率，对于具有完整感受野的那个分支，引入了交叉注意力缓存机制，避免了频繁计算完整的3D注意力。这显著降低了计算复杂度。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>降低超高分辨率视频生成的门槛：</strong> 训练无关的特性极大地降低了生成超高分辨率视频的计算和时间成本，使得更多研究者和开发者能够进行相关实验和应用。</li>
<li><strong>推动训练无关生成模型的发展：</strong> 证明了在复杂生成任务（如高分辨率视频）中，训练无关方法的可行性和有效性，可能激发更多关于如何利用预训练模型进行下游任务的创新。</li>
<li><strong>提升视频生成质量和效率的平衡：</strong> 在保证高分辨率和细节的同时，通过缓存等技术提高了效率，为未来视频生成模型的设计提供了新的思路，即如何在质量和效率之间取得更好的平衡。</li>
<li><strong>为视频编辑和增强提供新工具：</strong> 这种方法可以直接应用于现有视频的超分辨率处理，而无需重新训练，为视频编辑、修复和增强等领域提供了强大的新工具。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>电影和媒体制作：</strong> 能够以更低的成本生成高质量、高分辨率的视频内容，用于特效、动画、后期制作等。</li>
<li><strong>虚拟现实（VR）和增强现实（AR）：</strong> 生成更逼真、更高分辨率的沉浸式内容。</li>
<li><strong>医学影像：</strong> 对医学视频进行超分辨率处理，以获得更清晰的诊断图像。</li>
<li><strong>监控和安防：</strong> 提升低分辨率监控视频的清晰度，便于分析和识别。</li>
<li><strong>游戏开发：</strong> 生成更高质量的游戏过场动画或游戏内资产。</li>
<li><strong>数字孪生和仿真：</strong> 创建更精细、更逼真的数字世界。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>对预训练模型的依赖性：</strong> 该方法高度依赖于预训练的视频Diffusion Transformer。如果预训练模型本身存在局限性（例如，在某些特定类型的视频内容上表现不佳），FreeSwim也可能继承这些局限性。</li>
<li><strong>“内向滑动窗口”的具体实现细节未知：</strong> 摘要中并未详细说明“内向滑动窗口”的具体机制，这可能影响其在不同场景下的泛化能力。例如，窗口的大小、移动步长、以及如何定义“内向”等细节都可能影响最终效果。</li>
<li><strong>“完整感受野”的计算成本：</strong> 尽管有缓存策略，但“完整感受野”分支的计算量可能仍然是整个流程中的瓶颈，尤其是在处理极长或极高分辨率的视频时。</li>
<li><strong>潜在的“幻觉”或不一致性：</strong> 尽管论文声称解决了全局不连贯问题，但任何生成模型都可能存在生成不符合逻辑或“幻觉”内容的风险，尤其是在训练无关的场景下，其控制能力可能不如端到端训练的模型。</li>
<li><strong>对特定类型视频的适应性：</strong> 摘要提到“在VBench上取得了优异的性能”，这表明其在通用视频基准测试上表现良好。但对于非常规或高度专业化的视频内容，其效果可能需要进一步验证。</li>
<li><strong>“训练无关”的定义：</strong> 虽然强调“训练无关”，但其“覆盖策略”和“缓存策略”可能需要一些超参数的调整，这在某种程度上可能需要一些实验性的探索，虽然不是严格意义上的模型权重更新。</li>
</ul>
<p>总而言之，FreeSwim通过巧妙地结合局部与全局信息处理，并利用预训练模型的强大能力，为解决超高分辨率视频生成中的效率和成本问题提供了一个非常有前景的解决方案。其训练无关的特性尤其令人兴奋，有望推动该领域的研究和应用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation.</li>
<li>At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail.</li>
<li>To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency.</li>
<li>Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14691v1'></a></p>
<h2 id="attention-via-synaptic-plasticity-is-all-you-need-a-biologically-inspired-spiking-neuromorphic-transformer"><a href="https://arxiv.org/abs/2511.14691v1">Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</a></h2>
<p><strong>Authors:</strong> Kallol Mondal, Ankush Kumar</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.NE, cs.AI, cs.CV, cs.ET, stat.ML</p>
<p><strong>Abstract:</strong></p>
<p>Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S<script type="math/tex">^{2}</script>TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S<script type="math/tex">^{2}</script>TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer
<strong>作者：</strong> Kallol Mondal, Ankush Kumar</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>现代 Transformer 模型在自然语言处理和计算机视觉等领域取得了巨大成功，但其巨大的计算和能量消耗导致了显著的碳足迹。现有基于脉冲神经网络（SNNs）的 Transformer 模型虽然在能效上有所提升，但其核心的注意力机制仍然依赖于不适合事件驱动脉冲计算的浮点运算（如点积相似度），并且存在冯·诺依曼瓶颈，限制了内存计算能力，且与生物大脑的计算方式存在较大差异。因此，研究如何构建一种在生物学上更具启发性、能效更高、且能充分利用神经形态硬件的 Transformer 注意力机制是本文要解决的核心问题。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>本文提出了一种名为 <strong>Spiking STDP Transformer (S²TDPT)</strong> 的新型神经形态 Transformer 模型。其核心创新在于：</p>
<ul>
<li><strong>基于脉冲时序依赖可塑性（STDP）的注意力机制：</strong> S²TDPT 将注意力计算的权重计算从传统的点积相似度替换为生物学上更真实的 STDP 机制。STDP 是一种核心的记忆和学习机制，它通过精确的脉冲时序交互来编码信息显著性，而不是依赖于脉冲的幅度。这使得注意力权重直接嵌入到突触权重中，实现了内存计算（in-memory computing）。</li>
<li><strong>完全事件驱动和加法运算：</strong> 通过 STDP 机制，模型消除了对 Softmax 等浮点运算的需求，注意力计算完全基于加法操作，这与 SNNs 的事件驱动和低功耗特性高度契合。</li>
<li><strong>消除中间注意力分数矩阵：</strong> STDP 直接在突触层面更新权重，避免了显式计算和存储 N×N 的中间注意力分数矩阵，从而解决了 Transformer 的内存瓶颈问题，显著降低了内存带宽需求。</li>
<li><strong>生物学上的合理性：</strong> 该模型在设计上更贴近大脑的计算原理，通过脉冲时序来计算相关性，而非幅度，增强了模型的生物学合理性和可解释性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>性能表现：</strong> 在 CIFAR-10 和 CIFAR-100 数据集上，S²TDPT 取得了优异的分类准确率，分别为 94.35% 和 78.08%。</li>
<li><strong>能效提升：</strong> 在仅使用四个时间步的情况下，S²TDPT 在 CIFAR-100 上的能耗仅为 0.49 mJ，相比于标准的 ANN Transformer 降低了 88.47%。与现有先进的脉冲 Transformer 模型相比，能效也显著提升（例如，相比 Spikformer 降低 37.97%）。</li>
<li><strong>可解释性：</strong> 通过 Grad-CAM 和脉冲发放率（SFR）图的可视化分析，证明了模型能够关注到语义相关的区域，其注意力机制具有良好的对象中心性和内部可解释性。</li>
<li><strong>意义：</strong> S²TDPT 的成功表明，将生物学上的 STDP 机制引入 Transformer 的注意力计算，不仅能够实现极高的能效，还能保持甚至超越现有模型的性能，同时增强了模型的可解释性，为构建更高效、更具生物学合理性的神经形态 AI 系统提供了新的方向。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>当前实现：</strong> 目前的模型使用了多步 Leaky-Integrate-and-Fire (LIF) 神经元模型。</li>
<li><strong>训练方式：</strong> 目前的训练依赖于 GPU 上的反向传播和量化。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>神经形态硬件实现：</strong> 探索在实际的神经形态硬件上实现 S²TDPT，利用其低功耗和内存计算的优势。</li>
<li><strong>其他 SNN 模型：</strong> 尝试使用其他更先进的脉冲神经元模型（如 CLIF, GLIF, KLIF, PLIF）来进一步提升模型的准确性和能效。</li>
<li><strong>在线学习：</strong> 研究基于设备端的 STDP 的神经形态原生学习方法，实现完全在线的适应性。</li>
<li><strong>更广泛的应用：</strong> 将 S²TDPT 扩展到更大的数据集（如 ImageNet）和更复杂的任务（如大型语言模型）。</li>
<li><strong>超参数调优：</strong> 通过更精细的超参数调优来进一步提升模型性能。</li>
</ul>
<p>总而言之，这篇论文提出了一种创新的、生物学上受启发的脉冲 Transformer 注意力机制，通过利用 STDP 实现了高效的内存计算和极低的能耗，为未来神经形态计算和 AI 的发展开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we propose the Spiking STDP Transformer (S<script type="math/tex">^{2}</script>TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14691v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14691v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14659v1'></a></p>
<h2 id="nora-15-a-vision-language-action-model-trained-using-world-model-and-action-based-preference-rewards"><a href="https://arxiv.org/abs/2511.14659v1">NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</a></h2>
<p><strong>Authors:</strong> Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3句话)</strong></p>
<p>本研究提出了 NORA-1.5，一个在预训练 NORA 模型基础上增强了流匹配动作专家的视觉-语言-动作 (VLA) 模型。通过引入基于世界模型和偏离真实情况的奖励信号进行后训练，NORA-1.5 在模拟和真实世界基准测试中均展现出显著的性能提升和可靠性增强，为构建更可靠的具身智能体提供了新途径。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>架构增强：</strong> 在现有的 NORA 预训练模型基础上，引入了一个<strong>流匹配 (flow-matching) 驱动的动作专家</strong>。流匹配是一种生成模型技术，能够学习数据分布的梯度，在这里可能用于更平滑、更精确地生成动作序列。</li>
<li><strong>创新的奖励模型：</strong> 提出了一个结合了两种信号的奖励模型，用于 VLA 策略的后训练：<ul>
<li><strong>动作条件世界模型 (Action-conditioned World Model, WM)：</strong> 这个模型能够预测给定动作后，环境状态的变化，并评估这些变化是否朝着目标前进。这是一种内在的、基于预测的奖励机制。</li>
<li><strong>偏离真实情况的启发式方法 (Deviation-from-ground-truth heuristic)：</strong> 这是一个更直接的奖励信号，用于区分生成动作的好坏，可能通过与真实世界或期望动作的对比来实现。</li>
</ul>
</li>
<li><strong>直接偏好优化 (Direct Preference Optimization, DPO)：</strong> 利用上述奖励信号构建偏好数据集，并使用 DPO 技术对 NORA-1.5 进行微调。DPO 是一种无需显式价值函数即可直接从偏好数据中学习策略的方法，通常比强化学习更稳定且数据效率更高。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升 VLA 模型在具身任务中的可靠性和泛化能力：</strong> 这是论文的核心目标。通过引入更精细的动作生成机制和更具指导性的奖励信号，NORA-1.5 有望克服当前 VLA 模型在跨环境部署时遇到的可靠性问题。</li>
<li><strong>为具身智能体提供更有效的训练范式：</strong> 结合世界模型预测和偏好学习的奖励机制，为训练更智能、更鲁棒的具身代理提供了一种新的、可能更高效的途径。</li>
<li><strong>推动 VLA 模型在真实世界中的应用：</strong> 论文强调了在真实机器人设置下的评估，表明其研究成果具有实际部署的潜力，可能加速 VLA 模型在机器人导航、操作等领域的落地。</li>
<li><strong>为奖励工程提供新思路：</strong> 论文提出的结合预测性世界模型和直接反馈的奖励设计，为如何设计有效的奖励信号以指导复杂序列生成任务提供了有价值的参考。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学：</strong> 尤其是在需要与物理环境交互的任务中，如家庭服务机器人、工业自动化、自动驾驶等。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 在需要用户与虚拟或增强环境进行自然交互的应用中，如游戏、培训模拟、虚拟助手等。</li>
<li><strong>人机交互：</strong> 提升人与智能系统之间通过自然语言和动作进行交互的流畅性和有效性。</li>
<li><strong>多模态学习：</strong> 进一步探索视觉、语言和动作信息融合的潜力，以及如何利用这些信息进行更复杂的推理和决策。</li>
<li><strong>生成模型：</strong> 流匹配技术的应用也可能为其他序列生成任务提供新的视角。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 引入世界模型和流匹配等复杂组件，可能会增加模型的训练和推理成本。</li>
<li><strong>奖励模型的准确性：</strong> 世界模型的预测能力和偏离真实情况启发式方法的有效性，直接影响奖励信号的质量，进而影响最终模型的性能。如果这些奖励模型不够准确或具有偏差，可能会导致模型学习到次优策略。</li>
<li><strong>对“真实情况”的定义：</strong> “偏离真实情况的启发式方法”依赖于对“真实情况”的定义和获取。在复杂多变的真实世界中，如何准确定义和获取“真实情况”可能是一个挑战。</li>
<li><strong>泛化到全新环境的挑战：</strong> 尽管论文声称提高了泛化能力，但从模拟到真实世界，以及在完全未见过的新环境中，模型的泛化能力仍可能受到限制，需要进一步验证。</li>
<li><strong>对预训练模型的依赖：</strong> NORA-1.5 是基于预训练的 NORA 模型构建的，其性能在一定程度上依赖于 NORA 的基础能力。</li>
</ul>
<p>总而言之，这篇论文提出了一种有前景的 VLA 模型增强和训练方法，通过结合先进的生成模型技术和创新的奖励机制，有望显著提升具身智能体的可靠性和泛化能力，为未来更智能、更实用的具身代理研究开辟了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert.</li>
<li>This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks.</li>
<li>To further improve robustness and task success, we develop a set of reward models for post-training VLA policies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14659v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14659v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14633v1'></a></p>
<h2 id="sparsesurf-sparse-view-3d-gaussian-splatting-for-surface-reconstruction"><a href="https://arxiv.org/abs/2511.14633v1">SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</a></h2>
<p><strong>Authors:</strong> Meiying Gu, Jiawei Zhang, Jiahe Li, Xiaohan Yu, Haonan Luo, Jin Zheng, Xiao Bai</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</p>
<p><strong>作者：</strong> Meiying Gu, Jiawei Zhang, Jiahe Li, Xiaohan Yu, Haonan Luo, Jin Zheng, Xiao Bai</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决在<strong>稀疏视图</strong>条件下，利用3D高斯泼溅（3D Gaussian Splatting, 3DGS）技术进行<strong>精确且细节丰富的表面重建</strong>的难题。现有的3DGS方法在视图充足时表现优异，但在输入图像稀疏时，容易出现<strong>过拟合</strong>现象，导致重建的几何质量下降，并且影响新视角合成（Novel View Synthesis, NVS）的性能。特别是，为了更好地拟合表面几何而采用的<strong>扁平化高斯原语（flattened Gaussian primitives）</strong>，虽然增加了对表面的贴合度，但其固有的各向异性（anisotropy）在稀疏视图下反而加剧了过拟合问题。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
SparseSurf 提出了一种新的方法，通过以下两个关键创新来解决上述挑战：</p>
<ul>
<li><strong>立体几何-纹理对齐（Stereo Geometry-Texture Alignment）：</strong> 该方法的核心在于建立渲染质量与几何估计之间的桥梁。它通过生成立体视图（stereo-view）图像，并利用预训练的立体匹配网络来获取几何先验（如深度图和法线图）。这些先验被用来监督高斯原语的几何形状，从而在稀疏视图下提供更可靠的几何指导。随着训练的进行，渲染质量的提升会反过来改进几何先验的准确性。</li>
<li><strong>伪特征增强几何一致性（Pseudo-Feature Enhanced Geometry Consistency）：</strong> 为了进一步缓解过拟合并增强多视图几何一致性，该方法引入了伪视图（pseudo-view）的监督。它通过<strong>伪特征一致性（Pseudo-view Feature Consistency）</strong>，利用学习到的特征空间来约束高斯原语，使其能够蒸馏并重现丰富的多视图线索。同时，<strong>训练视图特征对齐（Train-view Feature Alignment）</strong>则通过对齐训练视图的特征来提高模型对伪视图噪声的鲁棒性，并进一步提升表面细节。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
SparseSurf 在多个标准数据集（DTU, BlendedMVS, Mip-NeRF360）上的实验结果表明，该方法在<strong>稀疏视图下的表面重建方面取得了最先进（state-of-the-art）的性能</strong>。
*   在DTU数据集上，SparseSurf 获得了最低的平均 Chamfer Distance (CD)，显著优于现有方法，重建的网格在准确性、完整性和细节方面表现更佳。
*   在稀疏视图新视角合成方面，SparseSurf 也取得了优异的性能，能够生成更少伪影、几何对齐更准确的图像。
*   该方法通过引入立体几何先验和伪视图特征约束，有效解决了稀疏视图下3DGS的过拟合问题，实现了在保持高质量新视角合成的同时，获得更精确、更细致的3D表面重建。</p>
<p><strong>4. 提及的局限性：</strong>
论文中提到，尽管 SparseSurf 在稀疏视图下表现出色，但在<strong>极端稀疏的视图条件下，遮挡问题仍然不可避免</strong>，这可能导致在这些区域的表面重建出现困难或失败。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出具体的未来研究方向，但基于其提出的方法和遇到的局限性，可以推测以下潜在方向：
*   <strong>更鲁棒的遮挡处理：</strong> 进一步研究如何更有效地处理稀疏视图下的遮挡区域，以实现更完整的表面重建。
*   <strong>自适应立体基线：</strong> 探索更自适应的立体基线策略，以适应不同场景和视图配置下的立体匹配效果。
*   <strong>更高效的特征蒸馏与对齐：</strong> 优化伪特征增强几何一致性的计算效率，使其能够支持更大规模或更复杂的场景。
*   <strong>与其他几何先验的融合：</strong> 探索将 SparseSurf 的方法与其他的几何先验（如语义信息、先验形状模型等）进行融合，以进一步提升重建质量。</p>
<p><strong>总结：</strong>
SparseSurf 论文的核心贡献在于提出了一种创新的框架，通过<strong>立体几何-纹理对齐</strong>和<strong>伪特征增强几何一致性</strong>，有效解决了3D高斯泼溅在稀疏视图下进行表面重建时面临的过拟合和几何质量下降问题。该方法在多个数据集上取得了显著的性能提升，为在实际应用中（如3D内容创作、虚拟现实等）利用稀疏图像进行高质量3D重建提供了新的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance.</li>
<li>In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering.</li>
<li>In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision.</li>
<li>Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14633v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14633v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-19 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
