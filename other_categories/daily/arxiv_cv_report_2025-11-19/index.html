<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-19 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-18/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-20/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-19">Arxiv Computer Vision Papers - 2025-11-19</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#arc-is-a-vision-problem" class="nav-link">ARC Is a Vision Problem!</a>
                </li>
                <li class="nav-item">
                    <a href="#unigen-15-enhancing-image-generation-and-editing-through-reward-unification-in-reinforcement-learning" class="nav-link">UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#_06-a-vla-that-learns-from-experience" class="nav-link">π^{*}_{0.6}: a VLA That Learns From Experience</a>
                </li>
                <li class="nav-item">
                    <a href="#co-me-confidence-guided-token-merging-for-visual-geometric-transformers" class="nav-link">Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#zero-shot-synthetic-video-realism-enhancement-via-structure-aware-denoising" class="nav-link">Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-as-self-distillation-end-to-end-latent-diffusion-in-one-model" class="nav-link">Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</a>
                </li>
                <li class="nav-item">
                    <a href="#freeswim-revisiting-sliding-window-attention-mechanisms-for-training-free-ultra-high-resolution-video-generation" class="nav-link">FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#attention-via-synaptic-plasticity-is-all-you-need-a-biologically-inspired-spiking-neuromorphic-transformer" class="nav-link">Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#nora-15-a-vision-language-action-model-trained-using-world-model-and-action-based-preference-rewards" class="nav-link">NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</a>
                </li>
                <li class="nav-item">
                    <a href="#sparsesurf-sparse-view-3d-gaussian-splatting-for-surface-reconstruction" class="nav-link">SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-19">Arxiv Computer Vision Papers - 2025-11-19</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年11月18日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年11月18日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的快速进展：</p>
<ul>
<li><strong>多模态与通用模型（Multimodality &amp; Generalist Models）：</strong> 涌现出多篇致力于构建更通用、能够理解和生成多种模态（视觉、语言、动作）的模型，强调了模型在不同任务和数据类型上的泛化能力。</li>
<li><strong>生成模型（Generative Models）的持续演进：</strong> 特别是在图像和视频生成领域，研究人员在提升生成质量、可控性以及效率方面取得了显著进展，例如通过强化学习、扩散模型和新颖的注意力机制。</li>
<li><strong>效率与可扩展性（Efficiency &amp; Scalability）：</strong> 针对高分辨率数据和大规模模型训练的挑战，出现了如稀疏化、注意力机制优化等技术，旨在提高计算效率和模型的可扩展性。</li>
<li><strong>生物启发式方法（Biologically Inspired Approaches）：</strong> 开始探索将生物神经科学的原理（如突触可塑性）应用于构建更高效、更具适应性的模型。</li>
<li><strong>三维视觉（3D Vision）的进步：</strong> 在三维表面重建方面，出现了利用稀疏视图数据的高效方法。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>“ARC Is a Vision Problem!” (Hu et al.)</strong> 提出将 ARC（Abstraction and Reasoning Corpus）这一经典的抽象推理任务视为一个视觉问题，暗示了通用人工智能（AGI）的视觉理解能力可能与抽象推理能力紧密相连，具有重要的理论意义。</li>
<li><strong>“UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning” (Tian et al.)</strong> 和 <strong>“NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards” (Hung et al.)</strong> 都强调了通过统一的奖励机制（Reward Unification）来提升生成和多模态模型的性能，尤其是在强化学习的框架下，这为训练更强大、更具指令遵循能力的模型提供了新思路。</li>
<li><strong>“Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer” (Mondal, Kumar)</strong> 引入了生物启发式的脉冲神经形态Transformer，将突触可塑性机制融入注意力机制，为开发更节能、更类脑的AI模型开辟了新方向。</li>
<li><strong>“SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction” (Gu et al.)</strong> 在三维重建领域，利用稀疏视图数据实现高效的3D高斯溅射重建，对于在数据受限场景下的三维场景理解和建模具有实际应用价值。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>强化学习在生成模型中的深度应用：</strong> 通过精心设计的奖励函数来指导图像、视频生成和多模态模型的训练，实现更精细的控制和更高的质量。</li>
<li><strong>世界模型（World Models）与动作预测：</strong> 将世界模型和动作预测能力整合到视觉语言模型中，以实现更强的规划和交互能力。</li>
<li><strong>生物神经形态计算的融合：</strong> 将生物学原理（如脉冲神经网络、突触可塑性）与Transformer等现代深度学习架构相结合。</li>
<li><strong>高效注意力机制的探索：</strong> 针对超高分辨率视频生成等计算密集型任务，继续优化注意力机制，如滑动窗口注意力。</li>
<li><strong>零样本（Zero-shot）和训练无关（Training-free）的生成与编辑：</strong> 发展能够在无需特定领域训练的情况下，对生成内容进行增强或编辑的技术。</li>
</ul>
<p><strong>建议阅读的论文：</strong></p>
<p>考虑到其潜在的理论影响、技术创新性和对未来研究方向的指引作用，以下论文值得优先阅读：</p>
<ol>
<li><strong>“ARC Is a Vision Problem!” (Hu et al.)</strong>: 对于理解通用人工智能的视觉能力和抽象推理之间的关系至关重要。</li>
<li><strong>“UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning” (Tian et al.)</strong> 和 <strong>“NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards” (Hung et al.)</strong>: 这两篇论文代表了多模态和生成模型在训练策略上的重要进展，对于构建更智能、更具交互性的AI系统具有指导意义。</li>
<li><strong>“Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer” (Mondal, Kumar)</strong>: 探索了AI与生物学融合的未来方向，可能带来革命性的计算范式。</li>
<li><strong>“FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation” (Wu et al.)</strong>: 对于解决高分辨率视频生成中的计算瓶颈和效率问题，提供了实用的技术方案。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速把握本期 Arxiv 论文的关键信息，以便您能更有效地规划阅读和研究方向。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.14761v1">ARC Is a Vision Problem!</a></li>
<li><a href="#2511.14760v1">UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</a></li>
<li><a href="#2511.14759v1"><script type="math/tex">π^{*}_{0.6}</script>: a VLA That Learns From Experience</a></li>
<li><a href="#2511.14751v1">Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers</a></li>
<li><a href="#2511.14719v1">Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</a></li>
<li><a href="#2511.14716v1">Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</a></li>
<li><a href="#2511.14712v1">FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</a></li>
<li><a href="#2511.14691v1">Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</a></li>
<li><a href="#2511.14659v1">NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</a></li>
<li><a href="#2511.14633v1">SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.14761v1'></a></p>
<h2 id="arc-is-a-vision-problem"><a href="https://arxiv.org/abs/2511.14761v1">ARC Is a Vision Problem!</a></h2>
<p><strong>Authors:</strong> Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Yeyin Eva Zhu, Jacob Andreas, Kaiming He</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：ARC Is a Vision Problem!</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>这篇论文的核心贡献在于，它首次将抽象推理领域的经典数据集 ARC (Abstraction and Reasoning Corpus) 视为一个纯粹的视觉问题，并成功地构建了一个基于视觉模型（Vision Transformer）的解决方案。通过将 ARC 任务转化为图像到图像的翻译问题，该研究在仅使用 ARC 数据集进行训练的情况下，取得了显著优于现有从头训练方法的性能，并接近了人类平均水平。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>视觉范式重构 (Vision Paradigm Reframing):</strong> 最关键的创新是将 ARC 这个通常被视为语言或符号推理问题的任务，彻底地从视觉角度进行重新定义。</li>
<li><strong>“画布”表示 (Canvas Representation):</strong> 论文引入了一个“画布”的概念来表示输入，使其能够被当作标准的自然图像来处理。这使得现有的成熟的计算机视觉模型能够直接应用于 ARC 任务。</li>
<li><strong>标准视觉架构的应用 (Application of Standard Vision Architectures):</strong> 利用了标准的视觉模型，例如 vanilla Vision Transformer (ViT)，来执行图像到图像的映射。这表明了通用视觉模型在处理抽象推理任务上的潜力。</li>
<li><strong>从头训练与测试时训练 (Training from Scratch &amp; Test-Time Training):</strong> 模型完全从零开始仅在 ARC 数据上训练，并且通过测试时训练 (test-time training) 来实现对未见过任务的泛化。这强调了模型学习 ARC 核心规律的能力，而非依赖预训练的通用知识。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>重新定义抽象推理的研究范式:</strong> 这项工作可能促使研究人员重新审视其他抽象推理数据集和任务，探索其潜在的视觉本质，从而开辟新的研究方向。</li>
<li><strong>推动通用视觉模型在复杂推理任务中的应用:</strong> 证明了像 ViT 这样的标准视觉模型不仅能处理感知任务，还能在一定程度上解决需要抽象推理的认知任务，这极大地扩展了通用视觉模型的应用边界。</li>
<li><strong>为 ARC 数据集的研究提供新的基线:</strong> VARC 模型在从头训练的基准上取得了显著的性能提升，为后续研究提供了更强的竞争性基线。</li>
<li><strong>缩小与人类在抽象推理上的差距:</strong> 接近人类平均水平的性能表明，视觉模型在理解和执行抽象规则方面取得了重要进展。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>教育和儿童学习:</strong> ARC 的核心在于学习和应用规则，这与儿童学习新概念和解决问题的过程类似。这项研究可能为开发更智能的教育软件或辅助学习工具提供思路。</li>
<li><strong>机器人和自动化:</strong> 机器人需要理解和执行复杂的指令，并根据环境进行推理。将视觉推理能力应用于机器人控制和任务规划，可以使其更加智能和灵活。</li>
<li><strong>人工智能的通用性研究:</strong> 这项工作是朝着实现更通用人工智能（AGI）迈出的一步，展示了如何将不同类型的智能能力（感知与推理）融合到单一模型中。</li>
<li><strong>图像生成和编辑:</strong> 尽管 ARC 的目标是推理，但其图像到图像的转换特性也可能启发更高级的图像编辑和内容生成技术，能够根据抽象规则生成或修改图像。</li>
<li><strong>符号AI与连接主义AI的融合:</strong> 这项工作展示了如何用连接主义的视觉模型来解决原本更偏向符号AI的问题，为两种AI范式之间的融合提供了新的视角。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>对 ARC 数据集的依赖性:</strong> 虽然模型在 ARC 数据集上表现出色，但其泛化能力在多大程度上能迁移到其他不同类型或更复杂的抽象推理任务上，仍需进一步验证。</li>
<li><strong>“画布”表示的普适性:</strong> “画布”表示是否适用于所有类型的视觉推理任务，或者是否需要针对特定任务进行调整，这一点尚未明确。</li>
<li><strong>模型的可解释性:</strong> 尽管模型性能优异，但 Vision Transformer 在解释其决策过程方面通常存在挑战。理解模型是如何进行抽象推理的，将是未来研究的重要方向。</li>
<li><strong>与 LLMs 的直接比较:</strong> 摘要提到“竞争力”和“缩小差距”，但并未提供具体的性能指标对比，例如在特定任务上的准确率或效率。与顶尖 LLMs 的详细比较分析可能需要阅读全文。</li>
<li><strong>“测试时训练”的成本:</strong> 测试时训练 (test-time training) 是一种有效的泛化手段，但它可能会增加推理时的计算成本和时间。</li>
</ul>
<p><strong>总结来说，这篇论文的价值在于它打破了对 ARC 数据集的传统认知，成功地将其转化为一个视觉问题，并利用了强大的视觉模型取得了突破性的进展。这不仅为 ARC 的研究开辟了新路径，也为通用视觉模型在更广泛的认知任务中的应用提供了强有力的证据，具有重要的理论和实践意义。</strong></p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch.</li>
<li>Our results are competitive with those of leading LLMs and close the gap to average human performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14761v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14761v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14760v1'></a></p>
<h2 id="unigen-15-enhancing-image-generation-and-editing-through-reward-unification-in-reinforcement-learning"><a href="https://arxiv.org/abs/2511.14760v1">UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Rui Tian, Mingfei Gao, Haiming Gang, Jiasen Lu, Zhe Gan, Yinfei Yang, Zuxuan Wu, Afshin Dehghan</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning”的中文摘要，其中包含了您要求的各个方面：</p>
<p><strong>论文摘要：UniGen-1.5：通过强化学习中的奖励统一增强图像生成与编辑</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前统一多模态大语言模型（MLLM）在图像理解、生成和编辑能力上的不足，特别是如何有效地提升图像编辑的精细化控制能力，并实现生成与编辑任务的协同优化。现有模型在处理复杂的编辑指令理解和生成与编辑任务的联合训练方面存在挑战。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
UniGen-1.5 的核心创新在于其增强的架构和训练流程，主要体现在以下几个方面：
*   <strong>统一的强化学习（RL）策略：</strong> 提出了一种创新的统一 RL 策略，通过共享奖励模型，同时优化图像生成和图像编辑任务。这种方法能够更有效地利用文本到图像生成任务中成熟的奖励机制来指导图像编辑。
*   <strong>编辑指令对齐（Edit Instruction Alignment）阶段：</strong> 引入了一个轻量级的“编辑指令对齐”后 SFT（Supervised Fine-Tuning）阶段。该阶段专注于提升模型对编辑指令的理解能力，使其能更准确地把握目标编辑图像的语义内容，这对 RL 训练的成功至关重要。
*   <strong>增强的模型架构与训练管线：</strong> 在 UniGen 的基础上，全面增强了模型架构和训练管线，以提升图像理解和生成能力，并解锁强大的图像编辑能力。</p>
<p><strong>3. 主要结果与意义：</strong>
实验结果表明，UniGen-1.5 在图像理解、生成和编辑方面均取得了具有竞争力的性能。
*   在图像生成方面，UniGen-1.5 在 GenEval 和 DPG-Bench 基准上分别取得了 0.89 和 86.83 的分数，显著优于包括 BAGEL 在内的许多先进模型。
*   在图像编辑方面，UniGen-1.5 在 ImgEdit 基准上获得了 4.31 的总分，超越了许多最新的开源模型，并达到了与 GPT-Image-1 等专有模型相当的性能。
*   这些结果表明 UniGen-1.5 在统一多模态模型领域取得了显著进展，为未来的研究奠定了坚实的基础。</p>
<p><strong>4. 局限性：</strong>
论文中提到了 UniGen-1.5 的两个主要局限性：
*   <strong>文本渲染能力不足：</strong> 模型在准确渲染文本字符方面存在不足，这归因于其轻量级的离散式解码器难以精确控制文本的精细结构细节。
*   <strong>视觉一致性挑战：</strong> 在图像编辑任务中，模型仍然存在视觉不一致的问题，例如在猫的毛发纹理和形状变化，以及鸟类羽毛颜色差异等方面，这表明在强制执行视觉一致性方面仍需改进。</p>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，论文指出了以下未来研究方向：
*   <strong>集成扩散模型：</strong> 建议将扩散模型集成到框架中，以解决文本渲染能力不足的问题，从而更好地处理需要精细结构细节的生成任务。
*   <strong>开发专门的奖励模型：</strong> 提出需要开发专门的奖励模型来强制执行视觉一致性，以解决图像编辑中的视觉不一致性问题。</p>
<p>总而言之，UniGen-1.5 通过引入创新的统一 RL 策略和编辑指令对齐阶段，显著提升了统一多模态大语言模型在图像生成和编辑方面的能力，并在多个基准测试中取得了最先进的性能。尽管存在一些局限性，但该研究为未来在图像生成和编辑领域的研究提供了重要的方向和基线。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing.</li>
<li>Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models.</li>
<li>To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training.</li>
<li>Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance.</li>
<li>Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14760v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14760v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14759v1'></a></p>
<h2 id="_06-a-vla-that-learns-from-experience"><a href="https://arxiv.org/abs/2511.14759v1"><script type="math/tex">π^{*}_{0.6}</script>: a VLA That Learns From Experience</a></h2>
<p><strong>Authors:</strong> Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, Zhiyuan Zhou</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call <script type="math/tex">π^{*}_{0.6}</script>, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the <script type="math/tex">π^{*}_{0.6}</script> model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本研究提出了一种名为 RECAP 的新方法，用于通过强化学习（RL）来提升视觉-语言-动作（VLA）模型在真实世界部署中的表现。RECAP 能够整合多种异构数据源，并通过优势条件策略（advantage conditioning）进行训练，从而实现 VLA 模型的自我改进。研究成果展示了其预训练模型 <script type="math/tex">π^{*}_{0.6}</script> 在折叠衣物、组装盒子和制作浓缩咖啡等复杂任务上取得了显著的性能提升。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>RECAP 方法的核心创新在于其<strong>优势条件策略（advantage conditioning）</strong>的引入，以及<strong>整合异构数据源</strong>的能力。</p>
<ul>
<li><strong>优势条件策略 (Advantage Conditioning):</strong> 这是 RECAP 的一个关键技术点。在强化学习中，优势函数（advantage function）衡量了某个动作相对于平均策略的好坏。通过将策略的输出（动作）条件化在优势函数上，RECAP 能够更有效地引导模型学习到“好”的动作，从而加速学习过程并提高性能。这是一种更精细的策略优化方式，能够让模型理解哪些行为是真正有益的。</li>
<li><strong>整合异构数据源:</strong> RECAP 能够融合多种不同类型的数据，包括：<ul>
<li><strong>演示数据 (Demonstrations):</strong> 来自人类或其他模型的预先录制的行为轨迹。</li>
<li><strong>在线策略收集数据 (On-policy collection):</strong> 模型在实际执行任务过程中产生的数据。</li>
<li><strong>专家远程干预数据 (Expert teleoperated interventions):</strong> 在模型自主执行过程中，由人类专家进行实时纠正或指导产生的数据。
这种多模态、多来源的数据融合能力，使得模型能够从更广泛的经验中学习，克服单一数据源的局限性。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动 VLA 模型在真实世界中的广泛应用:</strong> 本研究直接解决了 VLA 模型在真实世界部署中的性能瓶颈。通过 RECAP 方法，VLA 模型能够更可靠、更高效地执行复杂任务，这将极大地加速其在机器人、智能助手等领域的落地应用。</li>
<li><strong>提升强化学习在复杂任务中的效率和鲁棒性:</strong> RECAP 的优势条件策略和数据融合方法，为在复杂、高维度的真实世界环境中进行强化学习训练提供了新的思路。这可能为其他需要从经验中学习的 RL 应用带来启发。</li>
<li><strong>降低机器人学习的门槛:</strong> 通过能够从多种数据源（包括演示和专家干预）中学习，RECAP 有可能减少对大量高质量标注数据或完全自主探索的需求，从而降低机器人学习的门槛。</li>
<li><strong>为通用机器人智能奠定基础:</strong> <script type="math/tex">π^{*}_{0.6}</script> 作为预训练的“通才”模型，展示了通过持续学习和特化，可以适应多种不同任务的能力。这符合构建更通用、更智能机器人的长期愿景。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学:</strong><ul>
<li><strong>家庭服务机器人:</strong> 如论文中提到的折叠衣物、制作咖啡等任务，以及其他家务劳动。</li>
<li><strong>工业自动化:</strong> 机器人装配、拾取、放置等任务的自动化和优化。</li>
<li><strong>物流和仓储:</strong> 机器人分拣、搬运等。</li>
</ul>
</li>
<li><strong>人机交互:</strong><ul>
<li><strong>智能助手:</strong> 更自然、更智能的语音和视觉交互，能够理解并执行更复杂的指令。</li>
<li><strong>虚拟现实/增强现实:</strong> 虚拟环境中的智能代理，能够与用户进行更真实的互动。</li>
</ul>
</li>
<li><strong>自动驾驶:</strong> 虽然摘要未直接提及，但 VLA 模型在理解环境、规划动作方面的能力，与自动驾驶中的感知、决策环节有共通之处。</li>
<li><strong>医疗保健:</strong> 辅助医疗机器人进行精细操作，或用于康复训练。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<ul>
<li><strong>计算资源需求:</strong> 尽管 RECAP 提高了学习效率，但训练如此复杂的 VLA 模型，尤其是在真实世界进行数据收集和 RL 训练，很可能需要大量的计算资源和时间。</li>
<li><strong>数据收集的成本和安全性:</strong> 在真实家庭环境中进行数据收集，需要考虑数据隐私、安全以及潜在的意外情况。专家远程干预也需要投入人力成本。</li>
<li><strong>泛化能力:</strong> 虽然模型在演示的几个任务上表现出色，但其在未见过的新任务上的泛化能力仍需进一步验证。摘要中提到的“通用”可能更多是指其作为基础模型的能力，而非在所有任务上都能达到顶尖水平。</li>
<li><strong>“经验”的定义和质量:</strong> RECAP 依赖于“经验”，但经验的质量、多样性和覆盖范围对最终性能至关重要。如果收集到的经验存在偏差或不足，模型性能也会受到限制。</li>
<li><strong>“专业”的定义:</strong> 摘要提到“专业咖啡机”，这暗示了模型可能需要针对特定领域的专业知识或设备进行微调，其通用性可能存在一定边界。</li>
<li><strong><script type="math/tex">π^{*}_{0.6}</script> 的具体含义:</strong> 摘要中 <script type="math/tex">π^{*}_{0.6}</script> 的命名方式（带有数字 0.6）可能暗示了某种参数设置、性能指标或训练阶段的特定含义，但具体细节并未在摘要中展开，需要阅读全文才能理解。</li>
</ul>
<p>总而言之，这篇论文提出的 RECAP 方法及其预训练模型 <script type="math/tex">π^{*}_{0.6}</script>，在解决 VLA 模型在真实世界部署中的挑战方面，展现了令人兴奋的潜力。其核心创新在于利用优势条件策略和异构数据融合来加速和优化强化学习过程，有望为机器人和人工智能的实际应用带来重大突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning.</li>
<li>Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution.</li>
<li>We show that the <script type="math/tex">π^{*}_{0.6}</script> model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14759v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14759v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14751v1'></a></p>
<h2 id="co-me-confidence-guided-token-merging-for-visual-geometric-transformers"><a href="https://arxiv.org/abs/2511.14751v1">Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers</a></h2>
<p><strong>Authors:</strong> Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to <script type="math/tex">11.3\times</script> and <script type="math/tex">7.2\times</script> speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers (Co-Me：置信度引导的 Token 合并用于视觉几何 Transformer)</p>
<p><strong>作者：</strong> Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>视觉几何 Transformer（如 VGGT 和 MapAnything）在 3D 重建和场景理解方面取得了显著进展，但其计算成本高昂，尤其是 Transformer 模型中注意力机制与输入序列长度呈二次方复杂度，这严重限制了它们在资源受限环境下的实时部署。现有加速方法（如 Token Pruning）可能导致关键几何信息丢失，而基于特征相似度的 Token 合并方法则效果有限。因此，研究如何有效加速视觉几何 Transformer，同时保持其几何理解能力和重建精度，是一个关键的挑战。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>本文提出了 <strong>Co-Me (Confidence-Guided Token Merging)</strong>，一种无需重新训练或微调基础模型即可加速视觉几何 Transformer 的机制。其核心创新在于：</p>
<ul>
<li><strong>置信度引导的 Token 合并：</strong> Co-Me 引入了一个轻量级的置信度预测器，该预测器通过蒸馏（distillation）自冻结的 Transformer 编码器中提取的中间特征，来估计每个 Token 的不确定性（即置信度）。</li>
<li><strong>选择性合并低置信度 Token：</strong> 基于预测的置信度分数，Co-Me 生成一个合并掩码，选择性地合并低置信度的 Token。这种方法旨在保留高置信度区域（通常包含重要的几何信息）的细节，同时减少低置信度区域（通常是背景或纹理稀疏区域）的计算量。</li>
<li><strong>自监督置信度蒸馏：</strong> 置信度预测器采用自监督方式进行训练，仅依赖于 Token 的相对置信度排序，而无需额外的标注数据。</li>
<li><strong>高效的合并与拆分操作：</strong> 论文设计了高效的合并（merge）和拆分（split）操作，并实现了优化的 CUDA 内核，以最小化合并过程带来的运行时开销。</li>
<li><strong>注意力偏置校正（Attention Bias Correction）：</strong> 为了解决 Token 合并可能导致的注意力分布失真问题，引入了注意力偏置校正机制，通过添加一个 log n 的偏置项来恢复合并后注意力权重的原始分布，从而提升性能。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著的加速效果：</strong> Co-Me 在 VGGT 和 MapAnything 模型上实现了显著的加速，在 VGGT 上可达 <strong>11.3 倍</strong>，在 MapAnything 上可达 <strong>7.2 倍</strong> 的速度提升，且加速效果随序列长度的增加而增强。</li>
<li><strong>保持几何精度：</strong> 尽管进行了大量的 Token 合并，Co-Me 在深度估计、姿态估计和点云重建等任务上，仅造成了非常小的性能下降，甚至在某些情况下（如 DTU 数据集上的点云重建）性能略有提升。</li>
<li><strong>通用性与兼容性：</strong> Co-Me 是一种即插即用的加速机制，可以无缝应用于多种视觉几何 Transformer 模型，并且可以与现有的加速策略（如高效注意力机制）组合使用，以获得额外的加速。</li>
<li><strong>边缘设备部署能力：</strong> Co-Me 使得视觉几何 Transformer 能够部署在 NVIDIA Jetson Thor 等边缘设备上，实现近乎实时的 3D 感知和重建，为机器人导航、增强现实等应用提供了可能。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>细小或细长结构的重建问题：</strong> 在某些失败案例中（如图 12 所示），当低置信度区域包含细小或细长的结构时（如路灯杆、自由女神像的手臂），Co-Me 的合并操作可能导致这些结构被过度平滑，从而影响局部细节的重建。</li>
<li><strong>合并比率的选择：</strong> 合并比率（merge ratio）的设置会影响速度和精度的权衡，需要根据具体任务和性能要求进行调整。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>支持非均匀批处理（Non-uniform batching）：</strong> 支持不同样本具有不同合并比率的批处理，以实现更灵活的加速。</li>
<li><strong>流式输入中的时间维度合并：</strong> 将 Token 合并应用于流式输入的时间维度，以进一步提升流式场景下的处理效率。</li>
<li><strong>训练阶段的应用：</strong> 将 Co-Me 应用于训练阶段，以提高训练效率。</li>
<li><strong>处理细小结构的方法：</strong> 进一步研究如何改进 Co-Me 在处理细小或细长结构时的鲁棒性。</li>
</ul>
<p>总而言之，Co-Me 提出了一种新颖且有效的置信度引导 Token 合并方法，成功地解决了视觉几何 Transformer 的计算效率瓶颈问题，同时保持了其关键的几何理解能力，为实现实时、高效的 3D 感知和重建开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14751v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14751v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14719v1'></a></p>
<h2 id="zero-shot-synthetic-video-realism-enhancement-via-structure-aware-denoising"><a href="https://arxiv.org/abs/2511.14719v1">Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising</a></h2>
<p><strong>Authors:</strong> Yifan Wang, Liya Ji, Zhanghan Ke, Harry Yang, Ser-Nam Lim, Qifeng Chen</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising”的全面中文摘要，重点突出其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文题目：</strong> Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising (通过结构感知去噪实现零样本合成视频真实感增强)</p>
<p><strong>作者：</strong> Yifan Wang, Liya Ji, Zhanghan Ke, Harry Yang, Ser-Nam Lim, Qifeng Chen</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决合成视频与真实世界视频之间的“域差距”问题。自动驾驶等领域需要大量数据进行模型训练，而真实世界数据的收集和标注成本高昂且难以覆盖所有极端场景。合成数据虽然可扩展，但其逼真度不足，可能导致模型在真实场景中表现不佳。现有的视频风格转换方法虽然能提升逼真度，但往往难以保持原始视频的结构和语义信息，尤其是在小物体（如交通信号灯、路标）的细节上。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
该研究提出了一种<strong>零样本（zero-shot）的合成视频真实感增强框架</strong>，核心在于利用<strong>结构感知去噪</strong>。其主要创新点包括：</p>
<ul>
<li><strong>基于预训练扩散视频模型的零样本框架：</strong> 该方法直接利用现有的、强大的扩散视频基础模型，无需针对特定任务进行额外的微调，大大降低了应用门槛。</li>
<li><strong>结构感知信息注入：</strong> 关键在于将合成视频的<strong>多层次结构信息（如深度图、语义图、边缘图）</strong>作为去噪过程的条件。与直接从模拟器提取信息不同，该方法通过一个辅助模型<strong>估计</strong>这些结构信息，并将其注入到扩散模型的生成/去噪过程中。</li>
<li><strong>DDIM Inversion 与 ControlNet 的结合：</strong> 借鉴了 DDIM Inversion 技术，将合成视频“编码”到一个与原始视频内容和运动相关的<strong>初始潜在表示（latent representation）</strong>中。然后，利用 ControlNet 结构将估计的结构信息作为条件，引导扩散模型的去噪过程，从而在生成逼真视频的同时，锚定其结构和语义。</li>
<li><strong>结构与语义的一致性保证：</strong> 通过结构感知信息的引导，确保增强后的视频在空间和时间维度上都与原始合成视频保持高度一致，尤其是在关键的、安全相关的物体上。</li>
<li><strong>Classifier-Free Guidance (CFG) 的应用：</strong> 在去噪阶段，利用 CFG 技术来选择性地修改视觉风格，去除计算机生成的纹理，并将整体风格导向模型学习到的真实世界美学。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能超越：</strong> 实验结果表明，该方法在<strong>结构一致性</strong>方面显著优于现有的基线方法，同时保持了<strong>最先进的图像逼真度</strong>。
*   <strong>小物体细节的精确还原：</strong> 在处理交通信号灯、路标等小而关键的物体时，该方法能够更准确地保留其颜色、形状和语义信息，解决了现有方法容易出现的颜色失真、模糊或形状扭曲等问题。
*   <strong>时间一致性提升：</strong> 相较于逐帧生成的方法，该方法通过视频生成模型，显著提高了生成视频的<strong>时间一致性</strong>和整体<strong>视频质量</strong>。
*   <strong>意义：</strong> 该研究为解决合成数据与真实数据之间的域差距提供了一个有效且通用的解决方案。它不仅提升了合成视频的逼真度，更重要的是保证了其结构和语义的完整性，这对于训练更鲁棒、更可靠的自动驾驶模型至关重要。该方法为生成高质量的合成训练数据提供了一个新的视角和强大的工具。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>基础模型窗口限制：</strong> 该方法受限于基础扩散模型的固定推理窗口（例如 121 帧），对于更长的视频需要采用分块处理，这可能在块边界引入时间不连续性。
*   <strong>对文本提示的敏感性：</strong> 作为零样本模型，当文本提示与源视频内容冲突时，可能会产生微小的视觉伪影。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>下游任务验证：</strong> 验证通过该方法增强的合成数据在实际自动驾驶模型训练中的有效性，以证明其能否有效缩小合成-真实域差距。
*   <strong>处理更长视频：</strong> 探索如何克服固定推理窗口的限制，实现更长视频的无缝增强，并消除边界处的时间不连续性。
*   <strong>鲁棒性提升：</strong> 进一步研究如何减少对文本提示的敏感性，使其在更广泛的场景下都能产生高质量的输出。</p>
<p>总而言之，这篇论文提出了一种创新的零样本视频真实感增强方法，通过巧妙地结合 DDIM Inversion 和结构感知信息，在保持高逼真度的同时，极大地提升了合成视频在结构和语义上的保真度，尤其是在关键的小物体细节上，为自动驾驶等领域的高质量合成数据生成提供了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion.</li>
<li>Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14719v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14719v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14716v1'></a></p>
<h2 id="diffusion-as-self-distillation-end-to-end-latent-diffusion-in-one-model"><a href="https://arxiv.org/abs/2511.14716v1">Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model</a></h2>
<p><strong>Authors:</strong> Xiyuan Wang, Muhan Zhang</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet <script type="math/tex">256\times 256</script> conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model (扩散作为自蒸馏：单一模型中的端到端潜在扩散)</p>
<p><strong>作者：</strong> Xiyuan Wang, Muhan Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决标准潜在扩散模型（LDMs）固有的复杂性、多阶段训练以及与当前主流的单一网络视觉基础模型不兼容的问题。LDMs通常由独立的编码器、解码器和扩散网络组成，这种模块化设计导致计算效率低下、性能次优，并且阻碍了将扩散模型统一到单一网络架构中。直接将这三个组件联合端到端训练会导致灾难性的“潜在空间坍塌”，即扩散训练目标干扰了网络学习良好潜在表示的能力。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>作者通过将扩散模型与自蒸馏（SD）无监督学习方法进行类比，深入分析了潜在空间坍塌的两个根本原因：
*   <strong>潜在方差抑制：</strong> L2损失项隐式地包含与潜在表示方差成比例的项，迫使编码器最小化方差，导致潜在向量聚集在均值周围。
*   <strong>秩区分能力失效：</strong> 标准扩散模型的目标（预测速度）输出高秩信号，而自蒸馏需要预测器（扩散模型）输出低秩信号以避免坍塌。</p>
<p>基于这些洞察，论文提出了<strong>扩散作为自蒸馏（DSD）</strong>框架，通过以下两个关键技术创新来稳定训练：
*   <strong>解耦（Decoupling）：</strong> 通过在目标干净潜在表示上应用stop-gradient（sg）操作符，消除了对潜在方差的梯度惩罚，从而保护了潜在表示的表达能力。
*   <strong>损失变换（Loss Transformation）：</strong> 分析证明了速度预测损失在数学上等价于预测干净潜在表示的损失。通过直接预测去噪后的图像潜在表示，迫使预测器充当低通滤波器，从而激活了自蒸馏稳定的秩区分机制。</p>
<p>此外，DSD还通过引入EMA更新目标编码器、使用数据增强以及整合辅助损失（如ViT层对齐、表示级别自蒸馏和分类损失）来进一步增强性能。</p>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>DSD框架首次实现了编码器、解码器和扩散模型在一个单一网络中的稳定端到端训练。该方法在ImageNet 256x256条件生成任务上取得了卓越的性能，即使在参数量远小于基线模型的情况下，也能达到甚至超越最先进的水平。例如，DSD-B模型仅使用2.05亿参数，在无分类器引导的情况下取得了FID=4.25的优异成绩，显著优于参数量高达7亿的现有模型。这表明DSD在参数效率和模型统一性方面具有显著优势，为构建高效、统一的生成模型提供了一条新途径。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li>由于计算资源限制，论文未能将DSD扩展到与基线模型相匹配的更大模型规模。</li>
<li>论文未进行实验来验证DSD作为无监督学习方法的有效性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li>将DSD扩展到更大的模型规模，以进一步验证其可扩展性。</li>
<li>探索DSD在纯无监督学习任务中的应用和有效性。</li>
<li>进一步研究DSD在其他下游视觉任务中的潜力。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method.</li>
<li>Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14716v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14716v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14712v1'></a></p>
<h2 id="freeswim-revisiting-sliding-window-attention-mechanisms-for-training-free-ultra-high-resolution-video-generation"><a href="https://arxiv.org/abs/2511.14712v1">FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</a></h2>
<p><strong>Authors:</strong> Yunfeng Wu, Jiayi Song, Zhenxiong Tan, Zihao He, Songhua Liu</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本研究提出了一种名为 FreeSwim 的新颖训练无关（training-free）方法，旨在解决现有基于 Transformer 的视频生成模型在处理超高分辨率视频时面临的计算成本过高问题。其核心贡献在于引入了一种改进的滑动窗口注意力机制，并结合了跨注意力覆盖策略和缓存机制，从而在不进行任何额外训练的情况下，实现高质量、高分辨率视频的生成，并展现出优异的性能和效率。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>训练无关（Training-Free）的超高分辨率视频生成：</strong> 这是最核心的创新点。利用已在原生分辨率下预训练好的视频 Diffusion Transformer 模型，通过巧妙的设计，直接用于生成更高分辨率的视频，避免了昂贵的端到端训练。</li>
<li><strong>改进的内向滑动窗口注意力（Inward Sliding Window Attention）：</strong><ul>
<li><strong>核心观察：</strong> 论文强调了保持查询（query）令牌在训练时的感受野（receptive field）对于保留视觉保真度和细节至关重要。</li>
<li><strong>挑战：</strong> 传统的局部窗口注意力容易导致内容重复和全局不一致。</li>
<li><strong>解决方案：</strong> FreeSwim 提出的滑动窗口注意力机制旨在克服这一挑战，但具体实现细节（如“内向”）需要进一步阅读论文才能完全理解。</li>
</ul>
</li>
<li><strong>双路径流水线（Dual-Path Pipeline）与跨注意力覆盖策略（Cross-Attention Override Strategy）：</strong><ul>
<li><strong>目的：</strong> 解决局部窗口注意力带来的全局不一致问题。</li>
<li><strong>机制：</strong> 将局部窗口注意力的输出与一个具有完整感受野的另一分支（通过跨注意力）进行融合。这使得局部注意力产生的语义内容能够被具有全局视角的另一分支所指导，从而确保整体的一致性。</li>
</ul>
</li>
<li><strong>跨注意力缓存策略（Cross-Attention Caching Strategy）：</strong><ul>
<li><strong>目的：</strong> 提高效率。</li>
<li><strong>机制：</strong> 为具有完整感受野的另一分支引入缓存机制，避免了频繁计算完整的 3D 注意力，从而显著降低了计算开销。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>降低超高分辨率视频生成的门槛：</strong> 训练无关的特性极大地降低了研究和应用超高分辨率视频生成的成本和技术门槛，使得更多研究者和开发者能够探索和利用这一领域。</li>
<li><strong>推动视频生成模型的可扩展性：</strong> 该方法为解决 Transformer 模型在处理高分辨率数据时的二次复杂度问题提供了一个有效的范例，可能启发其他领域（如图像生成）的类似研究。</li>
<li><strong>提升视频生成质量和效率的平衡：</strong> 在保证生成质量的同时，显著提高了效率，这对于实时或近实时的视频生成应用具有重要意义。</li>
<li><strong>为视频编辑和增强提供新工具：</strong> 训练无关的特性也可能使其在视频超分辨率、视频修复等任务中具有潜在应用价值，无需重新训练模型即可提升现有视频的质量。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>电影和媒体制作：</strong> 生成更高分辨率、更逼真的电影片段、特效素材。</li>
<li><strong>虚拟现实（VR）和增强现实（AR）：</strong> 创建沉浸式、高细节的虚拟环境和交互体验。</li>
<li><strong>游戏开发：</strong> 生成高质量的游戏过场动画和游戏内场景。</li>
<li><strong>医学影像：</strong> 生成高分辨率的医学视频，辅助诊断和研究。</li>
<li><strong>科学可视化：</strong> 生成高分辨率的模拟和实验过程可视化视频。</li>
<li><strong>视频编辑和后期制作：</strong> 提供强大的视频增强和风格迁移工具。</li>
<li><strong>内容创作平台：</strong> 赋能用户生成高质量的短视频或长视频内容。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>依赖于预训练模型：</strong> 该方法的核心是利用已有的预训练模型。其生成视频的质量和风格在很大程度上受限于预训练模型的性能和训练数据。如果预训练模型本身存在缺陷或不适用于特定任务，FreeSwim 的效果也会受到影响。</li>
<li><strong>“内向”滑动窗口注意力的具体细节未知：</strong> 摘要中提到了“内向”滑动窗口注意力，但具体如何实现以及其在何种程度上限制了感受野的扩展，需要进一步阅读论文来理解。这可能在某些情况下限制其捕捉极远距离依赖关系的能力。</li>
<li><strong>全局一致性的“保证”程度：</strong> 虽然提出了双路径流水线来确保全局一致性，但“确保”的程度以及在何种复杂场景下可能出现不一致，仍需通过实验结果来验证。</li>
<li><strong>计算效率的相对性：</strong> 尽管声称提高了效率，但与传统的低分辨率生成相比，超高分辨率视频生成本身仍然是计算密集型的。其效率提升是相对于“端到端训练超高分辨率模型”而言的，而非绝对的低计算量。</li>
<li><strong>对特定 Transformer 架构的依赖性：</strong> 该方法是基于“现代 Transformer 基于视频生成器”的，可能对特定的 Transformer 架构和注意力机制设计有一定依赖性。</li>
</ul>
<p>总而言之，FreeSwim 论文的亮点在于其“训练无关”的超高分辨率视频生成能力，通过巧妙的注意力机制设计和多分支融合策略，有效地解决了现有方法的计算瓶颈，为视频生成领域带来了新的可能性。其潜在影响广泛，但具体效果和适用范围仍需结合论文的详细内容和实验结果进行评估。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation.</li>
<li>At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail.</li>
<li>To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency.</li>
<li>Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14691v1'></a></p>
<h2 id="attention-via-synaptic-plasticity-is-all-you-need-a-biologically-inspired-spiking-neuromorphic-transformer"><a href="https://arxiv.org/abs/2511.14691v1">Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer</a></h2>
<p><strong>Authors:</strong> Kallol Mondal, Ankush Kumar</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.NE, cs.AI, cs.CV, cs.ET, stat.ML</p>
<p><strong>Abstract:</strong></p>
<p>Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S<script type="math/tex">^{2}</script>TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S<script type="math/tex">^{2}</script>TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer (通过突触可塑性实现注意力机制：一种受生物启发的脉冲神经形态Transformer)</p>
<p><strong>作者：</strong> Kallol Mondal, Ankush Kumar</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>现代Transformer模型在自然语言处理（NLP）和计算机视觉等领域取得了巨大成功，但其巨大的计算量和高能耗（导致碳足迹增加）是其主要缺点。现有基于脉冲神经网络（SNN）的Transformer模型虽然在能效方面有所改进，但其核心的注意力机制仍然依赖于非神经形态的计算方式，例如浮点数点积相似性计算、需要显式存储的注意力矩阵（导致冯·诺依曼瓶颈）以及与生物大脑计算方式的差异。因此，研究如何构建一种真正生物启发、能量高效且支持内存计算的脉冲神经形态Transformer注意力机制是本文要解决的核心问题。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>本文提出了一种名为<strong>Spiking STDP Transformer (S²TDPT)</strong> 的新型神经形态Transformer架构，其核心创新在于：</p>
<ul>
<li><strong>基于STDP的注意力机制：</strong> S²TDPT将注意力机制的计算完全基于<strong>尖峰时间依赖可塑性 (Spike-Timing-Dependent Plasticity, STDP)</strong>。STDP是一种模拟生物大脑中突触可塑性的核心机制，它通过精确的尖峰时间交互来编码信息相关性，而不是依赖于浮点数相似性。这使得注意力权重的计算能够直接嵌入到突触权重中，实现了<strong>内存计算 (in-memory computing)</strong>。</li>
<li><strong>消除中间注意力分数矩阵：</strong> 通过将Q（Query）和K（Key）的关联性直接编码到突触权重中，S²TDPT避免了显式计算和存储N×N的中间注意力分数矩阵，从而消除了冯·诺依曼瓶颈，显著减少了内存带宽需求。</li>
<li><strong>纯加法运算：</strong> 整个注意力模块的计算过程被设计为纯加法运算，这与脉冲神经网络的事件驱动特性高度兼容，进一步提升了能效。</li>
<li><strong>生物学上的合理性：</strong> 该模型通过模拟生物大脑中突触可塑性和尖峰时间编码的方式来计算注意力，使其在计算原理上更接近生物大脑，增强了模型的可解释性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>性能表现：</strong> 在CIFAR-10和CIFAR-100数据集上，S²TDPT取得了优异的分类准确率，分别为94.35%和78.08%。</li>
<li><strong>能效提升：</strong> 在仅使用四个时间步的情况下，S²TDPT在CIFAR-100上的能耗仅为0.49 mJ，相比于标准的ANN Transformer，能耗降低了88.47%。与现有的Spikformer等脉冲Transformer相比，能效也显著提升。</li>
<li><strong>可解释性：</strong> 通过Grad-CAM可视化分析，S²TDPT能够准确地将注意力集中在与分类任务相关的语义区域，表明其注意力机制具有良好的可解释性。</li>
<li><strong>意义：</strong> S²TDPT的成功表明，通过将生物学原理（如STDP）融入Transformer的注意力机制，可以构建出在性能、能效和生物学合理性方面都具有显著优势的神经形态模型。这为开发更高效、更具可解释性的AI系统提供了新的方向，并为未来在神经形态硬件上部署复杂的AI模型铺平了道路。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>当前实现：</strong> 目前的实现仍然依赖于GPU上的反向传播进行训练，而非完全的端到端神经形态硬件上的在线学习。</li>
<li><strong>神经元模型：</strong> 模型使用了多步Leaky-Integrate-and-Fire (LIF) 神经元，虽然这是标准做法，但论文也提到可以探索其他更先进的脉冲神经元模型以进一步提升性能。</li>
<li><strong>数据集：</strong> 主要在CIFAR-10和CIFAR-100等图像分类数据集上进行了评估，未来可以扩展到更大规模的数据集（如ImageNet）或更复杂的任务（如NLP）。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>端到端神经形态硬件实现：</strong> 将S²TDPT部署到实际的神经形态硬件上进行训练和推理，以充分发挥其低功耗优势。</li>
<li><strong>在线学习：</strong> 研究基于STDP的在线学习方法，实现模型在设备上的实时自适应和学习。</li>
<li><strong>更复杂的任务和数据集：</strong> 将S²TDPT扩展到自然语言处理、视频理解等更复杂的任务，以及更大规模的数据集。</li>
<li><strong>探索其他脉冲神经元模型：</strong> 集成更先进的脉冲神经元模型，以进一步提升模型的精度和能效。</li>
<li><strong>与其他生物学机制的结合：</strong> 探索将更多生物学上的学习和计算机制（如Hebbian学习、不同类型的突触可塑性）融入Transformer架构。</li>
</ul>
<p>总而言之，这篇论文提出了一种具有开创性的脉冲神经形态Transformer架构，通过将STDP机制引入注意力计算，成功地解决了现有Transformer模型在能耗和生物学合理性方面存在的关键问题，为构建更高效、更智能的AI系统提供了重要的理论和实践基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these issues, we propose the Spiking STDP Transformer (S<script type="math/tex">^{2}</script>TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14691v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14691v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14659v1'></a></p>
<h2 id="nora-15-a-vision-language-action-model-trained-using-world-model-and-action-based-preference-rewards"><a href="https://arxiv.org/abs/2511.14659v1">NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</a></h2>
<p><strong>Authors:</strong> Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名在计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析。以下是我的评估：</p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本研究提出了 NORA-1.5，一个在预训练 NORA 模型基础上增强了流匹配动作专家的视觉-语言-动作（VLA）模型。通过引入基于世界模型和偏离真实情况的奖励信号进行后训练，NORA-1.5 在模拟和真实世界基准测试中均展现出显著的性能提升和可靠性增强，为构建更可靠的具身智能体提供了有效途径。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>流匹配（Flow-Matching）的动作专家集成：</strong> 这是 NORA-1.5 架构上的一个核心改进。流匹配是一种生成模型技术，用于学习数据分布的连续变换。将其应用于动作生成，意味着模型能够更平滑、更准确地生成一系列动作，这对于需要精细控制和连续运动的具身任务至关重要。</li>
<li><strong>结合世界模型（WM）和偏离真实情况的奖励模型进行后训练：</strong> 这是论文在训练策略上的一个重要创新。<ul>
<li><strong>动作条件世界模型：</strong> 这种模型能够预测给定动作后，环境状态会如何变化，并评估这些变化是否朝着目标前进。这提供了一种内在的、基于预测的奖励信号，能够指导模型生成更具目的性的动作。</li>
<li><strong>偏离真实情况的启发式：</strong> 这种奖励机制旨在区分“好”动作和“差”动作，可能通过与期望的或最优的动作序列进行比较来实现。这提供了一种更直接的反馈，帮助模型避免产生无效或有害的动作。</li>
</ul>
</li>
<li><strong>直接偏好优化（DPO）的应用：</strong> DPO 是一种无需显式学习奖励函数即可直接从偏好数据中优化策略的方法。结合上述奖励模型构建的偏好数据集，DPO 能够有效地将模型调整到更符合人类或预设标准的行为。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升具身智能体的可靠性和泛化能力：</strong> 当前 VLA 模型在部署到不同环境或实体时存在可靠性和泛化性不足的问题。NORA-1.5 的方法，特别是其创新的奖励机制和后训练策略，有望显著改善这一点，使其在更广泛的场景下表现更稳定。</li>
<li><strong>推动 VLA 模型在真实世界中的应用：</strong> 论文强调了在模拟和真实机器人设置下的评估，表明其方法具有实际部署的潜力。更可靠的 VLA 模型是实现智能机器人、自动化助手等应用的关键。</li>
<li><strong>为 VLA 模型训练提供新的范式：</strong> 将世界模型和偏好学习相结合的奖励驱动后训练方法，为 VLA 模型的设计和训练提供了新的思路，可能启发后续研究。</li>
<li><strong>降低对大量标注数据的依赖：</strong> 相较于传统的强化学习方法，DPO 和世界模型可能在一定程度上减少对大量手工标注奖励信号的需求，通过模型自身的预测和评估来指导学习。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学：</strong> 尤其是在需要与物理环境交互的任务中，如家庭服务机器人、工业自动化、自动驾驶等。</li>
<li><strong>虚拟现实（VR）和增强现实（AR）：</strong> 用于创建更具交互性和沉浸感的虚拟体验，以及更智能的 AR 应用。</li>
<li><strong>游戏 AI：</strong> 开发更智能、更具适应性的游戏 NPC。</li>
<li><strong>人机交互：</strong> 构建更自然、更直观的人机协作系统。</li>
<li><strong>多模态学习：</strong> 进一步探索视觉、语言和动作之间的深层联系。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 流匹配模型和世界模型的训练通常需要大量的计算资源。虽然摘要未明确提及，但这是这类模型普遍存在的挑战。</li>
<li><strong>奖励模型的准确性：</strong> 尽管论文提出了创新的奖励模型，但其有效性仍然依赖于世界模型和偏离真实情况启发式的准确性。如果这些奖励信号本身存在偏差或不准确，可能会限制最终模型的性能。</li>
<li><strong>“偏离真实情况”的定义：</strong> 摘要中“偏离真实情况的启发式”的具体实现细节并未完全披露。其有效性可能取决于如何精确地定义和量化“偏离”以及“好”与“差”的动作。</li>
<li><strong>对特定实体的适应性：</strong> 虽然论文提到“目标实体”，但摘要并未说明模型在适应全新、高度不同的实体时，需要多大的调整或数据量。</li>
<li><strong>泛化到未见过任务的能力：</strong> 尽管论文声称提高了泛化能力，但摘要并未提供具体证据表明模型能处理与训练数据完全不同类型的新任务。</li>
</ul>
<p>总而言之，NORA-1.5 的研究在 VLA 模型领域具有重要的理论和实践意义。其核心在于通过引入更精细的动作生成机制（流匹配）和更智能的奖励信号（世界模型+偏离真实情况），并结合 DPO 进行高效的后训练，从而显著提升了具身智能体的可靠性和性能。这为构建能够胜任更复杂、更具挑战性现实世界任务的智能体铺平了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert.</li>
<li>This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks.</li>
<li>To further improve robustness and task success, we develop a set of reward models for post-training VLA policies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14659v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14659v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.14633v1'></a></p>
<h2 id="sparsesurf-sparse-view-3d-gaussian-splatting-for-surface-reconstruction"><a href="https://arxiv.org/abs/2511.14633v1">SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</a></h2>
<p><strong>Authors:</strong> Meiying Gu, Jiawei Zhang, Jiahe Li, Xiaohan Yu, Haonan Luo, Jin Zheng, Xiao Bai</p>
<p><strong>Published:</strong> 2025-11-18</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</p>
<p><strong>作者：</strong> Meiying Gu, Jiawei Zhang, Jiahe Li, Xiaohan Yu, Haonan Luo, Jin Zheng, Xiao Bai</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决了在<strong>稀疏视图</strong>条件下进行<strong>高质量三维表面重建</strong>和<strong>新视图合成</strong>的挑战。现有的基于3D高斯泼溅（3DGS）的方法在视图充足时表现出色，但在输入图像数量很少的情况下，容易出现<strong>过拟合</strong>，导致重建的表面质量下降，并且新视图合成效果不佳。虽然一些方法尝试使用扁平化的高斯原语来更好地拟合表面几何，并引入深度正则化，但扁平化高斯固有的各向异性反而加剧了稀疏视图下的过拟合问题。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>SparseSurf 提出了一种新颖的方法，旨在同时实现<strong>更准确、更精细的表面重建</strong>和<strong>高质量的新视图合成</strong>。其核心贡献包括：</p>
<ul>
<li><strong>立体几何-纹理对齐 (Stereo Geometry-Texture Alignment)：</strong> 引入了一种新颖的机制，将渲染质量与几何估计联系起来。通过估计和更新立体视图图像，并利用预训练的立体匹配网络来获得几何先验，从而实现更可靠的几何监督，并随着训练的进行，立体视图的质量提升会反过来增强几何先验的准确性。</li>
<li><strong>伪特征增强几何一致性 (Pseudo-Feature Enhanced Geometry Consistency)：</strong> 为了解决扁平化高斯在稀疏视图下过拟合的风险，该方法引入了一种新的几何一致性约束。它通过结合<strong>训练视图</strong>和<strong>伪（虚拟）视图</strong>来强制执行多视图几何一致性。具体来说，它利用多视图特征表示，并通过特征蒸馏来增强伪视图的特征表示，从而有效缓解了稀疏监督带来的过拟合问题。</li>
<li><strong>训练视图特征对齐 (Train-view Feature Alignment)：</strong> 为了进一步提高鲁棒性，该方法还引入了训练视图特征对齐，利用高置信度的特征来强制执行像素级别的多视图一致性，从而进一步提升表面细节。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>定量结果：</strong> 在 DTU、BlendedMVS 和 Mip-NeRF360 等多个数据集上进行了广泛实验。SparseSurf 在 DTU 数据集上的 Chamfer Distance (CD) 指标上取得了<strong>最先进的性能</strong>，无论是在小重叠还是大重叠的稀疏视图设置下，都显著优于现有方法。在 Mip-NeRF360 数据集上，其新视图合成性能也达到了<strong>最高水平</strong>（SSIM 指标）。</li>
<li><strong>定性结果：</strong> 可视化结果表明，SparseSurf 能够重建出<strong>更准确、更完整、细节更丰富</strong>的表面，并且在稀疏视图下能够生成<strong>清晰、无伪影</strong>的新视图图像，有效避免了现有方法中常见的几何失真和模糊问题。</li>
<li><strong>意义：</strong> SparseSurf 的成功表明，通过巧妙地结合立体几何先验和伪视图特征一致性，可以有效地克服稀疏视图下 3DGS 的过拟合问题，从而在保持高质量渲染的同时，实现更可靠的三维表面重建。这降低了对输入图像数量的要求，为实际应用中的三维重建提供了更便捷的解决方案。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中提到，在<strong>极端稀疏视图</strong>的情况下，<strong>遮挡问题</strong>是不可避免的，并且在这些区域，表面重建可能会失败。</p>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>虽然论文没有明确列出未来研究方向，但基于其提出的方法和遇到的局限性，可以推测以下潜在方向：</p>
<ul>
<li><strong>更鲁棒的遮挡处理：</strong> 进一步研究如何更有效地处理稀疏视图下的遮挡区域，以实现更完整的表面重建。</li>
<li><strong>更高效的立体匹配和特征提取：</strong> 探索更轻量级或更高效的立体匹配网络和特征提取模型，以进一步提高训练和推理效率。</li>
<li><strong>自适应的正则化策略：</strong> 开发能够根据输入视图的稀疏程度和场景复杂度自适应调整正则化强度的策略，以获得更优的性能。</li>
<li><strong>结合其他先验信息：</strong> 探索将其他类型的先验信息（如语义信息、物体类别信息等）融入到 SparseSurf 框架中，以进一步提升重建的准确性和鲁棒性。</li>
<li><strong>动态场景重建：</strong> 将 SparseSurf 的思想扩展到动态场景的重建，以应对更复杂的真实世界应用。</li>
</ul>
<hr />
<p>总而言之，SparseSurf 是一项重要的研究成果，它通过创新的“立体几何-纹理对齐”和“伪特征增强几何一致性”方法，有效解决了稀疏视图下 3DGS 的过拟合问题，在三维表面重建和新视图合成方面均取得了显著的性能提升，为在图像稀疏条件下进行高质量三维重建开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance.</li>
<li>In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering.</li>
<li>In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision.</li>
<li>Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.14633v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.14633v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-19 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
