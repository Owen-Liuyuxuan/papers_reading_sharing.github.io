<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-11 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-10/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-12/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-11">Arxiv Computer Vision Papers - 2025-09-11</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#computational-imaging-for-enhanced-computer-vision" class="nav-link">Computational Imaging for Enhanced Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#a-structured-review-of-underwater-object-detection-challenges-and-solutions-from-traditional-to-large-vision-language-models" class="nav-link">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#foundation-models-for-autonomous-driving-perception-a-survey-through-core-capabilities" class="nav-link">Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</a>
                </li>
                <li class="nav-item">
                    <a href="#argotweak-towards-self-updating-hd-maps-through-structured-priors" class="nav-link">ArgoTweak: Towards Self-Updating HD Maps through Structured Priors</a>
                </li>
                <li class="nav-item">
                    <a href="#crowdquery-density-guided-query-module-for-enhanced-2d-and-3d-detection-in-crowded-scenes" class="nav-link">CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</a>
                </li>
                <li class="nav-item">
                    <a href="#bcqlm-efficient-vision-language-understanding-with-distilled-q-gated-cross-modal-fusion" class="nav-link">BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion</a>
                </li>
                <li class="nav-item">
                    <a href="#tango-traversability-aware-navigation-with-local-metric-control-for-topological-goals" class="nav-link">TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</a>
                </li>
                <li class="nav-item">
                    <a href="#humo-human-centric-video-generation-via-collaborative-multi-modal-conditioning" class="nav-link">HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</a>
                </li>
                <li class="nav-item">
                    <a href="#apml-adaptive-probabilistic-matching-loss-for-robust-3d-point-cloud-reconstruction" class="nav-link">APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#one-view-many-worlds-single-image-to-3d-object-meets-generative-domain-randomization-for-one-shot-6d-pose-estimation" class="nav-link">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-11">Arxiv Computer Vision Papers - 2025-09-11</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ10æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§é¢åææ°è®ºææ§è¡æè¦ (2025å¹´9æ10æ¥)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>æ¬æ¬¡æ¥åæ¶µçç10ç¯è®ºæå±ç°äºè®¡ç®æºè§è§é¢åå ä¸ªå³é®ä¸ç¸äºå³èçè¶å¿ï¼</p>
<ul>
<li><strong>å¤æ¨¡æèåä¸å¤§åæ¨¡åï¼LLMs/LVLMsï¼çå´èµ·ï¼</strong> å¤ç¯è®ºææ¢ç´¢äºè§è§ä¸è¯­è¨æå¶ä»æ¨¡æçèåï¼ç¹å«æ¯å°å¤§åè§è§è¯­è¨æ¨¡åï¼LVLMsï¼åºç¨äºæ´å¤æçæç¥ä»»å¡ï¼å¦æ°´ä¸ç®æ æ£æµåé«æçè§è§-è¯­è¨çè§£ã</li>
<li><strong>èªå¨é©¾é©¶æç¥ä¸å°å¾æå»ºçæç»­å³æ³¨ï¼</strong> èªå¨é©¾é©¶ä»ç¶æ¯ç ç©¶ç­ç¹ï¼è®ºææ¶ååºç¡æ¨¡åå¨èªå¨é©¾é©¶æç¥ä¸­çåºç¨ãé«ç²¾å°å¾çèªæ´æ°æºå¶ä»¥åæ¥æ¤åºæ¯ä¸çæ£æµä¼åã</li>
<li><strong>3D è§è§ä¸éå»ºçè¿æ­¥ï¼</strong> 3D ç¹äºéå»ºãåå¾åå°3Då¯¹è±¡çæä»¥å6Då§¿æä¼°è®¡æ¯éè¦çç ç©¶æ¹åï¼å¼ºè°äºé²æ£æ§åçæè½åã</li>
<li><strong>ä»¥äººä¸ºä¸­å¿çè§è§çè§£ä¸çæï¼</strong> è§é¢çæé¢åå¼å§å³æ³¨ä»¥äººä¸ºä¸­å¿ççæï¼éè¿å¤æ¨¡ææ¡ä»¶æ§å¶å®ç°æ´èªç¶çè§é¢åå®¹ã</li>
<li><strong>é²æ£æ§ä¸æççè¿½æ±ï¼</strong> æ è®ºæ¯æ°´ä¸ç¯å¢ãæ¥æ¤åºæ¯è¿æ¯3Déå»ºï¼ç ç©¶äººåé½å¨åªåæåæ¨¡åçé²æ£æ§ãåç¡®æ§åè®¡ç®æçã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities" (Rajendramayavan Sathyam, Yueqi Li):</strong> è¿ç¯ç»¼è¿°æ§è®ºæéå¸¸åæ¶åéè¦ãå®ç³»ç»å°æ¢³çäºåºç¡æ¨¡åå¨èªå¨é©¾é©¶æç¥ä¸­çåºç¨ï¼ä¸ºè¯¥é¢åçæªæ¥ç ç©¶æä¾äºæ¸æ°çè·¯çº¿å¾åææåæï¼å¯¹äºçè§£å®è§è¶å¿è³å³éè¦ã</li>
<li><strong>"BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion" (Sike Xiang, Shuang Chen, Amir Atapour-Abarghouei):</strong> è¿ç¯è®ºæå¨å¤æ¨¡æèåæ¹é¢å±ç°äºåæ°æ§ï¼éè¿è¸é¦åQé¨æ§äº¤åæ¨¡æèåï¼è§£å³äºLVLMså¨æçåçè§£è½åä¸çå¹³è¡¡é®é¢ï¼å¯¹äºèµæºåéçåºç¨åºæ¯å·æéè¦æä¹ã</li>
<li><strong>"One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation" (Zheng Geng et al.):</strong> è¿ç¯è®ºæç»åäºåå¾åå°3Dçæåçæå¼åéæºåï¼ä»¥è§£å³åæ¬¡6Då§¿æä¼°è®¡çææãå¶åæ°ç¹å¨äºå©ç¨çææ¨¡åæ¥å¢å¼ºè®­ç»æ°æ®çå¤æ ·æ§åé²æ£æ§ï¼ä¸ºè§£å³æ°æ®ç¨ç¼ºé®é¢æä¾äºæ°æè·¯ã</li>
<li><strong>"HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning" (Liyang Chen et al.):</strong> å¨çæå¼AIæ¥çæççèæ¯ä¸ï¼è¿ç¯è®ºæä¸æ³¨äºä»¥äººä¸ºä¸­å¿çè§é¢çæï¼éè¿å¤æ¨¡ææ¡ä»¶æ§å¶å®ç°æ´ç²¾ç»ãæ´å¯æ§ççæï¼é¢ç¤ºçæªæ¥è§é¢åå®¹åä½çæ°æ¹åã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>åºç¡æ¨¡åå¨ç¹å®é¢åï¼å¦èªå¨é©¾é©¶ãæ°´ä¸è§è§ï¼çæ·±å¥åºç¨ä¸éåºï¼</strong> ä¸åä»ä»æ¯éç¨æ¨¡åï¼èæ¯å¦ä½å°åºç¡æ¨¡åçè½åææè¿ç§»åä¼åå°ç¹å®ãå¤æçåºç¨åºæ¯ã</li>
<li><strong>é«æä¸é²æ£çå¤æ¨¡æèåæ¶æï¼</strong> å¦ä½å¨ä¿æé«æ§è½çåæ¶ï¼éä½å¤§åå¤æ¨¡ææ¨¡åçè®¡ç®ææ¬åå¤ææ§ï¼ä¾å¦éè¿è¸é¦ãé¨æ§æºå¶ç­ã</li>
<li><strong>èªæ´æ°/èªéåºç³»ç»ï¼</strong> å¦é«ç²¾å°å¾çèªæ´æ°ï¼ArgoTweakï¼ï¼ä»¥åæªæ¥å¯è½åºç°çèªéåºæç¥ç³»ç»ï¼è½å¤æ ¹æ®ç¯å¢ååè¿è¡èªæè°æ´åä¼åã</li>
<li><strong>çæå¼AIå¨æ°æ®å¢å¼ºåé²æ£æ§æåä¸­çåºç¨ï¼</strong> å©ç¨çææ¨¡ååå»ºå¤æ ·åçè®­ç»æ°æ®ï¼ä»¥æé«æ¨¡åå¨çå®ä¸çå¤æåºæ¯ä¸­çæ³åè½ååé²æ£æ§ã</li>
<li><strong>ä»¥äººä¸ºä¸­å¿ççæä¸çè§£ï¼</strong> éçAIGCçåå±ï¼å¯¹äººç±»è¡ä¸ºãå§¿æãäº¤äºçç²¾ç¡®çè§£åçæå°æä¸ºå³é®ã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>ä¸ºäºå¨é¢äºè§£è¿äºé¢åçææ°è¿å±ï¼æå»ºè®®æ¨ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>"Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities" (Rajendramayavan Sathyam, Yueqi Li):</strong> å¯¹äºä»»ä½ä»äºèªå¨é©¾é©¶æå¯¹åºç¡æ¨¡ååºç¨æå´è¶£çç ç©¶äººåï¼è¿ç¯ç»¼è¿°æ¯å¿è¯»çï¼å®æä¾äºé«å±æ¬¡çæ¦è§åæªæ¥æ¹åã</li>
<li><strong>"BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion" (Sike Xiang, Shuang Chen, Amir Atapour-Abarghouei):</strong> å¦ææ¨å³æ³¨å¤æ¨¡æå­¦ä¹ çæçåæ¶æåæ°ï¼è¿ç¯è®ºææä¾äºå·ä½çææ¯ç»èåæ½å¨çè§£å³æ¹æ¡ã</li>
<li><strong>"One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation" (Zheng Geng et al.):</strong> å¯¹äº3Dè§è§ãå§¿æä¼°è®¡ä»¥åå¦ä½å©ç¨çæå¼AIè§£å³æ°æ®æææå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºææä¾äºåæ°çæ¹æ³ã</li>
<li><strong>"ArgoTweak: Towards Self-Updating HD Maps through Structured Priors" (Lena Wild, Rafael Valencia, Patric Jensfelt):</strong> å¯¹äºèªå¨é©¾é©¶ä¸­çé«ç²¾å°å¾ååºç¡è®¾æ½ç»´æ¤æå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºææä¾äºå®ç¨çå·¥ç¨è§£å³æ¹æ¡åç ç©¶æ¹åã</li>
</ul>
<hr />
<p>è¿ä»½æè¦å¸æè½ä¸ºæ¨æä¾ä¸ä¸ªå¿«éèå¨é¢çæ¦è§ï¼å¸®å©æ¨é«æå°ç­éåæ·±å¥ç ç©¶æç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.08712v1">Computational Imaging for Enhanced Computer Vision</a></li>
<li><a href="#2509.08490v1">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a></li>
<li><a href="#2509.08302v1">Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</a></li>
<li><a href="#2509.08764v1">ArgoTweak: Towards Self-Updating HD Maps through Structured Priors</a></li>
<li><a href="#2509.08738v1">CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</a></li>
<li><a href="#2509.08715v1">BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion</a></li>
<li><a href="#2509.08699v1">TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</a></li>
<li><a href="#2509.08519v1">HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</a></li>
<li><a href="#2509.08104v1">APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</a></li>
<li><a href="#2509.07978v1">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.08712v1'></a></p>
<h2 id="computational-imaging-for-enhanced-computer-vision"><a href="https://arxiv.org/abs/2509.08712v1">Computational Imaging for Enhanced Computer Vision</a></h2>
<p><strong>Authors:</strong> Humera Shaikh, Kaur Jashanpreet</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents a comprehensive survey of computational imaging (CI)
techniques and their transformative impact on computer vision (CV)
applications. Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems. Computational imaging techniques, including light field imaging, high
dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare
mitigation, address these limitations by enhancing image acquisition and
reconstruc- tion processes. This survey systematically explores the synergies
between CI techniques and core CV tasks, including object detection, depth
estimation, optical flow, face recognition, and keypoint detection. By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions. We emphasize the potential for
task-specific, adaptive imaging pipelines that improve robustness, accuracy,
and efficiency in real-world scenarios, such as autonomous navigation,
surveillance, augmented reality, and robotics.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Humera ShaikhåKaur Jashanpreetæ°åçè®ºæâComputational Imaging for Enhanced Computer Visionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="computational-imaging-for-enhanced-computer-vision_1">è®ºæãComputational Imaging for Enhanced Computer Visionãæè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä¼ ç»æåæ¹æ³å¨æææ§æ¡ä»¶ä¸ï¼å¦ä½åç§ãè¿å¨æ¨¡ç³ãé«å¨æèå´åºæ¯ï¼æ æ³æä¾é«ä¿çè§è§æ°æ®ï¼ä»èéå¶äºæåè¿è®¡ç®æºè§è§ï¼CVï¼ç³»ç»æ§è½çé®é¢ãç ç©¶çæ ¸å¿å¨äºæ¢ç´¢è®¡ç®æåï¼CIï¼ææ¯å¦ä½éè¿å¢å¼ºå¾åééåéå»ºè¿ç¨æ¥åæè¿äºéå¶ï¼å¹¶æåæ ¸å¿CVä»»å¡çæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºæçä¸»è¦è´¡ç®å¨äºå¯¹è®¡ç®æåææ¯åå¶å¯¹è®¡ç®æºè§è§åºç¨çåé©æ§å½±åè¿è¡äºå¨é¢èç³»ç»çç»¼è¿°ãå®è¯¦ç»éè¿°äºä»¥ä¸å³é®CIææ¯ï¼
*   <strong>ååºæåï¼Light Field Imagingï¼ï¼</strong> éè¿ææåçº¿çè§åº¦æ¹åï¼æä¾å¤è§è§è§å¾ï¼æ¾èå¢å¼ºæ·±åº¦ä¼°è®¡ãç©ä½åå²åé®æ¡å¤çï¼å¯¹3Déå»ºåé¢é¨è¯å«è³å³éè¦ã
*   <strong>é«å¨æèå´ï¼HDRï¼æåï¼</strong> éè¿èåä¸åæåæ°´å¹³çå¾åï¼ææäº®åºåæåºçç»èï¼åæä¼ ç»ä¼ æå¨çéå¶ï¼æåé«å¯¹æ¯åº¦ç¯å¢ä¸çç©ä½æ£æµåé¢é¨è¯å«æ§è½ã
*   <strong>å¾åå»æ¨¡ç³ï¼Image Deblurringï¼ï¼</strong> éè¿å»ºæ¨¡åéè½¬è¿å¨æ¨¡ç³ææ£ç¦æ¨¡ç³ï¼æ¢å¤å¾åæ¸æ°åº¦ï¼æ¹ååå­¦æµä¼°è®¡ãè¿å¨è·è¸ªåç©ä½æ£æµã
*   <strong>é«éæåï¼High-Speed Imagingï¼ï¼</strong> ææå¿«éæ¶é´äºä»¶ï¼æä¾ç²¾ç»çè¿å¨ç»èï¼å¯¹ç©ä½è·è¸ªãåå­¦æµè®¡ç®åè¿å¨åæè³å³éè¦ã
*   <strong>ç©åç¼è§£ï¼Glare Mitigationï¼ï¼</strong> éè¿åæ¯æåæå¤æåæ¹æ³æå¶åå°åå¼ºåæºå¼èµ·çä¼ªå½±ï¼ç¡®ä¿å¨åå°æä¸åååç§ç¯å¢ä¸çCVç³»ç»åç¡®æ§ã</p>
<p>è®ºæç³»ç»å°æ¢è®¨äºè¿äºCIææ¯ä¸æ ¸å¿CVä»»å¡ï¼åæ¬ç©ä½æ£æµãæ·±åº¦ä¼°è®¡ãåå­¦æµãäººè¸è¯å«åå³é®ç¹æ£æµï¼ä¹é´çååä½ç¨ï¼åæäºCIæ¹æ³å¦ä½è§£å³ç°æCVç®æ³çå±éæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿åæCIæ¹æ³ä¸CVåºç¨ä¹é´çå³ç³»ï¼å¼ºè°äºCIææ¯å¯¹CVæ§è½çæ¾èæåï¼
*   <strong>æ·±åº¦ä¼°è®¡åå³é®ç¹æ£æµï¼</strong> ååºæåéè¿æä¾ç©ºé´åè§åº¦ä¿¡æ¯ï¼æ¾èæé«äºè¿äºä»»å¡çåç¡®æ§åé²æ£æ§ï¼å°¤å¶æ¯å¨é®æ¡åæ çº¹çåºåã
*   <strong>ç©ä½æ£æµåäººè¸è¯å«ï¼</strong> HDRæåå¨é«å¯¹æ¯åº¦ç¯å¢ä¸ï¼å¦èªå¨é©¾é©¶åçæ§ï¼ç¡®ä¿äºç»èå¯è§æ§ï¼èååºæååéè¿å¤è§è§ä¿¡æ¯å¢å¼ºäºäººè¸è¯å«å¨ä¸åå§¿æåé®æ¡ä¸çé²æ£æ§ã
*   <strong>åå­¦æµåè¿å¨åæï¼</strong> é«éæååå»æ¨¡ç³ææ¯éè¿æä¾æ¸æ°çè¿å¨ç»èï¼æ¾èæé«äºåå­¦æµä¼°è®¡åç©ä½è·è¸ªçåç¡®æ§ï¼å°¤å¶æ¯å¨å¨æåºæ¯ä¸­ã
*   <strong>è·¨ä»»å¡ååï¼</strong> è®ºææåºï¼CIææ¯å¸¦æ¥çæ¹è¿å¹¶éå­¤ç«çï¼èæ¯éè¿ä¸°å¯çæ°æ®è¡¨ç¤ºå¨ä¸åCVä»»å¡ä¹é´äº§çååæåºï¼ä¾å¦æ·±åº¦ä¼°è®¡çæ¹è¿å¯ä»¥é´æ¥æåç©ä½æ£æµåå³é®ç¹å¹éã</p>
<p>è¿äºç»æè¡¨æï¼CIææ¯ä¸ºCVç³»ç»æä¾äºæ´å¯é ãæ´ä¸°å¯ãæ´å·ä¿¡æ¯éçè¾å¥ï¼ä»èæ¾èæé«äºå¶å¨å¤æåæææ§ç°å®åºæ¯ä¸­çåç¡®æ§ãé²æ£æ§åæçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹æåºäºCIææ¯ä¸CVéæè¿ç¨ä¸­é¢ä¸´çä¸äºææï¼
*   <strong>è®¡ç®å¤ææ§ï¼</strong> è®¸å¤CIæ¹æ³ï¼å¦ååºæååå¤æåHDRï¼æ¶åå¤§éæ°æ®åé«è®¡ç®éæ±ï¼å°¤å¶æ¯å¨å®æ¶åºç¨ä¸­ã
*   <strong>ç¡¬ä»¶ä¸è½¯ä»¶çæè¡¡ï¼</strong> CIææ¯ä¾èµä¸ä¸åå­¦è®¾è®¡åä¼ æå¨æ¶æï¼éè¦å¨ç©çå¤ææ§ãææ¬åå¯æ©å±æ§ä¹é´åå¾å¹³è¡¡ã
*   <strong>æ°æ®è´¨éåæ³åæ§ï¼</strong> å°½ç®¡CIæé«äºæ°æ®ä¿çåº¦ï¼ä½å¨ç¨çè§åº¦éæ ·æå¿«éè¿å¨ç­ç¹å®æåæ¡ä»¶ä¸ï¼ä»å¯è½åºç°ä¼ªå½±ææ§è½ä¸éã
*   <strong>äºæä½æ§ï¼</strong> æ°å´CIç¡¬ä»¶ä¸ç°æCVæ¡æ¶ä¹é´çäºæä½æ§éè¦æ ååç®¡éåæ°æ®éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºå ä¸ªæªæ¥ç ç©¶æ¹ååæºä¼ï¼
*   <strong>ä»»å¡ç¹å®ãèªéåºæåç®¡éï¼</strong> å¼åè½å¤æ ¹æ®ç¹å®CVä»»å¡éæ±å¨æè°æ´å¾åééè¿ç¨çCIç³»ç»ï¼ä¾å¦ååºç¸æºæ ¹æ®åºæ¯æ·±åº¦å¤ææ§è°æ´è§åº¦éæ ·ã
*   <strong>æ·±åº¦å­¦ä¹ ä¸æ°æ®é©±å¨æ¹æ³ï¼</strong> å°æ·±åº¦å­¦ä¹ ä¸CIç®¡éç»åï¼å©ç¨ç¥ç»ç½ç»å­¦ä¹ åå§ä¼ æå¨æ°æ®ä¸ä»»å¡ç¹å®è¾åºä¹é´çç«¯å°ç«¯æ å°ï¼ä»¥å®ç°æ´å¿«ãæ´åç¡®ãæ´çµæ´»çæåè§£å³æ¹æ¡ã
*   <strong>å¤æ¨¡ææåç³»ç»éæï¼</strong> ç»åç©ºé´ãè§åº¦ãæ¶é´ãåè°±ç­å¤ç§æåæ¹æ³ï¼ä»¥è·å¾æ´ä¸°å¯çåºæ¯è¡¨ç¤ºï¼ä»èå¨å¤æç¯å¢ä¸­å®ç°æ´é²æ£çCVæ§è½ã
*   <strong>è¾¹ç¼è®¡ç®åAIå éå¨ï¼</strong> å©ç¨è½»éçº§ç¥ç»ç½ç»åé«æç¡¬ä»¶æ¶æï¼å¨èµæºåéçè¾¹ç¼è®¾å¤åç§»å¨å¹³å°ä¸å®ç°CIå¢å¼ºåCVç³»ç»çå®æ¶é¨ç½²ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæå¨é¢æ¦è¿°äºè®¡ç®æåææ¯å¦ä½éè¿æä¾æ´ä¸°å¯ãæ´åç¡®çè§è§æ°æ®æ¥åæä¼ ç»æåçå±éæ§ï¼ä»èæ¾èå¢å¼ºäºè®¡ç®æºè§è§åºç¨çæ§è½ãå®ä¸ä»è¯¦ç»ä»ç»äºåç§CIæ¹æ³åå¶å¯¹æ ¸å¿CVä»»å¡çè´¡ç®ï¼è¿å¼ºè°äºè¿äºææ¯ä¹é´çååä½ç¨ï¼å¹¶æåºäºæªæ¥ç ç©¶çææåæºéï¼ä¸ºè¯¥é¢åçè¿ä¸æ­¥åå±å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems.</li>
<li>By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08490v1'></a></p>
<h2 id="a-structured-review-of-underwater-object-detection-challenges-and-solutions-from-traditional-to-large-vision-language-models"><a href="https://arxiv.org/abs/2509.08490v1">A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</a></h2>
<p><strong>Authors:</strong> Edwine Nabahirwa, Wei Song, Minghua Zhang, Yi Fang, Zhou Ni</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Underwater object detection (UOD) is vital to diverse marine applications,
including oceanographic research, underwater robotics, and marine conservation.
However, UOD faces numerous challenges that compromise its performance. Over
the years, various methods have been proposed to address these issues, but they
often fail to fully capture the complexities of underwater environments. This
review systematically categorizes UOD challenges into five key areas: Image
quality degradation, target-related issues, data-related challenges,
computational and processing constraints, and limitations in detection
methodologies. To address these challenges, we analyze the progression from
traditional image processing and object detection techniques to modern
approaches. Additionally, we explore the potential of large vision-language
models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated
in other domains. We also present case studies, including synthetic dataset
generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review
identifies three key insights: (i) Current UOD methods are insufficient to
fully address challenges like image degradation and small object detection in
dynamic underwater environments. (ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability. (iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Edwine Nabahirwaç­äººçè®ºæâA Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Modelsâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼æ°´ä¸ç®æ æ£æµçææä¸å¤§åè§è§è¯­è¨æ¨¡åçæ½å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨ç³»ç»æ§å°åé¡¾ååææ°´ä¸ç®æ æ£æµï¼UODï¼æé¢ä¸´çå¤æææï¼å¹¶æ¢è®¨ä»ä¼ ç»æ¹æ³å°ç°ä»£æ·±åº¦å­¦ä¹ ææ¯ï¼ç¹å«æ¯å¤§åè§è§è¯­è¨æ¨¡åLVLMsï¼çè§£å³æ¹æ¡ãæ ¸å¿ç ç©¶é®é¢æ¯ï¼å¦ä½åææ°´ä¸ç¯å¢ä¸­å¾åè´¨ééåãç®æ ç¹æ§å¤æãæ°æ®ç¨ç¼ºãè®¡ç®èµæºåéä»¥åç°ææ£æµæ¹æ³å±éæ§ç­é®é¢ï¼ä»¥å®ç°é²æ£ãåç¡®ä¸å®æ¶çUODï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç³»ç»æ§ææåç±»ï¼</strong> è®ºæå°UODææç³»ç»å°å½çº³ä¸ºäºå¤§ç±»ï¼å¾åè´¨ééåãç®æ ç¸å³é®é¢ãæ°æ®ç¸å³ææãè®¡ç®åå¤çéå¶ä»¥åæ£æµæ¹æ³å±éæ§ãè¿ç§ç»æåçåç±»ä¸ºçè§£UODçå¤ææ§æä¾äºæ¸æ°çæ¡æ¶ã
*   <strong>å¨é¢åé¡¾è§£å³æ¹æ¡ï¼</strong> è®ºæè¯¦ç»åæäºä»ä¼ ç»å¾åå¤çï¼å¦å¾åå¢å¼ºãæ¢å¤ï¼å°ç°ä»£ç®æ æ£æµææ¯ï¼å¦åºäºæ·±åº¦å­¦ä¹ ãTransformeråæ··åæ¨¡åï¼çæ¼è¿ï¼ä»¥åºå¯¹ä¸è¿°ææã
*   <strong>LVLMså¨UODä¸­çæ½åæ¢ç´¢ï¼</strong> è®ºæé¦æ¬¡æ·±å¥æ¢è®¨äºå¤§åè§è§è¯­è¨æ¨¡åï¼LVLMsï¼å¨UODé¢åçåºç¨æ½åï¼å¼ºè°å¶å¤æ¨¡æè½åå¨è§£å³æ°´ä¸ç¯å¢å¤ææ§æ¹é¢çä¼å¿ã
*   <strong>æ¡ä¾ç ç©¶ï¼</strong> è®ºæéè¿ä¸¤ä¸ªå·ä½çæ¡ä¾ç ç©¶éªè¯äºLVLMsçæ½åï¼
    *   <strong>åºäºDALL-E 3çåææ°æ®çæï¼</strong> å±ç¤ºäºå¦ä½å©ç¨LVLMsçæåææ°´ä¸å¾åï¼å¹¶éè¿å¾åå¢å¼ºææ¯ï¼å¦é¢è²è°æ´ãæ¨¡ç³ï¼æé«å¶çå®æï¼ä»¥æ©åæ°æ®éå¹¶æ¹åæ¨¡åæ§è½ã
    *   <strong>Florence-2 LVLMçå¾®è°ï¼</strong> æ¢è®¨äºä½¿ç¨LoRAï¼ä½ç§©éåºï¼ææ¯å¯¹Florence-2 LVLMè¿è¡å¾®è°ä»¥å®ç°UODï¼å±ç¤ºäºå¶å¨å°ç®æ æ£æµåå®ä½æ¹é¢çå¼ºå¤§è½åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ç°æUODæ¹æ³çå±éæ§ï¼</strong> è®ºææåºï¼å½åçUODæ¹æ³å¨å®å¨è§£å³å¨ææ°´ä¸ç¯å¢ä¸­çå¾åéååå°ç®æ æ£æµç­æææ¹é¢ä»æ¾ä¸è¶³ã
*   <strong>åææ°æ®çæçæ½åä¸ææï¼</strong> æ¡ä¾ç ç©¶è¡¨æï¼å©ç¨LVLMsï¼å¦DALL-E 3ï¼çæåææ°æ®å¨æ©åæ°æ®éæ¹é¢å·æå·¨å¤§æ½åï¼è½å¤æé«æ¨¡åå¨æ··åæ°æ®éä¸çæ§è½ï¼ä¾å¦ï¼YOLO11å¨ç»åæ°æ®éä¸çmAP@50ä»0.793æé«å°0.796ï¼å¬åçä»0.714æé«å°0.736ï¼ãç¶èï¼åææ°æ®ä»éè¿ä¸æ­¥ç»åä»¥ç¡®ä¿çå®æåéç¨æ§ï¼å°¤å¶æ¯å¨æææ°´ä¸ç¯å¢çèªç¶å¤ææ§ï¼å¦æµæµåº¦ãåç§ååãé®æ¡ï¼æ¹é¢ã
*   <strong>LVLMså¨UODä¸­çåæ¯ä¸æªæ¢ç´¢é¢åï¼</strong> Florence-2 LVLMçå¾®è°å®éªå±ç¤ºäºå¶å¨æ°´ä¸å°ç®æ å®ä½æ¹é¢çå¼ºå¤§è½åãè¿è¡¨æLVLMså¨UODä¸­å·ææ¾èæ½åï¼ä½å¶å¨å®éåºç¨ä¸­ä»é¢ä¸´ææï¼å¦ç±»åå¹»è§ï¼æ¨¡åçææ¼åéè¯¯çç±»åï¼åç¾é¾æ§éå¿ï¼æ¨¡åé¾ä»¥æ³åå°è®­ç»éä¹å¤çå¯¹è±¡ï¼ãæ­¤å¤ï¼LVLMsçå®æ¶åºç¨åå¶ä¼åææ¯ä»æå¾æ·±å¥ç ç©¶ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åææ°æ®çå®æä¸è¶³ï¼</strong> DALL-E 3çæçåæå¾åè½ç¶æ¸æ°ï¼ä½ç¼ºä¹çå®æ°´ä¸åºæ¯ä¸­çèªç¶ç¼ºé·ï¼å¦æµæµåº¦ãåç§åååå¤æé®æ¡ã
*   <strong>åææ°æ®æ æ³¨ææ¬ï¼</strong> çæçåæå¾åä»éè¦æå¨æ æ³¨ï¼è¿ä»ç¶æ¯ä¸ä¸ªèæ¶ä¸èµæºå¯éçè¿ç¨ã
*   <strong>LVLMsçå¹»è§é®é¢ï¼</strong> å¾®è°åçFlorence-2æ¨¡åå¨çæç±»åæ¶åºç°âå¹»è§âï¼å³çææ¼åéè¯¯æä¸åç¡®çç±»åï¼ä¸¥éå½±åäºè¯ä¼°ææ çå¯é æ§ã
*   <strong>LVLMsçç¾é¾æ§éå¿ï¼</strong> å¾®è°åçæ¨¡åé¾ä»¥ä¿çå¶æ´å¹¿æ³çé¢è®­ç»ç¥è¯ï¼å¯¼è´å¨ç¹å®ä»»å¡éåºåå¯¹è®­ç»éä¹å¤çå¯¹è±¡æ³åè½åä¸éã
*   <strong>LVLMså®æ¶åºç¨æªååæ¢ç´¢ï¼</strong> å°½ç®¡LVLMsæ½åå·¨å¤§ï¼ä½å¶å¨èµæºåéçæ°´ä¸ç¯å¢ä¸­çå®æ¶åºç¨åä¼åä»æ¯æªååæ¢ç´¢çé¢åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>é«æå¾®è°ææ¯ï¼</strong> è¿ä¸æ­¥ç ç©¶ééå¨å¾®è°åæç¤ºå¾®è°ç­åæ°é«æææ¯ï¼ä»¥æ´å¥½å°éåºLVLMså¨ä½åç§ãæ£å°åé¢è²å¤±çç­å¤ææ°´ä¸æ¡ä»¶ä¸çåºç¨ï¼åæ¶æå°åè®¡ç®å¼éã
*   <strong>æ´çå®çåææ°æ®çæï¼</strong> ç»åæ©æ£æ¨¡åä¸å¶ä»çææ¹æ³ï¼å¦VAEãGANï¼ï¼ä»¥æé«åææ°´ä¸å¾åçä¿çåº¦ï¼æ´å¥½å°ææå¤æçæ°´ä¸ç¹å¾åç°è±¡ãåæ¶ï¼æ¢ç´¢èªå¨æ æ³¨ç®æ³ä»¥ç®åæ æ³¨æµç¨ã
*   <strong>æ°æ®éæ æ³¨èªå¨åï¼</strong> å©ç¨Label-driven Automated Prompt Tuning (LAPT) ç­æ¡æ¶ï¼éè¿èªå¨åæç¤ºå·¥ç¨åå¾ååæ/æ£ç´¢æ¹æ³ï¼åå°æ°´ä¸å¾åçæå¨æ æ³¨å·¥ä½ã
*   <strong>è½»éçº§å®æ¶å¤çæ¶æï¼</strong> éå¯¹AUVåå®æ¶çæµç³»ç»ï¼å¼ååä¼åè½»éçº§LVLMsæ¶æï¼ä¾å¦éç¨æ¨¡ååªæåTransformeråç¼©ææ¯ï¼ä»¥å®ç°å®æ¶æ£æµã
*   <strong>æ··åæ¹æ³ï¼</strong> æ¢ç´¢ç»åå¤ç§ææ¯ï¼å¦å¾åå¢å¼ºãåææ°æ®ãLVLMsï¼çæ··åæ¹æ³ï¼ä»¥å¨å¤ææ°´ä¸ç¯å¢ä¸­å®ç°æ´é²æ£çæ£æµã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæä¸ºæ°´ä¸ç®æ æ£æµé¢åæä¾äºä¸ä¸ªå¨é¢çè§è§ï¼ä¸ä»æ¸æ°å°éè¿°äºç°æææï¼è¿å¼åæ§å°æ¢è®¨äºå¤§åè§è§è¯­è¨æ¨¡åå¨è§£å³è¿äºé®é¢ä¸çå·¨å¤§æ½åï¼å¹¶ææäºæªæ¥ç ç©¶çå³é®æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>(ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability.</li>
<li>(iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08490v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08490v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08302v1'></a></p>
<h2 id="foundation-models-for-autonomous-driving-perception-a-survey-through-core-capabilities"><a href="https://arxiv.org/abs/2509.08302v1">Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</a></h2>
<p><strong>Authors:</strong> Rajendramayavan Sathyam, Yueqi Li</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Foundation models are revolutionizing autonomous driving perception,
transitioning the field from narrow, task-specific deep learning models to
versatile, general-purpose architectures trained on vast, diverse datasets.
This survey examines how these models address critical challenges in autonomous
perception, including limitations in generalization, scalability, and
robustness to distributional shifts. The survey introduces a novel taxonomy
structured around four essential capabilities for robust performance in dynamic
driving environments: generalized knowledge, spatial understanding,
multi-sensor robustness, and temporal reasoning. For each capability, the
survey elucidates its significance and comprehensively reviews cutting-edge
approaches. Diverging from traditional method-centric surveys, our unique
framework prioritizes conceptual design principles, providing a
capability-driven guide for model development and clearer insights into
foundational aspects. We conclude by discussing key challenges, particularly
those associated with the integration of these capabilities into real-time,
scalable systems, and broader deployment challenges related to computational
demands and ensuring model reliability against issues like hallucinations and
out-of-distribution failures. The survey also outlines crucial future research
directions to enable the safe and effective deployment of foundation models in
autonomous driving systems.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºRajendramayavan SathyamåYueqi Liæ°åçè®ºæâFoundation Models for Autonomous Driving Perception: A Survey Through Core Capabilitiesâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼èªå¨é©¾é©¶æç¥åºç¡æ¨¡åï¼åºäºæ ¸å¿è½åçç»¼è¿°</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³èªå¨é©¾é©¶æç¥é¢åé¢ä¸´çæ ¸å¿ææï¼å³ä¼ ç»çªèå´ãä»»å¡ç¹å®æ·±åº¦å­¦ä¹ æ¨¡åå¨æ³åè½åãå¯æ©å±æ§ä»¥åå¯¹åå¸åç§»çé²æ£æ§æ¹é¢çå±éæ§ãéçåºç¡æ¨¡åï¼Foundation Modelsï¼çå´èµ·ï¼è®ºææ¢è®¨äºè¿äºéç¨åæ¶æå¦ä½é©æ°èªå¨é©¾é©¶æç¥ï¼å¹¶æåºäºä¸ä¸ªæ°é¢çåç±»æ¡æ¶æ¥ç³»ç»å°çè§£ååå±è¿äºæ¨¡åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿åæ°å¨äºæåºäºä¸ä¸ªä»¥âæ ¸å¿è½åâä¸ºå¯¼åçåç±»æ¡æ¶ï¼èéä¼ ç»çä»¥æ¹æ³æä»»å¡ä¸ºä¸­å¿çç»¼è¿°ãè¿åé¡¹æ ¸å¿è½åå¯¹äºå¨å¨æé©¾é©¶ç¯å¢ä¸­å®ç°é²æ£æ§è½è³å³éè¦ï¼
*   <strong>æ³åç¥è¯ï¼Generalized Knowledgeï¼ï¼</strong> æ¨¡ååºè½éåºå¹¿æ³çé©¾é©¶åºæ¯ï¼åæ¬ç½è§ææªæ¾åºç°çæåµï¼å¹¶è½ä»¥ååæ§æ¹å¼æ¨æ­ç»æåå¤çéçä»£çãå®ç°æ¹æ³åæ¬ç¹å¾çº§è¸é¦ãä¼ªæ ç­¾çç£åç´æ¥éæè§è§åºç¡æ¨¡åï¼VFMsï¼ãè§è§è¯­è¨æ¨¡åï¼VLMsï¼åå¤§åè¯­è¨æ¨¡åï¼LLMsï¼ã
*   <strong>ç©ºé´çè§£ï¼Spatial Understandingï¼ï¼</strong> æ¨¡åéå¯¹3Dç©ºé´ç»æåå³ç³»ææ·±å»çè§£ï¼åæ¬æ£æµå·²ç¥åæªç¥ç©ä½ï¼å¹¶æ¨æ­å¶ç©çäº¤äºåæªæ¥è½¨è¿¹ãä¸»è¦æ¹æ³åæ¬æ¾å¼å ç¨ç½ç»ï¼Volumetric Modelsï¼ãåºäºç¥ç»æ¸²æç2Dçç£3Då­¦ä¹ ï¼å¦NeRFå3D Gaussian Splattingï¼ä»¥å3Dæ©ç èªç¼ç å¨ã
*   <strong>å¤ä¼ æå¨é²æ£æ§ï¼Multi-Sensor Robustnessï¼ï¼</strong> ç³»ç»åºå¨åç§ç¯å¢æ¡ä»¶ãä¼ æå¨åªå£°åç¡¬ä»¶éåä¸ä¿æé«æ§è½ãå®ç°æ¹æ³åæ¬è·¨æ¨¡æå¯¹æ¯å­¦ä¹ ãè·¨æ¨¡æç¥è¯è¸é¦ãå¤è§è§å¾åä¸è´æ§ãå¤æ¨¡ææ©ç èªç¼ç å¨åå¤æ¨¡ææ©æ£æ¨¡åã
*   <strong>æ¶é´æ¨çï¼Temporal Reasoningï¼ï¼</strong> æ¨¡åéæææ¶é´ä¾èµæ§å¹¶é¢æµç¯å¢çæªæ¥ç¶æï¼åæ¬å»ºæ¨¡è¿å¨æ¨¡å¼ãè¯å«è¢«é®æ¡ä»£çåæ¨çç©ä½æä¹æ§ãå®ç°æ¹æ³åæ¬æ¶é´ä¸è´æ§4Dé¢æµæ¨¡åï¼å¦åºäºæ©æ£æ¨¡åï¼åæ¶é´å¯¹æ¯å­¦ä¹ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿è¯¦ç»åé¡¾æ¯é¡¹è½åä¸çåæ²¿æ¹æ³ï¼å±ç¤ºäºåºç¡æ¨¡åå¦ä½éè¿å¤§è§æ¨¡ãå¤æ ·åæ°æ®éçèªçç£é¢è®­ç»ï¼å­¦ä¹ éç¨è¡¨ç¤ºå¹¶æè·æ½å¨ä¸çç¥è¯ï¼ä»èå¨èªå¨é©¾é©¶æç¥ä¸­å®ç°æ¾èä¼å¿ãè¿äºæ¨¡åè½å¤ï¼
*   <strong>æé«æ³åè½åï¼</strong> æ´å¥½å°éåºé¿å°¾åºæ¯åæªè§æåµã
*   <strong>å¢å¼ºå¯æ©å±æ§ï¼</strong> åå°å¯¹æè´µæå¨æ æ³¨æ°æ®çä¾èµã
*   <strong>æåé²æ£æ§ï¼</strong> å¨åå¸åç§»ãä¼ æå¨éååæ¶å£å¤©æ°æ¡ä»¶ä¸ä¿ææ§è½ã
*   <strong>å®ç°ç»ä¸è¡¨ç¤ºï¼</strong> ä¿è¿æç¥ä»»å¡é´çæ ç¼éæï¼æé«å¯¹å¤æé©¾é©¶ç¯å¢è§£éçä¸è´æ§ã
*   <strong>æ¯æé«çº§æ¨çï¼</strong> ç»åè¯­è¨æ¨¡åå®ç°æ´é«çº§å«çåºæ¯çè§£åè§åã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
å°½ç®¡åºç¡æ¨¡åæ½åå·¨å¤§ï¼è®ºæä¹æç¡®æåºäºå½åé¢ä¸´çææï¼
*   <strong>è®¡ç®ææ¬åå»¶è¿ï¼</strong> å¤§ååºç¡æ¨¡åçé«è®¡ç®éæ±åæ¨çå»¶è¿ä¸èªå¨é©¾é©¶ç³»ç»çå®æ¶æ§è¦æ±ï¼æ¯«ç§çº§ï¼å­å¨å²çªã
*   <strong>ç³»ç»éæå¤ææ§ï¼</strong> å°è¿äºè½åæ´åå°ç»ä¸ãå®æ¶ãå¯æ©å±çç³»ç»ä¸­æ¯ä¸ä¸ªéå¹³å¡çå·¥ç¨ä»»å¡ã
*   <strong>é¢åé¸¿æ²ï¼</strong> åºç¡æ¨¡åå¨ç½ç»è§æ¨¡æ°æ®ä¸é¢è®­ç»çéç¨ç¥è¯ä¸èªå¨é©¾é©¶ä¸ç¨ä¼ æå¨æ°æ®ï¼å¦LiDARåé·è¾¾ï¼çç¹å®è¦æ±ä¹é´å­å¨å·®å¼ã
*   <strong>å¹»è§é£é©ï¼</strong> æ¨¡åå¯è½äº§çä¸ç°å®ä¸ç¬¦çè¾åºï¼å¸¦æ¥ä¸¥éçå®å¨éæ£ã
*   <strong>åºåæµè¯å±éæ§ï¼</strong> ç°æåºåæµè¯å¤å³æ³¨éç¨åºæ¯ï¼å¿½è§äºç½è§æå®å¨å³é®çæç«¯æåµï¼å¯¼è´æ¨¡åå¨å®éé¨ç½²ä¸­å¯é æ§ä¸è¶³ã
*   <strong>å¯è§£éæ§ä¸è¶³ï¼</strong> å¤§åæç¥åºç¡æ¨¡åçâé»ç®±âæ§è´¨é»ç¢äºå¶å¨å®å¨å³é®ç³»ç»ä¸­çé¨ç½²åçç®¡ä¿¡ä»»ã
*   <strong>æ°æ®åå·®ï¼</strong> è®­ç»æ°æ®å¯è½å­å¨åå·®ï¼å¯¼è´æ¨¡åå¨ç¹å®åºæ¯ï¼å¦æ¶å£å¤©æ°ãå¼±å¿éè·¯ä½¿ç¨èï¼ä¸­è¡¨ç°ä¸ä½³ã
*   <strong>éç¡®å®æ§AIççç®¡ææï¼</strong> ä¼ ç»æ±½è½¦å®å¨æ åé¾ä»¥éªè¯åè®¤è¯å·ææ°å´è¡ä¸ºçéç¡®å®æ§AIç³»ç»ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸å³é®æªæ¥ç ç©¶æ¹åï¼ä»¥å®ç°åºç¡æ¨¡åå¨èªå¨é©¾é©¶ç³»ç»ä¸­çå®å¨ææé¨ç½²ï¼
*   <strong>æ ¸å¿è½åçéæï¼</strong> å¼åè½å¤æ ç¼æ´åæ³åç¥è¯ãç©ºé´çè§£ãå¤ä¼ æå¨é²æ£æ§åæ¶é´æ¨ççç»ä¸ãå®æ¶æä½æ¡æ¶ã
*   <strong>å®æ¶å»¶è¿ç¼è§£ï¼</strong> æ·±å¥ç ç©¶æ¨¡åä¼åææ¯ï¼å¦éåãåªæãç¥è¯è¸é¦ï¼ãä¸ç¨ç¡¬ä»¶å éå¨åè¿è¡æ¶ç­ç¥ï¼å¦å¤éçå¼æ­¥ç®¡éãéæ¶æ¨çï¼ä»¥æ»¡è¶³ä¸¥æ ¼çå»¶è¿è¦æ±ã
*   <strong>æ¹è¿åºåæµè¯ï¼</strong> è½¬ååºäºåºæ¯çæµè¯ï¼ç»åçå®ä¸çæ°æ®ååæå¢å¼ºï¼æç¡®æµè¯é²æ£æ§ï¼å¹¶ä»èåç²¾åº¦è½¬åé¨ç½²ç¸å³å¼¹æ§ã
*   <strong>è§£å³æ°æ®åå·®åç¡®ä¿å¬å¹³æ§è½ï¼</strong> å¼åæ´å·åå®¹æ§åä»£è¡¨æ§çæ°æ®éï¼å©ç¨æ°æ®å¢å¼ºååææ°æ®ï¼å¹¶ç ç©¶ç®æ³å¬å¹³æ§ååå·®ç¼è§£ææ¯ã
*   <strong>ç¼è§£æ¨¡åå¹»è§åå®å¨é£é©ï¼</strong> å¼åä¸æäº§çå¹»è§çæ°åæ¶æï¼åå»ºå¨é¢çåºåæµè¯ä»¥è¯ä¼°æ¨¡åé²æ£æ§ï¼å¹¶å®æ½å®æ¶çæ§ç³»ç»ã
*   <strong>å¢å¼ºå¯è§£éæ§ï¼</strong> æ¨è¿å¯è§£éAIï¼XAIï¼ææ¯ï¼æä¾æ¨¡åå³ç­è¿ç¨çæ¸æ°æ´å¯ï¼ä»¥å»ºç«çç®¡ä¿¡ä»»å¹¶ä¿è¿æéæ¨¡å¼è¯å«ã
*   <strong>éåºçç®¡ææï¼</strong> åå±é²æ£çéªè¯åéªè¯æ¡æ¶ï¼ä»¥éåºéç¡®å®æ§AIççç®¡è¦æ±ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯ç»¼è¿°ä¸ºèªå¨é©¾é©¶æç¥é¢åçåºç¡æ¨¡åç ç©¶æä¾äºä¸ä¸ªç»æåä¸å¯ææ´å¯åçè·¯çº¿å¾ï¼å¼ºè°äºä»è½åè§åº¦åºåè¿è¡æ¨¡åå¼ååè¯ä¼°çéè¦æ§ï¼å¹¶ææäºæªæ¥ç ç©¶çå³é®æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The survey introduces a novel taxonomy
structured around four essential capabilities for robust performance in dynamic
driving environments: generalized knowledge, spatial understanding,
multi-sensor robustness, and temporal reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08302v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08302v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08764v1'></a></p>
<h2 id="argotweak-towards-self-updating-hd-maps-through-structured-priors"><a href="https://arxiv.org/abs/2509.08764v1">ArgoTweak: Towards Self-Updating HD Maps through Structured Priors</a></h2>
<p><strong>Authors:</strong> Lena Wild, Rafael Valencia, Patric Jensfelt</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reliable integration of prior information is crucial for self-verifying and
self-updating HD maps. However, no public dataset includes the required triplet
of prior maps, current maps, and sensor data. As a result, existing methods
must rely on synthetic priors, which create inconsistencies and lead to a
significant sim2real gap. To address this, we introduce ArgoTweak, the first
dataset to complete the triplet with realistic map priors. At its core,
ArgoTweak employs a bijective mapping framework, breaking down large-scale
modifications into fine-grained atomic changes at the map element level, thus
ensuring interpretability. This paradigm shift enables accurate change
detection and integration while preserving unchanged elements with high
fidelity. Experiments show that training models on ArgoTweak significantly
reduces the sim2real gap compared to synthetic priors. Extensive ablations
further highlight the impact of structured priors and detailed change
annotations. By establishing a benchmark for explainable, prior-aided HD
mapping, ArgoTweak advances scalable, self-improving mapping solutions. The
dataset, baselines, map modification toolbox, and further resources are
available at https://kth-rpl.github.io/ArgoTweak/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºä¸åè®¡ç®æºè§è§åæºå¨å­¦ä¹ ä¸å®¶ï¼æå°ä¸ºæ¨æä¾Lena Wild, Rafael Valencia, Patric Jensfeltæ°åçè®ºæâArgoTweak: Towards Self-Updating HD Maps through Structured Priorsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="argotweak-towards-self-updating-hd-maps-through-structured-priors_1">è®ºææè¦ï¼ArgoTweak: Towards Self-Updating HD Maps through Structured Priors</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
èªå¨é©¾é©¶é¢åå¯¹é«ç²¾å°å¾ï¼HD mapsï¼çå¯é èªéªè¯åèªæ´æ°è½åè³å³éè¦ãç¶èï¼ç°ææ¹æ³å¨æ´ååéªä¿¡æ¯æ¶é¢ä¸´æ ¸å¿ææï¼ç¼ºä¹åå«âåéªå°å¾ãå½åä¼ æå¨æ°æ®åææ°çå¼å°å¾âä¸åç»çå¬å±æ°æ®éãè¿å¯¼è´ç°ææ¹æ³ä¸å¾ä¸ä¾èµåæåéªï¼ä»èå¼å¥ä¸ä¸è´æ§å¹¶é ææ¾èçâæ¨¡æå°ç°å®âï¼sim2realï¼å·®è·ï¼ä½¿å¾æ¨¡åé¾ä»¥æ³åå°çå®ä¸çåºæ¯ãæ­¤å¤ï¼ç°æè¯ä¼°ææ æªè½ææåºåæªåååºåçæ§è½åæ°æ´æ°åºåçæ§è½ï¼ä¸åææ°å¨æ¹æ³æ æ³ææçå®ä¸çååçç»æåãè¯­ä¹ç¸å³æ§è´¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ArgoTweakæ°æ®éï¼</strong> è®ºæå¼å¥äºArgoTweakï¼è¿æ¯é¦ä¸ªåå«çå®ä¸çåéªå°å¾ãå½åä¼ æå¨æ°æ®åææ°çå¼å°å¾ä¸åç»çæå¨æ æ³¨æ°æ®éãå®åºäºArgoverse 2 Map Change Dataset [12] æå»ºï¼å¹¶å¯¹çå®ä¸çååè¿è¡äºéæ°æ æ³¨ï¼ä½¿å¶ç¬¦åç°ä»£HDå°å¾æ åï¼ä»èå®ç°äºåéªæ´åæ¹æ³çæ ååè®­ç»åè¯ä¼°ã
*   <strong>åå°ååæ å°æ¡æ¶ï¼</strong> ArgoTweakçæ ¸å¿æ¯ä¸ä¸ªåå°æ å°æ¡æ¶ï¼å®å°å¤§è§æ¨¡å°å¾ä¿®æ¹åè§£ä¸ºå°å¾åç´ çº§å«çç»ç²åº¦åå­ååï¼å¦æå¥ãå é¤ãå ä½ä¿®æ¹ãæ è®°æ´æ°ç­ï¼ï¼ç¡®ä¿äºä¿®æ¹çå¯è§£éæ§ãè¿ç§èå¼è½¬åä½¿å¾ç²¾ç¡®çååæ£æµåæ´åæä¸ºå¯è½ï¼åæ¶ä»¥é«ä¿çåº¦ä¿çæªååçåç´ ã
*   <strong>å¯è§£éçåéªè¾å©æ å°ç½ç»ï¼</strong> è®ºææåºäºä¸ç§çµæ´»çåºçº¿æ¶æï¼è½å¤ä»¥ä¸åå¯è§£éæ§çº§å«ï¼æ æ¾å¼ååè¯ä¼°ãäºåååæ£æµæå®æ´çå¯è§£éæ§æ¨¡åï¼è¿è¡æä½ï¼å¹¶éç¨å¤å¤´è®­ç»æ¥é¢æµæ´æ°åçå°å¾åç´ åå¶ååç¶æã
*   <strong>ç»ç²åº¦è¯ä¼°ææ ï¼</strong> å¼å¥äºä¸ä¸ªå¨é¢çåææ æ¡æ¶ï¼åæ¬ç²ç²åº¦ååæ£æµåç¡®çï¼mACCï¼åç»ç²åº¦å°å¾çæå¹³åç²¾åº¦ï¼mAPCï¼ï¼è½å¤åå«è¯ä¼°æªåååºåçç¨³å®æ§ä»¥åå¯¹æ´æ°çååºè½åï¼ä»èæ­ç¤ºç°æææ çä¸è¶³ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èåå°Sim2Realå·®è·ï¼</strong> å®éªè¡¨æï¼å¨ArgoTweakä¸è®­ç»çæ¨¡åä¸ä½¿ç¨åæåéªçæ¹æ³ç¸æ¯ï¼æ¾èåå°äºsim2realå·®è·ãå¯¹äºmACCcçç»åææ ï¼ArgoTweakæ°æ®éå°sim2realå·®è·åå°äºååä»¥ä¸ã
*   <strong>ç»æååéªåè¯¦ç»æ æ³¨çå½±åï¼</strong> å¹¿æ³çæ¶èå®éªè¿ä¸æ­¥å¼ºè°äºç»æååéªåè¯¦ç»ååæ æ³¨å¯¹æ¨¡åæ§è½åå¯è§£éæ§çéè¦å½±åãç»æè¡¨æï¼ArgoTweakè®­ç»çæ¨¡åè½å¤ææå¤æçå°å¾æ´æ°ï¼èåºäºè§åçåéªæ¨¡åå¾åäºè¿æåè½¦éæ è®°ååï¼åºäºåªå£°çåéªä»æ¯æå¾®å°çå ä½æ ¡æ­£ã
*   <strong>æ°åºåçå»ºç«ï¼</strong> ArgoTweakéè¿æä¾æ°æ®éãåºçº¿æ¨¡åãå°å¾ä¿®æ¹å·¥å·åè¯ä¼°åè®®ï¼ä¸ºå¯è§£éçãåéªè¾å©çHDå°å¾ç»å¶å»ºç«äºæ°åºåï¼æ¨å¨äºå¯æ©å±ãèªæ¹è¿çå°å¾è§£å³æ¹æ¡çåå±ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>çå®ä¸çåéªå°å¾çç¨ç¼ºæ§ï¼</strong> çå®ä¸çä¸­è¿æ¶å°å¾çç¨ç¼ºæ§ä½¿å¾é¾ä»¥è·åè¶³å¤ççå®åéªæ°æ®è¿è¡è®­ç»ï¼å¯¼è´ç°ææ¹æ³ä¸å¾ä¸ä¾èµåæåéªã
*   <strong>ç°æè¯ä¼°ææ çä¸è¶³ï¼</strong> ä¼ ç»çå¹³åç²¾åº¦ï¼mAPï¼ç­ææ æ æ³åºåæ¨¡åå¨æªåååºåçæ§è½åæ°æ´æ°åºåçæ§è½ï¼ä¹æ æ³åæ æ¨¡åè¡ä¸ºçæ·±å±å·®å¼ï¼ä»èæ©çäºç¨³å®æ§ä¸éåºæ§ä¹é´çå³é®æè¡¡ã
*   <strong>å ä½ç²¾åº¦ææï¼</strong> å½åå°å¾çææ¹æ³å¾å¾ç¼ºä¹å¯é åºåç»å¾®éè·¯å½¢ç¶ä¿®æ¹ä¸åªå£°æéçå ä½ç²¾åº¦ã
*   <strong>è®¡ç®æçï¼</strong> å°½ç®¡æ¨¡åæªè¿è¡éåº¦ä¼åï¼ä½å¨åä¸ªNVIDIA A10G GPUä¸ä»è½ä»¥çº¦4 FPSè¿è¡ï¼è¿å¯¹äºå®æ¶åºç¨å¯è½ä»ææåç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ç»ä¸å°å¾çæãååæ£æµåå°å¾æ´æ°ï¼</strong> æªæ¥çæ¹æ³åºå°è¿äºè¿ç¨ç»ä¸èµ·æ¥ï¼éè¿å©ç¨åéªãå¼ºå¶ä¸è´æ§ä»¥åæä¾ç»æåãå¯è§£éçå¤§è§æ¨¡ä¿®æ¹ï¼ä½¿HDå°å¾è½å¤æ¼åä¸ºèªæ¹è¿ãæç»­æ´æ°çéè·¯è¡¨ç¤ºã
*   <strong>æ´å¤æçåå­ååå¤çï¼</strong> è¿ä¸æ­¥æ¢ç´¢åéææ´å¤åå­ååç±»å«ï¼ä»¥å¤çæ´å¤æãå¤æ ·åçå°å¾ä¿®æ¹åºæ¯ã
*   <strong>æåæ¨¡åæ³åè½åï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æåæ¨¡åå¨ä¸åå°çåºååç¯å¢æ¡ä»¶ä¸çæ³åè½åï¼åå°sim2realå·®è·ã
*   <strong>å®æ¶æ§è½ä¼åï¼</strong> ä¼åæ¨¡åæ¶æåç®æ³ï¼ä»¥å®ç°æ´å¿«çæ¨çéåº¦ï¼æ»¡è¶³èªå¨é©¾é©¶ç³»ç»å¯¹å®æ¶å°å¾æ´æ°çéæ±ã
*   <strong>ç»åæ´å¤ä¼ æå¨æ¨¡æï¼</strong> æ¢ç´¢å¦ä½æ´ææå°æ´åæ´å¤ä¼ æå¨æ¨¡æï¼å¦æ¿åé·è¾¾ãæ¯«ç±³æ³¢é·è¾¾ç­ï¼æ¥è¿ä¸æ­¥æé«å°å¾æ´æ°çé²æ£æ§ååç¡®æ§ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce ArgoTweak, the first
dataset to complete the triplet with realistic map priors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08764v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08764v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08738v1'></a></p>
<h2 id="crowdquery-density-guided-query-module-for-enhanced-2d-and-3d-detection-in-crowded-scenes"><a href="https://arxiv.org/abs/2509.08738v1">CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</a></h2>
<p><strong>Authors:</strong> Marius DÃ¤hling, Sebastian Krebs, J. Marius ZÃ¶llner</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper introduces a novel method for end-to-end crowd detection that
leverages object density information to enhance existing transformer-based
detectors. We present CrowdQuery (CQ), whose core component is our CQ module
that predicts and subsequently embeds an object density map. The embedded
density information is then systematically integrated into the decoder.
Existing density map definitions typically depend on head positions or
object-based spatial statistics. Our method extends these definitions to
include individual bounding box dimensions. By incorporating density
information into object queries, our method utilizes density-guided queries to
improve detection in crowded scenes. CQ is universally applicable to both 2D
and 3D detection without requiring additional data. Consequently, we are the
first to design a method that effectively bridges 2D and 3D detection in
crowded environments. We demonstrate the integration of CQ into both a general
2D and 3D transformer-based object detector, introducing the architectures CQ2D
and CQ3D. CQ is not limited to the specific transformer models we selected.
Experiments on the STCrowd dataset for both 2D and 3D domains show significant
performance improvements compared to the base models, outperforming most
state-of-the-art methods. When integrated into a state-of-the-art crowd
detector, CQ can further improve performance on the challenging CrowdHuman
dataset, demonstrating its generalizability. The code is released at
https://github.com/mdaehl/CrowdQuery.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Marius DÃ¤hling, Sebastian Krebs, J. Marius ZÃ¶llneræ°åçè®ºæâCrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenesâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨æ¥æ¤åºæ¯ä¸­è¿è¡2Då3Dç®æ æ£æµï¼ç¹å«æ¯è¡äººæ£æµï¼çææãç°æçåºäºTransformerçæ£æµå¨å¨å¤çå¯éäººç¾¤æ¶å¸¸éå°å°é¾ï¼å ä¸ºå®ä»¬æååååå¸çæ¥è¯¢éè¦éåºå¾åä¸­çå±é¨å¯éåºåãæ­¤å¤ï¼å°½ç®¡2Däººç¾¤æ£æµç ç©¶è¾å¤ï¼ä½3Däººç¾¤æ£æµé¢åä»ç¼ºä¹éå¯¹å¯éåºæ¯çéåºæ§æ¹æ³ï¼å¯¼è´2Då3Dæ£æµä¹é´å­å¨é¸¿æ²ãè®ºæçæ ¸å¿é®é¢æ¯å¦ä½ææå°å©ç¨ç®æ å¯åº¦ä¿¡æ¯æ¥æå¯¼Transformeræ¨¡åï¼ä»èæé«å¨æ¥æ¤ç¯å¢ä¸­çæ£æµæ§è½ï¼å¹¶å¼¥å2Då3Däººç¾¤æ£æµä¹é´çå·®è·ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªåä¸º<strong>CrowdQuery (CQ)</strong> çæ°é¢æ¨¡åï¼å¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>å¯åº¦å¾å®ä¹æ©å±ï¼</strong> ä¼ ç»çå¯åº¦å¾éå¸¸åºäºå¤´é¨ä½ç½®æå¯¹è±¡ç©ºé´ç»è®¡ãCQæ¹æ³å°è¿äºå®ä¹æ©å±ï¼çº³å¥äºåä¸ªè¾¹çæ¡çå°ºå¯¸ï¼å®½åº¦åé«åº¦ï¼ï¼ä»èçææ´ç²¾ç»ãæ´åç¡®çå¯åº¦è¡¨ç¤ºï¼è½å¤åæ éæ¹å½¢ç©ä½å½¢ç¶åä¸å¯¹ç§°æ§ã</li>
<li><strong>å¯åº¦å¼å¯¼çæ¥è¯¢æ¨¡åï¼CQæ¨¡åï¼ï¼</strong> CQæ¨¡åé¢æµå¹¶åµå¥ä¸ä¸ªå¯¹è±¡å¯åº¦å¾ãè¯¥æ¨¡ååå«ä¸¤ä¸ªåæ¯ï¼ä¸ä¸ªåæ¯éè¿å¤å¤´èªæ³¨æåå¢å¼ºå¯åº¦ç¹å¾å¹¶ç¡®ä¿å¨å±ä¿¡æ¯äº¤æ¢ï¼å¦ä¸ä¸ªåæ¯åå­¦ä¹ åºäºé¢æµå¯åº¦å¾çåéå¯åº¦å¾åµå¥ã</li>
<li><strong>å¯åº¦ä¿¡æ¯ç³»ç»éæï¼</strong> åµå¥çå¯åº¦ä¿¡æ¯éè¿äº¤åæ³¨æåæºå¶ç³»ç»å°éæå°Transformerè§£ç å¨ä¸­ï¼ä»¥å¯åº¦å¼å¯¼æ¥è¯¢çæ¹å¼æå¯¼æ£æµè¿ç¨ãè¿ç§éææ¹å¼å¨å¾åç¼ç å¨ä¿¡æ¯ä¹åå¤çå¯åº¦ä¿¡æ¯ï¼ä»¥é¢ååå¤æ¥è¯¢ã</li>
<li><strong>éç¨æ§åè·¨é¢åéç¨æ§ï¼</strong> CQæ¨¡åè¢«è®¾è®¡ä¸ºæ®ééç¨äº2Då3Dæ£æµä»»å¡ï¼æ éé¢å¤æ°æ®ãè®ºæé¦æ¬¡æåºäºææè¿æ¥2Då3Dæ¥æ¤ç¯å¢ä¸­æ£æµçæ¹æ³ï¼å¹¶å±ç¤ºäºå¶å¨éç¨2Dï¼CQ2Dï¼å3Dï¼CQ3Dï¼Transformeræ£æµå¨ä¸­çéæã</li>
<li><strong>ç«¯å°ç«¯è®­ç»ï¼</strong> CQæ¨¡åçéæä¸æ¹åæ´ä½è®­ç»æµç¨ï¼ä»å½±åæå¤±å½æ°ï¼éè¿åç´ çº§L1æå¤±å°ç®æ å¯åº¦å¾ä¸é¢æµå¯åº¦å¾è¿è¡æ¯è¾ï¼ä¿æäºTransformeræ¨¡åçç«¯å°ç«¯å­¦ä¹ ç¹æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæéè¿å¨STCrowdæ°æ®éä¸è¿è¡2Då3Dæ£æµå®éªï¼ä»¥åå¨CrowdHumanæ°æ®éä¸è¿è¡æ³åæ§æµè¯ï¼åå¾äºæ¾èææï¼</p>
<ul>
<li><strong>2Dæ£æµæ§è½æåï¼</strong> å¨STCrowdæ°æ®éä¸ï¼CQ2Då¨APææ ä¸è¶è¶äºå¤§å¤æ°ç°æSOTAæ¹æ³ï¼åæ¬GigaHumanDetï¼APè¾¾å°91.4%ï¼æ¯åºçº¿Deformable DETRæé«äº1.8ä¸ªç¹ï¼MR-2éä½äº6.3ä¸ªç¹ã</li>
<li><strong>3Dæ£æµæ§è½æåï¼</strong> å¨STCrowdæ°æ®éä¸ï¼CQ3Då¨æææ¥åçææ ä¸åä¼äºåºçº¿MonoDETRï¼mAPæé«äº4.2ä¸ªç¹ï¼AR0ãAR1ãAR2åå«æé«äº3.2ã3.4å3.6ä¸ªç¹ãå³ä½¿ä½¿ç¨æ´ç®åçResNet-50éª¨å¹²ç½ç»ï¼CQ3DçmAPä¹æ¯å¶ä»æµè¯æ¹æ³è³å°é«åº3.2ä¸ªç¹ã</li>
<li><strong>ä¸SOTAäººç¾¤æ£æµå¨éæï¼</strong> å½CQæ¨¡åéæå°SOTAäººç¾¤æ£æµå¨DDQ [24]ä¸­æ¶ï¼ç§°ä¸ºCQ2D++ï¼ï¼å¨CrowdHumanæ°æ®éä¸ï¼APè¿ä¸æ­¥æé«å°94.0%ï¼MR-2éä½å°39.0%ï¼è¶è¶äºææå¶ä»æ¹æ³ï¼åæ¬UniHCPåIter-Def-DETRãè¿è¯æäºCQçæ³åè½ååä¸ç°ææ¨¡åçè¯å¥½å¼å®¹æ§ã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> å¯åº¦å¼å¯¼çTransformerãå¯åº¦å¾åµå¥çæ å°æ¹å¼ãåµå¥binçæ°éä»¥åå¯åº¦å¾çç¼©æ¾å å­ç­ç»ä»¶çæææ§å¾å°äºéªè¯ãç»æè¡¨æï¼åååµå¥åè¾é«åè¾¨ççbinè®¡æ°æææä½³ã</li>
<li><strong>è®¡ç®æçï¼</strong> CQ3Då¼å¥äº16.0%çè¿è¡æ¶å¢å ï¼å¶ä¸­ä¸åä¹äºå½å äºCQæ¨¡åä¸­çèªæ³¨æåãåæ°æ°éä»å¢å äº12.9%ï¼è¡¨æè¯¥æ¹æ³å·æè¾é«çè®¡ç®æçã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>3Dæ°æ®éçç¼ºä¹ï¼</strong> è®ºææåºï¼å½åç¼ºä¹éç¨äºæ¥æ¤åºæ¯çåç®3Dæ°æ®éï¼è¿éå¶äº3Däººç¾¤æ£æµçè¿ä¸æ­¥ç ç©¶åè¯ä¼°ã
*   <strong>MR-2æ§è½ç¨³å®æ§ï¼</strong> å¨2Dæ£æµä¸­ï¼æ¥è¯¢å¼æ¹æ³ï¼åæ¬CQ2Dï¼å¨è®­ç»è¿ç¨ä¸­è¡¨ç°åºè¾ä¸ç¨³å®çMR-2æ§è½ï¼å¹¶ä¸æç»æ§è½éå¸¸ä¸å¦åºäºè¾¹çæ¡çæ¹æ³ã
*   <strong>ç¹å®Transformeræ¨¡åçéæ©ï¼</strong> å°½ç®¡CQå·æéç¨æ§ï¼ä½è®ºæä¸»è¦å¨Deformable DETRåMonoDETRä¸è¿è¡äºéªè¯ï¼å¶å¨å¶ä»Transformeræ¨¡åä¸çå·ä½è¡¨ç°ä»éè¿ä¸æ­¥æ¢ç´¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±å°æ´å¤ç½ç»åæ°æ®éï¼</strong> æªæ¥å·¥ä½å°æ¢ç´¢CQæ¹æ³å¨æ´å¤ä¸åTransformerç½ç»åæ°æ®éä¸çæ³åè½åã
*   <strong>éæå°LiDAR-basedæ£æµå¨ï¼</strong> å°CQæ¨¡åæ©å±å°åºäºLiDARçæ£æµå¨ï¼ä»¥è¿ä¸æ­¥æåå¶å¨èªå¨é©¾é©¶ç­é¢åçåºç¨æ½åã
*   <strong>ç»ä¸2Då3Däººç¾¤æ£æµï¼</strong> è®ºæå¸æå¯åç ç©¶äººåå°2Då3Däººç¾¤æ£æµè§ä¸ºä¸ä¸ªæ´ç»ä¸çè¯¾é¢ï¼ä»¥å¼åæ´å¨é¢çè§£å³æ¹æ¡ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥CrowdQueryæ¨¡åï¼å·§å¦å°å°ç²¾ç»åçå¯¹è±¡å¯åº¦ä¿¡æ¯éæå°Transformeræ£æµå¨ä¸­ï¼æ¾èæåäºå¨æ¥æ¤åºæ¯ä¸­2Då3Dç®æ æ£æµçæ§è½ãå¶æ¹æ³è®ºçéç¨æ§åå¨å¤ä¸ªæ°æ®éä¸çä¼å¼è¡¨ç°ï¼ä½¿å¶æä¸ºäººç¾¤æ£æµé¢åçä¸é¡¹éè¦è´¡ç®ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces a novel method for end-to-end crowd detection that
leverages object density information to enhance existing transformer-based
detectors.</li>
<li>We present CrowdQuery (CQ), whose core component is our CQ module
that predicts and subsequently embeds an object density map.</li>
<li>Our method extends these definitions to
include individual bounding box dimensions.</li>
<li>By incorporating density
information into object queries, our method utilizes density-guided queries to
improve detection in crowded scenes.</li>
<li>We demonstrate the integration of CQ into both a general
2D and 3D transformer-based object detector, introducing the architectures CQ2D
and CQ3D.</li>
<li>Experiments on the STCrowd dataset for both 2D and 3D domains show significant
performance improvements compared to the base models, outperforming most
state-of-the-art methods.</li>
<li>When integrated into a state-of-the-art crowd
detector, CQ can further improve performance on the challenging CrowdHuman
dataset, demonstrating its generalizability.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08738v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08738v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08715v1'></a></p>
<h2 id="bcqlm-efficient-vision-language-understanding-with-distilled-q-gated-cross-modal-fusion"><a href="https://arxiv.org/abs/2509.08715v1">BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion</a></h2>
<p><strong>Authors:</strong> Sike Xiang, Shuang Chen, Amir Atapour-Abarghouei</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>As multimodal large language models (MLLMs) advance, their large-scale
architectures pose challenges for deployment in resource-constrained
environments. In the age of large models, where energy efficiency,
computational scalability and environmental sustainability are paramount, the
development of lightweight and high-performance models is critical for
real-world applications. As such, we propose a lightweight MLLM framework for
end-to-end visual question answering. Our proposed approach centres on
BreezeCLIP, a compact yet powerful vision-language encoder optimised for
efficient multimodal understanding. With only 1.2 billion parameters overall,
our model significantly reduces computational cost while achieving performance
comparable to standard-size MLLMs. Experiments conducted on multiple datasets
further validate its effectiveness in balancing accuracy and efficiency. The
modular and extensible design enables generalisation to broader multimodal
tasks. The proposed lightweight vision-language framework is denoted as BcQLM
(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising
path toward deployable MLLMs under practical hardware constraints. The source
code is available at https://github.com/thico0224/BcQLM.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Sike Xiang, Shuang Chen, Amir Atapour-Abarghoueiæ°åçè®ºæâBcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="bcqlm-q">BcQLM: é«æè§è§-è¯­è¨çè§£ä¸è¸é¦Qé¨æ§è·¨æ¨¡æèå</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
éçå¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼çå¿«éåå±ï¼å¶åºå¤§çæ¶æå¨èµæºåéç¯å¢ä¸­é¨ç½²é¢ä¸´å·¨å¤§ææãå¨å¯¹è½æºæçãè®¡ç®å¯æ©å±æ§åç¯å¢å¯æç»­æ§è¦æ±æ¥çæé«çèæ¯ä¸ï¼å¼åè½»éçº§ãé«æ§è½çæ¨¡åå¯¹äºå®éåºç¨è³å³éè¦ãæ¬ææ¨å¨è§£å³å¦ä½å¨ä¿æé«æ§è½çåæ¶ï¼æ¾èéä½MLLMsçè®¡ç®ææ¬ååæ°éï¼ä»èå®ç°é«æçè§è§é®ç­ï¼VQAï¼åå¶ä»å¤æ¨¡æä»»å¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªåä¸º<strong>BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model)</strong> çè½»éçº§MLLMæ¡æ¶ï¼å¶æ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>BreezeCLIPç¼ç å¨ï¼</strong> è¿æ¯ä¸ä¸ªç´§åä½å¼ºå¤§çè§è§-è¯­è¨ç¼ç å¨ï¼æ¨å¨æ¾èéä½è®¡ç®ææ¬ï¼åæ¶å®ç°é«æçå¤æ¨¡æè¡¨ç¤ºãå®éè¿å°åå§CLIPä¸­çBERTåViTéª¨å¹²æ¿æ¢ä¸ºååç½®ç¶é¢è®¾è®¡å¯åçç´§ååTransformeræ¨¡åï¼ä»èå®ç°åæ°éçå¤§å¹åå°ãBreezeCLIPéç¨åéè®­ç»ç­ç¥ï¼å¯¹æ¯å­¦ä¹ ï¼ç¡®ä¿è§è§åææ¬ç¹å¾å¨å±äº«åµå¥ç©ºé´ä¸­è¯å¥½å¯¹é½ï¼åç¥è¯è¸é¦ï¼ä»CLIPæå¸æ¨¡åè½¬ç§»é«çº§è¯­ä¹å¯¹é½ï¼ã</li>
<li><strong>Qé¨æ§è·¨æ¨¡æèåæ¨¡åï¼Q-GCAMï¼ï¼</strong> è¯¥æ¨¡åå¨æè°æ´è§è§åææ¬ç¹å¾çè´¡ç®ï¼ä»¥éåºè¾å¥é®é¢ï¼ä»èå®ç°ç»ç²åº¦ãé®é¢æç¥çæ¨¡æé´äº¤äºãè¿ä½¿å¾æ¨¡åè½å¤æ ¹æ®é®é¢çè¯­ä¹è¿è¡æ´ç²¾ç¡®çæ¨çã</li>
<li><strong>è½»éçº§LLaMA-3.2-1Bè§£ç å¨ï¼</strong> éç¨ä¸ä¸ªåæ°éè¾å°çLLaMAæ¨¡åä½ä¸ºè§£ç å¨ï¼ç¨äºçæç­æ¡ï¼è¿ä¸æ­¥éä½äºè®¡ç®å¼éã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åæ°æçï¼</strong> BcQLMæ¨¡åæ»åæ°éä»ä¸º1.2äº¿ï¼ä¸ç°ææ åå°ºå¯¸çMLLMsç¸æ¯ï¼åæ°éæ¾èåå°ï¼ä»ä¸ºæåè¿æ¹æ³ç10%ï¼ã
*   <strong>æ§è½è¡¨ç°ï¼</strong> å¨GQAãVQAv2åVizWizç­å¤ä¸ªVQAåºåæ°æ®éä¸ï¼BcQLMåå¾äºä¸æ åå°ºå¯¸MLLMsç¸å½çæ§è½ãä¾å¦ï¼å¨224x224åè¾¨çä¸ï¼BcQLMå¨GQAä¸è¾¾å°60.8%ï¼VQAv2ä¸è¾¾å°71.0%ï¼VizWizä¸è¾¾å°49.5%ï¼çè³å¨æäºæåµä¸è¶è¶äºä½¿ç¨æ´é«åè¾¨ççç°ææ¹æ³ãå½è¾å¥åè¾¨çæé«å°336x336æ¶ï¼æ§è½è¿ä¸æ­¥æåï¼å¨GQAä¸è¾¾å°62.4%ï¼VQAv2ä¸è¾¾å°78.7%ï¼VizWizä¸è¾¾å°56.1%ã
*   <strong>æçåæï¼</strong> å¨NVIDIA RTX 4070 Tiä¸è¿è¡çæ¨çæçæµè¯è¡¨æï¼BcQLMçè¿è¡éåº¦æ¯Qwen2.5-VL-3BåGemma3-4Bçä¸¤åï¼åæ¶åå­ä½¿ç¨ééä½çº¦30%ï¼FLOPsæ°éç¸å½ææ´å°ï¼çªæ¾äºå¶å¨è¾¹ç¼é¨ç½²åºæ¯ä¸­çåè¶æçã
*   <strong>ç¹å¾å¤å«è½åï¼</strong> å®éªè¡¨æï¼BreezeCLIPå¨å¤æ¨¡æåµå¥ç©ºé´ä¸­å®ç°äºæ´å¼ºçå¯¹é½ï¼è½å¤æ¸æ°å°åç¦»æ­£è´æ ·æ¬å¯¹ï¼æ¾èæé«äºç¹å¾å¤å«è½åã</p>
<p>è¿äºç»æè¯æäºBcQLMå¨å¹³è¡¡åç¡®æ§åæçæ¹é¢çæææ§ï¼ä¸ºå¨å®éç¡¬ä»¶çº¦æä¸é¨ç½²MLLMsæä¾äºä¸æ¡æåæ¯çéå¾ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®éä¾èµï¼</strong> æ¨¡åä¾èµäºå¬å¼å¯ç¨çè§è§-è¯­è¨æ°æ®éï¼è¿äºæ°æ®éå¯è½æ æ³å®å¨ææçå®ä¸çå¤æ¨¡æåºæ¯çå¤ææ§ååå¸ãæ°æ®éå¤æ ·æ§åè´¨éçéå¶å¯è½å½±åæ¨¡åçæ³åè½åï¼å°¤å¶æ¯å¨éè¦ç»ç²åº¦æä¸ä¸æ¨ççé¢åã
*   <strong>è§£ç å¨éå¶ï¼</strong> ç³»ç»ä¸­ä½¿ç¨çè§£ç å¨ï¼LLaMA-3.2-1Bï¼ä»å¨ææ¬ä¸è¿è¡é¢è®­ç»ï¼å¹¶å¨èåè®­ç»æé´ä¿æå»ç»ãå°½ç®¡è¿ç§è®¾è®¡æé«äºæçåå¯æ§æ§ï¼ä½å¯è½éå¶äºæ¨¡åå¨çæè¿ç¨ä¸­èªéåºæ´åè§è§è¯­ä¹çè½åã
*   <strong>ä»»å¡èå´ï¼</strong> ç®åçæ¹æ³ä¸»è¦å¨VQAåºåä¸è¿è¡äºéªè¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±å°æ´å¨æçæ¨¡æï¼</strong> æªæ¥çå·¥ä½åºæ¢ç´¢å°BcQLMæ©å±å°æ´å¨æçæ¨¡æï¼åæ¬è§é¢ãé³é¢åå®æ¶äº¤äºå¼è®¾ç½®ã
*   <strong>å¢å¼ºæ³åè½åï¼</strong> è§£å³æ°æ®éå¤æ ·æ§åè´¨éçéå¶ï¼ä»¥æé«æ¨¡åå¨æ´å¹¿æ³ãæ´å¤æççå®ä¸çåºæ¯ä¸­çæ³åè½åã
*   <strong>èªéåºè§è§è¯­ä¹æ´åï¼</strong> æ¢ç´¢å¨çæè¿ç¨ä¸­æ´ææå°èªéåºæ´åè§è§è¯­ä¹çæ¹æ³ï¼å¯è½éè¿å¯¹è§£ç å¨è¿è¡æ´çµæ´»çå¾®è°æå¼å¥æ°çèåæºå¶ã</p>
<hr />
<p>è¿ä»½æè¦çªåºäºBcQLMå¨è§£å³MLLMsé¨ç½²æææ¹é¢çåæ°æ§ï¼éè¿è½»éçº§ç¼ç å¨åæºè½èåæ¨¡åï¼å¨ä¿æé«æ§è½çåæ¶æ¾èæåäºæçï¼ä¸ºèµæºåéç¯å¢ä¸çå¤æ¨¡æAIåºç¨å¼è¾äºæ°éå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>As such, we propose a lightweight MLLM framework for
end-to-end visual question answering.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08715v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08715v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08699v1'></a></p>
<h2 id="tango-traversability-aware-navigation-with-local-metric-control-for-topological-goals"><a href="https://arxiv.org/abs/2509.08699v1">TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</a></h2>
<p><strong>Authors:</strong> Stefan Podgorski, Sourav Garg, Mehdi Hosseinzadeh, Lachlan Mares, Feras Dayoub, Ian Reid</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY</p>
<p><strong>Abstract:</strong></p>
<p>Visual navigation in robotics traditionally relies on globally-consistent 3D
maps or learned controllers, which can be computationally expensive and
difficult to generalize across diverse environments. In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers. Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles. We address key
limitations of previous methods by continuously predicting local trajectory
using monocular depth and traversability estimation, and incorporating an
auto-switching mechanism that falls back to a baseline controller when
necessary. The system operates using foundational models, ensuring open-set
applicability without the need for domain-specific fine-tuning. We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability. Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments. The source code is
made publicly available: https://github.com/podgorki/TANGO.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Stefan Podgorskiç­äººæ°åçè®ºæâTANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goalsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="tango-traversability-aware-navigation-with-local-metric-control-for-topological-goals_1">è®ºææè¦ï¼TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ä¼ ç»çæºå¨äººè§è§å¯¼èªæ¹æ³éå¸¸ä¾èµäºå¨å±ä¸è´ç3Då°å¾æé¢è®­ç»æ§å¶å¨ï¼è¿äºæ¹æ³è®¡ç®ææ¬é«æï¼ä¸é¾ä»¥æ³åå°å¤æ ·åçç¯å¢ãè¿éå¶äºæºå¨äººå¨å¼æ¾éï¼open-setï¼ç¯å¢ä¸­è¿è¡é¶æ ·æ¬ï¼zero-shotï¼ãé¿è·ç¦»å¯¼èªçè½åãè®ºææ¨å¨è§£å³å¦ä½å¨ä¸ä¾èµ3Då°å¾æé¢è®­ç»æ§å¶å¨çæåµä¸ï¼å®ç°æºå¨äººå¯¹ç©ä½çº§ææç®æ çé²æ£ãå¯æ³åå¯¼èªï¼åæ¶ææé¿å¼éç¢ç©ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
TANGOæåºäºä¸ç§æ°é¢çãä»åºäºRGBå¾åçãç©ä½çº§ææåº¦éå¯¼èªï¼topometric navigationï¼ç®¡éï¼å¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>ææå¨å±è·¯å¾è§åä¸å±é¨åº¦éè½¨è¿¹æ§å¶çç¬ç¹èåï¼</strong> è®ºæå°å¨å±ææè·¯å¾è§åä¸å±é¨åº¦éè½¨è¿¹æ§å¶ç¸ç»åï¼ä½¿æºå¨äººè½å¤åç©ä½çº§å­ç®æ å¯¼èªï¼åæ¶é¿å¼éç¢ç©ãè¿éè¿è®¡ç®BEVï¼Bird's Eye Viewï¼å¯éè¡æ§å°å¾å®ç°ï¼è¯¥å°å¾ç»åäºåç®æ·±åº¦ä¼°è®¡åå¯éè¡æ§è¯­ä¹ä¼°è®¡ã</li>
<li><strong>è¿ç»­é¢æµå±é¨è½¨è¿¹ï¼</strong> å©ç¨åç®æ·±åº¦åå¯éè¡æ§ä¼°è®¡ï¼ç³»ç»è½å¤è¿ç»­é¢æµå±é¨è½¨è¿¹ï¼åæäºä»¥å¾æ¹æ³å¨å¤çå¨æç¯å¢åéç¢ç©æ¶çå±éæ§ã</li>
<li><strong>èªå¨åæ¢æ§å¶æºå¶ï¼</strong> å½åº¦éå¯éè¡æ§é¢æµä¸å¯é æä¸å¯ç¨æ¶ï¼ä¾å¦æºå¨äººç¦»å¢å¤ªè¿æè¢«éç¢ç©é»æ¡ï¼ï¼ç³»ç»ä¼èªå¨åæ¢ååºäºææçåºçº¿æ§å¶å¨ï¼RoboHopï¼ï¼ç¡®ä¿å¨æææ§åºæ¯ä¸ä»è½ææå¯¼èªã</li>
<li><strong>åºäºåºç¡æ¨¡åçæä½ï¼</strong> ç³»ç»å©ç¨åºç¡æ¨¡åï¼å¦SAMãCLIPãDepth Anythingï¼è¿è¡æç¥ä»»å¡ï¼å¦å¾ååå²ãå¯éè¡æ§ä¼°è®¡åæ·±åº¦ä¼°è®¡ï¼ä»èå®ç°å¼æ¾ééç¨æ§ï¼æ éç¹å®é¢åçå¾®è°ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæå¨æ¨¡æç¯å¢ï¼Habitat-Matterport 3D Datasetï¼åçå®ä¸çæµè¯ä¸­éªè¯äºTANGOæ¹æ³çæææ§ã</p>
<ul>
<li><strong>æ§è½è¶è¶ç°æSOTAæ¹æ³ï¼</strong> TANGOå¨ä¸åè½¨è¿¹é¿åº¦ï¼âeasyâãâhardâãâfullâï¼ä¸ï¼å¨å¯¼èªæåçæ¹é¢æ¾èä¼äºç°æçå­¦ä¹ åæ§å¶å¨ï¼PixNavï¼åä¸å·å¤å¯éè¡æ§æè¯çé¶æ ·æ¬æ§å¶å¨ï¼RoboHopï¼ã</li>
<li><strong>å¼æ¾éæ³åè½åï¼</strong> å®éªè¯æäºTANGOå¨âå·²è§ä½æªè®¿é®è¿âç®æ ï¼seen-but-unvisited goalsï¼å¯¼èªæ¹é¢çè½åï¼è¿å¼ºè°äºç©ä½çº§ææå°å¾çéè¦æ§ï¼è¶è¶äºç®åçâæå¯¼-éå¤âï¼teach-and-repeatï¼èå¼ã</li>
<li><strong>æ¨¡åååé²æ£æ§ï¼</strong> ç³»ç»çæ¨¡ååè®¾è®¡åå¯¹åºç¡æ¨¡åçä¾èµï¼ä½¿å¶å¨é¢å¯¹å°å¾æ¾èååï¼å¦éç¢ç©ï¼æ¶ä»è½ææé¿éï¼å±ç°äºå¶é²æ£æ§åå¯é¨ç½²æ§ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹å¦è¯å°æåºäºå½åç®¡éçä¸äºå±éæ§ï¼è¿äºå±éæ§å¯è½å¯¼è´å¯¼èªå¤±è´¥ï¼</p>
<ul>
<li><strong>æç¥éè¯¯ï¼</strong> å½åè§å¾æ®µä¸åèæ®µå°å¾ä¹é´çä¸æ­£ç¡®å¹éå¯è½å¯¼è´ä¸åç¡®çå­ç®æ ã</li>
<li><strong>è§åä¸è¶³ï¼</strong> å°å¾å¾ä¸­çº¯ææçè¾¹ç¼ºä¹å ä½ä¿¡æ¯ï¼æ æ³å¨å½åå¾åä¸­åºåä¸åå­ç®æ çç¸å³æ§ã</li>
<li><strong>å¯éè¡æ§ä¼°è®¡è¯¯å·®ï¼</strong> åºäºææ¬ååå²çå¯éè¡æ§ä¼°è®¡è½ç¶æ¹ä¾¿ï¼ä½å®¹æåºéï¼è¿å¯¼è´äºéè¦åéæ§å¶å¨çæºå¶ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
å°½ç®¡è®ºææ²¡ææç¡®ååºæªæ¥ç ç©¶æ¹åï¼ä½ä»å¶å±éæ§ä¸­å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼</p>
<ul>
<li><strong>æ¹è¿æç¥æ¨¡åï¼</strong> è¿ä¸æ­¥æååå²åå¹éæ¹æ³çåç¡®æ§ï¼ä»¥åå°ä¸æ­£ç¡®çå­ç®æ åéã</li>
<li><strong>å¢å¼ºææå¾çå ä½æç¥ï¼</strong> æ¢ç´¢å°å ä½ä¿¡æ¯æ´ç´§å¯å°éæå°ææå¾ç»æä¸­ï¼ä»¥æé«è§åçç²¾ç¡®æ§ã</li>
<li><strong>æ´é²æ£çå¯éè¡æ§ä¼°è®¡ï¼</strong> å¼åæ´ç²¾ç¡®ãæ´å°ä¾èµææ¬æç¤ºçå¯éè¡æ§ä¼°è®¡æ¹æ³ï¼åå°å¯¹åéæºå¶çä¾èµã</li>
<li><strong>å¨æç¯å¢éåºæ§ï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½å¨é«åº¦å¨æçç¯å¢ä¸­ï¼å³ä½¿å°å¾åçæ¾èååï¼ä¹è½ä¿æå¯¼èªçé²æ£æ§ã</li>
<li><strong>å¤æ¨¡æèåï¼</strong> æ¢ç´¢æ´å¤æçå¤æ¨¡æä¿¡æ¯èåï¼ä»¥æé«æºå¨äººå¯¹ç¯å¢ççè§£åå¯¼èªè½åã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼TANGOä¸ºå¼æ¾éç¯å¢ä¸­çè§è§å¯¼èªæä¾äºä¸ä¸ªæ°é¢ä¸é«æçè§£å³æ¹æ¡ï¼éè¿å°ææå¨å±è§åä¸å±é¨åº¦éæ§å¶ç¸ç»åï¼å¹¶å©ç¨åºç¡æ¨¡åå®ç°é¶æ ·æ¬æ³åï¼æ¾èæåäºæºå¨äººå¨å¤æåºæ¯ä¸çå¯¼èªè½åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers.</li>
<li>Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles.</li>
<li>We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability.</li>
<li>Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08699v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08699v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08519v1'></a></p>
<h2 id="humo-human-centric-video-generation-via-collaborative-multi-modal-conditioning"><a href="https://arxiv.org/abs/2509.08519v1">HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</a></h2>
<p><strong>Authors:</strong> Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, Zhiyong Wu</p>
<p><strong>Published:</strong> 2025-09-10</p>
<p><strong>Categories:</strong> cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Human-Centric Video Generation (HCVG) methods seek to synthesize human videos
from multimodal inputs, including text, image, and audio. Existing methods
struggle to effectively coordinate these heterogeneous modalities due to two
challenges: the scarcity of training data with paired triplet conditions and
the difficulty of collaborating the sub-tasks of subject preservation and
audio-visual sync with multimodal inputs. In this work, we present HuMo, a
unified HCVG framework for collaborative multimodal control. For the first
challenge, we construct a high-quality dataset with diverse and paired text,
reference images, and audio. For the second challenge, we propose a two-stage
progressive multimodal training paradigm with task-specific strategies. For the
subject preservation task, to maintain the prompt following and visual
generation abilities of the foundation model, we adopt the minimal-invasive
image injection strategy. For the audio-visual sync task, besides the commonly
adopted audio cross-attention layer, we propose a focus-by-predicting strategy
that implicitly guides the model to associate audio with facial regions. For
joint learning of controllabilities across multimodal inputs, building on
previously acquired capabilities, we progressively incorporate the audio-visual
sync task. During inference, for flexible and fine-grained multimodal control,
we design a time-adaptive Classifier-Free Guidance strategy that dynamically
adjusts guidance weights across denoising steps. Extensive experimental results
demonstrate that HuMo surpasses specialized state-of-the-art methods in
sub-tasks, establishing a unified framework for collaborative
multimodal-conditioned HCVG. Project Page:
https://phantom-video.github.io/HuMo.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºLiyang Chenç­äººæ°åçè®ºæâHuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioningâçæè¦ï¼åå®¹åºäºæ¨æä¾çPDFåæè¦ï¼</p>
<p><strong>è®ºææè¦ï¼HuMoï¼éè¿ååå¤æ¨¡ææ¡ä»¶å®ç°ä»¥äººä¸ºä¸­å¿çè§é¢çæ</strong></p>
<p>è¿ç¯è®ºæãHuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioningãç±Liyang Chenåå¶å¢éæ°åï¼æ¨å¨è§£å³ä»¥äººä¸ºä¸­å¿çè§é¢çæï¼HCVGï¼é¢åä¸­çæ ¸å¿ææãHCVGçç®æ æ¯ä»ææ¬ãå¾ååé³é¢ç­å¤æ¨¡æè¾å¥ä¸­åæäººç±»è§é¢ï¼ä½ç°ææ¹æ³å¨ææåè°è¿äºå¼ææ¨¡ææ¹é¢é¢ä¸´ä¸¤å¤§é¾é¢ï¼ä¸æ¯ç¼ºä¹å¸¦æéå¯¹ä¸åç»æ¡ä»¶ï¼ææ¬ãåèå¾åãé³é¢ï¼çé«è´¨éè®­ç»æ°æ®ï¼äºæ¯é¾ä»¥å¨å¤æ¨¡æè¾å¥ä¸ååå®æä¸»ä½ä¿çåé³è§é¢åæ­¥ç­å­ä»»å¡ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³ç°æHCVGæ¹æ³å¨å¤çå¤æ¨¡æè¾å¥ï¼ææ¬ãå¾åãé³é¢ï¼æ¶ï¼é¾ä»¥ææåè°ä¸åæ¨¡æä»¥å®ç°é«è´¨éãå¯æ§çè§é¢çæï¼ç¹å«æ¯å¦ä½åææ°æ®ç¨ç¼ºæ§åå¤æ¨¡æå­ä»»å¡ï¼å¦ä¸»ä½ä¿çåé³è§é¢åæ­¥ï¼ä¹é´çååå°é¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºåºå¯¹ä¸è¿°ææï¼HuMoæåºäºä¸ä¸ªç»ä¸çHCVGæ¡æ¶ï¼å¶å³é®åæ°åæ¹æ³è®ºè´¡ç®åæ¬ï¼
*   <strong>é«è´¨éå¤æ¨¡ææ°æ®éæå»ºï¼</strong> éå¯¹æ°æ®ç¨ç¼ºé®é¢ï¼HuMoæå»ºäºä¸ä¸ªåå«å¤æ ·åãéå¯¹ææ¬ãåèå¾ååé³é¢çé«è´¨éæ°æ®éãè¿éè¿ä¸ä¸ªä¸¤é¶æ®µçæ°æ®å¤çæµç¨å®ç°ï¼é¦åä»å¤§è§æ¨¡ææ¬-è§é¢æ ·æ¬ä¸­æ£ç´¢å·æç¸åè¯­ä¹ä½ä¸åè§è§å±æ§çåèå¾åï¼ç¶åè¿ä¸æ­¥è¿æ»¤å¸¦æåæ­¥é³é¢è½¨éçè§é¢æ ·æ¬ï¼å¹¶è¿è¡è¯­é³å¢å¼ºååé¨å¯¹é½ä¼°è®¡ã
*   <strong>æ¸è¿å¼å¤æ¨¡æè®­ç»èå¼ï¼</strong> éå¯¹å¤æ¨¡æååå°é¾ï¼HuMoæåºäºä¸ç§ä¸¤é¶æ®µæ¸è¿å¼å¤æ¨¡æè®­ç»èå¼ï¼å¹¶éç¨ä»»å¡ç¹å®ç­ç¥ï¼
    *   <strong>ä¸»ä½ä¿çä»»å¡ï¼</strong> éç¨âæå°ä¾µå¥å¼å¾åæ³¨å¥ç­ç¥âï¼å°åèå¾åçVAEæ½å¨è¡¨ç¤ºä¸åªå£°è§é¢æ½å¨è¡¨ç¤ºæ²¿æ¶é´ç»´åº¦æ¼æ¥ï¼å¹¶éå¶åæ°æ´æ°å¨DiTçèªæ³¨æåå±ï¼ä»¥å¨ä¸æå®³åºç¡æ¨¡åææ¬éµå¾ªåè§è§çæè½åçåæä¸ï¼å®ç°ä¸»ä½ä¸è´æ§ã
    *   <strong>é³è§é¢åæ­¥ä»»å¡ï¼</strong> é¤äºå¸¸ç¨çé³é¢äº¤åæ³¨æåå±å¤ï¼HuMoå¼å¥äºâèç¦é¢æµç­ç¥âï¼focus-by-predicting strategyï¼ãéè¿å¼å¥ä¸ä¸ªé¢é¨åºåé¢æµå¨Fmaskï¼å¹¶ä½¿ç¨äºè¿å¶äº¤åçµæå¤±è¿è¡çç£ï¼éå¼å¼å¯¼æ¨¡åå°é³é¢ä¸é¢é¨åºåå³èèµ·æ¥ï¼ä»èå¢å¼ºé³è§é¢åæ­¥ãä¸ºäºç¡®ä¿ä¸»ä½ä¿çè½åä¸è¢«åå¼±ï¼é³è§é¢åæ­¥ä»»å¡æ¯æ¸è¿å¼å°æ´åå°è®­ç»ä¸­çã
*   <strong>æ¶é´èªéåºåç±»å¨èªç±å¼å¯¼ï¼CFGï¼ç­ç¥ï¼</strong> å¨æ¨çé¶æ®µï¼HuMoè®¾è®¡äºä¸ç§æ¶é´èªéåºCFGç­ç¥ï¼å¨æè°æ´å»åªæ­¥éª¤ä¸­çå¼å¯¼æéãæ©ææ­¥éª¤ä¾§éäºææ¬/å¾åä¸»å¯¼çè¯­ä¹ç»æåç©ºé´å¸å±ï¼åææ­¥éª¤åå¼ºè°é³é¢åå¾åæ§å¶ï¼ä»¥å®ç°çµæ´»ãç»ç²åº¦åååçå¤æ¨¡ææ§å¶ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éªç»æè¡¨æï¼HuMoå¨ä¸»ä½ä¿çåé³è§é¢åæ­¥ç­å­ä»»å¡ä¸è¶è¶äºä¸é¨çç°æï¼SOTAï¼æ¹æ³ãè¿è¯æäºHuMoä½ä¸ºä¸ä¸ªç»ä¸æ¡æ¶ï¼è½å¤å®ç°ååå¤æ¨¡ææ¡ä»¶ä¸çHCVGï¼å¹¶è½çæé«è´¨éãä¸»ä½ä¸è´ä¸é³è§é¢åæ­¥çè§é¢ãè®ºæè¿éè¿å¨1.7Bå17Båæ°æ¨¡åä¸çéªè¯ï¼å±ç¤ºäºå¶æææ§åå¯æ©å±æ§ãå®æ§ç»æï¼å¦å¾5åå¾6æç¤ºï¼è¿ä¸æ­¥æ¯æäºHuMoå¨ææ¬éµå¾ªãä¸»ä½ä¿çåé³è§é¢åæ­¥æ¹é¢çåè¶æ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®ååºå½åæ¹æ³çå±éæ§ï¼ä½ä»å¶âä¼¦çèéâé¨åå¯ä»¥æ¨æ­åºæ½å¨çç¤¾ä¼åææ¯ææãä¾å¦ï¼çæé¼çäººç±»è§é¢çè½åå¯è½è¢«æ»¥ç¨ï¼å¦å¶ä½æ·±åº¦ä¼ªé ææªç»åæçåå®¹ãæ­¤å¤ï¼å¯¹çæåå®¹çç»ç²åº¦æ§å¶ä¹è¦æ±è´è´£ä»»çä½¿ç¨æåï¼ä»¥é²æ­¢æçºµæéè¯¯ä¿¡æ¯ãè¿äºè½ç¶ä¸æ¯ææ¯ä¸çå±éï¼ä½ä»£è¡¨äºè¯¥ææ¯å¨å®éåºç¨ä¸­éè¦é¢å¯¹çéè¦ææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åä¼¦çèéå¯ä»¥æ¨æ­ï¼
*   <strong>å¢å¼ºé²æ£æ§åæ³åæ§ï¼</strong> è¿ä¸æ­¥æåæ¨¡åå¨æ´å¤æãå¤æ ·ååºæ¯åæªè§æ°æ®ä¸çé²æ£æ§åæ³åè½åã
*   <strong>æ´ç²¾ç»çæ§å¶ç²åº¦ï¼</strong> æ¢ç´¢æ´ç»ç²åº¦çå¤æ¨¡ææ§å¶ï¼ä¾å¦å¯¹ç¹å®é¢é¨è¡¨æãèº«ä½å§¿æææè£ç»èçç²¾ç¡®æ§å¶ã
*   <strong>ä¼¦çåå®å¨æºå¶ï¼</strong> éå¯¹æ½å¨çæ»¥ç¨é£é©ï¼å¼ååéææ´å¼ºå¤§çä¼¦çåå®å¨æºå¶ï¼åæ¬æ°´å°ãæ£æµå·¥å·åè´è´£ä»»çä½¿ç¨ç­ç¥ï¼ä»¥ç¡®ä¿ææ¯è¢«ç¨äºç§¯æç®çã
*   <strong>æ©å±å°æ´å¹¿æ³çåºç¨ï¼</strong> å°HCVGææ¯åºç¨äºæ´å¹¿æ³çé¢åï¼å¦èæç°å®ãæ¸¸æãæè²åä¸ªæ§ååå®¹åä½ã</p>
<p>æ»èè¨ä¹ï¼HuMoä¸ºä»¥äººä¸ºä¸­å¿çè§é¢çæé¢åæä¾äºä¸ä¸ªå¨é¢ä¸åæ°çè§£å³æ¹æ¡ï¼éè¿å¶æ°æ®å¤çãè®­ç»èå¼åæ¨çç­ç¥ï¼ææå°è§£å³äºå¤æ¨¡æåè°çé¾é¢ï¼ä¸ºè¯¥é¢åæªæ¥çåå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present HuMo, a
unified HCVG framework for collaborative multimodal control.</li>
<li>For the second challenge, we propose a two-stage
progressive multimodal training paradigm with task-specific strategies.</li>
<li>For the audio-visual sync task, besides the commonly
adopted audio cross-attention layer, we propose a focus-by-predicting strategy
that implicitly guides the model to associate audio with facial regions.</li>
<li>Extensive experimental results
demonstrate that HuMo surpasses specialized state-of-the-art methods in
sub-tasks, establishing a unified framework for collaborative
multimodal-conditioned HCVG.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08519v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08519v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.08104v1'></a></p>
<h2 id="apml-adaptive-probabilistic-matching-loss-for-robust-3d-point-cloud-reconstruction"><a href="https://arxiv.org/abs/2509.08104v1">APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</a></h2>
<p><strong>Authors:</strong> Sasan Sharifipour, Constantino Ãlvarez Casado, Mohammad Sabokrou, Miguel Bordallo LÃ³pez</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Training deep learning models for point cloud prediction tasks such as shape
completion and generation depends critically on loss functions that measure
discrepancies between predicted and ground-truth point sets. Commonly used
functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on
nearest-neighbor assignments, which often induce many-to-one correspondences,
leading to point congestion in dense regions and poor coverage in sparse
regions. These losses also involve non-differentiable operations due to index
selection, which may affect gradient-based optimization. Earth Mover Distance
(EMD) enforces one-to-one correspondences and captures structural similarity
more effectively, but its cubic computational complexity limits its practical
use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully
differentiable approximation of one-to-one matching that leverages Sinkhorn
iterations on a temperature-scaled similarity matrix derived from pairwise
distances. We analytically compute the temperature to guarantee a minimum
assignment probability, eliminating manual tuning. APML achieves near-quadratic
runtime, comparable to Chamfer-based losses, and avoids non-differentiable
operations. When integrated into state-of-the-art architectures (PoinTr, PCN,
FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)
that generates 3D human point clouds from WiFi CSI measurements, APM loss
yields faster convergence, superior spatial distribution, especially in
low-density regions, and improved or on-par quantitative performance without
additional hyperparameter search. The code is available at:
https://github.com/apm-loss/apml.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Sasan Sharifipour, Constantino Ãlvarez Casado, Mohammad Sabokrou, Miguel Bordallo LÃ³pezæ°åçè®ºæâAPML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstructionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³3Dç¹äºé¢æµä»»å¡ï¼å¦å½¢ç¶è¡¥å¨åçæï¼ä¸­ç°ææå¤±å½æ°çå±éæ§ãå½åå¸¸ç¨çæå¤±å½æ°ï¼å¦Chamfer Distance (CD)åå¶åä½ï¼HyperCD, InfoCDï¼ï¼ä¾èµäºæè¿é»åéï¼è¿ä¼å¯¼è´å¤å¯¹ä¸çå¯¹åºå³ç³»ï¼é æç¹äºå¨å¯éåºåæ¥å µãç¨çåºåè¦çä¸è¶³ï¼å¹¶ä¸ç±äºç´¢å¼éæ©æ¶åä¸å¯å¾®åæä½ï¼å½±åæ¢¯åº¦ä¼åãèEarth Mover Distance (EMD)è½ç¶è½ææææç»æç¸ä¼¼æ§å¹¶å¼ºå¶ä¸å¯¹ä¸å¯¹åºï¼ä½å¶ç«æ¹çº§çè®¡ç®å¤æåº¦ä½¿å¶å¨å¤§è§æ¨¡æ·±åº¦å­¦ä¹ ä¸­ä¸åå®éãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¼åä¸ç§æ¢è½æä¾EMDçå ä½çç£ä¼å¿ï¼åè½é¿åå¶é«æè®¡ç®ææ¬åä¸å¯å¾®åé®é¢çæå¤±å½æ°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
è®ºææåºäº<strong>èªéåºæ¦çå¹éæå¤± (Adaptive Probabilistic Matching Loss, APML)</strong>ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>è¿ä¼¼ä¸å¯¹ä¸å¹éä¸Sinkhornè¿­ä»£ï¼</strong> APMLéè¿å¨åºäºæå¯¹è·ç¦»çæ¸©åº¦ç¼©æ¾ç¸ä¼¼æ§ç©éµä¸åºç¨Sinkhornè¿­ä»£ï¼æä¾äºä¸ç§å®å¨å¯å¾®åçä¸å¯¹ä¸å¹éè¿ä¼¼ãè¿ä½¿å¾æ¨¡åè½å¤å»ºç«è½¯æ§ãæ¦çæ§çå¯¹åºå³ç³»ï¼ä»èæ´å¥½å°ææç¹äºçç»æç¸ä¼¼æ§ã
*   <strong>æ°æ®é©±å¨çèªéåºæ¸©åº¦è®¡ç®ï¼</strong> APMLå¼å¥äºä¸ç§æ°é¢çãæ°æ®é©±å¨çãåææ¨å¯¼çæ¸©åº¦è°åº¦æºå¶ãè¯¥æºå¶æ ¹æ®ç¹äºçå±é¨å ä½ä¸ä¸æèªå¨è®¡ç®æ¸©åº¦ï¼ä»¥ä¿è¯æ¯ä¸ªç¹è³å°æä¸ä¸ªæå°çåéæ¦çï¼ä»èæ¶é¤äºæå¨è°æ´æ­£åååæ°çéè¦ï¼æé«äºè®­ç»çç¨³å®æ§åæ³åè½åã
*   <strong>è¿äºæ¬¡æ¶é´å¤æåº¦ï¼</strong> APMLçè®¡ç®å¤æåº¦æ¥è¿äºæ¬¡ï¼O(NM(d+L))ï¼ï¼ä¸Chamfer-basedæå¤±ç¸å½ï¼è¿ä½äºEMDçç«æ¹å¤æåº¦ãåæ¶ï¼å®é¿åäºä¸å¯å¾®åæä½ï¼ç¡®ä¿äºå¹³æ»çæ¢¯åº¦æµã
*   <strong>ååä¸è´æ§ä¸Sinkhornå½ä¸åï¼</strong> APMLéè¿ååï¼é¢æµå°çå¼åçå¼å°é¢æµï¼çè½¯åéç©éµå¹³ååæ¥ç¡®ä¿ä¸è´æ§ï¼å¹¶è¿ä¸æ­¥éè¿Sinkhorn-Knoppç®æ³è¿è¡è¿­ä»£å½ä¸åï¼ä»¥çæè¿ä¼¼åéæºçä¼ è¾è®¡åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æåï¼</strong> APMLå¨ShapeNetåºåæµè¯ï¼PCN, FoldingNet, PoinTrï¼ååºäºWiFi-CSIæµéçæ3Däººä½ç¹äºçæ¶ç©ºTransformer (CSI2PC) ç­æåè¿æ¶æä¸è¿è¡éæåè¯ä¼°ãç»ææ¾ç¤ºï¼APMLå¨EMDææ ä¸æ¾èä¼äºç°ææå¤±å½æ°ï¼éå¸¸éä½15-81%ï¼ï¼åæ¶ä¿ææç¥å¾®æåF1åæ°ãè¿è¡¨æAPMLè½çæå ä½ä¸æ´å¿ å®ãç»ææ´è¿è´¯çç¹äºéå»ºã
*   <strong>æ´å¿«çæ¶æéåº¦ï¼</strong> APMLå¨è®­ç»è¿ç¨ä¸­è¡¨ç°åºæ´å¿«çæ¶æéåº¦ï¼å¨æ´å°çepochåè¾¾å°æ´é«çæ§è½ï¼ä»èéä½äºææçè®¡ç®é¢ç®ã
*   <strong>æ´å¥½çç©ºé´åå¸åç¨çåºåè¦çï¼</strong> ç¸æ¯Chamfer-basedæå¤±ï¼APMLå¨ä½å¯åº¦åºåè½æ´å¥½å°ä¿çç»æï¼åå°ç¹äºèæ¢ï¼å¹¶è½è·¨ä¸åè¾å¥æ¨¡æè¿è¡æ³åã
*   <strong>æ éé¢å¤è¶åæ°æç´¢ï¼</strong> APMLçèªéåºæ¸©åº¦æºå¶æ¶é¤äºæå¨è°æ´æ­£åååæ°çéè¦ï¼ä½¿å¶æä¸ºä¸ä¸ªæäºä½¿ç¨çâå³æå³ç¨âæ¿ä»£åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è¶åæ°Pminï¼</strong> å°½ç®¡APMLæ¶é¤äºSinkhornæ­£åååæ°Îµï¼ä½å¼å¥äºä¸ä¸ªæ°çè¶åæ°ââè½¯åééå¼Pminãè®ºæä¸­Pminè¢«åºå®ä¸º0.8ï¼å¹¶æªè¿è¡è°ä¼ï¼å¶æææ§åææ¯æªæ¥çå·¥ä½ã
*   <strong>åå­äºæ¬¡æ©å±ï¼</strong> APMLæéçåå­ä»ç¶éç¹äºæ°éåäºæ¬¡æ¹å¢é¿ãè½ç¶ç»éªæ§ç ç©¶è¡¨æä¼ è¾ç©éµå¨Sinkhornå½ä¸ååå¤§é¨åæ¯ç¨ççï¼è¶è¿90%çæ¡ç®æ¥è¿é¶ï¼ï¼ä½ç®åä»ä»¥å¯éå½¢å¼å­å¨ãè¿éå¶äºæ´å¤§æ¹éå¤§å°çä½¿ç¨ã
*   <strong>æªååå©ç¨ç¨çæ§ï¼</strong> å½åå®ç°æªååå©ç¨ä¼ è¾ç©éµçç¨çæ§ï¼ä¹æªä½¿ç¨FP16æ··åç²¾åº¦ï¼è¿å¯¼è´åå­å ç¨è¾é«ã
*   <strong>è¯ä¼°èå´ï¼</strong> è¯ä¼°ä¸»è¦éä¸­å¨ä¸¤ä¸ªåæè¡¥å¨æ°æ®éåä¸ä¸ªçå®çææ°æ®éï¼å°æªå¨ScanNetæKITTIç­çå®æ«ææ°æ®éä¸è¿è¡è¡¥å¨è¯ä¼°ï¼ä¹æªè¶è¶è½®å»çæä»»å¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>Pminçæ¿ä»£æ¹æ¡ï¼</strong> æ¢ç´¢Pminçå¯å­¦ä¹ æåºäºè°åº¦çæ¹æ³ã
*   <strong>åå­ä¼åï¼</strong> å¼åä½ç§©æåçSinkhornåä½ä»¥åå°åå­ä½¿ç¨ï¼å¹¶å®ç°å®å¨ä¼åçCUDAåæ ¸ä»¥å©ç¨ä¼ è¾ç©éµçç¨çæ§ï¼ä»èå°ææææ¬æ¨åO(N log N)ã
*   <strong>æ©å±è¯ä¼°èå´ï¼</strong> å°APMLçè¯ä¼°æ©å±å°åªå£°ãçå®ä¸çæ«æåéæ¬§å éå¾é¢åï¼å¦è¡¨é¢æå¾ï¼ï¼ä»¥è¿ä¸æ­¥éªè¯å¶é²æ£æ§ã
*   <strong>å®éåºç¨ï¼</strong> å°APMLåºç¨äºæºå¨äººãARåæ°å­å­ªçç­éè¦é«æç¥ç»æä¿çåº¦çå®éé¨ç½²åºæ¯ï¼å¹¶å¯è½ä¿è¿è¾¹ç¼è®¾å¤çè½»éçº§ãèè½æ¨¡åå¼åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose the Adaptive Probabilistic Matching Loss (APML), a fully
differentiable approximation of one-to-one matching that leverages Sinkhorn
iterations on a temperature-scaled similarity matrix derived from pairwise
distances.</li>
<li>When integrated into state-of-the-art architectures (PoinTr, PCN,
FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)
that generates 3D human point clouds from WiFi CSI measurements, APM loss
yields faster convergence, superior spatial distribution, especially in
low-density regions, and improved or on-par quantitative performance without
additional hyperparameter search.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.08104v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.08104v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07978v1'></a></p>
<h2 id="one-view-many-worlds-single-image-to-3d-object-meets-generative-domain-randomization-for-one-shot-6d-pose-estimation"><a href="https://arxiv.org/abs/2509.07978v1">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></h2>
<p><strong>Authors:</strong> Zheng Geng, Nan Wang, Shaocong Xu, Chongjie Ye, Bohan Li, Zhaoxi Chen, Sida Peng, Hao Zhao</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Estimating the 6D pose of arbitrary unseen objects from a single reference
image is critical for robotics operating in the long-tail of real-world
instances. However, this setting is notoriously challenging: 3D models are
rarely available, single-view reconstructions lack metric scale, and domain
gaps between generated models and real-world images undermine robustness. We
propose OnePoseViaGen, a pipeline that tackles these challenges through two key
components. First, a coarse-to-fine alignment module jointly refines scale and
pose by combining multi-view feature matching with render-and-compare
refinement. Second, a text-guided generative domain randomization strategy
diversifies textures, enabling effective fine-tuning of pose estimators with
synthetic data. Together, these steps allow high-fidelity single-view 3D
generation to support reliable one-shot 6D pose estimation. On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches. We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation. Project page:
https://gzwsama.github.io/OnePoseviaGen.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zheng Gengç­äººæ°åçè®ºæâOne View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººææ¯ä¸­ä¸ä¸ªæ ¸å¿ä¸æå·æææ§çé®é¢ï¼å¦ä½ä»å­ä¸å¼ åèå¾åï¼å¯¹ä»»ææªè§è¿çç©ä½è¿è¡ä¸æ¬¡æ§ï¼one-shotï¼6Då§¿æä¼°è®¡ï¼å³ç¡®å®å¶å¨ä¸ç»´ç©ºé´ä¸­çç²¾ç¡®ä½ç½®åæ¹åï¼ãè¿ä¸ä»»å¡å¨ç°å®ä¸çä¸­å°¤å¶å°é¾ï¼å ä¸º3Dæ¨¡åéå¸¸ä¸å¯ç¨ï¼åè§å¾éå»ºç¼ºä¹åº¦éå°ºåº¦ï¼ä¸çææ¨¡åä¸çå®å¾åä¹é´å­å¨é¢åé¸¿æ²ï¼è¿äºå ç´ é½ä¸¥éå½±åäºå§¿æä¼°è®¡çé²æ£æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
ä½èæåºäºä¸ä¸ªåä¸º <strong>OnePoseViaGen</strong> çç«¯å°ç«¯ç®¡éï¼éè¿ä¸¤ä¸ªå³é®ç»ä»¶åæäºä¸è¿°ææï¼</p>
<ul>
<li><strong>çæå¼ä¸æ¬¡æ§6Då§¿æç®¡éï¼</strong> OnePoseViaGenæ¯é¦ä¸ªå°åè§å¾3Dçææ´åå°è®­ç»åæ¨çä¸­ï¼ç¨äºä¸æ¬¡æ§6Då§¿æåå°ºåº¦ä¼°è®¡çç®¡éãå®è¯æäºçæå¼å»ºæ¨¡å¯ä»¥ç´æ¥ä¿è¿å§¿æä¼°è®¡ã</li>
<li><strong>ç²å°ç²¾çåº¦éå¯¹é½æ¨¡åï¼</strong> è¯¥æ¨¡åéè¿ç»åå¤è§å¾ç¹å¾å¹éåæ¸²æ-æ¯è¾ï¼render-and-compareï¼ç»åï¼å±åä¼åäºç©ä½çå°ºåº¦åå§¿æãé¦åï¼ä»åå¼ RGB-Déç¹å¾åçæä¸ä¸ªç¼ºä¹çå®ä¸çå°ºåº¦åå§¿æççº¹ç3Dæ¨¡åãç¶åï¼éè¿ç²ç¥å¯¹é½ï¼ä½¿ç¨SuperGlueè¿è¡2D-3Dç¹å¾å¹éåPnPæ±è§£ï¼è·å¾åå§å§¿æåå°ºåº¦ä¼°è®¡ãæ¥çï¼éè¿è¿­ä»£çæ¸²æ-æ¯è¾ç»åï¼å©ç¨FoundationPoseç½ç»ï¼è¿ä¸æ­¥ç²¾ç»åå§¿æåå°ºåº¦ï¼ä»èå®ç°ä»åå¼ å¾åä¸­åç¡®æ¢å¤åº¦éå°ºåº¦ã</li>
<li><strong>ææ¬å¼å¯¼ççæå¼é¢åéæºåç­ç¥ï¼</strong> ä¸ºäºå¼¥åçææ¨¡åä¸çå®ä¸çå¾åä¹é´çé¢åé¸¿æ²ï¼è¯¥æ¹æ³å¼å¥äºä¸ç§ææ¬é©±å¨çå¢å¼ºç­ç¥ãå®å©ç¨ææ¬æç¤ºï¼ä¾å¦ï¼éè¿VLMçæè¯¦ç»æè¿°ï¼å¼å¯¼3Dçææ¨¡åï¼å¦Trellisï¼åå»ºç»æä¸è´ä½çº¹çå¤æ ·çç©ä½åä½ãè¿äºåä½å¨éæºåçåç§ãèæ¯åé®æ¡æ¡ä»¶ä¸è¿è¡æ¸²æï¼çæå¤§è§æ¨¡åææ°æ®éï¼ç¨äºå§¿æä¼°è®¡å¨çææå¾®è°ï¼ä»èæ¾èæé«é²æ£æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
OnePoseViaGenå¨å¤ä¸ªå·ææææ§çåºåæµè¯ï¼YCBInEOATãToyota-LightãLM-Oï¼ä¸åå¾äºæ¾èçSOTAï¼State-of-the-Artï¼æ§è½ï¼è¿è¶ç°ææ¹æ³ãä¾å¦ï¼å¨YCBInEOATæ°æ®éä¸ï¼OnePoseViaGenå¨ADDææ ä¸åå¾äº81.3çå¹³ååæ°ï¼èåºçº¿æ¹æ³ï¼å¦OryonãLoFTRãGediï¼å¨è¯¥ä¸æ¬¡æ§è®¾ç½®ä¸è¡¨ç°ä¸ä½³ãå¨LM-OåTOYLç­æ°æ®éä¸ï¼è¯¥æ¹æ³å¨ARãMSSDãMSPDåVSDç­ææè¯ä¼°æ åä¸åæ¾ç¤ºåºæç»­æ¹è¿ã</p>
<p>æ­¤å¤ï¼è®ºæéè¿å¨çå®æºå¨äººï¼éå¤XHAND1çµå·§æï¼ä¸è¿è¡é²æ£ççµå·§æåä»»å¡ï¼éªè¯äºè¯¥æ¹æ³å¨å®éä¸çæä½ä¸­çå®ç¨æ§ãè¿è¡¨æOnePoseViaGenä¸ä»å¨çè®ºä¸ææï¼èä¸å¨å®éåºç¨ä¸­ä¹å·æå¾é«çå¯é æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
å°½ç®¡OnePoseViaGenåå¾äºä»¤äººé¼èçææï¼ä½å®å¨å¤ç<strong>å¯åå½¢æå³èå¼ç©ä½</strong>æ¶ä»é¢ä¸´ææãå¨è¿ç§æåµä¸ï¼ç©ä½å½¢ç¶çååå¯è½å¯¼è´6Då§¿æä¼°è®¡ä¸åç¡®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
æªæ¥çå·¥ä½å°ä¾§éäºå°<strong>æµè¯æ¶è®­ç»ï¼test-time trainingï¼</strong>æ´åå°æ¨çç®¡éä¸­ï¼ä»¥å®ç°å¯¹å¯åå½¢ç©ä½å ä½å½¢ç¶çæç»­ç»åååç¡®å§¿æä¼°è®¡ãè¿å°ååå©ç¨çææ¨¡åå¨6Då§¿æä¼°è®¡ä»»å¡ä¸­ççµæ´»æ§åæ³åè½åã</p>
<hr />
<p>æ»èè¨ä¹ï¼OnePoseViaGenéè¿å¶åæ°çç²å°ç²¾å¯¹é½æ¨¡ååææ¬å¼å¯¼ççæå¼é¢åéæºåç­ç¥ï¼ä¸ºä¸æ¬¡æ§6Då§¿æä¼°è®¡æä¾äºä¸ä¸ªå¼ºå¤§ä¸å®ç¨çè§£å³æ¹æ¡ï¼æ¾èæ¨å¨äºæºå¨äººæä½åè®¡ç®æºè§è§é¢åçåå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches.</li>
<li>We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07978v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07978v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-11 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
